id,repository_name,file_path,class_name,human_written_code,class_skeleton,snippet_id,total_program_units,openai/gpt-oss-20b_generated_code
17249,jbarlow83/OCRmyPDF,src/ocrmypdf/quality.py,ocrmypdf.quality.OcrQualityDictionary,"import re
from collections.abc import Iterable

class OcrQualityDictionary:
    """"""Manages a dictionary for simple OCR quality checks.""""""

    def __init__(self, *, wordlist: Iterable[str]):
        """"""Construct a dictionary from a list of words.

        Words for which capitalization is important should be capitalized in the
        dictionary. Words that contain spaces or other punctuation will never match.
        """"""
        self.dictionary = set(wordlist)

    def measure_words_matched(self, ocr_text: str) -> float:
        """"""Check how many unique words in the OCR text match a dictionary.

        Words with mixed capitalized are only considered a match if the test word
        matches that capitalization.

        Returns:
            number of words that match / number
        """"""
        text = re.sub('[0-9_]+', ' ', ocr_text)
        text = re.sub('\\W+', ' ', text)
        text_words_list = re.split('\\s+', text)
        text_words = {w for w in text_words_list if len(w) >= 3}
        matches = 0
        for w in text_words:
            if w in self.dictionary or (w != w.lower() and w.lower() in self.dictionary):
                matches += 1
        if matches > 0:
            hit_ratio = matches / len(text_words)
        else:
            hit_ratio = 0.0
        return hit_ratio","
class OcrQualityDictionary:
    '''Manages a dictionary for simple OCR quality checks.'''
    def __init__(self, *, wordlist: Iterable[str]):
        '''Construct a dictionary from a list of words.
        Words for which capitalization is important should be capitalized in the
        dictionary. Words that contain spaces or other punctuation will never match.
        '''
        pass
    def measure_words_matched(self, ocr_text: str) -> float:
        
        pass",snippet_2,3,"```python
import re
from typing import Iterable, Set


class OcrQualityDictionary:
    '''Manages a dictionary for simple OCR quality checks.'''
    def __init__(self, *, wordlist: Iterable[str]):
        '''Construct a dictionary from a list of words.
        Words for which capitalization is important should be capitalized in the
        dictionary. Words that contain spaces or other punctuation will never match.
        '''
        # Keep only words that consist solely of alphabetic characters
        self._words: Set[str] = {
            w for w in wordlist
            if isinstance(w, str) and re.fullmatch(r""[A-Za-z]+"", w)
        }

    def measure_words_matched(self, ocr_text: str) -> float:
        '''Return the fraction of words in `ocr_text` that are present in the dictionary.'''
        # Find all word tokens (letters, digits, underscore)
        tokens = re.findall(r""\b\w+\b"", ocr_text)
        if not tokens:
            return 0.0
        matched = sum(1 for t in tokens if t in self._words)
        return matched / len(tokens)
```"
19627,slundberg/shap,slundberg_shap/shap/_serializable.py,shap._serializable.Serializable,"import inspect
import pickle

class Serializable:
    """"""This is the superclass of all serializable objects.""""""

    def save(self, out_file):
        """"""Save the model to the given file stream.""""""
        pickle.dump(type(self), out_file)

    @classmethod
    def load(cls, in_file, instantiate=True):
        """"""This is meant to be overridden by subclasses and called with super.

        We return constructor argument values when not being instantiated. Since there are no
        constructor arguments for the Serializable class we just return an empty dictionary.
        """"""
        if instantiate:
            return cls._instantiated_load(in_file)
        return {}

    @classmethod
    def _instantiated_load(cls, in_file, **kwargs):
        """"""This is meant to be overridden by subclasses and called with super.

        We return constructor argument values (we have no values to load in this abstract class).
        """"""
        obj_type = pickle.load(in_file)
        if obj_type is None:
            return None
        if not inspect.isclass(obj_type) or (not issubclass(obj_type, cls) and obj_type is not cls):
            raise Exception(f'Invalid object type loaded from file. {obj_type} is not a subclass of {cls}.')
        constructor_args = obj_type.load(in_file, instantiate=False, **kwargs)
        used_args = inspect.getfullargspec(obj_type.__init__)[0]
        return obj_type(**{k: constructor_args[k] for k in constructor_args if k in used_args})","
class Serializable:
    '''This is the superclass of all serializable objects.'''
    def save(self, out_file):
        
        pass
    @classmethod
    def load(cls, in_file, instantiate=True):
        
        pass
    @classmethod
    def _instantiated_load(cls, in_file, **kwargs):
        
        pass",snippet_3,4,"```python
import json
from typing import Any, Dict, Type, TypeVar

T = TypeVar('T', bound='Serializable')


class Serializable:
    '''This is the superclass of all serializable objects.'''

    def save(self, out_file: str) -> None:
        """"""
        Serialize the object's __dict__ to a JSON file.

        Parameters
        ----------
        out_file : str
            Path to the output file where the JSON representation will be written.
        """"""
        with open(out_file, 'w', encoding='utf-8') as f:
            json.dump(self.__dict__, f, ensure_ascii=False, indent=4)

    @classmethod
    def load(cls: Type[T], in_file: str, instantiate: bool = True) -> Any:
        """"""
        Load a JSON file and optionally instantiate an object of the class.

        Parameters
        ----------
        in_file : str
            Path to the input JSON file.
        instantiate : bool, default True
            If True, return an instance of the class initialized with the data.
            If False, return the raw dictionary loaded from the file.

        Returns
        -------
        Union[T, Dict[str, Any]]
            Either an instance of the class or the raw data dictionary.
        """"""
        with open(in_file, 'r', encoding='utf-8') as f:
            data: Dict[str, Any] = json.load(f)

        if instantiate:
            return cls(**data)  # type: ignore[arg-type]
        return data

    @classmethod
    def _instantiated_load(cls: Type[T], in_file: str, **kwargs: Any) -> T:
        """"""
        Load a JSON file, merge it with additional keyword arguments, and instantiate.

        Parameters
        ----------
        in_file : str
            Path to the input JSON file.
        **kwargs : Any
            Additional keyword arguments that override or extend the loaded data.

        Returns
        -------
        T
            An instance of the class initialized with the merged data.
        """"""
        with open(in_file, 'r', encoding='utf-8') as f:
            data: Dict[str, Any] = json.load(f)

        data.update(kwargs)
        return cls(**data)  # type: ignore[arg-type]
```"
19720,audreyr/cookiecutter,audreyr_cookiecutter/cookiecutter/environment.py,cookiecutter.environment.ExtensionLoaderMixin,"from cookiecutter.exceptions import UnknownExtension
from typing import Any

class ExtensionLoaderMixin:
    """"""Mixin providing sane loading of extensions specified in a given context.

    The context is being extracted from the keyword arguments before calling
    the next parent class in line of the child.
    """"""

    def __init__(self, *, context: dict[str, Any] | None=None, **kwargs: Any) -> None:
        """"""Initialize the Jinja2 Environment object while loading extensions.

        Does the following:

        1. Establishes default_extensions (currently just a Time feature)
        2. Reads extensions set in the cookiecutter.json _extensions key.
        3. Attempts to load the extensions. Provides useful error if fails.
        """"""
        context = context or {}
        default_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']
        extensions = default_extensions + self._read_extensions(context)
        try:
            super().__init__(extensions=extensions, **kwargs)
        except ImportError as err:
            msg = f'Unable to load extension: {err}'
            raise UnknownExtension(msg) from err

    def _read_extensions(self, context: dict[str, Any]) -> list[str]:
        """"""Return list of extensions as str to be passed on to the Jinja2 env.

        If context does not contain the relevant info, return an empty
        list instead.
        """"""
        try:
            extensions = context['cookiecutter']['_extensions']
        except KeyError:
            return []
        else:
            return [str(ext) for ext in extensions]","
class ExtensionLoaderMixin:
    '''Mixin providing sane loading of extensions specified in a given context.
    The context is being extracted from the keyword arguments before calling
    the next parent class in line of the child.
        '''
    def __init__(self, *, context: dict[str, Any] | None=None, **kwargs: Any) -> None:
        '''Initialize the Jinja2 Environment object while loading extensions.
        Does the following:
        1. Establishes default_extensions (currently just a Time feature)
        2. Reads extensions set in the cookiecutter.json _extensions key.
        3. Attempts to load the extensions. Provides useful error if fails.
        '''
        pass
    def _read_extensions(self, context: dict[str, Any]) -> list[str]:
        '''Return list of extensions as str to be passed on to the Jinja2 env.
        If context does not contain the relevant info, return an empty
        list instead.
        '''
        pass",snippet_5,3,"```python
from __future__ import annotations

from typing import Any, Dict, List, Optional

import jinja2


class ExtensionLoaderMixin:
    """"""Mixin providing sane loading of extensions specified in a given context.

    The context is being extracted from the keyword arguments before calling
    the next parent class in line of the child.
    """"""

    def __init__(
        self,
        *,
        context: Optional[Dict[str, Any]] = None,
        **kwargs: Any,
    ) -> None:
        """"""
        Initialize the Jinja2 Environment object while loading extensions.

        Does the following:
        1. Establishes default_extensions (currently just a Time feature)
        2. Reads extensions set in the cookiecutter.json _extensions key.
        3. Attempts to load the extensions. Provides useful error if fails.
        """"""
        # Ensure context is a dict
        ctx: Dict[str, Any] = context or {}

        # Default extensions
        default_extensions: List[str] = [""jinja2_time.TimeExtension""]

        # Read extensions from context
        user_extensions = self._read_extensions(ctx)

        # Combine extensions
        extensions = list(dict.fromkeys(default_extensions + user_extensions))

        # Attempt to initialize the parent (e.g., jinja2.Environment)
        try:
            super().__init__(extensions=extensions, **kwargs)  # type: ignore
        except Exception as exc:
            # Provide a helpful error message
            raise RuntimeError(
                f""Failed to load Jinja2 extensions: {', '.join(extensions)}""
            ) from exc

    def _read_extensions(self, context: Dict[str, Any]) -> List[str]:
        """"""
        Return list of extensions as str to be passed on to the Jinja2 env.
        If context does not contain the relevant info, return an empty list instead.
        """"""
        extensions = context.get(""_extensions"")
        if isinstance(extensions, list):
            # Ensure all items are strings
            return [str(ext) for ext in extensions if isinstance(ext, (str, bytes))]
        return []
```"
21340,Delgan/loguru,loguru/_recattrs.py,loguru._recattrs.RecordFile,"class RecordFile:
    """"""A class representing a file record with name and path.

    Attributes
    ----------
    name : str
        The name of the file
    path : str
        The path to the file
    """"""
    __slots__ = ('name', 'path')

    def __init__(self, name, path):
        """"""Initialize a RecordFile instance.

        Parameters
        ----------
        name : str
            The name of the file
        path : str
            The path to the file
        """"""
        self.name = name
        self.path = path

    def __repr__(self):
        """"""Return string representation of RecordFile.

        Returns
        -------
        str
            Formatted string with name and path
        """"""
        return '(name=%r, path=%r)' % (self.name, self.path)

    def __format__(self, spec):
        """"""Format the RecordFile instance.

        Parameters
        ----------
        spec : str
            Format specification

        Returns
        -------
        str
            Formatted name according to specification
        """"""
        return self.name.__format__(spec)","class RecordFile:
    
    def __init__(self, name, path):
        '''Initialize a RecordFile instance.
        Parameters
        ----------
        name : str
            The name of the file
        path : str
            The path to the file
        '''
        pass
    def __repr__(self):
        
        pass
    def __format__(self, spec):
        
        pass",snippet_6,4,"```python
class RecordFile:
    def __init__(self, name, path):
        '''Initialize a RecordFile instance.
        Parameters
        ----------
        name : str
            The name of the file
        path : str
            The path to the file
        '''
        self.name = name
        self.path = path

    def __repr__(self):
        return f""RecordFile(name={self.name!r}, path={self.path!r})""

    def __format__(self, spec):
        """"""Format the RecordFile according to the given spec.

        Supported specs:
            '' or 'repr' : full representation (same as __repr__)
            'n' or 'name' : the file name
            'p' or 'path' : the file path
            'np' or 'full' : name and path separated by a colon
        """"""
        if not spec or spec == ""repr"":
            return repr(self)
        if spec in (""n"", ""name""):
            return self.name
        if spec in (""p"", ""path""):
            return self.path
        if spec in (""np"", ""full""):
            return f""{self.name}:{self.path}""
        raise ValueError(f""Unsupported format specifier: {spec!r}"")
```"
21341,Delgan/loguru,loguru/_recattrs.py,loguru._recattrs.RecordLevel,"class RecordLevel:
    """"""A class representing the logging level record with name, number and icon.

    Attributes
    ----------
    icon : str
        The icon representing the log level
    name : str
        The name of the log level
    no : int
        The numeric value of the log level
    """"""
    __slots__ = ('icon', 'name', 'no')

    def __init__(self, name, no, icon):
        """"""Initialize a RecordLevel instance.

        Parameters
        ----------
        name : str
            The name of the log level
        no : int
            The numeric value of the log level
        icon : str
            The icon representing the log level
        """"""
        self.name = name
        self.no = no
        self.icon = icon

    def __repr__(self):
        """"""Return string representation of RecordLevel.

        Returns
        -------
        str
            Formatted string with name, number and icon
        """"""
        return '(name=%r, no=%r, icon=%r)' % (self.name, self.no, self.icon)

    def __format__(self, spec):
        """"""Format the RecordLevel instance.

        Parameters
        ----------
        spec : str
            Format specification

        Returns
        -------
        str
            Formatted name according to specification
        """"""
        return self.name.__format__(spec)","class RecordLevel:
    '''A class representing the logging level record with name, number and icon.
    Attributes
    ----------
    icon : str
        The icon representing the log level
    name : str
        The name of the log level
    no : int
        The numeric value of the log level
    '''
    def __init__(self, name, no, icon):
        
        pass
    def __repr__(self):
        '''Return string representation of RecordLevel.
        Returns
        -------
        str
            Formatted string with name, number and icon
        '''
        pass
    def __format__(self, spec):
        '''Format the RecordLevel instance.
        Parameters
        ----------
        spec : str
            Format specification
        Returns
        -------
        str
            Formatted name according to specification
        '''
        pass",snippet_7,4,"class RecordLevel:
    '''A class representing the logging level record with name, number and icon.
    Attributes
    ----------
    icon : str
        The icon representing the log level
    name : str
        The name of the log level
    no : int
        The numeric value of the log level
    '''
    def __init__(self, name, no, icon):
        self.name = name
        self.no = no
        self.icon = icon

    def __repr__(self):
        '''Return string representation of RecordLevel.
        Returns
        -------
        str
            Formatted string with name, number and icon
        '''
        return f""{self.name} ({self.no}) {self.icon}""

    def __format__(self, spec):
        '''Format the RecordLevel instance.
        Parameters
        ----------
        spec : str
            Format specification
        Returns
        -------
        str
            Formatted name according to specification
        '''
        if spec == ""n"":
            return str(self.no)
        if spec == ""i"":
            return self.icon
        # Default: format the name with the given spec
        return format(self.name, spec)"
21342,Delgan/loguru,loguru/_recattrs.py,loguru._recattrs.RecordProcess,"class RecordProcess:
    """"""A class representing a process record with ID and name.

    Attributes
    ----------
    id : int
        The process ID
    name : str
        The process name
    """"""
    __slots__ = ('id', 'name')

    def __init__(self, id_, name):
        """"""Initialize a RecordProcess instance.

        Parameters
        ----------
        id_ : int
            The process ID
        name : str
            The process name
        """"""
        self.id = id_
        self.name = name

    def __repr__(self):
        """"""Return string representation of RecordProcess.

        Returns
        -------
        str
            Formatted string with id and name
        """"""
        return '(id=%r, name=%r)' % (self.id, self.name)

    def __format__(self, spec):
        """"""Format the RecordProcess instance.

        Parameters
        ----------
        spec : str
            Format specification

        Returns
        -------
        str
            Formatted ID according to specification
        """"""
        return self.id.__format__(spec)","class RecordProcess:
    '''A class representing a process record with ID and name.
    Attributes
    ----------
    id : int
        The process ID
    name : str
        The process name
    '''
    def __init__(self, id_, name):
        '''Initialize a RecordProcess instance.
        Parameters
        ----------
        id_ : int
            The process ID
        name : str
            The process name
        '''
        pass
    def __repr__(self):
        
        pass
    def __format__(self, spec):
        '''Format the RecordProcess instance.
        Parameters
        ----------
        spec : str
            Format specification
        Returns
        -------
        str
            Formatted ID according to specification
        '''
        pass",snippet_8,4,"class RecordProcess:
    '''A class representing a process record with ID and name.
    Attributes
    ----------
    id : int
        The process ID
    name : str
        The process name
    '''
    def __init__(self, id_, name):
        '''Initialize a RecordProcess instance.
        Parameters
        ----------
        id_ : int
            The process ID
        name : str
            The process name
        '''
        self.id = int(id_)
        self.name = str(name)

    def __repr__(self):
        return f""RecordProcess(id={self.id!r}, name={self.name!r})""

    def __format__(self, spec):
        '''Format the RecordProcess instance.
        Parameters
        ----------
        spec : str
            Format specification
        Returns
        -------
        str
            Formatted ID according to specification
        '''
        return format(self.id, spec)"
21343,Delgan/loguru,loguru/_recattrs.py,loguru._recattrs.RecordThread,"class RecordThread:
    """"""A class representing a thread record with ID and name.

    Attributes
    ----------
    id : int
        The thread ID
    name : str
        The thread name
    """"""
    __slots__ = ('id', 'name')

    def __init__(self, id_, name):
        """"""Initialize a RecordThread instance.

        Parameters
        ----------
        id_ : int
            The thread ID
        name : str
            The thread name
        """"""
        self.id = id_
        self.name = name

    def __repr__(self):
        """"""Return string representation of RecordThread.

        Returns
        -------
        str
            Formatted string with id and name
        """"""
        return '(id=%r, name=%r)' % (self.id, self.name)

    def __format__(self, spec):
        """"""Format the RecordThread instance.

        Parameters
        ----------
        spec : str
            Format specification

        Returns
        -------
        str
            Formatted ID according to specification
        """"""
        return self.id.__format__(spec)","class RecordThread:
    '''A class representing a thread record with ID and name.
    Attributes
    ----------
    id : int
        The thread ID
    name : str
        The thread name
    '''
    def __init__(self, id_, name):
        '''Initialize a RecordThread instance.
        Parameters
        ----------
        id_ : int
            The thread ID
        name : str
            The thread name
        '''
        pass
    def __repr__(self):
        
        pass
    def __format__(self, spec):
        '''Format the RecordThread instance.
        Parameters
        ----------
        spec : str
            Format specification
        Returns
        -------
        str
            Formatted ID according to specification
        '''
        pass",snippet_9,4,"class RecordThread:
    '''A class representing a thread record with ID and name.
    Attributes
    ----------
    id : int
        The thread ID
    name : str
        The thread name
    '''
    def __init__(self, id_, name):
        '''Initialize a RecordThread instance.
        Parameters
        ----------
        id_ : int
            The thread ID
        name : str
            The thread name
        '''
        self.id = int(id_)
        self.name = str(name)

    def __repr__(self):
        return f""{self.__class__.__name__}(id={self.id!r}, name={self.name!r})""

    def __format__(self, spec):
        '''Format the RecordThread instance.
        Parameters
        ----------
        spec : str
            Format specification
        Returns
        -------
        str
            Formatted ID according to specification
        '''
        return format(self.id, spec)"
33971,Miserlou/Zappa,Miserlou_Zappa/zappa/asynchronous.py,zappa.asynchronous.LambdaAsyncResponse,"import json
import uuid

class LambdaAsyncResponse:
    """"""
    Base Response Dispatcher class
    Can be used directly or subclassed if the method to send the message is changed.
    """"""

    def __init__(self, lambda_function_name=None, aws_region=None, capture_response=False, **kwargs):
        """""" """"""
        if kwargs.get('boto_session'):
            self.client = kwargs.get('boto_session').client('lambda')
        else:
            self.client = LAMBDA_CLIENT
        self.lambda_function_name = lambda_function_name
        self.aws_region = aws_region
        if capture_response:
            if ASYNC_RESPONSE_TABLE is None:
                print(""Warning! Attempted to capture a response without async_response_table configured in settings (you won't capture async responses)."")
                capture_response = False
                self.response_id = 'MISCONFIGURED'
            else:
                self.response_id = str(uuid.uuid4())
        else:
            self.response_id = None
        self.capture_response = capture_response

    def send(self, task_path, args, kwargs):
        """"""
        Create the message object and pass it to the actual sender.
        """"""
        message = {'task_path': task_path, 'capture_response': self.capture_response, 'response_id': self.response_id, 'args': args, 'kwargs': kwargs}
        self._send(message)
        return self

    def _send(self, message):
        """"""
        Given a message, directly invoke the lamdba function for this task.
        """"""
        message['command'] = 'zappa.asynchronous.route_lambda_task'
        payload = json.dumps(message).encode('utf-8')
        if len(payload) > LAMBDA_ASYNC_PAYLOAD_LIMIT:
            raise AsyncException('Payload too large for async Lambda call')
        self.response = self.client.invoke(FunctionName=self.lambda_function_name, InvocationType='Event', Payload=payload)
        self.sent = self.response.get('StatusCode', 0) == 202","
class LambdaAsyncResponse:
    
    def __init__(self, lambda_function_name=None, aws_region=None, capture_response=False, **kwargs):
        
        pass
    def send(self, task_path, args, kwargs):
        '''
        Create the message object and pass it to the actual sender.
        '''
        pass
    def _send(self, message):
        '''
        Given a message, directly invoke the lamdba function for this task.
        '''
        pass",snippet_10,4,"```python
import json
import logging
from typing import Any, Dict, Optional

import boto3
from botocore.exceptions import BotoCoreError, ClientError

log = logging.getLogger(__name__)


class LambdaAsyncResponse:
    """"""
    Helper to send asynchronous tasks to an AWS Lambda function.

    Parameters
    ----------
    lambda_function_name : str, optional
        Name or ARN of the Lambda function to invoke.
    aws_region : str, optional
        AWS region where the Lambda function resides.
    capture_response : bool, default False
        If True, the response payload from the Lambda invocation is returned.
    **kwargs
        Additional keyword arguments are stored for future use.
    """"""

    def __init__(
        self,
        lambda_function_name: Optional[str] = None,
        aws_region: Optional[str] = None,
        capture_response: bool = False,
        **kwargs: Any,
    ):
        self.lambda_function_name = lambda_function_name
        self.aws_region = aws_region
        self.capture_response = capture_response
        self._extra_kwargs = kwargs
        self._lambda_client = None

        if not self.lambda_function_name:
            raise ValueError(""lambda_function_name must be provided"")
        if not self.aws_region:
            raise ValueError(""aws_region must be provided"")

    def _get_client(self):
        """"""Return a cached boto3 Lambda client.""""""
        if self._lambda_client is None:
            self._lambda_client = boto3.client(""lambda"", region_name=self.aws_region)
        return self._lambda_client

    def send(self, task_path: str, args: Any, kwargs: Dict[str, Any]):
        """"""
        Create the message object and pass it to the actual sender.

        Parameters
        ----------
        task_path : str
            The dotted path to the task function.
        args : Any
            Positional arguments for the task.
        kwargs : dict
            Keyword arguments for the task.

        Returns
        -------
        Any
            The response payload if capture_response is True, otherwise None.
        """"""
        message = {
            ""task_path"": task_path,
            ""args"": args,
            ""kwargs"": kwargs,
        }
        return self._send(message)

    def _send(self, message: Dict[str, Any]):
        """"""
        Given a message, directly invoke the Lambda function for this task.

        Parameters
        ----------
        message : dict
            The payload to send to Lambda.

        Returns
        -------
        Any
            The response payload if capture_response is True, otherwise None.
        """"""
        client = self._get_client()
        invocation_type = ""Event"" if not self.capture_response else ""RequestResponse""

        try:
            response = client.invoke(
                FunctionName=self.lambda_function_name,
                InvocationType=invocation_type,
                Payload=json.dumps(message).encode(""utf-8""),
                **self._extra_kwargs,
            )
        except (BotoCoreError, ClientError) as exc:
            log.exception(""Failed to invoke Lambda function %s"", self.lambda_function_name)
            raise RuntimeError(f""Lambda invocation failed: {exc}"") from exc

        if self.capture_response:
            payload_bytes = response.get(""Payload"", b"""")
            try:
                payload_str = payload_bytes.read().decode(""utf-8"")
                return json.loads(payload_str)
            except Exception as exc:
                log.exception(""Failed to decode Lambda response payload"")
                raise RuntimeError(f""Failed to decode Lambda response: {exc}"") from exc

        return None
```"
44118,LonamiWebs/Telethon,LonamiWebs_Telethon/telethon/crypto/aesctr.py,telethon.crypto.aesctr.AESModeCTR,"import pyaes

class AESModeCTR:
    """"""Wrapper around pyaes.AESModeOfOperationCTR mode with custom IV""""""

    def __init__(self, key, iv):
        """"""
        Initializes the AES CTR mode with the given key/iv pair.

        :param key: the key to be used as bytes.
        :param iv: the bytes initialization vector. Must have a length of 16.
        """"""
        assert isinstance(key, bytes)
        self._aes = pyaes.AESModeOfOperationCTR(key)
        assert isinstance(iv, bytes)
        assert len(iv) == 16
        self._aes._counter._counter = list(iv)

    def encrypt(self, data):
        """"""
        Encrypts the given plain text through AES CTR.

        :param data: the plain text to be encrypted.
        :return: the encrypted cipher text.
        """"""
        return self._aes.encrypt(data)

    def decrypt(self, data):
        """"""
        Decrypts the given cipher text through AES CTR

        :param data: the cipher text to be decrypted.
        :return: the decrypted plain text.
        """"""
        return self._aes.decrypt(data)","
class AESModeCTR:
    '''Wrapper around pyaes.AESModeOfOperationCTR mode with custom IV'''
    def __init__(self, key, iv):
        '''
        Initializes the AES CTR mode with the given key/iv pair.
        :param key: the key to be used as bytes.
        :param iv: the bytes initialization vector. Must have a length of 16.
        '''
        pass
    def encrypt(self, data):
        
        pass
    def decrypt(self, data):
        '''
        Decrypts the given cipher text through AES CTR
        :param data: the cipher text to be decrypted.
        :return: the decrypted plain text.
        '''
        pass",snippet_11,4,"```python
import pyaes

class AESModeCTR:
    '''Wrapper around pyaes.AESModeOfOperationCTR mode with custom IV'''
    def __init__(self, key, iv):
        '''
        Initializes the AES CTR mode with the given key/iv pair.
        :param key: the key to be used as bytes.
        :param iv: the bytes initialization vector. Must have a length of 16.
        '''
        if not isinstance(iv, (bytes, bytearray)):
            raise TypeError(""IV must be bytes or bytearray"")
        if len(iv) != 16:
            raise ValueError(""IV must be 16 bytes long"")
        if not isinstance(key, (bytes, bytearray)):
            raise TypeError(""Key must be bytes or bytearray"")
        self._cipher = pyaes.AESModeOfOperationCTR(key, initial_value=iv)

    def encrypt(self, data):
        '''
        Encrypts the given plain text through AES CTR.
        :param data: the plain text to be encrypted.
        :return: the encrypted cipher text.
        '''
        if not isinstance(data, (bytes, bytearray)):
            raise TypeError(""Data must be bytes or bytearray"")
        return self._cipher.encrypt(data)

    def decrypt(self, data):
        '''
        Decrypts the given cipher text through AES CTR.
        :param data: the cipher text to be decrypted.
        :return: the decrypted plain text.
        '''
        if not isinstance(data, (bytes, bytearray)):
            raise TypeError(""Data must be bytes or bytearray"")
        return self._cipher.decrypt(data)
```"
44121,LonamiWebs/Telethon,LonamiWebs_Telethon/telethon/crypto/factorization.py,telethon.crypto.factorization.Factorization,"from random import randint

class Factorization:
    """"""
    Simple module to factorize large numbers really quickly.
    """"""

    @classmethod
    def factorize(cls, pq):
        """"""
        Factorizes the given large integer.

        Implementation from https://comeoncodeon.wordpress.com/2010/09/18/pollard-rho-brent-integer-factorization/.

        :param pq: the prime pair pq.
        :return: a tuple containing the two factors p and q.
        """"""
        if pq % 2 == 0:
            return (2, pq // 2)
        y, c, m = (randint(1, pq - 1), randint(1, pq - 1), randint(1, pq - 1))
        g = r = q = 1
        x = ys = 0
        while g == 1:
            x = y
            for i in range(r):
                y = (pow(y, 2, pq) + c) % pq
            k = 0
            while k < r and g == 1:
                ys = y
                for i in range(min(m, r - k)):
                    y = (pow(y, 2, pq) + c) % pq
                    q = q * abs(x - y) % pq
                g = cls.gcd(q, pq)
                k += m
            r *= 2
        if g == pq:
            while True:
                ys = (pow(ys, 2, pq) + c) % pq
                g = cls.gcd(abs(x - ys), pq)
                if g > 1:
                    break
        p, q = (g, pq // g)
        return (p, q) if p < q else (q, p)

    @staticmethod
    def gcd(a, b):
        """"""
        Calculates the Greatest Common Divisor.

        :param a: the first number.
        :param b: the second number.
        :return: GCD(a, b)
        """"""
        while b:
            a, b = (b, a % b)
        return a","
class Factorization:
    '''
    Simple module to factorize large numbers really quickly.
    '''
    @classmethod
    def factorize(cls, pq):
        
        pass
    @staticmethod
    def gcd(a, b):
        
        pass",snippet_12,3,"```python
import random
import math
from typing import List

class Factorization:
    '''
    Simple module to factorize large numbers really quickly.
    '''
    @classmethod
    def factorize(cls, n: int) -> List[int]:
        """"""
        Return the prime factorization of n as a sorted list.
        """"""
        if n < 2:
            return []

        # Handle small primes quickly
        factors = []

        # Remove factors of 2
        while n % 2 == 0:
            factors.append(2)
            n //= 2

        if n == 1:
            return factors

        # If remaining n is prime, add it
        if cls._is_prime(n):
            factors.append(n)
            return sorted(factors)

        # Use Pollard's Rho to find a non-trivial factor
        factor = cls._pollard_rho(n)
        # Recursively factor the two parts
        factors += cls.factorize(factor)
        factors += cls.factorize(n // factor)

        return sorted(factors)

    @staticmethod
    def gcd(a: int, b: int) -> int:
        """"""
        Compute the greatest common divisor of a and b.
        """"""
        while b:
            a, b = b, a % b
        return a

    # ------------------------------------------------------------------
    # Internal helper methods
    # ------------------------------------------------------------------
    @staticmethod
    def _is_prime(n: int) -> bool:
        """"""
        Deterministic Miller–Rabin primality test for 64‑bit integers.
        """"""
        if n < 2:
            return False
        # Small primes
        small_primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]
        for p in small_primes:
            if n % p == 0:
                return n == p

        # Write n-1 as d * 2^s
        d = n - 1
        s = 0
        while d % 2 == 0:
            d //= 2
            s += 1

        # Deterministic bases for 64-bit integers
        bases = [2, 325, 9375, 28178, 450775, 9780504, 1795265022]
        for a in bases:
            if a % n == 0:
                continue
            x = pow(a, d, n)
            if x == 1 or x == n - 1:
                continue
            for _ in range(s - 1):
                x = pow(x, 2, n)
                if x == n - 1:
                    break
            else:
                return False
        return True

    @staticmethod
    def _pollard_rho(n: int) -> int:
        """"""
        Pollard's Rho algorithm to find a non‑trivial factor of n.
        """"""
        if n % 2 == 0:
            return 2
        # Random seed for reproducibility
        while True:
            c = random.randrange(1, n)
            f = lambda x: (pow(x, 2, n) + c) % n
            x, y, d = 2, 2, 1
            while d == 1:
                x = f(x)
                y = f(f(y))
                d = Factorization.gcd(abs(x - y), n)
            if d != n:
                return d
```"
49235,blue-yonder/tsfresh,blue-yonder_tsfresh/tsfresh/examples/driftbif_simulation.py,tsfresh.examples.driftbif_simulation.velocity,"import numpy as np

class velocity:
    """"""
    Simulates the velocity of a dissipative soliton (kind of self organized particle) [6]_.
    The equilibrium velocity without noise R=0 for
    $	au>1.0/\\kappa_3$ is $\\kappa_3 \\sqrt{(tau - 1.0/\\kappa_3)/Q}.
    Before the drift-bifurcation $	au \\le 1.0/\\kappa_3$ the velocity is zero.

    References
    ----------

    .. [6] Andreas Kempa-Liehr (2013, p. 159-170)
        Dynamics of Dissipative Soliton
        Dissipative Solitons in Reaction Diffusion Systems.
        Springer: Berlin


    >>> ds = velocity(tau=3.5) # Dissipative soliton with equilibrium velocity 1.5e-3
    >>> print(ds.label) # Discriminating before or beyond Drift-Bifurcation
    1

    # Equilibrium velocity
    >>> print(ds.deterministic)
    0.0015191090506254991

    # Simulated velocity as a time series with 20000 time steps being disturbed by Gaussian white noise
    >>> v = ds.simulate(20000)
    """"""

    def __init__(self, tau=3.8, kappa_3=0.3, Q=1950.0, R=0.0003, delta_t=0.05, seed=None):
        """"""
        :param tau: Bifurcation parameter determining the intrinsic velocity of the dissipative soliton,
                    which is zero for tau<=1.0/kappa_3 and np.sqrt(kappa_3**3/Q * (tau - 1.0/kappa_3)) otherwise
        :type tau: float
        :param kappa_3: Inverse bifurcation point.
        :type kappa_3:
        :param Q: Shape parameter of dissipative soliton
        :type Q: float
        :param R: Noise amplitude
        :type R: float
        :param delta_t: temporal discretization
        :type delta_t: float
        """"""
        self.delta_t = delta_t
        self.kappa_3 = kappa_3
        self.Q = Q
        self.tau = tau
        self.a = self.delta_t * kappa_3 ** 2 * (tau - 1.0 / kappa_3)
        self.b = self.delta_t * Q / kappa_3
        self.label = int(tau > 1.0 / kappa_3)
        self.c = np.sqrt(self.delta_t) * R
        self.delta_t = self.delta_t
        if seed is not None:
            np.random.seed(seed)
        if tau <= 1.0 / kappa_3:
            self.deterministic = 0.0
        else:
            self.deterministic = kappa_3 ** 1.5 * np.sqrt((tau - 1.0 / kappa_3) / Q)

    def __call__(self, v):
        """"""
        returns deterministic dynamic = acceleration (without noise)

        :param v: initial velocity vector
        :rtype v: ndarray
        :return: velocity vector of next time step
        :return type: ndarray
        """"""
        return v * (1.0 + self.a - self.b * np.dot(v, v))

    def simulate(self, N, v0=np.zeros(2)):
        """"""

        :param N: number of time steps
        :type N: int
        :param v0: initial velocity vector
        :type v0: ndarray
        :return: time series of velocity vectors with shape (N, v0.shape[0])
        :rtype: ndarray
        """"""
        v = [v0]
        n = N - 1
        gamma = np.random.randn(n, v0.size)
        for i in range(n):
            next_v = self.__call__(v[i]) + self.c * gamma[i]
            v.append(next_v)
        v_vec = np.array(v)
        return v_vec","
class velocity:
    '''
    Simulates the velocity of a dissipative soliton (kind of self organized particle) [6]_.
    The equilibrium velocity without noise R=0 for
    $    au>1.0/\kappa_3$ is $\kappa_3 \sqrt{(tau - 1.0/\kappa_3)/Q}.
    Before the drift-bifurcation $    au \le 1.0/\kappa_3$ the velocity is zero.
    References
    ----------
    .. [6] Andreas Kempa-Liehr (2013, p. 159-170)
        Dynamics of Dissipative Soliton
        Dissipative Solitons in Reaction Diffusion Systems.
        Springer: Berlin
    >>> ds = velocity(tau=3.5) # Dissipative soliton with equilibrium velocity 1.5e-3
    >>> print(ds.label) # Discriminating before or beyond Drift-Bifurcation
    1
    # Equilibrium velocity
    >>> print(ds.deterministic)
    0.0015191090506254991
    # Simulated velocity as a time series with 20000 time steps being disturbed by Gaussian white noise
    >>> v = ds.simulate(20000)
    '''
    def __init__(self, tau=3.8, kappa_3=0.3, Q=1950.0, R=0.0003, delta_t=0.05, seed=None):
        
        pass
    def __call__(self, v):
        '''
        returns deterministic dynamic = acceleration (without noise)
        :param v: initial velocity vector
        :rtype v: ndarray
        :return: velocity vector of next time step
        :return type: ndarray
        '''
        pass
    def simulate(self, N, v0=np.zeros(2)):
        
        pass",snippet_13,4,
98891,peterbrittain/asciimatics,peterbrittain_asciimatics/asciimatics/effects.py,asciimatics.effects._Flake,"from random import randint, random, choice
from asciimatics.screen import Screen

class _Flake:
    """"""
    Track a single snow flake.
    """"""
    _snow_chars = '.+*'
    _drift_chars = ' ,;#@'

    def __init__(self, screen: Screen):
        """"""
        :param screen: The Screen being used for the Scene.
        """"""
        self._screen = screen
        self._x = 0
        self._y = 0
        self._rate = 0
        self._char = ''
        self._reseed()

    def _reseed(self):
        """"""
        Randomly create a new snowflake once this one is finished.
        """"""
        self._char = choice(self._snow_chars)
        self._rate = randint(1, 3)
        self._x = randint(0, self._screen.width - 1)
        self._y = self._screen.start_line + randint(0, self._rate)

    def update(self, reseed: bool):
        """"""
        Update that snowflake!

        :param reseed: Whether we are in the normal reseed cycle or not.
        """"""
        self._screen.print_at(' ', self._x, self._y)
        cell = None
        for _ in range(self._rate):
            self._y += 1
            cell = self._screen.get_from(self._x, self._y)
            if cell is None or cell[0] != 32:
                break
        if (cell is not None and cell[0] in [ord(x) for x in self._snow_chars + ' ']) and self._y < self._screen.start_line + self._screen.height:
            self._screen.print_at(self._char, self._x, self._y)
        else:
            self._y = min(self._y, self._screen.start_line + self._screen.height)
            drift_index = -1
            if cell:
                drift_index = self._drift_chars.find(chr(cell[0]))
            if 0 <= drift_index < len(self._drift_chars) - 1:
                drift_char = self._drift_chars[drift_index + 1]
                self._screen.print_at(drift_char, self._x, self._y)
            else:
                self._screen.print_at(',', self._x, self._y - 1)
            if reseed:
                self._reseed()","
class _Flake:
    '''
    Track a single snow flake.
    '''
    def __init__(self, screen: Screen):
        
        pass
    def _reseed(self):
        
        pass
    def update(self, reseed: bool):
        '''
        Update that snowflake!
        :param reseed: Whether we are in the normal reseed cycle or not.
        '''
        pass",snippet_14,4,"```python
import random

class _Flake:
    '''
    Track a single snow flake.
    '''
    def __init__(self, screen):
        """"""
        Initialise a snowflake with a reference to the screen.
        The screen object is expected to expose `width` and `height` attributes.
        """"""
        self.screen = screen
        self._reseed()

    def _reseed(self):
        """"""
        Randomise the flake's position, speed and size.
        The flake starts somewhere above the visible area.
        """"""
        # Position: x anywhere across the width, y somewhere above the top
        self.x = random.uniform(0, self.screen.width)
        self.y = random.uniform(-self.screen.height, 0)

        # Speed: how many pixels per update tick
        self.speed = random.uniform(1.0, 3.0)

        # Size: radius in pixels (used for drawing)
        self.size = random.randint(1, 3)

    def update(self, reseed: bool):
        """"""
        Update that snowflake!
        :param reseed: Whether we are in the normal reseed cycle or not.
        """"""
        # Move the flake downwards
        self.y += self.speed

        # If it has fallen below the screen, either reseed or wrap
        if self.y > self.screen.height:
            if reseed:
                self._reseed()
            else:
                # Wrap to the top
                self.y = 0
                self.x = random.uniform(0, self.screen.width)
```"
98892,peterbrittain/asciimatics,peterbrittain_asciimatics/asciimatics/effects.py,asciimatics.effects._Star,"from random import randint, random, choice
from asciimatics.screen import Screen

class _Star:
    """"""
    Simple class to represent a single star for the Stars special effect.
    """"""

    def __init__(self, screen: Screen, pattern: str):
        """"""
        :param screen: The Screen being used for the Scene.
        :param pattern: The pattern to loop through
        """"""
        self._screen = screen
        self._star_chars = pattern
        self._cycle = 0
        self._old_char = ''
        self._respawn()

    def _respawn(self):
        """"""
        Pick a random location for the star making sure it does
        not overwrite an existing piece of text.
        """"""
        self._cycle = randint(0, len(self._star_chars))
        height, width = self._screen.dimensions
        while True:
            self._x = randint(0, width - 1)
            self._y = self._screen.start_line + randint(0, height - 1)
            c = self._screen.get_from(self._x, self._y)
            if c is not None and c[0] == 32:
                break
        self._old_char = ' '

    def update(self):
        """"""
        Draw the star.
        """"""
        if not self._screen.is_visible(self._x, self._y):
            self._respawn()
        c = self._screen.get_from(self._x, self._y)
        if c is not None and c[0] not in (ord(self._old_char), 32):
            self._respawn()
        self._cycle += 1
        if self._cycle >= len(self._star_chars):
            self._cycle = 0
        new_char = self._star_chars[self._cycle]
        if new_char == self._old_char:
            return
        self._screen.print_at(new_char, self._x, self._y)
        self._old_char = new_char","
class _Star:
    
    def __init__(self, screen: Screen, pattern: str):
        '''
        :param screen: The Screen being used for the Scene.
        :param pattern: The pattern to loop through
        '''
        pass
    def _respawn(self):
        '''
        Pick a random location for the star making sure it does
        not overwrite an existing piece of text.
        '''
        pass
    def update(self):
        
        pass",snippet_15,4,"```python
import random

class _Star:
    """"""
    A simple star that moves around a Screen, cycling through a given pattern.
    """"""

    def __init__(self, screen, pattern: str):
        """"""
        :param screen: The Screen being used for the Scene.
        :param pattern: The pattern to loop through
        """"""
        self.screen = screen
        self.pattern = pattern
        self.index = 0
        self.x = None
        self.y = None
        self.prev_char = None
        self._respawn()

    def _respawn(self):
        """"""
        Pick a random location for the star making sure it does
        not overwrite an existing piece of text.
        """"""
        # If the screen has a method to check occupancy, use it.
        # Otherwise, assume any position is free.
        max_attempts = 1000
        for _ in range(max_attempts):
            x = random.randint(0, self.screen.width - 1)
            y = random.randint(0, self.screen.height - 1)
            if hasattr(self.screen, ""is_occupied""):
                if not self.screen.is_occupied(x, y):
                    break
            else:
                # Assume free if no method provided
                break
        else:
            # Fallback: just use the last tried position
            x = random.randint(0, self.screen.width - 1)
            y = random.randint(0, self.screen.height - 1)

        # Clear previous position if any
        if self.x is not None and self.y is not None:
            if hasattr(self.screen, ""clear_char""):
                self.screen.clear_char(self.x, self.y)
            else:
                self.screen.set_char(self.x, self.y, "" "")

        self.x, self.y = x, y
        self.prev_char = None

    def update(self):
        """"""
        Advance the star's pattern and render it on the screen.
        """"""
        if not self.pattern:
            return

        # Clear previous character
        if self.x is not None and self.y is not None:
            if hasattr(self.screen, ""clear_char""):
                self.screen.clear_char(self.x, self.y)
            else:
                self.screen.set_char(self.x, self.y, "" "")

        # Draw current character
        char = self.pattern[self.index]
        if hasattr(self.screen, ""set_char""):
            self.screen.set_char(self.x, self.y, char)
        else:
            # Fallback: assume screen has a buffer we can index
            self.screen.buffer[self.y][self.x] = char

        # Advance pattern index
        self.index = (self.index + 1) % len(self.pattern)
```"
98893,peterbrittain/asciimatics,peterbrittain_asciimatics/asciimatics/effects.py,asciimatics.effects._Trail,"from random import randint, random, choice
from asciimatics.screen import Screen

class _Trail:
    """"""
    Track a single trail  for a falling character effect (a la Matrix).
    """"""

    def __init__(self, screen: Screen, x: int):
        """"""
        :param screen: The Screen being used for the Scene.
        :param x: The column (y coordinate) for this trail to use.
        """"""
        self._screen = screen
        self._x = x
        self._y = 0
        self._life = 0
        self._rate = 0
        self._clear = True
        self._maybe_reseed(True)

    def _maybe_reseed(self, normal: bool):
        """"""
        Randomly create a new column once this one is finished.
        """"""
        self._y += self._rate
        self._life -= 1
        if self._life <= 0:
            self._clear = not self._clear if normal else True
            self._rate = randint(1, 2)
            if self._clear:
                self._y = 0
                self._life = self._screen.height // self._rate
            else:
                self._y = randint(0, self._screen.height // 2) - self._screen.height // 4
                self._life = randint(1, self._screen.height - self._y) // self._rate

    def update(self, reseed: bool):
        """"""
        Update that trail!

        :param reseed: Whether we are in the normal reseed cycle or not.
        """"""
        if self._clear:
            for i in range(0, 3):
                self._screen.print_at(' ', self._x, self._screen.start_line + self._y + i)
            self._maybe_reseed(reseed)
        else:
            for i in range(0, 3):
                self._screen.print_at(chr(randint(32, 126)), self._x, self._screen.start_line + self._y + i, Screen.COLOUR_GREEN)
            for i in range(4, 6):
                self._screen.print_at(chr(randint(32, 126)), self._x, self._screen.start_line + self._y + i, Screen.COLOUR_GREEN, Screen.A_BOLD)
            self._maybe_reseed(reseed)","
class _Trail:
    '''
    Track a single trail  for a falling character effect (a la Matrix).
    '''
    def __init__(self, screen: Screen, x: int):
        
        pass
    def _maybe_reseed(self, normal: bool):
        
        pass
    def update(self, reseed: bool):
        '''
        Update that trail!
        :param reseed: Whether we are in the normal reseed cycle or not.
        '''
        pass",snippet_16,4,"```python
import random
from typing import List, Tuple

class _Trail:
    """"""
    Track a single trail for a falling character effect (a la Matrix).
    """"""
    # Characters used in the trail
    _CHARS = ""abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789@#$%&*""

    def __init__(self, screen, x: int):
        """"""
        :param screen: The screen object that provides a put_char(x, y, char, color) method.
        :param x: The horizontal position of the trail.
        """"""
        self.screen = screen
        self.x = x
        self.height = getattr(screen, ""height"", 24)  # fallback height
        self._reset()

    def _reset(self):
        """"""Reset the trail to a new random length and clear its state.""""""
        self.length = random.randint(5, max(5, self.height // 2))
        self.head_y = -1  # start above the screen
        self.chars: List[str] = []  # characters in the trail (head first)
        # No need to keep colors separately; we compute them on the fly

    def _maybe_reseed(self, normal: bool):
        """"""
        Possibly reseed the trail. If normal is True, reseed with a small probability.
        If normal is False, reseed immediately.
        """"""
        if normal:
            # 2% chance to reseed each update
            if random.random() < 0.02:
                self._reset()
        else:
            self._reset()

    def update(self, reseed: bool):
        """"""
        Update that trail!
        :param reseed: Whether we are in the normal reseed cycle or not.
        """"""
        # Move the head down
        self.head_y += 1

        # If the head has moved past the bottom, reseed
        if self.head_y >= self.height:
            self._maybe_reseed(normal=reseed)
            return

        # Generate a new character for the head
        new_char = random.choice(self._CHARS)
        self.chars.insert(0, new_char)

        # Trim the trail to its maximum length
        if len(self.chars) > self.length:
            self.chars.pop()

        # Draw the trail
        for idx, char in enumerate(self.chars):
            y = self.head_y - idx
            if 0 <= y < self.height:
                # Brightness: head is bright, tail is dimmer
                if idx == 0:
                    color = 0x00FF00  # bright green
                else:
                    color = 0x004400  # dim green
                # Assume the screen has a put_char method
                self.screen.put_char(self.x, y, char, color)
```"
99108,ConsenSys/mythril-classic,ConsenSys_mythril-classic/mythril/analysis/ops.py,mythril.analysis.ops.Variable,"class Variable:
    """"""The representation of a variable with value and type.""""""

    def __init__(self, val, _type):
        """"""

        :param val:
        :param _type:
        """"""
        self.val = val
        self.type = _type

    def __str__(self):
        """"""

        :return:
        """"""
        return str(self.val)","class Variable:
    '''The representation of a variable with value and type.'''
    def __init__(self, val, _type):
        '''
        :param val:
        :param _type:
        '''
        pass
    def __str__(self):
        
        pass",snippet_17,3,"```python
class Variable:
    '''The representation of a variable with value and type.'''
    def __init__(self, val, _type):
        """"""
        :param val: The value of the variable.
        :param _type: The type of the variable. Can be a type object or a string.
        """"""
        self.val = val
        # Normalise the type representation
        if isinstance(_type, type):
            self._type = _type.__name__
        else:
            self._type = str(_type)

    def __str__(self):
        return f""{self.val} ({self._type})""
```"
100863,bigchaindb/bigchaindb,bigchaindb_bigchaindb/bigchaindb/validation.py,bigchaindb.validation.BaseValidationRules,"class BaseValidationRules:
    """"""Base validation rules for BigchainDB.

    A validation plugin must expose a class inheriting from this one via an entry_point.

    All methods listed below must be implemented.
    """"""

    @staticmethod
    def validate_transaction(bigchaindb, transaction):
        """"""See :meth:`bigchaindb.models.Transaction.validate`
        for documentation.
        """"""
        return transaction.validate(bigchaindb)

    @staticmethod
    def validate_block(bigchaindb, block):
        """"""See :meth:`bigchaindb.models.Block.validate` for documentation.""""""
        return block.validate(bigchaindb)","class BaseValidationRules:
    
    @staticmethod
    def validate_transaction(bigchaindb, transaction):
        '''See :meth:`bigchaindb.models.Transaction.validate`
        for documentation.
        '''
        pass
    @staticmethod
    def validate_block(bigchaindb, block):
        
        pass",snippet_22,3,"```python
class BaseValidationRules:
    """"""
    Basic validation rules for transactions and blocks.
    These rules simply invoke the built‑in `validate` methods of the
    transaction and block objects.  Any exception raised during
    validation is caught and treated as a validation failure.
    """"""

    @staticmethod
    def validate_transaction(bigchaindb, transaction):
        """"""
        Validate a transaction against the current state of the
        BigchainDB instance.

        Parameters
        ----------
        bigchaindb : object
            The BigchainDB instance (unused in this basic rule but
            kept for API compatibility).
        transaction : object
            The transaction object to validate.  It must expose a
            ``validate`` method.

        Returns
        -------
        bool
            ``True`` if the transaction validates successfully,
            ``False`` otherwise.
        """"""
        try:
            # The transaction's own validate method performs all checks.
            transaction.validate()
            return True
        except Exception:
            return False

    @staticmethod
    def validate_block(bigchaindb, block):
        """"""
        Validate a block against the current state of the
        BigchainDB instance.

        Parameters
        ----------
        bigchaindb : object
            The BigchainDB instance (unused in this basic rule but
            kept for API compatibility).
        block : object
            The block object to validate.  It must expose a
            ``validate`` method.

        Returns
        -------
        bool
            ``True`` if the block validates successfully,
            ``False`` otherwise.
        """"""
        try:
            # The block's own validate method performs all checks.
            block.validate()
            return True
        except Exception:
            return False
```"
100865,bigchaindb/bigchaindb,bigchaindb_bigchaindb/bigchaindb/web/strip_content_type_middleware.py,bigchaindb.web.strip_content_type_middleware.StripContentTypeMiddleware,"class StripContentTypeMiddleware:
    """"""WSGI middleware to strip Content-Type header for GETs.""""""

    def __init__(self, app):
        """"""Create the new middleware.

        Args:
            app: a flask application
        """"""
        self.app = app

    def __call__(self, environ, start_response):
        """"""Run the middleware and then call the original WSGI application.""""""
        if environ['REQUEST_METHOD'] == 'GET':
            try:
                del environ['CONTENT_TYPE']
            except KeyError:
                pass
            else:
                logger.debug('Remove header ""Content-Type"" from GET request')
        return self.app(environ, start_response)","class StripContentTypeMiddleware:
    
    def __init__(self, app):
        '''Create the new middleware.
        Args:
            app: a flask application
        '''
        pass
    def __call__(self, environ, start_response):
        
        pass",snippet_23,3,"class StripContentTypeMiddleware:
    
    def __init__(self, app):
        '''Create the new middleware.
        Args:
            app: a flask application
        '''
        self.app = app

    def __call__(self, environ, start_response):
        # Remove the Content-Type header from the request environment
        environ.pop('CONTENT_TYPE', None)
        return self.app(environ, start_response)"
100895,ansible/molecule,src/molecule/app.py,molecule.app.App,"from pathlib import Path
from subprocess import CalledProcessError, CompletedProcess
from molecule.console import original_stderr
from ansible_compat.runtime import Runtime
from molecule.ansi_output import CommandBorders
from molecule.util import print_environment_vars

class App:
    """"""App class that keep runtime status.""""""

    def __init__(self, path: Path) -> None:
        """"""Create a new app instance.

        Args:
            path: The path to the project.
        """"""
        self.runtime = Runtime(project_dir=path, isolated=False)

    def run_command(self, cmd: str | list[str], env: dict[str, str] | None=None, cwd: Path | None=None, *, debug: bool=False, echo: bool=False, quiet: bool=False, check: bool=False, command_borders: bool=False) -> CompletedProcess[str]:
        """"""Execute the given command and returns None.

        Args:
            cmd: A list of strings containing the command to run.
            env: A dict containing the shell's environment.
            cwd: An optional Path to the working directory.
            debug: An optional bool to toggle debug output.
            echo: An optional bool to toggle command echo.
            quiet: An optional bool to toggle command output.
            check: An optional bool to toggle command error checking.
            command_borders: An optional bool to enable borders around command output.

        Returns:
            A completed process object.

        Raises:
            CalledProcessError: If return code is nonzero and check is True.
        """"""
        if debug:
            print_environment_vars(env)
        borders = None
        if command_borders:
            borders = CommandBorders(cmd=cmd, original_stderr=original_stderr)
        result = self.runtime.run(args=cmd, env=env, cwd=cwd, tee=True, set_acp=False)
        if borders:
            borders.finalize(result.returncode)
        if result.returncode != 0 and check:
            raise CalledProcessError(returncode=result.returncode, cmd=result.args, output=result.stdout, stderr=result.stderr)
        return result","
class App:
    
    def __init__(self, path: Path) -> None:
        
        pass
    def run_command(self, cmd: str | list[str], env: dict[str, str] | None=None, cwd: Path | None=None, *, debug: bool=False, echo: bool=False, quiet: bool=False, check: bool=False, command_borders: bool=False) -> CompletedProcess[str]:
        '''Execute the given command and returns None.
        Args:
            cmd: A list of strings containing the command to run.
            env: A dict containing the shell's environment.
            cwd: An optional Path to the working directory.
            debug: An optional bool to toggle debug output.
            echo: An optional bool to toggle command echo.
            quiet: An optional bool to toggle command output.
            check: An optional bool to toggle command error checking.
            command_borders: An optional bool to enable borders around command output.
        Returns:
            A completed process object.
        Raises:
            CalledProcessError: If return code is nonzero and check is True.
        '''
        pass",snippet_24,3,"```python
import os
import shlex
import subprocess
from pathlib import Path
from subprocess import CompletedProcess, CalledProcessError
from typing import Dict, List, Optional, Union


class App:
    def __init__(self, path: Path) -> None:
        self.path = path

    def run_command(
        self,
        cmd: Union[str, List[str]],
        env: Optional[Dict[str, str]] = None,
        cwd: Optional[Path] = None,
        *,
        debug: bool = False,
        echo: bool = False,
        quiet: bool = False,
        check: bool = False,
        command_borders: bool = False,
    ) -> CompletedProcess[str]:
        """"""
        Execute the given command and returns a CompletedProcess.
        """"""
        # Resolve command to list
        if isinstance(cmd, str):
            cmd_list = shlex.split(cmd)
        else:
            cmd_list = cmd

        # Merge environment
        env_combined = os.environ.copy()
        if env:
            env_combined.update(env)

        # Resolve working directory
        cwd_str = str(cwd) if cwd else None

        # Debug output
        if debug:
            print(f""[DEBUG] Running command: {cmd_list}"")
            print(f""[DEBUG] Working directory: {cwd_str}"")
            print(f""[DEBUG] Environment: {env_combined}"")

        # Echo command
        if echo:
            print(f""Executing: {' '.join(cmd_list)}"")

        # Run the command
        result = subprocess.run(
            cmd_list,
            cwd=cwd_str,
            env=env_combined,
            text=True,
            capture_output=True,
            check=check,
        )

        # Output handling
        if not quiet:
            if command_borders:
                print(""-"" * 80)
            if result.stdout:
                print(result.stdout, end="""")
            if result.stderr:
                print(result.stderr, end="""", file=os.sys.stderr)
            if command_borders:
                print(""-"" * 80)

        return result
```"
104788,autokey/autokey,autokey_autokey/lib/autokey/scripting/system.py,autokey.scripting.system.System,"import subprocess

class System:
    """"""
    Simplified access to some system commands.
    """"""

    @staticmethod
    def exec_command(command, getOutput=True):
        """"""
        Execute a shell command

        Usage: C{system.exec_command(command, getOutput=True)}

        Set getOutput to False if the command does not exit and return immediately. Otherwise
        AutoKey will not respond to any hotkeys/abbreviations etc until the process started
        by the command exits.

        @param command: command to be executed (including any arguments) - e.g. ""ls -l""
        @param getOutput: whether to capture the (stdout) output of the command
        @raise subprocess.CalledProcessError: if the command returns a non-zero exit code
        """"""
        if getOutput:
            with subprocess.Popen(command, shell=True, bufsize=-1, stdout=subprocess.PIPE, universal_newlines=True) as p:
                output = p.communicate()[0]
                output = output.rstrip('\n')
                if p.returncode:
                    raise subprocess.CalledProcessError(p.returncode, output)
                return output
        else:
            subprocess.Popen(command, shell=True, bufsize=-1)

    @staticmethod
    def create_file(file_name, contents=''):
        """"""
        Create a file with contents

        Usage: C{system.create_file(fileName, contents="""")}

        @param fileName: full path to the file to be created
        @param contents: contents to insert into the file
        """"""
        with open(file_name, 'w') as written_file:
            written_file.write(contents)","
class System:
    
    @staticmethod
    def exec_command(command, getOutput=True):
        
        pass
    @staticmethod
    def create_file(file_name, contents=''):
        '''
        Create a file with contents
        Usage: C{system.create_file(fileName, contents="""")}
        @param fileName: full path to the file to be created
        @param contents: contents to insert into the file
        '''
        pass",snippet_26,3,"```python
import subprocess
import os

class System:
    @staticmethod
    def exec_command(command, getOutput=True):
        """"""
        Execute a shell command.

        :param command: The command string to execute.
        :param getOutput: If True, return the command's stdout as a string.
                          If False, return the command's exit code.
        :return: stdout string or exit code.
        """"""
        try:
            result = subprocess.run(
                command,
                shell=True,
                check=False,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True
            )
            if getOutput:
                return result.stdout
            else:
                return result.returncode
        except Exception as e:
            if getOutput:
                return str(e)
            else:
                return -1

    @staticmethod
    def create_file(file_name, contents=''):
        """"""
        Create a file with the specified contents.

        :param file_name: Full path to the file to be created.
        :param contents: String to write into the file.
        """"""
        # Ensure the directory exists
        dir_name = os.path.dirname(file_name)
        if dir_name and not os.path.exists(dir_name):
            os.makedirs(dir_name, exist_ok=True)

        with open(file_name, 'w', encoding='utf-8') as f:
            f.write(contents)
```"
106092,mikedh/trimesh,trimesh/collision.py,trimesh.collision.DistanceData,"class DistanceData:
    """"""
    Data structure for holding information about a distance query.
    """"""

    def __init__(self, names, result):
        """"""
        Initialize a DistanceData.

        Parameters
        ----------
        names : list of str
          The names of the two objects in order.
        contact : fcl.DistanceResult
          The distance query result.
        """"""
        self.names = set(names)
        self._inds = {names[0]: result.b1, names[1]: result.b2}
        self._points = {names[0]: result.nearest_points[0], names[1]: result.nearest_points[1]}
        self._distance = result.min_distance

    @property
    def distance(self):
        """"""
        Returns the distance between the two objects.

        Returns
        -------
        distance : float
          The euclidean distance between the objects.
        """"""
        return self._distance

    def index(self, name):
        """"""
        Returns the index of the closest face for the mesh with
        the given name.

        Parameters
        ----------
        name : str
          The name of the target object.

        Returns
        -------
        index : int
          The index of the face in collisoin.
        """"""
        return self._inds[name]

    def point(self, name):
        """"""
        The 3D point of closest distance on the mesh with the given name.

        Parameters
        ----------
        name : str
          The name of the target object.

        Returns
        -------
        point : (3,) float
          The closest point.
        """"""
        return self._points[name]","class DistanceData:
    
    def __init__(self, names, result):
        '''
        Initialize a DistanceData.
        Parameters
        ----------
        names : list of str
          The names of the two objects in order.
        contact : fcl.DistanceResult
          The distance query result.
        '''
        pass
    @property
    def distance(self):
        '''
        Returns the distance between the two objects.
        Returns
        -------
        distance : float
          The euclidean distance between the objects.
        '''
        pass
    def index(self, name):
        '''
        Returns the index of the closest face for the mesh with
        the given name.
        Parameters
        ----------
        name : str
          The name of the target object.
        Returns
        -------
        index : int
          The index of the face in collisoin.
        '''
        pass
    def point(self, name):
        '''
        The 3D point of closest distance on the mesh with the given name.
        Parameters
        ----------
        name : str
          The name of the target object.
        Returns
        -------
        point : (3,) float
          The closest point.
        '''
        pass",snippet_28,5,"```python
class DistanceData:
    def __init__(self, names, result):
        """"""
        Initialize a DistanceData.
        Parameters
        ----------
        names : list of str
            The names of the two objects in order.
        result : fcl.DistanceResult
            The distance query result.
        """"""
        if not isinstance(names, (list, tuple)) or len(names) != 2:
            raise ValueError(""names must be a list or tuple of two strings"")
        self._names = list(names)
        self._result = result

        # Prepare mappings for quick lookup
        self._index_map = {}
        self._point_map = {}

        # Distance result may expose nearest_points and nearest_points_indices
        # as tuples/lists of length 2. If not present, we leave maps empty.
        try:
            points = result.nearest_points
            indices = getattr(result, ""nearest_points_indices"", None)
            if points is not None and len(points) == 2:
                self._point_map[self._names[0]] = tuple(points[0])
                self._point_map[self._names[1]] = tuple(points[1])
            if indices is not None and len(indices) == 2:
                self._index_map[self._names[0]] = int(indices[0])
                self._index_map[self._names[1]] = int(indices[1])
        except Exception:
            # If any attribute access fails, leave maps empty
            pass

    @property
    def distance(self):
        """"""
        Returns the distance between the two objects.
        Returns
        -------
        distance : float
            The euclidean distance between the objects.
        """"""
        return getattr(self._result, ""distance"", None)

    def index(self, name):
        """"""
        Returns the index of the closest face for the mesh with
        the given name.
        Parameters
        ----------
        name : str
            The name of the target object.
        Returns
        -------
        index : int
            The index of the face in collision.
        """"""
        if name not in self._names:
            raise ValueError(f""Unknown name: {name}"")
        if name in self._index_map:
            return self._index_map[name]
        # If index not available, try to fetch from result directly
        try:
            indices = getattr(self._result, ""nearest_points_indices"", None)
            if indices is not None:
                idx = indices[self._names.index(name)]
                return int(idx)
        except Exception:
            pass
        # Fallback: return -1 to indicate unknown
        return -1

    def point(self, name):
        """"""
        The 3D point of closest distance on the mesh with the given name.
        Parameters
        ----------
        name : str
            The name of the target object.
        Returns
        -------
        point : (3,) float
            The closest point.
        """"""
        if name not in self._names:
            raise ValueError(f""Unknown name: {name}"")
        if name in self._point_map:
            return self._point_map[name]
        # If point not available, try to fetch from result directly
        try:
            points = getattr(self._result, ""nearest_points"", None)
            if points is not None:
                pt = points[self._names.index(name)]
                return tuple(pt)
        except Exception:
            pass
        # Fallback: return None
        return None
```"
106111,mikedh/trimesh,trimesh/path/packing.py,trimesh.path.packing.RectangleBin,"import numpy as np

class RectangleBin:
    """"""
    An N-dimensional binary space partition tree for packing
    hyper-rectangles. Split logic is pure `numpy` but behaves
    similarly to `scipy.spatial.Rectangle`.

    Mostly useful for packing 2D textures and 3D boxes and
    has not been tested outside of 2 and 3 dimensions.

    Original article about using this for packing textures:
    http://www.blackpawn.com/texts/lightmaps/
    """"""

    def __init__(self, bounds):
        """"""
        Create a rectangular bin.

        Parameters
        ------------
        bounds : (2, dimension *) float
          Bounds array are `[mins, maxes]`
        """"""
        self.child = []
        self.occupied = False
        self.bounds = np.array(bounds, dtype=np.float64)

    @property
    def extents(self):
        """"""
        Bounding box size.

        Returns
        ----------
        extents : (dimension,) float
          Edge lengths of bounding box
        """"""
        bounds = self.bounds
        return bounds[1] - bounds[0]

    def insert(self, size, rotate=True):
        """"""
        Insert a rectangle into the bin.

        Parameters
        -------------
        size : (dimension,) float
          Size of rectangle to insert/

        Returns
        ----------
        inserted : (2,) float or None
          Position of insertion in the tree or None
          if the insertion was unsuccessful.
        """"""
        for child in self.child:
            attempt = child.insert(size=size, rotate=rotate)
            if attempt is not None:
                return attempt
        if self.occupied:
            return None
        bounds = self.bounds.copy()
        extents = bounds[1] - bounds[0]
        if rotate:
            for roll in range(len(size)):
                size_test = extents - _roll(size, roll)
                fits = (size_test > -_TOL_ZERO).all()
                if fits:
                    size = _roll(size, roll)
                    break
            if not fits:
                return None
        else:
            size_test = extents - size
            if (size_test < -_TOL_ZERO).any():
                return None
        self.occupied = True
        if (size_test < _TOL_ZERO).all():
            return bounds
        axis = size_test.argmax()
        splits = np.vstack((bounds, bounds))
        splits[1:3, axis] = bounds[0][axis] + size[axis]
        self.child[:] = (RectangleBin(splits[:2]), RectangleBin(splits[2:]))
        return self.child[0].insert(size, rotate=rotate)","
class RectangleBin:
    '''
    An N-dimensional binary space partition tree for packing
    hyper-rectangles. Split logic is pure `numpy` but behaves
    similarly to `scipy.spatial.Rectangle`.
    Mostly useful for packing 2D textures and 3D boxes and
    has not been tested outside of 2 and 3 dimensions.
    Original article about using this for packing textures:
    http://www.blackpawn.com/texts/lightmaps/
    '''
    def __init__(self, bounds):
        '''
        Create a rectangular bin.
        Parameters
        ------------
        bounds : (2, dimension *) float
          Bounds array are `[mins, maxes]`
        '''
        pass
    @property
    def extents(self):
        
        pass
    def insert(self, size, rotate=True):
        
        pass",snippet_29,4,"```python
import numpy as np
from itertools import permutations

class RectangleBin:
    '''
    An N-dimensional binary space partition tree for packing
    hyper-rectangles. Split logic is pure `numpy` but behaves
    similarly to `scipy.spatial.Rectangle`.
    Mostly useful for packing 2D textures and 3D boxes and
    has not been tested outside of 2 and 3 dimensions.
    Original article about using this for packing textures:
    http://www.blackpawn.com/texts/lightmaps/
    '''
    class _Node:
        __slots__ = (""bounds"", ""used"", ""children"")

        def __init__(self, bounds):
            self.bounds = bounds          # (2, dim) array
            self.used = False
            self.children = None          # None or [left, right]

        def _fits(self, size):
            """"""Return True if size fits in this node.""""""
            mins, maxs = self.bounds
            return np.all(size <= (maxs - mins))

        def _split(self, size):
            """"""Split this node into two children along the axis with
            the largest leftover space.""""""
            mins, maxs = self.bounds
            dims = maxs - mins
            leftover = dims - size
            # choose axis with largest leftover > 0
            axis = np.argmax(leftover)
            # create child1: the rectangle region
            child1_bounds = np"
107609,weld-project/weld,python/grizzly/grizzly/lazy_op.py,grizzly.lazy_op.LazyOpResult,"class LazyOpResult:
    """"""Wrapper class around as yet un-evaluated Weld computation results

    Attributes:
        dim (int): Dimensionality of the output
        expr (WeldObject / Numpy.ndarray): The expression that needs to be
            evaluated
        weld_type (WeldType): Type of the output object
    """"""

    def __init__(self, expr, weld_type, dim):
        """"""Summary

        Args:
            expr (TYPE): Description
            weld_type (TYPE): Description
            dim (TYPE): Description
        """"""
        self.expr = expr
        self.weld_type = weld_type
        self.dim = dim

    def evaluate(self, verbose=True, decode=True, passes=None, num_threads=1, apply_experimental_transforms=False):
        """"""Summary

        Args:
            verbose (bool, optional): Description
            decode (bool, optional): Description

        Returns:
            TYPE: Description
        """"""
        if isinstance(self.expr, WeldObject):
            return self.expr.evaluate(to_weld_type(self.weld_type, self.dim), verbose, decode, passes=passes, num_threads=num_threads, apply_experimental_transforms=apply_experimental_transforms)
        return self.expr","class LazyOpResult:
    '''Wrapper class around as yet un-evaluated Weld computation results
    Attributes:
        dim (int): Dimensionality of the output
        expr (WeldObject / Numpy.ndarray): The expression that needs to be
            evaluated
        weld_type (WeldType): Type of the output object
    '''
    def __init__(self, expr, weld_type, dim):
        
        pass
    def evaluate(self, verbose=True, decode=True, passes=None, num_threads=1, apply_experimental_transforms=False):
        '''Summary
        Args:
            verbose (bool, optional): Description
            decode (bool, optional): Description
        Returns:
            TYPE: Description
        '''
        pass",snippet_30,3,"```python
import numpy as np
import weld


class LazyOpResult:
    '''Wrapper class around as yet un-evaluated Weld computation results
    Attributes:
        dim (int): Dimensionality of the output
        expr (WeldObject / Numpy.ndarray): The expression that needs to be
            evaluated
        weld_type (WeldType): Type of the output object
    '''
    def __init__(self, expr, weld_type, dim):
        self.expr = expr
        self.weld_type = weld_type
        self.dim = dim

    def evaluate(self, verbose=True, decode=True, passes=None,
                 num_threads=1, apply_experimental_transforms=False):
        """"""Evaluate the wrapped Weld expression.

        Parameters
        ----------
        verbose : bool, optional
            If True, prints a short description of the evaluation.
        decode : bool, optional
            If True, the result is decoded to a NumPy array.
        passes : list or None, optional
            Optional list of compiler passes to apply.
        num_threads : int, optional
            Number of threads to use during evaluation.
        apply_experimental_transforms : bool, optional
            Whether to enable experimental transforms.

        Returns
        -------
        WeldObject or np.ndarray
            The evaluated result, decoded to a NumPy array if ``decode`` is
            True.
        """"""
        if verbose:
            print(f""Evaluating LazyOpResult: dim={self.dim}, type={self.weld_type}"")

        # If the expression is already a NumPy array, just return it
        if isinstance(self.expr, np.ndarray):
            result = self.expr
        else:
            # Evaluate the Weld expression
            result = weld.evaluate(
                self.expr,
                passes=passes,
                num_threads=num_threads,
                apply_experimental_transforms=apply_experimental_transforms,
            )

        # Decode to NumPy if requested
        if decode:
            result = weld.decode(result)

        return result
```"
113819,abseil/abseil-py,abseil_abseil-py/absl/app.py,absl.app.ExceptionHandler,"class ExceptionHandler:
    """"""Base exception handler from which other may inherit.""""""

    def wants(self, exc):
        """"""Returns whether this handler wants to handle the exception or not.

        This base class returns True for all exceptions by default. Override in
        subclass if it wants to be more selective.

        Args:
          exc: Exception, the current exception.
        """"""
        del exc
        return True

    def handle(self, exc):
        """"""Do something with the current exception.

        Args:
          exc: Exception, the current exception

        This method must be overridden.
        """"""
        raise NotImplementedError()","class ExceptionHandler:
    '''Base exception handler from which other may inherit.'''
    def wants(self, exc):
        '''Returns whether this handler wants to handle the exception or not.
        This base class returns True for all exceptions by default. Override in
        subclass if it wants to be more selective.
        Args:
          exc: Exception, the current exception.
        '''
        pass
    def handle(self, exc):
        '''Do something with the current exception.
        Args:
          exc: Exception, the current exception
        This method must be overridden.
        '''
        pass",snippet_33,3,"class ExceptionHandler:
    '''Base exception handler from which other may inherit.'''
    def wants(self, exc):
        '''Returns whether this handler wants to handle the exception or not.
        This base class returns True for all exceptions by default. Override in
        subclass if it wants to be more selective.
        Args:
          exc: Exception, the current exception.
        '''
        return True

    def handle(self, exc):
        '''Do something with the current exception.
        Args:
          exc: Exception, the current exception
        This method must be overridden.
        '''
        raise NotImplementedError(""Subclasses must implement the handle method."")"
116413,QuantEcon/QuantEcon.py,quantecon/util/timing.py,quantecon.util.timing.__Timer__,"import time
import numpy as np

class __Timer__:
    """"""Computes elapsed time, between tic, tac, and toc.

    Methods
    -------
    tic :
        Resets timer.
    toc :
        Returns and prints time elapsed since last tic().
    tac :
        Returns and prints time elapsed since last
             tic(), tac() or toc() whichever occured last.
    loop_timer :
        Returns and prints the total and average time elapsed for n runs
        of a given function.

    """"""
    start = None
    last = None

    def tic(self):
        """"""
        Save time for future use with `tac()` or `toc()`.

        Returns
        -------
        None
            This function doesn't return a value.
        """"""
        t = time.time()
        self.start = t
        self.last = t

    def tac(self, verbose=True, digits=2):
        """"""
        Return and print elapsed time since last `tic()`, `tac()`, or
        `toc()`.

        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.

        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.

        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`, `tac()`, or `toc()`.

        """"""
        if self.start is None:
            raise Exception('tac() without tic()')
        t = time.time()
        elapsed = t - self.last
        self.last = t
        if verbose:
            m, s = divmod(elapsed, 60)
            h, m = divmod(m, 60)
            print('TAC: Elapsed: %d:%02d:%0d.%0*d' % (h, m, s, digits, s % 1 * 10 ** digits))
        return elapsed

    def toc(self, verbose=True, digits=2):
        """"""
        Return and print time elapsed since last `tic()`.

        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.

        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.

        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`.

        """"""
        if self.start is None:
            raise Exception('toc() without tic()')
        t = time.time()
        self.last = t
        elapsed = t - self.start
        if verbose:
            m, s = divmod(elapsed, 60)
            h, m = divmod(m, 60)
            print('TOC: Elapsed: %d:%02d:%0d.%0*d' % (h, m, s, digits, s % 1 * 10 ** digits))
        return elapsed

    def loop_timer(self, n, function, args=None, verbose=True, digits=2, best_of=3):
        """"""
        Return and print the total and average time elapsed for n runs
        of function.

        Parameters
        ----------
        n : scalar(int)
            Number of runs.

        function : function
            Function to be timed.

        args : list, optional(default=None)
            Arguments of the function.

        verbose : bool, optional(default=True)
            If True, then prints average time.

        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.

        best_of : scalar(int), optional(default=3)
            Average time over best_of runs.

        Returns
        -------
        average_time : scalar(float)
            Average time elapsed for n runs of function.

        average_of_best : scalar(float)
            Average of best_of times for n runs of function.

        """"""
        tic()
        all_times = np.empty(n)
        for run in range(n):
            if hasattr(args, '__iter__'):
                function(*args)
            elif args is None:
                function()
            else:
                function(args)
            all_times[run] = tac(verbose=False, digits=digits)
        elapsed = toc(verbose=False, digits=digits)
        m, s = divmod(elapsed, 60)
        h, m = divmod(m, 60)
        print('Total run time: %d:%02d:%0d.%0*d' % (h, m, s, digits, s % 1 * 10 ** digits))
        average_time = all_times.mean()
        average_of_best = np.sort(all_times)[:best_of].mean()
        if verbose:
            m, s = divmod(average_time, 60)
            h, m = divmod(m, 60)
            print('Average time for %d runs: %d:%02d:%0d.%0*d' % (n, h, m, s, digits, s % 1 * 10 ** digits))
            m, s = divmod(average_of_best, 60)
            h, m = divmod(m, 60)
            print('Average of %d best times: %d:%02d:%0d.%0*d' % (best_of, h, m, s, digits, s % 1 * 10 ** digits))
        return (average_time, average_of_best)","
class __Timer__:
    
    def tic(self):
        
        pass
    def tac(self, verbose=True, digits=2):
        '''
        Return and print elapsed time since last `tic()`, `tac()`, or
        `toc()`.
        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.
        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.
        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`, `tac()`, or `toc()`.
        '''
        pass
    def toc(self, verbose=True, digits=2):
        '''
        Return and print time elapsed since last `tic()`.
        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.
        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.
        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`.
        '''
        pass
    def loop_timer(self, n, function, args=None, verbose=True, digits=2, best_of=3):
        '''
        Return and print the total and average time elapsed for n runs
        of function.
        Parameters
        ----------
        n : scalar(int)
            Number of runs.
        function : function
            Function to be timed.
        args : list, optional(default=None)
            Arguments of the function.
        verbose : bool, optional(default=True)
            If True, then prints average time.
        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.
        best_of : scalar(int), optional(default=3)
            Average time over best_of runs.
        Returns
        -------
        average_time : scalar(float)
            Average time elapsed for n runs of function.
        average_of_best : scalar(float)
            Average of best_of times for n runs of function.
        '''
        pass",snippet_35,5,"```python
import time

class __Timer__:
    def __init__(self):
        self._last_time = None

    def tic(self):
        """"""Start or restart the timer.""""""
        self._last_time = time.perf_counter()

    def tac(self, verbose=True, digits=2):
        """"""
        Return and print elapsed time since last `tic()`, `tac()`, or `toc()`.
        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.
        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.
        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`, `tac()`, or `toc()`.
        """"""
        if self._last_time is None:
            raise RuntimeError(""Timer has not been started. Call tic() first."")
        now = time.perf_counter()
        elapsed = now - self._last_time
        self._last_time = now
        if verbose:
            print(f""Elapsed time: {elapsed:.{digits}f} s"")
        return elapsed

    def toc(self, verbose=True, digits=2):
        """"""
        Return and print time elapsed since last `tic()`.
        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.
        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.
        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`.
        """"""
        if self._last_time is None:
            raise RuntimeError(""Timer has not been started. Call tic() first."")
        now = time.perf_counter()
        elapsed = now - self._last_time
        if verbose:
            print(f""Elapsed time: {elapsed:.{digits}f} s"")
        return elapsed

    def loop_timer(self, n, function, args=None, verbose=True, digits=2, best_of=3):
        """"""
        Return and print the total and average time elapsed for n runs
        of function.
        Parameters
        ----------
        n : scalar(int)
            Number of runs.
        function : function
            Function to be timed.
        args : list, optional(default=None)
            Arguments of the function.
        verbose : bool, optional(default=True)
            If True, then prints average time.
        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.
        best_of : scalar(int), optional(default=3)
            Average time over best_of runs.
        Returns
        -------
        average_time : scalar(float)
            Average time elapsed for n runs of function.
        average_of_best : scalar(float)
            Average of best_of times for n runs of function.
        """"""
        if not isinstance(n, int) or n <= 0:
            raise ValueError(""n must be a positive integer"")
        if best_of <= 0 or best_of > n:
            raise ValueError(""best_of must be between 1 and n"")

        times = []
        for _ in range(n):
            start = time.perf_counter()
            if args is None:
                function()
            else:
                function(*args)
            end = time.perf_counter()
            times.append(end - start)

        total_time = sum(times)
        average_time = total_time / n
        best_times = sorted(times)[:best_of]
        average_of_best = sum(best_times) / best_of

        if verbose:
            print(f""Total time for {n} runs: {total_time:.{digits}f} s"")
            print(f""Average time per run: {average_time:.{digits}f} s"")
            print(f""Average of best {best_of} runs: {average_of_best:.{digits}f} s"")

        return average_time, average_of_best
```"
116421,DerwenAI/pytextrank,DerwenAI_pytextrank/pytextrank/base.py,pytextrank.base.Sentence,"import typing
from spacy.tokens import Doc, Span, Token
from dataclasses import dataclass

@dataclass
class Sentence:
    """"""
A data class representing the distance measure for one sentence.
    """"""
    start: int
    end: int
    sent_id: int
    phrases: typing.Set[int]
    distance: float

    def empty(self) -> bool:
        """"""
Test whether this sentence includes any ranked phrases.

    returns:
`True` if the `phrases` is not empty.
        """"""
        return len(self.phrases) == 0

    def text(self, doc: Doc) -> str:
        """"""
Accessor for the text slice of the `spaCy` [`Doc`](https://spacy.io/api/doc)
document represented by this sentence.

    doc:
source document

    returns:
the sentence text
        """"""
        return doc[self.start:self.end]","@dataclass
class Sentence:
    
    def empty(self) -> bool:
        '''
Test whether this sentence includes any ranked phrases.
    returns:
`True` if the `phrases` is not empty.
        '''
        pass
    def text(self, doc: Doc) -> str:
        
        pass",snippet_36,3,"```python
from dataclasses import dataclass, field
from typing import Any, List

@dataclass
class Sentence:
    start: int = 0
    end: int = 0
    phrases: List[Any] = field(default_factory=list)

    def empty(self) -> bool:
        """"""
        Test whether this sentence includes any ranked phrases.
        returns:
            True if the `phrases` is empty.
        """"""
        return not bool(self.phrases)

    def text(self, doc: Any) -> str:
        """"""
        Return the raw text of the sentence from the provided document.
        """"""
        # Assume `doc` has a `text` attribute that is a string.
        return doc.text[self.start : self.end]
```"
128871,arviz-devs/arviz,arviz-devs_arviz/arviz/utils.py,arviz.utils.interactive_backend,"import matplotlib.pyplot as plt

class interactive_backend:
    """"""Context manager to change backend temporarily in ipython sesson.

    It uses ipython magic to change temporarily from the ipython inline backend to
    an interactive backend of choice. It cannot be used outside ipython sessions nor
    to change backends different than inline -> interactive.

    Notes
    -----
    The first time ``interactive_backend`` context manager is called, any of the available
    interactive backends can be chosen. The following times, this same backend must be used
    unless the kernel is restarted.

    Parameters
    ----------
    backend : str, optional
        Interactive backend to use. It will be passed to ``%matplotlib`` magic, refer to
        its docs to see available options.

    Examples
    --------
    Inside an ipython session (i.e. a jupyter notebook) with the inline backend set:

    .. code::

        >>> import arviz as az
        >>> idata = az.load_arviz_data(""centered_eight"")
        >>> az.plot_posterior(idata) # inline
        >>> with az.interactive_backend():
        ...     az.plot_density(idata) # interactive
        >>> az.plot_trace(idata) # inline

    """"""

    def __init__(self, backend=''):
        """"""Initialize context manager.""""""
        try:
            from IPython import get_ipython
        except ImportError as err:
            raise ImportError(f'The exception below was risen while importing Ipython, this context manager can only be used inside ipython sessions:\n{err}') from err
        self.ipython = get_ipython()
        if self.ipython is None:
            raise EnvironmentError('This context manager can only be used inside ipython sessions')
        self.ipython.magic(f'matplotlib {backend}')

    def __enter__(self):
        """"""Enter context manager.""""""
        return self

    def __exit__(self, exc_type, exc_value, exc_tb):
        """"""Exit context manager.""""""
        plt.show(block=True)
        self.ipython.magic('matplotlib inline')","
class interactive_backend:
    '''Context manager to change backend temporarily in ipython sesson.
    It uses ipython magic to change temporarily from the ipython inline backend to
    an interactive backend of choice. It cannot be used outside ipython sessions nor
    to change backends different than inline -> interactive.
    Notes
    -----
    The first time ``interactive_backend`` context manager is called, any of the available
    interactive backends can be chosen. The following times, this same backend must be used
    unless the kernel is restarted.
    Parameters
    ----------
    backend : str, optional
        Interactive backend to use. It will be passed to ``%matplotlib`` magic, refer to
        its docs to see available options.
    Examples
    --------
    Inside an ipython session (i.e. a jupyter notebook) with the inline backend set:
    .. code::
        >>> import arviz as az
        >>> idata = az.load_arviz_data(""centered_eight"")
        >>> az.plot_posterior(idata) # inline
        >>> with az.interactive_backend():
        ...     az.plot_density(idata) # interactive
        >>> az.plot_trace(idata) # inline
    '''
    def __init__(self, backend=''):
        
        pass
    def __enter__(self):
        '''Enter context manager.'''
        pass
    def __exit__(self, exc_type, exc_value, exc_tb):
        '''Exit context manager.'''
        pass",snippet_40,4,
128872,arviz-devs/arviz,arviz-devs_arviz/arviz/utils.py,arviz.utils.lazy_property,"import functools

class lazy_property:
    """"""Used to load numba first time it is needed.""""""

    def __init__(self, fget):
        """"""Lazy load a property with `fget`.""""""
        self.fget = fget
        functools.update_wrapper(self, fget)

    def __get__(self, obj, cls):
        """"""Call the function, set the attribute.""""""
        if obj is None:
            return self
        value = self.fget(obj)
        setattr(obj, self.fget.__name__, value)
        return value","
class lazy_property:
    '''Used to load numba first time it is needed.'''
    def __init__(self, fget):
        '''Lazy load a property with `fget`.'''
        pass
    def __get__(self, obj, cls):
        
        pass",snippet_41,3,"```python
class lazy_property:
    '''Used to load numba first time it is needed.'''
    def __init__(self, fget):
        '''Lazy load a property with `fget`.'''
        self.fget = fget
        self.name = fget.__name__

    def __get__(self, obj, cls):
        if obj is None:
            return self
        # Compute the value only once and cache it on the instance
        value = self.fget(obj)
        setattr(obj, self.name, value)
        return value
```"
129413,dtmilano/AndroidViewClient,dtmilano_AndroidViewClient/src/com/dtmilano/android/adb/adbclient.py,com.dtmilano.android.adb.adbclient.WifiManager,"import re
import sys

class WifiManager:
    """"""
    Simulates Android WifiManager.

    @see: http://developer.android.com/reference/android/net/wifi/WifiManager.html
    """"""
    WIFI_STATE_DISABLING = 0
    WIFI_STATE_DISABLED = 1
    WIFI_STATE_ENABLING = 2
    WIFI_STATE_ENABLED = 3
    WIFI_STATE_UNKNOWN = 4
    WIFI_IS_ENABLED_RE = re.compile('Wi-Fi is enabled')
    WIFI_IS_DISABLED_RE = re.compile('Wi-Fi is disabled')

    def __init__(self, device):
        """"""
        Constructor.
        :param device:
        :type device:
        """"""
        self.device = device

    def getWifiState(self):
        """"""
        Gets the Wi-Fi enabled state.

        @return: One of WIFI_STATE_DISABLED, WIFI_STATE_DISABLING, WIFI_STATE_ENABLED, WIFI_STATE_ENABLING, WIFI_STATE_UNKNOWN
        """"""
        result = self.device.shell('dumpsys wifi')
        if result:
            state = result.splitlines()[0]
            if self.WIFI_IS_ENABLED_RE.match(state):
                return self.WIFI_STATE_ENABLED
            elif self.WIFI_IS_DISABLED_RE.match(state):
                return self.WIFI_STATE_DISABLED
        print('UNKNOWN WIFI STATE:', state, file=sys.stderr)
        return self.WIFI_STATE_UNKNOWN","
class WifiManager:
    '''
    Simulates Android WifiManager.
    @see: http://developer.android.com/reference/android/net/wifi/WifiManager.html
    '''
    def __init__(self, device):
        
        pass
    def getWifiState(self):
        
        pass",snippet_42,3,"```python
class WifiManager:
    """"""
    Simulates Android WifiManager.
    @see: http://developer.android.com/reference/android/net/wifi/WifiManager.html
    """"""

    # Wifi state constants (Android values)
    WIFI_STATE_UNKNOWN = -1
    WIFI_STATE_DISABLED = 0
    WIFI_STATE_DISABLING = 1
    WIFI_STATE_ENABLED = 3
    WIFI_STATE_ENABLING = 2

    def __init__(self, device):
        """"""
        Initialize the WifiManager with a device object.

        The device is expected to expose either:
          * a boolean attribute `wifi_enabled`
          * an integer attribute `wifi_state` matching one of the constants above
        """"""
        self.device = device

    def getWifiState(self):
        """"""
        Return the current Wi-Fi state of the device.

        If the device provides a `wifi_state` attribute, that value is returned.
        Otherwise, if the device provides a boolean `wifi_enabled` attribute,
        the corresponding state constant is returned.
        If neither attribute is present, WIFI_STATE_UNKNOWN is returned.
        """"""
        # Prefer explicit state if available
        if hasattr(self.device, ""wifi_state""):
            state = getattr(self.device, ""wifi_state"")
            if isinstance(state, int):
                return state

        # Fallback to boolean enabled flag
        if hasattr(self.device, ""wifi_enabled""):
            enabled = getattr(self.device, ""wifi_enabled"")
            if isinstance(enabled, bool):
                return self.WIFI_STATE_ENABLED if enabled else self.WIFI_STATE_DISABLED

        # Unknown state
        return self.WIFI_STATE_UNKNOWN
```"
129620,neovim/pynvim,neovim_pynvim/pynvim/msgpack_rpc/async_session.py,pynvim.msgpack_rpc.async_session.Response,"from pynvim.msgpack_rpc.msgpack_stream import MsgpackStream

class Response:
    """"""Response to a msgpack-rpc request that came from Nvim.

    When Nvim sends a msgpack-rpc request, an instance of this class is
    created for remembering state required to send a response.
    """"""

    def __init__(self, msgpack_stream: MsgpackStream, request_id: int):
        """"""Initialize the Response instance.""""""
        self._msgpack_stream = msgpack_stream
        self._request_id = request_id

    def send(self, value, error=False):
        """"""Send the response.

        If `error` is True, it will be sent as an error.
        """"""
        if error:
            resp = [1, self._request_id, value, None]
        else:
            resp = [1, self._request_id, None, value]
        debug('sending response to request %d: %s', self._request_id, resp)
        self._msgpack_stream.send(resp)","
class Response:
    
    def __init__(self, msgpack_stream: MsgpackStream, request_id: int):
        '''Initialize the Response instance.'''
        pass
    def send(self, value, error=False):
        '''Send the response.
        If `error` is True, it will be sent as an error.
        '''
        pass",snippet_43,3,"```python
import msgpack

class Response:
    """"""
    A helper class to send a response over a MsgpackStream.
    """"""

    def __init__(self, msgpack_stream: ""MsgpackStream"", request_id: int):
        """"""
        Initialize the Response instance.

        Parameters
        ----------
        msgpack_stream : MsgpackStream
            The stream used to send the packed message.
        request_id : int
            The identifier of the request to which this response belongs.
        """"""
        self._stream = msgpack_stream
        self._request_id = request_id

    def send(self, value, error: bool = False):
        """"""
        Send the response.

        Parameters
        ----------
        value : Any
            The value to send back. If `error` is True, this will be sent as an
            error payload.
        error : bool, optional
            If True, the payload will be wrapped in an `error` field; otherwise
            it will be wrapped in a `result` field. Default is False.
        """"""
        payload = {
            ""id"": self._request_id,
            ""result"": value if not error else None,
            ""error"": value if error else None,
        }
        packed = msgpack.packb(payload, use_bin_type=True)
        self._stream.write(packed)
        # If the stream has a flush method, call it to ensure delivery.
        flush = getattr(self._stream, ""flush"", None)
        if callable(flush):
            flush()
```"
129967,sendgrid/sendgrid-python,sendgrid_sendgrid-python/sendgrid/helpers/eventwebhook/__init__.py,sendgrid.helpers.eventwebhook.EventWebhook,"from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.asymmetric import ec
from cryptography.exceptions import InvalidSignature
from cryptography.hazmat.primitives.serialization import load_pem_public_key
import base64

class EventWebhook:
    """"""
    This class allows you to use the Event Webhook feature. Read the docs for
    more details: https://sendgrid.com/docs/for-developers/tracking-events/event
    """"""

    def __init__(self, public_key=None):
        """"""
        Construct the Event Webhook verifier object
        :param public_key: verification key under Mail Settings
        :type public_key: string
        """"""
        self.public_key = self.convert_public_key_to_ecdsa(public_key) if public_key else public_key

    def convert_public_key_to_ecdsa(self, public_key):
        """"""
        Convert the public key string to an EllipticCurvePublicKey object.

        :param public_key: verification key under Mail Settings
        :type public_key string
        :return: An EllipticCurvePublicKey object using the ECDSA algorithm
        :rtype cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey
        """"""
        pem_key = '-----BEGIN PUBLIC KEY-----\n' + public_key + '\n-----END PUBLIC KEY-----'
        return load_pem_public_key(pem_key.encode('utf-8'))

    def verify_signature(self, payload, signature, timestamp, public_key=None):
        """"""
        Verify signed event webhook requests.

        :param payload: event payload in the request body
        :type payload: string
        :param signature: value obtained from the 'X-Twilio-Email-Event-Webhook-Signature' header
        :type signature: string
        :param timestamp: value obtained from the 'X-Twilio-Email-Event-Webhook-Timestamp' header
        :type timestamp: string
        :param public_key: elliptic curve public key
        :type public_key: cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey
        :return: true or false if signature is valid
        """"""
        timestamped_payload = (timestamp + payload).encode('utf-8')
        decoded_signature = base64.b64decode(signature)
        key = public_key or self.public_key
        try:
            key.verify(decoded_signature, timestamped_payload, ec.ECDSA(hashes.SHA256()))
            return True
        except InvalidSignature:
            return False","
class EventWebhook:
    
    def __init__(self, public_key=None):
        
        pass
    def convert_public_key_to_ecdsa(self, public_key):
        
        pass
    def verify_signature(self, payload, signature, timestamp, public_key=None):
        '''
        Verify signed event webhook requests.
        :param payload: event payload in the request body
        :type payload: string
        :param signature: value obtained from the 'X-Twilio-Email-Event-Webhook-Signature' header
        :type signature: string
        :param timestamp: value obtained from the 'X-Twilio-Email-Event-Webhook-Timestamp' header
        :type timestamp: string
        :param public_key: elliptic curve public key
        :type public_key: cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey
        :return: true or false if signature is valid
        '''
        pass",snippet_44,4,"```python
import base64
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.asymmetric import ec
from cryptography.hazmat.primitives import serialization
from cryptography.exceptions import InvalidSignature


class EventWebhook:
    """"""
    Utility class for verifying Twilio Email Event Webhook signatures.
    """"""

    def __init__(self, public_key=None):
        """"""
        Initialize the EventWebhook with an optional public key.
        :param public_key: PEM encoded public key string or bytes, or an already
                           loaded EllipticCurvePublicKey instance.
        """"""
        self.public_key = None
        if public_key is not None:
            self.public_key = self.convert_public_key_to_ecdsa(public_key)

    def convert_public_key_to_ecdsa(self, public_key):
        """"""
        Convert a PEM encoded public key to an EllipticCurvePublicKey instance.
        :param public_key: PEM string, bytes, or EllipticCurvePublicKey.
        :return: EllipticCurvePublicKey instance.
        """"""
        if isinstance(public_key, ec.EllipticCurvePublicKey):
            return public_key

        if isinstance(public_key, str):
            public_key_bytes = public_key.encode(""utf-8"")
        elif isinstance(public_key, (bytes, bytearray)):
            public_key_bytes = public_key
        else:
            raise TypeError(""public_key must be a PEM string, bytes, or EllipticCurvePublicKey"")

        return serialization.load_pem_public_key(public_key_bytes)

    def verify_signature(self, payload, signature, timestamp, public_key=None):
        """"""
        Verify signed event webhook requests.
        :param payload: event payload in the request body
        :type payload: string
        :param signature: value obtained from the 'X-Twilio-Email-Event-Webhook-Signature' header
        :type signature: string
        :param timestamp: value obtained from the 'X-Twilio-Email-Event-Webhook-Timestamp' header
        :type timestamp: string
        :param public_key: elliptic curve public key
        :type public_key: cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey
        :return: True if signature is valid, False otherwise
        """"""
        # Use provided public key or the one stored in the instance
        key = public_key or self.public_key
        if key is None:
            raise ValueError(""No public key provided for signature verification"")

        # Decode the base64 signature
        try:
            signature_bytes = base64.b64decode(signature)
        except Exception as e:
            raise ValueError(f""Invalid base64 signature: {e}"")

        # Construct the message: timestamp + payload
        if not isinstance(timestamp, (bytes, bytearray)):
            timestamp_bytes = timestamp.encode(""utf-8"")
        else:
            timestamp_bytes = timestamp

        if not isinstance(payload, (bytes, bytearray)):
            payload_bytes = payload.encode(""utf-8"")
        else:
            payload_bytes = payload

        message = timestamp_bytes + payload_bytes

        # Verify the signature
        try:
            key.verify(signature_bytes, message, ec.ECDSA(hashes.SHA256()))
            return True
        except InvalidSignature:
            return False
        except Exception as e:
            # Any other exception is treated as a verification failure
            return False
```"
131054,SoCo/SoCo,SoCo_SoCo/soco/music_services/token_store.py,soco.music_services.token_store.TokenStoreBase,"class TokenStoreBase:
    """"""Token store base class""""""

    def __init__(self, token_collection='default'):
        """"""Instantiate instance variables

        Args:
            token_collection (str): The name of the token collection to use. This may be
                used to store different token collections for different client programs.
        """"""
        self.token_collection = token_collection

    def save_token_pair(self, music_service_id, household_id, token_pair):
        """"""Save a token value pair (token, key) which is a 2 item sequence""""""
        raise NotImplementedError

    def load_token_pair(self, music_service_id, household_id):
        """"""Load a token pair (token, key) which is a 2 item sequence""""""
        raise NotImplementedError

    def has_token(self, music_service_id, household_id):
        """"""Return True if a token is stored for the music service and household ID""""""
        raise NotImplementedError","class TokenStoreBase:
    '''Token store base class'''
    def __init__(self, token_collection='default'):
        '''Instantiate instance variables
        Args:
            token_collection (str): The name of the token collection to use. This may be
                used to store different token collections for different client programs.
        '''
        pass
    def save_token_pair(self, music_service_id, household_id, token_pair):
        
        pass
    def load_token_pair(self, music_service_id, household_id):
        
        pass
    def has_token(self, music_service_id, household_id):
        '''Return True if a token is stored for the music service and household ID'''
        pass",snippet_45,5,"```python
class TokenStoreBase:
    '''Token store base class'''
    def __init__(self, token_collection='default'):
        '''Instantiate instance variables
        Args:
            token_collection (str): The name of the token collection to use. This may be
                used to store different token collections for different client programs.
        '''
        self.token_collection = token_collection
        # Internal storage: {collection_name: {(music_service_id, household_id): token_pair}}
        self._store = {}

    def save_token_pair(self, music_service_id, household_id, token_pair):
        '''Store a token pair for the given music service and household ID.'''
        collection = self._store.setdefault(self.token_collection, {})
        collection[(music_service_id, household_id)] = token_pair

    def load_token_pair(self, music_service_id, household_id):
        '''Retrieve the token pair for the given music service and household ID.
        Returns:
            The stored token_pair or None if not found.
        '''
        collection = self._store.get(self.token_collection, {})
        return collection.get((music_service_id, household_id))

    def has_token(self, music_service_id, household_id):
        '''Return True if a token is stored for the music service and household ID'''
        collection = self._store.get(self.token_collection, {})
        return (music_service_id, household_id) in collection
```"
131060,SoCo/SoCo,SoCo_SoCo/soco/plugins/sharelink.py,soco.plugins.sharelink.ShareClass,"class ShareClass:
    """"""Base class for supported services.""""""

    def canonical_uri(self, uri):
        """"""Recognize a share link and return its canonical representation.

        Args:
            uri (str): A URI like ""https://tidal.com/browse/album/157273956"".

        Returns:
            str: The canonical URI or None if not recognized.
        """"""
        raise NotImplementedError

    def service_number(self):
        """"""Return the service number.

        Returns:
            int: A number identifying the supported music service.
        """"""
        raise NotImplementedError

    @staticmethod
    def magic():
        """"""Return magic.

        Returns:
            dict: Magic prefix/key/class values for each share type.
        """"""
        return {'album': {'prefix': 'x-rincon-cpcontainer:1004206c', 'key': '00040000', 'class': 'object.container.album.musicAlbum'}, 'episode': {'prefix': '', 'key': '00032020', 'class': 'object.item.audioItem.musicTrack'}, 'track': {'prefix': '', 'key': '00032020', 'class': 'object.item.audioItem.musicTrack'}, 'show': {'prefix': 'x-rincon-cpcontainer:1006206c', 'key': '1006206c', 'class': 'object.container.playlistContainer'}, 'song': {'prefix': '', 'key': '10032020', 'class': 'object.item.audioItem.musicTrack'}, 'playlist': {'prefix': 'x-rincon-cpcontainer:1006206c', 'key': '1006206c', 'class': 'object.container.playlistContainer'}}

    def extract(self, uri):
        """"""Extract the share type and encoded URI from a share link.

        Returns:
            share_type: The shared type, like ""album"" or ""track"".
            encoded_uri: An escaped URI with a service-specific format.
        """"""
        raise NotImplementedError","class ShareClass:
    
    def canonical_uri(self, uri):
        
        pass
    def service_number(self):
        '''Return the service number.
        Returns:
            int: A number identifying the supported music service.
        '''
        pass
    @staticmethod
    def magic():
        
        pass
    def extract(self, uri):
        '''Extract the share type and encoded URI from a share link.
        Returns:
            share_type: The shared type, like ""album"" or ""track"".
            encoded_uri: An escaped URI with a service-specific format.
        '''
        pass",snippet_46,5,"```python
import urllib.parse

class ShareClass:
    def canonical_uri(self, uri):
        """"""
        Return a canonical form of the given URI.
        For the base implementation this simply normalises the scheme and host
        to lower case and removes any trailing slash.
        """"""
        if not uri:
            return uri
        parsed = urllib.parse.urlparse(uri)
        scheme = parsed.scheme.lower()
        netloc = parsed.netloc.lower()
        path = parsed.path.rstrip(""/"")
        # Rebuild the URI without fragment
        canonical = urllib.parse.urlunparse(
            (scheme, netloc, path, parsed.params, parsed.query, """")
        )
        return canonical

    def service_number(self):
        """"""
        Return the service number.
        Returns:
            int: A number identifying the supported music service.
        """"""
        # Base class has no specific service; return 0
        return 0

    @staticmethod
    def magic():
        """"""
        Return a magic string used by subclasses for identification.
        """"""
        return ""magic""

    def extract(self, uri):
        """"""
        Extract the share type and encoded URI from a share link.
        Returns:
            share_type: The shared type, like ""album"" or ""track"".
            encoded_uri: An escaped URI with a service‑specific format.
        """"""
        if not uri:
            return None, None
        parsed = urllib.parse.urlparse(uri)
        # Split the path into segments, ignoring empty ones
        segments = [seg for seg in parsed.path.split(""/"") if seg]
        if not segments:
            return None, None
        share_type = segments[0]
        # Reconstruct the encoded part: everything after the share_type
        encoded_path = ""/"".join(segments[1:])
        if parsed.query:
            encoded_path = f""{encoded_path}?{parsed.query}""
        return share_type, encoded_path
```"
131928,mjg59/python-broadlink,mjg59_python-broadlink/broadlink/protocol.py,broadlink.protocol.Datetime,"import time
import datetime as dt

class Datetime:
    """"""Helps to pack and unpack datetime objects for the Broadlink protocol.""""""

    @staticmethod
    def pack(datetime: dt.datetime) -> bytes:
        """"""Pack the timestamp to be sent over the Broadlink protocol.""""""
        data = bytearray(12)
        utcoffset = int(datetime.utcoffset().total_seconds() / 3600)
        data[:4] = utcoffset.to_bytes(4, 'little', signed=True)
        data[4:6] = datetime.year.to_bytes(2, 'little')
        data[6] = datetime.minute
        data[7] = datetime.hour
        data[8] = int(datetime.strftime('%y'))
        data[9] = datetime.isoweekday()
        data[10] = datetime.day
        data[11] = datetime.month
        return data

    @staticmethod
    def unpack(data: bytes) -> dt.datetime:
        """"""Unpack a timestamp received over the Broadlink protocol.""""""
        utcoffset = int.from_bytes(data[0:4], 'little', signed=True)
        year = int.from_bytes(data[4:6], 'little')
        minute = data[6]
        hour = data[7]
        subyear = data[8]
        isoweekday = data[9]
        day = data[10]
        month = data[11]
        tz_info = dt.timezone(dt.timedelta(hours=utcoffset))
        datetime = dt.datetime(year, month, day, hour, minute, 0, 0, tz_info)
        if datetime.isoweekday() != isoweekday:
            raise ValueError('isoweekday does not match')
        if int(datetime.strftime('%y')) != subyear:
            raise ValueError('subyear does not match')
        return datetime

    @staticmethod
    def now() -> dt.datetime:
        """"""Return the current date and time with timezone info.""""""
        tz_info = dt.timezone(dt.timedelta(seconds=-time.timezone))
        return dt.datetime.now(tz_info)","
class Datetime:
    
    @staticmethod
    def pack(datetime: dt.datetime) -> bytes:
        
        pass
    @staticmethod
    def unpack(data: bytes) -> dt.datetime:
        '''Unpack a timestamp received over the Broadlink protocol.'''
        pass
    @staticmethod
    def now() -> dt.datetime:
        
        pass",snippet_48,4,"```python
import datetime as dt
import struct

class Datetime:
    @staticmethod
    def pack(datetime: dt.datetime) -> bytes:
        """"""Pack a datetime object into 4‑byte little‑endian timestamp.""""""
        # Convert to POSIX timestamp (seconds since epoch)
        ts = int(datetime.timestamp())
        # Pack as unsigned 32‑bit little‑endian integer
        return struct.pack(""<I"", ts)

    @staticmethod
    def unpack(data: bytes) -> dt.datetime:
        """"""Unpack a 4‑byte little‑endian timestamp received over the Broadlink protocol.""""""
        if len(data) != 4:
            raise ValueError(""Timestamp data must be exactly 4 bytes long"")
        # Unpack as unsigned 32‑bit little‑endian integer
        ts = struct.unpack(""<I"", data)[0]
        # Convert back to datetime
        return dt.datetime.fromtimestamp(ts)

    @staticmethod
    def now() -> dt.datetime:
        """"""Return the current local datetime.""""""
        return dt.datetime.now()
```"
132032,rigetti/pyquil,pyquil/quilatom.py,pyquil.quilatom.QuilAtom,"class QuilAtom:
    """"""Abstract class for atomic elements of Quil.""""""

    def out(self) -> str:
        """"""Return the element as a valid Quil string.""""""
        raise NotImplementedError()

    def __str__(self) -> str:
        """"""Get a string representation of the element, possibly not valid Quil.""""""
        raise NotImplementedError()

    def __eq__(self, other: object) -> bool:
        """"""Return True if the other object is equal to this one.""""""
        raise NotImplementedError()

    def __hash__(self) -> int:
        """"""Return a hash of the object.""""""
        raise NotImplementedError()","class QuilAtom:
    
    def out(self) -> str:
        '''Return the element as a valid Quil string.'''
        pass
    def __str__(self) -> str:
        
        pass
    def __eq__(self, other: object) -> bool:
        
        pass
    def __hash__(self) -> int:
        
        pass",snippet_49,5,"class QuilAtom:
    def __init__(self, element: str):
        self.element = element

    def out(self) -> str:
        '''Return the element as a valid Quil string.'''
        return self.element

    def __str__(self) -> str:
        return self.element

    def __eq__(self, other: object) -> bool:
        if isinstance(other, QuilAtom):
            return self.element == other.element
        return False

    def __hash__(self) -> int:
        return hash(self.element)"
134597,Unidata/MetPy,Unidata_MetPy/tools/flake8-metpy/flake8_metpy.py,flake8_metpy.MetPyChecker,"class MetPyChecker:
    """"""Flake8 plugin class to check MetPy style/best practice.""""""
    name = __name__
    version = '1.0'

    def __init__(self, tree):
        """"""Initialize the plugin.""""""
        self.tree = tree

    def run(self):
        """"""Run the plugin and yield errors.""""""
        visitor = MetPyVisitor()
        visitor.visit(self.tree)
        for err in visitor.errors:
            yield self.error(err)

    def error(self, err):
        """"""Format errors into Flake8's required format.""""""
        return (err.lineno, err.col, f'MPY{err.code:03d}: Multiplying/dividing by units--use units.Quantity()', type(self))","class MetPyChecker:
    
    def __init__(self, tree):
        '''Initialize the plugin.'''
        pass
    def run(self):
        
        pass
    def error(self, err):
        
        pass",snippet_50,4,"class MetPyChecker:
    name = ""metpy-checker""
    version = ""0.1.0""

    def __init__(self, tree):
        """"""Initialize the plugin.""""""
        self.tree = tree

    def run(self):
        """"""Run the checker and return a list of errors.""""""
        # No checks implemented; return an empty list.
        return []

    def error(self, err):
        """"""Format an error tuple.""""""
        # err is expected to be a tuple (line, col, message)
        line, col, msg = err
        return (line, col, msg, type(self))
"
134620,Unidata/MetPy,Unidata_MetPy/src/metpy/io/_tools.py,metpy.io._tools.Array,"from struct import Struct

class Array:
    """"""Use a Struct as a callable to unpack a bunch of bytes as a list.""""""

    def __init__(self, fmt):
        """"""Initialize the Struct unpacker.""""""
        self._struct = Struct(fmt)

    def __call__(self, buf):
        """"""Perform the actual unpacking.""""""
        return list(self._struct.unpack(buf))","
class Array:
    '''Use a Struct as a callable to unpack a bunch of bytes as a list.'''
    def __init__(self, fmt):
        
        pass
    def __call__(self, buf):
        '''Perform the actual unpacking.'''
        pass",snippet_51,3,"import struct

class Array:
    '''Use a Struct as a callable to unpack a bunch of bytes as a list.'''
    def __init__(self, fmt):
        self._struct = struct.Struct(fmt)

    def __call__(self, buf):
        '''Perform the actual unpacking.'''
        size = self._struct.size
        if len(buf) % size != 0:
            raise struct.error(""Buffer size is not a multiple of struct size"")
        return [self._struct.unpack(buf[i:i+size]) for i in range(0, len(buf), size)]"
134621,Unidata/MetPy,Unidata_MetPy/src/metpy/io/_tools.py,metpy.io._tools.BitField,"class BitField:
    """"""Convert an integer to a string for each bit.""""""

    def __init__(self, *names):
        """"""Initialize the list of named bits.""""""
        self._names = names

    def __call__(self, val):
        """"""Return a list with a string for each True bit in the integer.""""""
        if not val:
            return None
        bits = []
        for n in self._names:
            if val & 1:
                bits.append(n)
            val >>= 1
            if not val:
                break
        return bits[0] if len(bits) == 1 else bits","class BitField:
    '''Convert an integer to a string for each bit.'''
    def __init__(self, *names):
        '''Initialize the list of named bits.'''
        pass
    def __call__(self, val):
        
        pass",snippet_52,3,"class BitField:
    '''Convert an integer to a string for each bit.'''
    def __init__(self, *names):
        '''Initialize the list of named bits.'''
        self.names = list(names)

    def __call__(self, val):
        '''Return a string representation of the bits of `val` using the stored names.'''
        parts = []
        for i, name in enumerate(self.names):
            bit = (val >> i) & 1
            parts.append(f'{name}={bit}')
        return ' '.join(parts)"
134622,Unidata/MetPy,Unidata_MetPy/src/metpy/io/_tools.py,metpy.io._tools.Bits,"class Bits:
    """"""Breaks an integer into a specified number of True/False bits.""""""

    def __init__(self, num_bits):
        """"""Initialize the number of bits.""""""
        self._bits = range(num_bits)

    def __call__(self, val):
        """"""Convert the integer to the list of True/False values.""""""
        return [bool(val >> i & 1) for i in self._bits]","class Bits:
    
    def __init__(self, num_bits):
        '''Initialize the number of bits.'''
        pass
    def __call__(self, val):
        '''Convert the integer to the list of True/False values.'''
        pass",snippet_53,3,"class Bits:
    def __init__(self, num_bits):
        '''Initialize the number of bits.'''
        if not isinstance(num_bits, int) or num_bits <= 0:
            raise ValueError(""num_bits must be a positive integer"")
        self.num_bits = num_bits

    def __call__(self, val):
        '''Convert the integer to the list of True/False values.'''
        if not isinstance(val, int):
            raise TypeError(""val must be an integer"")
        # Mask to the specified number of bits
        mask = (1 << self.num_bits) - 1
        val &= mask
        bits = []
        for i in range(self.num_bits):
            bits.append(bool((val >> i) & 1))
        return bits"
134624,Unidata/MetPy,Unidata_MetPy/src/metpy/io/_tools.py,metpy.io._tools.Enum,"class Enum:
    """"""Map values to specific strings.""""""

    def __init__(self, *args, **kwargs):
        """"""Initialize the mapping.""""""
        self.val_map = dict(enumerate(args))
        self.val_map.update(zip(kwargs.values(), kwargs.keys(), strict=False))

    def __call__(self, val):
        """"""Map an integer to the string representation.""""""
        return self.val_map.get(val, f'Unknown ({val})')","class Enum:
    
    def __init__(self, *args, **kwargs):
        
        pass
    def __call__(self, val):
        '''Map an integer to the string representation.'''
        pass",snippet_54,3,"class Enum:
    def __init__(self, *args, **kwargs):
        """"""
        Create an enumeration.

        Positional arguments are treated as names and are automatically assigned
        integer values starting from 0. Keyword arguments allow explicit
        mapping of names to integer values.

        Example:
            Color = Enum('RED', 'GREEN', 'BLUE')
            Status = Enum(OK=0, ERROR=1, UNKNOWN=2)
        """"""
        self._value_to_name = {}
        self._name_to_value = {}

        # Handle positional names
        for i, name in enumerate(args):
            if not isinstance(name, str):
                raise TypeError(f""Enum name must be a string, got {type(name).__name__}"")
            if name in self._name_to_value:
                raise ValueError(f""Duplicate enum name: {name}"")
            if i in self._value_to_name:
                raise ValueError(f""Duplicate enum value: {i}"")
            self._name_to_value[name] = i
            self._value_to_name[i] = name

        # Handle keyword mappings
        for name, value in kwargs.items():
            if not isinstance(name, str):
                raise TypeError(f""Enum name must be a string, got {type(name).__name__}"")
            if not isinstance(value, int):
                raise TypeError(f""Enum value must be an integer, got {type(value).__name__}"")
            if name in self._name_to_value:
                raise ValueError(f""Duplicate enum name: {name}"")
            if value in self._value_to_name:
                raise ValueError(f""Duplicate enum value: {value}"")
            self._name_to_value[name] = value
            self._value_to_name[value] = name

    def __call__(self, val):
        """"""
        Map an integer to its string representation.

        Raises:
            TypeError: if the argument is not an integer.
            ValueError: if the integer is not a valid enum value.
        """"""
        if not isinstance(val, int):
            raise TypeError(f""Enum call expects an integer, got {type(val).__name__}"")
        try:
            return self._value_to_name[val]
        except KeyError:
            raise ValueError(f""Invalid enum value: {val}"") from None

    def __repr__(self):
        items = "", "".join(f""{name}={value}"" for name, value in sorted(self._name_to_value.items(), key=lambda x: x[1]))
        return f""<Enum {items}>""

    def __len__(self):
        return len(self._name_to_value)

    def __contains__(self, item):
        if isinstance(item, int):
            return item in self._value_to_name
        if isinstance(item, str):
            return item in self._name_to_value
        return False

    def names(self):
        """"""Return a list of enum names sorted by their integer values.""""""
        return [self._value_to_name[i] for i in sorted(self._value_to_name)]

    def values(self):
        """"""Return a list of enum integer values sorted.""""""
        return sorted(self._value_to_name)

    def items(self):
        """"""Return a list of (name, value) tuples sorted by value.""""""
        return [(self._value_to_name[i], i) for i in sorted(self._value_to_name)]"
134657,Unidata/MetPy,Unidata_MetPy/src/metpy/package_tools.py,metpy.package_tools.Exporter,"class Exporter:
    """"""Manages exporting of symbols from the module.

    Grabs a reference to `globals()` for a module and provides a decorator to add
    functions and classes to `__all__` rather than requiring a separately maintained list.
    Also provides a context manager to do this for instances by adding all instances added
    within a block to `__all__`.
    """"""

    def __init__(self, globls):
        """"""Initialize the Exporter.""""""
        self.globls = globls
        self.exports = globls.setdefault('__all__', [])

    def export(self, defn):
        """"""Declare a function or class as exported.""""""
        self.exports.append(defn.__name__)
        return defn

    def __enter__(self):
        """"""Start a block tracking all instances created at global scope.""""""
        self.start_vars = set(self.globls)

    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""Exit the instance tracking block.""""""
        self.exports.extend(set(self.globls) - self.start_vars)
        del self.start_vars","class Exporter:
    
    def __init__(self, globls):
        
        pass
    def export(self, defn):
        
        pass
    def __enter__(self):
        '''Start a block tracking all instances created at global scope.'''
        pass
    def __exit__(self, exc_type, exc_val, exc_tb):
        '''Exit the instance tracking block.'''
        pass",snippet_55,5,"```python
class Exporter:
    """"""
    A context manager that tracks definitions created inside a block and
    injects them into a supplied globals dictionary when the block exits.
    """"""

    def __init__(self, globls):
        """"""
        Parameters
        ----------
        globls : dict
            The globals dictionary where exported names will be inserted.
        """"""
        self.globls = globls
        self._exports = []

    def export(self, defn):
        """"""
        Register a definition (function, class, or any object with a __name__
        attribute) to be exported when the context exits.

        Parameters
        ----------
        defn : object
            The object to export.
        """"""
        if not hasattr(defn, ""__name__""):
            raise TypeError(""exported objects must have a __name__ attribute"")
        self._exports.append(defn)

    def __enter__(self):
        """"""
        Start a block tracking all definitions created at global scope.
        """"""
        # Nothing special to do on enter; just return self for chaining.
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""
        Exit the instance tracking block and inject all registered
        definitions into the supplied globals dictionary.
        """"""
        for obj in self._exports:
            self.globls[obj.__name__] = obj
        # Clear the list to avoid leaking references.
        self._exports.clear()
        # Returning False propagates any exception that occurred.
        return False
```"
137119,jpype-project/jpype,jpype-project_jpype/doc/java/util.py,java.util.Collection,"class Collection:
    """""" Customized interface representing a collection of items.

    JPype wraps ``java.util.Collection`` as a Python collection.
    """"""

    def __len__(self) -> int:
        """""" Get the length of this collection.

        Use ``len(collection)`` to find the number of items in this
        collection.

        """"""
        ...

    def __delitem__(self, item):
        """""" Collections do not support remove by index. """"""
        ...

    def __contains__(self, item) -> bool:
        """""" Check if this collection contains this item.

        Use ``item in collection`` to check if the item is 
        present.

        Args:
           item: is the item to check for.  This must be a Java
           object or an object which can be automatically converted
           such as a string.

        Returns:
           bool: True if the item is in the collection.
        """"""
        ...","class Collection:
    ''' Customized interface representing a collection of items.
    JPype wraps ``java.util.Collection`` as a Python collection.
    '''
    def __len__(self) -> int:
        ''' Get the length of this collection.
        Use ``len(collection)`` to find the number of items in this
        collection.
        '''
        pass
    def __delitem__(self, item):
        
        pass
    def __contains__(self, item) -> bool:
        ''' Check if this collection contains this item.
        Use ``item in collection`` to check if the item is 
        present.
        Args:
           item: is the item to check for.  This must be a Java
           object or an object which can be automatically converted
           such as a string.
        Returns:
           bool: True if the item is in the collection.
        '''
        pass",snippet_56,4,"```python
class Collection:
    ''' Customized interface representing a collection of items.
    JPype wraps ``java.util.Collection`` as a Python collection.
    '''
    def __len__(self) -> int:
        ''' Get the length of this collection.
        Use ``len(collection)`` to find the number of items in this
        collection.
        '''
        return self.size()

    def __delitem__(self, item):
        ''' Remove an item from the collection.
        Raises KeyError if the item is not present.
        '''
        removed = self.remove(item)
        if not removed:
            raise KeyError(item)

    def __contains__(self, item) -> bool:
        ''' Check if this collection contains this item.
        Use ``item in collection`` to check if the item is 
        present.
        Args:
           item: is the item to check for.  This must be a Java
           object or an object which can be automatically converted
           such as a string.
        Returns:
           bool: True if the item is in the collection.
        '''
        return self.contains(item)
```"
138700,sebp/scikit-survival,sebp_scikit-survival/sksurv/util.py,sksurv.util.Surv,"from sklearn.utils.validation import check_array, check_consistent_length
import numpy as np
import pandas as pd

class Surv:
    """"""A helper class to create a structured array for survival analysis.

    This class provides helper functions to create a structured array that
    encapsulates the event indicator and the observed time. The resulting
    structured array is the recommended format for the ``y`` argument in
    scikit-survival's estimators.
    """"""

    @staticmethod
    def from_arrays(event, time, name_event=None, name_time=None):
        """"""Create structured array from event indicator and time arrays.

        Parameters
        ----------
        event : array-like, shape=(n_samples,)
            Event indicator. A boolean array or array with values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : array-like, shape=(n_samples,)
            Observed time. Time to event or time of censoring.
        name_event : str, optional, default: 'event'
            Name of the event field in the structured array.
        name_time : str, optional, default: 'time'
            Name of the observed time field in the structured array.

        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are set to the values of `name_event` and `name_time`.

        Examples
        --------
        >>> from sksurv.util import Surv
        >>>
        >>> y = Surv.from_arrays(event=[True, False, True],
        ...                      time=[10, 25, 15])
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('event', '?'), ('time', '<f8')])
        >>> y['event']
        array([ True, False,  True])
        >>> y['time']
        array([10., 25., 15.])
        """"""
        name_event = name_event or 'event'
        name_time = name_time or 'time'
        if name_time == name_event:
            raise ValueError('name_time must be different from name_event')
        time = np.asanyarray(time, dtype=float)
        y = np.empty(time.shape[0], dtype=[(name_event, bool), (name_time, float)])
        y[name_time] = time
        event = np.asanyarray(event)
        check_consistent_length(time, event)
        if np.issubdtype(event.dtype, np.bool_):
            y[name_event] = event
        else:
            events = np.unique(event)
            events.sort()
            if len(events) != 2:
                raise ValueError('event indicator must be binary')
            if np.all(events == np.array([0, 1], dtype=events.dtype)):
                y[name_event] = event.astype(bool)
            else:
                raise ValueError('non-boolean event indicator must contain 0 and 1 only')
        return y

    @staticmethod
    def from_dataframe(event, time, data):
        """"""Create structured array from columns in a pandas DataFrame.

        Parameters
        ----------
        event : str
            Name of the column in ``data`` containing the event indicator.
            It must be a boolean or have values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : str
            Name of the column in ``data`` containing the observed time
            (time to event or time of censoring).
        data : pandas.DataFrame
            A DataFrame with columns for event and time.

        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are the respective column names.

        Examples
        --------
        >>> import pandas as pd
        >>> from sksurv.util import Surv
        >>>
        >>> df = pd.DataFrame({
        ...     'status': [True, False, True],
        ...     'followup_time': [10, 25, 15],
        ... })
        >>> y = Surv.from_dataframe(
        ...     event='status', time='followup_time', data=df,
        ... )
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('status', '?'), ('followup_time', '<f8')])
        >>> y['status']
        array([ True, False,  True])
        >>> y['followup_time']
        array([10., 25., 15.])
        """"""
        if not isinstance(data, pd.DataFrame):
            raise TypeError(f'expected pandas.DataFrame, but got {type(data)!r}')
        return Surv.from_arrays(data.loc[:, event].values, data.loc[:, time].values, name_event=str(event), name_time=str(time))","
class Surv:
    '''A helper class to create a structured array for survival analysis.
    This class provides helper functions to create a structured array that
    encapsulates the event indicator and the observed time. The resulting
    structured array is the recommended format for the ``y`` argument in
    scikit-survival's estimators.
    '''
    @staticmethod
    def from_arrays(event, time, name_event=None, name_time=None):
        '''Create structured array from event indicator and time arrays.
        Parameters
        ----------
        event : array-like, shape=(n_samples,)
            Event indicator. A boolean array or array with values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : array-like, shape=(n_samples,)
            Observed time. Time to event or time of censoring.
        name_event : str, optional, default: 'event'
            Name of the event field in the structured array.
        name_time : str, optional, default: 'time'
            Name of the observed time field in the structured array.
        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are set to the values of `name_event` and `name_time`.
        Examples
        --------
        >>> from sksurv.util import Surv
        >>>
        >>> y = Surv.from_arrays(event=[True, False, True],
        ...                      time=[10, 25, 15])
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('event', '?'), ('time', '<f8')])
        >>> y['event']
        array([ True, False,  True])
        >>> y['time']
        array([10., 25., 15.])
        '''
        pass
    @staticmethod
    def from_dataframe(event, time, data):
        '''Create structured array from columns in a pandas DataFrame.
        Parameters
        ----------
        event : str
            Name of the column in ``data`` containing the event indicator.
            It must be a boolean or have values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : str
            Name of the column in ``data`` containing the observed time
            (time to event or time of censoring).
        data : pandas.DataFrame
            A DataFrame with columns for event and time.
        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are the respective column names.
        Examples
        --------
        >>> import pandas as pd
        >>> from sksurv.util import Surv
        >>>
        >>> df = pd.DataFrame({
        ...     'status': [True, False, True],
        ...     'followup_time': [10, 25, 15],
        ... })
        >>> y = Surv.from_dataframe(
        ...     event='status', time='followup_time', data=df,
        ... )
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('status', '?'), ('followup_time', '<f8')])
        >>> y['status']
        array([ True, False,  True])
        >>> y['followup_time']
        array([10., 25., 15.])
        '''
        pass",snippet_57,3,"```python
import numpy as np
import pandas as pd


class Surv:
    '''A helper class to create a structured array for survival analysis.
    This class provides helper functions to create a structured array that
    encapsulates the event indicator and the observed time. The resulting
    structured array is the recommended format for the ``y`` argument in
    scikit-survival's estimators.
    '''
    @staticmethod
    def from_arrays(event, time, name_event=None, name_time=None):
        '''Create structured array from event indicator and time arrays.
        Parameters
        ----------
        event : array-like, shape=(n_samples,)
            Event indicator. A boolean array or array with values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : array-like, shape=(n_samples,)
            Observed time. Time to event or time of censoring.
        name_event : str, optional, default: 'event'
            Name of the event field in the structured array.
        name_time : str, optional, default: 'time'
            Name of the observed time field in the structured array.
        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are set to the values of `name_event` and `name_time`.
        Examples
        --------
        >>> from sksurv.util import Surv
        >>>
        >>> y = Surv.from_arrays(event=[True, False, True],
        ...                      time=[10, 25, 15])
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('event', '?'), ('time', '<f8')])
        >>> y['event']
        array([ True, False,  True])
        >>> y['time']
        array([10., 25., 15.])
        '''
        if name_event is None:
            name_event = 'event'
        if name_time is None:
            name_time = 'time'

        # Convert inputs to numpy arrays
        event_arr = np.asarray(event)
        time_arr = np.asarray(time)

        # Ensure event is boolean
        if event_arr.dtype.kind not in ('b', 'i', 'u'):
            raise ValueError(""event array must be boolean or integer type"")
        event_bool = event_arr.astype(bool)

        # Ensure time is float
        time_float = time_arr.astype(float)

        dtype = np.dtype([(name_event, '?'), (name_time, '<f8')])
        y = np.empty(event_bool.shape, dtype=dtype)
        y[name_event] = event_bool
        y[name_time] = time_float
        return y

    @staticmethod
    def from_dataframe(event, time, data):
        '''Create structured array from columns in a pandas DataFrame.
        Parameters
        ----------
        event : str
            Name of the column in ``data`` containing the event indicator.
            It must be a boolean or have values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : str
            Name of the column in ``data`` containing the observed time
            (time to event or time of censoring).
        data : pandas.DataFrame
            A DataFrame with columns for event and time.
        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are the respective column names.
        Examples
        --------
        >>> import pandas as pd
        >>> from sksurv.util import Surv
        >>>
        >>> df = pd.DataFrame({
        ...     'status': [True, False, True],
        ...     'followup_time': [10, 25, 15],
        ... })
        >>> y = Surv.from_dataframe(
        ...     event='status', time='followup_time', data=df,
        ... )
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('status', '?'), ('followup_time', '<f8')])
        >>> y['status']
        array([ True, False,  True])
        >>> y['followup_time']
        array([10., 25., 15.])
        '''
        if not isinstance(data, pd.DataFrame):
            raise TypeError(""data must be a pandas DataFrame"")

        if event not in data.columns:
            raise KeyError(f""event column '{event}' not found in DataFrame"")
        if time not in data.columns:
            raise KeyError(f""time column '{time}' not found in DataFrame"")

        event_arr = data[event].values
        time_arr = data[time].values

        return Surv.from_arrays(event_arr, time_arr, name_event=event, name_time=time)
```"
139652,sentinel-hub/eo-learn,sentinel-hub_eo-learn/eolearn/geometry/morphology.py,eolearn.geometry.morphology.MorphologicalStructFactory,"import cv2
import numpy as np

class MorphologicalStructFactory:
    """"""
    Factory methods for generating morphological structuring elements
    """"""

    @staticmethod
    def get_disk(radius: int) -> np.ndarray:
        """"""
        :param radius: Radius of disk
        :return: The structuring element where elements of the neighborhood are 1 and 0 otherwise.
        """"""
        return cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (radius, radius))

    @staticmethod
    def get_rectangle(width: int, height: int) -> np.ndarray:
        """"""
        :param width: Width of rectangle
        :param height: Height of rectangle
        :return: A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.
        """"""
        return cv2.getStructuringElement(cv2.MORPH_RECT, (height, width))

    @staticmethod
    def get_square(width: int) -> np.ndarray:
        """"""
        :param width: Size of square
        :return: A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.
        """"""
        return cv2.getStructuringElement(cv2.MORPH_RECT, (width, width))","
class MorphologicalStructFactory:
    
    @staticmethod
    def get_disk(radius: int) -> np.ndarray:
        '''
        :param radius: Radius of disk
        :return: The structuring element where elements of the neighborhood are 1 and 0 otherwise.
        '''
        pass
    @staticmethod
    def get_rectangle(width: int, height: int) -> np.ndarray:
        '''
        :param width: Width of rectangle
        :param height: Height of rectangle
        :return: A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.
        '''
        pass
    @staticmethod
    def get_square(width: int) -> np.ndarray:
        '''
        :param width: Size of square
        :return: A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.
        '''
        pass",snippet_58,4,"```python
import numpy as np

class MorphologicalStructFactory:
    
    @staticmethod
    def get_disk(radius: int) -> np.ndarray:
        """"""
        Return a binary structuring element of a disk with the given radius.
        The element is a 2D array of shape (2*radius+1, 2*radius+1) where
        elements inside the disk are 1 and outside are 0.
        """"""
        if radius < 0:
            raise ValueError(""radius must be non‑negative"")
        size = 2 * radius + 1
        y, x = np.ogrid[-radius:radius+1, -radius:radius+1]
        mask = x**2 + y**2 <= radius**2
        return mask.astype(np.uint8)

    @staticmethod
    def get_rectangle(width: int, height: int) -> np.ndarray:
        """"""
        Return a rectangular structuring element of the given width and height.
        The element is a 2D array of shape (height, width) filled with 1s.
        """"""
        if width <= 0 or height <= 0:
            raise ValueError(""width and height must be positive"")
        return np.ones((height, width), dtype=np.uint8)

    @staticmethod
    def get_square(width: int) -> np.ndarray:
        """"""
        Return a square structuring element of the given width.
        The element is a 2D array of shape (width, width) filled with 1s.
        """"""
        if width <= 0:
            raise ValueError(""width must be positive"")
        return np.ones((width, width), dtype=np.uint8)
```"
139672,sentinel-hub/eo-learn,sentinel-hub_eo-learn/eolearn/visualization/eoexecutor.py,eolearn.visualization.eoexecutor._ErrorSummary,"from dataclasses import dataclass

@dataclass()
class _ErrorSummary:
    """"""Contains data for errors of a node.""""""
    origin: str
    example_message: str
    failed_indexed_executions: list[tuple[int, str]]

    def add_execution(self, index: int, name: str) -> None:
        """"""Adds an execution to the summary.""""""
        self.failed_indexed_executions.append((index, name))

    @property
    def num_failed(self) -> int:
        """"""Helps with jinja""""""
        return len(self.failed_indexed_executions)","@dataclass()
class _ErrorSummary:
    '''Contains data for errors of a node.'''
    def add_execution(self, index: int, name: str) -> None:
        '''Adds an execution to the summary.'''
        pass
    @property
    def num_failed(self) -> int:
        
        pass",snippet_59,3,"```python
from dataclasses import dataclass, field
from typing import List, Tuple

@dataclass()
class _ErrorSummary:
    '''Contains data for errors of a node.'''
    _executions: List[Tuple[int, str]] = field(default_factory=list)

    def add_execution(self, index: int, name: str) -> None:
        '''Adds an execution to the summary.'''
        self._executions.append((index, name))

    @property
    def num_failed(self) -> int:
        return len(self._executions)
```"
139835,python-escpos/python-escpos,python-escpos_python-escpos/src/escpos/codepages.py,escpos.codepages.CodePageManager,"class CodePageManager:
    """"""Holds information about all the code pages.

    Information as defined in escpos-printer-db.
    """"""

    def __init__(self, data):
        """"""Initialize code page manager.""""""
        self.data = data

    @staticmethod
    def get_encoding_name(encoding):
        """"""Get encoding name.

        .. todo:: Resolve the encoding alias.
        """"""
        return encoding.upper()

    def get_encoding(self, encoding):
        """"""Return the encoding data.""""""
        return self.data[encoding]","class CodePageManager:
    '''Holds information about all the code pages.
    Information as defined in escpos-printer-db.
        '''
    def __init__(self, data):
        
        pass
    @staticmethod
    def get_encoding_name(encoding):
        
        pass
    def get_encoding_name(encoding):
        '''Return the encoding data.'''
        pass",snippet_60,4,"```python
class CodePageManager:
    '''Holds information about all the code pages.
    Information as defined in escpos-printer-db.
    '''
    def __init__(self, data):
        """"""
        Initialize the manager with a dictionary of code page data.
        The keys are expected to be canonical encoding names.
        """"""
        # Store a case‑insensitive copy of the data
        self._data = {k.lower(): v for k, v in data.items()}

    @staticmethod
    def get_encoding_name(encoding):
        """"""
        Return the canonical encoding name for a given alias.
        """"""
        # Common aliases mapping to canonical names
        alias_map = {
            'cp437': '437',
            '437': '437',
            'cp850': '850',
            '850': '850',
            'cp852': '852',
            '852': '852',
            'cp855': '855',
            '855': '855',
            'cp857': '857',
            '857': '857',
            'cp860': '860',
            '860': '860',
            'cp861': '861',
            '861': '861',
            'cp862': '862',
            '862': '862',
            'cp863': '863',
            '863': '863',
            'cp864': '864',
            '864': '864',
            'cp865': '865',
            '865': '865',
            'cp866': '866',
            '866': '866',
            'cp869': '869',
            '869': '869',
            'cp1252': '1252',
            'windows-1252': '1252',
            '1252': '1252',
            'utf-8': 'utf8',
            'utf8': 'utf8',
            'latin1': 'latin1',
            'iso8859-1': 'latin1',
            'iso8859-15': 'latin9',
            'latin9': 'latin9',
        }
        key = encoding.lower()
        return alias_map.get(key, key)

    def get_encoding_name(self, encoding):
        """"""
        Return the encoding data for the given encoding.
        Raises KeyError if the encoding is not known.
        """"""
        # Try the raw key first
        key = encoding.lower()
        if key in self._data:
            return self._data[key]

        # Try canonical name mapping
        canonical = self.get_encoding_name(encoding)
        if canonical in self._data:
            return self._data[canonical]

        # Not found
        raise KeyError(f""Encoding '{encoding}' not found in code page data."")
```"
141706,mar10/wsgidav,mar10_wsgidav/wsgidav/rw_lock.py,wsgidav.rw_lock.ReadWriteLock,"from threading import Condition, Lock, current_thread
from time import time

class ReadWriteLock:
    """"""Read-Write lock class. A read-write lock differs from a standard
    threading.RLock() by allowing multiple threads to simultaneously hold a
    read lock, while allowing only a single thread to hold a write lock at the
    same point of time.

    When a read lock is requested while a write lock is held, the reader
    is blocked; when a write lock is requested while another write lock is
    held or there are read locks, the writer is blocked.

    Writers are always preferred by this implementation: if there are blocked
    threads waiting for a write lock, current readers may request more read
    locks (which they eventually should free, as they starve the waiting
    writers otherwise), but a new thread requesting a read lock will not
    be granted one, and block. This might mean starvation for readers if
    two writer threads interweave their calls to acquire_write() without
    leaving a window only for readers.

    In case a current reader requests a write lock, this can and will be
    satisfied without giving up the read locks first, but, only one thread
    may perform this kind of lock upgrade, as a deadlock would otherwise
    occur. After the write lock has been granted, the thread will hold a
    full write lock, and not be downgraded after the upgrading call to
    acquire_write() has been match by a corresponding release().
    """"""

    def __init__(self):
        """"""Initialize this read-write lock.""""""
        self.__condition = Condition(Lock())
        self.__writer = None
        self.__upgradewritercount = 0
        self.__pendingwriters = []
        self.__readers = {}

    def acquire_read(self, *, timeout=None):
        """"""Acquire a read lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.

        In case timeout is None, the call to acquire_read blocks until the
        lock request can be serviced.

        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.""""""
        if timeout is not None:
            endtime = time() + timeout
        me = current_thread()
        self.__condition.acquire()
        try:
            if self.__writer is me:
                self.__writercount += 1
                return
            while True:
                if self.__writer is None:
                    if self.__upgradewritercount or self.__pendingwriters:
                        if me in self.__readers:
                            self.__readers[me] += 1
                            return
                    else:
                        self.__readers[me] = self.__readers.get(me, 0) + 1
                        return
                if timeout is not None:
                    remaining = endtime - time()
                    if remaining <= 0:
                        raise RuntimeError('Acquiring read lock timed out')
                    self.__condition.wait(remaining)
                else:
                    self.__condition.wait()
        finally:
            self.__condition.release()

    def acquire_write(self, *, timeout=None):
        """"""Acquire a write lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.

        In case the write lock cannot be serviced due to the deadlock
        condition mentioned above, a ValueError is raised.

        In case timeout is None, the call to acquire_write blocks until the
        lock request can be serviced.

        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.""""""
        if timeout is not None:
            endtime = time() + timeout
        me, upgradewriter = (current_thread(), False)
        self.__condition.acquire()
        try:
            if self.__writer is me:
                self.__writercount += 1
                return
            elif me in self.__readers:
                if self.__upgradewritercount:
                    raise ValueError('Inevitable dead lock, denying write lock')
                upgradewriter = True
                self.__upgradewritercount = self.__readers.pop(me)
            else:
                self.__pendingwriters.append(me)
            while True:
                if not self.__readers and self.__writer is None:
                    if self.__upgradewritercount:
                        if upgradewriter:
                            self.__writer = me
                            self.__writercount = self.__upgradewritercount + 1
                            self.__upgradewritercount = 0
                            return
                    elif self.__pendingwriters[0] is me:
                        self.__writer = me
                        self.__writercount = 1
                        self.__pendingwriters = self.__pendingwriters[1:]
                        return
                if timeout is not None:
                    remaining = endtime - time()
                    if remaining <= 0:
                        if upgradewriter:
                            self.__readers[me] = self.__upgradewritercount
                            self.__upgradewritercount = 0
                        else:
                            self.__pendingwriters.remove(me)
                        raise RuntimeError('Acquiring write lock timed out')
                    self.__condition.wait(remaining)
                else:
                    self.__condition.wait()
        finally:
            self.__condition.release()

    def release(self):
        """"""Release the currently held lock.

        In case the current thread holds no lock, a ValueError is thrown.""""""
        me = current_thread()
        self.__condition.acquire()
        try:
            if self.__writer is me:
                self.__writercount -= 1
                if not self.__writercount:
                    self.__writer = None
                    self.__condition.notify_all()
            elif me in self.__readers:
                self.__readers[me] -= 1
                if not self.__readers[me]:
                    del self.__readers[me]
                    if not self.__readers:
                        self.__condition.notify_all()
            else:
                raise ValueError('Trying to release unheld lock')
        finally:
            self.__condition.release()","
class ReadWriteLock:
    '''Read-Write lock class. A read-write lock differs from a standard
    threading.RLock() by allowing multiple threads to simultaneously hold a
    read lock, while allowing only a single thread to hold a write lock at the
    same point of time.
    When a read lock is requested while a write lock is held, the reader
    is blocked; when a write lock is requested while another write lock is
    held or there are read locks, the writer is blocked.
    Writers are always preferred by this implementation: if there are blocked
    threads waiting for a write lock, current readers may request more read
    locks (which they eventually should free, as they starve the waiting
    writers otherwise), but a new thread requesting a read lock will not
    be granted one, and block. This might mean starvation for readers if
    two writer threads interweave their calls to acquire_write() without
    leaving a window only for readers.
    In case a current reader requests a write lock, this can and will be
    satisfied without giving up the read locks first, but, only one thread
    may perform this kind of lock upgrade, as a deadlock would otherwise
    occur. After the write lock has been granted, the thread will hold a
    full write lock, and not be downgraded after the upgrading call to
    acquire_write() has been match by a corresponding release().
    '''
    def __init__(self):
        '''Initialize this read-write lock.'''
        pass
    def acquire_read(self, *, timeout=None):
        '''Acquire a read lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.
        In case timeout is None, the call to acquire_read blocks until the
        lock request can be serviced.
        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.'''
        pass
    def acquire_write(self, *, timeout=None):
        '''Acquire a write lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.
        In case the write lock cannot be serviced due to the deadlock
        condition mentioned above, a ValueError is raised.
        In case timeout is None, the call to acquire_write blocks until the
        lock request can be serviced.
        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.'''
        pass
    def release(self):
        '''Release the currently held lock.
        In case the current thread holds no lock, a ValueError is thrown.'''
        pass",snippet_62,5,"```python
import threading
import time
import sys

class ReadWriteLock:
    '''Read-Write lock class. A read-write lock differs from a standard
    threading.RLock() by allowing multiple threads to simultaneously hold a
    read lock, while allowing only a single thread to hold a write lock at the
    same point of time.
    When a read lock is requested while a write lock is held, the reader
    is blocked; when a write lock is requested while another write lock is
    held or there are read locks, the writer is blocked.
    Writers are always preferred by this implementation: if there are blocked
    threads waiting for a write lock, current readers may request more read
    locks (which they eventually should free, as they starve the waiting
    writers otherwise), but a new thread requesting a read lock will not
    be granted one, and block. This might mean starvation for readers if
    two writer threads interweave their calls to acquire_write() without
    leaving a window only for readers.
    In case a current reader requests a write lock, this can and will be
    satisfied without giving up the read locks first, but, only one thread
    may perform this kind of lock upgrade, as a deadlock would otherwise
    occur. After the write lock has been granted, the thread will hold a
    full write lock, and not be downgraded after the upgrading call to
    acquire_write() has been match by a corresponding release().
    '''
    def __init__(self):
        '''Initialize this read-write lock.'''
        self._lock = threading.RLock()
        self._cond = threading.Condition(self._lock)
        self._readers = {}          # thread_id -> count
        self._writer = None         # thread_id
        self._write_requests = 0    # pending write requests
        self._upgrade_request = None  # thread_id that wants to upgrade

    def _now(self):
        return time.monotonic()

    def acquire_read(self, *, timeout=None):
        '''Acquire a read lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.
        In case timeout is None, the call to acquire_read blocks until the
        lock request can be serviced.
        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.'''
        me = threading.get_ident()
        end = None if timeout is None else self._now() + timeout
        with self._cond:
            while True:
                # If writer active or writers waiting, block
                if self._writer is not None or self._write_requests > 0:
                    if timeout is not None:
                        remaining = end - self._now()
                        if remaining <= 0:
                            raise RuntimeError(""Timeout while acquiring read lock"")
                        self._cond.wait(remaining)
                    else:
                        self._cond.wait()
                    continue
                # If this thread already holds write lock, allow re-entrant read
                if self._writer == me:
                    # treat as read lock
                    self._readers[me] = self._readers.get(me, 0) + 1
                    return
                # Normal read acquisition
                self._readers[me] = self._readers.get(me, 0) + 1
                return

    def acquire_write(self, *, timeout=None):
        '''Acquire a write lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.
        In case the write lock cannot be serviced due to the deadlock
        condition mentioned above, a ValueError is raised.
        In case timeout is None, the call to acquire_write blocks until the
        lock request can be serviced.
        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.'''
        me = threading.get_ident()
        end = None if timeout is None else self._now() + timeout
        with self._cond:
            # Check for upgrade deadlock
            if self._upgrade_request is not None and self._upgrade_request != me:
                raise ValueError(""Another thread is already upgrading"")
            # If this thread already holds write lock, re-entrant
            if self._writer == me:
                return
            # If this thread holds read locks and no other readers, allow upgrade
            if me in self._readers and len(self._readers) == 1 and self._readers[me] > 0:
                # Upgrade path
                self._upgrade_request = me
                # Wait until no other readers (should be none)
                while True:
                    if len(self._readers) == 1 and self._readers[me] > 0:
                        # Acquire write lock
                        self._writer = me
                        self._upgrade_request = None
                        # Remove read lock count
                        del self._readers[me]
                        return
                    if timeout is not None:
                        remaining = end - self._now()
                        if remaining <= 0:
                            raise RuntimeError(""Timeout while upgrading to write lock"")
                        self._cond.wait(remaining)
                    else:
                        self._cond.wait()
                # unreachable
            # Normal write acquisition
            self._write_requests += 1
            try:
                while True:
                    # No writer and no readers
                    if self._writer is None and len(self._readers) == 0:
                        self._writer = me
                        return
                    if timeout is not None:
                        remaining = end - self._now()
                        if remaining <= 0:
                            raise RuntimeError(""Timeout while acquiring write lock"")
                        self._cond.wait(remaining)
                    else:
                        self._cond.wait()
            finally:
                self._write_requests -= 1

    def release(self):
        '''Release the currently held lock.
        In case the current thread holds no lock, a ValueError is thrown.'''
        me = threading.get_ident()
        with self._cond:
            if self._writer == me:
                self._writer = None
                self._cond.notify_all()
                return
            if me in self._readers:
                self._readers[me] -= 1
                if self._readers[me] == 0:
                    del self._readers[me]
                self._cond.notify_all()
                return
            raise ValueError(""Current thread holds no lock"")
```"
141733,mar10/wsgidav,mar10_wsgidav/wsgidav/stream_tools.py,wsgidav.stream_tools.StreamingFile,"class StreamingFile:
    """"""A file object wrapped around an iterator / data stream.""""""

    def __init__(self, data_stream):
        """"""Initialise the object with the data stream.""""""
        self.data_stream = data_stream
        self.buffer = ''

    def read(self, size=None):
        """"""Read bytes from an iterator.""""""
        while size is None or len(self.buffer) < size:
            try:
                self.buffer += next(self.data_stream)
            except StopIteration:
                break
        sized_chunk = self.buffer[:size]
        if size is None:
            self.buffer = ''
        else:
            self.buffer = self.buffer[size:]
        return sized_chunk","class StreamingFile:
    '''A file object wrapped around an iterator / data stream.'''
    def __init__(self, data_stream):
        '''Initialise the object with the data stream.'''
        pass
    def read(self, size=None):
        
        pass",snippet_63,3,"class StreamingFile:
    '''A file object wrapped around an iterator / data stream.'''
    def __init__(self, data_stream):
        '''Initialise the object with the data stream.'''
        self._stream = iter(data_stream)
        self._buffer = b''

    def read(self, size=None):
        '''Read up to size bytes from the stream. If size is None or negative, read all remaining data.'''
        # Handle reading all data
        if size is None or size < 0:
            chunks = [self._buffer]
            self._buffer = b''
            for chunk in self._stream:
                if isinstance(chunk, str):
                    chunk = chunk.encode()
                chunks.append(chunk)
            return b''.join(chunks)

        # Handle zero or negative size explicitly
        if size == 0:
            return b''

        # Ensure we have enough data in the buffer
        while len(self._buffer) < size:
            try:
                chunk = next(self._stream)
            except StopIteration:
                break
            if isinstance(chunk, str):
                chunk = chunk.encode()
            self._buffer += chunk

        # Slice the required amount
        result = self._buffer[:size]
        self._buffer = self._buffer[size:]
        return result
"
142723,PyThaiNLP/pythainlp,PyThaiNLP_pythainlp/pythainlp/augment/lm/fasttext.py,pythainlp.augment.lm.fasttext.FastTextAug,"from gensim.models.fasttext import FastText as FastText_gensim
from typing import List, Tuple
import itertools
from gensim.models.keyedvectors import KeyedVectors
from pythainlp.tokenize import word_tokenize

class FastTextAug:
    """"""
    Text Augment from fastText

    :param str model_path: path of model file
    """"""

    def __init__(self, model_path: str):
        """"""
        :param str model_path: path of model file
        """"""
        if model_path.endswith('.bin'):
            self.model = FastText_gensim.load_facebook_vectors(model_path)
        elif model_path.endswith('.vec'):
            self.model = KeyedVectors.load_word2vec_format(model_path)
        else:
            self.model = FastText_gensim.load(model_path)
        self.dict_wv = list(self.model.key_to_index.keys())

    def tokenize(self, text: str) -> List[str]:
        """"""
        Thai text tokenization for fastText

        :param str text: Thai text

        :return: list of words
        :rtype: List[str]
        """"""
        return word_tokenize(text, engine='icu')

    def modify_sent(self, sent: str, p: float=0.7) -> List[List[str]]:
        """"""
        :param str sent: text of sentence
        :param float p: probability
        :rtype: List[List[str]]
        """"""
        list_sent_new = []
        for i in sent:
            if i in self.dict_wv:
                w = [j for j, v in self.model.most_similar(i) if v >= p]
                if w == []:
                    list_sent_new.append([i])
                else:
                    list_sent_new.append(w)
            else:
                list_sent_new.append([i])
        return list_sent_new

    def augment(self, sentence: str, n_sent: int=1, p: float=0.7) -> List[Tuple[str]]:
        """"""
        Text Augment from fastText

        You may want to download the Thai model
        from https://fasttext.cc/docs/en/crawl-vectors.html.

        :param str sentence: Thai sentence
        :param int n_sent: number of sentences
        :param float p: probability of word

        :return: list of synonyms
        :rtype: List[Tuple[str]]
        """"""
        self.sentence = self.tokenize(sentence)
        self.list_synonym = self.modify_sent(self.sentence, p=p)
        new_sentences = []
        for x in list(itertools.product(*self.list_synonym))[0:n_sent]:
            new_sentences.append(x)
        return new_sentences","
class FastTextAug:
    
    def __init__(self, model_path: str):
        
        pass
    def tokenize(self, text: str) -> List[str]:
        '''
        Thai text tokenization for fastText
        :param str text: Thai text
        :return: list of words
        :rtype: List[str]
        '''
        pass
    def modify_sent(self, sent: str, p: float=0.7) -> List[List[str]]:
        
        pass
    def augment(self, sentence: str, n_sent: int=1, p: float=0.7) -> List[Tuple[str]]:
        
        pass",snippet_64,5,"```python
import fasttext
from typing import List, Tuple

try:
    # Try to import a Thai tokenizer if available
    from pythainlp import word_tokenize
except ImportError:
    word_tokenize = None


class FastTextAug:
    """"""
    A simple augmentation class that uses a fastText model to replace words
    in a Thai sentence with their nearest neighbours in the embedding space.
    """"""

    def __init__(self, model_path: str):
        """"""
        Load a fastText model from the given path.

        :param model_path: Path to the fastText binary or text model file.
        """"""
        self.model = fasttext.load_model(model_path)

    def tokenize(self, text: str) -> List[str]:
        """"""
        Tokenize Thai text for fastText.

        :param text: Thai sentence.
        :return: List of tokens.
        """"""
        if word_tokenize:
            # Use the newmm engine for better Thai tokenization
            return word_tokenize(text, engine=""newmm"")
        # Fallback: simple whitespace split
        return text.split()

    def modify_sent(self, sent: str, p: float = 0.7) -> List[List[str]]:
        """"""
        Replace words in the sentence with their nearest neighbours with probability p.

        :param sent: Original sentence.
        :param p: Probability of replacing a word.
        :return: List containing a single list of modified tokens.
        """"""
        tokens = self.tokenize(sent)
        new_tokens = []

        for token in tokens:
            if token.strip() == """":
                new_tokens.append(token)
                continue

            # Decide whether to replace this token
            if p > 0 and random.random() < p:
                # fastText returns a list of tuples (word, similarity)
                neighbors = self.model.get_nearest_neighbors(token, k=1)
                if neighbors:
                    # Use the nearest neighbour word
                    new_token = neighbors[0][1]
                    new_tokens.append(new_token)
                    continue
            # Keep original token if not replaced
            new_tokens.append(token)

        return [new_tokens]

    def augment(self, sentence: str, n_sent: int = 1, p: float = 0.7) -> List[Tuple[str]]:
        """"""
        Generate n_sent augmented sentences from the original sentence.

        :param sentence: Original sentence.
        :param n_sent: Number of augmented sentences to generate.
        :param p: Probability of replacing a word.
        :return: List of tuples, each containing one augmented sentence.
        """"""
        augmented = []
        for _ in range(n_sent):
            modified_tokens_list = self.modify_sent(sentence, p)
            # modify_sent returns a list with one token list
            modified_tokens = modified_tokens_list[0]
            augmented_sentence = "" "".join(modified_tokens)
            augmented.append((augmented_sentence,))
        return augmented
```"
142777,PyThaiNLP/pythainlp,PyThaiNLP_pythainlp/pythainlp/tokenize/core.py,pythainlp.tokenize.core.Tokenizer,"from typing import Iterable, List, Union
from pythainlp.tokenize import DEFAULT_SENT_TOKENIZE_ENGINE, DEFAULT_SUBWORD_TOKENIZE_ENGINE, DEFAULT_SYLLABLE_DICT_TRIE, DEFAULT_SYLLABLE_TOKENIZE_ENGINE, DEFAULT_WORD_DICT_TRIE, DEFAULT_WORD_TOKENIZE_ENGINE
from pythainlp.util.trie import Trie, dict_trie

class Tokenizer:
    """"""
    Tokenizer class for a custom tokenizer.

    This class allows users to pre-define custom dictionary along with
    tokenizer and encapsulate them into one single object.
    It is an wrapper for both functions, that are
    :func:`pythainlp.tokenize.word_tokenize`,
    and :func:`pythainlp.util.dict_trie`

    :Example:

    Tokenizer object instantiated with :class:`pythainlp.util.Trie`::

        from pythainlp.tokenize import Tokenizer
        from pythainlp.corpus.common import thai_words
        from pythainlp.util import dict_trie

        custom_words_list = set(thai_words())
        custom_words_list.add('อะเฟเซีย')
        custom_words_list.add('Aphasia')
        trie = dict_trie(dict_source=custom_words_list)

        text = ""อะเฟเซีย (Aphasia*) เป็นอาการผิดปกติของการพูด""
        _tokenizer = Tokenizer(custom_dict=trie, engine='newmm')
        _tokenizer.word_tokenize(text)
        # output: ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ',
        'ผิดปกติ', 'ของ', 'การ', 'พูด']

    Tokenizer object instantiated with a list of words::

        text = ""อะเฟเซีย (Aphasia) เป็นอาการผิดปกติของการพูด""
        _tokenizer = Tokenizer(custom_dict=list(thai_words()), engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะ', 'เฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ',
        #   'ผิดปกติ', 'ของ', 'การ', 'พูด']

    Tokenizer object instantiated with a file path containing a list of
    words separated with *newline* and explicitly setting a new tokenizer
    after initiation::

        PATH_TO_CUSTOM_DICTIONARY = './custom_dictionary.txtt'

        # write a file
        with open(PATH_TO_CUSTOM_DICTIONARY, 'w', encoding='utf-8') as f:
            f.write('อะเฟเซีย\\nAphasia\\nผิด\\nปกติ')

        text = ""อะเฟเซีย (Aphasia) เป็นอาการผิดปกติของการพูด""

        # initiate an object from file with `attacut` as tokenizer
        _tokenizer = Tokenizer(custom_dict=PATH_TO_CUSTOM_DICTIONARY, \\
            engine='attacut')

        _tokenizer.word_tokenize(text)
        # output:
        # ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ', 'ผิด',
        #   'ปกติ', 'ของ', 'การ', 'พูด']

        # change tokenizer to `newmm`
        _tokenizer.set_tokenizer_engine(engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็นอาการ', 'ผิด',
        #   'ปกติ', 'ของการพูด']
    """"""

    def __init__(self, custom_dict: Union[Trie, Iterable[str], str]=[], engine: str='newmm', keep_whitespace: bool=True, join_broken_num: bool=True):
        """"""
        Initialize tokenizer object.

        :param str custom_dict: a file path, a list of vocaburaies* to be
                    used to create a trie, or an instantiated
                    :class:`pythainlp.util.Trie` object.
        :param str engine: choose between different options of tokenizer engines
                            (i.e.  *newmm*, *mm*, *longest*, *deepcut*)
        :param bool keep_whitespace: True to keep whitespace, a common mark
                                     for end of phrase in Thai
        """"""
        self.__trie_dict = Trie([])
        if custom_dict:
            self.__trie_dict = dict_trie(custom_dict)
        else:
            self.__trie_dict = DEFAULT_WORD_DICT_TRIE
        self.__engine = engine
        if self.__engine not in ['newmm', 'mm', 'longest', 'deepcut']:
            raise NotImplementedError('\n                The Tokenizer class is not support %s for custom tokenizer\n                ' % self.__engine)
        self.__keep_whitespace = keep_whitespace
        self.__join_broken_num = join_broken_num

    def word_tokenize(self, text: str) -> List[str]:
        """"""
        Main tokenization function.

        :param str text: text to be tokenized
        :return: list of words, tokenized from the text
        :rtype: list[str]
        """"""
        return word_tokenize(text, custom_dict=self.__trie_dict, engine=self.__engine, keep_whitespace=self.__keep_whitespace, join_broken_num=self.__join_broken_num)

    def set_tokenize_engine(self, engine: str) -> None:
        """"""
        Set the tokenizer's engine.

        :param str engine: choose between different options of tokenizer engines
                           (i.e. *newmm*, *mm*, *longest*, *deepcut*)
        """"""
        self.__engine = engine","
class Tokenizer:
    '''
    Tokenizer class for a custom tokenizer.
    This class allows users to pre-define custom dictionary along with
    tokenizer and encapsulate them into one single object.
    It is an wrapper for both functions, that are
    :func:`pythainlp.tokenize.word_tokenize`,
    and :func:`pythainlp.util.dict_trie`
    :Example:
    Tokenizer object instantiated with :class:`pythainlp.util.Trie`::
        from pythainlp.tokenize import Tokenizer
        from pythainlp.corpus.common import thai_words
        from pythainlp.util import dict_trie
        custom_words_list = set(thai_words())
        custom_words_list.add('อะเฟเซีย')
        custom_words_list.add('Aphasia')
        trie = dict_trie(dict_source=custom_words_list)
        text = ""อะเฟเซีย (Aphasia*) เป็นอาการผิดปกติของการพูด""
        _tokenizer = Tokenizer(custom_dict=trie, engine='newmm')
        _tokenizer.word_tokenize(text)
        # output: ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ',
        'ผิดปกติ', 'ของ', 'การ', 'พูด']
    Tokenizer object instantiated with a list of words::
        text = ""อะเฟเซีย (Aphasia) เป็นอาการผิดปกติของการพูด""
        _tokenizer = Tokenizer(custom_dict=list(thai_words()), engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะ', 'เฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ',
        #   'ผิดปกติ', 'ของ', 'การ', 'พูด']
    Tokenizer object instantiated with a file path containing a list of
    words separated with *newline* and explicitly setting a new tokenizer
    after initiation::
        PATH_TO_CUSTOM_DICTIONARY = './custom_dictionary.txtt'
        # write a file
        with open(PATH_TO_CUSTOM_DICTIONARY, 'w', encoding='utf-8') as f:
            f.write('อะเฟเซีย\nAphasia\nผิด\nปกติ')
        text = ""อะเฟเซีย (Aphasia) เป็นอาการผิดปกติของการพูด""
        # initiate an object from file with `attacut` as tokenizer
        _tokenizer = Tokenizer(custom_dict=PATH_TO_CUSTOM_DICTIONARY, \
            engine='attacut')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ', 'ผิด',
        #   'ปกติ', 'ของ', 'การ', 'พูด']
        # change tokenizer to `newmm`
        _tokenizer.set_tokenizer_engine(engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็นอาการ', 'ผิด',
        #   'ปกติ', 'ของการพูด']
    '''
    def __init__(self, custom_dict: Union[Trie, Iterable[str], str]=[], engine: str='newmm', keep_whitespace: bool=True, join_broken_num: bool=True):
        '''
        Initialize tokenizer object.
        :param str custom_dict: a file path, a list of vocaburaies* to be
                    used to create a trie, or an instantiated
                    :class:`pythainlp.util.Trie` object.
        :param str engine: choose between different options of tokenizer engines
                            (i.e.  *newmm*, *mm*, *longest*, *deepcut*)
        :param bool keep_whitespace: True to keep whitespace, a common mark
                                     for end of phrase in Thai
        '''
        pass
    def word_tokenize(self, text: str) -> List[str]:
        
        pass
    def set_tokenize_engine(self, engine: str) -> None:
        '''
        Set the tokenizer's engine.
        :param str engine: choose between different options of tokenizer engines
                           (i.e. *newmm*, *mm*, *longest*, *deepcut*)
        '''
        pass",snippet_65,4,"```python
from typing import Union, Iterable, List, Optional
from pythainlp.tokenize import word_tokenize
from pythainlp.util import dict_trie, Trie


class Tokenizer:
    '''
    Tokenizer class for a custom tokenizer.
    This class allows users to pre-define custom dictionary along with
    tokenizer and encapsulate them into one single object.
    It is an wrapper for both functions, that are
    :func:`pythainlp.tokenize.word_tokenize`,
    and :func:`pythainlp.util.dict_trie`
    '''
    _VALID_ENGINES = {'newmm', 'mm', 'longest', 'deepcut'}

    def __init__(
        self,
        custom_dict: Union[Trie, Iterable[str], str] = [],
        engine: str = 'newmm',
        keep_whitespace: bool = True,
        join_broken_num: bool = True,
    ):
        '''
        Initialize tokenizer object.
        :param custom_dict: a file path, a list of vocabularies to be
                            used to create a trie, or an instantiated
                            :class:`pythainlp.util.Trie` object.
        :param engine: choose between different options of tokenizer engines
                       (i.e.  *newmm*, *mm*, *longest*, *deepcut*)
        :param keep_whitespace: True to keep whitespace, a common mark
                                for end of phrase in Thai
        '''
        if engine not in self._VALID_ENGINES:
            raise ValueError(f""Unsupported engine '{engine}'. ""
                             f""Valid options: {sorted(self._VALID_ENGINES)}"")
        self.engine = engine
        self.keep_whitespace = keep_whitespace
        self.join_broken_num = join_broken_num

        # Resolve custom_dict to a Trie or None
        if isinstance(custom_dict, Trie):
            self.custom_dict = custom_dict
        elif isinstance(custom_dict, str):
            # Treat as file path
            try:
                with open(custom_dict, 'r', encoding='utf-8') as f:
                    words = [line.strip() for line in f if line.strip()]
            except OSError as exc:
                raise ValueError(f""Could not read custom dictionary file: {exc}"") from exc
            self.custom_dict = dict_trie(dict_source=words)
        elif isinstance(custom_dict, Iterable):
            # Iterable of strings
            words = [w for w in custom_dict if isinstance(w, str) and w]
            self.custom_dict = dict_trie(dict_source=words) if words else None
        else:
            # Empty or unsupported type
            self.custom_dict = None

    def word_tokenize(self, text: str) -> List[str]:
        '''
        Tokenize the given text using the configured engine and custom dictionary.
        '''
        return word_tokenize(
            text,
            engine=self.engine,
            custom_dict=self.custom_dict,
            keep_whitespace=self.keep_whitespace,
            join_broken_num=self.join_broken_num,
        )

    def set_tokenize_engine(self, engine: str) -> None:
        '''
        Set the tokenizer's engine.
        :param engine: choose between different options of tokenizer engines
                       (i.e. *newmm*, *mm*, *longest*, *deepcut*)
        '''
        if engine not in self._VALID_ENGINES:
            raise ValueError(f""Unsupported engine '{engine}'. ""
                             f""Valid options: {sorted(self._VALID_ENGINES)}"")
        self.engine = engine
```"
145446,TomasTomecek/sen,TomasTomecek_sen/sen/tui/commands/base.py,sen.tui.commands.base.ArgumentProcessor,"class ArgumentProcessor:
    """"""
    responsible for parsing given list of arguments
    """"""

    def __init__(self, options, arguments):
        """"""
        :param options: list of options
        :param arguments: list of arguments
        """"""
        self.given_arguments = {}
        self.options = {}
        for a in options:
            self.options[a.name] = a
            self.given_arguments[normalize_arg_name(a.name)] = a.default
            for alias in a.aliases:
                self.options[alias] = a
        for o in arguments:
            self.given_arguments[normalize_arg_name(o.name)] = o.default
        self.arguments = arguments
        logger.info('arguments = %s', arguments)
        logger.info('options = %s', options)

    def process(self, argument_list):
        """"""
        :param argument_list: list of str, input from user
        :return: dict:
            {""cleaned_arg_name"": ""value""}
        """"""
        arg_index = 0
        for a in argument_list:
            opt_and_val = a.split('=', 1)
            opt_name = opt_and_val[0]
            try:
                argument = self.options[opt_name]
            except KeyError:
                try:
                    argument = self.arguments[arg_index]
                except IndexError:
                    logger.error('option/argument %r not specified', a)
                    raise NoSuchOptionOrArgument('No such option or argument: %r' % opt_name)
            logger.info('argument found: %s', argument)
            safe_arg_name = normalize_arg_name(argument.name)
            logger.info('argument is available under name %r', safe_arg_name)
            if isinstance(argument, Argument):
                arg_index += 1
                value = (a,)
            else:
                try:
                    value = (opt_and_val[1],)
                except IndexError:
                    value = tuple()
            arg_val = argument.action(*value)
            logger.info('argument %r has value %r', safe_arg_name, arg_val)
            self.given_arguments[safe_arg_name] = arg_val
        return self.given_arguments","class ArgumentProcessor:
    
    def __init__(self, options, arguments):
        
        pass
    def process(self, argument_list):
        '''
        :param argument_list: list of str, input from user
        :return: dict:
            {""cleaned_arg_name"": ""value""}
        '''
        pass",snippet_66,3,"```python
class ArgumentProcessor:
    def __init__(self, options, arguments):
        """"""
        :param options: list of option strings (e.g. ['-h', '--output'])
        :param arguments: list of positional argument names (e.g. ['input_file'])
        """"""
        # Map each option to a cleaned name (e.g. '--output' -> 'output')
        self.options_map = {opt: self._clean_name(opt) for opt in options}
        self.positional_names = arguments

    def _clean_name(self, name):
        """"""Remove leading dashes and replace remaining dashes with underscores.""""""
        return name.lstrip('-').replace('-', '_')

    def process(self, argument_list):
        """"""
        :param argument_list: list of str, input from user
        :return: dict: {""cleaned_arg_name"": ""value""}
        """"""
        result = {}

        # Initialize defaults for options
        for dest in self.options_map.values():
            result[dest] = None

        pos_index = 0
        i = 0
        while i < len(argument_list):
            token = argument_list[i]
            if token.startswith('-'):  # option
                if token in self.options_map:
                    dest = self.options_map[token]
                    # If next token exists and is not another option, treat it as value
                    if i + 1 < len(argument_list) and not argument_list[i + 1].startswith('-'):
                        result[dest] = argument_list[i + 1]
                        i += 2
                    else:
                        # flag option (boolean)
                        result[dest] = True
                        i += 1
                else:
                    # Unknown option: skip it
                    i += 1
            else:  # positional argument
                if pos_index < len(self.positional_names):
                    dest = self.positional_names[pos_index]
                    result[dest] = token
                    pos_index += 1
                i += 1

        return result
```"
152154,hellock/icrawler,hellock_icrawler/icrawler/storage/base.py,icrawler.storage.base.BaseStorage,"from abc import ABCMeta, abstractmethod

class BaseStorage:
    """"""Base class of backend storage""""""
    __metaclass__ = ABCMeta

    @abstractmethod
    def write(self, id, data):
        """"""Abstract interface of writing data

        Args:
            id (str): unique id of the data in the storage.
            data (bytes or str): data to be stored.
        """"""
        return

    @abstractmethod
    def exists(self, id):
        """"""Check the existence of some data

        Args:
            id (str): unique id of the data in the storage

        Returns:
            bool: whether the data exists
        """"""
        return False

    @abstractmethod
    def max_file_idx(self):
        """"""Get the max existing file index

        Returns:
            int: the max index
        """"""
        return 0","
class BaseStorage:
    '''Base class of backend storage'''
    @abstractmethod
    def write(self, id, data):
        
        pass
    @abstractmethod
    def exists(self, id):
        '''Check the existence of some data
        Args:
            id (str): unique id of the data in the storage
        Returns:
            bool: whether the data exists
        '''
        pass
    @abstractmethod
    def max_file_idx(self):
        
        pass",snippet_69,4,"```python
from abc import ABC, abstractmethod

class BaseStorage(ABC):
    '''Base class of backend storage'''

    @abstractmethod
    def write(self, id, data):
        '''Write data to storage'''
        pass

    @abstractmethod
    def exists(self, id):
        '''Check the existence of some data
        Args:
            id (str): unique id of the data in the storage
        Returns:
            bool: whether the data exists
        '''
        pass

    @abstractmethod
    def max_file_idx(self):
        '''Return the maximum file index in storage'''
        pass
```"
155190,sentinel-hub/sentinelhub-py,sentinelhub/download/rate_limit.py,sentinelhub.download.rate_limit.SentinelHubRateLimit,"import time

class SentinelHubRateLimit:
    """"""Class implementing rate limiting logic of Sentinel Hub service

    It has 2 public methods:

    - register_next - tells if next download can start or if not, what is the wait before it can be asked again
    - update - updates expectations according to headers obtained from download

    The rate limiting object is collecting information about the status of rate limiting policy buckets from
    Sentinel Hub service. According to this information and a feedback from download requests it adapts expectations
    about when the next download attempt will be possible.
    """"""
    RETRY_HEADER = 'Retry-After'
    UNITS_SPENT_HEADER = 'X-ProcessingUnits-Spent'

    def __init__(self, num_processes: int=1, minimum_wait_time: float=0.05, maximum_wait_time: float=60.0):
        """"""
        :param num_processes: Number of parallel download processes running.
        :param minimum_wait_time: Minimum wait time between two consecutive download requests in seconds.
        :param maximum_wait_time: Maximum wait time between two consecutive download requests in seconds.
        """"""
        self.wait_time = min(num_processes * minimum_wait_time, maximum_wait_time)
        self.next_download_time = time.monotonic()

    def register_next(self) -> float:
        """"""Determines if next download request can start or not by returning the waiting time in seconds.""""""
        current_time = time.monotonic()
        wait_time = max(self.next_download_time - current_time, 0)
        if wait_time == 0:
            self.next_download_time = max(current_time + self.wait_time, self.next_download_time)
        return wait_time

    def update(self, headers: dict, *, default: float) -> None:
        """"""Update the next possible download time if the service has responded with the rate limit.

        :param headers: The headers that (may) contain information about waiting times.
        :param default: The default waiting time (in milliseconds) when retrying after getting a
            TOO_MANY_REQUESTS response without appropriate retry headers.
        """"""
        retry_after: float = int(headers.get(self.RETRY_HEADER, default))
        retry_after = retry_after / 1000
        if retry_after:
            self.next_download_time = max(time.monotonic() + retry_after, self.next_download_time)","
class SentinelHubRateLimit:
    
    def __init__(self, num_processes: int=1, minimum_wait_time: float=0.05, maximum_wait_time: float=60.0):
        '''
        :param num_processes: Number of parallel download processes running.
        :param minimum_wait_time: Minimum wait time between two consecutive download requests in seconds.
        :param maximum_wait_time: Maximum wait time between two consecutive download requests in seconds.
        '''
        pass
    def register_next(self) -> float:
        
        pass
    def update(self, headers: dict, *, default: float) -> None:
        '''Update the next possible download time if the service has responded with the rate limit.
        :param headers: The headers that (may) contain information about waiting times.
        :param default: The default waiting time (in milliseconds) when retrying after getting a
            TOO_MANY_REQUESTS response without appropriate retry headers.
        '''
        pass",snippet_73,4,"```python
import time
from typing import Dict, Optional


class SentinelHubRateLimit:
    """"""
    Simple rate‑limit helper for Sentinel‑Hub requests.

    The class keeps track of the earliest time a new request may be sent.
    After each request the caller should call :meth:`update` with the
    response headers so that the next allowed time can be adjusted
    according to the server’s rate‑limit information.
    """"""

    def __init__(
        self,
        num_processes: int = 1,
        minimum_wait_time: float = 0.05,
        maximum_wait_time: float = 60.0,
    ):
        """"""
        :param num_processes: Number of parallel download processes running.
        :param minimum_wait_time: Minimum wait time between two consecutive download requests in seconds.
        :param maximum_wait_time: Maximum wait time between two consecutive download requests in seconds.
        """"""
        self.num_processes = max(1, int(num_processes))
        self.minimum_wait_time = float(minimum_wait_time)
        self.maximum_wait_time = float(maximum_wait_time)
        # The earliest timestamp (in seconds since epoch) at which a request may be sent.
        self._next_time: float = 0.0

    def register_next(self) -> float:
        """"""
        Return the number of seconds that must be waited before the next request
        can be sent.  The internal ``_next_time`` is updated to enforce the
        minimum wait time between requests.

        :return: Seconds to wait (0.0 if no wait is required).
        """"""
        now = time.time()
        wait = max(0.0, self._next_time - now)
        # Schedule the next request after the minimum wait time.
        self._next_time = now + self.minimum_wait_time
        return wait

    def _clamp_wait(self, wait: float) -> float:
        """"""Clamp the wait time between the configured min/max limits.""""""
        return max(self.minimum_wait_time, min(self.maximum_wait_time, wait))

    def _parse_retry_after(self, headers: Dict[str, str]) -> Optional[float]:
        """"""
        Parse the ``Retry-After`` header if present.
        The header may contain either a number of seconds or an HTTP date.
        """"""
        retry = headers.get(""Retry-After"")
        if retry is None:
            return None
        retry = retry.strip()
        # Try to interpret as seconds
        try:
            return float(retry)
        except ValueError:
            pass
        # Try to interpret as HTTP date
        try:
            from email.utils import parsedate_to_datetime

            dt = parsedate_to_datetime(retry)
            return max(0.0, (dt - time.time()).total_seconds())
        except Exception:
            return None

    def _parse_rate_limit_reset(self, headers: Dict[str, str]) -> Optional[float]:
        """"""
        Parse the ``X-RateLimit-Reset`` header if present.
        The header is expected to be an epoch timestamp in seconds.
        """"""
        reset = headers.get(""X-RateLimit-Reset"")
        if reset is None:
            return None
        try:
            return float(reset)
        except ValueError:
            return None

    def update(self, headers: Dict[str, str], *, default: float) -> None:
        """"""
        Update the next possible download time if the service has responded with the rate limit.

        :param headers: The headers that (may) contain information about waiting times.
        :param default: The default waiting time (in milliseconds) when retrying after getting a
            TOO_MANY_REQUESTS response without appropriate retry headers.
        """"""
        now = time.time()
        # 1. Check for Retry-After header
        retry_after = self._parse_retry_after(headers)
        if retry_after is not None:
            wait = retry_after
        else:
            # 2. Check for X-RateLimit-Reset header
            reset_time = self._parse_rate_limit_reset(headers)
            if reset_time is not None:
                wait = max(0.0, reset_time - now)
            else:
                # 3. Fallback to default (milliseconds -> seconds)
                wait = default / 1000.0

        # Clamp the wait time to the configured limits
        wait = self._clamp_wait(wait)

        # Update the next allowed time
        self._next_time = max(self._next_time, now + wait)
```"
156520,textX/textX,textx/scoping/providers.py,textx.scoping.providers.PlainName,"from textx.exceptions import TextXSemanticError

class PlainName:
    """"""
    plain name scope provider
    """"""

    def __init__(self, multi_metamodel_support=True):
        """"""
        the default scope provider constructor

        Args:
            multi_metamodel_support: enable a AST based search, instead
            of using the parser._instances
        """"""
        self.multi_metamodel_support = multi_metamodel_support
        pass

    def __call__(self, obj, attr, obj_ref):
        """"""
        the default scope provider

        Args:
            obj: unused (used for multi_metamodel_support)
            attr: unused
            obj_ref: the cross reference to be resolved

        Returns:
            the resolved reference or None
        """"""
        from textx.const import RULE_ABSTRACT, RULE_COMMON
        from textx.model import ObjCrossRef
        from textx.scoping.tools import get_parser
        if obj_ref is None:
            return None
        assert type(obj_ref) is ObjCrossRef, type(obj_ref)
        if get_parser(obj).debug:
            get_parser(obj).dprint(f'Resolving obj crossref: {obj_ref.cls}:{obj_ref.obj_name}')

        def _inner_resolve_link_rule_ref(cls, obj_name):
            """"""
            Depth-first resolving of link rule reference.
            """"""
            if cls._tx_type is RULE_ABSTRACT:
                for inherited in cls._tx_inh_by:
                    result = _inner_resolve_link_rule_ref(inherited, obj_name)
                    if result:
                        return result
            elif cls._tx_type == RULE_COMMON and id(cls) in get_parser(obj)._instances:
                objs = get_parser(obj)._instances[id(cls)]
                return objs.get(obj_name)
        if self.multi_metamodel_support:
            from textx import get_children, get_model, textx_isinstance
            result_lst = get_children(lambda x: hasattr(x, 'name') and x.name == obj_ref.obj_name and textx_isinstance(x, obj_ref.cls), get_model(obj))
            if len(result_lst) == 1:
                result = result_lst[0]
            elif len(result_lst) > 1:
                line, col = get_parser(obj).pos_to_linecol(obj_ref.position)
                raise TextXSemanticError(f'name {obj_ref.obj_name} is not unique.', line=line, col=col, filename=get_model(obj)._tx_filename)
            else:
                result = None
        else:
            result = _inner_resolve_link_rule_ref(obj_ref.cls, obj_ref.obj_name)
        return result","
class PlainName:
    
    def __init__(self, multi_metamodel_support=True):
        '''
        the default scope provider constructor
        Args:
            multi_metamodel_support: enable a AST based search, instead
            of using the parser._instances
        '''
        pass
    def __call__(self, obj, attr, obj_ref):
        '''
        the default scope provider
        Args:
            obj: unused (used for multi_metamodel_support)
            attr: unused
            obj_ref: the cross reference to be resolved
        Returns:
            the resolved reference or None
        '''
        pass
        def _inner_resolve_link_rule_ref(cls, obj_name):
            '''
            Depth-first resolving of link rule reference.
                '''
                pass",snippet_74,4,"```python
class PlainName:
    _registry = {}

    def __init__(self, multi_metamodel_support=True):
        """"""
        the default scope provider constructor
        Args:
            multi_metamodel_support: enable a AST based search, instead
            of using the parser._instances
        """"""
        self.multi_metamodel_support = multi_metamodel_support

    def __call__(self, obj, attr, obj_ref):
        """"""
        the default scope provider
        Args:
            obj: unused (used for multi_metamodel_support)
            attr: unused
            obj_ref: the cross reference to be resolved
        Returns:
            the resolved reference or None
        """"""
        # If the reference is already a resolved object, return it
        if isinstance(obj_ref, (str, int, float)):
            return obj_ref

        # If the reference is a dict with a 'name' key, return the name
        if isinstance(obj_ref, dict) and 'name' in obj_ref:
            return obj_ref['name']

        # If multi_metamodel_support is enabled, attempt a simple AST-like search
        if self.multi_metamodel_support:
            # Simulate an AST search by looking for a 'name' attribute
            if hasattr(obj_ref, 'name'):
                return getattr(obj_ref, 'name')
            # If obj_ref is a list or tuple, search recursively
            if isinstance(obj_ref, (list, tuple)):
                for item in obj_ref:
                    result = self.__call__(obj, attr, item)
                    if result is not None:
                        return result

        # Fallback: try to resolve via the registry
        if isinstance(obj_ref, str):
            return self._inner_resolve_link_rule_ref(self, obj_ref)

        return None

    def _inner_resolve_link_rule_ref(self, cls, obj_name):
        """"""
        Depth-first resolving of link rule reference.
        """"""
        # Simple depth-first search in the registry
        visited = set()

        def dfs(name):
            if name in visited:
                return None
            visited.add(name)
            entry = cls._registry.get(name)
            if entry is None:
                return None
            # If the entry is a reference to another name, resolve it
            if isinstance(entry, str) and entry != name:
                return dfs(entry)
            return entry

        return dfs(obj_name)
```"
156861,csparpa/pyowm,csparpa_pyowm/pyowm/utils/geo.py,pyowm.utils.geo.Geometry,"class Geometry:
    """"""
    Abstract parent class for geotypes

    """"""

    def geojson(self):
        """"""
        Returns a GeoJSON string representation of this geotype, compliant to
        RFC 7946 (https://tools.ietf.org/html/rfc7946)
        :return: str
        """"""
        raise NotImplementedError()

    def to_dict(self):
        """"""
        Returns a dict representation of this geotype
        :return: dict
        """"""
        raise NotImplementedError()","class Geometry:
    '''
    Abstract parent class for geotypes
        '''
    def geojson(self):
        
        pass
    def to_dict(self):
        '''
        Returns a dict representation of this geotype
        :return: dict
        '''
        pass",snippet_75,3,"```python
from abc import ABC, abstractmethod


class Geometry(ABC):
    """"""
    Abstract parent class for geotypes.
    """"""

    @abstractmethod
    def geojson(self):
        """"""
        Return a GeoJSON representation of the geometry.
        """"""
        pass

    @abstractmethod
    def to_dict(self):
        """"""
        Return a dictionary representation of this geotype.
        """"""
        pass
```"
157718,jwkvam/bowtie,bowtie/_cache.py,bowtie._cache._Cache,"from eventlet.queue import LightQueue
from flask_socketio import emit
import eventlet
import flask
import msgpack
from bowtie._component import pack

class _Cache:
    """"""Store data in the browser.

    This cache uses session storage so data will stay
    in the browser until the tab is closed.
    All data must be serializable, which means if the
    serialization transforms the data it won't be the same
    when it is fetched.

    Examples
    --------
    >>> from bowtie import cache
    >>> cache['a'] = True  # doctest: +SKIP
    >>> cache['a']  # doctest: +SKIP
    True
    >>> cache['b'] = np.arange(5)  # doctest: +SKIP
    >>> cache['b']  # doctest: +SKIP
    [1, 2, 3, 4, 5]

    """"""

    def __getitem__(self, key):
        """"""Load the value stored with the key.

        Parameters
        ----------
        key : str
            The key to lookup the value stored.

        Returns
        -------
        object
            The value if the key exists in the cache, otherwise None.

        """"""
        validate(key)
        signal = 'cache_load'
        event = LightQueue(1)
        if flask.has_request_context():
            emit(signal, {'data': pack(key)}, callback=event.put)
        else:
            sio = flask.current_app.extensions['socketio']
            sio.emit(signal, {'data': pack(key)}, callback=event.put)
        return msgpack.unpackb(bytes(event.get(timeout=10)), encoding='utf8')

    def __setitem__(self, key, value):
        """"""Store the key value pair.

        Parameters
        ----------
        key : str
            The key to determine where it's stored, you'll need this to load the value later.
        value : object
            The value to store in the cache.

        Returns
        -------
        None

        """"""
        validate(key)
        signal = 'cache_save'
        if flask.has_request_context():
            emit(signal, {'key': pack(key), 'data': pack(value)})
        else:
            sio = flask.current_app.extensions['socketio']
            sio.emit(signal, {'key': pack(key), 'data': pack(value)})
        eventlet.sleep()","
class _Cache:
    '''Store data in the browser.
    This cache uses session storage so data will stay
    in the browser until the tab is closed.
    All data must be serializable, which means if the
    serialization transforms the data it won't be the same
    when it is fetched.
    Examples
    --------
    >>> from bowtie import cache
    >>> cache['a'] = True  # doctest: +SKIP
    >>> cache['a']  # doctest: +SKIP
    True
    >>> cache['b'] = np.arange(5)  # doctest: +SKIP
    >>> cache['b']  # doctest: +SKIP
    [1, 2, 3, 4, 5]
    '''
    def __getitem__(self, key):
        '''Load the value stored with the key.
        Parameters
        ----------
        key : str
            The key to lookup the value stored.
        Returns
        -------
        object
            The value if the key exists in the cache, otherwise None.
        '''
        pass
    def __setitem__(self, key, value):
        
        pass",snippet_78,3,"```python
import json
try:
    # Pyodide / JupyterLite environment
    from js import sessionStorage
except Exception:
    # Fallback: no browser storage available
    sessionStorage = None


class _Cache:
    '''Store data in the browser.
    This cache uses session storage so data will stay
    in the browser until the tab is closed.
    All data must be serializable, which means if the
    serialization transforms the data it won't be the same
    when it is fetched.
    Examples
    --------
    >>> from bowtie import cache
    >>> cache['a'] = True  # doctest: +SKIP
    >>> cache['a']  # doctest: +SKIP
    True
    >>> cache['b'] = np.arange(5)  # doctest: +SKIP
    >>> cache['b']  # doctest: +SKIP
    [1, 2, 3, 4, 5]
    '''
    def __getitem__(self, key):
        '''Load the value stored with the key.
        Parameters
        ----------
        key : str
            The key to lookup the value stored.
        Returns
        -------
        object
            The value if the key exists in the cache, otherwise None.
        '''
        if sessionStorage is None:
            return None
        raw = sessionStorage.getItem(key)
        if raw is None:
            return None
        try:
            return json.loads(raw)
        except Exception:
            return None

    def __setitem__(self, key, value):
        if sessionStorage is None:
            return
        # Serialize value to JSON, handling numpy arrays and other non-JSON types
        def default(o):
            try:
                return o.tolist()
            except Exception:
                return str(o)
        try:
            raw = json.dumps(value, default=default)
            sessionStorage.setItem(key, raw)
        except Exception:
            # If serialization fails, do nothing
            pass
```"
159496,Erotemic/ubelt,Erotemic_ubelt/ubelt/util_mixins.py,ubelt.util_mixins.NiceRepr,"import warnings

class NiceRepr:
    """"""
    Inherit from this class and define ``__nice__`` to ""nicely"" print your
    objects.

    Defines ``__str__`` and ``__repr__`` in terms of ``__nice__`` function
    Classes that inherit from :class:`NiceRepr` should redefine ``__nice__``.
    If the inheriting class has a ``__len__``, method then the default
    ``__nice__`` method will return its length.

    Example:
        >>> import ubelt as ub
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        return 'info'
        >>> foo = Foo()
        >>> assert str(foo) == '<Foo(info)>'
        >>> assert repr(foo).startswith('<Foo(info) at ')

    Example:
        >>> import ubelt as ub
        >>> class Bar(ub.NiceRepr):
        ...    pass
        >>> bar = Bar()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     assert 'object at' in str(bar)
        >>>     assert 'object at' in repr(bar)

    Example:
        >>> import ubelt as ub
        >>> class Baz(ub.NiceRepr):
        ...    def __len__(self):
        ...        return 5
        >>> baz = Baz()
        >>> assert str(baz) == '<Baz(5)>'

    Example:
        >>> import ubelt as ub
        >>> # If your nice message has a bug, it shouldn't bring down the house
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        assert False
        >>> foo = Foo()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     print('foo = {!r}'.format(foo))
        foo = <...Foo ...>

    Example:
        >>> import ubelt as ub
        >>> class Animal(ub.NiceRepr):
        ...    def __init__(self):
        ...        ...
        ...    def __nice__(self):
        ...        return ''
        >>> class Cat(Animal):
        >>>     ...
        >>> class Dog(Animal):
        >>>     ...
        >>> class Beagle(Dog):
        >>>     ...
        >>> class Ragdoll(Cat):
        >>>     ...
        >>> instances = [Animal(), Cat(), Dog(), Beagle(), Ragdoll()]
        >>> for inst in instances:
        >>>     print(str(inst))
        <Animal()>
        <Cat()>
        <Dog()>
        <Beagle()>
        <Ragdoll()>

    In the case where you cant or dont want to use ubelt.NiceRepr you can get
    similar behavior by pasting the methods from the following snippet into
    your class:

    .. code:: python

        class MyClass:

            def __nice__(self):
                return 'your concise information'

            def __repr__(self):
                nice = self.__nice__()
                classname = self.__class__.__name__
                return '<{0}({1}) at {2}>'.format(classname, nice, hex(id(self)))

            def __str__(self):
                classname = self.__class__.__name__
                nice = self.__nice__()
                return '<{0}({1})>'.format(classname, nice)
    """"""

    def __nice__(self):
        """"""
        Returns:
            str
        """"""
        if hasattr(self, '__len__'):
            return str(len(self))
        else:
            raise NotImplementedError('Define the __nice__ method for {!r}'.format(self.__class__))

    def __repr__(self):
        """"""
        Returns:
            str
        """"""
        try:
            nice = self.__nice__()
            classname = self.__class__.__name__
            return '<{0}({1}) at {2}>'.format(classname, nice, hex(id(self)))
        except Exception as ex:
            warnings.warn(str(ex), category=RuntimeWarning)
            return object.__repr__(self)

    def __str__(self):
        """"""
        Returns:
            str
        """"""
        try:
            classname = self.__class__.__name__
            nice = self.__nice__()
            return '<{0}({1})>'.format(classname, nice)
        except Exception as ex:
            warnings.warn(str(ex), category=RuntimeWarning)
            return object.__repr__(self)","
class NiceRepr:
    '''
    Inherit from this class and define ``__nice__`` to ""nicely"" print your
    objects.
    Defines ``__str__`` and ``__repr__`` in terms of ``__nice__`` function
    Classes that inherit from :class:`NiceRepr` should redefine ``__nice__``.
    If the inheriting class has a ``__len__``, method then the default
    ``__nice__`` method will return its length.
    Example:
        >>> import ubelt as ub
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        return 'info'
        >>> foo = Foo()
        >>> assert str(foo) == '<Foo(info)>'
        >>> assert repr(foo).startswith('<Foo(info) at ')
    Example:
        >>> import ubelt as ub
        >>> class Bar(ub.NiceRepr):
        ...    pass
        >>> bar = Bar()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     assert 'object at' in str(bar)
        >>>     assert 'object at' in repr(bar)
    Example:
        >>> import ubelt as ub
        >>> class Baz(ub.NiceRepr):
        ...    def __len__(self):
        ...        return 5
        >>> baz = Baz()
        >>> assert str(baz) == '<Baz(5)>'
    Example:
        >>> import ubelt as ub
        >>> # If your nice message has a bug, it shouldn't bring down the house
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        assert False
        >>> foo = Foo()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     print('foo = {!r}'.format(foo))
        foo = <...Foo ...>
    Example:
        >>> import ubelt as ub
        >>> class Animal(ub.NiceRepr):
        ...    def __init__(self):
        ...        ...
        ...    def __nice__(self):
        ...        return ''
        >>> class Cat(Animal):
        >>>     ...
        >>> class Dog(Animal):
        >>>     ...
        >>> class Beagle(Dog):
        >>>     ...
        >>> class Ragdoll(Cat):
        >>>     ...
        >>> instances = [Animal(), Cat(), Dog(), Beagle(), Ragdoll()]
        >>> for inst in instances:
        >>>     print(str(inst))
        <Animal()>
        <Cat()>
        <Dog()>
        <Beagle()>
        <Ragdoll()>
    In the case where you cant or dont want to use ubelt.NiceRepr you can get
    similar behavior by pasting the methods from the following snippet into
    your class:
    .. code:: python
        class MyClass:
            def __nice__(self):
                return 'your concise information'
            def __repr__(self):
                nice = self.__nice__()
                classname = self.__class__.__name__
                return '<{0}({1}) at {2}>'.format(classname, nice, hex(id(self)))
            def __str__(self):
                classname = self.__class__.__name__
                nice = self.__nice__()
                return '<{0}({1})>'.format(classname, nice)
                    '''
            def __nice__(self):
                '''
        Returns:
            str
                        '''
                        pass
            def __repr__(self):
                '''
        Returns:
            str
                        '''
                        pass
            def __str__(self):
                '''
        Returns:
            str
                        '''
                        pass",snippet_80,4,"```python
import warnings


class NiceRepr:
    """"""
    Inherit from this class and define ``__nice__`` to ""nicely"" print your
    objects.
    Defines ``__str__`` and ``__repr__`` in terms of ``__nice__`` function.
    Classes that inherit from :class:`NiceRepr` should redefine ``__nice__``.
    If the inheriting class has a ``__len__`` method then the default
    ``__nice__`` method will return its length.
    """"""

    def __nice__(self):
        """"""
        Return a concise string representation of the object.
        Subclasses should override this method.  The default implementation
        returns the length of the object if it defines ``__len__``; otherwise
        it returns a warning string.
        """"""
        # Check if the subclass has overridden __nice__
        subclass_nice = type(self).__dict__.get('__nice__', None)
        if subclass_nice is not None and subclass_nice is not NiceRepr.__nice__:
            # The subclass has its own __nice__ implementation
            return subclass_nice(self)

        # Default behaviour
        if hasattr(self, '__len__'):
            try:
                return str(len(self))
            except Exception as e:
                warnings.warn(
                    f'NiceRepr: __len__ raised {e} for {type(self).__name__}',
                    RuntimeWarning,
                )
                return f'object at {hex(id(self))}'
        else:
            warnings.warn(
                f'NiceRepr: no __nice__ defined for {type(self).__name__}',
                RuntimeWarning,
            )
            return f'object at {hex(id(self))}'

    def __repr__(self):
        """"""
        Return a detailed representation of the object.
        """"""
        try:
            nice = self.__nice__()
        except Exception as e:
            warnings.warn(
                f'NiceRepr: __nice__ raised {e} for {type(self).__name__}',
                RuntimeWarning,
            )
            return f'<...{self.__class__.__name__} ...>'
        else:
            return (
                f'<{self.__class__.__name__}({nice}) at {hex(id(self))}>'
            )

    def __str__(self):
        """"""
        Return a concise representation of the object.
        """"""
        try:
            nice = self.__nice__()
        except Exception:
            return f'<...{self.__class__.__name__} ...>'
        else:
            return f'<{self.__class__.__name__}({nice})>'
```"
159497,Erotemic/ubelt,Erotemic_ubelt/ubelt/util_path.py,ubelt.util_path.ChDir,"import os

class ChDir:
    """"""
    Context manager that changes the current working directory and then
    returns you to where you were.

    This is nearly the same as the stdlib :func:`contextlib.chdir`, with the
    exception that it will do nothing if the input path is None (i.e. the user
    did not want to change directories).

    SeeAlso:
        :func:`contextlib.chdir`

    Example:
        >>> import ubelt as ub
        >>> dpath = ub.Path.appdir('ubelt/tests/chdir').ensuredir()
        >>> dir1 = (dpath / 'dir1').ensuredir()
        >>> dir2 = (dpath / 'dir2').ensuredir()
        >>> with ChDir(dpath):
        >>>     assert ub.Path.cwd() == dpath
        >>>     # change to the given directory, and then returns back
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(dir2):
        >>>             assert ub.Path.cwd() == dir2
        >>>             # changes inside the context manager will be reset
        >>>             os.chdir(dpath)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(None):
        >>>             assert ub.Path.cwd() == dir1
        >>>             # When disabled, the cwd does *not* reset at context exit
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir2
        >>>         os.chdir(dir1)
        >>>         # Dont change dirs, but reset to your cwd at context end
        >>>         with ChDir('.'):
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
    """"""

    def __init__(self, dpath):
        """"""
        Args:
            dpath (str | PathLike | None):
                The new directory to work in.
                If None, then the context manager is disabled.
        """"""
        self._context_dpath = dpath
        self._orig_dpath = None

    def __enter__(self):
        """"""
        Returns:
            ChDir: self
        """"""
        if self._context_dpath is not None:
            self._orig_dpath = os.getcwd()
            os.chdir(self._context_dpath)
        return self

    def __exit__(self, ex_type, ex_value, ex_traceback):
        """"""
        Args:
            ex_type (Type[BaseException] | None):
            ex_value (BaseException | None):
            ex_traceback (TracebackType | None):

        Returns:
            bool | None
        """"""
        if self._context_dpath is not None:
            os.chdir(self._orig_dpath)","
class ChDir:
    '''
    Context manager that changes the current working directory and then
    returns you to where you were.
    This is nearly the same as the stdlib :func:`contextlib.chdir`, with the
    exception that it will do nothing if the input path is None (i.e. the user
    did not want to change directories).
    SeeAlso:
        :func:`contextlib.chdir`
    Example:
        >>> import ubelt as ub
        >>> dpath = ub.Path.appdir('ubelt/tests/chdir').ensuredir()
        >>> dir1 = (dpath / 'dir1').ensuredir()
        >>> dir2 = (dpath / 'dir2').ensuredir()
        >>> with ChDir(dpath):
        >>>     assert ub.Path.cwd() == dpath
        >>>     # change to the given directory, and then returns back
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(dir2):
        >>>             assert ub.Path.cwd() == dir2
        >>>             # changes inside the context manager will be reset
        >>>             os.chdir(dpath)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(None):
        >>>             assert ub.Path.cwd() == dir1
        >>>             # When disabled, the cwd does *not* reset at context exit
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir2
        >>>         os.chdir(dir1)
        >>>         # Dont change dirs, but reset to your cwd at context end
        >>>         with ChDir('.'):
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
    '''
    def __init__(self, dpath):
        '''
        Args:
            dpath (str | PathLike | None):
                The new directory to work in.
                If None, then the context manager is disabled.
        '''
        pass
    def __enter__(self):
        '''
        Returns:
            ChDir: self
        '''
        pass
    def __exit__(self, ex_type, ex_value, ex_traceback):
        '''
        Args:
            ex_type (Type[BaseException] | None):
            ex_value (BaseException | None):
            ex_traceback (TracebackType | None):
        Returns:
            bool | None
        '''
        pass",snippet_81,4,
159980,pazz/alot,alot/completion/completer.py,alot.completion.completer.Completer,"import abc

class Completer:
    """"""base class for completers""""""
    __metaclass__ = abc.ABCMeta

    @abc.abstractmethod
    def complete(self, original, pos):
        """"""returns a list of completions and cursor positions for the string
        `original` from position `pos` on.

        :param original: the string to complete
        :type original: str
        :param pos: starting position to complete from
        :type pos: int
        :returns: pairs of completed string and cursor position in the
                  new string
        :rtype: list of (str, int)
        :raises: :exc:`CompletionError`
        """"""
        pass

    def relevant_part(self, original, pos):
        """"""
        Calculate the subword in a ' '-separated list of substrings of
        `original` that `pos` is in.
        """"""
        start = original.rfind(' ', 0, pos) + 1
        end = original.find(' ', pos - 1)
        if end == -1:
            end = len(original)
        return (original[start:end], start, end, pos - start)","
class Completer:
    
    @abc.abstractmethod
    def complete(self, original, pos):
        
        pass
    def relevant_part(self, original, pos):
        '''
        Calculate the subword in a ' '-separated list of substrings of
        `original` that `pos` is in.
        '''
        pass",snippet_82,3,"```python
import abc

class Completer(abc.ABC):
    @abc.abstractmethod
    def complete(self, original, pos):
        """"""
        Abstract method to be implemented by subclasses.
        Should return a list of completion candidates for the word
        at the given position `pos` in the string `original`.
        """"""
        pass

    def relevant_part(self, original, pos):
        """"""
        Return the subword in a space-separated list of substrings of
        `original` that contains the character at index `pos`.

        Parameters
        ----------
        original : str
            The original string containing space-separated words.
        pos : int
            The index (0‑based) of the character whose word should be
            extracted.

        Returns
        -------
        str
            The word that contains the character at `pos`. If `pos`
            is out of bounds or the character is a space, an empty
            string is returned.
        """"""
        if not isinstance(original, str):
            raise TypeError(""original must be a string"")
        if not isinstance(pos, int):
            raise TypeError(""pos must be an integer"")
        if pos < 0 or pos >= len(original):
            return """"

        # If the character at pos is a space, there's no word.
        if original[pos].isspace():
            return """"

        # Find the start of the word.
        start = pos
        while start > 0 and not original[start - 1].isspace():
            start -= 1

        # Find the end of the word.
        end = pos
        while end < len(original) - 1 and not original[end + 1].isspace():
            end += 1

        return original[start:end + 1]
```"
160363,splunk/splunk-sdk-python,splunk_splunk-sdk-python/splunklib/modularinput/argument.py,splunklib.modularinput.argument.Argument,"import xml.etree.ElementTree as ET

class Argument:
    """"""Class representing an argument to a modular input kind.

    ``Argument`` is meant to be used with ``Scheme`` to generate an XML
    definition of the modular input kind that Splunk understands.

    ``name`` is the only required parameter for the constructor.

        **Example with least parameters**::

            arg1 = Argument(name=""arg1"")

        **Example with all parameters**::

            arg2 = Argument(
                name=""arg2"",
                description=""This is an argument with lots of parameters"",
                validation=""is_pos_int('some_name')"",
                data_type=Argument.data_type_number,
                required_on_edit=True,
                required_on_create=True
            )
    """"""
    data_type_boolean = 'BOOLEAN'
    data_type_number = 'NUMBER'
    data_type_string = 'STRING'

    def __init__(self, name, description=None, validation=None, data_type=data_type_string, required_on_edit=False, required_on_create=False, title=None):
        """"""
        :param name: ``string``, identifier for this argument in Splunk.
        :param description: ``string``, human-readable description of the argument.
        :param validation: ``string`` specifying how the argument should be validated, if using internal validation.
               If using external validation, this will be ignored.
        :param data_type: ``string``, data type of this field; use the class constants.
               ""data_type_boolean"", ""data_type_number"", or ""data_type_string"".
        :param required_on_edit: ``Boolean``, whether this arg is required when editing an existing modular input of this kind.
        :param required_on_create: ``Boolean``, whether this arg is required when creating a modular input of this kind.
        :param title: ``String``, a human-readable title for the argument.
        """"""
        self.name = name
        self.description = description
        self.validation = validation
        self.data_type = data_type
        self.required_on_edit = required_on_edit
        self.required_on_create = required_on_create
        self.title = title

    def add_to_document(self, parent):
        """"""Adds an ``Argument`` object to this ElementTree document.

        Adds an <arg> subelement to the parent element, typically <args>
        and sets up its subelements with their respective text.

        :param parent: An ``ET.Element`` to be the parent of a new <arg> subelement
        :returns: An ``ET.Element`` object representing this argument.
        """"""
        arg = ET.SubElement(parent, 'arg')
        arg.set('name', self.name)
        if self.title is not None:
            ET.SubElement(arg, 'title').text = self.title
        if self.description is not None:
            ET.SubElement(arg, 'description').text = self.description
        if self.validation is not None:
            ET.SubElement(arg, 'validation').text = self.validation
        subelements = [('data_type', self.data_type), ('required_on_edit', self.required_on_edit), ('required_on_create', self.required_on_create)]
        for name, value in subelements:
            ET.SubElement(arg, name).text = str(value).lower()
        return arg","
class Argument:
    '''Class representing an argument to a modular input kind.
    ``Argument`` is meant to be used with ``Scheme`` to generate an XML
    definition of the modular input kind that Splunk understands.
    ``name`` is the only required parameter for the constructor.
        **Example with least parameters**::
            arg1 = Argument(name=""arg1"")
        **Example with all parameters**::
            arg2 = Argument(
                name=""arg2"",
                description=""This is an argument with lots of parameters"",
                validation=""is_pos_int('some_name')"",
                data_type=Argument.data_type_number,
                required_on_edit=True,
                required_on_create=True
            )
    '''
    def __init__(self, name, description=None, validation=None, data_type=data_type_string, required_on_edit=False, required_on_create=False, title=None):
        '''
        :param name: ``string``, identifier for this argument in Splunk.
        :param description: ``string``, human-readable description of the argument.
        :param validation: ``string`` specifying how the argument should be validated, if using internal validation.
               If using external validation, this will be ignored.
        :param data_type: ``string``, data type of this field; use the class constants.
               ""data_type_boolean"", ""data_type_number"", or ""data_type_string"".
        :param required_on_edit: ``Boolean``, whether this arg is required when editing an existing modular input of this kind.
        :param required_on_create: ``Boolean``, whether this arg is required when creating a modular input of this kind.
        :param title: ``String``, a human-readable title for the argument.
        '''
        pass
    def add_to_document(self, parent):
        '''Adds an ``Argument`` object to this ElementTree document.
        Adds an <arg> subelement to the parent element, typically <args>
        and sets up its subelements with their respective text.
        :param parent: An ``ET.Element`` to be the parent of a new <arg> subelement
        :returns: An ``ET.Element`` object representing this argument.
        '''
        pass",snippet_83,3,"```python
import xml.etree.ElementTree as ET


class Argument:
    """"""Class representing an argument to a modular input kind.

    ``Argument`` is meant to be used with ``Scheme`` to generate an XML
    definition of the modular input kind that Splunk understands.
    ``name`` is the only required parameter for the constructor.

    Example with least parameters::
        arg1 = Argument(name=""arg1"")

    Example with all parameters::
        arg2 = Argument(
            name=""arg2"",
            description=""This is an argument with lots of parameters"",
            validation=""is_pos_int('some_name')"",
            data_type=Argument.data_type_number,
            required_on_edit=True,
            required_on_create=True
        )
    """"""

    # Data type constants
    data_type_string = ""string""
    data_type_number = ""number""
    data_type_boolean = ""boolean""

    def __init__(
        self,
        name,
        description=None,
        validation=None,
        data_type=data_type_string,
        required_on_edit=False,
        required_on_create=False,
        title=None,
    ):
        """"""
        :param name: ``string``, identifier for this argument in Splunk.
        :param description: ``string``, human-readable description of the argument.
        :param validation: ``string`` specifying how the argument should be validated,
                           if using internal validation. If using external validation,
                           this will be ignored.
        :param data_type: ``string``, data type of this field; use the class constants.
                          ""data_type_boolean"", ""data_type_number"", or ""data_type_string"".
        :param required_on_edit: ``Boolean``, whether this arg is required when editing an
                                 existing modular input of this kind.
        :param required_on_create: ``Boolean``, whether this arg is required when creating a
                                   modular input of this kind.
        :param title: ``String``, a human-readable title for the argument.
        """"""
        self.name = name
        self.description = description
        self.validation = validation
        self.data_type = data_type
        self.required_on_edit = required_on_edit
        self.required_on_create = required_on_create
        self.title = title

    def add_to_document(self, parent):
        """"""Adds an ``Argument`` object to this ElementTree document.

        Adds an <arg> subelement to the parent element, typically <args>
        and sets up its subelements with their respective text.

        :param parent: An ``ET.Element`` to be the parent of a new <arg> subelement
        :returns: An ``ET.Element`` object representing this argument.
        """"""
        arg_el = ET.SubElement(parent, ""arg"")

        # Helper to add subelement if value is not None
        def _add_sub(name, value):
            if value is not None:
                sub = ET.SubElement(arg_el, name)
                sub.text = str(value)

        _add_sub(""name"", self.name)
        _add_sub(""title"", self.title)
        _add_sub(""description"", self.description)
        _add_sub(""data_type"", self.data_type)
        _add_sub(""validation"", self.validation)
        _add_sub(""required_on_edit"", ""true"" if self.required_on_edit else ""false"")
        _add_sub(""required_on_create"", ""true"" if self.required_on_create else ""false"")

        return arg_el
```"
160367,splunk/splunk-sdk-python,splunk_splunk-sdk-python/splunklib/modularinput/scheme.py,splunklib.modularinput.scheme.Scheme,"import xml.etree.ElementTree as ET

class Scheme:
    """"""Class representing the metadata for a modular input kind.

    A ``Scheme`` specifies a title, description, several options of how Splunk should run modular inputs of this
    kind, and a set of arguments which define a particular modular input's properties.

    The primary use of ``Scheme`` is to abstract away the construction of XML to feed to Splunk.
    """"""
    streaming_mode_simple = 'SIMPLE'
    streaming_mode_xml = 'XML'

    def __init__(self, title):
        """"""
        :param title: ``string`` identifier for this Scheme in Splunk.
        """"""
        self.title = title
        self.description = None
        self.use_external_validation = True
        self.use_single_instance = False
        self.streaming_mode = Scheme.streaming_mode_xml
        self.arguments = []

    def add_argument(self, arg):
        """"""Add the provided argument, ``arg``, to the ``self.arguments`` list.

        :param arg: An ``Argument`` object to add to ``self.arguments``.
        """"""
        self.arguments.append(arg)

    def to_xml(self):
        """"""Creates an ``ET.Element`` representing self, then returns it.

        :returns: an ``ET.Element`` representing this scheme.
        """"""
        root = ET.Element('scheme')
        ET.SubElement(root, 'title').text = self.title
        if self.description is not None:
            ET.SubElement(root, 'description').text = self.description
        subelements = [('use_external_validation', self.use_external_validation), ('use_single_instance', self.use_single_instance), ('streaming_mode', self.streaming_mode)]
        for name, value in subelements:
            ET.SubElement(root, name).text = str(value).lower()
        endpoint = ET.SubElement(root, 'endpoint')
        args = ET.SubElement(endpoint, 'args')
        for arg in self.arguments:
            arg.add_to_document(args)
        return root","
class Scheme:
    '''Class representing the metadata for a modular input kind.
    A ``Scheme`` specifies a title, description, several options of how Splunk should run modular inputs of this
    kind, and a set of arguments which define a particular modular input's properties.
    The primary use of ``Scheme`` is to abstract away the construction of XML to feed to Splunk.
    '''
    def __init__(self, title):
        
        pass
    def add_argument(self, arg):
        '''Add the provided argument, ``arg``, to the ``self.arguments`` list.
        :param arg: An ``Argument`` object to add to ``self.arguments``.
        '''
        pass
    def to_xml(self):
        '''Creates an ``ET.Element`` representing self, then returns it.
        :returns: an ``ET.Element`` representing this scheme.
        '''
        pass",snippet_84,4,
160991,ncclient/ncclient,ncclient_ncclient/ncclient/transport/session.py,ncclient.transport.session.SessionListener,"class SessionListener:
    """"""Base class for :class:`Session` listeners, which are notified when a new
    NETCONF message is received or an error occurs.

    .. note::
        Avoid time-intensive tasks in a callback's context.
    """"""

    def callback(self, root, raw):
        """"""Called when a new XML document is received. The *root* argument allows the callback to determine whether it wants to further process the document.

        Here, *root* is a tuple of *(tag, attributes)* where *tag* is the qualified name of the root element and *attributes* is a dictionary of its attributes (also qualified names).

        *raw* will contain the XML document as a string.
        """"""
        raise NotImplementedError

    def errback(self, ex):
        """"""Called when an error occurs.

        :type ex: :exc:`Exception`
        """"""
        raise NotImplementedError","class SessionListener:
    '''Base class for :class:`Session` listeners, which are notified when a new
    NETCONF message is received or an error occurs.
    .. note::
        Avoid time-intensive tasks in a callback's context.
    '''
    def callback(self, root, raw):
        
        pass
    def errback(self, ex):
        '''Called when an error occurs.
        :type ex: :exc:`Exception`
        '''
        pass",snippet_85,3,"```python
class SessionListener:
    '''Base class for :class:`Session` listeners, which are notified when a new
    NETCONF message is received or an error occurs.
    .. note::
        Avoid time-intensive tasks in a callback's context.
    '''
    def callback(self, root, raw):
        '''Called when a new NETCONF message is received.

        :param root: XML root element of the message.
        :param raw: Raw XML string of the message.
        '''
        raise NotImplementedError(""Subclasses must implement 'callback'"")

    def errback(self, ex):
        '''Called when an error occurs.

        :type ex: :exc:`Exception`
        '''
        raise NotImplementedError(""Subclasses must implement 'errback'"")
```"
163149,python-cmd2/cmd2,python-cmd2_cmd2/cmd2/argparse_custom.py,cmd2.argparse_custom.Cmd2AttributeWrapper,"from typing import TYPE_CHECKING, Any, ClassVar, NoReturn, Protocol, cast, runtime_checkable

class Cmd2AttributeWrapper:
    """"""Wraps a cmd2-specific attribute added to an argparse Namespace.

    This makes it easy to know which attributes in a Namespace are
    arguments from a parser and which were added by cmd2.
    """"""

    def __init__(self, attribute: Any) -> None:
        """"""Initialize Cmd2AttributeWrapper instances.""""""
        self.__attribute = attribute

    def get(self) -> Any:
        """"""Get the value of the attribute.""""""
        return self.__attribute

    def set(self, new_val: Any) -> None:
        """"""Set the value of the attribute.""""""
        self.__attribute = new_val","
class Cmd2AttributeWrapper:
    '''Wraps a cmd2-specific attribute added to an argparse Namespace.
    This makes it easy to know which attributes in a Namespace are
    arguments from a parser and which were added by cmd2.
    '''
    def __init__(self, attribute: Any) -> None:
        
        pass
    def get(self) -> Any:
        '''Get the value of the attribute.'''
        pass
    def set(self, new_val: Any) -> None:
        
        pass",snippet_86,4,"```python
from typing import Any

class Cmd2AttributeWrapper:
    '''Wraps a cmd2-specific attribute added to an argparse Namespace.
    This makes it easy to know which attributes in a Namespace are
    arguments from a parser and which were added by cmd2.
    '''
    def __init__(self, attribute: Any) -> None:
        self._attribute = attribute

    def get(self) -> Any:
        '''Get the value of the attribute.'''
        return self._attribute

    def set(self, new_val: Any) -> None:
        self._attribute = new_val
```"
163184,python-cmd2/cmd2,python-cmd2_cmd2/cmd2/utils.py,cmd2.utils.ContextFlag,"class ContextFlag:
    """"""A context manager which is also used as a boolean flag value within the default sigint handler.

    Its main use is as a flag to prevent the SIGINT handler in cmd2 from raising a KeyboardInterrupt
    while a critical code section has set the flag to True. Because signal handling is always done on the
    main thread, this class is not thread-safe since there is no need.
    """"""

    def __init__(self) -> None:
        """"""When this flag has a positive value, it is considered set. When it is 0, it is not set.

        It should never go below 0.
        """"""
        self.__count = 0

    def __bool__(self) -> bool:
        """"""Define the truth value of an object when it is used in a boolean context.""""""
        return self.__count > 0

    def __enter__(self) -> None:
        """"""When a with block is entered, the __enter__ method of the context manager is called.""""""
        self.__count += 1

    def __exit__(self, *args: object) -> None:
        """"""When the execution flow exits a with statement block this is called, regardless of whether an exception occurred.""""""
        self.__count -= 1
        if self.__count < 0:
            raise ValueError('count has gone below 0')","class ContextFlag:
    '''A context manager which is also used as a boolean flag value within the default sigint handler.
    Its main use is as a flag to prevent the SIGINT handler in cmd2 from raising a KeyboardInterrupt
    while a critical code section has set the flag to True. Because signal handling is always done on the
    main thread, this class is not thread-safe since there is no need.
        '''
    def __init__(self) -> None:
        
        pass
    def __bool__(self) -> bool:
        
        pass
    def __enter__(self) -> None:
        '''When a with block is entered, the __enter__ method of the context manager is called.'''
        pass
    def __exit__(self, *args: object) -> None:
        '''When the execution flow exits a with statement block this is called, regardless of whether an exception occurred.'''
        pass",snippet_87,5,"class ContextFlag:
    '''A context manager which is also used as a boolean flag value within the default sigint handler.
    Its main use is as a flag to prevent the SIGINT handler in cmd2 from raising a KeyboardInterrupt
    while a critical code section has set the flag to True. Because signal handling is always done on the
    main thread, this class is not thread-safe since there is no need.
        '''
    def __init__(self) -> None:
        self._flag = False

    def __bool__(self) -> bool:
        return self._flag

    def __enter__(self) -> None:
        '''When a with block is entered, the __enter__ method of the context manager is called.'''
        self._flag = True
        return None

    def __exit__(self, *args: object) -> None:
        '''When the execution flow exits a with statement block this is called, regardless of whether an exception occurred.'''
        self._flag = False
        return None"
166548,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/cache/cache.py,pypyr.cache.cache.Cache,"import threading
from pypyr.config import config

class Cache:
    """"""Thread-safe general purpose cache for objects.

    Add things to the cache by calling get(key, creator). If the requested key
    doesn't exist, will add the item to the cache for you.
    """"""

    def __init__(self):
        """"""Instantiate the cache.""""""
        self._lock = threading.Lock()
        self._cache = {}

    def clear(self):
        """"""Clear the cache of all objects.""""""
        with self._lock:
            self._cache.clear()

    def get(self, key, creator):
        """"""Get key from cache. If key not exist, call creator and cache result.

        Looks for key in cache and returns object for that key.

        If key is not found, call creator and save the result to cache for that
        key.

        Be warned that get happens under the context of a Lock. . . so if
        creator takes a long time you might well be blocking.

        If config no_cache is True, bypasses cache entirely - will call
        creator each time and also not save the result to cache.

        Args:
            key: key (unique id) of cached item
            creator: callable that will create cached object if key not found

        Returns:
            Cached item at key or the result of creator()
        """"""
        if config.no_cache:
            logger.debug('no cache mode enabled. creating `%s` sans cache', key)
            return creator()
        with self._lock:
            if key in self._cache:
                logger.debug('`%s` loading from cache', key)
                obj = self._cache[key]
            else:
                logger.debug('`%s` not found in cache. . . creating', key)
                obj = creator()
                self._cache[key] = obj
        return obj","
class Cache:
    '''Thread-safe general purpose cache for objects.
    Add things to the cache by calling get(key, creator). If the requested key
    doesn't exist, will add the item to the cache for you.
    '''
    def __init__(self):
        
        pass
    def clear(self):
        '''Clear the cache of all objects.'''
        pass
    def get(self, key, creator):
        '''Get key from cache. If key not exist, call creator and cache result.
        Looks for key in cache and returns object for that key.
        If key is not found, call creator and save the result to cache for that
        key.
        Be warned that get happens under the context of a Lock. . . so if
        creator takes a long time you might well be blocking.
        If config no_cache is True, bypasses cache entirely - will call
        creator each time and also not save the result to cache.
        Args:
            key: key (unique id) of cached item
            creator: callable that will create cached object if key not found
        Returns:
            Cached item at key or the result of creator()
        '''
        pass",snippet_88,4,"```python
import threading

class Cache:
    '''Thread-safe general purpose cache for objects.
    Add things to the cache by calling get(key, creator). If the requested key
    doesn't exist, will add the item to the cache for you.
    '''
    def __init__(self, no_cache=False):
        self._cache = {}
        self._lock = threading.Lock()
        self.no_cache = no_cache

    def clear(self):
        '''Clear the cache of all objects.'''
        with self._lock:
            self._cache.clear()

    def get(self, key, creator):
        '''Get key from cache. If key not exist, call creator and cache result.
        Looks for key in cache and returns object for that key.
        If key is not found, call creator and save the result to cache for that
        key.
        Be warned that get happens under the context of a Lock. . . so if
        creator takes a long time you might well be blocking.
        If config no_cache is True, bypasses cache entirely - will call
        creator each time and also not save the result to cache.
        Args:
            key: key (unique id) of cached item
            creator: callable that will create cached object if key not found
        Returns:
            Cached item at key or the result of creator()
        '''
        if self.no_cache:
            return creator()

        with self._lock:
            if key in self._cache:
                return self._cache[key]
            value = creator()
            self._cache[key] = value
            return value
```"
166549,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/cache/loadercache.py,pypyr.cache.loadercache.Loader,"from pypyr.pipedef import PipelineDefinition, PipelineInfo
from pypyr.cache.cache import Cache
from collections.abc import Mapping
from pypyr.errors import PipelineDefinitionError

class Loader:
    """"""A single pipeline loader & the cache for all pipelines it has loaded.

    It loads pipelines using the get_pipeline_definition you assign to the
    loader at initialization.

    Attributes:
        name (str): Absolute module name of loader.
    """"""
    __slots__ = ['name', '_get_pipeline_definition', '_pipeline_cache']

    def __init__(self, name, get_pipeline_definition):
        """"""Initialize the loader and its pipeline cache.

        The expected function signature is:
        get_pipeline_definition(name: str,
                                parent: any) -> PipelineDefinition | Mapping

        Args:
            name: Absolute name of loader
            get_pipeline_definition: Reference to the function to call when
                loading a pipeline with this Loader.
        """"""
        self.name = name
        self._get_pipeline_definition = get_pipeline_definition
        self._pipeline_cache = Cache()

    def clear(self):
        """"""Clear all the pipelines in this Loader's cache.""""""
        self._pipeline_cache.clear()

    def get_pipeline(self, name, parent):
        """"""Get cached PipelineDefinition. Adds it to cache if it doesn't exist.

        The cache is local to this Loader instance.

        The combination of parent+name must be unique for this Loader. Parent
        should therefore have a sensible __str__ implementation because it
        forms part of the pipeline's identifying str key in the cache.

        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.

        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        """"""
        normalized_name = f'{parent}+{name}' if parent else name
        return self._pipeline_cache.get(normalized_name, lambda: self._load_pipeline(name, parent))

    def _load_pipeline(self, name, parent):
        """"""Execute get_pipeline_definition(name, parent) for this loader.

        If the loader get_pipeline_definition does not return a
        PipelineDefinition, this method will wrap the payload inside a
        PipelineDefinition for you.

        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.

        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        """"""
        logger.debug('starting')
        logger.debug('loading the pipeline definition with %s', self.name)
        pipeline_definition = self._get_pipeline_definition(pipeline_name=name, parent=parent)
        if not isinstance(pipeline_definition, PipelineDefinition):
            pipeline_definition = PipelineDefinition(pipeline=pipeline_definition, info=PipelineInfo(pipeline_name=name, loader=self.name, parent=parent))
        if not isinstance(pipeline_definition.pipeline, Mapping):
            raise PipelineDefinitionError(""A pipeline must be a mapping at the top level. Does your top-level yaml have a 'steps:' key? For example:\n\nsteps:\n  - name: pypyr.steps.echo\n    in:\n      echoMe: this is a bare bones pipeline example.\n"")
        logger.debug('done')
        return pipeline_definition","
class Loader:
    '''A single pipeline loader & the cache for all pipelines it has loaded.
    It loads pipelines using the get_pipeline_definition you assign to the
    loader at initialization.
    Attributes:
        name (str): Absolute module name of loader.
    '''
    def __init__(self, name, get_pipeline_definition):
        '''Initialize the loader and its pipeline cache.
        The expected function signature is:
        get_pipeline_definition(name: str,
                                parent: any) -> PipelineDefinition | Mapping
        Args:
            name: Absolute name of loader
            get_pipeline_definition: Reference to the function to call when
                loading a pipeline with this Loader.
        '''
        pass
    def clear(self):
        '''Clear all the pipelines in this Loader's cache.'''
        pass
    def get_pipeline(self, name, parent):
        '''Get cached PipelineDefinition. Adds it to cache if it doesn't exist.
        The cache is local to this Loader instance.
        The combination of parent+name must be unique for this Loader. Parent
        should therefore have a sensible __str__ implementation because it
        forms part of the pipeline's identifying str key in the cache.
        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.
        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        '''
        pass
    def _load_pipeline(self, name, parent):
        '''Execute get_pipeline_definition(name, parent) for this loader.
        If the loader get_pipeline_definition does not return a
        PipelineDefinition, this method will wrap the payload inside a
        PipelineDefinition for you.
        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.
        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        '''
        pass",snippet_89,5,"```python
from typing import Mapping, Any
from pypyr.pipedef import PipelineDefinition


class Loader:
    '''A single pipeline loader & the cache for all pipelines it has loaded.
    It loads pipelines using the get_pipeline_definition you assign to the
    loader at initialization.
    Attributes:
        name (str): Absolute module name of loader.
    '''
    def __init__(self, name: str, get_pipeline_definition):
        '''Initialize the loader and its pipeline cache.
        The expected function signature is:
        get_pipeline_definition(name: str,
                                parent: any) -> PipelineDefinition | Mapping
        Args:
            name: Absolute name of loader
            get_pipeline_definition: Reference to the function to call when
                loading a pipeline with this Loader.
        '''
        self.name = name
        self.get_pipeline_definition = get_pipeline_definition
        self._cache = {}

    def clear(self):
        '''Clear all the pipelines in this Loader's cache.'''
        self._cache.clear()

    def get_pipeline(self, name: str, parent: Any) -> PipelineDefinition:
        '''Get cached PipelineDefinition. Adds it to cache if it doesn't exist.
        The cache is local to this Loader instance.
        The combination of parent+name must be unique for this Loader. Parent
        should therefore have a sensible __str__ implementation because it
        forms part of the pipeline's identifying str key in the cache.
        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.
        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        '''
        key = (str(parent), name)
        if key not in self._cache:
            self._cache[key] = self._load_pipeline(name, parent)
        return self._cache[key]

    def _load_pipeline(self, name: str, parent: Any) -> PipelineDefinition:
        '''Execute get_pipeline_definition(name, parent) for this loader.
        If the loader get_pipeline_definition does not return a
        PipelineDefinition, this method will wrap the payload inside a
        PipelineDefinition for you.
        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.
        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        '''
        result = self.get_pipeline_definition(name, parent)
        if isinstance(result, PipelineDefinition):
            return result
        if isinstance(result, Mapping):
            return PipelineDefinition(name, result, loader=self)
        # If result is None or unexpected type, raise an error
        raise TypeError(
            f'get_pipeline_definition returned unsupported type {type(result)}'
        )
```"
166587,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/pipedef.py,pypyr.pipedef.PipelineDefinition,"class PipelineDefinition:
    """"""The pipeline body and its metadata.

    A loader creates the PipelineDefinition and sets the metadata in .info.

    The PipelineDefinition is a globally shared cache of the pipeline body &
    meta-data.

    Attributes:
        pipeline (dict-like): The pipeline yaml body.
        info (PipelineInfo): Meta-data set by the loader for the pipeline.
    """"""
    __slots__ = ['pipeline', 'info']

    def __init__(self, pipeline, info):
        """"""Initialize a pipeline definition.

        Args:
            pipeline (dict-like): The pipeline yaml body itself.
            info (PipelineInfo): Meta-data set by the loader for the pipeline.
        """"""
        self.pipeline = pipeline
        self.info = info

    def __eq__(self, other):
        """"""Equality comparison checks Pipeline and info objects are equal.""""""
        type_self = type(self)
        if type_self is type(other):
            all_slots = [p for c in type_self.__mro__ for p in getattr(c, '__slots__', [])]
            return all((getattr(self, s, id(self)) == getattr(other, s, id(other)) for s in all_slots))
        else:
            return False","class PipelineDefinition:
    '''The pipeline body and its metadata.
    A loader creates the PipelineDefinition and sets the metadata in .info.
    The PipelineDefinition is a globally shared cache of the pipeline body &
    meta-data.
    Attributes:
        pipeline (dict-like): The pipeline yaml body.
        info (PipelineInfo): Meta-data set by the loader for the pipeline.
    '''
    def __init__(self, pipeline, info):
        '''Initialize a pipeline definition.
        Args:
            pipeline (dict-like): The pipeline yaml body itself.
            info (PipelineInfo): Meta-data set by the loader for the pipeline.
        '''
        pass
    def __eq__(self, other):
        '''Equality comparison checks Pipeline and info objects are equal.'''
        pass",snippet_92,3,"```python
class PipelineDefinition:
    '''The pipeline body and its metadata.
    A loader creates the PipelineDefinition and sets the metadata in .info.
    The PipelineDefinition is a globally shared cache of the pipeline body &
    meta-data.
    Attributes:
        pipeline (dict-like): The pipeline yaml body.
        info (PipelineInfo): Meta-data set by the loader for the pipeline.
    '''
    def __init__(self, pipeline, info):
        '''Initialize a pipeline definition.
        Args:
            pipeline (dict-like): The pipeline yaml body itself.
            info (PipelineInfo): Meta-data set by the loader for the pipeline.
        '''
        self.pipeline = pipeline
        self.info = info

    def __eq__(self, other):
        '''Equality comparison checks Pipeline and info objects are equal.'''
        if not isinstance(other, PipelineDefinition):
            return False
        return self.pipeline == other.pipeline and self.info == other.info
```"
166589,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/pipedef.py,pypyr.pipedef.PipelineInfo,"class PipelineInfo:
    """"""The common attributes that every pipeline loader should set.

    Custom loaders that want to add more properties to a pipeline's meta-data
    should probably derive from this class.

    Attributes:
        pipeline_name (str): Name of pipeline, as set by the loader.
        loader (str): Absolute module name of the pipeline loader.
        parent (any): pipeline_name resolves from parent. The parent can be any
            type - it is up to the loader to interpret the parent property.
        is_loader_cascading (bool): Loader cascades to child pipelines if not
            otherwise set on pype. Default True.
        is_parent_cascading (bool): Parent cascades to child pipelines if not
            otherwise set on pype. Default True.
    """"""
    __slots__ = ['pipeline_name', 'loader', 'parent', 'is_loader_cascading', 'is_parent_cascading']

    def __init__(self, pipeline_name, loader, parent, is_parent_cascading=True, is_loader_cascading=True):
        """"""Initialize PipelineInfo.

        Args:
            pipeline_name (str): name of pipeline, as set by the loader.
            loader (str): absolute module name of pypeloader.
            parent (any): pipeline_name resolves from parent.
            is_loader_cascading (bool): Loader cascades to child pipelines if
                not otherwise set on pype. Default True.
            is_parent_cascading (bool): Parent cascades to child pipelines if
                not otherwise set on pype. Default True.
        """"""
        self.pipeline_name = pipeline_name
        self.loader = loader
        self.parent = parent
        self.is_loader_cascading = is_loader_cascading
        self.is_parent_cascading = is_parent_cascading

    def __eq__(self, other):
        """"""Check all instance attributes are equal.""""""
        type_self = type(self)
        if type_self is type(other):
            all_slots = [p for c in type_self.__mro__ for p in getattr(c, '__slots__', [])]
            return all((getattr(self, s, id(self)) == getattr(other, s, id(other)) for s in all_slots))
        else:
            return False","class PipelineInfo:
    '''The common attributes that every pipeline loader should set.
    Custom loaders that want to add more properties to a pipeline's meta-data
    should probably derive from this class.
    Attributes:
        pipeline_name (str): Name of pipeline, as set by the loader.
        loader (str): Absolute module name of the pipeline loader.
        parent (any): pipeline_name resolves from parent. The parent can be any
            type - it is up to the loader to interpret the parent property.
        is_loader_cascading (bool): Loader cascades to child pipelines if not
            otherwise set on pype. Default True.
        is_parent_cascading (bool): Parent cascades to child pipelines if not
            otherwise set on pype. Default True.
    '''
    def __init__(self, pipeline_name, loader, parent, is_parent_cascading=True, is_loader_cascading=True):
        
        pass
    def __eq__(self, other):
        
        pass",snippet_93,3,"class PipelineInfo:
    '''The common attributes that every pipeline loader should set.
    Custom loaders that want to add more properties to a pipeline's meta-data
    should probably derive from this class.
    Attributes:
        pipeline_name (str): Name of pipeline, as set by the loader.
        loader (str): Absolute module name of the pipeline loader.
        parent (any): pipeline_name resolves from parent. The parent can be any
            type - it is up to the loader to interpret the parent property.
        is_loader_cascading (bool): Loader cascades to child pipelines if not
            otherwise set on pype. Default True.
        is_parent_cascading (bool): Parent cascades to child pipelines if not
            otherwise set on pype. Default True.
    '''
    def __init__(self, pipeline_name, loader, parent, is_parent_cascading=True, is_loader_cascading=True):
        self.pipeline_name = pipeline_name
        self.loader = loader
        self.parent = parent
        self.is_parent_cascading = is_parent_cascading
        self.is_loader_cascading = is_loader_cascading

    def __eq__(self, other):
        if not isinstance(other, PipelineInfo):
            return False
        return (
            self.pipeline_name == other.pipeline_name and
            self.loader == other.loader and
            self.parent == other.parent and
            self.is_parent_cascading == other.is_parent_cascading and
            self.is_loader_cascading == other.is_loader_cascading
        )"
166604,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/steps/dsl/cmd.py,pypyr.steps.dsl.cmd.CmdStep,"import pypyr.utils.types
from pypyr.errors import ContextError, KeyInContextHasNoValueError, KeyNotInContextError
from collections.abc import Mapping, Sequence
from pypyr.context import Context
import logging
from pypyr.subproc import Command, SimpleCommandTypes

class CmdStep:
    """"""A pypyr step to run an executable or command as a subprocess.

    This models a step that takes config like this:
        cmd: <<cmd string>>

    OR, expanded syntax is as a dict
        cmd:
            run: str. mandatory. command + args to execute.
            save: bool. defaults False. save output to cmdOut. Treats output
                as text in the system's encoding and removes newlines at end.
            cwd: str/Pathlike. optional. Working directory for this command.
            bytes (bool): Default False. When `save` return output bytes from
                cmd unaltered, without applying any encoding & text newline
                processing.
            encoding (str): Default None. When `save`, decode cmd output with
                this encoding. The default of None uses the system encoding and
                should ""just work"".
            stdout (str | Path): Default None. Write stdout to this file path.
                Special value `/dev/null` writes to the system null device.
            stderr (str | Path): Default None. Write stderr to this file path.
                Special value `/dev/null` writes to the system null device.
                Special value `/dev/stdout` redirects err output to stdout.
            append (bool): Default False. When stdout/stderr a file, append
                rather than overwrite. Default is to overwrite.

    In expanded syntax, `run` can be a simple string or a list:
        cmd:
          run:
            - my-executable --arg
            - cmd here
          save: False
          cwd: ./path/here

    OR, as a list in simplified syntax:
        cmd:
          - my-executable --arg
          - ./another-executable --arg

    Any or all of the list items can use expanded syntax:
        cmd:
          - ./simple-cmd-here --arg1 value
          - run: cmd here
            save: False
            cwd: ./path/here
          - run:
              - my-executable --arg
              - ./another-executable --arg
            save: True cwd: ./path/here

    If save is True, will save the output to context as follows:
        cmdOut:
            returncode: 0
            stdout: 'stdout str here. None if empty.'
            stderr: 'stderr str here. None if empty.'

    If the cmd input contains a list of executables, cmdOut will be a list of
    cmdOut objects, in order executed.

    cmdOut.returncode is the exit status of the called process. Typically 0
    means OK. A negative value -N indicates that the child was terminated by
    signal N (POSIX only).

    The run_step method does the actual work. init parses the input yaml.

    Attributes:
        logger (logger): Logger instantiated by name of calling step.
        context: (pypyr.context.Context): The current pypyr Context.
        commands (list[pypyr.subproc.Command]): Commands to run as subprocess.
        is_shell (bool): True if subprocess should run through default shell.
        name (str): Name of calling step. Used for logging output & error
            messages.
    """"""

    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        """"""Initialize the CmdStep.

        The step config in the context dict in simplified syntax:
            cmd: <<cmd string>>

        OR, as a dict in expanded syntax:
            cmd:
                run: str. mandatory. command + args to execute.
                save: bool. optional. defaults False. save output to cmdOut.
                cwd: str/path. optional. if specified, change the working
                     directory just for the duration of the command.

        `run` can be a single string, or it can be a list of string if there
        are multiple commands to execute with the same settings.

        OR, as a list:
            cmd:
                - my-executable --arg
                - ./another-executable --arg

        Any or all of the list items can be in expanded syntax.

        Args:
            name (str): Unique name for step. Likely __name__ of calling step.
            context (pypyr.context.Context): Look for step config in this
                context instance.
            is_shell (bool): Set to true to execute cmd through the default
                shell.
        """"""
        assert name, 'name parameter must exist for CmdStep.'
        assert context, 'context param must exist for CmdStep.'
        self.name = name
        self.logger = logging.getLogger(name)
        context.assert_key_has_value(key='cmd', caller=name)
        self.context = context
        self.is_shell = is_shell
        cmd_config = context.get_formatted('cmd')
        commands: list[Command] = []
        if isinstance(cmd_config, SimpleCommandTypes):
            commands.append(Command(cmd_config, is_shell=is_shell))
        elif isinstance(cmd_config, Mapping):
            commands.append(self.create_command(cmd_config))
        elif isinstance(cmd_config, Sequence):
            for cmd in cmd_config:
                if isinstance(cmd, SimpleCommandTypes):
                    commands.append(Command(cmd, is_shell=is_shell))
                elif isinstance(cmd, Mapping):
                    commands.append(self.create_command(cmd))
                else:
                    raise ContextError(f'{cmd} in {name} cmd config is wrong.\nEach list item should be either a simple string or a dict for expanded syntax:\ncmd:\n  - my-executable --arg\n  - run: another-executable --arg value\n    cwd: ../mydir/subdir\n  - run:\n      - arb-executable1 --arg value1\n      - arb-executable2 --arg value2\n    cwd: ../mydir/arbdir')
        else:
            raise ContextError(f'{name} cmd config should be either a simple string:\ncmd: my-executable --arg\n\nor a dictionary:\ncmd:\n  run: subdir/my-executable --arg\n  cwd: ./mydir\n\nor a list of commands:\ncmd:\n  - my-executable --arg\n  - run: another-executable --arg value\n    cwd: ../mydir/subdir')
        self.commands: list[Command] = commands

    def create_command(self, cmd_input: Mapping) -> Command:
        """"""Create a pypyr.subproc.Command object from expanded step input.""""""
        try:
            cmd = cmd_input['run']
            if not cmd:
                raise KeyInContextHasNoValueError(f'cmd.run must have a value for {self.name}.\nThe `run` input should look something like this:\ncmd:\n  run: my-executable-here --arg1\n  cwd: ./mydir/subdir\n\nOr, `run` could be a list of commands:\ncmd:\n  run:\n    - arb-executable1 --arg value1\n    - arb-executable2 --arg value2\n  cwd: ../mydir/arbdir')
        except KeyError as err:
            raise KeyNotInContextError(f""cmd.run doesn't exist for {self.name}.\nThe input should look like this in the simplified syntax:\ncmd: my-executable-here --arg1\n\nOr in the expanded syntax:\ncmd:\n  run: my-executable-here --arg1\n\nIf you're passing in a list of commands, each command should be a simple string,\nor a dict with a `run` entry:\ncmd:\n  - my-executable --arg\n  - run: another-executable --arg value\n    cwd: ../mydir/subdir\n  - run:\n      - arb-executable1 --arg value1\n      - arb-executable2 --arg value2\n    cwd: ../mydir/arbdir"") from err
        is_save = pypyr.utils.types.cast_to_bool(cmd_input.get('save', False))
        cwd = cmd_input.get('cwd')
        is_bytes = cmd_input.get('bytes')
        is_text = not is_bytes if is_save else False
        stdout = cmd_input.get('stdout')
        stderr = cmd_input.get('stderr')
        if is_save:
            if stderr or stderr:
                raise ContextError(""You can't set `stdout` or `stderr` when `save` is True."")
        encoding = cmd_input.get('encoding')
        append = cmd_input.get('append', False)
        is_shell_override = cmd_input.get('shell', None)
        is_shell = self.is_shell if is_shell_override is None else is_shell_override
        return Command(cmd=cmd, is_shell=is_shell, cwd=cwd, is_save=is_save, is_text=is_text, stdout=stdout, stderr=stderr, encoding=encoding, append=append)

    def run_step(self) -> None:
        """"""Spawn a subprocess to run the command or program.

        If cmd.is_save==True, save result of each command to context 'cmdOut'.
        """"""
        results = []
        try:
            for cmd in self.commands:
                try:
                    cmd.run()
                finally:
                    if cmd.results:
                        results.extend(cmd.results)
        finally:
            if results:
                if len(results) == 1:
                    self.context['cmdOut'] = results[0]
                else:
                    self.context['cmdOut'] = results","
class CmdStep:
    '''A pypyr step to run an executable or command as a subprocess.
    This models a step that takes config like this:
        cmd: <<cmd string>>
    OR, expanded syntax is as a dict
        cmd:
            run: str. mandatory. command + args to execute.
            save: bool. defaults False. save output to cmdOut. Treats output
                as text in the system's encoding and removes newlines at end.
            cwd: str/Pathlike. optional. Working directory for this command.
            bytes (bool): Default False. When `save` return output bytes from
                cmd unaltered, without applying any encoding & text newline
                processing.
            encoding (str): Default None. When `save`, decode cmd output with
                this encoding. The default of None uses the system encoding and
                should ""just work"".
            stdout (str | Path): Default None. Write stdout to this file path.
                Special value `/dev/null` writes to the system null device.
            stderr (str | Path): Default None. Write stderr to this file path.
                Special value `/dev/null` writes to the system null device.
                Special value `/dev/stdout` redirects err output to stdout.
            append (bool): Default False. When stdout/stderr a file, append
                rather than overwrite. Default is to overwrite.
    In expanded syntax, `run` can be a simple string or a list:
        cmd:
          run:
            - my-executable --arg
            - cmd here
          save: False
          cwd: ./path/here
    OR, as a list in simplified syntax:
        cmd:
          - my-executable --arg
          - ./another-executable --arg
    Any or all of the list items can use expanded syntax:
        cmd:
          - ./simple-cmd-here --arg1 value
          - run: cmd here
            save: False
            cwd: ./path/here
          - run:
              - my-executable --arg
              - ./another-executable --arg
            save: True cwd: ./path/here
    If save is True, will save the output to context as follows:
        cmdOut:
            returncode: 0
            stdout: 'stdout str here. None if empty.'
            stderr: 'stderr str here. None if empty.'
    If the cmd input contains a list of executables, cmdOut will be a list of
    cmdOut objects, in order executed.
    cmdOut.returncode is the exit status of the called process. Typically 0
    means OK. A negative value -N indicates that the child was terminated by
    signal N (POSIX only).
    The run_step method does the actual work. init parses the input yaml.
    Attributes:
        logger (logger): Logger instantiated by name of calling step.
        context: (pypyr.context.Context): The current pypyr Context.
        commands (list[pypyr.subproc.Command]): Commands to run as subprocess.
        is_shell (bool): True if subprocess should run through default shell.
        name (str): Name of calling step. Used for logging output & error
            messages.
    '''
    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        '''Initialize the CmdStep.
        The step config in the context dict in simplified syntax:
            cmd: <<cmd string>>
        OR, as a dict in expanded syntax:
            cmd:
                run: str. mandatory. command + args to execute.
                save: bool. optional. defaults False. save output to cmdOut.
                cwd: str/path. optional. if specified, change the working
                     directory just for the duration of the command.
        `run` can be a single string, or it can be a list of string if there
        are multiple commands to execute with the same settings.
        OR, as a list:
            cmd:
                - my-executable --arg
                - ./another-executable --arg
        Any or all of the list items can be in expanded syntax.
        Args:
            name (str): Unique name for step. Likely __name__ of calling step.
            context (pypyr.context.Context): Look for step config in this
                context instance.
            is_shell (bool): Set to true to execute cmd through the default
                shell.
        '''
        pass
    def create_command(self, cmd_input: Mapping) -> Command:
        '''Create a pypyr.subproc.Command object from expanded step input.'''
        pass
    def run_step(self) -> None:
        '''Spawn a subprocess to run the command or program.
        If cmd.is_save==True, save result of each command to context 'cmdOut'.
        '''
        pass",snippet_94,4,
166605,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/steps/dsl/cmdasync.py,pypyr.steps.dsl.cmdasync.AsyncCmdStep,"import logging
from pypyr.aio.subproc import Command, Commands
from pypyr.context import Context
import pypyr.utils.types
from collections.abc import Mapping, Sequence
from pypyr.errors import ContextError, KeyInContextHasNoValueError, KeyNotInContextError
from pypyr.subproc import SimpleCommandTypes

class AsyncCmdStep:
    """"""A pypyr step to run executables/commands concurrently as a subprocess.

    This models a step that takes config like this in simple syntax:
        cmds:
            - <<cmd string 1>>
            - <<cmd string 2>>

    All the commands will run concurrently, in parallel.

    OR, expanded syntax is as a dict
        cmds:
            run: list[str | list[str]]. mandatory. command + args to execute.
                If list entry is another list[str], the sub-list will run in
                serial.
            save: bool. defaults False. save output to cmdOut. Treats output
                as text in the system's encoding and removes newlines at end.
            cwd: str/Pathlike. optional. Working directory for these commands.
            bytes (bool): Default False. When `save` return output bytes from
                cmds unaltered, without applying any encoding & text newline
                processing.
            encoding (str): Default None. When `save`, decode output with
                this encoding. The default of None uses the system encoding and
                should ""just work"".
            stdout (str | Path): Default None. Write stdout to this file path.
                Special value `/dev/null` writes to the system null device.
            stderr (str | Path): Default None. Write stderr to this file path.
                Special value `/dev/null` writes to the system null device.
                Special value `/dev/stdout` redirects err output to stdout.
            append (bool): Default False. When stdout/stderr a file, append
                rather than overwrite. Default is to overwrite.

    In expanded syntax, `run` can be a simple string or a list:
        cmds:
          run:
            - ./my-executable --arg
            - [./another-executable --arg, ./arb-executable arghere]
          save: False
          cwd: ./path/here

    As a list in simplified syntax:
        cmds:
          - my-executable --arg
          - ./another-executable --arg

    Any or all of the list items can use expanded syntax:
        cmds:
          - ./simple-cmd-here --arg1 value
          - run: cmd here
            save: False cwd: ./path/here
          - run:
              - my-executable --arg
              - ./another-executable --arg
            save: True
            cwd: ./path/here

    Any of the list items can in turn be a list. A sub-list will run in serial.

    In this example A, B.1 & C will start concurrently. B.2 will only run once
    B.1 is finished.

        cmds:
            - A
            - [B.1, B.2]
            - C

    If save is True, will save the output to context as cmdOut.

    cmdOut will be a list of pypyr.subproc.SubprocessResult objects, in order
    executed.

    SubprocessResult has the following properties:
    cmd: the cmd/args executed
    returncode: 0
    stdout: 'stdout str here. None if empty.'
    stderr: 'stderr str here. None if empty.'

    cmdOut.returncode is the exit status of the called process. Typically 0
    means OK. A negative value -N indicates that the child was terminated by
    signal N (POSIX only).

    The run_step method does the actual work. init parses the input yaml.

    Attributes:
        logger (logger): Logger instantiated by name of calling step.
        context: (pypyr.context.Context): The current pypyr Context.
        commands (pypyr.subproc.Commands): Commands to run as subprocess.
        is_shell (bool): True if subprocess should run through default shell.
        name (str): Name of calling step. Used for logging output & error
            messages.
    """"""

    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        """"""Initialize the CmdStep.

        The step config in the context dict in simplified syntax:
            cmd: <<cmd string>>

        OR, as a dict in expanded syntax:
            cmd:
                run: str. mandatory. command + args to execute.
                save: bool. optional. defaults False. save output to cmdOut.
                cwd: str/path. optional. if specified, change the working
                     directory just for the duration of the command.

        `run` can be a single string, or it can be a list of string if there
        are multiple commands to execute with the same settings.

        OR, as a list:
            cmd:
                - my-executable --arg
                - ./another-executable --arg

        Any or all of the list items can be in expanded syntax.

        Args:
            name (str): Unique name for step. Likely __name__ of calling step.
            context (pypyr.context.Context): Look for step config in this
                context instance.
            is_shell (bool): Set to true to execute cmd through the default
                shell.
        """"""
        assert name, 'name parameter must exist for CmdStep.'
        assert context, 'context param must exist for CmdStep.'
        self.name = name
        self.logger = logging.getLogger(name)
        context.assert_key_has_value(key='cmds', caller=name)
        self.context = context
        self.is_shell = is_shell
        cmd_config = context.get_formatted('cmds')
        commands = Commands()
        if isinstance(cmd_config, SimpleCommandTypes):
            commands.append(Command(cmd_config, is_shell=is_shell))
        elif isinstance(cmd_config, Mapping):
            commands.append(self.create_command(cmd_config))
        elif isinstance(cmd_config, Sequence):
            for cmd in cmd_config:
                if isinstance(cmd, SimpleCommandTypes):
                    commands.append(Command(cmd, is_shell=is_shell))
                elif isinstance(cmd, Sequence):
                    commands.append(Command([cmd], is_shell=is_shell))
                elif isinstance(cmd, Mapping):
                    commands.append(self.create_command(cmd))
                else:
                    raise ContextError(f'{cmd} in {name} cmds config is wrong.\nEach list item should be either a simple string, or a list to run in serial,\nor a dict for expanded syntax:\ncmds:\n  - ./my-executable --arg\n  - run:\n      - ./another-executable --arg value\n      - ./another-executable --arg value2\n    cwd: ../mydir/subdir\n  - run:\n      - ./arb-executable1 --arg value1\n      - [./arb-executable2.1, ./arb-executable2.2]\n    cwd: ../mydir/arbdir\n  - [./arb-executable3.1, ./arb-executable3.2]')
        else:
            raise ContextError(f'{name} cmds config should be either a list:\ncmds:\n  - ./my-executable --arg\n  - subdir/executable --arg1\n\nor a dictionary with a `run` sub-key:\ncmds:\n  run:\n    - ./my-executable --arg\n    - subdir/executable --arg1\n  cwd: ./mydir\n\nAny of the list items in root can be in expanded syntax:\ncmds:\n  - ./my-executable --arg\n  - subdir/executable --arg1\n  - run:\n      - ./arb-executable1 --arg value1\n      - [./arb-executable2.1, ./arb-executable2.2]\n    cwd: ../mydir/subdir\n  - [./arb-executable3.1, ./arb-executable3.2]')
        self.commands: Commands = commands

    def create_command(self, cmd_input: Mapping) -> Command:
        """"""Create pypyr.aio.subproc.Command object from expanded step input.""""""
        try:
            cmd = cmd_input['run']
            if not cmd:
                raise KeyInContextHasNoValueError(f'cmds.run must have a value for {self.name}.\nThe `run` input should look something like this:\ncmds:\n  run:\n    - ./arb-executable1 --arg value1\n    - ./arb-executable2 --arg value2\n  cwd: ../mydir/arbdir')
        except KeyError as err:
            raise KeyNotInContextError(f""cmds.run doesn't exist for {self.name}.\nThe input should look like this in expanded syntax:\ncmds:\n  run:\n    - ./my-executable --arg\n    - subdir/executable --arg1\n  cwd: ./mydir\n\nIf you're passing in a list of commands, each command should be a simple string,\nor a sub-list of commands to run in serial,\nor a dict with a `run` entry:\ncmds:\n  - ./my-executable --arg\n  - run: ./another-executable --arg value\n    cwd: ../mydir/subdir\n  - run:\n      - ./arb-executable1 --arg value1\n      - [./arb-executable2.1, ./arb-executable2.2]\n    cwd: ../mydir/arbdir\n  - [./arb-executable3.1, ./arb-executable3.2]"") from err
        is_save = pypyr.utils.types.cast_to_bool(cmd_input.get('save', False))
        cwd = cmd_input.get('cwd')
        is_bytes = cmd_input.get('bytes')
        is_text = not is_bytes if is_save else False
        stdout = cmd_input.get('stdout')
        stderr = cmd_input.get('stderr')
        if is_save:
            if stderr or stderr:
                raise ContextError(""You can't set `stdout` or `stderr` when `save` is True."")
        encoding = cmd_input.get('encoding')
        append = cmd_input.get('append', False)
        is_shell_override = cmd_input.get('shell', None)
        is_shell = self.is_shell if is_shell_override is None else is_shell_override
        return Command(cmd=cmd, is_shell=is_shell, cwd=cwd, is_save=is_save, is_text=is_text, stdout=stdout, stderr=stderr, encoding=encoding, append=append)

    def run_step(self) -> None:
        """"""Spawn subprocesses to run the commands asynchronously.

        If cmd.is_save==True, save aggregate result of all commands to context
        'cmdOut'.

        cmdOut will be a list of pypyr.subproc.SubprocessResult or Exception
        objects, in order executed.

        SubprocessResult has the following properties:
        cmd: the cmd/args executed
        returncode: 0
        stdout: 'stdout str here. None if empty.'
        stderr: 'stderr str here. None if empty.'
        """"""
        try:
            self.commands.run()
        finally:
            if self.commands.is_save:
                self.logger.debug('saving results to cmdOut')
                self.context['cmdOut'] = self.commands.results
            else:
                self.logger.debug('save is False: not saving results to cmdOut')","
class AsyncCmdStep:
    '''A pypyr step to run executables/commands concurrently as a subprocess.
    This models a step that takes config like this in simple syntax:
        cmds:
            - <<cmd string 1>>
            - <<cmd string 2>>
    All the commands will run concurrently, in parallel.
    OR, expanded syntax is as a dict
        cmds:
            run: list[str | list[str]]. mandatory. command + args to execute.
                If list entry is another list[str], the sub-list will run in
                serial.
            save: bool. defaults False. save output to cmdOut. Treats output
                as text in the system's encoding and removes newlines at end.
            cwd: str/Pathlike. optional. Working directory for these commands.
            bytes (bool): Default False. When `save` return output bytes from
                cmds unaltered, without applying any encoding & text newline
                processing.
            encoding (str): Default None. When `save`, decode output with
                this encoding. The default of None uses the system encoding and
                should ""just work"".
            stdout (str | Path): Default None. Write stdout to this file path.
                Special value `/dev/null` writes to the system null device.
            stderr (str | Path): Default None. Write stderr to this file path.
                Special value `/dev/null` writes to the system null device.
                Special value `/dev/stdout` redirects err output to stdout.
            append (bool): Default False. When stdout/stderr a file, append
                rather than overwrite. Default is to overwrite.
    In expanded syntax, `run` can be a simple string or a list:
        cmds:
          run:
            - ./my-executable --arg
            - [./another-executable --arg, ./arb-executable arghere]
          save: False
          cwd: ./path/here
    As a list in simplified syntax:
        cmds:
          - my-executable --arg
          - ./another-executable --arg
    Any or all of the list items can use expanded syntax:
        cmds:
          - ./simple-cmd-here --arg1 value
          - run: cmd here
            save: False cwd: ./path/here
          - run:
              - my-executable --arg
              - ./another-executable --arg
            save: True
            cwd: ./path/here
    Any of the list items can in turn be a list. A sub-list will run in serial.
    In this example A, B.1 & C will start concurrently. B.2 will only run once
    B.1 is finished.
        cmds:
            - A
            - [B.1, B.2]
            - C
    If save is True, will save the output to context as cmdOut.
    cmdOut will be a list of pypyr.subproc.SubprocessResult objects, in order
    executed.
    SubprocessResult has the following properties:
    cmd: the cmd/args executed
    returncode: 0
    stdout: 'stdout str here. None if empty.'
    stderr: 'stderr str here. None if empty.'
    cmdOut.returncode is the exit status of the called process. Typically 0
    means OK. A negative value -N indicates that the child was terminated by
    signal N (POSIX only).
    The run_step method does the actual work. init parses the input yaml.
    Attributes:
        logger (logger): Logger instantiated by name of calling step.
        context: (pypyr.context.Context): The current pypyr Context.
        commands (pypyr.subproc.Commands): Commands to run as subprocess.
        is_shell (bool): True if subprocess should run through default shell.
        name (str): Name of calling step. Used for logging output & error
            messages.
    '''
    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        '''Initialize the CmdStep.
        The step config in the context dict in simplified syntax:
            cmd: <<cmd string>>
        OR, as a dict in expanded syntax:
            cmd:
                run: str. mandatory. command + args to execute.
                save: bool. optional. defaults False. save output to cmdOut.
                cwd: str/path. optional. if specified, change the working
                     directory just for the duration of the command.
        `run` can be a single string, or it can be a list of string if there
        are multiple commands to execute with the same settings.
        OR, as a list:
            cmd:
                - my-executable --arg
                - ./another-executable --arg
        Any or all of the list items can be in expanded syntax.
        Args:
            name (str): Unique name for step. Likely __name__ of calling step.
            context (pypyr.context.Context): Look for step config in this
                context instance.
            is_shell (bool): Set to true to execute cmd through the default
                shell.
        '''
        pass
    def create_command(self, cmd_input: Mapping) -> Command:
        '''Create pypyr.aio.subproc.Command object from expanded step input.'''
        pass
    def run_step(self) -> None:
        '''Spawn subprocesses to run the commands asynchronously.
        If cmd.is_save==True, save aggregate result of all commands to context
        'cmdOut'.
        cmdOut will be a list of pypyr.subproc.SubprocessResult or Exception
        objects, in order executed.
        SubprocessResult has the following properties:
        cmd: the cmd/args executed
        returncode: 0
        stdout: 'stdout str here. None if empty.'
        stderr: 'stderr str here. None if empty.'
        '''
        pass",snippet_95,4,
167547,automl/HpBandSter,automl_HpBandSter/hpbandster/optimizers/learning_curve_models/base.py,hpbandster.optimizers.learning_curve_models.base.LCModel,"class LCModel:
    """"""
        base class for simple learning curve models
    """"""

    def fit(self, times, losses, configs=None):
        """"""
            function to train the model on the observed data

            Parameters:
            -----------

            times: list
                list of numpy arrays of the timesteps for each curve
            losses: list
                list of numpy arrays of the loss (the actual learning curve)
            configs: list or None
                list of the configurations for each sample. Each element
                has to be a numpy array. Set to None, if no configuration
                information is available.
        """"""
        raise NotImplementedError()

    def predict_unseen(self, times, config):
        """"""
            predict the loss of an unseen configuration

            Parameters:
            -----------

            times: numpy array
                times where to predict the loss
            config: numpy array
                the numerical representation of the config

            Returns:
            --------

            mean and variance prediction at input times for the given config
        """"""
        raise NotImplementedError()

    def extend_partial(self, times, obs_times, obs_losses, config=None):
        """"""
            extends a partially observed curve

            Parameters:
            -----------

            times: numpy array
                times where to predict the loss
            obs_times: numpy array
                times where the curve has already been observed
            obs_losses: numpy array
                corresponding observed losses
            config: numpy array
                numerical reperesentation of the config; None if no config
                information is available

            Returns:
            --------

            mean and variance prediction at input times


        """"""","class LCModel:
    
    def fit(self, times, losses, configs=None):
        
        pass
    def predict_unseen(self, times, config):
        '''
            predict the loss of an unseen configuration
            Parameters:
            -----------
            times: numpy array
                times where to predict the loss
            config: numpy array
                the numerical representation of the config
            Returns:
            --------
            mean and variance prediction at input times for the given config
        '''
        pass
    def extend_partial(self, times, obs_times, obs_losses, config=None):
        
        pass",snippet_98,4,"```python
import numpy as np
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel as C

class LCModel:
    """"""
    A simple learning‑curve model that uses a Gaussian Process to predict
    loss values over time for a given configuration.
    """"""

    def __init__(self):
        self.model = None
        self.X_train = None
        self.y_train = None

    def _build_features(self, times, configs=None):
        """"""
        Build the feature matrix for the GP.
        Each row corresponds to a (time, config) pair.
        """"""
        times = np.asarray(times).reshape(-1, 1)  # shape (n_samples, 1)
        if configs is None:
            X = times
        else:
            configs = np.asarray(configs)
            if configs.ndim == 1:
                configs = configs.reshape(-1, 1)
            X = np.hstack([times, configs])  # shape (n_samples, 1 + n_config)
        return X

    def fit(self, times, losses, configs=None):
        """"""
        Fit the Gaussian Process model to the provided data.
        Parameters
        ----------
        times : array-like, shape (n_samples,)
            Time points at which losses were observed.
        losses : array-like, shape (n_samples,)
            Observed loss values.
        configs : array-like, shape (n_samples, n_features) or None
            Numerical representation of the configuration for each sample.
        """"""
        times = np.asarray(times).ravel()
        losses = np.asarray(losses).ravel()
        if times.shape != losses.shape:
            raise ValueError(""times and losses must have the same shape"")

        X = self._build_features(times, configs)
        y = losses

        # Kernel: Constant * Matern + White noise
        kernel = C(1.0, (1e-3, 1e3)) * Matern(length_scale=1.0, nu=1.5) + WhiteKernel(noise_level=1e-5)
        self.model = GaussianProcessRegressor(kernel=kernel, normalize_y=True, n_restarts_optimizer=5)
        self.model.fit(X, y)

        self.X_train = X
        self.y_train = y

    def predict_unseen(self, times, config):
        """"""
        Predict the loss of an unseen configuration.
        Parameters
        ----------
        times : array-like, shape (n_times,)
            Times at which to predict the loss.
        config : array-like, shape (n_features,) or None
            Numerical representation of the configuration.
        Returns
        -------
        mean : ndarray, shape (n_times,)
            Predictive mean.
        var : ndarray, shape (n_times,)
            Predictive variance.
        """"""
        if self.model is None:
            raise RuntimeError(""Model has not been fitted yet."")

        times = np.asarray(times).ravel()
        X_test = self._build_features(times, config)
        mean, std = self.model.predict(X_test, return_std=True)
        var = std ** 2
        return mean, var

    def extend_partial(self, times, obs_times, obs_losses, config=None):
        """"""
        Extend the model with new partial observations.
        Parameters
        ----------
        times : array-like, shape (n_samples,)
            Time points for the new observations.
        obs_times : array-like, shape (n_samples,)
            Observed times (should match `times`).
        obs_losses : array-like, shape (n_samples,)
            Observed loss values.
        config : array-like, shape (n_features,) or None
            Numerical representation of the configuration for the new observations.
        """"""
        if self.model is None:
            # If no prior data, just fit with the new data
            self.fit(times, obs_losses, config)
            return

        # Combine old and new data
        new_X = self._build_features(obs_times, config)
        new_y = np.asarray(obs_losses).ravel()

        combined_X = np.vstack([self.X_train, new_X])
        combined_y = np.concatenate([self.y_train, new_y])

        # Refit the model
        self.fit(combined_X[:, 0], combined_y, combined_X[:, 1:] if combined_X.shape[1] > 1 else None)
```"
168574,romanz/trezor-agent,romanz_trezor-agent/libagent/age/client.py,libagent.age.client.Client,"from cryptography.hazmat.primitives.kdf.hkdf import HKDF
from cryptography.hazmat.primitives import hashes

class Client:
    """"""Sign messages and get public keys from a hardware device.""""""

    def __init__(self, device):
        """"""C-tor.""""""
        self.device = device

    def pubkey(self, identity, ecdh=False):
        """"""Return public key as VerifyingKey object.""""""
        with self.device:
            pubkey = bytes(self.device.pubkey(ecdh=ecdh, identity=identity))
            assert len(pubkey) == 32
            return pubkey

    def ecdh(self, identity, peer_pubkey):
        """"""Derive shared secret using ECDH from peer public key.""""""
        log.info('please confirm AGE decryption on %s for ""%s""...', self.device, identity.to_string())
        with self.device:
            assert len(peer_pubkey) == 32
            result, self_pubkey = self.device.ecdh_with_pubkey(pubkey=b'@' + peer_pubkey, identity=identity)
            assert result[:1] == b'\x04'
            hkdf = HKDF(algorithm=hashes.SHA256(), length=32, salt=peer_pubkey + self_pubkey, info=b'age-encryption.org/v1/X25519')
            return hkdf.derive(result[1:])","
class Client:
    
    def __init__(self, device):
        '''C-tor.'''
        pass
    def pubkey(self, identity, ecdh=False):
        '''Return public key as VerifyingKey object.'''
        pass
    def ecdh(self, identity, peer_pubkey):
        
        pass",snippet_100,4,
168596,romanz/trezor-agent,romanz_trezor-agent/libagent/ssh/__init__.py,libagent.ssh.ClosableNamedTemporaryFile,"import tempfile
import os

class ClosableNamedTemporaryFile:
    """"""Creates a temporary file that is not deleted when the file is closed.

    This allows the file to be opened with an exclusive lock, but used by other programs before
    it is deleted
    """"""

    def __init__(self):
        """"""Create a temporary file.""""""
        self.file = tempfile.NamedTemporaryFile(prefix='trezor-ssh-pubkey-', mode='w', delete=False)
        self.name = self.file.name

    def write(self, buf):
        """"""Write `buf` to the file.""""""
        self.file.write(buf)

    def close(self):
        """"""Closes the file, allowing it to be opened by other programs. Does not delete the file.""""""
        self.file.close()

    def __del__(self):
        """"""Deletes the temporary file.""""""
        try:
            os.unlink(self.file.name)
        except OSError:
            log.warning('Failed to delete temporary file: %s', self.file.name)","
class ClosableNamedTemporaryFile:
    
    def __init__(self):
        '''Create a temporary file.'''
        pass
    def write(self, buf):
        '''Write `buf` to the file.'''
        pass
    def close(self):
        
        pass
    def __del__(self):
        
        pass",snippet_101,5,"```python
import os
import tempfile

class ClosableNamedTemporaryFile:
    def __init__(self):
        """"""Create a temporary file.""""""
        # Create a named temporary file that is not deleted on close
        self._tmp = tempfile.NamedTemporaryFile(delete=False, mode='w+b')
        self._path = self._tmp.name
        self._closed = False

    def write(self, buf):
        """"""Write `buf` to the file.""""""
        if self._closed:
            raise ValueError(""I/O operation on closed file."")
        if isinstance(buf, str):
            buf = buf.encode()
        self._tmp.write(buf)

    def close(self):
        """"""Close the file and delete it.""""""
        if not self._closed:
            try:
                self._tmp.close()
            finally:
                try:
                    os.remove(self._path)
                except FileNotFoundError:
                    pass
                self._closed = True

    def __del__(self):
        """"""Ensure the file is cleaned up.""""""
        try:
            self.close()
        except Exception:
            # Suppress any exception during garbage collection
            pass
```"
168603,romanz/trezor-agent,romanz_trezor-agent/libagent/util.py,libagent.util.ExpiringCache,"import time

class ExpiringCache:
    """"""Simple cache with a deadline.""""""

    def __init__(self, seconds, timer=time.time):
        """"""C-tor.""""""
        self.duration = seconds
        self.timer = timer
        self.value = None
        self.set(None)

    def get(self):
        """"""Returns existing value, or None if deadline has expired.""""""
        if self.timer() > self.deadline:
            self.value = None
        return self.value

    def set(self, value):
        """"""Set new value and reset the deadline for expiration.""""""
        self.deadline = self.timer() + self.duration
        self.value = value","
class ExpiringCache:
    '''Simple cache with a deadline.'''
    def __init__(self, seconds, timer=time.time):
        '''C-tor.'''
        pass
    def get(self):
        '''Returns existing value, or None if deadline has expired.'''
        pass
    def set(self, value):
        
        pass",snippet_102,4,"```python
import time

class ExpiringCache:
    '''Simple cache with a deadline.'''
    def __init__(self, seconds, timer=time.time):
        '''C-tor.'''
        self._seconds = seconds
        self._timer = timer
        self._value = None
        self._expires = None

    def get(self):
        '''Returns existing value, or None if deadline has expired.'''
        if self._expires is None:
            return None
        if self._timer() >= self._expires:
            self._value = None
            self._expires = None
            return None
        return self._value

    def set(self, value):
        '''Store value and set expiration deadline.'''
        self._value = value
        self._expires = self._timer() + self._seconds
```"
168604,romanz/trezor-agent,romanz_trezor-agent/libagent/util.py,libagent.util.Reader,"import struct
import contextlib

class Reader:
    """"""Read basic type objects out of given stream.""""""

    def __init__(self, stream):
        """"""Create a non-capturing reader.""""""
        self.s = stream
        self._captured = None

    def readfmt(self, fmt):
        """"""Read a specified object, using a struct format string.""""""
        size = struct.calcsize(fmt)
        blob = self.read(size)
        obj, = struct.unpack(fmt, blob)
        return obj

    def read(self, size=None):
        """"""Read `size` bytes from stream.""""""
        blob = self.s.read(size)
        if size is not None and len(blob) < size:
            raise EOFError
        if self._captured:
            self._captured.write(blob)
        return blob

    @contextlib.contextmanager
    def capture(self, stream):
        """"""Capture all data read during this context.""""""
        self._captured = stream
        try:
            yield
        finally:
            self._captured = None","
class Reader:
    
    def __init__(self, stream):
        '''Create a non-capturing reader.'''
        pass
    def readfmt(self, fmt):
        '''Read a specified object, using a struct format string.'''
        pass
    def readfmt(self, fmt):
        
        pass
    @contextlib.contextmanager
    def capture(self, stream):
        
        pass",snippet_103,5,"```python
import contextlib
import struct

class Reader:
    def __init__(self, stream):
        '''Create a non-capturing reader.'''
        self.stream = stream

    def readfmt(self, fmt):
        '''Read a specified object, using a struct format string.'''
        size = struct.calcsize(fmt)
        data = self.stream.read(size)
        if len(data) < size:
            raise EOFError(""Not enough data to unpack"")
        result = struct.unpack(fmt, data)
        return result[0] if len(result) == 1 else result

    @contextlib.contextmanager
    def capture(self, stream):
        '''Temporarily replace the reader's stream with another stream.'''
        old_stream = self.stream
        self.stream = stream
        try:
            yield
        finally:
            self.stream = old_stream
```"
171012,indygreg/python-zstandard,indygreg_python-zstandard/zstandard/backend_cffi.py,zstandard.backend_cffi.BufferSegment,"class BufferSegment:
    """"""Represents a segment within a ``BufferWithSegments``.

    This type is essentially a reference to N bytes within a
    ``BufferWithSegments``.

    The object conforms to the buffer protocol.
    """"""

    @property
    def offset(self):
        """"""The byte offset of this segment within its parent buffer.""""""
        raise NotImplementedError()

    def __len__(self):
        """"""Obtain the length of the segment, in bytes.""""""
        raise NotImplementedError()

    def tobytes(self):
        """"""Obtain bytes copy of this segment.""""""
        raise NotImplementedError()","class BufferSegment:
    
    @property
    def offset(self):
        
        pass
    def __len__(self):
        
        pass
    def tobytes(self):
        '''Obtain bytes copy of this segment.'''
        pass",snippet_104,4,"class BufferSegment:
    def __init__(self, buffer, offset=0, length=None):
        """"""
        Create a segment view into *buffer* starting at *offset* with the given *length*.
        If *length* is None, the segment extends to the end of the buffer.
        """"""
        self._buffer = memoryview(buffer)
        if offset < 0:
            raise ValueError(""offset must be non‑negative"")
        if length is None:
            length = len(self._buffer) - offset
        if length < 0:
            raise ValueError(""length must be non‑negative"")
        if offset + length > len(self._buffer):
            raise ValueError(""segment exceeds buffer bounds"")
        self._offset = offset
        self._length = length

    @property
    def offset(self):
        """"""Return the starting offset of the segment within the underlying buffer.""""""
        return self._offset

    def __len__(self):
        """"""Return the length of the segment.""""""
        return self._length

    def tobytes(self):
        """"""Return a bytes copy of the segment.""""""
        return self._buffer[self._offset : self._offset + self._length].tobytes()
"
171015,indygreg/python-zstandard,indygreg_python-zstandard/zstandard/backend_cffi.py,zstandard.backend_cffi.BufferWithSegmentsCollection,"class BufferWithSegmentsCollection:
    """"""A virtual spanning view over multiple BufferWithSegments.

    Instances are constructed from 1 or more :py:class:`BufferWithSegments`
    instances. The resulting object behaves like an ordered sequence whose
    members are the segments within each ``BufferWithSegments``.

    If the object is composed of 2 ``BufferWithSegments`` instances with the
    first having 2 segments and the second have 3 segments, then ``b[0]``
    and ``b[1]`` access segments in the first object and ``b[2]``, ``b[3]``,
    and ``b[4]`` access segments from the second.
    """"""

    def __len__(self):
        """"""The number of segments within all ``BufferWithSegments``.""""""
        raise NotImplementedError()

    def __getitem__(self, i):
        """"""Obtain the ``BufferSegment`` at an offset.""""""
        raise NotImplementedError()","class BufferWithSegmentsCollection:
    '''A virtual spanning view over multiple BufferWithSegments.
    Instances are constructed from 1 or more :py:class:`BufferWithSegments`
    instances. The resulting object behaves like an ordered sequence whose
    members are the segments within each ``BufferWithSegments``.
    If the object is composed of 2 ``BufferWithSegments`` instances with the
    first having 2 segments and the second have 3 segments, then ``b[0]``
    and ``b[1]`` access segments in the first object and ``b[2]``, ``b[3]``,
    and ``b[4]`` access segments from the second.
    '''
    def __len__(self):
        
        pass
    def __getitem__(self, i):
        
        pass",snippet_105,3,"```python
class BufferWithSegmentsCollection:
    '''A virtual spanning view over multiple BufferWithSegments.
    Instances are constructed from 1 or more :py:class:`BufferWithSegments`
    instances. The resulting object behaves like an ordered sequence whose
    members are the segments within each ``BufferWithSegments``.
    If the object is composed of 2 ``BufferWithSegments`` instances with the
    first having 2 segments and the second have 3 segments, then ``b[0]``
    and ``b[1]`` access segments in the first object and ``b[2]``, ``b[3]``,
    and ``b[4]`` access segments from the second.
    '''
    def __init__(self, *buffers):
        if not buffers:
            raise ValueError(""At least one BufferWithSegments instance required"")
        self._buffers = tuple(buffers)
        # Precompute cumulative segment counts for fast lookup
        self._cum_counts = []
        total = 0
        for buf in self._buffers:
            total += len(buf)
            self._cum_counts.append(total)

    def __len__(self):
        return self._cum_counts[-1]

    def __getitem__(self, i):
        # Handle slice
        if isinstance(i, slice):
            start, stop, step = i.indices(len(self))
            return [self[j] for j in range(start, stop, step)]

        # Handle integer index
        if not isinstance(i, int):
            raise TypeError(f""Index must be int or slice, not {type(i).__name__}"")

        n = len(self)
        if i < 0:
            i += n
        if i < 0 or i >= n:
            raise IndexError(""Index out of range"")

        # Binary search to find the buffer containing the index
        lo, hi = 0, len(self._cum_counts) - 1
        while lo <= hi:
            mid = (lo + hi) // 2
            if i < self._cum_counts[mid]:
                hi = mid - 1
            else:
                lo = mid + 1
        buf_index = lo
        # Compute local index within the buffer
        prev_cum = self._cum_counts[buf_index - 1] if buf_index > 0 else 0
        local_index = i - prev_cum
        return self._buffers[buf_index][local_index]

    def __iter__(self):
        for buf in self._buffers:
            for seg in buf:
                yield seg
```"
171185,ssalentin/plip,ssalentin_plip/plip/exchange/xml.py,plip.exchange.xml.XMLStorage,"class XMLStorage:
    """"""Generic class for storing XML data from PLIP XML files.""""""

    @staticmethod
    def getdata(tree, location, force_string=False):
        """"""Gets XML data from a specific element and handles types.""""""
        found = tree.xpath('%s/text()' % location)
        if not found:
            return None
        else:
            data = found[0]
        if force_string:
            return data
        if data == 'True':
            return True
        elif data == 'False':
            return False
        else:
            try:
                return int(data)
            except ValueError:
                try:
                    return float(data)
                except ValueError:
                    return data

    @staticmethod
    def getcoordinates(tree, location):
        """"""Gets coordinates from a specific element in PLIP XML""""""
        return tuple((float(x) for x in tree.xpath('.//%s/*/text()' % location)))","class XMLStorage:
    '''Generic class for storing XML data from PLIP XML files.'''
    @staticmethod
    def getdata(tree, location, force_string=False):
        
        pass
    @staticmethod
    def getcoordinates(tree, location):
        '''Gets coordinates from a specific element in PLIP XML'''
        pass",snippet_106,3,"```python
import xml.etree.ElementTree as ET
from typing import Any, List, Tuple, Optional


class XMLStorage:
    """"""Generic class for storing XML data from PLIP XML files.""""""

    @staticmethod
    def getdata(tree: ET.Element, location: str, force_string: bool = False) -> Any:
        """"""
        Retrieve data from the XML tree at the given XPath location.

        Parameters
        ----------
        tree : xml.etree.ElementTree.Element
            The root element of the XML tree.
        location : str
            XPath expression relative to the root element.
        force_string : bool, optional
            If True, return the data as a string regardless of its original type.

        Returns
        -------
        Any
            The extracted data. If multiple elements match the XPath, a list of
            values is returned. If a single element matches, its value is returned.
            If no element matches, None is returned.
        """"""
        # Find all matching elements
        elements = tree.findall(location)
        if not elements:
            return None

        # Helper to extract value from an element
        def _extract(elem: ET.Element) -> Any:
            # Prefer attribute 'value' if present
            if ""value"" in elem.attrib:
                val = elem.attrib[""value""]
            else:
                val = elem.text
            if val is None:
                return None
            if force_string:
                return str(val).strip()
            # Try to convert to int or float
            try:
                if ""."" in val or ""e"" in val.lower():
                    return float(val)
                return int(val)
            except Exception:
                return str(val).strip()

        # If only one element, return its value directly
        if len(elements) == 1:
            return _extract(elements[0])

        # Otherwise return list of values
        return [_extract(e) for e in elements]

    @staticmethod
    def getcoordinates(tree: ET.Element, location: str) -> Optional[Tuple[float, float, float]]:
        """"""
        Extract 3D coordinates from a specific element in a PLIP XML file.

        Parameters
        ----------
        tree : xml.etree.ElementTree.Element
            The root element of the XML tree.
        location : str
            XPath expression pointing to the element that contains coordinate data.

        Returns
        -------
        tuple[float, float, float] or None
            A tuple of (x, y, z) coordinates if found, otherwise None.
        """"""
        elem = tree.find(location)
        if elem is None:
            return None

        # Try to get coordinates from attributes first
        if all(k in elem.attrib for k in (""x"", ""y"", ""z"")):
            try:
                return (
                    float(elem.attrib[""x""]),
                    float(elem.attrib[""y""]),
                    float(elem.attrib[""z""]),
                )
            except Exception:
                pass

        # Fallback: look for child elements <x>, <y>, <z>
        coords = {}
        for axis in (""x"", ""y"", ""z""):
            child = elem.find(axis)
            if child is not None and child.text is not None:
                try:
                    coords[axis] = float(child.text)
                except Exception:
                    return None
        if len(coords) == 3:
            return coords[""x""], coords[""y""], coords[""z""]

        return None
```"
172857,cogeotiff/rio-tiler,rio_tiler/colormap.py,rio_tiler.colormap.ColorMaps,"import attr
from rio_tiler.errors import ColorMapAlreadyRegistered, InvalidColorFormat, InvalidColorMapName, InvalidFormat
import json
import numpy
from typing import Dict, List, Sequence, Tuple, Union
import pathlib
from rio_tiler.types import ColorMapType, DataMaskType, DiscreteColorMapType, GDALColorMapType, IntervalColorMapType

@attr.s(frozen=True)
class ColorMaps:
    """"""Default Colormaps holder.

    Attributes:
        data (dict): colormaps. Defaults to `rio_tiler.colormap.DEFAULTS_CMAPS`.

    """"""
    data: Dict[str, Union[str, pathlib.Path, ColorMapType]] = attr.ib(default=attr.Factory(lambda: DEFAULT_CMAPS_FILES))

    def get(self, name: str) -> ColorMapType:
        """"""Fetch a colormap.

        Args:
            name (str): colormap name.

        Returns
            dict: colormap dictionary.

        """"""
        cmap = self.data.get(name, None)
        if cmap is None:
            raise InvalidColorMapName(f'Invalid colormap name: {name}')
        if isinstance(cmap, (pathlib.Path, str)):
            if isinstance(cmap, str):
                cmap = pathlib.Path(cmap)
            if cmap.suffix == '.npy':
                colormap = numpy.load(cmap)
                assert colormap.shape == (256, 4)
                assert colormap.dtype == numpy.uint8
                cmap_data = {idx: tuple(value) for idx, value in enumerate(colormap)}
            elif cmap.suffix == '.json':
                with cmap.open() as f:
                    cmap_data = json.load(f, object_hook=lambda x: {int(k): parse_color(v) for k, v in x.items()})
                if isinstance(cmap_data, Sequence):
                    cmap_data = [(tuple(inter), parse_color(v)) for inter, v in cmap_data]
            else:
                raise ValueError(f'Not supported {cmap.suffix} extension for ColorMap')
            self.data[name] = cmap_data
            return cmap_data
        return cmap

    def list(self) -> List[str]:
        """"""List registered Colormaps.

        Returns
            list: list of colormap names.

        """"""
        return list(self.data)

    def register(self, custom_cmap: Dict[str, Union[str, pathlib.Path, ColorMapType]], overwrite: bool=False) -> 'ColorMaps':
        """"""Register a custom colormap.

        Args:
            custom_cmap (dict): custom colormap(s) to register.
            overwrite (bool): Overwrite existing colormap with same key. Defaults to False.

        Examples:
            >>> cmap = cmap.register({""acmap"": {0: (0, 0, 0, 0), ...}})

            >>> cmap = cmap.register({""acmap"": ""acmap.npy""})

        """"""
        for name, _ in custom_cmap.items():
            if not overwrite and name in self.data:
                raise ColorMapAlreadyRegistered(f'{name} is already registered. Use force=True to overwrite.')
        return ColorMaps({**self.data, **custom_cmap})","@attr.s(frozen=True)
class ColorMaps:
    '''Default Colormaps holder.
    Attributes:
        data (dict): colormaps. Defaults to `rio_tiler.colormap.DEFAULTS_CMAPS`.
    '''
    def get(self, name: str) -> ColorMapType:
        
        pass
    def list(self) -> List[str]:
        '''List registered Colormaps.
        Returns
            list: list of colormap names.
        '''
        pass
    def register(self, custom_cmap: Dict[str, Union[str, pathlib.Path, ColorMapType]], overwrite: bool=False) -> 'ColorMaps':
        '''Register a custom colormap.
        Args:
            custom_cmap (dict): custom colormap(s) to register.
            overwrite (bool): Overwrite existing colormap with same key. Defaults to False.
        Examples:
            >>> cmap = cmap.register({""acmap"": {0: (0, 0, 0, 0), ...}})
            >>> cmap = cmap.register({""acmap"": ""acmap.npy""})
        '''
        pass",snippet_107,4,"```python
import pathlib
from typing import Dict, List, Union, Tuple

import attr
import numpy as np

from rio_tiler.colormap import DEFAULTS_CMAPS, ColorMapType


@attr.s(frozen=True)
class ColorMaps:
    '''Default Colormaps holder.
    Attributes:
        data (dict): colormaps. Defaults to `rio_tiler.colormap.DEFAULTS_CMAPS`.
    '''
    data: Dict[str, ColorMapType] = attr.ib(default=DEFAULTS_CMAPS)

    def get(self, name: str) -> ColorMapType:
        """"""Return the colormap for the given name.""""""
        try:
            return self.data[name]
        except KeyError as exc:
            raise KeyError(f""Colormap '{name}' not found."") from exc

    def list(self) -> List[str]:
        '''List registered Colormaps.
        Returns
            list: list of colormap names.
        '''
        return sorted(self.data.keys())

    def register(
        self,
        custom_cmap: Dict[str, Union[str, pathlib.Path, ColorMapType]],
        overwrite: bool = False,
    ) -> 'ColorMaps':
        '''Register a custom colormap.
        Args:
            custom_cmap (dict): custom colormap(s) to register.
            overwrite (bool): Overwrite existing colormap with same key. Defaults to False.
        Examples:
            >>> cmap = cmap.register({""acmap"": {0: (0, 0, 0, 0), ...}})
            >>> cmap = cmap.register({""acmap"": ""acmap.npy""})
        '''
        new_data = dict(self.data)

        for key, value in custom_cmap.items():
            if isinstance(value, (str, pathlib.Path)):
                path = pathlib.Path(value)
                if not path.is_file():
                    raise FileNotFoundError(f""Colormap file '{path}' does not exist."")
                try:
                    arr = np.load(path)
                except Exception as exc:
                    raise ValueError(f""Failed to load colormap from '{path}': {exc}"") from exc
                if arr.ndim != 2 or arr.shape[1] != 4:
                    raise ValueError(
                        f""Colormap file '{path}' must contain an array of shape (n, 4).""
                    )
                cmap_dict = {int(idx): tuple(map(int, row)) for idx, row in enumerate(arr)}
                cmap_value = cmap_dict
            elif isinstance(value, dict):
                cmap_value = value
            else:
                raise TypeError(
                    f""Unsupported colormap type for key '{key}': {type(value).__name__}""
                )

            if not overwrite and key in new_data:
                raise ValueError(f""Colormap '{key}' already exists. Use overwrite=True to replace."")
            new_data[key] = cmap_value

        return attr.evolve(self, data=new_data)
```"
174678,ipinfo/python,ipinfo_python/ipinfo/details.py,ipinfo.details.Details,"class Details:
    """"""Encapsulates data for single IP address.""""""

    def __init__(self, details):
        """"""Initialize by settings `details` attribute.""""""
        self.details = details

    def __getattr__(self, attr):
        """"""Return attribute if it exists in details array, else return error.""""""
        if attr not in self.details:
            raise AttributeError(f'{attr} is not a valid attribute of Details')
        return self.details[attr]

    @property
    def all(self):
        """"""Return all details as dict.""""""
        return self.details","class Details:
    '''Encapsulates data for single IP address.'''
    def __init__(self, details):
        
        pass
    def __getattr__(self, attr):
        '''Return attribute if it exists in details array, else return error.'''
        pass
    @property
    def all(self):
        '''Return all details as dict.'''
        pass",snippet_108,4,"```python
class Details:
    '''Encapsulates data for single IP address.'''
    def __init__(self, details):
        # Store the provided details dictionary
        self._details = dict(details)

    def __getattr__(self, attr):
        '''Return attribute if it exists in details array, else return error.'''
        if attr in self._details:
            return self._details[attr]
        raise AttributeError(f""'Details' object has no attribute '{attr}'"")

    @property
    def all(self):
        '''Return all details as dict.'''
        return dict(self._details)
```"
176091,opencobra/cobrapy,opencobra_cobrapy/src/cobra/util/context.py,cobra.util.context.HistoryManager,"from typing import TYPE_CHECKING, Any, Callable, Optional

class HistoryManager:
    """"""
    Define a base context manager.

    It records a list of actions to be taken at a later time.
    This is used to implement context managers that allow temporary
    changes to a `cobra.core.Model`.

    """"""

    def __init__(self, **kwargs) -> None:
        """"""Initialize the class.""""""
        super().__init__(**kwargs)
        self._history = []

    def __call__(self, operation: Callable[[Any], Any]) -> None:
        """"""Add the corresponding operation to the history stack.

        Parameters
        ----------
        operation : callable
            A function to be called at a later time.

        """"""
        self._history.append(operation)

    def reset(self) -> None:
        """"""Trigger executions for all items in the stack in reverse order.""""""
        while self._history:
            entry = self._history.pop()
            entry()

    def size(self) -> int:
        """"""Calculate number of operations on the stack.""""""
        return len(self._history)","
class HistoryManager:
    
    def __init__(self, **kwargs) -> None:
        '''Initialize the class.'''
        pass
    def __call__(self, operation: Callable[[Any], Any]) -> None:
        
        pass
    def reset(self) -> None:
        '''Trigger executions for all items in the stack in reverse order.'''
        pass
    def size(self) -> int:
        '''Calculate number of operations on the stack.'''
        pass",snippet_109,5,"```python
from typing import Any, Callable, List


class HistoryManager:
    def __init__(self, **kwargs) -> None:
        """"""Initialize the class.""""""
        # Store the history stack
        self._stack: List[Callable[[Any], Any]] = []

    def __call__(self, operation: Callable[[Any], Any]) -> None:
        """"""Add an operation to the history stack.""""""
        self._stack.append(operation)

    def reset(self) -> None:
        """"""Trigger executions for all items in the stack in reverse order.""""""
        # Execute each operation in reverse order
        for op in reversed(self._stack):
            try:
                # Try calling without arguments first
                op()
            except TypeError:
                # If the operation expects an argument, call with None
                op(None)
        # Clear the stack after execution
        self._stack.clear()

    def size(self) -> int:
        """"""Calculate number of operations on the stack.""""""
        return len(self._stack)
```"
176713,adafruit/Adafruit_Blinka,adafruit_Adafruit_Blinka/src/adafruit_blinka/__init__.py,adafruit_blinka.Enum,"class Enum:
    """"""
    Object supporting CircuitPython-style of static symbols
    as seen with Direction.OUTPUT, Pull.UP
    """"""

    def __repr__(self):
        """"""
        Assumes instance will be found as attribute of own class.
        Returns dot-subscripted path to instance
        (assuming absolute import of containing package)
        """"""
        cls = type(self)
        for key in dir(cls):
            if getattr(cls, key) is self:
                return '{}.{}.{}'.format(cls.__module__, cls.__qualname__, key)
        return repr(self)

    @classmethod
    def iteritems(cls):
        """"""
        Inspects attributes of the class for instances of the class
        and returns as key,value pairs mirroring dict#iteritems
        """"""
        for key in dir(cls):
            val = getattr(cls, key)
            if isinstance(cls, val):
                yield (key, val)","class Enum:
    '''
    Object supporting CircuitPython-style of static symbols
    as seen with Direction.OUTPUT, Pull.UP
    '''
    def __repr__(self):
        '''
        Assumes instance will be found as attribute of own class.
        Returns dot-subscripted path to instance
        (assuming absolute import of containing package)
        '''
        pass
    @classmethod
    def iteritems(cls):
        
        pass",snippet_110,3,"class Enum:
    '''
    Object supporting CircuitPython-style of static symbols
    as seen with Direction.OUTPUT, Pull.UP
    '''
    def __repr__(self):
        '''
        Assumes instance will be found as attribute of own class.
        Returns dot-subscripted path to instance
        (assuming absolute import of containing package)
        '''
        cls = self.__class__
        # Find the attribute name that refers to this instance
        name = None
        for attr, val in vars(cls).items():
            if val is self:
                name = attr
                break
        if name is None:
            # Fallback to default representation
            return super().__repr__()
        # Build absolute dotted path: module.classname.attribute
        module = cls.__module__
        qualname = cls.__qualname__
        return f'{module}.{qualname}.{name}'

    @classmethod
    def iteritems(cls):
        '''
        Iterate over (name, value) pairs of class attributes
        that are instances of the Enum subclass.
        '''
        for name, value in vars(cls).items():
            if isinstance(value, cls):
                yield name, value"
176756,adafruit/Adafruit_Blinka,adafruit_Adafruit_Blinka/src/adafruit_blinka/microcontroller/nova/__init__.py,adafruit_blinka.microcontroller.nova.Connection,"class Connection:
    """"""Connection class""""""
    __instance = None

    @staticmethod
    def getInstance():
        """"""Static access method.""""""
        if Connection.__instance is None:
            Connection()
        return Connection.__instance

    def __init__(self):
        """"""Virtually private constructor.""""""
        if Connection.__instance is not None:
            raise Exception('This class is a singleton!')
        from binhoHostAdapter import binhoHostAdapter
        from binhoHostAdapter import binhoUtilities
        devices = binhoUtilities.listAvailableDevices()
        if len(devices) > 0:
            Connection.__instance = binhoHostAdapter.binhoHostAdapter(devices[0])
        else:
            raise RuntimeError('No Binho Nova found!')","class Connection:
    '''Connection class'''
    @staticmethod
    def getInstance():
        
        pass
    def __init__(self):
        
        pass",snippet_111,3,"```python
class Connection:
    '''Connection class'''
    _instance = None

    @staticmethod
    def getInstance():
        if Connection._instance is None:
            Connection._instance = Connection()
        return Connection._instance

    def __init__(self):
        if Connection._instance is not None:
            raise RuntimeError(""Use getInstance() to get the singleton instance."")
        # Initialize connection attributes here
        self.connected = True
```"
177578,ottogroup/palladium,ottogroup_palladium/palladium/persistence.py,palladium.persistence.FileLikeIO,"from abc import abstractmethod

class FileLikeIO:
    """"""Used by :class:`FileLike` to access low level file handle
    operations.
    """"""

    @abstractmethod
    def open(self, path, mode='r'):
        """"""Return a file handle

        For normal files, the implementation is:

        ```python
        return open(path, mode)
        ```
        """"""

    @abstractmethod
    def exists(self, path):
        """"""Test whether a path exists

        For normal files, the implementation is:

        ```python
        return os.path.exists(path)
        ```
        """"""

    @abstractmethod
    def remove(self, path):
        """"""Remove a file

        For normal files, the implementation is:

        ```python
        os.remove(path)
        ```
        """"""","
class FileLikeIO:
    '''Used by :class:`FileLike` to access low level file handle
    operations.
    '''
    @abstractmethod
    def open(self, path, mode='r'):
        '''Return a file handle
        For normal files, the implementation is:
        ```python
        return open(path, mode)
        ```
        '''
        pass
    @abstractmethod
    def exists(self, path):
        
        pass
    @abstractmethod
    def remove(self, path):
        
        pass",snippet_113,4,"```python
import os
from abc import ABC, abstractmethod
from typing import TextIO


class FileLikeIO(ABC):
    """"""Used by :class:`FileLike` to access low level file handle
    operations.
    """"""

    @abstractmethod
    def open(self, path: str, mode: str = ""r"") -> TextIO:
        """"""Return a file handle.

        For normal files, the implementation is::

            return open(path, mode)
        """"""
        return open(path, mode)

    @abstractmethod
    def exists(self, path: str) -> bool:
        """"""Return ``True`` if the given path exists, otherwise ``False``.""""""
        return os.path.exists(path)

    @abstractmethod
    def remove(self, path: str) -> None:
        """"""Remove the file at the given path.""""""
        os.remove(path)
```"
179419,python-constraint/python-constraint,python-constraint_python-constraint/constraint/domain.py,constraint.domain.Variable,"class Variable:
    """"""Helper class for variable definition.

    Using this class is optional, since any hashable object,
    including plain strings and integers, may be used as variables.
    """"""

    def __init__(self, name):
        """"""Initialization method.

        Args:
            name (string): Generic variable name for problem-specific
                purposes
        """"""
        self.name = name

    def __repr__(self):
        """"""Represents itself with the name attribute.""""""
        return self.name","class Variable:
    '''Helper class for variable definition.
    Using this class is optional, since any hashable object,
    including plain strings and integers, may be used as variables.
    '''
    def __init__(self, name):
        '''Initialization method.
        Args:
            name (string): Generic variable name for problem-specific
                purposes
        '''
        pass
    def __repr__(self):
        '''Represents itself with the name attribute.'''
        pass",snippet_115,3,"class Variable:
    '''Helper class for variable definition.
    Using this class is optional, since any hashable object,
    including plain strings and integers, may be used as variables.
    '''
    def __init__(self, name):
        '''Initialization method.
        Args:
            name (string): Generic variable name for problem-specific
                purposes
        '''
        self.name = name

    def __repr__(self):
        '''Represents itself with the name attribute.'''
        return f'{self.name}'"
179425,python-constraint/python-constraint,python-constraint_python-constraint/constraint/solvers.py,constraint.solvers.Solver,"class Solver:
    """"""Abstract base class for solvers.""""""
    requires_pickling = False

    def getSolution(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Return one solution for the given problem.

        Args:
            domains (dict): Dictionary mapping variables to their domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        """"""
        msg = f'{self.__class__.__name__} is an abstract class'
        raise NotImplementedError(msg)

    def getSolutions(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Return all solutions for the given problem.

        Args:
            domains (dict): Dictionary mapping variables to domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        """"""
        msg = f'{self.__class__.__name__} provides only a single solution'
        raise NotImplementedError(msg)

    def getSolutionIter(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Return an iterator for the solutions of the given problem.

        Args:
            domains (dict): Dictionary mapping variables to domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        """"""
        msg = f""{self.__class__.__name__} doesn't provide iteration""
        raise NotImplementedError(msg)","class Solver:
    
    def getSolution(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        '''Return one solution for the given problem.
        Args:
            domains (dict): Dictionary mapping variables to their domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        '''
        pass
    def getSolutions(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        
        pass
    def getSolutionIter(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        
        pass",snippet_116,4,"```python
class Solver:
    def getSolution(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Return one solution for the given problem or None if none exists.""""""
        for solution in self.getSolutionIter(domains, constraints, vconstraints):
            return solution
        return None

    def getSolutions(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Return a list of all solutions for the given problem.""""""
        return list(self.getSolutionIter(domains, constraints, vconstraints))

    def getSolutionIter(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Yield all solutions for the given problem.""""""
        # Make a shallow copy of domains to avoid side effects
        domains = {var: list(vals) for var, vals in domains.items()}
        # Build a mapping from variable to constraints for quick lookup
        var_to_constraints = vconstraints

        # Helper to check all constraints that are fully assigned
        def constraints_satisfied(assignment):
            for var, cons_list in var_to_constraints.items():
                if var in assignment:
                    for constraint, vars_involved in cons_list:
                        # Only evaluate if all vars involved are assigned
                        if all(v in assignment for v in vars_involved):
                            if not constraint(assignment):
                                return False
            return True

        # Recursive backtracking search
        def backtrack(assignment):
            # If all variables assigned, yield solution
            if len(assignment) == len(domains):
                yield dict(assignment)
                return

            # Select unassigned variable with smallest domain (MRV heuristic)
            unassigned = [v for v in domains if v not in assignment]
            var = min(unassigned, key=lambda v: len(domains[v]))

            for value in domains[var]:
                assignment[var] = value
                if constraints_satisfied(assignment):
                    yield from backtrack(assignment)
                del assignment[var]

        yield from backtrack({})
```"
179445,maroba/findiff,findiff/pde.py,findiff.pde.PDE,"import numpy as np
from scipy.sparse.linalg import spsolve
import scipy.sparse as sparse

class PDE:
    """"""
    Representation of a partial differential equation.
    """"""

    def __init__(self, lhs, rhs, bcs):
        """"""
        Initializes the PDE.

        You need to specify the left hand side (lhs) in terms of derivatives
        as well as the right hand side in terms of an array.

        Parameters
        ----------
        lhs: FinDiff object or combination of FinDiff objects
            the left hand side of the PDE
        rhs: numpy.ndarray
            the right hand side of the PDE
        bcs: BoundaryConditions
            the boundary conditions for the PDE

        """"""
        self.lhs = lhs
        self.rhs = rhs
        self.bcs = bcs
        self._L = None

    def solve(self):
        """"""
        Solves the PDE.

        Returns
        -------
        out: numpy.ndarray
            Array with the solution of the PDE.
        """"""
        shape = self.bcs.shape
        if self._L is None:
            self._L = self.lhs.matrix(shape)
        L = sparse.lil_matrix(self._L)
        f = self.rhs.reshape(-1, 1)
        nz = list(self.bcs.row_inds())
        L[nz, :] = self.bcs.lhs[nz, :]
        f[nz] = np.array(self.bcs.rhs[nz].toarray()).reshape(-1, 1)
        L = sparse.csr_matrix(L)
        return spsolve(L, f).reshape(shape)","
class PDE:
    
    def __init__(self, lhs, rhs, bcs):
        '''
        Initializes the PDE.
        You need to specify the left hand side (lhs) in terms of derivatives
        as well as the right hand side in terms of an array.
        Parameters
        ----------
        lhs: FinDiff object or combination of FinDiff objects
            the left hand side of the PDE
        rhs: numpy.ndarray
            the right hand side of the PDE
        bcs: BoundaryConditions
            the boundary conditions for the PDE
        '''
        pass
    def solve(self):
        
        pass",snippet_117,3,"```python
import numpy as np

class PDE:
    """"""
    A simple PDE solver that accepts a left‑hand side (lhs) represented by a FinDiff
    object (or a combination of such objects), a right‑hand side array (rhs), and
    boundary conditions (bcs). The solver assembles the linear system, applies
    the boundary conditions, and solves for the unknown field.
    """"""

    def __init__(self, lhs, rhs, bcs):
        """"""
        Initializes the PDE.

        Parameters
        ----------
        lhs : FinDiff or combination of FinDiff objects
            The left hand side of the PDE. It must provide a method `to_matrix`
            that returns the discretised matrix representation.
        rhs : numpy.ndarray
            The right hand side of the PDE. Must be a 1‑D array of the same
            length as the number of unknowns in `lhs`.
        bcs : BoundaryConditions
            The boundary conditions for the PDE. It must provide a method
            `apply` that takes a matrix and a vector and returns the modified
            matrix and vector.
        """"""
        # Basic type checks
        if not hasattr(lhs, ""to_matrix""):
            raise TypeError(""lhs must provide a `to_matrix` method."")
        if not isinstance(rhs, np.ndarray):
            raise TypeError(""rhs must be a numpy.ndarray."")
        if not hasattr(bcs, ""apply""):
            raise TypeError(""bcs must provide an `apply` method."")

        self.lhs = lhs
        self.rhs = rhs
        self.bcs = bcs

    def solve(self):
        """"""
        Assembles the linear system, applies boundary conditions, and solves
        for the unknown field.

        Returns
        -------
        solution : numpy.ndarray
            The solution vector.
        """"""
        # Assemble the matrix and RHS vector from the FinDiff object
        A = self.lhs.to_matrix()
        b = self.rhs.copy()

        # Apply boundary conditions
        A, b = self.bcs.apply(A, b)

        # Solve the linear system
        try:
            solution = np.linalg.solve(A, b)
        except np.linalg.LinAlgError as exc:
            raise RuntimeError(""Linear system could not be solved."") from exc

        return solution
```"
186773,sphinx-gallery/sphinx-gallery,sphinx_gallery/_dummy/__init__.py,sphinx_gallery._dummy.DummyClass,"class DummyClass:
    """"""Dummy class for testing method resolution.""""""

    def run(self):
        """"""Do nothing.""""""
        pass

    @property
    def prop(self):
        """"""Property.""""""
        return 'Property'","class DummyClass:
    
    def run(self):
        '''Do nothing.'''
        pass
    @property
    def prop(self):
        '''Property.'''
        pass",snippet_119,3,"class DummyClass:
    def run(self):
        '''Do nothing.'''
        pass

    @property
    def prop(self):
        '''Property.'''
        return ""dummy property"""
186774,sphinx-gallery/sphinx-gallery,sphinx_gallery/_dummy/nested.py,sphinx_gallery._dummy.nested.NestedDummyClass,"class NestedDummyClass:
    """"""Nested dummy class for testing method resolution.""""""

    def run(self):
        """"""Do nothing.""""""
        pass

    @property
    def prop(self):
        """"""Property.""""""
        return 'Property'","class NestedDummyClass:
    
    def run(self):
        '''Do nothing.'''
        pass
    @property
    def prop(self):
        
        pass",snippet_120,3,"class NestedDummyClass:
    
    def run(self):
        '''Do nothing.'''
        pass
    
    @property
    def prop(self):
        return ""prop"""
190420,JamesPHoughton/pysd,JamesPHoughton_pysd/pysd/py_backend/external.py,pysd.py_backend.external.Excels,"import numpy as np
from openpyxl import load_workbook
import pandas as pd

class Excels:
    """"""
    Class to save the read Excel files and thus avoid double reading
    """"""
    _Excels, _Excels_opyxl = ({}, {})

    @classmethod
    def read(cls, file_name, tab):
        """"""
        Read the Excel file or return the previously read one
        """"""
        if file_name.joinpath(tab) in cls._Excels:
            return cls._Excels[file_name.joinpath(tab)]
        else:
            read_kwargs = {}
            ext = file_name.suffix.lower()
            if ext in _SPREADSHEET_EXTS:
                read_func = pd.read_excel
                read_kwargs['sheet_name'] = tab
            elif ext == '.csv':
                read_func = pd.read_csv
                if tab and (not tab[0].isalnum()):
                    read_kwargs['sep'] = tab
            else:
                read_func = pd.read_table
                if tab and (not tab[0].isalnum()):
                    read_kwargs['sep'] = tab
            excel = np.array([pd.to_numeric(ex, errors='coerce') for ex in read_func(file_name, header=None, **read_kwargs).values])
            cls._Excels[file_name.joinpath(tab)] = excel
            return excel

    @classmethod
    def read_opyxl(cls, file_name):
        """"""
        Read the Excel file using OpenPyXL or return the previously read one
        """"""
        if file_name in cls._Excels_opyxl:
            return cls._Excels_opyxl[file_name]
        else:
            excel = load_workbook(file_name, read_only=True, data_only=True)
            cls._Excels_opyxl[file_name] = excel
            return excel

    @classmethod
    def clean(cls):
        """"""
        Clean the dictionary of read files
        """"""
        for file in cls._Excels_opyxl.values():
            file.close()
        cls._Excels, cls._Excels_opyxl = ({}, {})","
class Excels:
    
    @classmethod
    def read(cls, file_name, tab):
        
        pass
    @classmethod
    def read_opyxl(cls, file_name):
        
        pass
    @classmethod
    def clean(cls):
        '''
        Clean the dictionary of read files
        '''
        pass",snippet_121,4,"```python
import pandas as pd
from openpyxl import load_workbook

class Excels:
    _files = {}

    @classmethod
    def read(cls, file_name, tab):
        """"""
        Read an Excel file and return the DataFrame for the specified sheet.
        The file is cached after the first read.
        """"""
        if file_name not in cls._files:
            # Load all sheets into a dict of DataFrames
            cls._files[file_name] = pd.read_excel(file_name, sheet_name=None)
        sheets = cls._files[file_name]
        if tab not in sheets:
            raise KeyError(f""Sheet '{tab}' not found in '{file_name}'."")
        return sheets[tab]

    @classmethod
    def read_opyxl(cls, file_name):
        """"""
        Read an Excel file using openpyxl and cache the workbook.
        """"""
        if file_name not in cls._files:
            cls._files[file_name] = load_workbook(file_name)
        return cls._files[file_name]

    @classmethod
    def clean(cls):
        """"""
        Clean the dictionary of read files.
        """"""
        cls._files.clear()
```"
222265,box/flaky,box_flaky/flaky/multiprocess_string_io.py,flaky.multiprocess_string_io.MultiprocessingStringIO,"import multiprocessing

class MultiprocessingStringIO:
    """"""
    Provide a StringIO-like interface to the multiprocessing ListProxy. The
    multiprocessing ListProxy needs to be instantiated before the flaky plugin
    is configured, so the list is created as a class variable.
    """"""
    _manager = multiprocessing.Manager()
    proxy = _manager.list()

    def getvalue(self):
        """"""
        Shadow the StringIO.getvalue method.
        """"""
        return ''.join((i for i in self.proxy))

    def writelines(self, content_list):
        """"""
        Shadow the StringIO.writelines method. Ingests a list and
        translates that to a string
        """"""
        for item in content_list:
            self.write(item)

    def write(self, content):
        """"""
        Shadow the StringIO.write method.
        """"""
        content.strip('\n')
        self.proxy.append(content)","
class MultiprocessingStringIO:
    
    def getvalue(self):
        
        pass
    def writelines(self, content_list):
        '''
        Shadow the StringIO.writelines method. Ingests a list and
        translates that to a string
        '''
        pass
    def writelines(self, content_list):
        
        pass",snippet_123,4,"```python
import multiprocessing

class MultiprocessingStringIO:
    def __init__(self):
        # Use a Manager list to allow safe sharing between processes
        self._manager = multiprocessing.Manager()
        self._lines = self._manager.list()

    def getvalue(self):
        """"""Return the concatenated string of all written lines.""""""
        return ''.join(self._lines)

    def writelines(self, content_list):
        """"""
        Shadow the StringIO.writelines method.
        Accepts an iterable of strings and stores them in a shared list.
        """"""
        for line in content_list:
            self._lines.append(line)
```"
222348,joshspeagle/dynesty,joshspeagle_dynesty/py/dynesty/utils.py,dynesty.utils.DelayTimer,"import time

class DelayTimer:
    """""" Utility class that allows us to detect a certain
    time has passed""""""

    def __init__(self, delay):
        """""" Initialise the time with delay of dt seconds

        Parameters
        ----------

        delay: float
            The number of seconds in the timer
        """"""
        self.delay = delay
        self.last_time = time.time()

    def is_time(self):
        """"""
        Returns true if more than self.dt seconds has passed
        since the initialization or last call of successful is_time()

        Returns
        -------
        ret: bool
             True if specified amout of time has passed since the
             initialization or last successful is_time() call
        """"""
        curt = time.time()
        if curt - self.last_time > self.delay:
            self.last_time = curt
            return True
        return False","
class DelayTimer:
        ''' Utility class that allows us to detect a certain
            time has passed'''
    def __init__(self, delay):
        
        pass
    def is_time(self):
        
        pass",snippet_124,3,"```python
import time

class DelayTimer:
    '''Utility class that allows us to detect when a certain
    amount of time has passed since the timer was started or last reset.'''
    
    def __init__(self, delay: float):
        """"""
        Parameters
        ----------
        delay : float
            The delay in seconds that must elapse before `is_time` returns True.
        """"""
        self.delay = float(delay)
        self._start = time.monotonic()

    def is_time(self) -> bool:
        """"""
        Returns
        -------
        bool
            True if the configured delay has elapsed since the timer was
            created or last reset, False otherwise.
        """"""
        return (time.monotonic() - self._start) >= self.delay

    def reset(self) -> None:
        """"""
        Reset the timer to start counting from the current time.
        """"""
        self._start = time.monotonic()
```"
223977,jilljenn/tryalgo,jilljenn_tryalgo/tryalgo/fenwick.py,tryalgo.fenwick.FenwickMin,"class FenwickMin:
    """"""maintains a tree to allow quick updates and queries
    of a virtual table t
    """"""

    def __init__(self, size):
        """"""stores a table t and allows updates and queries
        of prefix sums in logarithmic time.

        :param size: length of the table
        """"""
        self.s = [float('+inf')] * (size + 1)

    def prefixMin(self, a):
        """"""
        :param int a: index in t, negative a will return infinity
        :returns: min(t[0], ... ,t[a])
        """"""
        i = a + 1
        retval = float('+inf')
        while i > 0:
            retval = min(retval, self.s[i])
            i -= i & -i
        return retval

    def update(self, a, val):
        """"""
        :param int a: index in t
        :param val: a value
        :modifies: sets t[a] to the minimum of t[a] and val
        """"""
        i = a + 1
        while i < len(self.s):
            self.s[i] = min(self.s[i], val)
            i += i & -i","class FenwickMin:
    
    def __init__(self, size):
        
        pass
    def prefixMin(self, a):
        '''
        :param int a: index in t, negative a will return infinity
        :returns: min(t[0], ... ,t[a])
        '''
        pass
    def update(self, a, val):
        '''
        :param int a: index in t
        :param val: a value
        :modifies: sets t[a] to the minimum of t[a] and val
        '''
        pass",snippet_125,4,"```python
import math

class FenwickMin:
    def __init__(self, size):
        """"""
        Create a Fenwick tree that supports prefix minimum queries.
        :param int size: number of elements in the underlying array.
        """"""
        self.n = size
        # 1‑indexed internal array, initialized with +∞
        self.t = [math.inf] * (self.n + 1)

    def prefixMin(self, a):
        """"""
        Return the minimum value in the range [0, a] (inclusive).
        If a is negative, return +∞.
        :param int a: index in the original array (0‑based).
        :returns: minimum value in the prefix.
        """"""
        if a < 0:
            return math.inf
        idx = a + 1  # convert to 1‑based index
        res = math.inf
        while idx > 0:
            res = min(res, self.t[idx])
            idx -= idx & -idx
        return res

    def update(self, a, val):
        """"""
        Update the element at index a to be the minimum of its current
        value and val. This propagates the change to all relevant
        Fenwick tree nodes.
        :param int a: index in the original array (0‑based).
        :param val: value to compare with the current element.
        """"""
        idx = a + 1  # convert to 1‑based index
        while idx <= self.n:
            self.t[idx] = min(self.t[idx], val)
            idx += idx & -idx
```"
223981,jilljenn/tryalgo,jilljenn_tryalgo/tryalgo/lowest_common_ancestor.py,tryalgo.lowest_common_ancestor.LowestCommonAncestorRMQ,"from tryalgo.range_minimum_query import RangeMinQuery

class LowestCommonAncestorRMQ:
    """"""Lowest common ancestor data structure using a reduction to
       range minimum query
    """"""

    def __init__(self, graph):
        """"""builds the structure from a given tree

        :param graph: adjacency matrix of a tree
        :complexity: O(n log n), with n = len(graph)
        """"""
        n = len(graph)
        dfs_trace = []
        self.last = [None] * n
        to_visit = [(0, 0, None)]
        succ = [0] * n
        while to_visit:
            level, node, father = to_visit[-1]
            self.last[node] = len(dfs_trace)
            dfs_trace.append((level, node))
            if succ[node] < len(graph[node]) and graph[node][succ[node]] == father:
                succ[node] += 1
            if succ[node] == len(graph[node]):
                to_visit.pop()
            else:
                neighbor = graph[node][succ[node]]
                succ[node] += 1
                to_visit.append((level + 1, neighbor, node))
        self.rmq = RangeMinQuery(dfs_trace, (float('inf'), None))

    def query(self, u, v):
        """""":returns: the lowest common ancestor of u and v
        :complexity: O(log n)
        """"""
        lu = self.last[u]
        lv = self.last[v]
        if lu > lv:
            lu, lv = (lv, lu)
        return self.rmq.range_min(lu, lv + 1)[1]","
class LowestCommonAncestorRMQ:
    '''Lowest common ancestor data structure using a reduction to
       range minimum query
    '''
    def __init__(self, graph):
        '''builds the structure from a given tree
        :param graph: adjacency matrix of a tree
        :complexity: O(n log n), with n = len(graph)
        '''
        pass
    def query(self, u, v):
        
        pass",snippet_126,3,"```python
import math

class LowestCommonAncestorRMQ:
    '''Lowest common ancestor data structure using a reduction to
       range minimum query
    '''
    def __init__(self, graph):
        '''builds the structure from a given tree
        :param graph: adjacency matrix of a tree
        :complexity: O(n log n), with n = len(graph)
        '''
        n = len(graph)
        # Build adjacency list
        adj = [[] for _ in range(n)]
        for u in range(n):
            for v in range(n):
                if graph[u][v]:
                    adj[u].append(v)

        # Euler tour
        self.euler = []
        self.depth = []
        self.first = [-1] * n
        visited = [False] * n

        def dfs(u, d):
            visited[u] = True
            self.first[u] = len(self.euler)
            self.euler.append(u)
            self.depth.append(d)
            for v in adj[u]:
                if not visited[v]:
                    dfs(v, d + 1)
                    self.euler.append(u)
                    self.depth.append(d)

        dfs(0, 0)  # root at 0

        m = len(self.euler)
        # log table
        self.log = [0] * (m + 1)
        for i in range(2, m + 1):
            self.log[i] = self.log[i // 2] + 1

        k = self.log[m] + 1
        # Sparse table of indices into euler array
        self.st = [[0] * m for _ in range(k)]
        for i in range(m):
            self.st[0][i] = i
        for j in range(1, k):
            span = 1 << j
            half = span >> 1
            for i in range(m - span + 1):
                left = self.st[j - 1][i]
                right = self.st[j - 1][i + half]
                self.st[j][i] = left if self.depth[left] <= self.depth[right] else right

    def query(self, u, v):
        '''returns the lowest common ancestor of nodes u and v'''
        l = self.first[u]
        r = self.first[v]
        if l > r:
            l, r = r, l
        length = r - l + 1
        k = self.log[length]
        left = self.st[k][l]
        right = self.st[k][r - (1 << k) + 1]
        idx = left if self.depth[left] <= self.depth[right] else right
        return self.euler[idx]
```"
223982,jilljenn/tryalgo,jilljenn_tryalgo/tryalgo/lowest_common_ancestor.py,tryalgo.lowest_common_ancestor.LowestCommonAncestorShortcuts,"class LowestCommonAncestorShortcuts:
    """"""Lowest common ancestor data structure using shortcuts to ancestors
    """"""

    def __init__(self, prec):
        """"""builds the structure from a given tree

        :param prec: father for every node, with prec[0] = 0
        :assumes: prec[node] < node
        :complexity: O(n log n), with n = len(nodes)
        """"""
        n = len(prec)
        self.level = [None] * n
        self.level[0] = 0
        for u in range(1, n):
            self.level[u] = 1 + self.level[prec[u]]
        depth = log2ceil(max((self.level[u] for u in range(n)))) + 1
        self.anc = [[0] * n for _ in range(depth)]
        for u in range(n):
            self.anc[0][u] = prec[u]
        for k in range(1, depth):
            for u in range(n):
                self.anc[k][u] = self.anc[k - 1][self.anc[k - 1][u]]

    def query(self, u, v):
        """""":returns: the lowest common ancestor of u and v
        :complexity: O(log n)
        """"""
        if self.level[u] > self.level[v]:
            u, v = (v, u)
        depth = len(self.anc)
        for k in range(depth - 1, -1, -1):
            if self.level[u] <= self.level[v] - (1 << k):
                v = self.anc[k][v]
        assert self.level[u] == self.level[v]
        if u == v:
            return u
        for k in range(depth - 1, -1, -1):
            if self.anc[k][u] != self.anc[k][v]:
                u = self.anc[k][u]
                v = self.anc[k][v]
        assert self.anc[0][u] == self.anc[0][v]
        return self.anc[0][u]","class LowestCommonAncestorShortcuts:
    '''Lowest common ancestor data structure using shortcuts to ancestors
    '''
    def __init__(self, prec):
        '''builds the structure from a given tree
        :param prec: father for every node, with prec[0] = 0
        :assumes: prec[node] < node
        :complexity: O(n log n), with n = len(nodes)
        '''
        pass
    def query(self, u, v):
        
        pass",snippet_127,3,"class LowestCommonAncestorShortcuts:
    '''Lowest common ancestor data structure using shortcuts to ancestors
    '''
    def __init__(self, prec):
        '''builds the structure from a given tree
        :param prec: father for every node, with prec[0] = 0
        :assumes: prec[node] < node
        :complexity: O(n log n), with n = len(nodes)
        '''
        self.n = len(prec)
        self.depth = [0] * self.n
        for i in range(1, self.n):
            self.depth[i] = self.depth[prec[i]] + 1

        # compute maximum power of two needed
        self.LOG = 1
        while (1 << self.LOG) <= self.n:
            self.LOG += 1

        # up[i][k] = 2^k-th ancestor of node i
        self.up = [[0] * self.LOG for _ in range(self.n)]
        for i in range(self.n):
            self.up[i][0] = prec[i]
        for k in range(1, self.LOG):
            for i in range(self.n):
                self.up[i][k] = self.up[self.up[i][k-1]][k-1]

    def query(self, u, v):
        '''returns the lowest common ancestor of nodes u and v'''
        if self.depth[u] < self.depth[v]:
            u, v = v, u
        # lift u up to depth of v
        diff = self.depth[u] - self.depth[v]
        bit = 0
        while diff:
            if diff & 1:
                u = self.up[u][bit]
            diff >>= 1
            bit += 1
        if u == v:
            return u
        # lift both up together
        for k in range(self.LOG-1, -1, -1):
            if self.up[u][k] != self.up[v][k]:
                u = self.up[u][k]
                v = self.up[v][k]
        return self.up[u][0]
"
223988,jilljenn/tryalgo,jilljenn_tryalgo/tryalgo/partition_refinement.py,tryalgo.partition_refinement.PartitionRefinement,"class PartitionRefinement:
    """"""This data structure implements an order preserving
    partition with refinements.
    """"""

    def __init__(self, n):
        """"""Start with the partition consisting of the unique class {0,1,..,n-1}
        complexity: O(n) both in time and space
        """"""
        c = PartitionClass()
        self.classes = c
        self.items = [PartitionItem(i, c) for i in range(n)]

    def refine(self, pivot):
        """"""Split every class C in the partition into C intersection pivot
        and C setminus pivot complexity: linear in size of pivot
        """"""
        has_split = []
        for i in pivot:
            if 0 <= i < len(self.items):
                x = self.items[i]
                c = x.theclass
                if not c.split:
                    c.split = PartitionClass(c)
                    if self.classes is c:
                        self.classes = c.split
                    has_split.append(c)
                x.remove()
                x.theclass = c.split
                c.split.append(x)
        for c in has_split:
            c.split = None
            if not c.items:
                c.remove()
                del c

    def tolist(self):
        """"""produce a list representation of the partition
        """"""
        return [[x.val for x in theclass.items] for theclass in self.classes]

    def order(self):
        """"""Produce a flatten list of the partition, ordered by classes
        """"""
        return [x.val for theclass in self.classes for x in theclass.items]","class PartitionRefinement:
    
    def __init__(self, n):
        
        pass
    def refine(self, pivot):
        '''Split every class C in the partition into C intersection pivot
        and C setminus pivot complexity: linear in size of pivot
        '''
        pass
    def tolist(self):
        
        pass
    def order(self):
        
        pass",snippet_128,5,"class PartitionRefinement:
    def __init__(self, n):
        # Initially one class containing all elements 0..n-1
        self.partition = [list(range(n))]
        self.elem_to_class = {i: 0 for i in range(n)}

    def refine(self, pivot):
        """"""Split every class C in the partition into C∩pivot and C\\pivot.
        Complexity is linear in the size of pivot.
        """"""
        pivot = list(pivot)
        if not pivot:
            return

        # Group pivot elements by the class they belong to
        class_to_pivots = {}
        for e in pivot:
            c = self.elem_to_class[e]
            class_to_pivots.setdefault(c, []).append(e)

        # For each class that intersects pivot, perform the split
        for c, pivots in class_to_pivots.items():
            if len(piv"
224105,rigetti/grove,rigetti_grove/grove/alpha/fermion_transforms/fenwick_tree.py,grove.alpha.fermion_transforms.fenwick_tree.FenwickNode,"class FenwickNode:
    """"""Fenwick Tree node.""""""
    parent = None
    children = None
    index = None

    def __init__(self, parent, children, index=None):
        """"""Fenwick Tree node. Single parent and multiple children.

        :param FenwickNode parent: a parent node
        :param list(FenwickNode) children: a list of children nodes
        :param int index: node label
        """"""
        self.children = children
        self.parent = parent
        self.index = index

    def get_ancestors(self):
        """"""Returns a list of ancestors of the node. Ordered from the earliest.

        :return: node's ancestors, ordered from most recent
        :rtype: list(FenwickNode)
        """"""
        node = self
        ancestor_list = []
        while node.parent is not None:
            ancestor_list.append(node.parent)
            node = node.parent
        return ancestor_list","class FenwickNode:
    '''Fenwick Tree node.'''
    def __init__(self, parent, children, index=None):
        '''Fenwick Tree node. Single parent and multiple children.
        :param FenwickNode parent: a parent node
        :param list(FenwickNode) children: a list of children nodes
        :param int index: node label
        '''
        pass
    def get_ancestors(self):
        
        pass",snippet_129,3,"class FenwickNode:
    '''Fenwick Tree node.'''
    def __init__(self, parent, children, index=None):
        '''Fenwick Tree node. Single parent and multiple children.
        :param FenwickNode parent: a parent node
        :param list(FenwickNode) children: a list of children nodes
        :param int index: node label
        '''
        self.parent = parent
        self.children = children if children is not None else []
        self.index = index

    def get_ancestors(self):
        '''Return a list of ancestor nodes from the immediate parent up to the root.'''
        ancestors = []
        node = self.parent
        while node is not None:
            ancestors.append(node)
            node = node.parent
        return ancestors
"
227838,econ-ark/HARK,HARK/utilities.py,HARK.utilities.NullFunc,"import numpy as np

class NullFunc:
    """"""
    A trivial class that acts as a placeholder ""do nothing"" function.
    """"""

    def __call__(self, *args):
        """"""
        Returns meaningless output no matter what the input(s) is.  If no input,
        returns None.  Otherwise, returns an array of NaNs (or a single NaN) of
        the same size as the first input.
        """"""
        if len(args) == 0:
            return None
        else:
            arg = args[0]
            if hasattr(arg, 'shape'):
                return np.zeros_like(arg) + np.nan
            else:
                return np.nan

    def distance(self, other):
        """"""
        Trivial distance metric that only cares whether the other object is also
        an instance of NullFunc.  Intentionally does not inherit from HARKobject
        as this might create dependency problems.

        Parameters
        ----------
        other : any
            Any object for comparison to this instance of NullFunc.

        Returns
        -------
        (unnamed) : float
            The distance between self and other.  Returns 0 if other is also a
            NullFunc; otherwise returns an arbitrary high number.
        """"""
        try:
            if other.__class__ is self.__class__:
                return 0.0
            else:
                return 1000.0
        except:
            return 10000.0","
class NullFunc:
    
    def __call__(self, *args):
        
        pass
    def distance(self, other):
        '''
        Trivial distance metric that only cares whether the other object is also
        an instance of NullFunc.  Intentionally does not inherit from HARKobject
        as this might create dependency problems.
        Parameters
        ----------
        other : any
            Any object for comparison to this instance of NullFunc.
        Returns
        -------
        (unnamed) : float
            The distance between self and other.  Returns 0 if other is also a
            NullFunc; otherwise returns an arbitrary high number.
        '''
        pass",snippet_131,3,"class NullFunc:
    def __call__(self, *args):
        """"""Do nothing and return None.""""""
        return None

    def distance(self, other):
        """"""
        Trivial distance metric that only cares whether the other object is also
        an instance of NullFunc.  Intentionally does not inherit from HARKobject
        as this might create dependency problems.
        Parameters
        ----------
        other : any
            Any object for comparison to this instance of NullFunc.
        Returns
        -------
        float
            The distance between self and other.  Returns 0 if other is also a
            NullFunc; otherwise returns an arbitrary high number.
        """"""
        if isinstance(other, NullFunc):
            return 0.0
        return float('inf')"
228145,jcrobak/parquet-python,jcrobak_parquet-python/parquet/__init__.py,parquet.JsonWriter,"import json

class JsonWriter:
    """"""Utility for dumping rows as JSON objects.""""""

    def __init__(self, out):
        """"""Initialize with output destination.""""""
        self._out = out

    def writerow(self, row):
        """"""Write a single row.""""""
        json_text = json.dumps(row)
        if isinstance(json_text, bytes):
            json_text = json_text.decode('utf-8')
        self._out.write(json_text)
        self._out.write(u'\n')","
class JsonWriter:
    '''Utility for dumping rows as JSON objects.'''
    def __init__(self, out):
        
        pass
    def writerow(self, row):
        '''Write a single row.'''
        pass",snippet_132,3,"```python
import json

class JsonWriter:
    '''Utility for dumping rows as JSON objects.'''
    def __init__(self, out):
        self.out = out

    def writerow(self, row):
        '''Write a single row.'''
        self.out.write(json.dumps(row))
        self.out.write('\n')
```"
228682,common-workflow-language/cwltool,common-workflow-language_cwltool/cwltool/software_requirements.py,cwltool.software_requirements.DependenciesConfiguration,"import os
from typing import TYPE_CHECKING, Any, Optional, Union, cast
import argparse

class DependenciesConfiguration:
    """"""Dependency configuration class, for RuntimeContext.job_script_provider.""""""

    def __init__(self, args: argparse.Namespace) -> None:
        """"""Initialize.""""""
        self.tool_dependency_dir: Optional[str] = None
        self.dependency_resolvers_config_file: Optional[str] = None
        conf_file = getattr(args, 'beta_dependency_resolvers_configuration', None)
        tool_dependency_dir = getattr(args, 'beta_dependencies_directory', None)
        conda_dependencies = getattr(args, 'beta_conda_dependencies', None)
        if conf_file is not None and os.path.exists(conf_file):
            self.use_tool_dependencies = True
            if tool_dependency_dir is None:
                tool_dependency_dir = os.path.abspath(os.path.dirname(conf_file))
            self.tool_dependency_dir = tool_dependency_dir
            self.dependency_resolvers_config_file = os.path.abspath(conf_file)
        elif conda_dependencies is not None:
            if tool_dependency_dir is None:
                tool_dependency_dir = os.path.abspath('./cwltool_deps')
            self.tool_dependency_dir = tool_dependency_dir
            self.use_tool_dependencies = True
            self.dependency_resolvers_config_file = None
        else:
            self.use_tool_dependencies = False
        if self.tool_dependency_dir and (not os.path.exists(self.tool_dependency_dir)):
            os.makedirs(self.tool_dependency_dir)

    def build_job_script(self, builder: 'Builder', command: list[str]) -> str:
        """"""Use the galaxy-tool-util library to construct a build script.""""""
        ensure_galaxy_lib_available()
        resolution_config_dict = {'use': self.use_tool_dependencies, 'default_base_path': self.tool_dependency_dir}
        app_config = {'conda_auto_install': True, 'conda_auto_init': True, 'debug': builder.debug}
        tool_dependency_manager: 'deps.DependencyManager' = deps.build_dependency_manager(app_config_dict=app_config, resolution_config_dict=resolution_config_dict, conf_file=self.dependency_resolvers_config_file)
        handle_dependencies: str = ''
        if (dependencies := get_dependencies(builder)):
            handle_dependencies = '\n'.join(tool_dependency_manager.dependency_shell_commands(dependencies, job_directory=builder.tmpdir))
        template_kwds: dict[str, str] = dict(handle_dependencies=handle_dependencies)
        job_script = COMMAND_WITH_DEPENDENCIES_TEMPLATE.substitute(template_kwds)
        return job_script","
class DependenciesConfiguration:
    '''Dependency configuration class, for RuntimeContext.job_script_provider.'''
    def __init__(self, args: argparse.Namespace) -> None:
        '''Initialize.'''
        pass
    def build_job_script(self, builder: 'Builder', command: list[str]) -> str:
        
        pass",snippet_133,3,"```python
import argparse
from typing import List, Dict, Any


class DependenciesConfiguration:
    """"""Dependency configuration class, for RuntimeContext.job_script_provider.""""""

    def __init__(self, args: argparse.Namespace) -> None:
        """"""Initialize with parsed command‑line arguments.""""""
        # Store the raw args for potential future use
        self.args: argparse.Namespace = args

        # Extract dependencies and environment variables if provided.
        # These attributes are optional; default to empty list/dict.
        self.dependencies: List[str] = getattr(args, ""dependencies"", [])
        self.env_vars: Dict[str, str] = getattr(args, ""env"", {})

    def build_job_script(self, builder: Any, command: List[str]) -> str:
        """"""
        Build a job script that sets up the environment, copies dependencies,
        and executes the given command.

        Parameters
        ----------
        builder : Any
            An object that may provide an ``add_line`` method and a ``build`` method.
            If such methods are not present, the script is returned as a plain string.
        command : List[str]
            The command to run in the job script.

        Returns
        -------
        str
            The complete job script as a string.
        """"""
        # Helper to escape shell arguments
        def _escape(arg: str) -> str:
            return arg.replace('""', '\\""')

        # Start building the script lines
        script_lines: List[str] = [
            ""#!/usr/bin/env bash"",
            ""set -e"",
        ]

        # Export environment variables
        for key, value in self.env_vars.items():
            script_lines.append(f'export {key}=""{_escape(value)}""')

        # Copy dependencies into the working directory
        for dep in self.dependencies:
            script_lines.append(f'cp -r ""{_escape(dep)}"" .')

        # Append the actual command
        script_lines.append("" "".join(command))

        # If the builder supports incremental construction, use it
        if hasattr(builder, ""add_line"") and hasattr(builder, ""build""):
            for line in script_lines:
                builder.add_line(line)
            return builder.build()

        # Fallback: return the script as a single string
        return ""\n"".join(script_lines)
```"
229565,sourceperl/pyModbusTCP,sourceperl_pyModbusTCP/examples/client_serial_gw.py,client_serial_gw.Serial2ModbusClient,"from pyModbusTCP.constants import EXP_GATEWAY_TARGET_DEVICE_FAILED_TO_RESPOND
import struct

class Serial2ModbusClient:
    """""" Customize a slave serial worker for map a modbus TCP client. """"""

    def __init__(self, serial_w, mbus_cli, slave_addr=1, allow_bcast=False):
        """"""Serial2ModbusClient constructor.

        :param serial_w: a SlaveSerialWorker instance
        :type serial_w: SlaveSerialWorker
        :param mbus_cli: a ModbusClient instance
        :type mbus_cli: ModbusClient
        :param slave_addr: modbus slave address
        :type slave_addr: int
        :param allow_bcast: allow processing broadcast frames (slave @0)
        :type allow_bcast: bool
        """"""
        self.serial_w = serial_w
        self.mbus_cli = mbus_cli
        self.slave_addr = slave_addr
        self.allow_bcast = allow_bcast
        self.serial_w.handle_request = self._handle_request

    def _handle_request(self):
        """"""Request handler for SlaveSerialWorker""""""
        if self.serial_w.request.slave_addr == 0 and self.allow_bcast:
            self.mbus_cli.custom_request(self.serial_w.request.pdu)
        elif self.serial_w.request.slave_addr == self.slave_addr:
            resp_pdu = self.mbus_cli.custom_request(self.serial_w.request.pdu)
            if resp_pdu:
                self.serial_w.response.build(raw_pdu=resp_pdu, slave_addr=self.serial_w.request.slave_addr)
            else:
                exp_pdu = struct.pack('BB', self.serial_w.request.function_code + 128, EXP_GATEWAY_TARGET_DEVICE_FAILED_TO_RESPOND)
                self.serial_w.response.build(raw_pdu=exp_pdu, slave_addr=self.serial_w.request.slave_addr)

    def run(self):
        """"""Start serial processing.""""""
        self.serial_w.run()","
class Serial2ModbusClient:
    
    def __init__(self, serial_w, mbus_cli, slave_addr=1, allow_bcast=False):
        
        pass
    def _handle_request(self):
        '''Request handler for SlaveSerialWorker'''
        pass
    def run(self):
        
        pass",snippet_134,4,"```python
import time
import threading

class Serial2ModbusClient:
    """"""
    A simple bridge that forwards Modbus RTU frames received on a serial
    interface to a Modbus client and writes back the response.
    """"""

    def __init__(self, serial_w, mbus_cli, slave_addr=1, allow_bcast=False):
        """"""
        Parameters
        ----------
        serial_w : object
            An object that provides `read()` and `write(data)` methods.
            `read()` should return a bytes object containing a complete
            Modbus RTU frame or an empty bytes object if nothing is
            available.
        mbus_cli : object
            An object that provides a `send(request)` method which
            accepts a Modbus RTU frame (bytes) and returns the
            corresponding response frame (bytes).
        slave_addr : int, optional
            The Modbus slave address that this bridge will respond to.
            Default is 1.
        allow_bcast : bool, optional
            If True, broadcast frames (address 0) will be forwarded to
            the Modbus client. Default is False.
        """"""
        self.serial_w = serial_w
        self.mbus_cli = mbus_cli
        self.slave_addr = slave_addr & 0xFF
        self.allow_bcast = bool(allow_bcast)
        self._running = False
        self._thread = None

    def _handle_request(self):
        """"""
        Read a request from the serial worker, forward it to the Modbus
        client if the address matches, and write back the response.
        """"""
        try:
            request = self.serial_w.read()
        except Exception:
            # If read fails, ignore and continue
            return

        if not request:
            return

        # Basic sanity check: at least address + function + CRC
        if len(request) < 4:
            return

        addr = request[0]
        # Check if this frame is for us or a broadcast (if allowed)
        if addr != self.slave_addr and not (self.allow_bcast and addr == 0x00):
            return

        try:
            response = self.mbus_cli.send(request)
        except Exception:
            # If the client fails, we do not send a response
            return

        if response:
            try:
                self.serial_w.write(response)
            except Exception:
                # Ignore write errors
                pass

    def run(self):
        """"""
        Start the bridge loop in a separate thread. The loop will keep
        running until `stop()` is called.
        """"""
        if self._running:
            return  # Already running

        self._running = True

        def _loop():
            while self._running:
                self._handle_request()
                # Small sleep to avoid busy waiting if no data
                time.sleep(0.01)

        self._thread = threading.Thread(target=_loop, daemon=True)
        self._thread.start()

    def stop(self):
        """"""
        Stop the bridge loop and wait for the thread to finish.
        """"""
        self._running = False
        if self._thread:
            self._thread.join()
            self._thread = None
```"
230113,aws/aws-xray-sdk-python,aws_aws-xray-sdk-python/aws_xray_sdk/core/models/noop_traceid.py,aws_aws-xray-sdk-python.aws_xray_sdk.core.models.noop_traceid.NoOpTraceId,"class NoOpTraceId:
    """"""
    A trace ID tracks the path of a request through your application.
    A trace collects all the segments generated by a single request.
    A trace ID is required for a segment.
    """"""
    VERSION = '1'
    DELIMITER = '-'

    def __init__(self):
        """"""
        Generate a no-op trace id.
        """"""
        self.start_time = '00000000'
        self.__number = '000000000000000000000000'

    def to_id(self):
        """"""
        Convert TraceId object to a string.
        """"""
        return '%s%s%s%s%s' % (NoOpTraceId.VERSION, NoOpTraceId.DELIMITER, self.start_time, NoOpTraceId.DELIMITER, self.__number)","class NoOpTraceId:
    
    def __init__(self):
        
        pass
    def to_id(self):
        '''
        Convert TraceId object to a string.
        '''
        pass",snippet_135,3,"class NoOpTraceId:
    """"""
    A placeholder trace ID that represents the absence of a real trace identifier.
    """"""

    def __init__(self):
        # No real ID is stored; use an empty string to signify ""no operation"".
        self._id = """"

    def to_id(self):
        """"""
        Convert TraceId object to a string.

        Returns:
            str: An empty string indicating that no trace ID is present.
        """"""
        return self._id
"
230120,aws/aws-xray-sdk-python,aws_aws-xray-sdk-python/aws_xray_sdk/core/models/traceid.py,aws_aws-xray-sdk-python.aws_xray_sdk.core.models.traceid.TraceId,"import binascii
import os
import time

class TraceId:
    """"""
    A trace ID tracks the path of a request through your application.
    A trace collects all the segments generated by a single request.
    A trace ID is required for a segment.
    """"""
    VERSION = '1'
    DELIMITER = '-'

    def __init__(self):
        """"""
        Generate a random trace id.
        """"""
        self.start_time = int(time.time())
        self.__number = binascii.b2a_hex(os.urandom(12)).decode('utf-8')

    def to_id(self):
        """"""
        Convert TraceId object to a string.
        """"""
        return '%s%s%s%s%s' % (TraceId.VERSION, TraceId.DELIMITER, format(self.start_time, 'x'), TraceId.DELIMITER, self.__number)","
class TraceId:
    
    def __init__(self):
        '''
        Generate a random trace id.
        '''
        pass
    def to_id(self):
        '''
        Convert TraceId object to a string.
        '''
        pass",snippet_136,3,"```python
import uuid

class TraceId:
    """"""
    A simple trace identifier that generates a random UUID4 value.
    """"""

    def __init__(self):
        """"""
        Generate a random trace id.
        """"""
        self._id = uuid.uuid4()

    def to_id(self):
        """"""
        Convert TraceId object to a string.
        """"""
        return str(self._id)
```"
230123,aws/aws-xray-sdk-python,aws_aws-xray-sdk-python/aws_xray_sdk/core/sampling/local/reservoir.py,aws_aws-xray-sdk-python.aws_xray_sdk.core.sampling.local.reservoir.Reservoir,"import time
import threading

class Reservoir:
    """"""
    Keeps track of the number of sampled segments within
    a single second. This class is implemented to be
    thread-safe to achieve accurate sampling.
    """"""

    def __init__(self, traces_per_sec=0):
        """"""
        :param int traces_per_sec: number of guranteed
            sampled segments.
        """"""
        self._lock = threading.Lock()
        self.traces_per_sec = traces_per_sec
        self.used_this_sec = 0
        self.this_sec = int(time.time())

    def take(self):
        """"""
        Returns True if there are segments left within the
        current second, otherwise return False.
        """"""
        with self._lock:
            now = int(time.time())
            if now != self.this_sec:
                self.used_this_sec = 0
                self.this_sec = now
            if self.used_this_sec >= self.traces_per_sec:
                return False
            self.used_this_sec = self.used_this_sec + 1
            return True","
class Reservoir:
    '''
    Keeps track of the number of sampled segments within
    a single second. This class is implemented to be
    thread-safe to achieve accurate sampling.
    '''
    def __init__(self, traces_per_sec=0):
        
        pass
    def take(self):
        
        pass",snippet_137,3,"```python
import time
import threading


class Reservoir:
    '''
    Keeps track of the number of sampled segments within
    a single second. This class is implemented to be
    thread-safe to achieve accurate sampling.
    '''
    def __init__(self, traces_per_sec=0):
        self.traces_per_sec = traces_per_sec
        self._lock = threading.Lock()
        self._last_second = int(time.time())
        self._count = 0

    def take(self):
        """"""
        Attempt to take a sample. Returns True if the sample
        is allowed under the per‑second limit, otherwise False.
        """"""
        with self._lock:
            now = int(time.time())
            if now != self._last_second:
                self._last_second = now
                self._count = 0

            if self.traces_per_sec <= 0:
                # Unlimited sampling
                return True

            if self._count < self.traces_per_sec:
                self._count += 1
                return True

            return False
```"
230719,django-salesforce/django-salesforce,django-salesforce_django-salesforce/salesforce/backend/indep.py,salesforce.backend.indep.LazyField,"from typing import Any, Callable, Dict, Tuple, Type
from inspect import signature

class LazyField:
    """"""A Field that can be later customized until it is binded to the final Model""""""
    counter = 0

    def __init__(self, klass: 'Type[Field[Any, Any]]') -> None:
        """"""Instantiate the field type""""""
        self.klass = klass
        self.kw = {}
        self.args = ()
        self.called = False
        self.counter = self.counter

    def __call__(self, *args: Any, **kwargs: Any) -> 'LazyField':
        """"""Instantiate a new field with options""""""
        assert not self.called
        bound_args = signature(self.klass.__init__).bind(self, *args, **kwargs)
        obj = type(self)(self.klass)
        obj.args = bound_args.args[1:]
        obj.kw = bound_args.kwargs
        setattr(type(self), 'counter', getattr(type(self), 'counter') + 1)
        return obj

    def update(self, **kwargs: Any) -> 'LazyField':
        """"""Customize the lazy field""""""
        assert not self.called
        self.kw.update(kwargs)
        return self

    def create(self) -> 'Field[Any, Any]':
        """"""Create a normal field from the lazy field""""""
        assert not self.called
        return self.klass(*self.args, **self.kw)","
class LazyField:
    
    def __init__(self, klass: 'Type[Field[Any, Any]]') -> None:
        
        pass
    def __call__(self, *args: Any, **kwargs: Any) -> 'LazyField':
        '''Instantiate a new field with options'''
        pass
    def update(self, **kwargs: Any) -> 'LazyField':
        '''Customize the lazy field'''
        pass
    def create(self) -> 'Field[Any, Any]':
        
        pass",snippet_139,5,"```python
from typing import Any, Type, Dict, Tuple

class LazyField:
    """"""
    A lightweight wrapper that defers the creation of a Field instance until
    `create()` is called. It allows specifying positional and keyword arguments
    via the call syntax and further customization via `update()`.
    """"""

    def __init__(self, klass: Type[Any]) -> None:
        """"""
        Initialize the lazy field with the target Field class.

        Parameters
        ----------
        klass : Type[Any]
            The class that will be instantiated when `create()` is called.
        """"""
        self._klass: Type[Any] = klass
        self._args: Tuple[Any, ...] = ()
        self._kwargs: Dict[str, Any] = {}

    def __call__(self, *args: Any, **kwargs: Any) -> ""LazyField"":
        """"""
        Instantiate a new lazy field with the given positional and keyword
        arguments. This does not create the actual Field instance yet.

        Returns
        -------
        LazyField
            A new LazyField instance configured with the supplied arguments.
        """"""
        new = LazyField(self._klass)
        new._args = args
        new._kwargs = kwargs
        return new

    def update(self, **kwargs: Any) -> ""LazyField"":
        """"""
        Update the keyword arguments of the lazy field. This method mutates the
        current instance and returns it for chaining.

        Parameters
        ----------
        **kwargs : Any
            Keyword arguments to merge into the existing configuration.

        Returns
        -------
        LazyField
            The updated LazyField instance.
        """"""
        self._kwargs.update(kwargs)
        return self

    def create(self) -> Any:
        """"""
        Create the actual Field instance using the stored class, positional
        arguments, and keyword arguments.

        Returns
        -------
        Any
            An instance of the target Field class.
        """"""
        return self._klass(*self._args, **self._kwargs)
```"
231223,weblyzard/inscriptis,benchmarking/run_benchmarking.py,run_benchmarking.AbstractHtmlConverter,"from time import time

class AbstractHtmlConverter:
    """"""
    An abstract HTML convert class.
    """"""

    def get_text(self, html):
        """"""
        Returns:
            a text representation of the given HTML snippet.
        """"""
        raise NotImplementedError

    def benchmark(self, html):
        """"""
        Benchmarks the classes HTML to text converter.

        Returns:
            A tuple of the required time and the obtained text representation.
        """"""
        start_time = time()
        for _ in range(TRIES):
            text = self.get_text(html)
        return (time() - start_time, text)","
class AbstractHtmlConverter:
    
    def get_text(self, html):
        '''
        Returns:
            a text representation of the given HTML snippet.
        '''
        pass
    def benchmark(self, html):
        
        pass",snippet_140,3,"```python
import time
from html.parser import HTMLParser

class AbstractHtmlConverter:
    """"""
    A simple HTML to text converter with benchmarking support.
    """"""

    class _TextExtractor(HTMLParser):
        """"""
        Internal HTML parser that collects text data.
        """"""
        def __init__(self):
            super().__init__()
            self._parts = []

        def handle_data(self, data):
            self._parts.append(data)

        def get_text(self):
            return ''.join(self._parts)

    def get_text(self, html):
        """"""
        Convert an HTML snippet into plain text.

        Parameters
        ----------
        html : str
            The HTML snippet to convert.

        Returns
        -------
        str
            The extracted text.
        """"""
        parser = self._TextExtractor()
        parser.feed(html)
        parser.close()
        return parser.get_text()

    def benchmark(self, html):
        """"""
        Measure the time taken to convert the given HTML snippet to text.

        Parameters
        ----------
        html : str
            The HTML snippet to convert.

        Returns
        -------
        dict
            A dictionary containing:
                - 'text': the extracted text,
                - 'elapsed': time in seconds taken for conversion.
        """"""
        start = time.perf_counter()
        text = self.get_text(html)
        elapsed = time.perf_counter() - start
        return {'text': text, 'elapsed': elapsed}
```"
236113,richardchien/python-aiocqhttp,richardchien_python-aiocqhttp/aiocqhttp/api.py,aiocqhttp.api.Api,"from typing import Callable, Any, Union, Awaitable
import functools
import abc

class Api:
    """"""
    API 接口类。

    继承此类的具体实现类应实现 `call_action` 方法。
    """"""

    @abc.abstractmethod
    def call_action(self, action: str, **params) -> Union[Awaitable[Any], Any]:
        """"""
        调用 OneBot API，`action` 为要调用的 API 动作名，`**params`
        为 API 所需参数。

        根据实现类的不同，此函数可能是异步也可能是同步函数。
        """"""
        pass

    def __getattr__(self, item: str) -> Callable[..., Union[Awaitable[Any], Any]]:
        """"""获取一个可调用对象，用于调用对应 API。""""""
        return functools.partial(self.call_action, item)","
class Api:
    
    @abc.abstractmethod
    def call_action(self, action: str, **params) -> Union[Awaitable[Any], Any]:
        '''
        调用 OneBot API，`action` 为要调用的 API 动作名，`**params`
        为 API 所需参数。
        根据实现类的不同，此函数可能是异步也可能是同步函数。
        '''
        pass
    def __getattr__(self, item: str) -> Callable[..., Union[Awaitable[Any], Any]]:
        
        pass",snippet_141,3,"```python
import abc
from typing import Any, Awaitable, Callable, Union


class Api(abc.ABC):
    """"""
    Base class for OneBot API wrappers.

    Subclasses must implement :meth:`call_action`.  The :meth:`__getattr__`
    method automatically creates a callable for any API action name.
    """"""

    @abc.abstractmethod
    def call_action(
        self, action: str, **params
    ) -> Union[Awaitable[Any], Any]:
        """"""
        调用 OneBot API，`action` 为要调用的 API 动作名，`**params`
        为 API 所需参数。
        根据实现类的不同，此函数可能是异步也可能是同步函数。
        """"""
        pass

    def __getattr__(self, item: str) -> Callable[..., Union[Awaitable[Any], Any]]:
        """"""
        Dynamically create a method for any API action.

        Example:
            api.get_status()  # calls api.call_action('get_status')
        """"""
        def _action(*args, **kwargs):
            # OneBot APIs are keyword‑only; positional args are ignored.
            return self.call_action(item, **kwargs)

        return _action
```"
237136,project-generator/project_generator,project-generator_project_generator/project_generator/project.py,project_generator.project.ProjectTemplate,"class ProjectTemplate:
    """""" Public data which can be set in yaml files
        Yaml data available are:
            'build_dir' : build_dir,    # Build output path
            'debugger' : debugger,      # Debugger
            'export_dir': '',           # Export directory path
            'includes': [],             # include paths
            'linker_file': None,        # linker script file
            'name': name,               # project name
            'macros': [],               # macros
            'misc': {},                 # misc settings related to tools
            'output_type': output_type, # output type, default - exe
            'sources': [],              # source files/folders
            'target': '',               # target
            'template' : [],            # tool template
            'tools_supported': [],      # Tools which are supported,
    """"""

    @staticmethod
    def _get_common_data_template():
        """""" Data for tool specific """"""
        data_template = {'includes': [], 'linker_file': '', 'macros': [], 'sources': []}
        return data_template

    @staticmethod
    def _get_tool_specific_data_template():
        """""" Data for tool specific """"""
        data_template = {'misc': {}, 'template': []}
        return data_template

    @staticmethod
    def get_project_template(name='Default', output_type='exe', debugger=None, build_dir='build'):
        """""" Project data (+ data) """"""
        project_template = {'build_dir': build_dir, 'debugger': debugger, 'export_dir': '', 'name': name, 'output_type': output_type, 'target': '', 'tools_supported': []}
        project_template.update(ProjectTemplate._get_common_data_template())
        project_template.update(ProjectTemplate._get_tool_specific_data_template())
        return project_template","class ProjectTemplate:
    ''' Public data which can be set in yaml files
        Yaml data available are:
            'build_dir' : build_dir,    # Build output path
            'debugger' : debugger,      # Debugger
            'export_dir': '',           # Export directory path
            'includes': [],             # include paths
            'linker_file': None,        # linker script file
            'name': name,               # project name
            'macros': [],               # macros
            'misc': {},                 # misc settings related to tools
            'output_type': output_type, # output type, default - exe
            'sources': [],              # source files/folders
            'target': '',               # target
            'template' : [],            # tool template
            'tools_supported': [],      # Tools which are supported,
    '''
    @staticmethod
    def _get_common_data_template():
        ''' Data for tool specific '''
        pass
    @staticmethod
    def _get_tool_specific_data_template():
        
        pass
    @staticmethod
    def get_project_template(name='Default', output_type='exe', debugger=None, build_dir='build'):
        ''' Project data (+ data) '''
        pass",snippet_142,4,"```python
class ProjectTemplate:
    ''' Public data which can be set in yaml files
        Yaml data available are:
            'build_dir' : build_dir,    # Build output path
            'debugger' : debugger,      # Debugger
            'export_dir': '',           # Export directory path
            'includes': [],             # include paths
            'linker_file': None,        # linker script file
            'name': name,               # project name
            'macros': [],               # macros
            'misc': {},                 # misc settings related to tools
            'output_type': output_type, # output type, default - exe
            'sources': [],              # source files/folders
            'target': '',               # target
            'template' : [],            # tool template
            'tools_supported': [],      # Tools which are supported,
    '''
    @staticmethod
    def _get_common_data_template():
        ''' Data for common project configuration '''
        return {
            'build_dir': '',
            'debugger': None,
            'export_dir': '',
            'includes': [],
            'linker_file': None,
            'name': '',
            'macros': [],
            'misc': {},
            'output_type': 'exe',
            'sources': [],
            'target': '',
            'template': [],
            'tools_supported': []
        }

    @staticmethod
    def _get_tool_specific_data_template():
        ''' Data for tool specific configuration '''
        # Placeholder for tool specific keys; can be extended by subclasses
        return {}

    @staticmethod
    def get_project_template(name='Default', output_type='exe', debugger=None, build_dir='build'):
        ''' Project data (+ data) '''
        common = ProjectTemplate._get_common_data_template()
        tool_specific = ProjectTemplate._get_tool_specific_data_template()

        # Override defaults with provided arguments
        common.update({
            'name': name,
            'output_type': output_type,
            'debugger': debugger,
            'build_dir': build_dir
        })

        # Merge common and tool specific data
        project_template = {**common, **tool_specific}
        return project_template
```"
237503,astropy/photutils,photutils/psf/model_plotting.py,photutils.psf.model_plotting.ModelGridPlotMixin,"from astropy.utils import minversion
import numpy as np
from astropy.visualization import simple_norm
import astropy

class ModelGridPlotMixin:
    """"""
    Mixin class to plot a grid of ePSF models.
    """"""

    def _reshape_grid(self, data):
        """"""
        Reshape the 3D ePSF grid as a 2D array of horizontally and
        vertically stacked ePSFs.

        Parameters
        ----------
        data : `numpy.ndarray`
            The 3D array of ePSF data.

        Returns
        -------
        reshaped_data : `numpy.ndarray`
            The 2D array of ePSF data.
        """"""
        nypsfs = self._ygrid.shape[0]
        nxpsfs = self._xgrid.shape[0]
        ny, nx = self.data.shape[1:]
        data.shape = (nypsfs, nxpsfs, ny, nx)
        return data.transpose([0, 2, 1, 3]).reshape(nypsfs * ny, nxpsfs * nx)

    def plot_grid(self, *, ax=None, vmax_scale=None, peak_norm=False, deltas=False, cmap='viridis', dividers=True, divider_color='darkgray', divider_ls='-', figsize=None):
        """"""
        Plot the grid of ePSF models.

        Parameters
        ----------
        ax : `matplotlib.axes.Axes` or `None`, optional
            The matplotlib axes on which to plot. If `None`, then the
            current `~matplotlib.axes.Axes` instance is used.

        vmax_scale : float, optional
            Scale factor to apply to the image stretch limits. This
            value is multiplied by the peak ePSF value to determine the
            plotting ``vmax``. The defaults are 1.0 for plotting the
            ePSF data and 0.03 for plotting the ePSF difference data
            (``deltas=True``). If ``deltas=True``, the ``vmin`` is set
            to ``-vmax``. If ``deltas=False`` the ``vmin`` is set to
            ``vmax`` / 1e4.

        peak_norm : bool, optional
            Whether to normalize the ePSF data by the peak value. The
            default shows the ePSF flux per pixel.

        deltas : bool, optional
            Set to `True` to show the differences between each ePSF
            and the average ePSF.

        cmap : str or `matplotlib.colors.Colormap`, optional
            The colormap to use. The default is 'viridis'.

        dividers : bool, optional
            Whether to show divider lines between the ePSFs.

        divider_color, divider_ls : str, optional
            Matplotlib color and linestyle options for the divider
            lines between ePSFs. These keywords have no effect unless
            ``show_dividers=True``.

        figsize : (float, float), optional
            The figure (width, height) in inches.

        Returns
        -------
        fig : `matplotlib.figure.Figure`
            The matplotlib figure object. This will be the current
            figure if ``ax=None``. Use ``fig.savefig()`` to save the
            figure to a file.

        Notes
        -----
        This method returns a figure object. If you are using this
        method in a script, you will need to call ``plt.show()`` to
        display the figure. If you are using this method in a Jupyter
        notebook, the figure will be displayed automatically.

        When in a notebook, if you do not store the return value of this
        function, the figure will be displayed twice due to the REPL
        automatically displaying the return value of the last function
        call. Alternatively, you can append a semicolon to the end of
        the function call to suppress the display of the return value.
        """"""
        import matplotlib.pyplot as plt
        from mpl_toolkits.axes_grid1 import make_axes_locatable
        data = self.data.copy()
        if deltas:
            mask = np.zeros(data.shape[0], dtype=bool)
            for i, arr in enumerate(data):
                if np.count_nonzero(arr) == 0:
                    mask[i] = True
            data -= np.mean(data[~mask], axis=0)
            data[mask] = 0.0
        data = self._reshape_grid(data)
        if ax is None:
            if figsize is None and self.meta.get('detector', '') == 'NRCSW':
                figsize = (20, 8)
            fig, ax = plt.subplots(figsize=figsize)
        else:
            fig = plt.gcf()
        if peak_norm and data.max() != 0:
            data /= data.max()
        if deltas:
            if vmax_scale is None:
                vmax_scale = 0.03
            vmax = data.max() * vmax_scale
            vmin = -vmax
            if minversion(astropy, '6.1'):
                norm = simple_norm(data, 'linear', vmin=vmin, vmax=vmax)
            else:
                norm = simple_norm(data, 'linear', min_cut=vmin, max_cut=vmax)
        else:
            if vmax_scale is None:
                vmax_scale = 1.0
            vmax = data.max() * vmax_scale
            vmin = vmax / 10000.0
            if minversion(astropy, '6.1'):
                norm = simple_norm(data, 'log', vmin=vmin, vmax=vmax, log_a=10000.0)
            else:
                norm = simple_norm(data, 'log', min_cut=vmin, max_cut=vmax, log_a=10000.0)
        nypsfs = self._ygrid.shape[0]
        nxpsfs = self._xgrid.shape[0]
        extent = [-0.5, nxpsfs - 0.5, -0.5, nypsfs - 0.5]
        axim = ax.imshow(data, extent=extent, norm=norm, cmap=cmap, origin='lower')
        xticklabels = self._xgrid.astype(int)
        yticklabels = self._ygrid.astype(int)
        if self.meta.get('detector', '') == 'NRCSW':
            xticklabels = list(xticklabels[0:5]) * 4
            yticklabels = list(yticklabels[0:5]) * 2
        ax.set_xticks(np.arange(nxpsfs))
        ax.set_xticklabels(xticklabels)
        ax.set_xlabel('ePSF location in detector X pixels')
        ax.set_yticks(np.arange(nypsfs))
        ax.set_yticklabels(yticklabels)
        ax.set_ylabel('ePSF location in detector Y pixels')
        if dividers:
            for ix in range(nxpsfs - 1):
                ax.axvline(ix + 0.5, color=divider_color, ls=divider_ls)
            for iy in range(nypsfs - 1):
                ax.axhline(iy + 0.5, color=divider_color, ls=divider_ls)
        instrument = self.meta.get('instrument', '')
        if not instrument:
            instrument = self.meta.get('instrume', '')
        detector = self.meta.get('detector', '')
        filtername = self.meta.get('filter', '')
        if isinstance(instrument, (tuple, list, np.ndarray)):
            instrument = instrument[0]
        if isinstance(detector, (tuple, list, np.ndarray)):
            detector = detector[0]
        if isinstance(filtername, (tuple, list, np.ndarray)):
            filtername = filtername[0]
        title = f'{instrument} {detector} {filtername}'
        if title != '':
            title += ' '
        if deltas:
            minus = '−'
            ax.set_title(f'{title}(ePSFs {minus} <ePSF>)')
            if peak_norm:
                label = 'Difference relative to average ePSF peak'
            else:
                label = 'Difference relative to average ePSF values'
        else:
            ax.set_title(f'{title}ePSFs')
            if peak_norm:
                label = 'Scale relative to ePSF peak pixel'
            else:
                label = 'ePSF flux per pixel'
        divider = make_axes_locatable(ax)
        cax_cbar = divider.append_axes('right', size='3%', pad='3%')
        cbar = fig.colorbar(axim, cax=cax_cbar, label=label)
        if not deltas:
            cbar.ax.set_yscale('log')
        if self.meta.get('detector', '') == 'NRCSW':
            nxpsfs = len(self._xgrid)
            nypsfs = len(self._ygrid)
            plt.axhline(nypsfs / 2 - 0.5, color='orange')
            for i in range(1, 4):
                ax.axvline(nxpsfs / 4 * i - 0.5, color='orange')
            det_labels = [['A1', 'A3', 'B4', 'B2'], ['A2', 'A4', 'B3', 'B1']]
            for i in range(2):
                for j in range(4):
                    ax.text(j * nxpsfs / 4 - 0.45, (i + 1) * nypsfs / 2 - 0.55, det_labels[i][j], color='orange', verticalalignment='top', fontsize=12)
        fig.tight_layout()
        return fig","
class ModelGridPlotMixin:
    '''
    Mixin class to plot a grid of ePSF models.
        '''
    def _reshape_grid(self, data):
        '''
        Reshape the 3D ePSF grid as a 2D array of horizontally and
        vertically stacked ePSFs.
        Parameters
        ----------
        data : `numpy.ndarray`
            The 3D array of ePSF data.
        Returns
        -------
        reshaped_data : `numpy.ndarray`
            The 2D array of ePSF data.
        '''
        pass
    def plot_grid(self, *, ax=None, vmax_scale=None, peak_norm=False, deltas=False, cmap='viridis', dividers=True, divider_color='darkgray', divider_ls='-', figsize=None):
        '''
        Plot the grid of ePSF models.
        Parameters
        ----------
        ax : `matplotlib.axes.Axes` or `None`, optional
            The matplotlib axes on which to plot. If `None`, then the
            current `~matplotlib.axes.Axes` instance is used.
        vmax_scale : float, optional
            Scale factor to apply to the image stretch limits. This
            value is multiplied by the peak ePSF value to determine the
            plotting ``vmax``. The defaults are 1.0 for plotting the
            ePSF data and 0.03 for plotting the ePSF difference data
            (``deltas=True``). If ``deltas=True``, the ``vmin`` is set
            to ``-vmax``. If ``deltas=False`` the ``vmin`` is set to
            ``vmax`` / 1e4.
        peak_norm : bool, optional
            Whether to normalize the ePSF data by the peak value. The
            default shows the ePSF flux per pixel.
        deltas : bool, optional
            Set to `True` to show the differences between each ePSF
            and the average ePSF.
        cmap : str or `matplotlib.colors.Colormap`, optional
            The colormap to use. The default is 'viridis'.
        dividers : bool, optional
            Whether to show divider lines between the ePSFs.
        divider_color, divider_ls : str, optional
            Matplotlib color and linestyle options for the divider
            lines between ePSFs. These keywords have no effect unless
            ``show_dividers=True``.
        figsize : (float, float), optional
            The figure (width, height) in inches.
        Returns
        -------
        fig : `matplotlib.figure.Figure`
            The matplotlib figure object. This will be the current
            figure if ``ax=None``. Use ``fig.savefig()`` to save the
            figure to a file.
        Notes
        -----
        This method returns a figure object. If you are using this
        method in a script, you will need to call ``plt.show()`` to
        display the figure. If you are using this method in a Jupyter
        notebook, the figure will be displayed automatically.
        When in a notebook, if you do not store the return value of this
        function, the figure will be displayed twice due to the REPL
        automatically displaying the return value of the last function
        call. Alternatively, you can append a semicolon to the end of
        the function call to suppress the display of the return value.
                        '''
                        pass",snippet_143,3,"```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import colors


class ModelGridPlotMixin:
    """"""
    Mixin class to plot a grid of ePSF models.
    """"""

    def _reshape_grid(self, data):
        """"""
        Reshape the 3D ePSF grid as a 2D array of horizontally and
        vertically stacked ePSFs.
        Parameters
        ----------
        data : `numpy.ndarray`
            The 3D or 4D array of ePSF data.
        Returns
        -------
        reshaped_data : `numpy.ndarray`
            The 2D array of ePSF data.
        """"""
        if data.ndim == 4:
            n_rows, n_cols, h, w = data.shape
            # transpose to (n_rows, h, n_cols, w) then reshape
            reshaped = data.transpose(0, 2, 1, 3).reshape(n_rows * h, n_cols * w)
            return reshaped
        elif data.ndim == 3:
            # Assume shape (n_rows, n_cols, n_pixels)
            n_rows, n_cols, n_pixels = data.shape
            # Try to infer square shape
            side = int(np.sqrt(n_pixels))
            if side * side != n_pixels:
                raise ValueError(
                    ""3D data with non-square pixel count cannot be reshaped.""
                )
            data_4d = data.reshape(n_rows, n_cols, side, side)
            return self._reshape_grid(data_4d)
        else:
            raise ValueError(""Data must be a 3D or 4D array."")

    def plot_grid(
        self,
        *,
        ax=None,
        vmax_scale=None,
        peak_norm=False,
        deltas=False,
        cmap=""viridis"",
        dividers=True,
        divider_color=""darkgray"",
        divider_ls=""-"",
        figsize=None,
    ):
        """"""
        Plot the grid of ePSF models.
        Parameters
        ----------
        ax : `matplotlib.axes.Axes` or `None`, optional
            The matplotlib axes on which to plot. If `None`, then the
            current `~matplotlib.axes.Axes` instance is used.
        vmax_scale : float, optional
            Scale factor to apply to the image stretch limits. This
            value is multiplied by the peak ePSF value to determine the
            plotting ``vmax``. The defaults are 1.0 for plotting the
            ePSF data and 0.03 for plotting the ePSF difference data
            (``deltas=True``). If ``deltas=True``, the ``vmin`` is set
            to ``-vmax``. If ``deltas=False`` the ``vmin`` is set to
            ``vmax`` / 1e4.
        peak_norm : bool, optional
            Whether to normalize the ePSF data by the peak value. The
            default shows the ePSF flux per pixel.
        deltas : bool, optional
            Set to `True` to show the differences between each ePSF
            and the average ePSF.
        cmap : str or `matplotlib.colors.Colormap`, optional
            The colormap to use. The default is 'viridis'.
        dividers : bool, optional
            Whether to show divider lines between the ePSFs.
        divider_color, divider_ls : str, optional
            Matplotlib color and linestyle options for the divider
            lines between ePSFs. These keywords have no effect unless
            ``show_dividers=True``.
        figsize : (float, float), optional
            The figure (width, height) in inches.
        Returns
        -------
        fig : `matplotlib.figure.Figure`
            The matplotlib figure object. This will be the current
            figure if ``ax=None``. Use ``fig.savefig()`` to save the
            figure to a file.
        """"""
        # Retrieve the grid data from the instance
        if not hasattr(self, ""grid_data""):
            raise AttributeError(
                ""The instance must have a 'grid_data' attribute containing the ePSF grid.""
            )
        data = self.grid_data

        # Compute deltas if requested
        if deltas:
            # Compute mean over the grid
            mean_epsf = np.mean(data, axis=(0, 1))
            data = data - mean_epsf
            peak = np.max(np.abs(data))
        else:
            peak = np.max(data)

        # Normalize by peak if requested
        if peak_norm:
            data = data / peak

        # Determine default vmax_scale
        if vmax_scale is None:
            vmax_scale = 0.03 if deltas else 1.0

        vmax = vmax_scale * peak
        vmin = -vmax if deltas else vmax / 1e4

        # Reshape the grid for plotting
        reshaped = self._reshape_grid(data)

        # Create figure and axes if needed
        if ax is None:
            if figsize is not None:
                fig, ax = plt.subplots(figsize=figsize)
            else:
                fig = plt.gcf()
                ax = plt.gca"
237588,matthew-brett/delocate,matthew-brett_delocate/delocate/tmpdirs.py,delocate.tmpdirs.InGivenDirectory,"import os

class InGivenDirectory:
    """"""Change directory to given directory for duration of ``with`` block.

    Useful when you want to use `InTemporaryDirectory` for the final test, but
    you are still debugging.  For example, you may want to do this in the end:

    >>> with InTemporaryDirectory() as tmpdir:
    ...     # do something complicated which might break
    ...     pass

    But indeed the complicated thing does break, and meanwhile the
    ``InTemporaryDirectory`` context manager wiped out the directory with the
    temporary files that you wanted for debugging.  So, while debugging, you
    replace with something like:

    >>> with InGivenDirectory() as tmpdir: # Use working directory by default
    ...     # do something complicated which might break
    ...     pass

    You can then look at the temporary file outputs to debug what is happening,
    fix, and finally replace ``InGivenDirectory`` with ``InTemporaryDirectory``
    again.
    """"""

    def __init__(self, path=None):
        """"""Initialize directory context manager.

        Parameters
        ----------
        path : None or str, optional
            path to change directory to, for duration of ``with`` block.
            Defaults to ``os.getcwd()`` if None
        """"""
        if path is None:
            path = os.getcwd()
        self.path = os.path.abspath(path)

    def __enter__(self):
        """"""Chdir to the managed directory, creating it if needed.""""""
        self._pwd = os.path.abspath(os.getcwd())
        if not os.path.isdir(self.path):
            os.mkdir(self.path)
        os.chdir(self.path)
        return self.path

    def __exit__(self, exc, value, tb):
        """"""Revert the working directory.""""""
        os.chdir(self._pwd)","
class InGivenDirectory:
    '''Change directory to given directory for duration of ``with`` block.
    Useful when you want to use `InTemporaryDirectory` for the final test, but
    you are still debugging.  For example, you may want to do this in the end:
    >>> with InTemporaryDirectory() as tmpdir:
    ...     # do something complicated which might break
    ...     pass
    But indeed the complicated thing does break, and meanwhile the
    ``InTemporaryDirectory`` context manager wiped out the directory with the
    temporary files that you wanted for debugging.  So, while debugging, you
    replace with something like:
    >>> with InGivenDirectory() as tmpdir: # Use working directory by default
    ...     # do something complicated which might break
    ...     pass
    You can then look at the temporary file outputs to debug what is happening,
    fix, and finally replace ``InGivenDirectory`` with ``InTemporaryDirectory``
    again.
    '''
    def __init__(self, path=None):
        '''Initialize directory context manager.
        Parameters
        ----------
        path : None or str, optional
            path to change directory to, for duration of ``with`` block.
            Defaults to ``os.getcwd()`` if None
        '''
        pass
    def __enter__(self):
        
        pass
    def __exit__(self, exc, value, tb):
        '''Revert the working directory.'''
        pass",snippet_144,4,"```python
import os

class InGivenDirectory:
    '''Change directory to given directory for duration of ``with`` block.
    Useful when you want to use `InTemporaryDirectory` for the final test, but
    you are still debugging.  For example, you may want to do this in the end:
    >>> with InTemporaryDirectory() as tmpdir:
    ...     # do something complicated which might break
    ...     pass
    But indeed the complicated thing does break, and meanwhile the
    ``InTemporaryDirectory`` context manager wiped out the directory with the
    temporary files that you wanted for debugging.  So, while debugging, you
    replace with something like:
    >>> with InGivenDirectory() as tmpdir: # Use working directory by default
    ...     # do something complicated which might break
    ...     pass
    You can then look at the temporary file outputs to debug what is happening,
    fix, and finally replace ``InGivenDirectory`` with ``InTemporaryDirectory``
    again.
    '''
    def __init__(self, path=None):
        '''Initialize directory context manager.
        Parameters
        ----------
        path : None or str, optional
            path to change directory to, for duration of ``with`` block.
            Defaults to ``os.getcwd()`` if None
        '''
        self.path = path if path is not None else os.getcwd()
        self._original_cwd = None

    def __enter__(self):
        self._original_cwd = os.getcwd()
        os.chdir(self.path)
        return self.path

    def __exit__(self, exc, value, tb):
        '''Revert the working directory.'''
        if self._original_cwd is not None:
            os.chdir(self._original_cwd)
        # Do not suppress exceptions
        return False
```"
239516,blink1073/oct2py,oct2py/dynamic.py,oct2py.dynamic._MethodDocDescriptor,"class _MethodDocDescriptor:
    """"""An object that dynamically fetches the documentation
    for an Octave user class method.
    """"""

    def __init__(self, session_weakref, class_name, name):
        """"""Initialize the descriptor.""""""
        self.ref = session_weakref
        self.class_name = class_name
        self.name = name
        self.doc = None

    def __get__(self, instance, owner=None):
        """"""Get the documentation.""""""
        if self.doc is not None:
            return self.doc
        session = self.ref()
        class_name = self.class_name
        method = self.name
        doc = session._get_doc(f'@{class_name}/{method}')
        self.doc = doc or session._get_doc(method)
        return self.doc","class _MethodDocDescriptor:
    
    def __init__(self, session_weakref, class_name, name):
        '''Initialize the descriptor.'''
        pass
    def __get__(self, instance, owner=None):
        '''Get the documentation.'''
        pass",snippet_145,3,"```python
import weakref

class _MethodDocDescriptor:
    """"""
    Descriptor that lazily retrieves method documentation from a session object.
    """"""

    def __init__(self, session_weakref, class_name, name):
        """"""
        Initialize the descriptor.

        Parameters
        ----------
        session_weakref : weakref.ref
            Weak reference to the session object that holds documentation.
        class_name : str
            Name of the class containing the method.
        name : str
            Name of the method whose documentation is requested.
        """"""
        # Store the weak reference to the session
        self._session_ref = session_weakref
        self._class_name = class_name
        self._method_name = name

    def __get__(self, instance, owner=None):
        """"""
        Retrieve the documentation for the method.

        Parameters
        ----------
        instance : object or None
            The instance on which the descriptor is accessed. If None,
            the descriptor is accessed through the class.
        owner : type or None
            The owner class.

        Returns
        -------
        str or None
            The documentation string for the method, or None if it cannot be
            retrieved.
        """"""
        # If accessed through the class, return the descriptor itself
        if instance is None:
            return self

        # Resolve the session from the weak reference
        session = self._session_ref() if callable(self._session_ref) else None
        if session is None:
            return None

        # Attempt to retrieve the documentation from the session
        try:
            # The session is expected to provide a method `get_method_doc`
            # that accepts a class name and method name.
            doc = session.get_method_doc(self._class_name, self._method_name)
            return doc
        except Exception:
            # If anything goes wrong (attribute missing, etc.), return None
            return None
```"
244777,markokr/rarfile,markokr_rarfile/rarfile.py,rarfile.NoHashContext,"class NoHashContext:
    """"""No-op hash function.""""""

    def __init__(self, data=None):
        """"""Initialize""""""

    def update(self, data):
        """"""Update data""""""

    def digest(self):
        """"""Final hash""""""

    def hexdigest(self):
        """"""Hexadecimal digest.""""""","class NoHashContext:
    
    def __init__(self, data=None):
        '''Initialize'''
        pass
    def update(self, data):
        
        pass
    def digest(self):
        '''Final hash'''
        pass
    def hexdigest(self):
        '''Hexadecimal digest.'''
        pass",snippet_147,5,"class NoHashContext:
    def __init__(self, data=None):
        '''Initialize'''
        self._buffer = bytearray()
        if data is not None:
            self.update(data)

    def update(self, data):
        if isinstance(data, str):
            data = data.encode('utf-8')
        if not isinstance(data, (bytes, bytearray)):
            raise TypeError(""data must be bytes or bytearray"")
        self._buffer.extend(data)

    def digest(self):
        '''Final hash'''
        return bytes(self._buffer)

    def hexdigest(self):
        '''Hexadecimal digest.'''
        return self._buffer.hex()
"
244940,meejah/txtorcon,meejah_txtorcon/txtorcon/addrmap.py,txtorcon.addrmap.Addr,"import datetime
from txtorcon.util import maybe_ip_addr

class Addr:
    """"""
    One address mapping (e.g. example.com -> 127.0.0.1)
    """"""

    def __init__(self, map):
        """"""
        map is an AddrMap instance, used for scheduling expiries and
        updating the map.
        """"""
        self.map = map
        self.ip = None
        self.name = None
        self.expiry = None
        self.expires = None
        self.created = None

    def update(self, *args):
        """"""
        deals with an update from Tor; see parsing logic in torcontroller
        """"""
        gmtexpires = None
        name, ip, expires = args[:3]
        for arg in args:
            if arg.lower().startswith('expires='):
                gmtexpires = arg[8:]
        if gmtexpires is None:
            if len(args) == 3:
                gmtexpires = expires
            elif args[2] == 'NEVER':
                gmtexpires = args[2]
            else:
                gmtexpires = args[3]
        self.name = name
        self.ip = maybe_ip_addr(ip)
        if self.ip == '<error>':
            self._expire()
            return
        fmt = '%Y-%m-%d %H:%M:%S'
        oldexpires = self.expires
        if gmtexpires.upper() == 'NEVER':
            self.expires = None
        else:
            self.expires = datetime.datetime.strptime(gmtexpires, fmt)
        self.created = datetime.datetime.utcnow()
        if self.expires is not None:
            if oldexpires is None:
                if self.expires <= self.created:
                    diff = datetime.timedelta(seconds=0)
                else:
                    diff = self.expires - self.created
                self.expiry = self.map.scheduler.callLater(diff.seconds, self._expire)
            else:
                diff = self.expires - oldexpires
                self.expiry.delay(diff.seconds)

    def _expire(self):
        """"""
        callback done via callLater
        """"""
        del self.map.addr[self.name]
        self.map.notify('addrmap_expired', *[self.name], **{})","
class Addr:
    
    def __init__(self, map):
        '''
        map is an AddrMap instance, used for scheduling expiries and
        updating the map.
        '''
        pass
    def update(self, *args):
        '''
        deals with an update from Tor; see parsing logic in torcontroller
        '''
        pass
    def _expire(self):
        '''
        callback done via callLater
        '''
        pass",snippet_148,4,"```python
import threading
from datetime import datetime, timedelta


class Addr:
    """"""
    Represents a Tor address that can be updated and automatically expires.
    The `map` argument is an instance of AddrMap (or any mapping-like object)
    that is responsible for storing and removing Addr objects.
    """"""

    def __init__(self, map):
        """"""
        Initialise the Addr instance.

        Parameters
        ----------
        map : AddrMap
            The map that holds this Addr. It is used to schedule expiries
            and to update the map when the address changes.
        """"""
        self.map = map
        self.address = None          # the address string (e.g. ""127.0.0.1:9050"")
        self.expire_time = None      # UTC datetime when this Addr should expire
        self._timer = None           # threading.Timer used for expiry
        self._lock = threading.Lock()  # protects concurrent access

    def update(self, *args):
        """"""
        Update the address information.

        The first argument is expected to be the address string.
        The second optional argument is the number of seconds until
        the address should expire.  If the expiry argument is omitted,
        the address will not expire automatically.

        Parameters
        ----------
        *args : tuple
            Variable length argument list.  At least one element
            (the address) is required.
        """"""
        if not args:
            return

        with self._lock:
            # Parse arguments
            address = args[0]
            expires_in = args[1] if len(args) > 1 else None

            # Store the new address
            self.address = address

            # Cancel any existing timer
            if self._timer:
                self._timer.cancel()
                self._timer = None

            # Schedule a new expiry if requested
            if expires_in is not None:
                self.expire_time = datetime.utcnow() + timedelta(seconds=expires_in)
                self._timer = threading.Timer(expires_in, self._expire)
                self._timer.daemon = True
                self._timer.start()
            else:
                self.expire_time = None

            # Notify the map that this Addr has been updated
            if hasattr(self.map, ""update""):
                # The map may expect the Addr instance itself
                self.map.update(self)
            else:
                # Fallback: store the Addr in a plain dict
                self.map[self.address] = self

    def _expire(self):
        """"""
        Internal callback invoked when the address expires.
        Removes the Addr from the map and cleans up internal state.
        """"""
        with self._lock:
            # Remove from the map
            if hasattr(self.map, ""expire""):
                self.map.expire(self)
            else:
                # Fallback: remove from a plain dict
                self.map.pop(self.address, None)

            # Clean up timer and expiry time
            self._timer = None
            self.expire_time = None
```"
245022,meejah/txtorcon,meejah_txtorcon/txtorcon/torconfig.py,txtorcon.torconfig.TorConfigType,"class TorConfigType:
    """"""
    Base class for all configuration types, which function as parsers
    and un-parsers.
    """"""

    def parse(self, s):
        """"""
        Given the string s, this should return a parsed representation
        of it.
        """"""
        return s

    def validate(self, s, instance, name):
        """"""
        If s is not a valid type for this object, an exception should
        be thrown. The validated object should be returned.
        """"""
        return s","class TorConfigType:
    '''
    Base class for all configuration types, which function as parsers
    and un-parsers.
    '''
    def parse(self, s):
        
        pass
    def validate(self, s, instance, name):
        
        pass",snippet_149,3,"class TorConfigType:
    """"""
    Base class for all configuration types, which function as parsers
    and un-parsers.
    """"""

    def parse(self, s):
        """"""
        Parse the input string `s` into a Python object.
        The default implementation simply returns the string unchanged.
        Subclasses should override this method to provide custom parsing logic.
        """"""
        if not isinstance(s, str):
            raise TypeError(f""Expected a string for parsing, got {type(s).__name__}"")
        return s

    def validate(self, s, instance, name):
        """"""
        Validate that the parsed `instance` is acceptable for the configuration
        identified by `name`. The default implementation performs basic checks
        and returns True if validation passes.
        Subclasses may override this method to enforce stricter rules.
        """"""
        if not isinstance(s, str):
            raise TypeError(f""Expected a string for validation, got {type(s).__name__}"")
        if instance is None:
            raise ValueError(f""Instance for '{name}' cannot be None"")
        return True"
245057,paterva/maltego-trx,paterva_maltego-trx/maltego_trx/oauth.py,maltego_trx.oauth.MaltegoOauth,"from cryptography.hazmat.backends import default_backend
import base64
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
from cryptography.hazmat.primitives import serialization, padding as primitives_padding
from cryptography.hazmat.primitives.asymmetric import padding as asymmetric_padding

class MaltegoOauth:
    """"""
    A Crypto Helper for Maltego OAuth Secrets received from the Transform Distribution Server
    The TDS will send back an encrypted combination of the following :
    1. Token
    2. Token Secret
    3. Refresh Token
    4. Expires In

    Contains Methods:
        1. decrypt_secrets(private_key_path=""pem file"", ciphertext=""request.getTransformSetting('name from TDS')"")
    """"""

    @staticmethod
    def _rsa_decrypt(private_key_path=None, ciphertext=None, password=None):
        """"""
        RSA Decryption function, returns decrypted plaintext in b64 encoding
        """"""
        ciphertext = base64.b64decode(ciphertext)
        with open(private_key_path, 'rb') as key_file:
            private_key = serialization.load_pem_private_key(key_file.read(), password, backend=None)
            plaintext = private_key.decrypt(ciphertext, asymmetric_padding.PKCS1v15())
        return plaintext

    @staticmethod
    def _aes_decrypt(key=None, ciphertext=None):
        """"""
        AES Decryption function, returns decrypted plaintext value
        """"""
        key = base64.b64decode(key)
        cipher = Cipher(algorithms.AES(key), modes.ECB(), backend=default_backend())
        decryptor = cipher.decryptor()
        ciphertext = base64.b64decode(ciphertext)
        padded_b64_plaintext = decryptor.update(ciphertext) + decryptor.finalize()
        unpadder = primitives_padding.PKCS7(128).unpadder()
        plaintext = (unpadder.update(padded_b64_plaintext) + unpadder.finalize()).decode('utf8')
        return plaintext

    @classmethod
    def decrypt_secrets(cls, private_key_path=None, encoded_ciphertext=None):
        """"""
        The TDS will send back an encrypted combination of the following :
        1. Token
        2. Token Secret
        3. Refresh Token
        4. Expires In

        This function decodes the combinations and decrypts as required and returns a dictionary with the following keys
                {""token"":"""",
                ""token_secret"": """",
                ""refresh_token"": """",
                ""expires_in"": """"}
        """"""
        encrypted_fields = encoded_ciphertext.split('$')
        if len(encrypted_fields) == 1:
            token = cls._rsa_decrypt(private_key_path, encrypted_fields[0])
            token_fields = {'token': token}
        elif len(encrypted_fields) == 2:
            token = cls._rsa_decrypt(private_key_path, encrypted_fields[0])
            token_secret = cls._rsa_decrypt(private_key_path, encrypted_fields[1])
            token_fields = {'token': token, 'token_secret': token_secret}
        elif len(encrypted_fields) == 3:
            aes_key = cls._rsa_decrypt(private_key_path, encrypted_fields[2])
            token = cls._aes_decrypt(aes_key, encrypted_fields[0])
            token_secret = cls._aes_decrypt(aes_key, encrypted_fields[1])
            token_fields = {'token': token, 'token_secret': token_secret}
        elif len(encrypted_fields) == 4:
            token = cls._rsa_decrypt(private_key_path, encrypted_fields[0])
            token_secret = cls._rsa_decrypt(private_key_path, encrypted_fields[1])
            refresh_token = cls._rsa_decrypt(private_key_path, encrypted_fields[2])
            expires_in = cls._rsa_decrypt(private_key_path, encrypted_fields[3])
            token_fields = {'token': token, 'token_secret': token_secret, 'refresh_token': refresh_token, 'expires_in': expires_in}
        elif len(encrypted_fields) == 5:
            aes_key = cls._rsa_decrypt(private_key_path, encrypted_fields[4])
            token = cls._aes_decrypt(aes_key, encrypted_fields[0])
            token_secret = cls._aes_decrypt(aes_key, encrypted_fields[1])
            refresh_token = cls._aes_decrypt(aes_key, encrypted_fields[2])
            expires_in = cls._aes_decrypt(aes_key, encrypted_fields[3])
            token_fields = {'token': token, 'token_secret': token_secret, 'refresh_token': refresh_token, 'expires_in': expires_in}
        else:
            token_fields = {'token': '', 'token_secret': '', 'refresh_token': '', 'expires_in': ''}
        return token_fields","
class MaltegoOauth:
    '''
    A Crypto Helper for Maltego OAuth Secrets received from the Transform Distribution Server
    The TDS will send back an encrypted combination of the following :
    1. Token
    2. Token Secret
    3. Refresh Token
    4. Expires In
    Contains Methods:
        1. decrypt_secrets(private_key_path=""pem file"", ciphertext=""request.getTransformSetting('name from TDS')"")
    '''
    @staticmethod
    def _rsa_decrypt(private_key_path=None, ciphertext=None, password=None):
        '''
        RSA Decryption function, returns decrypted plaintext in b64 encoding
        '''
        pass
    @staticmethod
    def _aes_decrypt(key=None, ciphertext=None):
        
        pass
    @classmethod
    def decrypt_secrets(cls, private_key_path=None, encoded_ciphertext=None):
        '''
        The TDS will send back an encrypted combination of the following :
        1. Token
        2. Token Secret
        3. Refresh Token
        4. Expires In
        This function decodes the combinations and decrypts as required and returns a dictionary with the following keys
                {""token"":"""",
                ""token_secret"": """",
                ""refresh_token"": """",
                ""expires_in"": """"}
        '''
        pass",snippet_150,4,"```python
import base64
import json
from pathlib import Path
from typing import Dict, Optional

from cryptography.hazmat.primitives import hashes, padding as asym_padding
from cryptography.hazmat.primitives.asymmetric import rsa, padding
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.backends import default_backend


class MaltegoOauth:
    """"""
    A Crypto Helper for Maltego OAuth Secrets received from the Transform Distribution Server.
    The TDS will send back an encrypted combination of the following:
    1. Token
    2. Token Secret
    3. Refresh Token
    4. Expires In
    Contains Methods:
        1. decrypt_secrets(private_key_path=""pem file"",
                            encoded_ciphertext=""request.getTransformSetting('name from TDS')"")
    """"""

    @staticmethod
    def _rsa_decrypt(
        private_key_path: Optional[str] = None,
        ciphertext: Optional[str] = None,
        password: Optional[bytes] = None,
    ) -> bytes:
        """"""
        RSA Decryption function, returns decrypted plaintext bytes.
        The ciphertext is expected to be base64 encoded.
        """"""
        if not private_key_path or not ciphertext:
            raise ValueError(""private_key_path and ciphertext must be provided"")

        # Load private key
        key_data = Path(private_key_path).read_bytes()
        private_key = serialization.load_pem_private_key(
            key_data, password=password, backend=default_backend()
        )

        # Decode ciphertext
        ct_bytes = base64.b64decode(ciphertext)

        # Decrypt
        plaintext = private_key.decrypt(
            ct_bytes,
            padding.OAEP(
                mgf=padding.MGF1(algorithm=hashes.SHA256()), algorithm=hashes.SHA256(), label=None
            ),
        )
        return plaintext

    @staticmethod
    def _aes_decrypt(key: bytes, ciphertext: bytes) -> bytes:
        """"""
        AES decryption function.
        The ciphertext is expected to be base64 encoded and to contain the IV as the first 16 bytes.
        """"""
        if not key or not ciphertext:
            raise ValueError(""key and ciphertext must be provided"")

        # Decode ciphertext
        ct_bytes = base64.b64decode(ciphertext)

        # Extract IV and actual ciphertext
        iv = ct_bytes[:16]
        ct = ct_bytes[16:]

        # Create cipher
        cipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())
        decryptor = cipher.decryptor()
        padded_plaintext = decryptor.update(ct) + decryptor.finalize()

        # Unpad
        unpadder = padding.PKCS7(algorithms.AES.block_size).unpadder()
        plaintext = unpadder.update(padded_plaintext) + unpadder.finalize()
        return plaintext

    @classmethod
    def decrypt_secrets(
        cls,
        private_key_path: Optional[str] = None,
        encoded_ciphertext: Optional[str] = None,
        password: Optional[bytes] = None,
    ) -> Dict[str, str]:
        """"""
        The TDS will send back an encrypted combination of the following:
        1. Token
        2. Token Secret
        3. Refresh Token
        4. Expires In
        This function decodes the combinations and decrypts as required and returns a dictionary with the following keys
            {""token"":"""", ""token_secret"":"""", ""refresh_token"":"""", ""expires_in"":""""}
        """"""
        if not private_key_path or not encoded_ciphertext:
            raise ValueError(""private_key_path and encoded_ciphertext must be provided"")

        # First, RSA decrypt to get the AES key and the AES encrypted payload
        # We assume the RSA encrypted payload is a JSON string containing:
        #   {""aes_key"":""<base64>"", ""payload"":""<base64>""}
        try:
            rsa_plaintext = cls._rsa_decrypt(
                private_key_path=private_key_path,
                ciphertext=encoded_ciphertext,
                password=password,
            )
            rsa_json = json.loads(rsa_plaintext.decode(""utf-8""))
            aes_key_b64 = rsa_json.get(""aes_key"")
            payload_b64 = rsa_json.get(""payload"")
            if not aes_key_b64 or not payload_b64:
                raise ValueError(""Invalid RSA payload structure"")
            aes_key = base64.b64decode(aes_key_b64)
        except Exception as e:
            raise ValueError(f""RSA decryption failed: {e}"")

        # Now AES decrypt the payload
        try:
            aes_plaintext = cls._aes_decrypt(key=aes_key, ciphertext=payload_b64)
            payload_json = json.loads(aes_plaintext.decode(""utf-8""))
        except Exception as e:
            raise ValueError(f""AES decryption failed: {e}"")

        # Extract required fields
        result = {
            ""token"": payload_json.get(""token"", """"),
            ""token_secret"": payload_json.get(""token_secret"", """"),
            ""refresh_token"": payload_json.get(""refresh_token"", """"),
            ""expires_in"": payload_json.get(""expires_in"", """"),
        }
        return result
```"
247043,CiscoDevNet/webexteamssdk,src/webexpythonsdk/models/cards/adaptive_card_component.py,webexpythonsdk.models.cards.adaptive_card_component.AdaptiveCardComponent,"import json
import enum

class AdaptiveCardComponent:
    """"""
    Base class for all Adaptive Card elements.

    Each element should inherit from this class and specify which of its
    properties fall into the following two categories:

    * Simple properties are basic types (int, float, str, etc.).

    * Serializable properties are properties that can themselves be serialized.
      This includes lists of items (i.e. the 'body' field of the adaptive card)
      or single objects that also inherit from Serializable
    """"""

    def __init__(self, serializable_properties, simple_properties):
        """"""
        Initialize a serializable object.

        Args:
            serializable_properties(list): List of all serializable properties
            simple_properties(list): List of all simple properties.
        """"""
        self.serializable_properties = serializable_properties
        self.simple_properties = simple_properties

    def to_dict(self):
        """"""
        Serialize the element into a Python dictionary.

        The to_dict() method recursively serializes the object's data into
        a Python dictionary.

        Returns:
            dict: Dictionary representation of this element.
        """"""
        serialized_data = {}
        for property_name in self.simple_properties:
            property_value = getattr(self, property_name, None)
            if property_value is not None:
                if isinstance(property_value, enum.Enum):
                    property_value = str(property_value)
                serialized_data[property_name] = property_value
        for property_name in self.serializable_properties:
            property_value = getattr(self, property_name, None)
            if property_value is not None:
                if isinstance(property_value, list):
                    serialized_data[property_name] = [item.to_dict() if hasattr(item, 'to_dict') else item for item in property_value]
                else:
                    serialized_data[property_name] = property_value.to_dict()
        return serialized_data

    def to_json(self, **kwargs):
        """"""
        Serialize the element into JSON text.

        Any keyword arguments provided are passed through the Python JSON
        encoder.
        """"""
        return json.dumps(self.to_dict(), **kwargs)","
class AdaptiveCardComponent:
    '''
    Base class for all Adaptive Card elements.
    Each element should inherit from this class and specify which of its
    properties fall into the following two categories:
    * Simple properties are basic types (int, float, str, etc.).
    * Serializable properties are properties that can themselves be serialized.
      This includes lists of items (i.e. the 'body' field of the adaptive card)
      or single objects that also inherit from Serializable
    '''
    def __init__(self, serializable_properties, simple_properties):
        
        pass
    def to_dict(self):
        
        pass
    def to_json(self, **kwargs):
        '''
        Serialize the element into JSON text.
        Any keyword arguments provided are passed through the Python JSON
        encoder.
        '''
        pass",snippet_151,4,"```python
import json
from typing import Any, Dict, List, Union


class AdaptiveCardComponent:
    """"""
    Base class for all Adaptive Card elements.
    Each element should inherit from this class and specify which of its
    properties fall into the following two categories:
    * Simple properties are basic types (int, float, str, etc.).
    * Serializable properties are properties that can themselves be serialized.
      This includes lists of items (i.e. the 'body' field of the adaptive card)
      or single objects that also inherit from Serializable
    """"""

    def __init__(self, serializable_properties: List[str], simple_properties: List[str]):
        """"""
        Parameters
        ----------
        serializable_properties : List[str]
            Names of attributes that are themselves serializable objects or lists of such objects.
        simple_properties : List[str]
            Names of attributes that are simple (primitive) types.
        """"""
        self._serializable_properties = serializable_properties
        self._simple_properties = simple_properties

    def to_dict(self) -> Dict[str, Any]:
        """"""
        Convert the component into a dictionary representation suitable for JSON serialization.
        """"""
        result: Dict[str, Any] = {}

        # Handle simple properties
        for prop in self._simple_properties:
            value = getattr(self, prop, None)
            if value is not None:
                result[prop] = value

        # Handle serializable properties
        for prop in self._serializable_properties:
            value = getattr(self, prop, None)
            if value is None:
                continue

            # If it's a list, serialize each element
            if isinstance(value, list):
                serialized_list = []
                for item in value:
                    if hasattr(item, ""to_dict"") and callable(item.to_dict):
                        serialized_list.append(item.to_dict())
                    else:
                        serialized_list.append(item)
                result[prop] = serialized_list
            # If it's a single serializable object
            elif hasattr(value, ""to_dict"") and callable(value.to_dict):
                result[prop] = value.to_dict()
            else:
                # Fallback: use the value as-is
                result[prop] = value

        return result

    def to_json(self, **kwargs: Any) -> str:
        """"""
        Serialize the element into JSON text.
        Any keyword arguments provided are passed through the Python JSON encoder.
        """"""
        return json.dumps(self.to_dict(), **kwargs)
```"
248349,facelessuser/soupsieve,facelessuser_soupsieve/soupsieve/css_match.py,soupsieve.css_match._FakeParent,"import bs4

class _FakeParent:
    """"""
    Fake parent class.

    When we have a fragment with no `BeautifulSoup` document object,
    we can't evaluate `nth` selectors properly.  Create a temporary
    fake parent so we can traverse the root element as a child.
    """"""

    def __init__(self, element: bs4.Tag) -> None:
        """"""Initialize.""""""
        self.contents = [element]

    def __len__(self) -> int:
        """"""Length.""""""
        return len(self.contents)","
class _FakeParent:
    
    def __init__(self, element: bs4.Tag) -> None:
        
        pass
    def __len__(self) -> int:
        '''Length.'''
        pass",snippet_152,3,"```python
import bs4

class _FakeParent:
    """"""
    A lightweight wrapper around a BeautifulSoup Tag that behaves like a container.
    """"""

    def __init__(self, element: bs4.Tag) -> None:
        if not isinstance(element, bs4.Tag):
            raise TypeError(""element must be a bs4.Tag instance"")
        self.element = element

    def __len__(self) -> int:
        """"""
        Return the number of direct children of the wrapped Tag.
        """"""
        return len(self.element)
```"
248351,facelessuser/soupsieve,facelessuser_soupsieve/soupsieve/css_parser.py,soupsieve.css_parser.SelectorPattern,"from typing import Match, Any, Iterator, cast
import re

class SelectorPattern:
    """"""Selector pattern.""""""

    def __init__(self, name: str, pattern: str) -> None:
        """"""Initialize.""""""
        self.name = name
        self.re_pattern = re.compile(pattern, re.I | re.X | re.U)

    def get_name(self) -> str:
        """"""Get name.""""""
        return self.name

    def match(self, selector: str, index: int, flags: int) -> Match[str] | None:
        """"""Match the selector.""""""
        return self.re_pattern.match(selector, index)","
class SelectorPattern:
    '''Selector pattern.'''
    def __init__(self, name: str, pattern: str) -> None:
        '''Initialize.'''
        pass
    def get_name(self) -> str:
        
        pass
    def match(self, selector: str, index: int, flags: int) -> Match[str] | None:
        
        pass",snippet_153,4,"```python
import re
from typing import Match

class SelectorPattern:
    '''Selector pattern.'''
    def __init__(self, name: str, pattern: str) -> None:
        '''Initialize.'''
        self._name = name
        self._pattern = pattern

    def get_name(self) -> str:
        return self._name

    def match(self, selector: str, index: int, flags: int) -> Match[str] | None:
        '''Return a regex match object if the pattern matches the selector
        starting at the given index, otherwise None.'''
        regex = re.compile(self._pattern, flags)
        return regex.match(selector, index)
```"
250153,Unidata/siphon,src/siphon/catalog.py,siphon.catalog.CatalogRef,"from urllib.parse import urljoin, urlparse

class CatalogRef:
    """"""
    An object for holding catalog references obtained from a THREDDS Client Catalog.

    Attributes
    ----------
    name : str
        The name of the :class:`CatalogRef` element
    href : str
        url to the :class:`CatalogRef`'s THREDDS Client Catalog
    title : str
        Title of the :class:`CatalogRef` element

    """"""

    def __init__(self, base_url, element_node):
        """"""
        Initialize the catalogRef object.

        Parameters
        ----------
        base_url : str
            URL to the base catalog that owns this reference
        element_node : :class:`~xml.etree.ElementTree.Element`
            An :class:`~xml.etree.ElementTree.Element` representing a catalogRef node

        """"""
        self.title = element_node.attrib['{http://www.w3.org/1999/xlink}title']
        self.name = element_node.attrib.get('name', self.title)
        href = element_node.attrib['{http://www.w3.org/1999/xlink}href']
        self.href = urljoin(base_url, href)

    def __str__(self):
        """"""Return a string representation of the catalog reference.""""""
        return str(self.title)

    def follow(self):
        """"""Follow the catalog reference and return a new :class:`TDSCatalog`.

        Returns
        -------
        TDSCatalog
            The referenced catalog

        """"""
        return TDSCatalog(self.href)
    __repr__ = __str__","
class CatalogRef:
    
    def __init__(self, base_url, element_node):
        '''
        Initialize the catalogRef object.
        Parameters
        ----------
        base_url : str
            URL to the base catalog that owns this reference
        element_node : :class:`~xml.etree.ElementTree.Element`
            An :class:`~xml.etree.ElementTree.Element` representing a catalogRef node
        '''
        pass
    def __str__(self):
        
        pass
    def follow(self):
        
        pass",snippet_154,4,"```python
import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET


class CatalogRef:
    """"""
    Represents a <catalogRef> element in an XML catalog.
    """"""

    def __init__(self, base_url, element_node):
        """"""
        Initialize the catalogRef object.

        Parameters
        ----------
        base_url : str
            URL to the base catalog that owns this reference
        element_node : xml.etree.ElementTree.Element
            An Element representing a catalogRef node
        """"""
        self.base_url = base_url
        self.element_node = element_node
        # The href attribute contains the relative or absolute URL of the referenced catalog
        self.href = element_node.get(""href"")
        if self.href is None:
            raise ValueError(""catalogRef element must have an 'href' attribute"")

    def __str__(self):
        return f""CatalogRef(href={self.href})""

    def follow(self):
        """"""
        Resolve the href relative to the base URL, fetch the referenced catalog,
        parse it, and return the root Element.

        Returns
        -------
        xml.etree.ElementTree.Element
            The root element of the referenced catalog.

        Raises
        ------
        RuntimeError
            If the referenced catalog cannot be fetched or parsed.
        """"""
        # Resolve the full URL
        full_url = urllib.parse.urljoin(self.base_url, self.href)

        try:
            with urllib.request.urlopen(full_url) as response:
                data = response.read()
        except Exception as exc:
            raise RuntimeError(f""Failed to fetch catalog at {full_url}"") from exc

        try:
            root = ET.fromstring(data)
        except ET.ParseError as exc:
            raise RuntimeError(f""Failed to parse catalog XML from {full_url}"") from exc

        return root
```"
250158,Unidata/siphon,src/siphon/catalog.py,siphon.catalog.SimpleService,"class SimpleService:
    """"""Hold information about an access service enabled on a dataset.

    Attributes
    ----------
    name : str
        The name of the service
    service_type : str
        The service type (i.e. ""OPENDAP"", ""NetcdfSubset"", ""WMS"", etc.)
    access_urls : dict[str, str]
        A dictionary of access urls whose keywords are the access service
        types defined in the catalog (for example, ""OPENDAP"", ""NetcdfSubset"",
        ""WMS"", etc.)

    """"""

    def __init__(self, service_node):
        """"""Initialize the Dataset object.

        Parameters
        ----------
        service_node : :class:`~xml.etree.ElementTree.Element`
            An :class:`~xml.etree.ElementTree.Element` representing a service node

        """"""
        self.name = service_node.attrib['name']
        self.service_type = CaseInsensitiveStr(service_node.attrib['serviceType'])
        self.base = service_node.attrib['base']
        self.access_urls = {}

    def is_resolver(self):
        """"""Return whether the service is a resolver service.""""""
        return self.service_type == 'Resolver'","class SimpleService:
    '''Hold information about an access service enabled on a dataset.
    Attributes
    ----------
    name : str
        The name of the service
    service_type : str
        The service type (i.e. ""OPENDAP"", ""NetcdfSubset"", ""WMS"", etc.)
    access_urls : dict[str, str]
        A dictionary of access urls whose keywords are the access service
        types defined in the catalog (for example, ""OPENDAP"", ""NetcdfSubset"",
        ""WMS"", etc.)
    '''
    def __init__(self, service_node):
        
        pass
    def is_resolver(self):
        '''Return whether the service is a resolver service.'''
        pass",snippet_156,3,"```python
class SimpleService:
    '''Hold information about an access service enabled on a dataset.
    Attributes
    ----------
    name : str
        The name of the service
    service_type : str
        The service type (i.e. ""OPENDAP"", ""NetcdfSubset"", ""WMS"", etc.)
    access_urls : dict[str, str]
        A dictionary of access urls whose keywords are the access service
        types defined in the catalog (for example, ""OPENDAP"", ""NetcdfSubset"",
        ""WMS"", etc.)
    '''
    def __init__(self, service_node):
        # service_node may be an lxml Element or a plain dict
        if isinstance(service_node, dict):
            self.name = service_node.get('name')
            self.service_type = service_node.get('service_type')
            access = service_node.get('access_urls', {})
            # ensure dict of str->str
            self.access_urls = {k: str(v) for k, v in access.items()}
        else:
            # assume Element
            self.name = service_node.get('name')
            if self.name is None:
                self.name = service_node.findtext('name')
            self.service_type = service_node.get('service_type')
            if self.service_type is None:
                self.service_type = service_node.findtext('service_type')
            self.access_urls = {}
            for child in service_node.findall('.//access_url'):
                # try attributes first
                typ = child.get('type')
                url = child.get('url')
                # fallback to subelements
                if typ is None:
                    typ = child.findtext('type')
                if url is None:
                    url = child.findtext('url')
                if typ and url:
                    self.access_urls[typ] = url

    def is_resolver(self):
        '''Return whether the service is a resolver service.'''
        if self.service_type:
            return self.service_type.lower() == 'resolver'
        if self.name:
            return 'resolver' in self.name.lower()
        return False
```"
250272,incuna/django-pgcrypto-fields,incuna_django-pgcrypto-fields/pgcrypto/mixins.py,pgcrypto.mixins.HashMixin,"class HashMixin:
    """"""Keyed hash mixin.

    `HashMixin` uses 'pgcrypto' to encrypt data in a postgres database.
    """"""
    encrypt_sql = None

    def __init__(self, original=None, *args, **kwargs):
        """"""Tells the init the original attr.""""""
        self.original = original
        super(HashMixin, self).__init__(*args, **kwargs)

    def pre_save(self, model_instance, add):
        """"""Save the original_value.""""""
        if self.original:
            original_value = getattr(model_instance, self.original)
            setattr(model_instance, self.attname, original_value)
        return super(HashMixin, self).pre_save(model_instance, add)

    def get_placeholder(self, value=None, compiler=None, connection=None):
        """"""
        Tell postgres to encrypt this field with a hashing function.

        The `value` string is checked to determine if we need to hash or keep
        the current value.

        `compiler` and `connection` is ignored here as we don't need custom operators.
        """"""
        if value is None or value.startswith('\\x'):
            return '%s'
        return self.get_encrypt_sql(connection)

    def get_encrypt_sql(self, connection):
        """"""Get encrypt sql. This may be overidden by some implementations.""""""
        return self.encrypt_sql","class HashMixin:
    '''Keyed hash mixin.
    `HashMixin` uses 'pgcrypto' to encrypt data in a postgres database.
    '''
    def __init__(self, original=None, *args, **kwargs):
        
        pass
    def pre_save(self, model_instance, add):
        '''Save the original_value.'''
        pass
    def get_placeholder(self, value=None, compiler=None, connection=None):
        
        pass
    def get_encrypt_sql(self, connection):
        '''Get encrypt sql. This may be overidden by some implementations.'''
        pass",snippet_157,5,"```python
class HashMixin:
    '''Keyed hash mixin.
    `HashMixin` uses 'pgcrypto' to encrypt data in a postgres database.
    '''
    def __init__(self, original=None, *args, **kwargs):
        # Store the original value if provided
        self.original = original
        # Allow a key to be passed in kwargs (optional)
        self.key = kwargs.pop('key', None)
        super().__init__(*args, **kwargs)

    def pre_save(self, model_instance, add):
        '''Save the original_value.'''
        # Retrieve the current value from the instance
        current_value = getattr(model_instance, self.attname)
        # Store it on the instance for later use
        setattr(model_instance, f'_original_{self.attname}', current_value)
        return current_value

    def get_placeholder(self, value=None, compiler=None, connection=None):
        '''Return a SQL placeholder that will encrypt the value using pgcrypto.'''
        # Use the pgp_sym_encrypt function; the key can be a literal or a column name
        if self.key is None:
            # If no key is provided, just return the value placeholder
            return '%s'
        # If the key is a string literal, quote it
        if isinstance(self.key, str) and not self.key.startswith('""'):
            key_sql = f""'{self.key}'""
        else:
            key_sql = self.key
        return f'pgp_sym_encrypt(%s, {key_sql})'

    def get_encrypt_sql(self, connection):
        '''Get encrypt sql. This may be overidden by some implementations.'''
        # Ensure we are on PostgreSQL
        if connection.vendor != 'postgresql':
            raise NotImplementedError('pgcrypto encryption is only supported on PostgreSQL')
        # Default encryption SQL using pgp_sym_encrypt
        if self.key is None:
            raise ValueError('Encryption key must be provided')
        # Quote the key if it is a literal string
        if isinstance(self.key, str) and not self.key.startswith('""'):
            key_sql = f""'{self.key}'""
        else:
            key_sql = self.key
        return f'pgp_sym_encrypt(%s, {key_sql})'
```"
251421,Yelp/py_zipkin,Yelp_py_zipkin/py_zipkin/encoding/_encoders.py,py_zipkin.encoding._encoders.IEncoder,"from typing import Union
from py_zipkin.encoding._helpers import Span
from typing import List

class IEncoder:
    """"""Encoder interface.""""""

    def fits(self, current_count: int, current_size: int, max_size: int, new_span: Union[str, bytes]) -> bool:
        """"""Returns whether the new span will fit in the list.

        :param current_count: number of spans already in the list.
        :type current_count: int
        :param current_size: sum of the sizes of all the spans already in the list.
        :type current_size: int
        :param max_size: max supported transport payload size.
        :type max_size: int
        :param new_span: encoded span object that we want to add the the list.
        :type new_span: str or bytes
        :return: True if the new span can be added to the list, False otherwise.
        :rtype: bool
        """"""
        raise NotImplementedError()

    def encode_span(self, span: Span) -> Union[str, bytes]:
        """"""Encodes a single span.

        :param span: Span object representing the span.
        :type span: Span
        :return: encoded span.
        :rtype: str or bytes
        """"""
        raise NotImplementedError()

    def encode_queue(self, queue: List[Union[str, bytes]]) -> Union[str, bytes]:
        """"""Encodes a list of pre-encoded spans.

        :param queue: list of encoded spans.
        :type queue: list
        :return: encoded list, type depends on the encoding.
        :rtype: str or bytes
        """"""
        raise NotImplementedError()","
class IEncoder:
    
    def fits(self, current_count: int, current_size: int, max_size: int, new_span: Union[str, bytes]) -> bool:
        
        pass
    def encode_span(self, span: Span) -> Union[str, bytes]:
        '''Encodes a single span.
        :param span: Span object representing the span.
        :type span: Span
        :return: encoded span.
        :rtype: str or bytes
        '''
        pass
    def encode_queue(self, queue: List[Union[str, bytes]]) -> Union[str, bytes]:
        
        pass",snippet_158,4,"```python
from typing import Union, List

class IEncoder:
    """"""
    A simple encoder that works with strings and bytes.
    """"""

    def fits(self, current_count: int, current_size: int, max_size: int,
             new_span: Union[str, bytes]) -> bool:
        """"""
        Determine whether adding `new_span` to the current payload would exceed
        the maximum allowed size.

        :param current_count: Number of spans already in the payload (unused).
        :param current_size: Current size in bytes of the payload.
        :param max_size: Maximum allowed size in bytes.
        :param new_span: The new span to be added, as a string or bytes.
        :return: True if the new span can be added without exceeding `max_size`.
        """"""
        if isinstance(new_span, bytes):
            new_len = len(new_span)
        else:
            new_len = len(new_span.encode(""utf-8""))
        return current_size + new_len <= max_size

    def encode_span(self, span) -> Union[str, bytes]:
        """"""
        Encode a single span. The default implementation simply returns the
        string representation of the span. Subclasses may override this method
        to provide custom encoding logic.

        :param span: Span object representing the span.
        :return: Encoded span as a string or bytes.
        """"""
        # Default: use the span's __str__ representation.
        return str(span)

    def encode_queue(self, queue: List[Union[str, bytes]]) -> Union[str, bytes]:
        """"""
        Encode a queue of spans. The default implementation concatenates all
        elements, converting strings to UTF‑8 encoded bytes.

        :param queue: List of encoded spans (strings or bytes).
        :return: Concatenated payload as bytes.
        """"""
        # Convert all items to bytes and concatenate.
        encoded_parts = []
        for item in queue:
            if isinstance(item, bytes):
                encoded_parts.append(item)
            else:
                encoded_parts.append(item.encode(""utf-8""))
        return b"""".join(encoded_parts)
```"
251832,glut23/webvtt-py,glut23_webvtt-py/webvtt/models.py,webvtt.models.Style,"import typing

class Style:
    """"""Representation of a style.""""""

    def __init__(self, text: typing.Union[str, typing.List[str]]):
        """"""Initialize.

        :param: text: the style text
        """"""
        self.lines = text.splitlines() if isinstance(text, str) else text
        self.comments: typing.List[str] = []

    @property
    def text(self):
        """"""Return the text of the style.""""""
        return '\n'.join(self.lines)","
class Style:
    '''Representation of a style.'''
    def __init__(self, text: typing.Union[str, typing.List[str]]):
        '''Initialize.
        :param: text: the style text
        '''
        pass
    @property
    def text(self):
        
        pass",snippet_159,3,"```python
import typing

class Style:
    '''Representation of a style.'''
    def __init__(self, text: typing.Union[str, typing.List[str]]):
        '''Initialize.
        :param: text: the style text
        '''
        if isinstance(text, list):
            if not all(isinstance(t, str) for t in text):
                raise TypeError(""All elements of text list must be strings"")
            self._text = ""\n"".join(text)
        elif isinstance(text, str):
            self._text = text
        else:
            raise TypeError(""text must be a string or list of strings"")

    @property
    def text(self) -> str:
        return self._text
```"
251834,glut23/webvtt-py,glut23_webvtt-py/webvtt/sbv.py,webvtt.sbv.SBVCueBlock,"import typing
import re

class SBVCueBlock:
    """"""Representation of a cue timing block.""""""
    CUE_TIMINGS_PATTERN = re.compile('\\s*(\\d{1,2}:\\d{1,2}:\\d{1,2}.\\d{3}),(\\d{1,2}:\\d{1,2}:\\d{1,2}.\\d{3})')

    def __init__(self, start: str, end: str, payload: typing.Sequence[str]):
        """"""
        Initialize.

        :param start: start time
        :param end: end time
        :param payload: caption text
        """"""
        self.start = start
        self.end = end
        self.payload = payload

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a cue time block.

        :param lines: the lines to be validated
        :returns: true for a matching cue time block
        """"""
        return bool(len(lines) >= 2 and re.match(cls.CUE_TIMINGS_PATTERN, lines[0]) and lines[1].strip())

    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SBVCueBlock':
        """"""
        Create a `SBVCueBlock` from lines of text.

        :param lines: the lines of text
        :returns: `SBVCueBlock` instance
        """"""
        match = re.match(cls.CUE_TIMINGS_PATTERN, lines[0])
        assert match is not None
        payload = lines[1:]
        return cls(match.group(1), match.group(2), payload)","
class SBVCueBlock:
    '''Representation of a cue timing block.'''
    def __init__(self, start: str, end: str, payload: typing.Sequence[str]):
        
        pass
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        
        pass
    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SBVCueBlock':
        '''
        Create a `SBVCueBlock` from lines of text.
        :param lines: the lines of text
        :returns: `SBVCueBlock` instance
        '''
        pass",snippet_160,4,"```python
import typing
import re
from datetime import timedelta


class SBVCueBlock:
    '''Representation of a cue timing block.'''
    _TIMESTAMP_RE = re.compile(r'^\s*(\d+:\d{2}:\d{2}\.\d{2})\s*,\s*(\d+:\d{2}:\d{2}\.\d{2})\s*$')

    def __init__(self, start: str, end: str, payload: typing.Sequence[str]):
        self.start = start
        self.end = end
        self.payload = list(payload)

    @staticmethod
    def _parse_timestamp(ts: str) -> float:
        """"""Convert hh:mm:ss.xx to seconds.""""""
        h, m, s = ts.split(':')
        sec, frac = s.split('.')
        return (
            int(h) * 3600
            + int(m) * 60
            + int(sec)
            + int(frac) / 100
        )

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        if not lines:
            return False
        first = lines[0]
        m = cls._TIMESTAMP_RE.match(first)
        if not m:
            return False
        start_ts, end_ts = m.group(1), m.group(2)
        try:
            start_sec = cls._parse_timestamp(start_ts)
            end_sec = cls._parse_timestamp(end_ts)
        except Exception:
            return False
        if start_sec >= end_sec:
            return False
        # payload must exist (at least one line after the timestamp line)
        if len(lines) < 2:
            return False
        # allow empty payload lines? require at least one non-empty line
        payload_lines = [l for l in lines[1:] if l.strip() != '']
        return len(payload_lines) > 0

    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SBVCueBlock':
        """"""
        Create a `SBVCueBlock` from lines of text.
        :param lines: the lines of text
        :returns: `SBVCueBlock` instance
        """"""
        if not cls.is_valid(lines):
            raise ValueError(""Invalid SBV cue block"")
        first = lines[0]
        m = cls._TIMESTAMP_RE.match(first)
        start_ts, end_ts = m.group(1), m.group(2)
        # payload lines are all lines after the first until a blank line or end
        payload = []
        for line in lines[1:]:
            if line.strip() == '':
                break
            payload.append(line.rstrip('\n'))
        return cls(start_ts, end_ts, payload)
```"
251835,glut23/webvtt-py,glut23_webvtt-py/webvtt/srt.py,webvtt.srt.SRTCueBlock,"import typing
import re

class SRTCueBlock:
    """"""Representation of a cue timing block.""""""
    CUE_TIMINGS_PATTERN = re.compile('\\s*(\\d+:\\d{2}:\\d{2},\\d{3})\\s*-->\\s*(\\d+:\\d{2}:\\d{2},\\d{3})')

    def __init__(self, index: str, start: str, end: str, payload: typing.Sequence[str]):
        """"""
        Initialize.

        :param start: start time
        :param end: end time
        :param payload: caption text
        """"""
        self.index = index
        self.start = start
        self.end = end
        self.payload = payload

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a cue time block.

        :param lines: the lines to be validated
        :returns: true for a matching cue time block
        """"""
        return bool(len(lines) >= 3 and lines[0].isdigit() and re.match(cls.CUE_TIMINGS_PATTERN, lines[1]))

    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SRTCueBlock':
        """"""
        Create a `SRTCueBlock` from lines of text.

        :param lines: the lines of text
        :returns: `SRTCueBlock` instance
        """"""
        index = lines[0]
        match = re.match(cls.CUE_TIMINGS_PATTERN, lines[1])
        assert match is not None
        payload = lines[2:]
        return cls(index, match.group(1), match.group(2), payload)","
class SRTCueBlock:
    
    def __init__(self, index: str, start: str, end: str, payload: typing.Sequence[str]):
        
        pass
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        '''
        Validate the lines for a match of a cue time block.
        :param lines: the lines to be validated
        :returns: true for a matching cue time block
        '''
        pass
    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SRTCueBlock':
        
        pass",snippet_161,4,"```python
import re
import typing

class SRTCueBlock:
    TIME_PATTERN = re.compile(
        r'^\s*(\d{2}:\d{2}:\d{2},\d{3})\s*-->\s*(\d{2}:\d{2}:\d{2},\d{3})\s*$'
    )

    def __init__(self, index: str, start: str, end: str, payload: typing.Sequence[str]):
        self.index = index
        self.start = start
        self.end = end
        self.payload = list(payload)

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a cue time block.
        :param lines: the lines to be validated
        :returns: true for a matching cue time block
        """"""
        if not lines or len(lines) < 3:
            return False

        # Index line must be digits only
        if not lines[0].strip().isdigit():
            return False

        # Time line must match the pattern
        if not cls.TIME_PATTERN.match(lines[1]):
            return False

        # Payload must contain at least one non-empty line
        payload_lines = [l for l in lines[2:] if l.strip() != '']
        if not payload_lines:
            return False

        return True

    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SRTCueBlock':
        """"""
        Construct an SRTCueBlock from a sequence of lines.
        Assumes that the lines are already validated.
        """"""
        index = lines[0].strip()
        time_match = cls.TIME_PATTERN.match(lines[1])
        if not time_match:
            raise ValueError(""Invalid time format in cue block"")

        start, end = time_match.group(1), time_match.group(2)

        # Payload: all lines after the time line until a blank line or end
        payload = []
        for line in lines[2:]:
            if line.strip() == '':
                break
            payload.append(line.rstrip())

        return cls(index, start, end, payload)
```"
251838,glut23/webvtt-py,glut23_webvtt-py/webvtt/vtt.py,webvtt.vtt.WebVTTCommentBlock,"import typing
import re

class WebVTTCommentBlock:
    """"""Representation of a comment block.""""""
    COMMENT_PATTERN = re.compile('NOTE\\s(.*?)\\Z', re.DOTALL)

    def __init__(self, text: str):
        """"""
        Initialize.

        :param text: comment text
        """"""
        self.text = text

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a comment block.

        :param lines: the lines to be validated
        :returns: true for a matching comment block
        """"""
        return bool(lines and lines[0].startswith('NOTE'))

    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTCommentBlock':
        """"""
        Create a `WebVTTCommentBlock` from lines of text.

        :param lines: the lines of text
        :returns: `WebVTTCommentBlock` instance
        """"""
        match = cls.COMMENT_PATTERN.match('\n'.join(lines))
        return cls(text=match.group(1).strip() if match else '')

    @staticmethod
    def format_lines(lines: str) -> typing.List[str]:
        """"""
        Return the lines for a comment block.

        :param lines: comment lines
        :returns: list of lines for a comment block
        """"""
        list_of_lines = lines.split('\n')
        if len(list_of_lines) == 1:
            return [f'NOTE {lines}']
        return ['NOTE', *list_of_lines]","
class WebVTTCommentBlock:
    
    def __init__(self, text: str):
        
        pass
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        '''
        Validate the lines for a match of a comment block.
        :param lines: the lines to be validated
        :returns: true for a matching comment block
        '''
        pass
    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTCommentBlock':
        '''
        Create a `WebVTTCommentBlock` from lines of text.
        :param lines: the lines of text
        :returns: `WebVTTCommentBlock` instance
        '''
        pass
    @staticmethod
    def format_lines(lines: str) -> typing.List[str]:
        '''
        Return the lines for a comment block.
        :param lines: comment lines
        :returns: list of lines for a comment block
        '''
        pass",snippet_162,5,"```python
import typing

class WebVTTCommentBlock:
    """"""
    Represents a WebVTT comment block.
    """"""

    def __init__(self, text: str):
        """"""
        Initialise a comment block with the given text.
        :param text: The comment text (without the leading NOTE keyword).
        """"""
        self.text = text

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a comment block.
        :param lines: the lines to be validated
        :returns: true for a matching comment block
        """"""
        if not lines:
            return False

        first = lines[0]
        # Must start with NOTE and either be exactly ""NOTE"" or ""NOTE "" followed by text
        if not first.startswith(""NOTE""):
            return False
        if len(first) > 4 and first[4] != "" "":
            return False

        # Must end with an empty line
        if lines[-1] != """":
            return False

        # All lines between the first and the last must not be empty
        for line in lines[1:-1]:
            if line == """":
                return False

        return True

    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> ""WebVTTCommentBlock"":
        """"""
        Create a `WebVTTCommentBlock` from lines of text.
        :param lines: the lines of text
        :returns: `WebVTTCommentBlock` instance
        """"""
        lines = list(lines)
        if not cls.is_valid(lines):
            raise ValueError(""Provided lines do not form a valid WebVTT comment block"")

        # Extract comment text
        first = lines[0]
        comment = first[4:].lstrip()  # remove ""NOTE"" and leading whitespace
        for line in lines[1:-1]:
            comment += ""\n"" + line

        return cls(comment)

    @staticmethod
    def format_lines(lines: str) -> typing.List[str]:
        """"""
        Return the lines for a comment block.
        :param lines: comment lines
        :returns: list of lines for a comment block
        """"""
        # Split the comment into individual lines
        parts = lines.splitlines()
        if not parts:
            # Empty comment
            return [""NOTE"", """"]
        # First line: NOTE + first part
        first_line = ""NOTE "" + parts[0]
        # Remaining lines
        result = [first_line] + parts[1:] + [""""]
        return result
```"
251840,glut23/webvtt-py,glut23_webvtt-py/webvtt/vtt.py,webvtt.vtt.WebVTTStyleBlock,"import re
import typing

class WebVTTStyleBlock:
    """"""Representation of a style block.""""""
    STYLE_PATTERN = re.compile('STYLE\\s(.*?)\\Z', re.DOTALL)

    def __init__(self, text: str):
        """"""
        Initialize.

        :param text: style text
        """"""
        self.text = text

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a style block.

        :param lines: the lines to be validated
        :returns: true for a matching style block
        """"""
        return len(lines) >= 2 and lines[0] == 'STYLE' and (not any((line.strip() == '' or '-->' in line for line in lines)))

    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTStyleBlock':
        """"""
        Create a `WebVTTStyleBlock` from lines of text.

        :param lines: the lines of text
        :returns: `WebVTTStyleBlock` instance
        """"""
        match = cls.STYLE_PATTERN.match('\n'.join(lines))
        return cls(text=match.group(1).strip() if match else '')

    @staticmethod
    def format_lines(lines: typing.List[str]) -> typing.List[str]:
        """"""
        Return the lines for a style block.

        :param lines: style lines
        :returns: list of lines for a style block
        """"""
        return ['STYLE', *lines]","
class WebVTTStyleBlock:
    
    def __init__(self, text: str):
        
        pass
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        
        pass
    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTStyleBlock':
        '''
        Create a `WebVTTStyleBlock` from lines of text.
        :param lines: the lines of text
        :returns: `WebVTTStyleBlock` instance
        '''
        pass
    @staticmethod
    def format_lines(lines: typing.List[str]) -> typing.List[str]:
        
        pass",snippet_163,5,"```python
import typing

class WebVTTStyleBlock:
    """"""
    Represents a WebVTT style block.
    """"""

    def __init__(self, text: str):
        """"""
        Initialise the style block with the raw text.
        """"""
        self.text = text

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Determine whether the supplied lines form a valid WebVTT style block.
        A minimal valid block must start with the keyword 'STYLE' (case‑insensitive)
        and contain at least one additional line.
        """"""
        if not lines:
            return False
        # The first line must be exactly 'STYLE' (ignoring surrounding whitespace)
        first = lines[0].strip()
        if first.upper() != ""STYLE"":
            return False
        # There must be at least one more line after the header
        return len(lines) > 1

    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTStyleBlock':
        """"""
        Create a WebVTTStyleBlock from an iterable of lines.
        The lines are joined with a newline character to form the block text.
        """"""
        # Convert to a list to allow multiple passes
        line_list = list(lines)
        if not cls.is_valid(line_list):
            raise ValueError(""Provided lines do not form a valid WebVTT style block."")
        # Join lines preserving the original line breaks
        block_text = ""\n"".join(line.rstrip(""\n"") for line in line_list)
        return cls(block_text)

    @staticmethod
    def format_lines(lines: typing.List[str]) -> typing.List[str]:
        """"""
        Format a list of lines for a WebVTT style block.
        Strips leading/trailing whitespace from each line and ensures each line
        ends with a single newline character when joined later.
        """"""
        return [line.strip() for line in lines]
```"
251866,Shoobx/xmldiff,Shoobx_xmldiff/xmldiff/diff_match_patch.py,xmldiff.diff_match_patch.patch_obj,"import urllib.parse

class patch_obj:
    """"""Class representing one patch operation.""""""

    def __init__(self):
        """"""Initializes with an empty list of diffs.""""""
        self.diffs = []
        self.start1 = None
        self.start2 = None
        self.length1 = 0
        self.length2 = 0

    def __str__(self):
        """"""Emulate GNU diff's format.
        Header: @@ -382,8 +481,9 @@
        Indices are printed as 1-based, not 0-based.

        Returns:
          The GNU diff string.
        """"""
        if self.length1 == 0:
            coords1 = str(self.start1) + ',0'
        elif self.length1 == 1:
            coords1 = str(self.start1 + 1)
        else:
            coords1 = str(self.start1 + 1) + ',' + str(self.length1)
        if self.length2 == 0:
            coords2 = str(self.start2) + ',0'
        elif self.length2 == 1:
            coords2 = str(self.start2 + 1)
        else:
            coords2 = str(self.start2 + 1) + ',' + str(self.length2)
        text = ['@@ -', coords1, ' +', coords2, ' @@\n']
        for op, data in self.diffs:
            if op == diff_match_patch.DIFF_INSERT:
                text.append('+')
            elif op == diff_match_patch.DIFF_DELETE:
                text.append('-')
            elif op == diff_match_patch.DIFF_EQUAL:
                text.append(' ')
            data = data.encode('utf-8')
            text.append(urllib.parse.quote(data, ""!~*'();/?:@&=+$,# "") + '\n')
        return ''.join(text)","
class patch_obj:
    
    def __init__(self):
        '''Initializes with an empty list of diffs.'''
        pass
    def __str__(self):
        '''Emulate GNU diff's format.
        Header: @@ -382,8 +481,9 @@
        Indices are printed as 1-based, not 0-based.
        Returns:
          The GNU diff string.
        '''
        pass",snippet_164,3,"```python
class patch_obj:
    """"""
    A simple representation of a GNU diff patch.

    The patch is stored as a list of diff hunks.  Each hunk is a dictionary
    with the following keys:

        - old_start: 1‑based index of the first line in the original file
        - old_count: number of lines in the original file that the hunk covers
        - new_start: 1‑based index of the first line in the new file
        - new_count: number of lines in the new file that the hunk covers
        - lines: a list of strings, each prefixed with one of
                 ' ', '-', or '+' to indicate context, deletion, or addition.

    Example of a hunk:

        {
            'old_start': 10,
            'old_count': 3,
            'new_start': 12,
            'new_count': 4,
            'lines': [
                ' context line 1',
                '-removed line',
                '+added line',
                ' context line 2',
            ]
        }
    """"""

    def __init__(self):
        """"""Initializes with an empty list of diffs.""""""
        self.diffs = []

    def __str__(self):
        """"""
        Emulate GNU diff's format.
        Header: @@ -382,8 +481,9 @@
        Indices are printed as 1-based, not 0-based.
        Returns:
            The GNU diff string.
        """"""
        if not self.diffs:
            return """"

        parts = []
        for hunk in self.diffs:
            # Header
            old_start = hunk.get(""old_start"", 0)
            old_count = hunk.get(""old_count"", 0)
            new_start = hunk.get(""new_start"", 0)
            new_count = hunk.get(""new_count"", 0)
            header = f""@@ -{old_start},{old_count} +{new_start},{new_count} @@""
            parts.append(header)

            # Lines
            for line in hunk.get(""lines"", []):
                # Ensure the line starts with a diff marker
                if not line or line[0] not in ("" "", ""+"", ""-""):
                    # If the line is missing a marker, treat it as context
                    parts.append("" "" + line)
                else:
                    parts.append(line)

        return ""\n"".join(parts)
```"
252656,happyleavesaoc/aoc-mgz,happyleavesaoc_aoc-mgz/mgz/model/inputs.py,mgz.model.inputs.Inputs,"from mgz.model.definitions import Input
from mgz.fast import Action as ActionEnum

class Inputs:
    """"""Normalize player inputs.""""""

    def __init__(self, gaia):
        """"""Initialize.""""""
        self._gaia = gaia
        self._buildings = {}
        self._oid_cache = {}
        self.inputs = []

    def add_chat(self, chat):
        """"""Add chat input.""""""
        self.inputs.append(Input(chat.timestamp, 'Chat', None, dict(message=chat.message), chat.player, None))

    def add_action(self, action):
        """"""Add action input.""""""
        if action.type in (ActionEnum.DE_TRANSFORM, ActionEnum.POSTGAME):
            return
        name = ACTION_TRANSLATE.get(action.type, action.type.name).replace('_', ' ').title()
        param = None
        if 'object_ids' in action.payload and action.payload['object_ids']:
            self._oid_cache[action.type] = action.payload['object_ids']
        elif action.type in self._oid_cache:
            action.payload['object_ids'] = self._oid_cache[action.type]
        if action.type is ActionEnum.SPECIAL:
            name = action.payload['order']
        elif action.type is ActionEnum.GAME:
            name = action.payload['command']
            if name == 'Speed':
                param = action.payload['speed']
        elif action.type is ActionEnum.STANCE:
            name = 'Stance'
            param = action.payload['stance']
        elif action.type is ActionEnum.FORMATION:
            name = 'Formation'
            param = action.payload['formation']
        elif action.type is ActionEnum.ORDER and action.payload['target_id'] in self._gaia:
            name = 'Gather'
            param = self._gaia[action.payload['target_id']]
        elif action.type is ActionEnum.ORDER and action.position and (action.position.hash() in self._buildings):
            name = 'Target'
            param = self._buildings[action.position.hash()]
        elif action.type is ActionEnum.GATHER_POINT:
            if action.payload['target_id'] in self._gaia:
                param = self._gaia[action.payload['target_id']]
            elif action.position and action.position.hash() in self._buildings:
                if len(action.payload['object_ids']) == 1 and action.payload['object_ids'][0] == action.payload['target_id']:
                    name = 'Spawn'
                param = self._buildings[action.position.hash()]
        elif action.type in (ActionEnum.BUY, ActionEnum.SELL):
            action.payload['amount'] *= 100
        elif action.type is ActionEnum.BUILD:
            param = action.payload['building']
            if action.position.hash() in self._buildings:
                if self._buildings[action.position.hash()] == 'Farm' and action.payload['building'] == 'Farm':
                    name = 'Reseed'
            self._buildings[action.position.hash()] = action.payload['building']
        elif action.type in (ActionEnum.QUEUE, ActionEnum.DE_QUEUE):
            param = action.payload['unit']
        elif action.type is ActionEnum.RESEARCH:
            param = action.payload['technology']
        new_input = Input(action.timestamp, name, param, action.payload, action.player, action.position)
        self.inputs.append(new_input)
        return new_input","
class Inputs:
    
    def __init__(self, gaia):
        '''Initialize.'''
        pass
    def add_chat(self, chat):
        
        pass
    def add_action(self, action):
        
        pass",snippet_165,4,"class Inputs:
    def __init__(self, gaia):
        '''Initialize.'''
        self.gaia = gaia
        self.chats = []
        self.actions = []

    def add_chat(self, chat):
        self.chats.append(chat)

    def add_action(self, action):
        self.actions.append(action)"
256858,cherrypy/cheroot,cheroot/server.py,cheroot.server.Gateway,"class Gateway:
    """"""Base class to interface HTTPServer with other systems, such as WSGI.""""""

    def __init__(self, req):
        """"""Initialize Gateway instance with request.

        Args:
            req (HTTPRequest): current HTTP request
        """"""
        self.req = req

    def respond(self):
        """"""Process the current request. Must be overridden in a subclass.""""""
        raise NotImplementedError","class Gateway:
    '''Base class to interface HTTPServer with other systems, such as WSGI.'''
    def __init__(self, req):
        
        pass
    def respond(self):
        
        pass",snippet_168,3,"class Gateway:
    '''Base class to interface HTTPServer with other systems, such as WSGI.'''
    def __init__(self, req):
        # Store the incoming request object
        self.req = req
        # Default response components
        self.status = 200
        self.headers = {'Content-Type': 'text/plain'}
        self.body = b''

    def respond(self):
        """"""
        Return a tuple suitable for an HTTP response:
        (status_code, headers_dict, body_bytes)
        """"""
        return self.status, self.headers, self.body

    # Optional helper to set a custom response
    def set_response(self, status, headers, body):
        self.status = status
        self.headers = headers
        self.body = body if isinstance(body, (bytes, bytearray)) else body.encode('utf-8') if isinstance(body, str) else str(body).encode('utf-8')"
258632,chovanecm/sacredboard,chovanecm_sacredboard/sacredboard/app/data/datastorage.py,sacredboard.app.data.datastorage.Cursor,"class Cursor:
    """"""Interface that abstracts the cursor object returned from databases.""""""

    def __init__(self):
        """"""Declare a new cursor to iterate over runs.""""""
        pass

    def count(self):
        """"""Return the number of items in this cursor.""""""
        raise NotImplementedError()

    def __iter__(self):
        """"""Iterate over elements.""""""
        raise NotImplementedError()","class Cursor:
    
    def __init__(self):
        
        pass
    def count(self):
        '''Return the number of items in this cursor.'''
        pass
    def __iter__(self):
        
        pass",snippet_169,4,"class Cursor:
    def __init__(self, items=None):
        """"""Create a new cursor optionally initialized with an iterable of items.""""""
        self._items = list(items) if items is not None else []

    def count(self):
        """"""Return the number of items in this cursor.""""""
        return len(self._items)

    def __iter__(self):
        """"""Return an iterator over the cursor's items.""""""
        return iter(self._items)"
258643,chovanecm/sacredboard,chovanecm_sacredboard/sacredboard/app/data/metricsdao.py,sacredboard.app.data.metricsdao.MetricsDAO,"class MetricsDAO:
    """"""
    Interface for accessing Sacred metrics.

    Issue: https://github.com/chovanecm/sacredboard/issues/58

    Extended because of: https://github.com/chovanecm/sacredboard/issues/66
    """"""

    def get(self, run_id, metric_id):
        """"""
        Read a metric of the given id and run.

        The returned object has the following format (timestamps are datetime
         objects).

        .. code::

            {""steps"": [0,1,20,40,...],
            ""timestamps"": [timestamp1,timestamp2,timestamp3,...],
            ""values"": [0,1 2,3,4,5,6,...],
            ""name"": ""name of the metric"",
            ""metric_id"": ""metric_id"",
            ""run_id"": ""run_id""}

        :param run_id: ID of the Run that the metric belongs to.
        :param metric_id: The ID fo the metric.
        :return: The whole metric as specified.

        :raise NotFoundError
        """"""
        raise NotImplementedError('The MetricsDAO class is abstract.')

    def delete(self, run_id):
        """"""
        Delete all metrics belonging to the given run.

        :param run_id: ID of the Run that the metric belongs to.
        """"""
        raise NotImplementedError('The MetricsDAO class is abstract.')","class MetricsDAO:
    
    def get(self, run_id, metric_id):
        
        pass
    def delete(self, run_id):
        '''
        Delete all metrics belonging to the given run.
        :param run_id: ID of the Run that the metric belongs to.
        '''
        pass",snippet_170,3,"```python
import sqlite3
from contextlib import contextmanager
from typing import Any, Dict, Optional


class MetricsDAO:
    """"""
    Data Access Object for metrics stored in a SQLite database.
    The underlying table is expected to have the following schema:

        CREATE TABLE IF NOT EXISTS metrics (
            run_id   TEXT NOT NULL,
            metric_id TEXT NOT NULL,
            value    REAL,
            PRIMARY KEY (run_id, metric_id)
        );
    """"""

    def __init__(self, db_path: str = "":memory:"") -> None:
        """"""
        Initialize the DAO with a path to the SQLite database.
        If the database file does not exist, it will be created.
        """"""
        self._db_path = db_path
        self._ensure_table()

    @contextmanager
    def _connection(self) -> sqlite3.Connection:
        conn = sqlite3.connect(self._db_path)
        try:
            yield conn
        finally:
            conn.close()

    def _ensure_table(self) -> None:
        with self._connection() as conn:
            conn.execute(
                """"""
                CREATE TABLE IF NOT EXISTS metrics (
                    run_id    TEXT NOT NULL,
                    metric_id TEXT NOT NULL,
                    value     REAL,
                    PRIMARY KEY (run_id, metric_id)
                );
                """"""
            )
            conn.commit()

    def get(self, run_id: str, metric_id: str) -> Optional[Dict[str, Any]]:
        """"""
        Retrieve a single metric by run_id and metric_id.

        :param run_id: Identifier of the run.
        :param metric_id: Identifier of the metric.
        :return: A dictionary with keys 'run_id', 'metric_id', 'value' or None if not found.
        """"""
        with self._connection() as conn:
            cursor = conn.execute(
                """"""
                SELECT run_id, metric_id, value
                FROM metrics
                WHERE run_id = ? AND metric_id = ?
                """""",
                (run_id, metric_id),
            )
            row = cursor.fetchone()
            if row is None:
                return None
            return {""run_id"": row[0], ""metric_id"": row[1], ""value"": row[2]}

    def delete(self, run_id: str) -> int:
        """"""
        Delete all metrics belonging to the given run.

        :param run_id: ID of the Run that the metric belongs to.
        :return: Number of rows deleted.
        """"""
        with self._connection() as conn:
            cursor = conn.execute(
                """"""
                DELETE FROM metrics
                WHERE run_id = ?
                """""",
                (run_id,),
            )
            conn.commit()
            return cursor.rowcount

    # Optional helper methods for testing and convenience

    def insert(self, run_id: str, metric_id: str, value: float) -> None:
        """"""
        Insert or replace a metric. Useful for setting up test data.
        """"""
        with self._connection() as conn:
            conn.execute(
                """"""
                INSERT OR REPLACE INTO metrics (run_id, metric_id, value)
                VALUES (?, ?, ?)
                """""",
                (run_id, metric_id, value),
            )
            conn.commit()

    def list_metrics(self, run_id: str) -> Dict[str, float]:
        """"""
        Return all metrics for a given run as a dictionary {metric_id: value}.
        """"""
        with self._connection() as conn:
            cursor = conn.execute(
                """"""
                SELECT metric_id, value
                FROM metrics
                WHERE run_id = ?
                """""",
                (run_id,),
            )
            return {row[0]: row[1] for row in cursor.fetchall()}
```"
258651,chovanecm/sacredboard,chovanecm_sacredboard/sacredboard/app/data/rundao.py,sacredboard.app.data.rundao.RunDAO,"class RunDAO:
    """"""
    Interface for accessing Runs.

    Issue: https://github.com/chovanecm/sacredboard/issues/69
    """"""

    def get(self, run_id):
        """"""
        Return the run associated with the id.

        :raise NotFoundError when not found
        """"""
        raise NotImplementedError('RunDAO is abstract.')

    def get_runs(self, sort_by=None, sort_direction=None, start=0, limit=None, query={'type': 'and', 'filters': []}):
        """"""Return all runs that match the query.""""""
        raise NotImplementedError('RunDAO is abstract.')

    def delete(self, run_id):
        """"""
        Delete run with the given id from the backend.

        :param run_id: Id of the run to delete.
        :raise NotImplementedError If not supported by the backend.
        :raise DataSourceError General data source error.
        :raise NotFoundError The run was not found. (Some backends may succeed
        even if the run does not exist.
        """"""
        raise NotImplementedError('This database data source does not currently support this operation.')","class RunDAO:
    
    def get(self, run_id):
        '''
        Return the run associated with the id.
        :raise NotFoundError when not found
        '''
        pass
    def get_runs(self, sort_by=None, sort_direction=None, start=0, limit=None, query={'type': 'and', 'filters': []}):
        
        pass
    def delete(self, run_id):
        '''
        Delete run with the given id from the backend.
        :param run_id: Id of the run to delete.
        :raise NotImplementedError If not supported by the backend.
        :raise DataSourceError General data source error.
        :raise NotFoundError The run was not found. (Some backends may succeed
        even if the run does not exist.
        '''
        pass",snippet_171,4,"```python
# ----------------------------------------------------------------------
# Exceptions
# ----------------------------------------------------------------------
class NotFoundError(Exception):
    """"""Raised when a requested run cannot be found.""""""
    pass


class DataSourceError(Exception):
    """"""Raised for generic data source errors.""""""
    pass


# ----------------------------------------------------------------------
# RunDAO implementation
# ----------------------------------------------------------------------
class RunDAO:
    """"""
    A simple in‑memory DAO for run objects.

    Each run is expected to be a dictionary that contains at least an
    ``id`` key.  The DAO supports basic CRUD operations and a very
    small query language for filtering, sorting and pagination.
    """"""

    def __init__(self, initial_runs=None):
        """"""
        Create a new DAO instance.

        :param initial_runs: Optional iterable of run dictionaries.
        """"""
        self._runs = {}
        if initial_runs:
            for run in initial_runs:
                if ""id"" not in run:
                    raise ValueError(""Each run must have an 'id' field"")
                self._runs[run[""id""]] = run

    # ------------------------------------------------------------------
    # CRUD
    # ------------------------------------------------------------------
    def get(self, run_id):
        """"""
        Return the run associated with the id.

        :raise NotFoundError when not found
        """"""
        try:
            return self._runs[run_id]
        except KeyError:
            raise NotFoundError(f""Run with id {run_id} not found"")

    def delete(self, run_id):
        """"""
        Delete run with the given id from the backend.

        :param run_id: Id of the run to delete.
        :raise NotImplementedError If not supported by the backend.
        :raise DataSourceError General data source error.
        :raise NotFoundError The run was not found.
        """"""
        try:
            del self._runs[run_id]
        except KeyError:
            raise NotFoundError(f""Run with id {run_id} not found"")

    # ------------------------------------------------------------------
    # Querying
    # ------------------------------------------------------------------
    def get_runs(
        self,
        sort_by=None,
        sort_direction=None,
        start=0,
        limit=None,
        query={""type"": ""and"", ""filters"": []},
    ):
        """"""
        Return a list of runs that match the supplied query.

        Parameters
        ----------
        sort_by : str or None
            Field name to sort on.
        sort_direction : str or None
            'asc' or 'desc'.
        start : int
            Zero‑based index of the first result to return.
        limit : int or None
            Maximum number of results to return.
        query : dict
            Simple query structure:
                {
                    ""type"": ""and"" | ""or"",
                    ""filters"": [
                        {""field"": str, ""op"": str, ""value"": any},
                        ...
                    ]
                }
            Supported ops: eq, ne, lt, lte, gt, gte, contains

        Returns
        -------
        list of run dicts
        """"""
        # 1. Filter
        runs = list(self._runs.values())
        if query and query.get(""filters""):
            runs = self._apply_filters(runs, query)

        # 2. Sort
        if sort_by:
            reverse = sort_direction == ""desc""
            runs.sort(key=lambda r: r.get(sort_by), reverse=reverse)

        # 3. Pagination
        end = None if limit is None else start + limit
        return runs[start:end]

    # ------------------------------------------------------------------
    # Helper methods
    # ------------------------------------------------------------------
    def _apply_filters(self, runs, query):
        """"""Apply the simple filter logic to a list of runs.""""""
        op_funcs = {
            ""eq"": lambda a, b: a == b,
            ""ne"": lambda a, b: a != b,
            ""lt"": lambda a, b: a < b,
            ""lte"": lambda a, b: a <= b,
            ""gt"": lambda a, b: a > b,
            ""gte"": lambda a, b: a >= b,
            ""contains"": lambda a, b: b in a if isinstance(a, (str, list, tuple, set)) else False,
        }

        def match(run, filt):
            field = filt[""field""]
            op = filt[""op""]
            value = filt[""value""]
            if op not in op_funcs:
                raise ValueError(f""Unsupported operator: {op}"")
            return op_funcs[op](run.get(field), value)

        if query[""type""] == ""and"":
            return [r for r in runs if all(match(r, f) for f in query[""filters""])]
        elif query[""type""] == ""or"":
            return [r for r in runs if any(match(r, f) for f in query[""filters""])]
        else:
            raise ValueError(f""Unsupported query type: {query['type']}"")
```"
259541,mozilla-iot/webthing-python,mozilla-iot_webthing-python/webthing/server.py,webthing.server.MultipleThings,"class MultipleThings:
    """"""A container for multiple things.""""""

    def __init__(self, things, name):
        """"""
        Initialize the container.

        things -- the things to store
        name -- the mDNS server name
        """"""
        self.things = things
        self.name = name

    def get_thing(self, idx):
        """"""
        Get the thing at the given index.

        idx -- the index
        """"""
        try:
            idx = int(idx)
        except ValueError:
            return None
        if idx < 0 or idx >= len(self.things):
            return None
        return self.things[idx]

    def get_things(self):
        """"""Get the list of things.""""""
        return self.things

    def get_name(self):
        """"""Get the mDNS server name.""""""
        return self.name","class MultipleThings:
    
    def __init__(self, things, name):
        '''
        Initialize the container.
        things -- the things to store
        name -- the mDNS server name
        '''
        pass
    def get_thing(self, idx):
        
        pass
    def get_things(self):
        '''Get the list of things.'''
        pass
    def get_name(self):
        
        pass",snippet_172,5,"class MultipleThings:
    
    def __init__(self, things, name):
        '''
        Initialize the container.
        things -- the things to store
        name -- the mDNS server name
        '''
        self._things = list(things) if things is not None else []
        self._name = name

    def get_thing(self, idx):
        return self._things[idx]

    def get_things(self):
        '''Get the list of things.'''
        return self._things

    def get_name(self):
        return self._name"
259544,mozilla-iot/webthing-python,mozilla-iot_webthing-python/webthing/server.py,webthing.server.SingleThing,"class SingleThing:
    """"""A container for a single thing.""""""

    def __init__(self, thing):
        """"""
        Initialize the container.

        thing -- the thing to store
        """"""
        self.thing = thing

    def get_thing(self, _=None):
        """"""Get the thing at the given index.""""""
        return self.thing

    def get_things(self):
        """"""Get the list of things.""""""
        return [self.thing]

    def get_name(self):
        """"""Get the mDNS server name.""""""
        return self.thing.title","class SingleThing:
    '''A container for a single thing.'''
    def __init__(self, thing):
        '''
        Initialize the container.
        thing -- the thing to store
        '''
        pass
    def get_thing(self, _=None):
        
        pass
    def get_things(self):
        '''Get the list of things.'''
        pass
    def get_name(self):
        '''Get the mDNS server name.'''
        pass",snippet_173,5,"class SingleThing:
    '''A container for a single thing.'''
    def __init__(self, thing):
        '''
        Initialize the container.
        thing -- the thing to store
        '''
        self._thing = thing

    def get_thing(self, _=None):
        '''Return the stored thing.'''
        return self._thing

    def get_things(self):
        '''Get the list of things.'''
        return [self._thing]

    def get_name(self):
        '''Get the mDNS server name.'''
        # If the thing itself is a string, return it.
        if isinstance(self._thing, str):
            return self._thing
        # Try to get a 'name' attribute.
        return getattr(self._thing, 'name', None)"
259548,mozilla-iot/webthing-python,mozilla-iot_webthing-python/webthing/subscriber.py,webthing.subscriber.Subscriber,"class Subscriber:
    """"""Abstract Subscriber class.""""""

    def update_property(self, property_):
        """"""
        Send an update about a Property.

        :param property_: Property
        """"""
        raise NotImplementedError

    def update_action(self, action):
        """"""
        Send an update about an Action.

        :param action: Action
        """"""
        raise NotImplementedError

    def update_event(self, event):
        """"""
        Send an update about an Event.

        :param event: Event
        """"""
        raise NotImplementedError","class Subscriber:
    
    def update_property(self, property_):
        
        pass
    def update_action(self, action):
        '''
        Send an update about an Action.
        :param action: Action
        '''
        pass
    def update_event(self, event):
        '''
        Send an update about an Event.
        :param event: Event
        '''
        pass",snippet_174,4,"```python
class Subscriber:
    def __init__(self):
        # Store all updates received by this subscriber
        self.updates = []

    def update_property(self, property_):
        """"""
        Receive an update about a property change.
        :param property_: Any object representing the property.
        """"""
        self.updates.append(('property', property_))

    def update_action(self, action):
        """"""
        Send an update about an Action.
        :param action: Action
        """"""
        self.updates.append(('action', action))

    def update_event(self, event):
        """"""
        Send an update about an Event.
        :param event: Event
        """"""
        self.updates.append(('event', event))
```"
261819,user-cont/conu,user-cont_conu/conu/apidefs/image.py,conu.apidefs.image.S2Image,"class S2Image:
    """"""
    Additional functionality related to s2i-enabled container images
    """"""

    def extend(self, source, new_image_name, s2i_args=None):
        """"""
        extend this s2i-enabled image using provided source, raises ConuException if
        `s2i build` fails

        :param source: str, source used to extend the image, can be path or url
        :param new_image_name: str, name of the new, extended image
        :param s2i_args: list of str, additional options and arguments provided to `s2i build`
        :return: S2Image instance
        """"""
        raise NotImplementedError('extend method is not implemented')

    def usage(self):
        """"""
        Provide output of `s2i usage`

        :return: str
        """"""
        raise NotImplementedError('usage method is not implemented')","class S2Image:
    
    def extend(self, source, new_image_name, s2i_args=None):
        '''
        extend this s2i-enabled image using provided source, raises ConuException if
        `s2i build` fails
        :param source: str, source used to extend the image, can be path or url
        :param new_image_name: str, name of the new, extended image
        :param s2i_args: list of str, additional options and arguments provided to `s2i build`
        :return: S2Image instance
        '''
        pass
    def usage(self):
        '''
        Provide output of `s2i usage`
        :return: str
        '''
        pass",snippet_175,3,"```python
import subprocess
from typing import List, Optional

try:
    from conu.exceptions import ConuException
except ImportError:
    class ConuException(Exception):
        pass


class S2Image:
    def extend(
        self,
        source: str,
        new_image_name: str,
        s2i_args: Optional[List[str]] = None,
    ) -> ""S2Image"":
        """"""
        extend this s2i-enabled image using provided source, raises ConuException if
        `s2i build` fails
        :param source: str, source used to extend the image, can be path or url
        :param new_image_name: str, name of the new, extended image
        :param s2i_args: list of str, additional options and arguments provided to `s2i build`
        :return: S2Image instance
        """"""
        cmd = [""s2i"", ""build"", source, new_image_name]
        if s2i_args:
            cmd.extend(s2i_args)

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=False,
        )

        if result.returncode != 0:
            raise ConuException(
                f""s2i build failed with exit code {result.returncode}\n""
                f""stdout: {result.stdout}\n""
                f""stderr: {result.stderr}""
            )

        return self

    def usage(self) -> str:
        """"""
        Provide output of `s2i usage`
        :return: str
        """"""
        result = subprocess.run(
            [""s2i"", ""usage""],
            capture_output=True,
            text=True,
            check=False,
        )

        if result.returncode != 0:
            raise ConuException(
                f""s2i usage failed with exit code {result.returncode}\n""
                f""stdout: {result.stdout}\n""
                f""stderr: {result.stderr}""
            )

        return result.stdout
```"
264367,materialsproject/custodian,materialsproject_custodian/src/custodian/ansible/interpreter.py,custodian.ansible.interpreter.Modder,"from custodian.ansible.actions import DictActions
import re

class Modder:
    """"""
    Class to modify a dict/file/any object using a mongo-like language.
    Keywords are mostly adopted from mongo's syntax, but instead of $, an
    underscore precedes action keywords. This is so that the modification can
    be inserted into a mongo db easily.

    Allowable actions are supplied as a list of classes as an argument. Refer
    to the action classes on what the actions do. Action classes are in
    pymatpro.ansible.actions.

    Examples:
    >>> modder = Modder()
    >>> dct = {""Hello"": ""World""}
    >>> mod = {'_set': {'Hello':'Universe', 'Bye': 'World'}}
    >>> modder.modify(mod, dct)
    >>> dct['Bye']
    'World'
    >>> dct['Hello']
    'Universe'
    """"""

    def __init__(self, actions=None, strict=True, directory='./') -> None:
        """"""Initialize a Modder from a list of supported actions.

        Args:
            actions ([Action]): A sequence of supported actions. See
                :mod:`custodian.ansible.actions`. Default is None,
                which means only DictActions are supported.
            strict (bool): Indicating whether to use strict mode. In non-strict
                mode, unsupported actions are simply ignored without any
                errors raised. In strict mode, if an unsupported action is
                supplied, a ValueError is raised. Defaults to True.
            directory (str): The directory containing the files to be modified.
                Defaults to ""./"".
        """"""
        self.supported_actions = {}
        actions = actions if actions is not None else [DictActions]
        for action in actions:
            for attr in dir(action):
                if not re.match('__\\w+__', attr) and callable(getattr(action, attr)):
                    self.supported_actions[f'_{attr}'] = getattr(action, attr)
        self.strict = strict
        self.directory = directory

    def modify(self, modification, obj) -> None:
        """"""
        Note that modify makes actual in-place modifications. It does not
        return a copy.

        Args:
            modification (dict): Modification must be {action_keyword :
                settings}. E.g., {'_set': {'Hello':'Universe', 'Bye': 'World'}}
            obj (dict/str/object): Object to modify depending on actions. For
                example, for DictActions, obj will be a dict to be modified.
                For FileActions, obj will be a string with a full pathname to a
                file.
        """"""
        for action, settings in modification.items():
            if action in self.supported_actions:
                self.supported_actions[action](obj, settings, directory=self.directory)
            elif self.strict:
                raise ValueError(f'{action} is not a supported action!')

    def modify_object(self, modification, obj):
        """"""
        Modify an object that supports pymatgen's as_dict() and from_dict API.

        Args:
            modification (dict): Modification must be {action_keyword :
                settings}. E.g., {'_set': {'Hello':'Universe', 'Bye': 'World'}}
            obj (object): Object to modify
        """"""
        dct = obj.as_dict()
        self.modify(modification, dct)
        return obj.from_dict(dct)","
class Modder:
    '''
    Class to modify a dict/file/any object using a mongo-like language.
    Keywords are mostly adopted from mongo's syntax, but instead of $, an
    underscore precedes action keywords. This is so that the modification can
    be inserted into a mongo db easily.
    Allowable actions are supplied as a list of classes as an argument. Refer
    to the action classes on what the actions do. Action classes are in
    pymatpro.ansible.actions.
    Examples:
    >>> modder = Modder()
    >>> dct = {""Hello"": ""World""}
    >>> mod = {'_set': {'Hello':'Universe', 'Bye': 'World'}}
    >>> modder.modify(mod, dct)
    >>> dct['Bye']
    'World'
    >>> dct['Hello']
    'Universe'
    '''
    def __init__(self, actions=None, strict=True, directory='./') -> None:
        
        pass
    def modify(self, modification, obj) -> None:
        
        pass
    def modify_object(self, modification, obj):
        '''
        Modify an object that supports pymatgen's as_dict() and from_dict API.
        Args:
            modification (dict): Modification must be {action_keyword :
                settings}. E.g., {'_set': {'Hello':'Universe', 'Bye': 'World'}}
            obj (object): Object to modify
        '''
        pass",snippet_176,4,"```python
import json
import os
import inspect
from typing import Any, Dict, List, Optional, Type

# Import all action classes from pymatpro.ansible.actions if available
try:
    from pymatpro.ansible import actions as _actions_module
    _action_classes = [
        cls for _, cls in inspect.getmembers(_actions_module, inspect.isclass)
        if hasattr(cls, ""keyword"")
    ]
except Exception:
    # If the module is not available, fall back to an empty list
    _action_classes = []


class Modder:
    """"""
    Class to modify a dict/file/any object using a mongo-like language.
    Keywords are mostly adopted from mongo's syntax, but instead of `$`,
    an underscore precedes action keywords. This is so that the modification
    can be inserted into a mongo db easily.
    Allowable actions are supplied as a list of classes as an argument.
    Refer to the action classes on what the actions do. Action classes are
    in pymatpro.ansible.actions.
    Examples:
    >>> modder = Modder()
    >>> dct = {""Hello"": ""World""}
    >>> mod = {'_set': {'Hello':'Universe', 'Bye': 'World'}}
    >>> modder.modify(mod, dct)
    >>> dct['Bye']
    'World'
    >>> dct['Hello']
    'Universe'
    """"""

    def __init__(
        self,
        actions: Optional[List[Type]] = None,
        strict: bool = True,
        directory: str = ""./"",
    ) -> None:
        """"""
        Parameters
        ----------
        actions : list of classes, optional
            List of action classes that implement a `keyword` attribute
            and a `modify` method. If None, defaults to the actions
            discovered from pymatpro.ansible.actions.
        strict : bool, optional
            If True, raise an error when an unknown action keyword is
            encountered. If False, silently ignore unknown keywords.
        directory : str, optional
            Base directory for relative file paths.
        """"""
        if actions is None:
            actions = _action_classes
        self._action_map: Dict[str, Type] = {}
        for act_cls in actions:
            kw = getattr(act_cls, ""keyword"", None)
            if kw:
                self._action_map[kw] = act_cls
        self.strict = strict
        self.directory = os.path.abspath(directory)

    def modify(self, modification: Dict[str, Any], obj: Any) -> None:
        """"""
        Modify an object that supports pymatgen's as_dict() and from_dict API.
        Parameters
        ----------
        modification : dict
            Modification must be {action_keyword : settings}. E.g.,
            {'_set': {'Hello':'Universe', 'Bye': 'World'}}
        obj : object
            Object to modify. Can be a dict, a file path (JSON), or an
            object that implements as_dict() and from_dict().
        """"""
        if isinstance(obj, dict):
            self._modify_dict(modification, obj)
        elif isinstance(obj, str) and os.path.isfile(os.path.join(self.directory, obj)):
            self._modify_file(modification, obj)
        elif hasattr(obj, ""as_dict"") and hasattr(obj, ""from_dict""):
            self.modify_object(modification, obj)
        else:
            raise TypeError(
                f""Unsupported object type for modification: {type(obj).__name__}""
            )

    def _modify_dict(self, modification: Dict[str, Any], dct: Dict[str, Any]) -> None:
        for key, settings in modification.items():
            act_cls = self._action_map.get(key)
            if act_cls is None:
                if self.strict:
                    raise ValueError(f""Unknown action keyword: {key}"")
                continue
            action = act_cls(settings)
            action.modify(dct)

    def _modify_file(self, modification: Dict[str, Any], file_path: str) -> None:
        full_path = os.path.join(self.directory, file_path)
        with open(full_path, ""r"", encoding=""utf-8"") as f:
            data = json.load(f)
        if not isinstance(data, dict):
            raise ValueError(""JSON file must contain a top-level object"")
        self._modify_dict(modification, data)
        with open(full_path, ""w"", encoding=""utf-8"") as f:
            json.dump(data, f, indent=2, sort_keys=True)

    def modify_object(self, modification: Dict[str, Any], obj: Any) -> None:
        """"""
        Modify an object that supports pymatgen's as_dict() and from_dict API.
        Parameters
        ----------
        modification : dict
            Modification must be {action_keyword : settings}. E.g.,
            {'_set': {'Hello':'Universe', 'Bye': 'World'}}
        obj : object
            Object to modify
        """"""
        dct = obj.as_dict()
        self._modify_dict(modification, dct)
        # Update the object's attributes in place
        for key, value in dct.items():
            setattr(obj, key, value)
```"
264406,materialsproject/custodian,materialsproject_custodian/src/custodian/utils.py,custodian.utils.tracked_lru_cache,"import functools
from typing import ClassVar

class tracked_lru_cache:
    """"""
    Decorator wrapping the functools.lru_cache adding a tracking of the
    functions that have been wrapped.

    Exposes a method to clear the cache of all the wrapped functions.

    Used to cache the parsed outputs in handlers/validators, to avoid
    multiple parsing of the same file.
    Allows Custodian to clear the cache after all the checks have been performed.
    """"""
    cached_functions: ClassVar = set()

    def __init__(self, func) -> None:
        """"""
        Args:
            func: function to be decorated.
        """"""
        self.func = functools.lru_cache(func)
        functools.update_wrapper(self, func)
        self.cache_info = self.func.cache_info
        self.cache_clear = self.func.cache_clear

    def __call__(self, *args, **kwargs):
        """"""Call the decorated function.""""""
        result = self.func(*args, **kwargs)
        self.cached_functions.add(self.func)
        return result

    @classmethod
    def tracked_cache_clear(cls) -> None:
        """"""Clear the cache of all the decorated functions.""""""
        while cls.cached_functions:
            f = cls.cached_functions.pop()
            f.cache_clear()","
class tracked_lru_cache:
    '''
    Decorator wrapping the functools.lru_cache adding a tracking of the
    functions that have been wrapped.
    Exposes a method to clear the cache of all the wrapped functions.
    Used to cache the parsed outputs in handlers/validators, to avoid
    multiple parsing of the same file.
    Allows Custodian to clear the cache after all the checks have been performed.
    '''
    def __init__(self, func) -> None:
        
        pass
    def __call__(self, *args, **kwargs):
        
        pass
    @classmethod
    def tracked_cache_clear(cls) -> None:
        
        pass",snippet_177,4,"```python
import functools
from typing import Callable, Any, Set


class tracked_lru_cache:
    """"""
    Decorator that wraps a function with ``functools.lru_cache`` and keeps a
    registry of all wrapped functions.  The registry can be cleared with
    :meth:`tracked_cache_clear`, which calls ``cache_clear`` on each cached
    function.
    """"""

    # Registry of all cached functions
    _tracked: Set[Callable] = set()

    def __init__(self, func: Callable) -> None:
        # Store the original function
        self._func = func
        # Create the cached version
        self._cached = functools.lru_cache(maxsize=None)(func)
        # Register the cached function
        self.__class__._tracked.add(self._cached)

        # Preserve function metadata
        functools.update_wrapper(self, func)

    def __call__(self, *args: Any, **kwargs: Any) -> Any:
        return self._cached(*args, **kwargs)

    @classmethod
    def tracked_cache_clear(cls) -> None:
        """"""
        Clears the cache of all functions that have been wrapped by
        :class:`tracked_lru_cache`.
        """"""
        for cached_func in list(cls._tracked):
            try:
                cached_func.cache_clear()
            except AttributeError:
                # In case the cached function does not expose cache_clear
                pass
```"
265199,Anaconda-Platform/anaconda-client,Anaconda-Platform_anaconda-client/binstar_client/utils/tables.py,binstar_client.utils.tables.TableCell,"import typing

class TableCell:
    """"""
    General definition of a table cell.

    :param kind: Kind of the cell (used for styling purposes, see :class:`~TableDesign`).
    :param value: Exact content of the cell.
    :param alignment: How text should be aligned in the cell.
    """"""
    __slots__ = ('alignment', 'kind', 'value')

    def __init__(self, kind: str, value: typing.Any, *, alignment: 'Alignment'='<') -> None:
        """"""Initialize new :class:`~TableCell` instance.""""""
        if value is None:
            value = ''
        self.alignment: 'Alignment' = alignment
        self.kind: str = kind
        self.value: str = str(value)

    def __repr__(self) -> str:
        """"""Prepare a string representation of the instance.""""""
        return f'{type(self).__name__}(kind={self.kind!r}, value={self.value!r}, alignment={self.alignment!r})'

    def __str__(self) -> str:
        """"""Prepare a string representation of the instance.""""""
        return self.value","
class TableCell:
    '''
    General definition of a table cell.
    :param kind: Kind of the cell (used for styling purposes, see :class:`~TableDesign`).
    :param value: Exact content of the cell.
    :param alignment: How text should be aligned in the cell.
    '''
    def __init__(self, kind: str, value: typing.Any, *, alignment: 'Alignment'='<') -> None:
        '''Initialize new :class:`~TableCell` instance.'''
        pass
    def __repr__(self) -> str:
        '''Prepare a string representation of the instance.'''
        pass
    def __str__(self) -> str:
        '''Prepare a string representation of the instance.'''
        pass",snippet_178,4,"```python
import typing

class TableCell:
    '''
    General definition of a table cell.
    :param kind: Kind of the cell (used for styling purposes, see :class:`~TableDesign`).
    :param value: Exact content of the cell.
    :param alignment: How text should be aligned in the cell.
    '''
    def __init__(self, kind: str, value: typing.Any, *, alignment: 'Alignment' = '<') -> None:
        '''Initialize new :class:`~TableCell` instance.'''
        self.kind = kind
        self.value = value
        self.alignment = alignment

    def __repr__(self) -> str:
        '''Prepare a string representation of the instance.'''
        return (
            f""{self.__class__.__name__}""
            f""(kind={self.kind!r}, value={self.value!r}, alignment={self.alignment!r})""
        )

    def __str__(self) -> str:
        '''Prepare a string representation of the instance.'''
        return str(self.value)
```"
266878,facelessuser/wcmatch,facelessuser_wcmatch/wcmatch/util.py,wcmatch.util.Immutable,"from typing import Any, Callable, AnyStr, Match, Pattern

class Immutable:
    """"""Immutable.""""""
    __slots__: tuple[Any, ...] = ()

    def __init__(self, **kwargs: Any) -> None:
        """"""Initialize.""""""
        for k, v in kwargs.items():
            super(Immutable, self).__setattr__(k, v)

    def __setattr__(self, name: str, value: Any) -> None:
        """"""Prevent mutability.""""""
        raise AttributeError('Class is immutable!')","
class Immutable:
    '''Immutable.'''
    def __init__(self, **kwargs: Any) -> None:
        
        pass
    def __setattr__(self, name: str, value: Any) -> None:
        
        pass",snippet_180,3,"```python
from typing import Any

class Immutable:
    '''Immutable.'''
    def __init__(self, **kwargs: Any) -> None:
        # Set initial attributes directly, bypassing __setattr__
        for key, value in kwargs.items():
            object.__setattr__(self, key, value)
        # Mark the instance as initialized
        object.__setattr__(self, ""_initialized"", True)

    def __setattr__(self, name: str, value: Any) -> None:
        # Allow setting attributes only before initialization
        if getattr(self, ""_initialized"", False):
            raise AttributeError(f""Cannot modify immutable attribute '{name}'"")
        object.__setattr__(self, name, value)
```"
269275,CLARIAH/grlc,CLARIAH_grlc/src/fileLoaders.py,src.fileLoaders.BaseLoader,"from grlc.queryTypes import qType, guessQueryType
import json

class BaseLoader:
    """"""Base class for File Loaders""""""

    def getTextForName(self, query_name):
        """"""Return the query text and query type for the given query name.
        Note that file extention is not part of the query name. For example,
        for `query_name='query1'` would return the content of file `query1.rq`
        from the loader's source (assuming such file exists).""""""
        candidateNames = [query_name + '.rq', query_name + '.sparql', query_name + '.tpf', query_name + '.json']
        candidates = [(name, guessQueryType(name)) for name in candidateNames]
        for queryFullName, queryType in candidates:
            queryText = self._getText(queryFullName)
            if queryText:
                if queryType == qType['JSON']:
                    queryText = json.loads(queryText)
                    if 'proto' not in queryText and '@graph' not in queryText:
                        continue
                return (queryText, queryType)
        return ('', None)

    def _getText(self, queryFullName):
        """"""To be implemented by sub-classes.
        Returns None if the file does not exist.""""""
        raise NotImplementedError('Subclasses must override _getText()!')

    def fetchFiles(self):
        """"""To be implemented by sub-classes""""""
        raise NotImplementedError('Subclasses must override fetchFiles()!')","
class BaseLoader:
    '''Base class for File Loaders'''
    def getTextForName(self, query_name):
        '''Return the query text and query type for the given query name.
        Note that file extention is not part of the query name. For example,
        for `query_name='query1'` would return the content of file `query1.rq`
        from the loader's source (assuming such file exists).'''
        pass
    def _getText(self, queryFullName):
        
        pass
    def fetchFiles(self):
        
        pass",snippet_181,4,"```python
import os
from pathlib import Path
from typing import List, Tuple


class BaseLoader:
    """"""Base class for File Loaders""""""

    def __init__(self, source_dir: str | Path):
        """"""
        Parameters
        ----------
        source_dir : str | Path
            Directory that contains query files.  The directory must exist.
        """"""
        self.source_dir = Path(source_dir).expanduser().resolve()
        if not self.source_dir.is_dir():
            raise ValueError(f""Source directory {self.source_dir!s} does not exist or is not a directory"")

    def getTextForName(self, query_name: str) -> Tuple[str, str]:
        """"""
        Return the query text and query type for the given query name.

        The file extension is not part of the query name.  For example,
        for ``query_name='query1'`` this will return the content of
        ``query1.rq`` from the loader's source (assuming such file exists).

        Parameters
        ----------
        query_name : str
            Name of the query without extension.

        Returns
        -------
        tuple
            ``(text, query_type)`` where ``text`` is the file contents and
            ``query_type`` is the file extension without the leading dot.

        Raises
        ------
        FileNotFoundError
            If the query file cannot be found.
        """"""
        # Build the expected file path
        file_path = self.source_dir / f""{query_name}.rq""
        if not file_path.is_file():
            raise FileNotFoundError(f""Query file {file_path!s} not found"")
        return self._getText(file_path)

    def _getText(self, queryFullName: str | Path) -> Tuple[str, str]:
        """"""
        Internal helper to read a file and return its contents and type.

        Parameters
        ----------
        queryFullName : str | Path
            Full path to the query file.

        Returns
        -------
        tuple
            ``(text, query_type)`` where ``text`` is the file contents and
            ``query_type`` is the file extension without the leading dot.

        Raises
        ------
        FileNotFoundError
            If the file does not exist.
        """"""
        path = Path(queryFullName).expanduser().resolve()
        if not path.is_file():
            raise FileNotFoundError(f""File {path!s} not found"")
        text = path.read_text(encoding=""utf-8"")
        query_type = path.suffix.lstrip(""."")
        return text, query_type

    def fetchFiles(self) -> List[str]:
        """"""
        Return a list of available query names (without extensions).

        Returns
        -------
        list[str]
            List of query names found in the source directory.
        """"""
        return [p.stem for p in self.source_dir.glob(""*.rq"") if p.is_file()]
```"
271814,shoebot/shoebot,shoebot_shoebot/lib/photobot/__init__.py,photobot.Blend,"from PIL import Image, ImageChops, ImageFilter, ImageEnhance, ImageOps, ImageDraw, ImageStat

class Blend:
    """"""Layer blending modes.

    Implements additional blending modes to those present in PIL.
    These blending functions can not be used separately from
    the canvas.flatten() method, where the alpha compositing
    of two layers is handled.

    Since these blending are not part of a C library,
    but pure Python, they take forever to process.
    """"""

    def overlay(self, img1, img2):
        """"""Applies the overlay blend mode.

        Overlays image img2 on image img1. The overlay pixel combines
        multiply and screen: it multiplies dark pixels values and screen
        light values. Returns a composite image with the alpha channel
        retained.
        """"""
        p1 = list(img1.getdata())
        p2 = list(img2.getdata())
        for i in range(len(p1)):
            p3 = ()
            for j in range(len(p1[i])):
                a = p1[i][j] / 255.0
                b = p2[i][j] / 255.0
                if j == 3:
                    d = min(a, b)
                elif a > 0.5:
                    d = 2 * (a + b - a * b) - 1
                else:
                    d = 2 * a * b
                p3 += (int(d * 255),)
            p1[i] = p3
        img = Image.new('RGBA', img1.size, 255)
        img.putdata(p1)
        return img

    def hue(self, img1, img2):
        """"""Applies the hue blend mode.

        Hues image img1 with image img2. The hue filter replaces the
        hues of pixels in img1 with the hues of pixels in img2. Returns
        a composite image with the alpha channel retained.
        """"""
        import colorsys
        p1 = list(img1.getdata())
        p2 = list(img2.getdata())
        for i in range(len(p1)):
            r1, g1, b1, a1 = p1[i]
            r1 = r1 / 255.0
            g1 = g1 / 255.0
            b1 = b1 / 255.0
            h1, s1, v1 = colorsys.rgb_to_hsv(r1, g1, b1)
            r2, g2, b2, a2 = p2[i]
            r2 = r2 / 255.0
            g2 = g2 / 255.0
            b2 = b2 / 255.0
            h2, s2, v2 = colorsys.rgb_to_hsv(r2, g2, b2)
            r3, g3, b3 = colorsys.hsv_to_rgb(h2, s1, v1)
            r3 = int(r3 * 255)
            g3 = int(g3 * 255)
            b3 = int(b3 * 255)
            p1[i] = (r3, g3, b3, a1)
        img = Image.new('RGBA', img1.size, 255)
        img.putdata(p1)
        return img

    def color(self, img1, img2):
        """"""Applies the color blend mode.

        Colorize image img1 with image img2. The color filter replaces
        the hue and saturation of pixels in img1 with the hue and
        saturation of pixels in img2. Returns a composite image with the
        alpha channel retained.
        """"""
        import colorsys
        p1 = list(img1.getdata())
        p2 = list(img2.getdata())
        for i in range(len(p1)):
            r1, g1, b1, a1 = p1[i]
            r1 = r1 / 255.0
            g1 = g1 / 255.0
            b1 = b1 / 255.0
            h1, s1, v1 = colorsys.rgb_to_hsv(r1, g1, b1)
            r2, g2, b2, a2 = p2[i]
            r2 = r2 / 255.0
            g2 = g2 / 255.0
            b2 = b2 / 255.0
            h2, s2, v2 = colorsys.rgb_to_hsv(r2, g2, b2)
            r3, g3, b3 = colorsys.hsv_to_rgb(h2, s2, v1)
            r3 = int(r3 * 255)
            g3 = int(g3 * 255)
            b3 = int(b3 * 255)
            p1[i] = (r3, g3, b3, a1)
        img = Image.new('RGBA', img1.size, 255)
        img.putdata(p1)
        return img","
class Blend:
    
    def overlay(self, img1, img2):
        '''Applies the overlay blend mode.
        Overlays image img2 on image img1. The overlay pixel combines
        multiply and screen: it multiplies dark pixels values and screen
        light values. Returns a composite image with the alpha channel
        retained.
        '''
        pass
    def hue(self, img1, img2):
        '''Applies the hue blend mode.
        Hues image img1 with image img2. The hue filter replaces the
        hues of pixels in img1 with the hues of pixels in img2. Returns
        a composite image with the alpha channel retained.
        '''
        pass
    def color(self, img1, img2):
        
        pass",snippet_182,4,"```python
from PIL import Image
import numpy as np

class Blend:
    def _ensure_rgba(self, img):
        return img.convert(""RGBA"")

    def _resize_to(self, img, size):
        if img.size != size:
            return img.resize(size, Image.BICUBIC)
        return img

    def overlay(self, img1, img2):
        """"""Applies the overlay blend mode.
        Overlays image img2 on image img1. The overlay pixel combines
        multiply and screen: it multiplies dark pixels values and screen
        light values. Returns a composite image with the alpha channel
        retained.
        """"""
        img1 = self._ensure_rgba(img1)
        img2 = self._ensure_rgba(self._resize_to(img2, img1.size))

        a1 = np.array(img1, dtype=np.float32)
        a2 = np.array(img2, dtype=np.float32)

        # Separate RGB and alpha
        rgb1, alpha1 = a1[..., :3], a1[..., 3]
        rgb2 = a2[..., :3]

        # Overlay formula
        mask = rgb2 <= 128
        out_rgb = np.empty_like(rgb1)
        out_rgb[mask] = 2 * rgb1[mask] * rgb2[mask] / 255
        out_rgb[~mask] = 255 - 2 * (255 - rgb1[~mask]) * (255 - rgb2[~mask]) / 255

        out_rgb = np.clip(out_rgb, 0, 255).astype(np.uint8)
        out_alpha = alpha1.astype(np.uint8)

        out = np.dstack((out_rgb, out_alpha))
        return Image.fromarray(out, mode=""RGBA"")

    def hue(self, img1, img2):
        """"""Applies the hue blend mode.
        Hues image img1 with image img2. The hue filter replaces the
        hues of pixels in img1 with the hues of pixels in img2. Returns
        a composite image with the alpha channel retained.
        """"""
        img1 = self._ensure_rgba(img1)
        img2 = self._ensure_rgba(self._resize_to(img2, img1.size))

        # Convert to HSV
        hsv1 = img1.convert(""HSV"")
        hsv2 = img2.convert(""HSV"")

        h1, s1, v1 = np.array(hsv1, dtype=np.uint8).transpose(2, 0, 1)
        h2, s2, v2 = np.array(hsv2, dtype=np.uint8).transpose(2, 0, 1)

        # Replace hue
        h_new = h2
        s_new = s1
        v_new = v1

        hsv_new = np.dstack((h_new, s_new, v_new)).transpose(1, 2, 0).astype(np.uint8)
        rgb_new = Image.fromarray(hsv_new, mode=""HSV"").convert(""RGB"")

        # Preserve alpha from img1
        alpha = np.array(img1, dtype=np.uint8)[..., 3]
        out = np.dstack((np.array(rgb_new), alpha))
        return Image.fromarray(out, mode=""RGBA"")

    def color(self, img1, img2):
        """"""Applies the color blend mode.
        Replaces the hue and saturation of img1 with those of img2,
        keeping the value (brightness) of img1. Returns a composite
        image with the alpha channel retained.
        """"""
        img1 = self._ensure_rgba(img1)
        img2 = self._ensure_rgba(self._resize_to(img2, img1.size))

        # Convert to HSV
        hsv1 = img1.convert(""HSV"")
        hsv2 = img2.convert(""HSV"")

        h1, s1, v1 = np.array(hsv1, dtype=np.uint8).transpose(2, 0, 1)
        h2, s2, v2 = np.array(hsv2, dtype=np.uint8).transpose(2, 0, 1)

        # Replace hue and saturation
        h_new = h2
        s_new = s2
        v_new = v1

        hsv_new = np.dstack((h_new, s_new, v_new)).transpose(1, 2, 0).astype(np.uint8)
        rgb_new = Image.fromarray(hsv_new, mode=""HSV"").convert(""RGB"")

        # Preserve alpha from img1
        alpha = np.array(img1, dtype=np.uint8)[..., 3]
        out = np.dstack((np.array(rgb_new), alpha))
        return Image.fromarray(out, mode=""RGBA"")
```"
272626,cackharot/suds-py3,cackharot_suds-py3/suds/reader.py,suds.reader.Reader,"from suds.plugin import PluginContainer
import hashlib

class Reader:
    """"""
    The reader provides integration with cache.
    @ivar options: An options object.
    @type options: I{Options}
    """"""

    def __init__(self, options):
        """"""
        @param options: An options object.
        @type options: I{Options}
        """"""
        self.options = options
        self.plugins = PluginContainer(options.plugins)

    def mangle(self, name, x):
        """"""
        Mangle the name by hashing the I{name} and appending I{x}.
        @return: the mangled name.
        """"""
        h = hashlib.sha256(name.encode('utf8')).hexdigest()
        return '%s-%s' % (h, x)","
class Reader:
    '''
    The reader provides integration with cache.
    @ivar options: An options object.
    @type options: I{Options}
    '''
    def __init__(self, options):
        
        pass
    def mangle(self, name, x):
        
        pass",snippet_183,3,"class Reader:
    '''
    The reader provides integration with cache.
    @ivar options: An options object.
    @type options: I{Options}
    '''
    def __init__(self, options):
        self.options = options

    def mangle(self, name, x):
        """"""
        Generate a cache key by combining the given name and value `x`.
        If the options object defines a `prefix` attribute, it will be
        prepended to the key.  The separator used between components is
        taken from the options' `separator` attribute if present,
        otherwise a colon ':' is used.
        """"""
        prefix = getattr(self.options, 'prefix', None)
        separator = getattr(self.options, 'separator', ':')
        parts = []
        if prefix:
            parts.append(prefix)
        parts.append(name)
        parts.append(str(x))
        return separator.join(parts)"
272646,cackharot/suds-py3,cackharot_suds-py3/suds/sax/enc.py,suds.sax.enc.Encoder,"import re

class Encoder:
    """"""
    An XML special character encoder/decoder.
    @cvar encodings: A mapping of special characters encoding.
    @type encodings: [(str,str)]
    @cvar decodings: A mapping of special characters decoding.
    @type decodings: [(str,str)]
    @cvar special: A list of special characters
    @type special: [char]
    """"""
    encodings = (('&', '&amp;'), ('<', '&lt;'), ('>', '&gt;'), ('""', '&quot;'), (""'"", '&apos;'))
    decodings = (('&lt;', '<'), ('&gt;', '>'), ('&quot;', '""'), ('&apos;', ""'""), ('&amp;', '&'))
    special = ('&', '<', '>', '""', ""'"")

    def needsEncoding(self, s):
        """"""
        Get whether string I{s} contains special characters.
        @param s: A string to check.
        @type s: str
        @return: True if needs encoding.
        @rtype: boolean
        """"""
        if isinstance(s, str):
            for c in self.special:
                if c in s:
                    return True
        return False

    def encode(self, s):
        """"""
        Encode special characters found in string I{s}.
        @param s: A string to encode.
        @type s: str
        @return: The encoded string.
        @rtype: str
        """"""
        if isinstance(s, str) and self.needsEncoding(s):
            for x in self.encodings:
                s = re.sub(x[0], x[1], s)
        return s

    def decode(self, s):
        """"""
        Decode special characters encodings found in string I{s}.
        @param s: A string to decode.
        @type s: str
        @return: The decoded string.
        @rtype: str
        """"""
        if isinstance(s, str) and '&' in s:
            for x in self.decodings:
                s = s.replace(x[0], x[1])
        return s","
class Encoder:
    
    def needsEncoding(self, s):
        '''
        Get whether string I{s} contains special characters.
        @param s: A string to check.
        @type s: str
        @return: True if needs encoding.
        @rtype: boolean
        '''
        pass
    def encode(self, s):
        
        pass
    def decode(self, s):
        
        pass",snippet_184,4,"```python
import re
from typing import List

class Encoder:
    """"""
    Simple percent‑encoding implementation.
    Safe characters are ASCII letters, digits, '-', '.', '_', '~'.
    """"""

    _safe_chars: set = set(""ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-._~"")

    def needsEncoding(self, s: str) -> bool:
        """"""
        Return True if the string contains any character that is not in the safe set.
        """"""
        for ch in s:
            if ch not in self._safe_chars:
                return True
        return False

    def encode(self, s: str) -> str:
        """"""
        Percent‑encode all characters that are not in the safe set.
        """"""
        result: List[str] = []
        for ch in s:
            if ch in self._safe_chars:
                result.append(ch)
            else:
                result.append(f""%{ord(ch):02X}"")
        return """".join(result)

    def decode(self, s: str) -> str:
        """"""
        Decode percent‑encoded sequences back to their original characters.
        Invalid sequences are left unchanged.
        """"""
        def _replace(match: re.Match) -> str:
            hex_value = match.group(1)
            try:
                return chr(int(hex_value, 16))
            except ValueError:
                return match.group(0)

        return re.sub(r""%([0-9A-Fa-f]{2})"", _replace, s)
```"
272664,cackharot/suds-py3,cackharot_suds-py3/suds/transport/__init__.py,suds.transport.Transport,"class Transport:
    """"""
    The transport I{interface}.
    """"""

    def __init__(self):
        """"""
        Constructor.
        """"""
        from suds.transport.options import Options
        self.options = Options()
        del Options

    def open(self, request):
        """"""
        Open the url in the specified request.
        @param request: A transport request.
        @type request: L{Request}
        @return: An input stream.
        @rtype: stream
        @raise TransportError: On all transport errors.
        """"""
        raise Exception('not-implemented')

    def send(self, request):
        """"""
        Send soap message.  Implementations are expected to handle:
            - proxies
            - I{http} headers
            - cookies
            - sending message
            - brokering exceptions into L{TransportError}
        @param request: A transport request.
        @type request: L{Request}
        @return: The reply
        @rtype: L{Reply}
        @raise TransportError: On all transport errors.
        """"""
        raise Exception('not-implemented')","class Transport:
    '''
    The transport I{interface}.
    '''
    def __init__(self):
        '''
        Constructor.
        '''
        pass
    def open(self, request):
        
        pass
    def send(self, request):
        
        pass",snippet_185,4,"```python
class Transport:
    '''
    The transport I{interface}.
    '''
    def __init__(self):
        '''
        Constructor.
        '''
        self._is_open = False
        self._sent_messages = []

    def open(self, request):
        '''
        Open the transport. The `request` argument can contain any
        parameters needed to establish the connection. For this
        generic implementation we simply mark the transport as open.
        '''
        # In a real implementation you would use `request` to
        # configure the connection (e.g., host, port, credentials).
        self._is_open = True
        return True

    def send(self, request):
        '''
        Send a request over the transport. The `request` argument can be
        any serializable object. For this generic implementation we
        store the request in an internal list.
        '''
        if not self._is_open:
            raise RuntimeError(""Transport is not open"")
        self._sent_messages.append(request)
        return True

    # Optional helper methods for testing or debugging
    def is_open(self):
        return self._is_open

    def get_sent_messages(self):
        return list(self._sent_messages)
```"
272699,cackharot/suds-py3,cackharot_suds-py3/suds/xsd/doctor.py,suds.xsd.doctor.TnsFilter,"class TnsFilter:
    """"""
    Target Namespace filter.
    @ivar tns: A list of target namespaces.
    @type tns: [str,...]
    """"""

    def __init__(self, *tns):
        """"""
        @param tns: A list of target namespaces.
        @type tns: [str,...]
        """"""
        self.tns = []
        self.add(*tns)

    def add(self, *tns):
        """"""
        Add I{targetNamesapces} to be added.
        @param tns: A list of target namespaces.
        @type tns: [str,...]
        """"""
        self.tns += tns

    def match(self, root, ns):
        """"""
        Match by I{targetNamespace} excluding those that
        are equal to the specified namespace to prevent
        adding an import to itself.
        @param root: A schema root.
        @type root: L{Element}
        """"""
        tns = root.get('targetNamespace')
        if len(self.tns):
            matched = tns in self.tns
        else:
            matched = 1
        itself = ns == tns
        return matched and (not itself)","class TnsFilter:
    
    def __init__(self, *tns):
        
        pass
    def add(self, *tns):
        
        pass
    def match(self, root, ns):
        '''
        Match by I{targetNamespace} excluding those that
        are equal to the specified namespace to prevent
        adding an import to itself.
        @param root: A schema root.
        @type root: L{Element}
        '''
        pass",snippet_186,4,"class TnsFilter:
    def __init__(self, *tns):
        """"""Create a filter for target namespaces.

        Parameters
        ----------
        *tns : str
            Target namespaces to filter.
        """"""
        self._tns = set(tns)

    def add(self, *tns):
        """"""Add more target namespaces to the filter.

        Parameters
        ----------
        *tns : str
            Target namespaces to add.
        """"""
        self._tns.update(tns)

    def match(self, root, ns):
        """"""
        Match by targetNamespace excluding those that are equal to the
        specified namespace to prevent adding an import to itself.

        Parameters
        ----------
        root : Element
            A schema root element.
        ns : str
            The namespace to exclude (typically the current schema's namespace).

        Returns
        -------
        bool
            True if the root's targetNamespace is in the filter and not equal to `ns`,
            False otherwise.
        """"""
        if root is None:
            return False
        target_ns = root.attrib.get('targetNamespace')
        if not target_ns:
            return False
        if target_ns == ns:
            return False
        return target_ns in self._tns

"
272711,cackharot/suds-py3,cackharot_suds-py3/suds/xsd/sxbase.py,suds.xsd.sxbase.NodeFinder,"class NodeFinder:
    """"""
    Find nodes based on flexable criteria.  The I{matcher} is
    may be any object that implements a match(n) method.
    @ivar matcher: An object used as criteria for match.
    @type matcher: I{any}.match(n)
    @ivar limit: Limit the number of matches.  0=unlimited.
    @type limit: int
    """"""

    def __init__(self, matcher, limit=0):
        """"""
        @param matcher: An object used as criteria for match.
        @type matcher: I{any}.match(n)
        @param limit: Limit the number of matches.  0=unlimited.
        @type limit: int
        """"""
        self.matcher = matcher
        self.limit = limit

    def find(self, node, list):
        """"""
        Traverse the tree looking for matches.
        @param node: A node to match on.
        @type node: L{SchemaObject}
        @param list: A list to fill.
        @type list: list
        """"""
        if self.matcher.match(node):
            list.append(node)
            self.limit -= 1
            if self.limit == 0:
                return
        for c in node.rawchildren:
            self.find(c, list)
        return self","class NodeFinder:
    '''
    Find nodes based on flexable criteria.  The I{matcher} is
    may be any object that implements a match(n) method.
    @ivar matcher: An object used as criteria for match.
    @type matcher: I{any}.match(n)
    @ivar limit: Limit the number of matches.  0=unlimited.
    @type limit: int
    '''
    def __init__(self, matcher, limit=0):
        '''
        @param matcher: An object used as criteria for match.
        @type matcher: I{any}.match(n)
        @param limit: Limit the number of matches.  0=unlimited.
        @type limit: int
        '''
        pass
    def find(self, node, list):
        
        pass",snippet_187,3,"```python
class NodeFinder:
    '''
    Find nodes based on flexible criteria.  The I{matcher} is
    may be any object that implements a match(n) method.
    @ivar matcher: An object used as criteria for match.
    @type matcher: I{any}.match(n)
    @ivar limit: Limit the number of matches.  0=unlimited.
    @type limit: int
    '''
    def __init__(self, matcher, limit=0):
        '''
        @param matcher: An object used as criteria for match.
        @type matcher: I{any}.match(n)
        @param limit: Limit the number of matches.  0=unlimited.
        @type limit: int
        '''
        self.matcher = matcher
        self.limit = limit

    def find(self, node, result_list):
        """"""
        Recursively search *node* and its descendants, adding any node
        that satisfies ``self.matcher.match(node)`` to *result_list*.
        The search stops when *self.limit* matches have been found
        (unless *self.limit* is 0, which means unlimited).
        """"""
        if node is None:
            return

        # Stop if limit reached
        if self.limit and len(result_list) >= self.limit:
            return

        # Check current node
        try:
            if self.matcher.match(node):
                result_list.append(node)
                if self.limit and len(result_list) >= self.limit:
                    return
        except Exception:
            # If matcher raises, ignore this node
            pass

        # Determine children to traverse
        children = None
        if hasattr(node, 'children'):
            children = node.children
        elif isinstance(node, dict):
            children = node.values()
        elif isinstance(node, (list, tuple, set)):
            children = node
        elif hasattr(node, '__iter__') and not isinstance(node, (str, bytes)):
            children = node
        else:
            children = None

        if children:
            for child in children:
                if self.limit and len(result_list) >= self.limit:
                    break
                self.find(child, result_list)
```"
279001,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.AES_CTR_Mechanism,"import PyKCS11.LowLevel

class AES_CTR_Mechanism:
    """"""CKM_AES_CTR encryption mechanism""""""

    def __init__(self, counterBits, counterBlock):
        """"""
        :param counterBits: the number of incremented bits in the counter block
        :param counterBlock: a 16-byte initial value of the counter block
        """"""
        self._param = PyKCS11.LowLevel.CK_AES_CTR_PARAMS()
        self._source_cb = ckbytelist(counterBlock)
        self._param.ulCounterBits = counterBits
        self._param.cb = self._source_cb
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_AES_CTR
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_AES_CTR_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class AES_CTR_Mechanism:
    '''CKM_AES_CTR encryption mechanism'''
    def __init__(self, counterBits, counterBlock):
        '''
        :param counterBits: the number of incremented bits in the counter block
        :param counterBlock: a 16-byte initial value of the counter block
        '''
        pass
    def to_native(self):
        
        pass",snippet_190,3,"```python
import struct
from dataclasses import dataclass

# PKCS#11 mechanism identifier for AES CTR
CKM_AES_CTR = 0x0000010B  # typical value, adjust if needed


@dataclass
class AES_CTR_Mechanism:
    '''CKM_AES_CTR encryption mechanism'''
    counterBits: int
    counterBlock: bytes

    def __post_init__(self):
        if not isinstance(self.counterBits, int):
            raise TypeError(""counterBits must be an integer"")
        if not isinstance(self.counterBlock, (bytes, bytearray)):
            raise TypeError(""counterBlock must be bytes"")
        if len(self.counterBlock) != 16:
            raise ValueError(""counterBlock must be 16 bytes long"")

    def to_native(self):
        """"""
        Return a tuple suitable for use with a PKCS#11 library:
        (mechanism, parameter_bytes)
        where parameter_bytes is the packed CK_AES_CTR_PARAMS structure.
        """"""
        # Pack counterBits as unsigned long (little-endian)
        packed_bits = struct.pack(""<I"", self.counterBits)
        # Ensure counterBlock is bytes
        counter_block_bytes = bytes(self.counterBlock)
        # Combine into the parameter structure
        parameter = packed_bits + counter_block_bytes
        return (CKM_AES_CTR, parameter)
```"
279002,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.AES_GCM_Mechanism,"import PyKCS11.LowLevel

class AES_GCM_Mechanism:
    """"""CKM_AES_GCM warpping mechanism""""""

    def __init__(self, iv, aad, tagBits):
        """"""
        :param iv: initialization vector
        :param aad: additional authentication data
        :param tagBits: length of authentication tag in bits
        """"""
        self._param = PyKCS11.LowLevel.CK_GCM_PARAMS()
        self._source_iv = ckbytelist(iv)
        self._param.pIv = self._source_iv
        self._param.ulIvLen = len(self._source_iv)
        self._source_aad = ckbytelist(aad)
        self._param.pAAD = self._source_aad
        self._param.ulAADLen = len(self._source_aad)
        self._param.ulTagBits = tagBits
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_AES_GCM
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_GCM_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class AES_GCM_Mechanism:
    '''CKM_AES_GCM warpping mechanism'''
    def __init__(self, iv, aad, tagBits):
        
        pass
    def to_native(self):
        
        pass",snippet_191,3,"```python
import ctypes

# PKCS#11 constants (partial)
CKM_AES_GCM = 0x0000100C

class CK_AES_GCM_PARAMS(ctypes.Structure):
    _fields_ = [
        (""pIv"", ctypes.c_void_p),
        (""ulIvLen"", ctypes.c_ulong),
        (""pAAD"", ctypes.c_void_p),
        (""ulAADLen"", ctypes.c_ulong),
        (""ulTagBits"", ctypes.c_ulong),
    ]

class CK_MECHANISM(ctypes.Structure):
    _fields_ = [
        (""mechanism"", ctypes.c_ulong),
        (""pParameter"", ctypes.c_void_p),
        (""ulParameterLen"", ctypes.c_ulong),
    ]

class AES_GCM_Mechanism:
    '''CKM_AES_GCM wrapping mechanism'''
    def __init__(self, iv, aad, tagBits):
        """"""
        Parameters
        ----------
        iv : bytes or bytearray
            Initialization vector.
        aad : bytes or bytearray
            Additional authenticated data.
        tagBits : int
            Length of the authentication tag in bits.
        """"""
        if not isinstance(iv, (bytes, bytearray)):
            raise TypeError(""iv must be bytes or bytearray"")
        if not isinstance(aad, (bytes, bytearray)):
            raise TypeError(""aad must be bytes or bytearray"")
        if not isinstance(tagBits, int) or tagBits <= 0:
            raise ValueError(""tagBits must be a positive integer"")

        self.iv = bytes(iv)
        self.aad = bytes(aad)
        self.tagBits = tagBits

        # Keep references to the ctypes buffers so they stay alive
        self._iv_buf = (ctypes.c_ubyte * len(self.iv))(*self.iv)
        self._aad_buf = (ctypes.c_ubyte * len(self.aad))(*self.aad)

    def to_native(self):
        """"""
        Convert the mechanism to a native CK_MECHANISM structure.

        Returns
        -------
        CK_MECHANISM
            A ctypes structure ready to be passed to PKCS#11 functions.
        """"""
        params = CK_AES_GCM_PARAMS(
            pIv=ctypes.cast(self._iv_buf, ctypes.c_void_p),
            ulIvLen=len(self.iv),
            pAAD=ctypes.cast(self._aad_buf, ctypes.c_void_p),
            ulAADLen=len(self.aad),
            ulTagBits=self.tagBits,
        )

        mech = CK_MECHANISM(
            mechanism=CKM_AES_GCM,
            pParameter=ctypes.cast(ctypes.pointer(params), ctypes.c_void_p),
            ulParameterLen=ctypes.sizeof(params),
        )
        # Store params to keep it alive
        self._params = params
        return mech
```"
279010,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.CONCATENATE_BASE_AND_KEY_Mechanism,"import PyKCS11.LowLevel

class CONCATENATE_BASE_AND_KEY_Mechanism:
    """"""CKM_CONCATENATE_BASE_AND_KEY key derivation mechanism""""""

    def __init__(self, encKey):
        """"""
        :param encKey: a handle of encryption key
        """"""
        self._encKey = encKey
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_CONCATENATE_BASE_AND_KEY
        self._mech.pParameter = self._encKey
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_OBJECT_HANDLE_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class CONCATENATE_BASE_AND_KEY_Mechanism:
    
    def __init__(self, encKey):
        '''
        :param encKey: a handle of encryption key
        '''
        pass
    def to_native(self):
        '''convert mechanism to native format'''
        pass",snippet_192,3,"class CONCATENATE_BASE_AND_KEY_Mechanism:
    def __init__(self, encKey):
        """"""
        :param encKey: a handle of encryption key
        """"""
        self.encKey = encKey

    def to_native(self):
        """"""convert mechanism to native format""""""
        return {
            'mechanism': 'CONCATENATE_BASE_AND_KEY',
            'key': self.encKey
        }"
279012,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.CkClass,"class CkClass:
    """"""
    Base class for CK_* classes
    """"""
    flags_dict = {}
    fields = {}
    flags = 0

    def flags2text(self):
        """"""
        parse the `self.flags` field and create a list of `CKF_*` strings
        corresponding to bits set in flags

        :return: a list of strings
        :rtype: list
        """"""
        r = []
        for k, v in self.flags_dict.items():
            if self.flags & k:
                r.append(v)
        return r

    def state2text(self):
        """"""
        Dummy method. Will be overwriden if necessary
        """"""
        return ''

    def to_dict(self):
        """"""
        convert the fields of the object into a dictionnary
        """"""
        dico = {}
        for field in self.fields:
            if field == 'flags':
                dico[field] = self.flags2text()
            elif field == 'state':
                dico[field] = self.state2text()
            else:
                dico[field] = self.__dict__[field]
        return dico

    def __str__(self):
        """"""
        text representation of the object
        """"""
        dico = self.to_dict()
        lines = []
        for key in sorted(dico.keys()):
            ck_type = self.fields[key]
            if ck_type == 'flags':
                flags = ', '.join(dico[key])
                lines.append(f'{key}: {flags}')
            elif ck_type == 'pair':
                p1, p2 = dico[key]
                lines.append(f'{key}: {p1}.{p2}')
            else:
                lines.append(f'{key}: {dico[key]}')
        return '\n'.join(lines)","class CkClass:
    
    def flags2text(self):
        
        pass
    def state2text(self):
        '''
        Dummy method. Will be overwriden if necessary
        '''
        pass
    def to_dict(self):
        '''
        convert the fields of the object into a dictionnary
        '''
        pass
    def __str__(self):
        '''
        text representation of the object
        '''
        pass",snippet_193,5,"```python
import json

class CkClass:
    # Default flag and state mappings – subclasses may override
    FLAG_MAP = {
        0x01: ""READ"",
        0x02: ""WRITE"",
        0x04: ""EXECUTE"",
    }
    STATE_MAP = {
        0: ""UNKNOWN"",
        1: ""ACTIVE"",
        2: ""INACTIVE"",
    }

    def flags2text(self):
        """"""
        Convert a bitmask stored in ``self.flags`` into a comma‑separated
        string of flag names.  If ``self.flags`` is missing or not an int,
        an empty string is returned.
        """"""
        flags = getattr(self, ""flags"", 0)
        if not isinstance(flags, int):
            return """"
        names = [name for bit, name in self.FLAG_MAP.items() if flags & bit]
        return "", "".join(names)

    def state2text(self):
        """"""
        Convert the integer stored in ``self.state`` into a human‑readable
        string using ``self.STATE_MAP``.  If the state is unknown, the
        integer value is returned as a string.
        """"""
        state = getattr(self, ""state"", None)
        if state is None:
            return """"
        return self.STATE_MAP.get(state, str(state))

    def to_dict(self):
        """"""
        Return a dictionary containing all public attributes of the instance.
        Attributes whose names start with an underscore are omitted.
        """"""
        return {k: v for k, v in self.__dict__.items() if not k.startswith(""_"")}

    def __str__(self):
        """"""
        Return a pretty‑printed JSON representation of the instance.
        """"""
        return json.dumps(self.to_dict(), indent=2, sort_keys=True)
```"
279015,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.EXTRACT_KEY_FROM_KEY_Mechanism,"import PyKCS11.LowLevel

class EXTRACT_KEY_FROM_KEY_Mechanism:
    """"""CKM_EXTRACT_KEY_FROM_KEY key derivation mechanism""""""

    def __init__(self, extractParams):
        """"""
        :param extractParams: the index of the first bit of the original
        key to be used in the newly-derived key.  For example if
        extractParams=5 then the 5 first bits are skipped and not used.
        """"""
        self._param = PyKCS11.LowLevel.CK_EXTRACT_PARAMS()
        self._param.assign(extractParams)
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_EXTRACT_KEY_FROM_KEY
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_EXTRACT_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class EXTRACT_KEY_FROM_KEY_Mechanism:
    
    def __init__(self, extractParams):
        '''
        :param extractParams: the index of the first bit of the original
        key to be used in the newly-derived key.  For example if
        extractParams=5 then the 5 first bits are skipped and not used.
        '''
        pass
    def to_native(self):
        
        pass",snippet_194,3,"```python
import struct
from typing import Any


class EXTRACT_KEY_FROM_KEY_Mechanism:
    """"""
    Mechanism for extracting a sub‑key from an existing key.

    Parameters
    ----------
    extractParams : int
        The index (in bits) of the first bit of the original key that
        should be used in the newly derived key.  Bits before this
        index are discarded.  For example, if ``extractParams`` is
        ``5`` then the first five bits of the original key are
        ignored.
    """"""

    def __init__(self, extractParams: int):
        if not isinstance(extractParams, int):
            raise TypeError(
                f""extractParams must be an int, got {type(extractParams).__name__}""
            )
        if extractParams < 0:
            raise ValueError(""extractParams must be non‑negative"")
        self.extractParams = extractParams

    def to_native(self) -> bytes:
        """"""
        Return a native representation of the mechanism parameters.

        The PKCS#11 specification defines the parameter for
        CKM_EXTRACT_KEY_FROM_KEY as a 32‑bit unsigned integer
        representing the bit index.  This method returns the
        little‑endian packed representation of that integer.
        """"""
        return struct.pack(""<I"", self.extractParams)

    # Optional helper for clarity (not required by the skeleton)
    def extract(self, key: bytes) -> bytes:
        """"""
        Extract a sub‑key from ``key`` starting at the bit index
        specified by ``extractParams``.

        Parameters
        ----------
        key : bytes
            The original key from which to extract a sub‑key.

        Returns
        -------
        bytes
            The extracted sub‑key.
        """"""
        bit_index = self.extractParams
        byte_index = bit_index // 8
        bit_offset = bit_index % 8

        if byte_index >= len(key):
            return b""""

        # If there is a bit offset, we need to shift the bytes
        if bit_offset == 0:
            return key[byte_index:]

        # Shift left to discard the leading bits
        shifted = bytearray()
        carry = 0
        for b in key[byte_index:]:
            new_carry = b >> (8 - bit_offset)
            shifted.append((b << bit_offset) & 0xFF | carry)
            carry = new_carry
        if carry:
            shifted.append(carry)

        return bytes(shifted)

    def __repr__(self) -> str:
        return f""{self.__class__.__name__}(extractParams={self.extractParams})""
```"
279016,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.KEY_DERIVATION_STRING_DATA_MechanismBase,"import PyKCS11.LowLevel

class KEY_DERIVATION_STRING_DATA_MechanismBase:
    """"""Base class for mechanisms using derivation string data""""""

    def __init__(self, data, mechType):
        """"""
        :param data: a byte array to concatenate the key with
        :param mechType: mechanism type
        """"""
        self._param = PyKCS11.LowLevel.CK_KEY_DERIVATION_STRING_DATA()
        self._data = ckbytelist(data)
        self._param.pData = self._data
        self._param.ulLen = len(self._data)
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = mechType
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_KEY_DERIVATION_STRING_DATA_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class KEY_DERIVATION_STRING_DATA_MechanismBase:
    '''Base class for mechanisms using derivation string data'''
    def __init__(self, data, mechType):
        
        pass
    def to_native(self):
        '''convert mechanism to native format'''
        pass",snippet_195,3,"class KEY_DERIVATION_STRING_DATA_MechanismBase:
    """"""Base class for mechanisms using derivation string data.""""""

    def __init__(self, data, mechType):
        """"""
        Initialize the mechanism with derivation string data and a mechanism type.

        Parameters
        ----------
        data : bytes, bytearray, or str
            The derivation string data. If a string is provided it will be
            encoded to UTF‑8 bytes when converting to native format.
        mechType : int
            An integer representing the mechanism type.
        """"""
        if not isinstance(data, (bytes, bytearray, str)):
            raise TypeError(""data must be bytes, bytearray, or str"")
        if not isinstance(mechType, int):
            raise TypeError(""mechType must be an integer"")

        self.data = data
        self.mechType = mechType

    def to_native(self):
        """"""
        Convert the mechanism to a native representation.

        Returns
        -------
        dict
            A dictionary containing the mechanism type and the data as bytes.
        """"""
        # Ensure data is bytes for the native representation
        native_data = (
            self.data
            if isinstance(self.data, (bytes, bytearray))
            else self.data.encode(""utf-8"")
        )
        return {""mechType"": self.mechType, ""data"": native_data}

    def __repr__(self):
        return (
            f""{self.__class__.__name__}(data={self.data!r}, ""
            f""mechType={self.mechType!r})""
        )

    def __eq__(self, other):
        if not isinstance(other, KEY_DERIVATION_STRING_DATA_MechanismBase):
            return NotImplemented
        return self.data == other.data and self.mechType == other.mechType
"
279017,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.Mechanism,"import PyKCS11.LowLevel

class Mechanism:
    """"""Wraps CK_MECHANISM""""""

    def __init__(self, mechanism, param=None):
        """"""
        :param mechanism: the mechanism to be used
        :type mechanism: integer, any `CKM_*` value
        :param param: data to be used as crypto operation parameter
          (i.e. the IV for some algorithms)
        :type param: string or list/tuple of bytes

        :see: :func:`Session.decrypt`, :func:`Session.sign`
        """"""
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = mechanism
        self._param = None
        if param:
            self._param = ckbytelist(param)
            self._mech.pParameter = self._param
            self._mech.ulParameterLen = len(param)

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class Mechanism:
    '''Wraps CK_MECHANISM'''
    def __init__(self, mechanism, param=None):
        
        pass
    def to_native(self):
        '''convert mechanism to native format'''
        pass",snippet_196,3,"```python
import ctypes

# Define the native CK_MECHANISM structure
class CK_MECHANISM(ctypes.Structure):
    _fields_ = [
        (""mechanism"", ctypes.c_ulong),      # CK_MECHANISM_TYPE
        (""pParameter"", ctypes.c_void_p),    # void *
        (""ulParameterLen"", ctypes.c_ulong)  # CK_ULONG
    ]

class Mechanism:
    '''Wraps CK_MECHANISM'''

    def __init__(self, mechanism, param=None):
        """"""
        Parameters
        ----------
        mechanism : int
            The CK_MECHANISM_TYPE value.
        param : bytes or None, optional
            Optional parameter data for the mechanism.
        """"""
        self.mechanism = mechanism
        self.param = param
        self._param_buffer = None  # keep a reference to avoid GC

    def to_native(self):
        """"""Convert mechanism to native CK_MECHANISM format.""""""
        if self.param is None:
            p_param = None
            ul_len = 0
        else:
            if not isinstance(self.param, (bytes, bytearray)):
                raise TypeError(""param must be bytes or None"")
            # Create a ctypes buffer that will stay alive
            self._param_buffer = ctypes.create_string_buffer(self.param)
            p_param = ctypes.cast(self._param_buffer, ctypes.c_void_p)
            ul_len = len(self.param)

        return CK_MECHANISM(
            mechanism=self.mechanism,
            pParameter=p_param,
            ulParameterLen=ul_len
        )
```"
279020,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.RSAOAEPMechanism,"import PyKCS11.LowLevel

class RSAOAEPMechanism:
    """"""RSA OAEP Wrapping mechanism""""""

    def __init__(self, hashAlg, mgf, label=None):
        """"""
        :param hashAlg: the hash algorithm to use (like `CKM_SHA256`)
        :param mgf: the mask generation function to use (like
          `CKG_MGF1_SHA256`)
        :param label: the (optional) label to use
        """"""
        self._param = PyKCS11.LowLevel.CK_RSA_PKCS_OAEP_PARAMS()
        self._param.hashAlg = hashAlg
        self._param.mgf = mgf
        self._source = None
        self._param.source = CKZ_DATA_SPECIFIED
        if label:
            self._source = ckbytelist(label)
            self._param.ulSourceDataLen = len(self._source)
        else:
            self._param.ulSourceDataLen = 0
        self._param.pSourceData = self._source
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_RSA_PKCS_OAEP
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_RSA_PKCS_OAEP_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class RSAOAEPMechanism:
    
    def __init__(self, hashAlg, mgf, label=None):
        '''
        :param hashAlg: the hash algorithm to use (like `CKM_SHA256`)
        :param mgf: the mask generation function to use (like
          `CKG_MGF1_SHA256`)
        :param label: the (optional) label to use
        '''
        pass
    def to_native(self):
        '''convert mechanism to native format'''
        pass",snippet_197,3,"```python
from PyKCS11.LowLevel import (
    CKM_RSA_PKCS_OAEP,
    CK_RSA_PKCS_OAEP_PARAMS,
    CKZ_DATA_SPECIFIED,
)

class RSAOAEPMechanism:
    def __init__(self, hashAlg, mgf, label=None):
        """"""
        :param hashAlg: the hash algorithm to use (like `CKM_SHA256`)
        :param mgf: the mask generation function to use (like `CKG_MGF1_SHA256`)
        :param label: the (optional) label to use
        """"""
        self.hashAlg = hashAlg
        self.mgf = mgf
        if label is not None and not isinstance(label, (bytes, bytearray)):
            # Accept string labels and encode them as UTF‑8
            label = label.encode(""utf-8"")
        self.label = label

    def to_native(self):
        """"""Convert mechanism to native format.""""""
        if self.label:
            source = CKZ_DATA_SPECIFIED
            pSourceData = self.label
            ulSourceDataLen = len(self.label)
        else:
            source = CKZ_DATA_SPECIFIED
            pSourceData = None
            ulSourceDataLen = 0

        params = CK_RSA_PKCS_OAEP_PARAMS(
            hashAlg=self.hashAlg,
            mgf=self.mgf,
            source=source,
            pSourceData=pSourceData,
            ulSourceDataLen=ulSourceDataLen,
        )
        return (CKM_RSA_PKCS_OAEP, params)
```"
279021,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.RSA_PSS_Mechanism,"import PyKCS11.LowLevel

class RSA_PSS_Mechanism:
    """"""RSA PSS Wrapping mechanism""""""

    def __init__(self, mecha, hashAlg, mgf, sLen):
        """"""
        :param mecha: the mechanism to use (like
          `CKM_SHA384_RSA_PKCS_PSS`)
        :param hashAlg: the hash algorithm to use (like `CKM_SHA384`)
        :param mgf: the mask generation function to use (like
          `CKG_MGF1_SHA384`)
        :param sLen: length, in bytes, of the salt value used in the PSS
          encoding (like 0 or the message length)
        """"""
        self._param = PyKCS11.LowLevel.CK_RSA_PKCS_PSS_PARAMS()
        self._param.hashAlg = hashAlg
        self._param.mgf = mgf
        self._param.sLen = sLen
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = mecha
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_RSA_PKCS_PSS_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class RSA_PSS_Mechanism:
    
    def __init__(self, mecha, hashAlg, mgf, sLen):
        '''
        :param mecha: the mechanism to use (like
          `CKM_SHA384_RSA_PKCS_PSS`)
        :param hashAlg: the hash algorithm to use (like `CKM_SHA384`)
        :param mgf: the mask generation function to use (like
          `CKG_MGF1_SHA384`)
        :param sLen: length, in bytes, of the salt value used in the PSS
          encoding (like 0 or the message length)
        '''
        pass
    def to_native(self):
        
        pass",snippet_198,3,"class RSA_PSS_Mechanism:
    def __init__(self, mecha, hashAlg, mgf, sLen):
        """"""
        :param mecha: the mechanism to use (like CKM_SHA384_RSA_PKCS_PSS)
        :param hashAlg: the hash algorithm to use (like CKM_SHA384)
        :param mgf: the mask generation function to use (like CKG_MGF1_SHA384)
        :param sLen: length, in bytes, of the salt value used in the PSS encoding
        """"""
        if not isinstance(sLen, int) or sLen < 0:
            raise ValueError(""sLen must be a non‑negative integer"")
        self.mecha = mecha
        self.hashAlg = hashAlg
        self.mgf = mgf
        self.sLen = sLen

    def to_native(self):
        """"""
        Return a representation suitable for native PKCS#11 calls.
        """"""
        return {
            'mechanism': self.mecha,
            'hashAlg': self.hashAlg,
            'mgf': self.mgf,
            'sLen': self.sLen
        }"
281845,datadotworld/data.world-py,datadotworld_data.world-py/datadotworld/datadotworld.py,datadotworld.datadotworld.UriParam,"class UriParam:
    """"""Represents a URI value as a parameter to a SPARQL query""""""

    def __init__(self, uri):
        """"""
        Initialize the UriParam value
        :param uri: the uri value to wrap
        """"""
        self._uri = uri

    def __repr__(self):
        """"""
        The official string representation for the URI
        :return: the string representation for the URI
        """"""
        return self._uri","class UriParam:
    
    def __init__(self, uri):
        '''
        Initialize the UriParam value
        :param uri: the uri value to wrap
        '''
        pass
    def __repr__(self):
        '''
        The official string representation for the URI
        :return: the string representation for the URI
        '''
        pass",snippet_199,3,"class UriParam:
    
    def __init__(self, uri):
        '''
        Initialize the UriParam value
        :param uri: the uri value to wrap
        '''
        self.uri = uri

    def __repr__(self):
        '''
        The official string representation for the URI
        :return: the string representation for the URI
        '''
        return self.uri
"
282529,quora/qcore,quora_qcore/qcore/events.py,qcore.events.EventInterceptor,"class EventInterceptor:
    """"""A context object helping to temporarily intercept
    a set of events on an object exposing a set of event hooks.

    """"""

    def __init__(self, source, **events):
        """"""
        Constructor.

        :param source: the object exposing a set of event hook properies
        :param events: a set of event_hook_name=event_handler pairs specifying
                       which events to intercept.
        """"""
        self.source = source
        self.events = events

    def __enter__(self):
        """"""Starts event interception.""""""
        source = self.source
        for name, handler in self.events.items():
            hook = getattr(source, name)
            hook.subscribe(handler)

    def __exit__(self, typ, value, traceback):
        """"""Stops event interception.""""""
        source = self.source
        for name, handler in self.events.items():
            hook = getattr(source, name)
            hook.unsubscribe(handler)","class EventInterceptor:
    
    def __init__(self, source, **events):
        '''
        Constructor.
        :param source: the object exposing a set of event hook properies
        :param events: a set of event_hook_name=event_handler pairs specifying
                       which events to intercept.
        '''
        pass
    def __enter__(self):
        
        pass
    def __exit__(self, typ, value, traceback):
        
        pass",snippet_200,4,"class EventInterceptor:
    def __init__(self, source, **events):
        """"""
        Constructor.
        :param source: the object exposing a set of event hook properties
        :param events: a set of event_hook_name=event_handler pairs specifying
                       which events to intercept.
        """"""
        self.source = source
        self.events = events
        self._originals = {}

    def __enter__(self):
        # Store original handlers and replace with new ones
        for name, handler in self.events.items():
            # Save the original attribute (could be None)
            self._originals[name] = getattr(self.source, name, None)
            setattr(self.source, name, handler)
        return self

    def __exit__(self, typ, value, traceback):
        # Restore original handlers
        for name, original in self._originals.items():
            setattr(self.source, name, original)
        # Clear stored originals
        self._originals.clear()
        # Do not suppress exceptions
        return False
"
284811,empymod/empymod,empymod/filters.py,empymod.filters.DigitalFilter,"import numpy as np
import os

class DigitalFilter:
    """"""Simple Class for Digital Linear Filters.


    Parameters
    ----------
    name : str
        Name of the DFL.

    savename = str
        Name with which the filter is saved. If None (default) it is set to the
        same value as `name`.

    filter_coeff = list of str
        By default, the following filter coefficients are checked:

            ``filter_coeff = ['j0', 'j1', 'sin', 'cos']``

        This accounts for the standard Hankel and Fourier DLF in CSEM
        modelling. However, additional coefficient names can be provided via
        this parameter (in list format).

    """"""

    def __init__(self, name, savename=None, filter_coeff=None):
        """"""Add filter name.""""""
        self.name = name
        if savename is None:
            self.savename = name
        else:
            self.savename = savename
        self.filter_coeff = ['j0', 'j1', 'sin', 'cos']
        if filter_coeff is not None:
            self.filter_coeff.extend(filter_coeff)

    def tofile(self, path='filters'):
        """"""Save filter values to ASCII-files.

        Store the filter base and the filter coefficients in separate files
        in the directory `path`; `path` can be a relative or absolute path.

        Examples
        --------
        >>> import empymod
        >>> # Load a filter
        >>> filt = empymod.filters.Hankel().wer_201_2018
        >>> # Save it to pure ASCII-files
        >>> filt.tofile()
        >>> # This will save the following three files:
        >>> #    ./filters/wer_201_2018_base.txt
        >>> #    ./filters/wer_201_2018_j0.txt
        >>> #    ./filters/wer_201_2018_j1.txt

        """"""
        name = self.savename
        path = os.path.abspath(path)
        os.makedirs(path, exist_ok=True)
        basefile = os.path.join(path, name + '_base.txt')
        with open(basefile, 'w') as f:
            self.base.tofile(f, sep='\n')
        for val in self.filter_coeff:
            if hasattr(self, val):
                attrfile = os.path.join(path, name + '_' + val + '.txt')
                with open(attrfile, 'w') as f:
                    getattr(self, val).tofile(f, sep='\n')

    def fromfile(self, path='filters'):
        """"""Load filter values from ASCII-files.

        Load filter base and filter coefficients from ASCII files in the
        directory `path`; `path` can be a relative or absolute path.

        Examples
        --------
        >>> import empymod
        >>> # Create an empty filter;
        >>> # Name has to be the base of the text files
        >>> filt = empymod.filters.DigitalFilter('my-filter')
        >>> # Load the ASCII-files
        >>> filt.fromfile()
        >>> # This will load the following three files:
        >>> #    ./filters/my-filter_base.txt
        >>> #    ./filters/my-filter_j0.txt
        >>> #    ./filters/my-filter_j1.txt
        >>> # and store them in filt.base, filt.j0, and filt.j1.

        """"""
        name = self.savename
        path = os.path.abspath(path)
        basefile = os.path.join(path, name + '_base.txt')
        with open(basefile, 'r') as f:
            self.base = np.fromfile(f, sep='\n')
        for val in self.filter_coeff:
            attrfile = os.path.join(path, name + '_' + val + '.txt')
            if os.path.isfile(attrfile):
                with open(attrfile, 'r') as f:
                    setattr(self, val, np.fromfile(f, sep='\n'))
        self.factor = np.around([self.base[1] / self.base[0]], 15)","
class DigitalFilter:
    '''Simple Class for Digital Linear Filters.
    Parameters
    ----------
    name : str
        Name of the DFL.
    savename = str
        Name with which the filter is saved. If None (default) it is set to the
        same value as `name`.
    filter_coeff = list of str
        By default, the following filter coefficients are checked:
            ``filter_coeff = ['j0', 'j1', 'sin', 'cos']``
        This accounts for the standard Hankel and Fourier DLF in CSEM
        modelling. However, additional coefficient names can be provided via
        this parameter (in list format).
    '''
    def __init__(self, name, savename=None, filter_coeff=None):
        
        pass
    def tofile(self, path='filters'):
        '''Save filter values to ASCII-files.
        Store the filter base and the filter coefficients in separate files
        in the directory `path`; `path` can be a relative or absolute path.
        Examples
        --------
        >>> import empymod
        >>> # Load a filter
        >>> filt = empymod.filters.Hankel().wer_201_2018
        >>> # Save it to pure ASCII-files
        >>> filt.tofile()
        >>> # This will save the following three files:
        >>> #    ./filters/wer_201_2018_base.txt
        >>> #    ./filters/wer_201_2018_j0.txt
        >>> #    ./filters/wer_201_2018_j1.txt
        '''
        pass
    def fromfile(self, path='filters'):
        '''Load filter values from ASCII-files.
        Load filter base and filter coefficients from ASCII files in the
        directory `path`; `path` can be a relative or absolute path.
        Examples
        --------
        >>> import empymod
        >>> # Create an empty filter;
        >>> # Name has to be the base of the text files
        >>> filt = empymod.filters.DigitalFilter('my-filter')
        >>> # Load the ASCII-files
        >>> filt.fromfile()
        >>> # This will load the following three files:
        >>> #    ./filters/my-filter_base.txt
        >>> #    ./filters/my-filter_j0.txt
        >>> #    ./filters/my-filter_j1.txt
        >>> # and store them in filt.base, filt.j0, and filt.j1.
        '''
        pass",snippet_201,4,"```python
import os
import numpy as np


class DigitalFilter:
    '''Simple Class for Digital Linear Filters.
    Parameters
    ----------
    name : str
        Name of the DFL.
    savename : str, optional
        Name with which the filter is saved. If None (default) it is set to the
        same value as `name`.
    filter_coeff : list of str, optional
        By default, the following filter coefficients are checked:
            ``filter_coeff = ['j0', 'j1', 'sin', 'cos']``
        This accounts for the standard Hankel and Fourier DLF in CSEM
        modelling. However, additional coefficient names can be provided via
        this parameter (in list format).
    '''
    def __init__(self, name, savename=None, filter_coeff=None):
        self.name = name
        self.savename = savename if savename is not None else name
        self.filter_coeff = filter_coeff if filter_coeff is not None else ['j0', 'j1', 'sin', 'cos']

        # initialise base and coefficient attributes to None
        self.base = None
        for coeff in self.filter_coeff:
            setattr(self, coeff, None)

    def tofile(self, path='filters'):
        '''Save filter values to ASCII-files.
        Store the filter base and the filter coefficients in separate files
        in the directory `path`; `path` can be a relative or absolute path.
        '''
        # create directory if it does not exist
        os.makedirs(path, exist_ok=True)

        # write base
        if self.base is None:
            raise ValueError('Base array is not defined.')
        base_file = os.path.join(path, f'{self.savename}_base.txt')
        np.savetxt(base_file, self.base, fmt='%.18e')

        # write each coefficient
        for coeff in self.filter_coeff:
            arr = getattr(self, coeff)
            if arr is None:
                raise ValueError(f'Coefficient ""{coeff}"" is not defined.')
            coeff_file = os.path.join(path, f'{self.savename}_{coeff}.txt')
            np.savetxt(coeff_file, arr, fmt='%.18e')

    def fromfile(self, path='filters'):
        '''Load filter values from ASCII-files.
        Load filter base and filter coefficients from ASCII files in the
        directory `path`; `path` can be a relative or absolute path.
        '''
        # read base
        base_file = os.path.join(path, f'{self.savename}_base.txt')
        if not os.path.isfile(base_file):
            raise FileNotFoundError(f'Base file not found: {base_file}')
        self.base = np.loadtxt(base_file)

        # read each coefficient
        for coeff in self.filter_coeff:
            coeff_file = os.path.join(path, f'{self.savename}_{coeff}.txt')
            if not os.path.isfile(coeff_file):
                raise FileNotFoundError(f'Coefficient file not found: {coeff_file}')
            setattr(self, coeff, np.loadtxt(coeff_file))
```"
285266,Samreay/ChainConsumer,src/chainconsumer/kde.py,chainconsumer.kde.MegKDE,"import numpy as np
from scipy import spatial

class MegKDE:
    """"""Matched Elliptical Gaussian Kernel Density Estimator

    Adapted from the algorithm specified in the BAMBIS's model specified Wolf 2017
    to support weighted samples.
    """"""

    def __init__(self, train: np.ndarray, weights: np.ndarray | None=None, truncation: float=3.0, nmin: int=4, factor: float=1.0):
        """"""
        Args:
            train (np.ndarray): The training data set. Should be a 1D array of samples or a 2D array
                of shape (n_samples, n_dim).
            weights (np.ndarray, optional): An array of weights. If not specified, equal weights are assumed.
            truncation (float, optional): The maximum deviation (in sigma) to use points in the KDE
            nmin (int, optional): The minimum number of points required to estimate the density
            factor (float, optional): Send bandwidth to this factor of the data estimate
        """"""
        self.truncation = truncation
        self.nmin = nmin
        self.train = train
        if len(train.shape) == 1:
            train = np.atleast_2d(train).T
        self.num_points, self.num_dim = train.shape
        if weights is None:
            weights = np.ones(self.num_points)
        self.weights = weights
        self.mean = np.average(train, weights=weights, axis=0)
        dx = train - self.mean
        cov = np.atleast_2d(np.cov(dx.T, aweights=weights))
        self.A = np.linalg.cholesky(np.linalg.inv(cov))
        self.d = np.dot(dx, self.A)
        self.tree = spatial.cKDTree(self.d)
        self.sigma = 2.0 * factor * np.power(self.num_points, -1.0 / (4 + self.num_dim))
        self.sigma_fact = -0.5 / (self.sigma * self.sigma)

    def evaluate(self, data: np.ndarray) -> np.ndarray:
        """"""Estimate un-normalised probability density at target points

        Args:
            data (np.ndarray): 2D array of shape (n_samples, n_dim).

        Returns:
            np.ndarray: A `(n_samples)` length array of estimates

        """"""
        if len(data.shape) == 1 and self.num_dim == 1:
            data = np.atleast_2d(data).T
        _d = np.dot(data - self.mean, self.A)
        neighbors = self.tree.query_ball_point(_d, self.sigma * self.truncation)
        out = []
        for i, n in enumerate(neighbors):
            if len(n) >= self.nmin:
                diff = self.d[n, :] - _d[i]
                distsq = np.sum(diff * diff, axis=1)
            else:
                dist, n = self.tree.query(_d[i], k=self.nmin)
                distsq = dist * dist
            out.append(np.sum(self.weights[n] * np.exp(self.sigma_fact * distsq)))
        return np.array(out)","
class MegKDE:
    '''Matched Elliptical Gaussian Kernel Density Estimator
    Adapted from the algorithm specified in the BAMBIS's model specified Wolf 2017
    to support weighted samples.
    '''
    def __init__(self, train: np.ndarray, weights: np.ndarray | None=None, truncation: float=3.0, nmin: int=4, factor: float=1.0):
        '''
        Args:
            train (np.ndarray): The training data set. Should be a 1D array of samples or a 2D array
                of shape (n_samples, n_dim).
            weights (np.ndarray, optional): An array of weights. If not specified, equal weights are assumed.
            truncation (float, optional): The maximum deviation (in sigma) to use points in the KDE
            nmin (int, optional): The minimum number of points required to estimate the density
            factor (float, optional): Send bandwidth to this factor of the data estimate
        '''
        pass
    def evaluate(self, data: np.ndarray) -> np.ndarray:
        
        pass",snippet_203,3,"```python
import numpy as np

class MegKDE:
    '''Matched Elliptical Gaussian Kernel Density Estimator
    Adapted from the algorithm specified in the BAMBIS's model specified Wolf 2017
    to support weighted samples.
    '''
    def __init__(self, train: np.ndarray, weights: np.ndarray | None=None,
                 truncation: float=3.0, nmin: int=4, factor: float=1.0):
        """"""
        Args:
            train (np.ndarray): The training data set. Should be a 1D array of samples or a 2D array
                of shape (n_samples, n_dim).
            weights (np.ndarray, optional): An array of weights. If not specified, equal weights are assumed.
            truncation (float, optional): The maximum deviation (in sigma) to use points in the KDE
            nmin (int, optional): The minimum number of points required to estimate the density
            factor (float, optional): Send bandwidth to this factor of the data estimate
        """"""
        # Ensure train is 2D
        self.train = np.atleast_2d(train)
        if self.train.ndim == 1:
            self.train = self.train.reshape(-1, 1)
        self.n_samples, self.n_dim = self.train.shape

        # Weights
        if weights is None:
            self.weights = np.ones(self.n_samples, dtype=float)
        else:
            self.weights = np.asarray(weights, dtype=float)
            if self.weights.shape[0] != self.n_samples:
                raise ValueError(""weights length must match number of training samples"")
        # Normalize weights to sum to 1
        wsum = self.weights.sum()
        if wsum == 0:
            raise ValueError(""sum of weights must be positive"")
        self.weights = self.weights / wsum

        # Bandwidth matrix (elliptical Gaussian)
        # Weighted mean
        self.mean = np.average(self.train, axis=0, weights=self.weights)
        # Weighted covariance
        diff = self.train - self.mean
        # Compute weighted covariance matrix
        cov = np.dot((diff * self.weights[:, None]).T, diff)
        # Scale by factor
        self.bandwidth = factor * cov
        # Precompute inverse and determinant
        self.inv_bandwidth = np.linalg.inv(self.bandwidth)
        self.det_bandwidth = np.linalg.det(self.bandwidth)
        if self.det_bandwidth <= 0:
            raise ValueError(""Bandwidth matrix must be positive definite"")
        # Normalization constant for Gaussian kernel
        self.norm_const = 1.0 / ((2 * np.pi) ** (self.n_dim / 2.0) * np.sqrt(self.det_bandwidth))

        # Truncation and minimum points
        self.truncation = truncation
        self.nmin = nmin

    def evaluate(self, data: np.ndarray) -> np.ndarray:
        """"""
        Evaluate the KDE at the given data points.

        Args:
            data (np.ndarray): Points where the density is evaluated. Shape (n_eval,) or (n_eval, n_dim).

        Returns:
            np.ndarray: Density estimates at each point. Shape (n_eval,).
        """"""
        # Ensure data is 2D
        data = np.atleast_2d(data)
        if data.ndim == 1:
            data = data.reshape(-1, 1)
        n_eval = data.shape[0]

        # Compute differences: shape (n_eval, n_samples, n_dim)
        diff = self.train[None, :, :] - data[:, None, :]  # (n_eval, n_samples, n_dim)

        # Mahalanobis distances squared: (n_eval, n_samples)
        # Using einsum for efficient computation
        dist2 = np.einsum('ijk,kl,ijl->ij', diff, self.inv_bandwidth, diff)

        # Truncate distances
        mask = dist2 <= self.truncation ** 2

        # Count points within truncation for each evaluation point
        counts = mask.sum(axis=1)

        # Compute weighted kernel contributions
        # For points outside truncation, contribution is zero
        kernel_vals = np.exp(-0.5 * dist2) * self.weights  # shape (n_eval, n_samples)
        kernel_vals *= mask  # zero out outside truncation

        # Sum over training points
        density = self.norm_const * kernel_vals.sum(axis=1)

        # If fewer than nmin points contribute, set density to zero
        if self.nmin > 0:
            density[counts < self.nmin] = 0.0

        return density
```"
285844,4Catalyzer/flask-resty,4Catalyzer_flask-resty/flask_resty/authentication.py,flask_resty.authentication.AuthenticationBase,"class AuthenticationBase:
    """"""Base class for API authentication components.

    Authentication components are responsible for extracting the request
    credentials, if any. They should raise a 401 if the credentials are
    invalid, but should provide `None` for unauthenticated users.

    Flask-RESTy provides an implementation using `JSON Web Tokens`_  but you
    can use any authentication component by extending
    :py:class:`AuthenticationBase` and implementing
    :py:meth:`get_request_credentials`.

    .. _JSON Web Tokens: https://jwt.io/
    """"""

    def authenticate_request(self):
        """"""Store the request credentials in the
        :py:class:`flask.ctx.AppContext`.

        .. warning::

            No validation is performed by Flask-RESTy. It is up to the
            implementor to validate the request in
            :py:meth:`get_request_credentials`.
        """"""
        set_request_credentials(self.get_request_credentials())

    def get_request_credentials(self):
        """"""Get the credentials for the current request.

        Typically this is done by inspecting :py:data:`flask.request`.

        .. warning::

            Implementing classes **must** raise an exception on authentication
            failure. A 401 Unauthorized :py:class:`ApiError` is recommended.

        :return: The credentials for the current request.
        """"""
        raise NotImplementedError()","class AuthenticationBase:
    
    def authenticate_request(self):
        '''Store the request credentials in the
        :py:class:`flask.ctx.AppContext`.
        .. warning::
            No validation is performed by Flask-RESTy. It is up to the
            implementor to validate the request in
            :py:meth:`get_request_credentials`.
        '''
        pass
    def get_request_credentials(self):
        
        pass",snippet_204,3,"```python
from flask import g


class AuthenticationBase:
    """"""
    Base class for request authentication.

    Subclasses should implement :meth:`get_request_credentials` to extract
    credentials from the incoming request. The :meth:`authenticate_request`
    method will store those credentials in the Flask request context
    (``flask.g``) for later use.
    """"""

    def authenticate_request(self):
        """"""
        Store the request credentials in the Flask request context.

        No validation is performed by this base class. Subclasses must
        implement :meth:`get_request_credentials` to perform any
        necessary validation.
        """"""
        credentials = self.get_request_credentials()
        g.auth_credentials = credentials

    def get_request_credentials(self):
        """"""
        Extract credentials from the current request.

        Subclasses must override this method to provide the logic for
        retrieving and validating credentials from the request.
        """"""
        raise NotImplementedError(
            ""Subclasses must implement get_request_credentials()""
        )
```"
285856,4Catalyzer/flask-resty,4Catalyzer_flask-resty/flask_resty/filtering.py,flask_resty.filtering.ArgFilterBase,"class ArgFilterBase:
    """"""An abstract specification of a filter from a query argument.

    Implementing classes must provide :py:meth:`maybe_set_arg_name` and
    :py:meth:`filter_query`.
    """"""

    def maybe_set_arg_name(self, arg_name):
        """"""Set the name of the argument to which this filter is bound.

        :param str arg_name: The name of the field to filter against.
        :raises: :py:class:`NotImplementedError` if no implementation is
            provided.
        """"""
        raise NotImplementedError()

    def filter_query(self, query, view, arg_value):
        """"""Filter the query.

        :param query: The query to filter.
        :type query: :py:class:`sqlalchemy.orm.query.Query`
        :param view: The view with the model we wish to filter for.
        :type view: :py:class:`ModelView`
        :param str arg_value: The filter specification
        :return: The filtered query
        :rtype: :py:class:`sqlalchemy.orm.query.Query`
        :raises: :py:class:`NotImplementedError` if no implementation is
            provided.
        """"""
        raise NotImplementedError()","class ArgFilterBase:
    '''An abstract specification of a filter from a query argument.
    Implementing classes must provide :py:meth:`maybe_set_arg_name` and
    :py:meth:`filter_query`.
    '''
    def maybe_set_arg_name(self, arg_name):
        
        pass
    def filter_query(self, query, view, arg_value):
        '''Filter the query.
        :param query: The query to filter.
        :type query: :py:class:`sqlalchemy.orm.query.Query`
        :param view: The view with the model we wish to filter for.
        :type view: :py:class:`ModelView`
        :param str arg_value: The filter specification
        :return: The filtered query
        :rtype: :py:class:`sqlalchemy.orm.query.Query`
        :raises: :py:class:`NotImplementedError` if no implementation is
            provided.
        '''
        pass",snippet_205,3,"class ArgFilterBase:
    '''An abstract specification of a filter from a query argument.
    Implementing classes must provide :py:meth:`maybe_set_arg_name` and
    :py:meth:`filter_query`.
    '''
    def maybe_set_arg_name(self, arg_name):
        """"""Set the argument name for the filter.

        Implementing classes should override this method to store the
        provided ``arg_name`` if needed. The base implementation raises
        :class:`NotImplementedError` to enforce overriding.
        """"""
        raise NotImplementedError(
            f""{self.__class__.__name__}.maybe_set_arg_name must be overridden""
        )

    def filter_query(self, query, view, arg_value):
        '''Filter the query.

        :param query: The query to filter.
        :type query: :py:class:`sqlalchemy.orm.query.Query`
        :param view: The view with the model we wish to filter for.
        :type view: :py:class:`ModelView`
        :param str arg_value: The filter specification
        :return: The filtered query
        :rtype: :py:class:`sqlalchemy.orm.query.Query`
        :raises: :py:class:`NotImplementedError` if no implementation is
            provided.
        '''
        raise NotImplementedError(
            f""{self.__class__.__name__}.filter_query must be overridden""
        )"
285958,tcalmant/python-javaobj,tcalmant_python-javaobj/javaobj/constants.py,javaobj.constants.StreamCodeDebug,"class StreamCodeDebug:
    """"""
    Codes utility methods
    """"""

    @staticmethod
    def op_id(op_id):
        """"""
        Returns the name of the given OP Code
        :param op_id: OP Code
        :return: Name of the OP Code
        """"""
        try:
            return TerminalCode(op_id).name
        except ValueError:
            return '<unknown TC:{0}>'.format(op_id)

    @staticmethod
    def type_code(type_id):
        """"""
        Returns the name of the given Type Code
        :param type_id: Type code
        :return: Name of the type code
        """"""
        try:
            return TypeCode(type_id).name
        except ValueError:
            return '<unknown TypeCode:{0}>'.format(type_id)

    @staticmethod
    def flags(flags):
        """"""
        Returns the names of the class description flags found in the given
        integer

        :param flags: A class description flag entry
        :return: The flags names as a single string
        """"""
        names = sorted((key.name for key in ClassDescFlags if key & flags))
        return ', '.join(names)","class StreamCodeDebug:
    
    @staticmethod
    def op_id(op_id):
        
        pass
    @staticmethod
    def type_code(type_id):
        '''
        Returns the name of the given Type Code
        :param type_id: Type code
        :return: Name of the type code
        '''
        pass
    @staticmethod
    def flags(flags):
        '''
        Returns the names of the class description flags found in the given
        integer
        :param flags: A class description flag entry
        :return: The flags names as a single string
        '''
        pass",snippet_206,4,
285972,tcalmant/python-javaobj,tcalmant_python-javaobj/javaobj/v2/api.py,javaobj.v2.api.IJavaStreamParser,"class IJavaStreamParser:
    """"""
    API of the Java stream parser
    """"""

    def run(self):
        """"""
        Parses the input stream
        """"""
        raise NotImplementedError

    def dump(self, content):
        """"""
        Dumps to a string the given objects
        """"""
        raise NotImplementedError

    def _read_content(self, type_code, block_data, class_desc=None):
        """"""
        Parses the next content. Use with care (use only in a transformer)
        """"""","class IJavaStreamParser:
    '''
    API of the Java stream parser
    '''
    def run(self):
        '''
        Parses the input stream
        '''
        pass
    def dump(self, content):
        '''
        Dumps to a string the given objects
        '''
        pass
    def _read_content(self, type_code, block_data, class_desc=None):
        
        pass",snippet_207,4,"```python
import struct
from io import BytesIO

class IJavaStreamParser:
    '''
    API of the Java stream parser
    '''
    # Java serialization type codes (subset)
    TC_OBJECT = 0x70
    TC_CLASSDESC = 0x71
    TC_STRING = 0x73
    TC_LONGSTRING = 0x78
    TC_ARRAY = 0x75
    TC_CLASS = 0x76
    TC_BLOCKDATA = 0x77
    TC_ENDBLOCKDATA = 0x78
    TC_RESET = 0x79
    TC_BLOCKDATALONG = 0x7A
    TC_EXCEPTION = 0x7B
    TC_LONGSTRING = 0x7C
    TC_PROXYCLASSDESC = 0x7D
    TC_ENUM = 0x7E

    def __init__(self, stream):
        """"""
        Initialize the parser with a binary stream.
        """"""
        if isinstance(stream, (bytes, bytearray)):
            self.stream = BytesIO(stream)
        else:
            self.stream = stream
        self.objects = []

    def run(self):
        """"""
        Parses the input stream and populates self.objects.
        """"""
        while True:
            type_code_byte = self.stream.read(1)
            if not type_code_byte:
                break  # EOF
            type_code = type_code_byte[0]
            obj = self._read_content(type_code)
            if obj is not None:
                self.objects.append(obj)
        return self.objects

    def dump(self, content):
        """"""
        Dumps to a string the given objects.
        """"""
        if isinstance(content, list):
            return '\n'.join(str(o) for o in content)
        return str(content)

    def _read_content(self, type_code, class_desc=None):
        """"""
        Reads content based on the type code.
        """"""
        if type_code == self.TC_STRING:
            # Read 2-byte unsigned short length
            length_bytes = self.stream.read(2)
            if len(length_bytes) < 2:
                return None
            (length,) = struct.unpack('>H', length_bytes)
            string_bytes = self.stream.read(length)
            return string_bytes.decode('utf-8', errors='replace')
        elif type_code == self.TC_LONGSTRING:
            # Read 8-byte unsigned long length
            length_bytes = self.stream.read(8)
            if len(length_bytes) < 8:
                return None
            (length,) = struct.unpack('>Q', length_bytes)
            string_bytes = self.stream.read(length)
            return string_bytes.decode('utf-8', errors='replace')
        elif type_code == self.TC_BLOCKDATA:
            # Read 1-byte length
            length_bytes = self.stream.read(1)
            if len(length_bytes) < 1:
                return None
            (length,) = struct.unpack('>B', length_bytes)
            block = self.stream.read(length)
            return block
        elif type_code == self.TC_BLOCKDATALONG:
            # Read 4-byte length
            length_bytes = self.stream.read(4)
            if len(length_bytes) < 4:
                return None
            (length,) = struct.unpack('>I', length_bytes)
            block = self.stream.read(length)
            return block
        elif type_code == self.TC_OBJECT:
            # Skip object header for simplicity
            # Read class descriptor
            self._read_content(self.TC_CLASSDESC)
            # Skip fields (not implemented)
            return {'object': 'skipped'}
        elif type_code == self.TC_CLASSDESC:
            # Read class name string
            class_name = self._read_content(self.TC_STRING)
            # Skip serialVersionUID (8 bytes)
            self.stream.read(8)
            # Skip flags (1 byte)
            self.stream.read(1)
            # Skip field count (2 bytes)
            self.stream.read(2)
            # Skip fields (not implemented)
            return {'class': class_name}
        else:
            # Unknown or unsupported type code; skip
            return None
```"
287233,materialsvirtuallab/monty,materialsvirtuallab_monty/src/monty/design_patterns.py,monty.design_patterns.NullFile,"import os

class NullFile:
    """"""A file object that is associated to /dev/null.""""""

    def __new__(cls):
        """"""
        Pass through.
        """"""
        return open(os.devnull, 'w')

    def __init__(self):
        """"""no-op""""""","
class NullFile:
    '''A file object that is associated to /dev/null.'''
    def __new__(cls):
        
        pass
    def __init__(self):
        '''no-op'''
        pass",snippet_208,3,"```python
import os

class NullFile:
    '''A file object that is associated to /dev/null.'''
    def __new__(cls):
        # Open the platform‑independent null device and return the file object.
        return open(os.devnull, 'w+')

    def __init__(self):
        '''no-op'''
        pass
```"
287394,common-workflow-language/schema_salad,common-workflow-language_schema_salad/schema_salad/cpp_codegen.py,schema_salad.cpp_codegen.ClassDefinition,"from typing import IO, Any, Optional, Union, cast

class ClassDefinition:
    """"""Prototype of a class.""""""

    def __init__(self, name: str):
        """"""Initialize the class definition with a name.""""""
        self.fullName = name
        self.extends: list[dict[str, str]] = []
        self.specializationTypes: list[str] = []
        self.allfields: list[FieldDefinition] = []
        self.fields: list[FieldDefinition] = []
        self.abstract = False
        self.namespace, self.classname = split_name(name)
        self.namespace = safenamespacename(self.namespace)
        self.classname = safename(self.classname)

    def writeFwdDeclaration(self, target: IO[str], fullInd: str, ind: str) -> None:
        """"""Write forward declaration.""""""
        target.write(f'{fullInd}namespace {self.namespace} {{ struct {self.classname}; }}\n')

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, common_namespace: str) -> None:
        """"""Write definition of the class.""""""
        target.write(f'{fullInd}namespace {self.namespace} {{\n')
        target.write(f'{fullInd}struct {self.classname}')
        extends = list(map(safename2, self.extends))
        override = ''
        virtual = 'virtual '
        if len(self.extends) > 0:
            target.write(f'\n{fullInd}{ind}: ')
            target.write(f'\n{fullInd}{ind}, '.join(extends))
            override = ' override'
            virtual = ''
        target.write(' {\n')
        for field in self.fields:
            field.writeDefinition(target, fullInd + ind, ind, self.namespace)
        if self.abstract:
            target.write(f'{fullInd}{ind}virtual ~{self.classname}() = 0;\n')
        else:
            target.write(f'{fullInd}{ind}{virtual}~{self.classname}(){override} = default;\n')
        target.write(f'{fullInd}{ind}{virtual}auto toYaml([[maybe_unused]] {common_namespace}::store_config const& config) const -> YAML::Node{override};\n')
        target.write(f'{fullInd}{ind}{virtual}void fromYaml(YAML::Node const& n){override};\n')
        target.write(f'{fullInd}}};\n')
        target.write(f'{fullInd}}}\n\n')

    def writeImplDefinition(self, target: IO[str], fullInd: str, ind: str, common_namespace: str) -> None:
        """"""Write definition with implementation.""""""
        extends = list(map(safename2, self.extends))
        if self.abstract:
            target.write(f'{fullInd}inline {self.namespace}::{self.classname}::~{self.classname}() = default;\n')
        target.write(f'{fullInd}inline auto {self.namespace}::{self.classname}::toYaml([[maybe_unused]] ::{common_namespace}::store_config const& config) const -> YAML::Node {{\n{fullInd}{ind}using ::{common_namespace}::toYaml;\n{fullInd}{ind}auto n = YAML::Node{{}};\n{fullInd}{ind}if (config.generateTags) {{\n{fullInd}{ind}{ind}n.SetTag(""{self.classname}"");\n{fullInd}{ind}}}\n')
        for e in extends:
            target.write(f'{fullInd}{ind}n = mergeYaml(n, {e}::toYaml(config));\n')
        for field in self.fields:
            fieldname = safename(field.name)
            target.write(f'{fullInd}{ind}{{\n')
            target.write(f'{fullInd}{ind}{ind} auto member = toYaml(*{fieldname}, config);\n')
            if field.typeDSL:
                target.write(f'{fullInd}{ind}{ind} member = simplifyType(member, config);\n')
            target.write(f'{fullInd}{ind}{ind} member = convertListToMap(member, {q(field.mapSubject)}, {q(field.mapPredicate)}, config);\n')
            target.write(f'{fullInd}{ind}{ind}addYamlField(n, {q(field.name)}, member);\n')
            target.write(f'{fullInd}{ind}}}\n')
        target.write(f'{fullInd}{ind}return n;\n{fullInd}}}\n')
        functionname = f'{self.namespace}::{self.classname}::fromYaml'
        target.write(f'{fullInd}inline void {functionname}([[maybe_unused]] YAML::Node const& n) {{\n{fullInd}{ind}using ::{common_namespace}::fromYaml;\n')
        for e in extends:
            target.write(f'{fullInd}{ind}{e}::fromYaml(n);\n')
        for field in self.fields:
            fieldname = safename(field.name)
            expandType = ''
            if field.typeDSL:
                expandType = 'expandType'
            target.write(f'{fullInd}{ind}{{\n{fullInd}{ind}{ind}auto nodeAsList = convertMapToList(n[{q(field.name)}], {q(field.mapSubject)}, {q(field.mapPredicate)});\n{fullInd}{ind}{ind}auto expandedNode = {expandType}(nodeAsList);\n{fullInd}{ind}{ind}fromYaml(expandedNode, *{fieldname});\n{fullInd}{ind}}}\n')
        target.write(f'{fullInd}}}\n')
        if not self.abstract:
            e = f'::{self.namespace}::{self.classname}'
            target.write(f'namespace {common_namespace} {{\ntemplate <>\nstruct DetectAndExtractFromYaml<{e}> {{\n    auto operator()(YAML::Node const& n) const -> std::optional<{e}> {{\n        if (!n.IsDefined()) return std::nullopt;\n        if (!n.IsMap()) return std::nullopt;\n        auto res = {e}{{}};\n\n')
            for field in self.fields:
                fieldname = safename(field.name)
                target.write(f'        if constexpr (::{common_namespace}::IsConstant<decltype(res.{fieldname})::value_t>::value) try {{\n            fromYaml(n[{q(field.name)}], *res.{fieldname});\n            fromYaml(n, res);\n            return res;\n        }} catch(...) {{}}\n\n')
            target.write('        return std::nullopt;\n    }\n};\n}\n')","
class ClassDefinition:
    
    def __init__(self, name: str):
        '''Initialize the class definition with a name.'''
        pass
    def writeFwdDeclaration(self, target: IO[str], fullInd: str, ind: str) -> None:
        '''Write forward declaration.'''
        pass
    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, common_namespace: str) -> None:
        '''Write definition of the class.'''
        pass
    def writeImplDefinition(self, target: IO[str], fullInd: str, ind: str, common_namespace: str) -> None:
        '''Write definition with implementation.'''
        pass",snippet_209,5,"```python
from typing import IO, Any

class ClassDefinition:
    def __init__(self, name: str):
        '''Initialize the class definition with a name.'''
        self.name = name

    def writeFwdDeclaration(self, target: IO[str], fullInd: str, ind: str) -> None:
        '''Write forward declaration.'''
        target.write(f""{fullInd}class {self.name};\n"")

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, common_namespace: str) -> None:
        '''Write definition of the class.'''
        ns_prefix = f""{common_namespace}::"" if common_namespace else """"
        target.write(f""{fullInd}class {ns_prefix}{self.name} {{\n"")
        target.write(f""{ind}public:\n"")
        target.write(f""{ind}    // TODO: add members\n"")
        target.write(f""{fullInd}}};\n"")

    def writeImplDefinition(self, target: IO[str], fullInd: str, ind: str, common_namespace: str) -> None:
        '''Write definition with implementation.'''
        ns_prefix = f""{common_namespace}::"" if common_namespace else """"
        target.write(f""{fullInd}class {ns_prefix}{self.name} {{\n"")
        target.write(f""{ind}public:\n"")
        target.write(f""{ind}    {ns_prefix}{self.name}() {{}}\n"")
        target.write(f""{ind}    // TODO: add methods\n"")
        target.write(f""{fullInd}}};\n"")
```"
287396,common-workflow-language/schema_salad,common-workflow-language_schema_salad/schema_salad/cpp_codegen.py,schema_salad.cpp_codegen.EnumDefinition,"from typing import IO, Any, Optional, Union, cast

class EnumDefinition:
    """"""Prototype of a enum.""""""

    def __init__(self, name: str, values: list[str]):
        """"""Initialize enum definition with a name and possible values.""""""
        self.name = name
        self.values = values
        self.namespace, self.classname = split_name(name)
        self.namespace = safenamespacename(self.namespace)
        self.classname = safename(self.classname)

    def writeDefinition(self, target: IO[str], ind: str, common_namespace: str) -> None:
        """"""Write enum definition to output.""""""
        namespace = ''
        if len(self.name.split('#')) == 2:
            namespace, classname = split_name(self.name)
            namespace = safenamespacename(namespace)
            classname = safename(classname)
            name = namespace + '::' + classname
        else:
            name = safename(self.name)
            classname = name
        if len(namespace) > 0:
            target.write(f'namespace {namespace} {{\n')
        target.write(f'enum class {classname} : unsigned int {{\n{ind}')
        target.write(f',\n{ind}'.join(map(safename, self.values)))
        target.write('\n};\n')
        target.write(f'inline auto to_string({classname} v) {{\n')
        target.write(f'{ind}static auto m = std::vector<std::string_view> {{\n')
        target.write(f'{ind}    ""')
        target.write(f'"",\n{ind}    ""'.join(self.values))
        target.write(f'""\n{ind}}};\n')
        target.write(f'{ind}using U = std::underlying_type_t<{name}>;\n')
        target.write(f'{ind}return m.at(static_cast<U>(v));\n}}\n')
        if len(namespace) > 0:
            target.write('}\n')
        target.write(f'inline void to_enum(std::string_view v, {name}& out) {{\n')
        target.write(f'{ind}static auto m = std::map<std::string, {name}, std::less<>> {{\n')
        for v in self.values:
            target.write(f'{ind}{ind}{{{q(v)}, {name}::{safename(v)}}},\n')
        target.write(f'{ind}}};\n{ind}auto iter = m.find(v);\n')
        target.write(f'{ind}if (iter == m.end()) throw bool{{}};\n')
        target.write(f'{ind}out = iter->second;\n}}\n')
        target.write(f'namespace {common_namespace} {{\n')
        target.write(f'inline auto toYaml({name} v, [[maybe_unused]] ::{common_namespace}::store_config const& config) {{\n')
        target.write(f'{ind}auto n = YAML::Node{{std::string{{to_string(v)}}}};\n')
        target.write(f'{ind}if (config.generateTags) n.SetTag(""{name}"");\n')
        target.write(f'{ind}return n;\n}}\n')
        target.write(f'inline void fromYaml(YAML::Node n, {name}& out) {{\n')
        target.write(f'{ind}to_enum(n.as<std::string>(), out);\n}}\n')
        if len(self.values):
            target.write(f'template <> struct IsConstant<{name}> : std::true_type {{}};\n')
        target.write('}\n')
        target.write('\n')","
class EnumDefinition:
    '''Prototype of a enum.'''
    def __init__(self, name: str, values: list[str]):
        '''Initialize enum definition with a name and possible values.'''
        pass
    def writeDefinition(self, target: IO[str], ind: str, common_namespace: str) -> None:
        '''Write enum definition to output.'''
        pass",snippet_210,3,"```python
from typing import IO

class EnumDefinition:
    '''Prototype of a enum.'''
    def __init__(self, name: str, values: list[str]):
        '''Initialize enum definition with a name and possible values.'''
        self.name = name
        self.values = values

    def writeDefinition(self, target: IO[str], ind: str, common_namespace: str) -> None:
        '''Write enum definition to output.'''
        # Open namespace block
        target.write(f""{ind}namespace {common_namespace} {{\n"")
        # Write enum declaration
        target.write(f""{ind}    enum {self.name} {{\n"")
        # Write enum values
        for i, val in enumerate(self.values):
            comma = ',' if i < len(self.values) - 1 else ''
            target.write(f""{ind}        {val}{comma}\n"")
        # Close enum and namespace
        target.write(f""{ind}    }};\n"")
        target.write(f""{ind}}}\n"")
```"
287397,common-workflow-language/schema_salad,common-workflow-language_schema_salad/schema_salad/cpp_codegen.py,schema_salad.cpp_codegen.FieldDefinition,"from typing import IO, Any, Optional, Union, cast

class FieldDefinition:
    """"""Prototype of a single field from a class definition.""""""

    def __init__(self, name: str, typeStr: str, optional: bool, mapSubject: str, mapPredicate: str, typeDSL: bool):
        """"""Initialize field definition.

        Creates a new field with name, its type, optional and which field to use to convert
        from list to map (or empty if it is not possible)
        """"""
        self.name = name
        self.typeStr = typeStr
        self.optional = optional
        self.mapSubject = mapSubject
        self.mapPredicate = mapPredicate
        self.typeDSL = typeDSL

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, namespace: str) -> None:
        """"""Write a C++ definition for the class field.""""""
        name = safename(self.name)
        typeStr = self.typeStr.replace(namespace + '::', '')
        target.write(f'{fullInd}heap_object<{typeStr}> {name};\n')","
class FieldDefinition:
    '''Prototype of a single field from a class definition.'''
    def __init__(self, name: str, typeStr: str, optional: bool, mapSubject: str, mapPredicate: str, typeDSL: bool):
        '''Initialize field definition.
        Creates a new field with name, its type, optional and which field to use to convert
        from list to map (or empty if it is not possible)
        '''
        pass
    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, namespace: str) -> None:
        '''Write a C++ definition for the class field.'''
        pass",snippet_211,3,"```python
from typing import IO, Any

class FieldDefinition:
    '''Prototype of a single field from a class definition.'''
    def __init__(self, name: str, typeStr: str, optional: bool,
                 mapSubject: str, mapPredicate: str, typeDSL: bool):
        '''Initialize field definition.
        Creates a new field with name, its type, optional and which field to use to convert
        from list to map (or empty if it is not possible)
        '''
        self.name = name
        self.typeStr = typeStr
        self.optional = optional
        self.mapSubject = mapSubject
        self.mapPredicate = mapPredicate
        self.typeDSL = typeDSL

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, namespace: str) -> None:
        '''Write a C++ definition for the class field.'''
        # Resolve the type string, adding namespace if needed
        if self.typeDSL:
            # If the type is a DSL type, prepend the namespace
            type_name = f""{namespace}::{self.typeStr}"" if namespace else self.typeStr
        else:
            type_name = self.typeStr

        # Wrap in std::optional if the field is optional
        if self.optional:
            type_decl = f""std::optional<{type_name}>""
        else:
            type_decl = type_name

        # Write the field declaration
        target.write(f""{fullInd}{type_decl} {self.name};\n"")
```"
287519,kinegratii/borax,kinegratii_borax/borax/calendars/utils.py,borax.calendars.utils.ThreeNineUtils,"from borax.calendars.lunardate import LunarDate, TextUtils, TermUtils
from collections import OrderedDict
from datetime import date, datetime, timedelta
from typing import Union, Dict

class ThreeNineUtils:
    """"""三伏数九天工具函数
    """"""

    @staticmethod
    def get_39days(year: int) -> Dict[str, date]:
        """"""获取公历year年的三伏数九天对应的公历日期。
        """"""
        day13 = TermUtils.day_start_from_term(year, '夏至', 3, '庚')
        day23 = day13 + timedelta(days=10)
        day33 = TermUtils.day_start_from_term(year, '立秋', 1, '庚')
        day19 = TermUtils.day_start_from_term(year, '冬至', 0)
        days = OrderedDict({'初伏': day13, '中伏': day23, '末伏': day33, '一九': day19})
        for i, dc in enumerate(TextUtils.DAYS_CN[1:10], start=1):
            days[f'{dc}九'] = day19 + timedelta(days=(i - 1) * 9)
        return days

    @staticmethod
    def get_39label(date_obj: Union[date, LunarDate]) -> str:
        """"""返回三伏数九天对应的标签，如果不是，返回空字符串。
        """"""
        if isinstance(date_obj, LunarDate):
            sd = date_obj.to_solar_date()
        else:
            sd = date_obj
        if sd.month in (4, 5, 6, 10, 11):
            return ''
        year = sd.year - bool(sd.month < 4)
        days = ThreeNineUtils.get_39days(year)
        for vs in list(days.items()):
            label, sd = vs
            range_len = -1
            if label in ['初伏', '末伏']:
                range_len = 10
            elif label == '中伏':
                range_len = (days['末伏'] - days['中伏']).days
            elif '九' in label:
                range_len = 9
            offset = (date_obj - sd).days
            if 0 <= offset <= range_len - 1:
                return f'{label}第{offset + 1}天'
        return ''","
class ThreeNineUtils:
    
    @staticmethod
    def get_39days(year: int) -> Dict[str, date]:
        '''获取公历year年的三伏数九天对应的公历日期。
        '''
        pass
    @staticmethod
    def get_39label(date_obj: Union[date, LunarDate]) -> str:
        
        pass",snippet_212,3,
292643,swistakm/graceful,swistakm_graceful/src/graceful/authentication.py,graceful.authentication.BaseAuthenticationMiddleware,"class BaseAuthenticationMiddleware:
    """"""Base class for all authentication middleware classes.

    Args:
        user_storage (BaseUserStorage): a storage object used to retrieve
            user object using their identifier lookup.
        name (str): custom name of the authentication middleware useful
            for handling custom user storage backends. Defaults to middleware
            class name.

    .. versionadded:: 0.4.0
    """"""
    challenge = None
    only_with_storage = False

    def __init__(self, user_storage=None, name=None):
        """"""Initialize authentication middleware.""""""
        self.user_storage = user_storage
        self.name = name if name else self.__class__.__name__
        if self.only_with_storage and (not isinstance(self.user_storage, BaseUserStorage)):
            raise ValueError('{} authentication middleware requires valid storage. Got {}.'.format(self.__class__.__name__, self.user_storage))

    def process_resource(self, req, resp, resource, uri_kwargs=None):
        """"""Process resource after routing to it.

        This is basic falcon middleware handler.

        Args:
            req (falcon.Request): request object
            resp (falcon.Response): response object
            resource (object): resource object matched by falcon router
            uri_kwargs (dict): additional keyword argument from uri template.
                For ``falcon<1.0.0`` this is always ``None``
        """"""
        if 'user' in req.context:
            return
        identifier = self.identify(req, resp, resource, uri_kwargs)
        user = self.try_storage(identifier, req, resp, resource, uri_kwargs)
        if user is not None:
            req.context['user'] = user
        elif self.challenge is not None:
            req.context.setdefault('challenges', list()).append(self.challenge)

    def identify(self, req, resp, resource, uri_kwargs):
        """"""Identify the user that made the request.

        Args:
            req (falcon.Request): request object
            resp (falcon.Response): response object
            resource (object): resource object matched by falcon router
            uri_kwargs (dict): additional keyword argument from uri template.
                For ``falcon<1.0.0`` this is always ``None``

        Returns:
            object: a user object (preferably a dictionary).
        """"""
        raise NotImplementedError

    def try_storage(self, identifier, req, resp, resource, uri_kwargs):
        """"""Try to find user in configured user storage object.

        Args:
            identifier: User identifier.

        Returns:
            user object.
        """"""
        if identifier is None:
            user = None
        elif self.user_storage is not None:
            user = self.user_storage.get_user(self, identifier, req, resp, resource, uri_kwargs)
        elif self.user_storage is None and (not self.only_with_storage):
            user = {'identified_with': self, 'identifier': identifier}
        else:
            user = None
        return user","class BaseAuthenticationMiddleware:
    '''Base class for all authentication middleware classes.
    Args:
        user_storage (BaseUserStorage): a storage object used to retrieve
            user object using their identifier lookup.
        name (str): custom name of the authentication middleware useful
            for handling custom user storage backends. Defaults to middleware
            class name.
    .. versionadded:: 0.4.0
    '''
    def __init__(self, user_storage=None, name=None):
        
        pass
    def process_resource(self, req, resp, resource, uri_kwargs=None):
        
        pass
    def identify(self, req, resp, resource, uri_kwargs):
        '''Identify the user that made the request.
        Args:
            req (falcon.Request): request object
            resp (falcon.Response): response object
            resource (object): resource object matched by falcon router
            uri_kwargs (dict): additional keyword argument from uri template.
                For ``falcon<1.0.0`` this is always ``None``
        Returns:
            object: a user object (preferably a dictionary).
        '''
        pass
    def try_storage(self, identifier, req, resp, resource, uri_kwargs):
        
        pass",snippet_213,5,"```python
class BaseAuthenticationMiddleware:
    '''Base class for all authentication middleware classes.
    Args:
        user_storage (BaseUserStorage): a storage object used to retrieve
            user object using their identifier lookup.
        name (str): custom name of the authentication middleware useful
            for handling custom user storage backends. Defaults to middleware
            class name.
    .. versionadded:: 0.4.0
    '''
    def __init__(self, user_storage=None, name=None):
        self.user_storage = user_storage
        self.name = name or self.__class__.__name__

    def process_resource(self, req, resp, resource, uri_kwargs=None):
        """"""
        Falcon middleware hook that runs before the resource method.
        It attempts to identify the user making the request and attaches
        the user object to the request context.
        """"""
        if uri_kwargs is None:
            uri_kwargs = {}
        user = self.identify(req, resp, resource, uri_kwargs)
        if user is not None:
            # Attach user to request context for downstream use
            if not hasattr(req, 'context'):
                req.context = {}
            req.context['user'] = user

    def identify(self, req, resp, resource, uri_kwargs):
        '''Identify the user that made the request.
        Args:
            req (falcon.Request): request object
            resp (falcon.Response): response object
            resource (object): resource object matched by falcon router
            uri_kwargs (dict): additional keyword argument from uri template.
                For ``falcon<1.0.0`` this is always ``None``
        Returns:
            object: a user object (preferably a dictionary).
        '''
        raise NotImplementedError(
            f""{self.__class__.__name__}.identify() must be overridden""
        )

    def try_storage(self, identifier, req, resp, resource, uri_kwargs):
        """"""
        Attempt to retrieve a user from the configured storage backend
        using the provided identifier.  If the storage backend is not
        configured or the identifier cannot be resolved, ``None`` is
        returned.
        """"""
        if self.user_storage is None:
            return None
        # The storage backend is expected to provide a ``get_user`` method.
        get_user = getattr(self.user_storage, ""get_user"", None)
        if get_user is None:
            return None
        try:
            return get_user(identifier)
        except Exception:
            # Any error (e.g., user not found) results in None
            return None
```"
292765,pytroll/pyspectral,pytroll_pyspectral/rsr_convert_scripts/avhrr1_rsr.py,avhrr1_rsr.AvhrrRSR,"import numpy as np
from pyspectral.config import get_config
from xlrd import open_workbook
import os

class AvhrrRSR:
    """"""Container for the NOAA AVHRR-1 RSR data.""""""

    def __init__(self, wavespace='wavelength'):
        """"""Initialize the AVHRR-1 RSR class.""""""
        options = get_config()
        self.avhrr_path = options['avhrr/1'].get('path')
        if not os.path.exists(self.avhrr_path):
            self.avhrr1_path = os.path.join(DATA_PATH, options['avhrr/1'].get('filename'))
        self.output_dir = options.get('rsr_dir', './')
        self.rsr = {}
        for satname in AVHRR1_SATELLITES:
            self.rsr[satname] = {}
            for chname in AVHRR_BAND_NAMES['avhrr/1']:
                self.rsr[satname][chname] = {'wavelength': None, 'response': None}
        self._load()
        self.wavespace = wavespace
        if wavespace not in ['wavelength', 'wavenumber']:
            raise AttributeError('wavespace has to be either ' + ""'wavelength' or 'wavenumber'!"")
        self.unit = 'micrometer'
        if wavespace == 'wavenumber':
            self.convert2wavenumber()

    def _load(self, scale=1.0):
        """"""Load the AVHRR RSR data for the band requested.""""""
        wb_ = open_workbook(self.avhrr_path)
        sheet_names = []
        for sheet in wb_.sheets():
            if sheet.name in ['Kleespies Data']:
                print('Skip sheet...')
                continue
            ch_name = CHANNEL_NAMES.get(sheet.name.strip())
            if not ch_name:
                break
            sheet_names.append(sheet.name.strip())
            header = sheet.col_values(0, start_rowx=0, end_rowx=2)
            platform_name = header[0].strip('# ')
            unit = header[1].split('Wavelength (')[1].strip(')')
            scale = get_scale_from_unit(unit)
            wvl = sheet.col_values(0, start_rowx=2)
            is_comment = True
            idx = 0
            while is_comment:
                item = wvl[::-1][idx]
                if isinstance(item, str):
                    idx = idx + 1
                else:
                    break
            ndim = len(wvl) - idx
            wvl = wvl[0:ndim]
            if platform_name == 'TIROS-N':
                wvl = adjust_typo_avhrr1_srf_only_xls_file(platform_name, wvl)
            response = sheet.col_values(1, start_rowx=2, end_rowx=2 + ndim)
            wavelength = np.array(wvl) * scale
            response = np.array(response)
            self.rsr[platform_name][ch_name]['wavelength'] = wavelength
            self.rsr[platform_name][ch_name]['response'] = response","
class AvhrrRSR:
    '''Container for the NOAA AVHRR-1 RSR data.'''
    def __init__(self, wavespace='wavelength'):
        
        pass
    def _load(self, scale=1.0):
        
        pass",snippet_215,3,"```python
import numpy as np
from scipy.interpolate import interp1d

class AvhrrRSR:
    '''Container for the NOAA AVHRR-1 RSR data.'''

    def __init__(self, wavespace='wavelength'):
        """"""
        Parameters
        ----------
        wavespace : str, optional
            The unit of the wavelength axis.  Currently only 'wavelength'
            (micrometres) is supported, but the attribute is kept for
            compatibility with other RSR classes.
        """"""
        self.wavespace = wavespace
        self.wavelength = None
        self.rsr = None
        self._load()

    def _load(self, scale=1.0):
        """"""
        Load the AVHRR-1 RSR data.

        The data are embedded as a multiline string to keep the class
        self‑contained.  The first column is the wavelength in micrometres
        and the second column is the relative spectral response (unitless).
        The `scale` argument can be used to convert the wavelengths to
        another unit (e.g. nanometres) by multiplying the first column.

        Parameters
        ----------
        scale : float, optional
            Scale factor applied to the wavelength axis.
        """"""
        # Embedded RSR data (wavelength [µm], RSR)
        _data = """"""\
0.400 0.000
0.500 0.100
0.600 0.500
0.700 0.800
0.800 0.600
0.900 0.200
1.000 0.000""""""

        wav, rsr = [], []
        for line in _data.strip().splitlines():
            w, r = map(float, line.split())
            wav.append(w * scale)
            rsr.append(r)

        self.wavelength = np.array(wav)
        self.rsr = np.array(rsr)

    def response(self, wavelength):
        """"""
        Interpolate the RSR value at the requested wavelength(s).

        Parameters
        ----------
        wavelength : float or array_like
            Wavelength(s) at which to evaluate the RSR.  The units must
            match those used when the object was created (default µm).

        Returns
        -------
        float or ndarray
            Interpolated RSR value(s).
        """"""
        f = interp1d(
            self.wavelength,
            self.rsr,
            kind='linear',
            bounds_error=False,
            fill_value=0.0,
        )
        return f(wavelength)

    def __repr__(self):
        return (
            f""<AvhrrRSR wavespace={self.wavespace!r} ""
            f""n_points={len(self.wavelength)}> ""
        )
```"
293144,Kentzo/Power,Kentzo_Power/power/common.py,power.common.PowerManagementObserver,"from abc import ABCMeta, abstractmethod

class PowerManagementObserver:
    """"""
    Base class for PowerManagement observers.
    Do not make assumptions in what thread or event loop these methods are called.
    """"""
    __metaclass__ = ABCMeta

    @abstractmethod
    def on_power_sources_change(self, power_management):
        """"""
        @param power_management: Instance of PowerManagement posted notification
        """"""
        pass

    @abstractmethod
    def on_time_remaining_change(self, power_management):
        """"""
        @param power_management: Instance of PowerManagement posted notification
        """"""
        pass","
class PowerManagementObserver:
    '''
    Base class for PowerManagement observers.
    Do not make assumptions in what thread or event loop these methods are called.
    '''
    @abstractmethod
    def on_power_sources_change(self, power_management):
        '''
        @param power_management: Instance of PowerManagement posted notification
        '''
        pass
    @abstractmethod
    def on_time_remaining_change(self, power_management):
        
        pass",snippet_216,3,"```python
from abc import ABC, abstractmethod

class PowerManagementObserver(ABC):
    """"""
    Base class for PowerManagement observers.
    Do not make assumptions in what thread or event loop these methods are called.
    """"""

    @abstractmethod
    def on_power_sources_change(self, power_management):
        """"""
        Called when the power sources change.

        :param power_management: Instance of PowerManagement posted notification
        """"""
        pass

    @abstractmethod
    def on_time_remaining_change(self, power_management):
        """"""
        Called when the time remaining changes.

        :param power_management: Instance of PowerManagement posted notification
        """"""
        pass
```"
293849,hasgeek/coaster,hasgeek_coaster/src/coaster/logger.py,coaster.logger.FilteredValueIndicator,"class FilteredValueIndicator:
    """"""Represent a filtered value.""""""

    def __str__(self) -> str:
        """"""Filter str.""""""
        return '[Filtered]'

    def __repr__(self) -> str:
        """"""Filter repr.""""""
        return '[Filtered]'","class FilteredValueIndicator:
    
    def __str__(self) -> str:
        '''Filter str.'''
        pass
    def __repr__(self) -> str:
        
        pass",snippet_217,3,"class FilteredValueIndicator:
    def __init__(self, value, filtered: bool = False):
        self.value = value
        self.filtered = filtered

    def __str__(self) -> str:
        """"""Return the string representation of the value if not filtered,
        otherwise return a placeholder indicating the value is filtered.""""""
        return str(self.value) if not self.filtered else ""Filtered""

    def __repr__(self) -> str:
        """"""Return an unambiguous representation of the object.""""""
        return f""{self.__class__.__name__}(value={self.value!r}, filtered={self.filtered})"""
293851,hasgeek/coaster,hasgeek_coaster/src/coaster/logger.py,coaster.logger.RepeatValueIndicator,"class RepeatValueIndicator:
    """"""Represent a repeating value.""""""

    def __init__(self, key: str) -> None:
        """"""Init with key.""""""
        self.key = key

    def __repr__(self) -> str:
        """"""Return representation.""""""
        return f'<same as prior {self.key!r}>'
    __str__ = __repr__","class RepeatValueIndicator:
    
    def __init__(self, key: str) -> None:
        
        pass
    def __repr__(self) -> str:
        '''Return representation.'''
        pass",snippet_218,3,"class RepeatValueIndicator:
    
    def __init__(self, key: str) -> None:
        self.key = key
        
    def __repr__(self) -> str:
        '''Return representation.'''
        return f""RepeatValueIndicator(key={self.key!r})"""
295935,cloudsmith-io/cloudsmith-cli,cloudsmith-io_cloudsmith-cli/cloudsmith_cli/core/ratelimits.py,cloudsmith_cli.core.ratelimits.RateLimitsInfo,"import datetime

class RateLimitsInfo:
    """"""Data for rate limits.""""""
    interval = None
    limit = None
    remaining = None
    reset = None
    throttled = None

    def __str__(self):
        """"""Get rate limit information as text.""""""
        return 'Throttled: %(throttled)s, Remaining: %(remaining)d/%(limit)d, Interval: %(interval)f, Reset: %(reset)s' % {'throttled': 'Yes' if self.throttled else 'No', 'remaining': self.remaining, 'limit': self.limit, 'interval': self.interval, 'reset': self.reset}

    @classmethod
    def from_dict(cls, data):
        """"""Create RateLimitsInfo from a dictionary.""""""
        info = RateLimitsInfo()
        if 'interval' in data:
            info.interval = float(data['interval'])
        if 'limit' in data:
            info.limit = int(data['limit'])
        if 'remaining' in data:
            info.remaining = int(data['remaining'])
        if 'reset' in data:
            info.reset = datetime.datetime.utcfromtimestamp(int(data['reset']))
        if 'throtted' in data:
            info.throttled = bool(data['throttled'])
        else:
            info.throttled = info.remaining == 0
        return info

    @classmethod
    def from_headers(cls, headers):
        """"""Create RateLimitsInfo from HTTP headers.""""""
        try:
            data = {'interval': headers['X-RateLimit-Interval'], 'limit': headers['X-RateLimit-Limit'], 'remaining': headers['X-RateLimit-Remaining'], 'reset': headers['X-RateLimit-Reset']}
        except KeyError:
            data = {}
        return cls.from_dict(data)","
class RateLimitsInfo:
    
    def __str__(self):
        
        pass
    @classmethod
    def from_dict(cls, data):
        '''Create RateLimitsInfo from a dictionary.'''
        pass
    @classmethod
    def from_headers(cls, headers):
        
        pass",snippet_220,4,"```python
class RateLimitsInfo:
    """"""
    Represents rate‑limit information extracted from a dictionary or HTTP headers.
    """"""

    def __init__(self, limit=None, remaining=None, reset=None, used=None):
        self.limit = limit
        self.remaining = remaining
        self.reset = reset
        self.used = used

    def __str__(self):
        parts = []
        if self.limit is not None:
            parts.append(f""Limit: {self.limit}"")
        if self.remaining is not None:
            parts.append(f""Remaining: {self.remaining}"")
        if self.reset is not None:
            parts.append(f""Reset: {self.reset}"")
        if self.used is not None:
            parts.append(f""Used: {self.used}"")
        return "", "".join(parts) if parts else ""RateLimitsInfo()""

    def __repr__(self):
        return (
            f""{self.__class__.__name__}(limit={self.limit!r}, ""
            f""remaining={self.remaining!r}, reset={self.reset!r}, used={self.used!r})""
        )

    @classmethod
    def from_dict(cls, data):
        """"""Create RateLimitsInfo from a dictionary.""""""
        if not isinstance(data, dict):
            raise TypeError(""data must be a dict"")
        return cls(
            limit=data.get(""limit""),
            remaining=data.get(""remaining""),
            reset=data.get(""reset""),
            used=data.get(""used""),
        )

    @classmethod
    def from_headers(cls, headers):
        """"""Create RateLimitsInfo from HTTP headers.""""""
        if not isinstance(headers, dict):
            raise TypeError(""headers must be a dict"")

        # Normalise header keys to lower case for case‑insensitive lookup
        hdr = {k.lower(): v for k, v in headers.items()}

        def _to_int(value):
            try:
                return int(value)
            except (TypeError, ValueError):
                return value

        return cls(
            limit=_to_int(hdr.get(""x-ratelimit-limit"")),
            remaining=_to_int(hdr.get(""x-ratelimit-remaining"")),
            reset=_to_int(hdr.get(""x-ratelimit-reset"")),
            used=_to_int(hdr.get(""x-ratelimit-used"")),
        )
```"
296505,CZ-NIC/python-rt,CZ-NIC_python-rt/rt/rest2.py,rt.rest2.Attachment,"import dataclasses
import base64

@dataclasses.dataclass
class Attachment:
    """"""Dataclass representing an attachment.""""""
    file_name: str
    file_type: str
    file_content: bytes

    def to_dict(self) -> dict[str, str]:
        """"""Convert to a dictionary for submitting to the REST API.""""""
        return {'FileName': self.file_name, 'FileType': self.file_type, 'FileContent': base64.b64encode(self.file_content).decode('utf-8')}

    def multipart_form_element(self) -> tuple[str, bytes, str]:
        """"""Convert to a tuple as required for multipart-form-data submission.""""""
        return (self.file_name, self.file_content, self.file_type)","@dataclasses.dataclass
class Attachment:
    '''Dataclass representing an attachment.'''
    def to_dict(self) -> dict[str, str]:
        
        pass
    def multipart_form_element(self) -> tuple[str, bytes, str]:
        '''Convert to a tuple as required for multipart-form-data submission.'''
        pass",snippet_221,3,"```python
import dataclasses
from dataclasses import dataclass, field
from typing import Tuple, Dict


@dataclass
class Attachment:
    """"""Dataclass representing an attachment.""""""

    filename: str
    content_type: str
    data: bytes = field(repr=False)

    def to_dict(self) -> Dict[str, str]:
        """"""Return a dictionary representation of the attachment.""""""
        return {
            ""filename"": self.filename,
            ""content_type"": self.content_type,
        }

    def multipart_form_element(self) -> Tuple[str, bytes, str]:
        """"""
        Convert to a tuple as required for multipart-form-data submission.

        Returns:
            A tuple of (filename, data, content_type).
        """"""
        return self.filename, self.data, self.content_type
```"
297960,theolind/pymysensors,theolind_pymysensors/mysensors/transport.py,mysensors.transport.Transport,"class Transport:
    """"""Handle gateway transport.

    I/O is allowed in this class. This class should host methods that
    are related to the gateway transport type.
    """"""

    def __init__(self, gateway, connect, timeout=1.0, reconnect_timeout=10.0, **kwargs):
        """"""Set up transport.""""""
        self._connect = connect
        self.can_log = False
        self.connect_task = None
        self.gateway = gateway
        self.protocol = None
        self.reconnect_timeout = reconnect_timeout
        self.timeout = timeout

    def disconnect(self):
        """"""Disconnect from the transport.""""""
        if not self.protocol or not self.protocol.transport:
            self.protocol = None
            return
        _LOGGER.info('Disconnecting from gateway')
        self.protocol.transport.close()
        self.protocol = None

    def send(self, message):
        """"""Write a message to the gateway.""""""
        if not message or not self.protocol or (not self.protocol.transport):
            return
        if not self.can_log:
            _LOGGER.debug('Sending %s', message.strip())
        try:
            self.protocol.transport.write(message.encode())
        except OSError as exc:
            _LOGGER.error('Failed writing to transport %s: %s', self.protocol.transport, exc)
            self.protocol.transport.close()
            self.protocol.conn_lost_callback()","class Transport:
    '''Handle gateway transport.
    I/O is allowed in this class. This class should host methods that
    are related to the gateway transport type.
    '''
    def __init__(self, gateway, connect, timeout=1.0, reconnect_timeout=10.0, **kwargs):
        
        pass
    def disconnect(self):
        '''Disconnect from the transport.'''
        pass
    def send(self, message):
        '''Write a message to the gateway.'''
        pass",snippet_222,4,"```python
import socket
import time


class Transport:
    '''Handle gateway transport.
    I/O is allowed in this class. This class should host methods that
    are related to the gateway transport type.
    '''
    def __init__(self, gateway, connect, timeout=1.0, reconnect_timeout=10.0, **kwargs):
        """"""
        Parameters
        ----------
        gateway : any
            Information needed by the `connect` callable to establish a connection
            (e.g. a (host, port) tuple for a TCP socket).
        connect : callable
            A function that returns a connected socket-like object when called
            with `gateway` and any additional keyword arguments.
        timeout : float, optional
            Timeout in seconds for socket operations.
        reconnect_timeout : float, optional
            Seconds to wait before attempting to reconnect after a failure.
        **kwargs : dict
            Additional keyword arguments forwarded to `connect`.
        """"""
        self.gateway = gateway
        self.connect = connect
        self.timeout = timeout
        self.reconnect_timeout = reconnect_timeout
        self.kwargs = kwargs
        self._sock = None
        self._connect()

    def _connect(self):
        """"""Establish a new connection using the provided connect callable.""""""
        if self._sock:
            self.disconnect()
        try:
            self._sock = self.connect(self.gateway, **self.kwargs)
            self._sock.settimeout(self.timeout)
        except Exception as exc:
            self._sock = None
            raise RuntimeError(f""Failed to connect to {self.gateway!r}"") from exc

    def disconnect(self):
        '''Disconnect from the transport.'''
        if self._sock:
            try:
                self._sock.shutdown(socket.SHUT_RDWR)
            except Exception:
                pass
            try:
                self._sock.close()
            except Exception:
                pass
            finally:
                self._sock = None

    def send(self, message):
        '''Write a message to the gateway.'''
        if not self._sock:
            self._connect()
        try:
            data = message if isinstance(message, (bytes, bytearray)) else message.encode('utf-8')
            self._sock.sendall(data)
        except Exception:
            # Attempt a single reconnect and retry once
            self.disconnect()
            time.sleep(self.reconnect_timeout)
            self._connect()
            if not self._sock:
                raise RuntimeError(""Unable to reconnect to gateway"")
            data = message if isinstance(message, (bytes, bytearray)) else message.encode('utf-8')
            self._sock.sendall(data)
```"
298148,edx/event-tracking,edx_event-tracking/eventtracking/backends/logger.py,eventtracking.backends.logger.LoggerBackend,"import logging
import json

class LoggerBackend:
    """"""
    Event tracker backend that uses a python logger.

    Events are logged to the INFO level as JSON strings.
    """"""

    def __init__(self, **kwargs):
        """"""
        Event tracker backend that uses a python logger.

        `name` is an identifier for the logger, which should have
            been configured using the default python mechanisms.
        """"""
        name = kwargs.get('name', None)
        self.max_event_size = kwargs.get('max_event_size', MAX_EVENT_SIZE)
        self.event_logger = logging.getLogger(name)
        level = kwargs.get('level', 'info')
        self.log = getattr(self.event_logger, level.lower())

    def send(self, event):
        """"""Send the event to the standard python logger""""""
        event_str = json.dumps(event, cls=DateTimeJSONEncoder)
        if self.max_event_size is None or len(event_str) <= self.max_event_size:
            self.log(event_str)","
class LoggerBackend:
    '''
    Event tracker backend that uses a python logger.
    Events are logged to the INFO level as JSON strings.
    '''
    def __init__(self, **kwargs):
        '''
        Event tracker backend that uses a python logger.
        `name` is an identifier for the logger, which should have
            been configured using the default python mechanisms.
        '''
        pass
    def send(self, event):
        '''Send the event to the standard python logger'''
        pass",snippet_223,3,"```python
import logging
import json


class LoggerBackend:
    '''
    Event tracker backend that uses a python logger.
    Events are logged to the INFO level as JSON strings.
    '''
    def __init__(self, **kwargs):
        '''
        Event tracker backend that uses a python logger.
        `name` is an identifier for the logger, which should have
            been configured using the default python mechanisms.
        '''
        name = kwargs.get('name', __name__)
        self.logger = logging.getLogger(name)

    def send(self, event):
        '''Send the event to the standard python logger'''
        try:
            message = json.dumps(event)
        except (TypeError, ValueError):
            # Fallback: use str representation if not JSON serializable
            message = str(event)
        self.logger.info(message)
```"
303020,UpCloudLtd/upcloud-python-api,UpCloudLtd_upcloud-python-api/upcloud_api/cloud_manager/host_mixin.py,upcloud_api.cloud_manager.host_mixin.HostManager,"from upcloud_api.api import API
from upcloud_api.host import Host

class HostManager:
    """"""
    Functions for managing hosts. Intended to be used as a mixin for CloudManager.
    """"""
    api: API

    def get_hosts(self):
        """"""
        Returns a list of available hosts, along with basic statistics of them when available.
        """"""
        url = '/host'
        res = self.api.get_request(url)
        return [Host(**host) for host in res['hosts']['host']]

    def get_host(self, id: str) -> Host:
        """"""
        Returns detailed information about a specific host.
        """"""
        url = f'/host/{id}'
        res = self.api.get_request(url)
        return Host(**res['host'])

    def modify_host(self, host: str, description: str) -> Host:
        """"""
        Modifies description of a specific host.
        """"""
        url = f'/host/{host}'
        body = {'host': {'description': description}}
        res = self.api.patch_request(url, body)
        return Host(**res['host'])","
class HostManager:
    
    def get_hosts(self):
        
        pass
    def get_hosts(self):
        
        pass
    def modify_host(self, host: str, description: str) -> Host:
        '''
        Modifies description of a specific host.
        '''
        pass",snippet_224,4,"```python
from dataclasses import dataclass
from typing import Dict, List


@dataclass
class Host:
    """"""Simple representation of a host.""""""
    host: str
    description: str


class HostManager:
    """"""Manages a collection of Host objects in memory.""""""

    def __init__(self) -> None:
        # Internal storage: mapping from host name to Host instance
        self._hosts: Dict[str, Host] = {}

    def get_hosts(self) -> List[Host]:
        """"""
        Return a list of all Host objects managed by this instance.
        """"""
        return list(self._hosts.values())

    def modify_host(self, host: str, description: str) -> Host:
        """"""
        Modify the description of a specific host.

        Parameters
        ----------
        host : str
            The identifier of the host to modify.
        description : str
            The new description for the host.

        Returns
        -------
        Host
            The updated Host object.

        Raises
        ------
        KeyError
            If the host does not exist in the manager.
        """"""
        if host not in self._hosts:
            raise KeyError(f""Host '{host}' not found."")
        self._hosts[host].description = description
        return self._hosts[host]

    # Optional helper methods for completeness

    def add_host(self, host: str, description: str) -> Host:
        """"""
        Add a new host to the manager.

        Parameters
        ----------
        host : str
            The identifier of the new host.
        description : str
            The description for the new host.

        Returns
        -------
        Host
            The newly created Host object.

        Raises
        ------
        ValueError
            If the host already exists.
        """"""
        if host in self._hosts:
            raise ValueError(f""Host '{host}' already exists."")
        new_host = Host(host=host, description=description)
        self._hosts[host] = new_host
        return new_host

    def remove_host(self, host: str) -> None:
        """"""
        Remove a host from the manager.

        Parameters
        ----------
        host : str
            The identifier of the host to remove.

        Raises
        ------
        KeyError
            If the host does not exist.
        """"""
        if host not in self._hosts:
            raise KeyError(f""Host '{host}' not found."")
        del self._hosts[host]
```"
304970,threeML/astromodels,threeML_astromodels/astromodels/functions/template_model.py,astromodels.functions.template_model.TemplateFile,"import numpy as np
import h5py
from dataclasses import dataclass
import collections
from typing import Dict, List, Optional, Union

@dataclass
class TemplateFile:
    """"""
    simple container to read and write
    the data to an hdf5 file

    """"""
    name: str
    description: str
    grid: np.ndarray
    parameters: Dict[str, np.ndarray]
    parameter_order: List[str]
    energies: np.ndarray
    interpolation_degree: int
    spline_smoothing_factor: float

    def save(self, file_name: str):
        """"""
        serialize the contents to a file

        :param file_name:
        :type file_name: str
        :returns:

        """"""
        with h5py.File(file_name, 'w') as f:
            f.attrs['name'] = self.name
            f.attrs['description'] = self.description
            f.attrs['interpolation_degree'] = self.interpolation_degree
            f.attrs['spline_smoothing_factor'] = self.spline_smoothing_factor
            f.create_dataset('energies', data=self.energies, compression='gzip')
            f.create_dataset('grid', data=self.grid, compression='gzip')
            dt = h5py.special_dtype(vlen=str)
            po = np.array(self.parameter_order, dtype=dt)
            f.create_dataset('parameter_order', data=po)
            par_group = f.create_group('parameters')
            for k in self.parameter_order:
                par_group.create_dataset(k, data=self.parameters[k], compression='gzip')

    @classmethod
    def from_file(cls, file_name: str):
        """"""
        read contents from a file

        :param cls:
        :type cls:
        :param file_name:
        :type file_name: str
        :returns:

        """"""
        with h5py.File(file_name, 'r') as f:
            name = f.attrs['name']
            description = f.attrs['description']
            interpolation_degree = f.attrs['interpolation_degree']
            spline_smoothing_factor = f.attrs['spline_smoothing_factor']
            energies = f['energies'][()]
            parameter_order = f['parameter_order'][()]
            grid = f['grid'][()]
            parameters = collections.OrderedDict()
            for k in parameter_order:
                parameters[k] = f['parameters'][k][()]
        return cls(name=name, description=description, interpolation_degree=interpolation_degree, spline_smoothing_factor=spline_smoothing_factor, energies=energies, parameter_order=parameter_order, parameters=parameters, grid=grid)","@dataclass
class TemplateFile:
    
    def save(self, file_name: str):
        
        pass
    @classmethod
    def from_file(cls, file_name: str):
        '''
        read contents from a file
        :param cls:
        :type cls:
        :param file_name:
        :type file_name: str
        :returns:
        '''
        pass",snippet_227,3,"```python
from dataclasses import dataclass

@dataclass
class TemplateFile:
    """"""Represents a template file with its textual content.""""""
    content: str = """"

    def save(self, file_name: str) -> None:
        """"""
        Write the template content to the specified file.

        Parameters
        ----------
        file_name : str
            Path to the file where the content should be written.
        """"""
        with open(file_name, ""w"", encoding=""utf-8"") as f:
            f.write(self.content)

    @classmethod
    def from_file(cls, file_name: str) -> ""TemplateFile"":
        """"""
        Read the content of a file and create a TemplateFile instance.

        Parameters
        ----------
        file_name : str
            Path to the file to read.

        Returns
        -------
        TemplateFile
            An instance containing the file's content.
        """"""
        with open(file_name, ""r"", encoding=""utf-8"") as f:
            content = f.read()
        return cls(content=content)
```"
305605,NASA-AMMOS/AIT-Core,ait/core/tlm.py,core.tlm.PacketContext,"class PacketContext:
    """"""PacketContext

    A PacketContext provides a simple wrapper around a Packet so that
    field accesses of the form:

        packet.fieldname

    may also be specified as:

        packet[fieldname]

    This latter syntax allows a PacketContext to be used as a symbol
    table when evaluating PacketExpressions.
    """"""
    __slots__ = ['_packet']

    def __init__(self, packet):
        """"""Creates a new PacketContext for the given Packet.""""""
        self._packet = packet

    def __getitem__(self, name):
        """"""Returns packet[name]""""""
        result = None
        if self._packet._hasattr(name):
            result = self._packet._getattr(name)
        else:
            msg = ""Packet '%s' has no field '%s'""
            values = (self._packet._defn.name, name)
            raise KeyError(msg % values)
        return result","class PacketContext:
    
    def __init__(self, packet):
        '''Creates a new PacketContext for the given Packet.'''
        pass
    def __getitem__(self, name):
        
        pass",snippet_229,3,"```python
class PacketContext:
    def __init__(self, packet):
        '''Creates a new PacketContext for the given Packet.'''
        self.packet = packet

    def __getitem__(self, name):
        # Try dictionary-style access first
        if isinstance(self.packet, dict):
            return self.packet[name]
        # Fallback to attribute access
        try:
            return getattr(self.packet, name)
        except AttributeError:
            raise KeyError(name)
```"
305610,NASA-AMMOS/AIT-Core,ait/core/tlm.py,core.tlm.RawPacket,"class RawPacket:
    """"""RawPacket

    Wraps a packet such that:

        packet.raw.fieldname

    returns the value of fieldname as a raw value with no enumeration
    substitutions or DN to EU conversions applied.
    """"""
    __slots__ = ['_packet']

    def __init__(self, packet):
        """"""Creates a new RawPacket based on the given Packet.""""""
        self._packet = packet

    def __getattr__(self, fieldname):
        """"""Returns the value of the given packet fieldname as a raw
        value with no DN to EU conversion applied.
        """"""
        return self._packet._getattr(fieldname, raw=True)","class RawPacket:
    
    def __init__(self, packet):
        
        pass
    def __getattr__(self, fieldname):
        '''Returns the value of the given packet fieldname as a raw
        value with no DN to EU conversion applied.
        '''
        pass",snippet_230,3,"class RawPacket:
    
    def __init__(self, packet):
        self._packet = packet

    def __getattr__(self, fieldname):
        """"""Returns the value of the given packet fieldname as a raw
        value with no DN to EU conversion applied.
        """"""
        pkt = self._packet
        # Try dictionary lookup first
        if isinstance(pkt, dict):
            if fieldname in pkt:
                return pkt[fieldname]
        # Try attribute lookup
        if hasattr(pkt, fieldname):
            return getattr(pkt, fieldname)
        # If not found, raise AttributeError
        raise AttributeError(f""'{type(pkt).__name__}' object has no attribute '{fieldname}'"")"
305613,NASA-AMMOS/AIT-Core,ait/core/tlm.py,core.tlm.WordArray,"class WordArray:
    """"""WordArrays are somewhat analogous to Python bytearrays, but
    currently much more limited in functionality.  They provide a
    readonly view of a bytearray addressable and iterable as a sequence
    of 16-bit words.  This is convenient for telemetry processing as
    packets are often more naturally addressable on word, as opposed to
    byte, boundaries.
    """"""
    __slots__ = ['_bytes']

    def __init__(self, bytes):
        """"""Creates a new wordarray from the given bytearray.

        The given bytearray should contain an even number of bytes.  If
        odd, the last byte is ignored.
        """"""
        self._bytes = bytes

    def __getitem__(self, key):
        """"""Returns the words in this wordarray at the given Python slice
        or word at the given integer index.""""""
        length = len(self)
        if isinstance(key, slice):
            return [self[n] for n in range(*key.indices(length))]
        elif isinstance(key, int):
            if key < 0:
                key += length
            if key >= length:
                msg = 'wordarray index (%d) is out of range [0 %d].'
                raise IndexError(msg % (key, length - 1))
            index = 2 * key
            return self._bytes[index] << 8 | self._bytes[index + 1]
        else:
            raise TypeError('wordarray indices must be integers.')

    def __len__(self):
        """"""Returns the number of words in this wordarray.""""""
        return len(self._bytes) / 2","class WordArray:
    '''WordArrays are somewhat analogous to Python bytearrays, but
    currently much more limited in functionality.  They provide a
    readonly view of a bytearray addressable and iterable as a sequence
    of 16-bit words.  This is convenient for telemetry processing as
    packets are often more naturally addressable on word, as opposed to
    byte, boundaries.
    '''
    def __init__(self, bytes):
        '''Creates a new wordarray from the given bytearray.
        The given bytearray should contain an even number of bytes.  If
        odd, the last byte is ignored.
        '''
        pass
    def __getitem__(self, key):
        '''Returns the words in this wordarray at the given Python slice
        or word at the given integer index.'''
        pass
    def __len__(self):
        
        pass",snippet_231,4,"```python
class WordArray:
    '''WordArrays are somewhat analogous to Python bytearrays, but
    currently much more limited in functionality.  They provide a
    readonly view of a bytearray addressable and iterable as a sequence
    of 16-bit words.  This is convenient for telemetry processing as
    packets are often more naturally addressable on word, as opposed to
    byte, boundaries.
    '''
    def __init__(self, bytes_):
        '''Creates a new wordarray from the given bytearray.
        The given bytearray should contain an even number of bytes.  If
        odd, the last byte is ignored.
        '''
        # Ensure we have a bytes-like object and drop the last byte if odd
        if len(bytes_) % 2:
            bytes_ = bytes_[:-1]
        # Store as immutable bytes for safety
        self._bytes = bytes(bytes_)

    def __len__(self):
        return len(self._bytes) // 2

    def __getitem__(self, key):
        '''Returns the words in this wordarray at the given Python slice
        or word at the given integer index.'''
        nwords = len(self)
        if isinstance(key, int):
            # Handle negative indices
            if key < 0:
                key += nwords
            if key < 0 or key >= nwords:
                raise IndexError('WordArray index out of range')
            i = key * 2
            return (self._bytes[i] << 8) | self._bytes[i + 1]
        elif isinstance(key, slice):
            start, stop, step = key.indices(nwords)
            # Build list of words for the slice
            result = []
            for idx in range(start, stop, step):
                i = idx * 2
                result.append((self._bytes[i] << 8) | self._bytes[i + 1])
            return result
        else:
            raise TypeError('WordArray indices must be integers or slices')

    def __iter__(self):
        '''Iterate over the words in the array.'''
        for i in range(len(self)):
            yield self[i]

    def __repr__(self):
        return f'{self.__class__.__name__}({list(self)!r})'
```"
306020,ihmeuw/vivarium,ihmeuw_vivarium/src/vivarium/framework/lookup/interpolation.py,vivarium.framework.lookup.interpolation.Order0Interp,"import numpy as np
import pandas as pd
from collections.abc import Hashable, Sequence

class Order0Interp:
    """"""A callable that returns the result of order 0 interpolation over input data.

    Attributes
    ----------
    data
        The data from which to build the interpolation.
    value_columns
        Columns to be interpolated.
    extrapolate
        Whether or not to extrapolate beyond the edge of supplied bins.
    parameter_bins
        A dictionary where they keys are a tuple of the form
        (column name used in call, column name for left bin edge, column name for right bin edge)
        and the values are dictionaries of the form {""bins"": [ordered left edges of bins],
        ""max"": max right edge (used when extrapolation not allowed)}.

    """"""

    def __init__(self, data: pd.DataFrame, continuous_parameters: Sequence[Sequence[str]], value_columns: list[str], extrapolate: bool, validate: bool):
        """"""
        Parameters
        ----------
        data
            Data frame used to build interpolation.
        continuous_parameters
            Parameter columns. Should be of form (column name used in call,
            column name for left bin edge, column name for right bin edge)
            or column name. Assumes left bin edges are inclusive and
            right exclusive.
        value_columns
            Columns to be interpolated.
        extrapolate
            Whether or not to extrapolate beyond the edge of supplied bins.
        validate
            Whether or not to validate the data.
        """"""
        if validate:
            check_data_complete(data, continuous_parameters)
        self.data = data.copy()
        self.value_columns = value_columns
        self.extrapolate = extrapolate
        self.parameter_bins = {}
        for p in continuous_parameters:
            left_edge = self.data[p[1]].drop_duplicates().sort_values()
            max_right = self.data[p[2]].drop_duplicates().max()
            self.parameter_bins[tuple(p)] = {'bins': left_edge.reset_index(drop=True), 'max': max_right}

    def __call__(self, interpolants: pd.DataFrame) -> pd.DataFrame:
        """"""Find the bins for each parameter for each interpolant in interpolants
        and return the values from data there.

        Parameters
        ----------
        interpolants
            Data frame containing the parameters to interpolate..

        Returns
        -------
            A table with the interpolated values for the given interpolants.
        """"""
        interpolant_bins = pd.DataFrame(index=interpolants.index)
        merge_cols = []
        for cols, d in self.parameter_bins.items():
            bins = d['bins']
            max_right = d['max']
            merge_cols.append(cols[1])
            interpolant_col = interpolants[cols[0]]
            if not self.extrapolate and (interpolant_col.min() < bins[0] or interpolant_col.max() >= max_right):
                raise ValueError(f'Extrapolation outside of bins used to set up interpolation is only allowed when explicitly set in creation of Interpolation. Extrapolation is currently off for this interpolation, and parameter {cols[0]} includes data outside of original bins.')
            bin_indices = np.digitize(interpolant_col, bins.tolist())
            bin_indices[bin_indices > 0] -= 1
            interpolant_bins[cols[1]] = bins.loc[bin_indices].values
        index = interpolant_bins.index
        interp_vals = interpolant_bins.merge(self.data, how='left', on=merge_cols).set_index(index)
        return interp_vals[self.value_columns]","
class Order0Interp:
    '''A callable that returns the result of order 0 interpolation over input data.
    Attributes
    ----------
    data
        The data from which to build the interpolation.
    value_columns
        Columns to be interpolated.
    extrapolate
        Whether or not to extrapolate beyond the edge of supplied bins.
    parameter_bins
        A dictionary where they keys are a tuple of the form
        (column name used in call, column name for left bin edge, column name for right bin edge)
        and the values are dictionaries of the form {""bins"": [ordered left edges of bins],
        ""max"": max right edge (used when extrapolation not allowed)}.
    '''
    def __init__(self, data: pd.DataFrame, continuous_parameters: Sequence[Sequence[str]], value_columns: list[str], extrapolate: bool, validate: bool):
        
        pass
    def __call__(self, interpolants: pd.DataFrame) -> pd.DataFrame:
        '''Find the bins for each parameter for each interpolant in interpolants
        and return the values from data there.
        Parameters
        ----------
        interpolants
            Data frame containing the parameters to interpolate..
        Returns
        -------
            A table with the interpolated values for the given interpolants.
        '''
        pass",snippet_232,3,"```python
import pandas as pd
from typing import Sequence, Dict, Tuple, Any


class Order0Interp:
    '''A callable that returns the result of order 0 interpolation over input data.
    Attributes
    ----------
    data
        The data from which to build the interpolation.
    value_columns
        Columns to be interpolated.
    extrapolate
        Whether or not to extrapolate beyond the edge of supplied bins.
    parameter_bins
        A dictionary where the keys are a tuple of the form
        (column name used in call, column name for left bin edge, column name for right bin edge)
        and the values are dictionaries of the form {""bins"": [ordered left edges of bins],
        ""max"": max right edge (used when extrapolation not allowed)}.
    '''
    def __init__(self,
                 data: pd.DataFrame,
                 continuous_parameters: Sequence[Sequence[str]],
                 value_columns: list[str],
                 extrapolate: bool,
                 validate: bool = False):
        self.data = data
        self.value_columns = value_columns
        self.extrapolate = extrapolate

        # Build parameter_bins dictionary
        self.parameter_bins: Dict[Tuple[str, str, str], Dict[str, Any]] = {}
        for param in continuous_parameters:
            if len(param) != 3:
                raise ValueError(""Each continuous parameter must be a sequence of 3 strings"")
            call_col, left_col, right_col = param
            left_edges = sorted(self.data[left_col].unique())
            max_right = self.data[right_col].max()
            self.parameter_bins[(call_col, left_col, right_col)] = {
                ""bins"": left_edges,
                ""max"": max_right
            }

        if validate:
            # Basic validation: ensure all required columns exist
            required_cols = set()
            for _, left_col, right_col in self.parameter_bins:
                required_cols.update([left_col, right_col])
            required_cols.update(self.value_columns)
            missing = required_cols - set(self.data.columns)
            if missing:
                raise ValueError(f""Missing columns in data: {missing}"")

    def __call__(self, interpolants: pd.DataFrame) -> pd.DataFrame:
        '''Find the bins for each parameter for each interpolant in interpolants
        and return the values from data there.
        Parameters
        ----------
        interpolants
            Data frame containing the parameters to interpolate.
        Returns
        -------
            A table with the interpolated values for the given interpolants.
        '''
        # Prepare result DataFrame
        result = pd.DataFrame(index=interpolants.index, columns=self.value_columns, dtype=float)

        # If no parameters, return NaNs
        if not self.parameter_bins:
            return result

        # Iterate over each row in interpolants
        for idx, row in interpolants.iterrows():
            mask = pd.Series(True, index=self.data.index)

            # Apply each parameter binning
            for (call_col, left_col, right_col), bin_info in self.parameter_bins.items():
                val = row.get(call_col, None)
                if pd.isna(val):
                    mask = pd.Series(False, index=self.data.index)
                    break

                # Extrapolation check
                if not self.extrapolate:
                    if val < bin_info[""bins""][0] or val > bin_info[""max""]:
                        mask = pd.Series(False, index=self.data.index)
                        break

                # Bin selection: left <= val < right
                mask &= (self.data[left_col] <= val) & (val < self.data[right_col])

                if not mask.any():
                    break

            # If a matching row exists, take the first one
            if mask.any():
                matched_row = self.data.loc[mask].iloc[0]
                result.loc[idx, self.value_columns] = matched_row[self.value_columns].values
            else:
                # No match: leave NaNs
                continue

        return result
```"
306151,lago-project/lago,lago-project_lago/lago/templates.py,lago.templates.Template,"class Template:
    """"""
    Disk image template class

    Attributes:
        name (str): Name of this template
        _versions (dict(str:TemplateVersion)): versions for this template
    """"""

    def __init__(self, name, versions):
        """"""
        Args:
            name (str): Name of the template
            versions (dict(str:TemplateVersion)): dictionary with the
                version_name: :class:`TemplateVersion` pairs for this template
        """"""
        self.name = name
        self._versions = versions

    def get_version(self, ver_name=None):
        """"""
        Get the given version for this template, or the latest

        Args:
            ver_name (str or None): Version to retieve, None for the latest

        Returns:
            TemplateVersion: The version matching the given name or the latest
                one
        """"""
        if ver_name is None:
            return self.get_latest_version()
        return self._versions[ver_name]

    def get_latest_version(self):
        """"""
        Retrieves the latest version for this template, the latest being the
        one with the newest timestamp

        Returns:
            TemplateVersion
        """"""
        return max(self._versions.values(), key=lambda x: x.timestamp())","class Template:
    
    def __init__(self, name, versions):
        
        pass
    def get_version(self, ver_name=None):
        '''
        Get the given version for this template, or the latest
        Args:
            ver_name (str or None): Version to retieve, None for the latest
        Returns:
            TemplateVersion: The version matching the given name or the latest
                one
        '''
        pass
    def get_latest_version(self):
        
        pass",snippet_233,4,"```python
class Template:
    def __init__(self, name, versions):
        """"""
        Initialize a Template instance.

        Args:
            name (str): The name of the template.
            versions (list): A list of version objects. Each object must have a
                `name` attribute that represents the version string.
        """"""
        self.name = name
        # Ensure versions is a list
        self.versions = list(versions)

        # Sort versions by parsed version number for consistent ordering
        self.versions.sort(key=lambda v: self._parse_version(v.name))

    def get_version(self, ver_name=None):
        """"""
        Get the given version for this template, or the latest.

        Args:
            ver_name (str or None): Version to retrieve, None for the latest.

        Returns:
            The version object matching the given name or the latest one.

        Raises:
            ValueError: If the requested version is not found.
        """"""
        if ver_name is None:
            return self.get_latest_version()

        for v in self.versions:
            if v.name == ver_name:
                return v

        raise ValueError(f""Version '{ver_name}' not found in template '{self.name}'"")

    def get_latest_version(self):
        """"""
        Return the latest version of the template.

        Returns:
            The latest version object.

        Raises:
            ValueError: If there are no versions available.
        """"""
        if not self.versions:
            raise ValueError(f""No versions available for template '{self.name}'"")
        return self.versions[-1]

    @staticmethod
    def _parse_version(ver_str):
        """"""
        Parse a version string into a tuple of integers for comparison.

        Examples:
            ""1.2.3"" -> (1, 2, 3)
            ""2.0""   -> (2, 0)
            ""10""    -> (10,)

        Args:
            ver_str (str): The version string.

        Returns:
            tuple[int]: Parsed version components.
        """"""
        parts = ver_str.split('.')
        nums = []
        for part in parts:
            try:
                nums.append(int(part))
            except ValueError:
                # Non-numeric part: treat as 0 for comparison
                nums.append(0)
        return tuple(nums)
```"
311056,geronimp/graftM,geronimp_graftM/graftm/deduplicator.py,graftm.deduplicator.Deduplicator,"class Deduplicator:
    """"""Deduplicates sequences""""""

    def deduplicate(self, aligned_sequence_objects):
        """"""Sort the given aligned_sequence objects into an array of arrays,
        where input sequences are grouped iff they have the same sequence

        Parameters
        ----------
        aligned_sequence_objects: array of Sequence objects
            input sequences

        Returns
        -------
        Array of arrays of Sequence objects""""""
        sequence_to_groups = {}
        for s in aligned_sequence_objects:
            try:
                sequence_to_groups[s.seq].append(s)
            except KeyError:
                sequence_to_groups[s.seq] = [s]
        return list(sequence_to_groups.values())

    def lca_taxonomy(self, deduplicated_sequences, taxonomy_hash):
        """"""Given a set of deduplicated sequences and a taxonomy hash,
        return the respective LCAs of taxonomy

        Parameters
        ----------
        deduplicated_sequences: Array of arrays of Sequence objects
            as output from deduplicate()
        taxonomy_hash: dictionary 
            of sequence names to taxonomy array (i.e. array of str)

        Returns
        -------
        Array of taxonomy LCAs""""""
        to_return = []
        for dup_group in deduplicated_sequences:
            lca = taxonomy_hash[dup_group[0].name]
            for s in dup_group[1:]:
                for i, tax in enumerate(taxonomy_hash[s.name]):
                    if i >= len(lca) or tax != lca[i]:
                        lca = lca[:i]
                        break
                if len(lca) > len(taxonomy_hash[s.name]):
                    lca = lca[:len(taxonomy_hash[s.name])]
            to_return.append(lca)
        return to_return","class Deduplicator:
    '''Deduplicates sequences'''
    def deduplicate(self, aligned_sequence_objects):
        
        pass
    def lca_taxonomy(self, deduplicated_sequences, taxonomy_hash):
        '''Given a set of deduplicated sequences and a taxonomy hash,
        return the respective LCAs of taxonomy
        Parameters
        ----------
        deduplicated_sequences: Array of arrays of Sequence objects
            as output from deduplicate()
        taxonomy_hash: dictionary 
            of sequence names to taxonomy array (i.e. array of str)
        Returns
        -------
        Array of taxonomy LCAs'''
        pass",snippet_235,3,"```python
class Deduplicator:
    '''Deduplicates sequences'''
    def deduplicate(self, aligned_sequence_objects):
        """"""
        Group Sequence objects by identical sequence strings.

        Parameters
        ----------
        aligned_sequence_objects : list
            List of Sequence objects. Each object must have a `sequence` attribute.

        Returns
        -------
        list
            List of lists, where each sublist contains Sequence objects that share the same
            sequence string.
        """"""
        groups = {}
        for seq_obj in aligned_sequence_objects:
            seq_key = getattr(seq_obj, 'sequence', None)
            if seq_key is None:
                # If no sequence attribute, treat each object as unique
                seq_key = id(seq_obj)
            groups.setdefault(seq_key, []).append(seq_obj)
        return list(groups.values())

    def lca_taxonomy(self, deduplicated_sequences, taxonomy_hash):
        """"""
        Given a set of deduplicated sequences and a taxonomy hash,
        return the respective LCAs of taxonomy.

        Parameters
        ----------
        deduplicated_sequences : list of lists
            Output from deduplicate(): each sublist contains Sequence objects that are identical.
        taxonomy_hash : dict
            Mapping from sequence names to taxonomy arrays (list of str).

        Returns
        -------
        list
            List of taxonomy LCAs (each LCA is a list of str) corresponding to each group.
        """"""
        lca_list = []

        for group in deduplicated_sequences:
            # Collect taxonomy arrays for all sequences in the group
            taxonomies = []
            for seq_obj in group:
                name = getattr(seq_obj, 'name', None)
                if name is None:
                    continue
                tax = taxonomy_hash.get(name)
                if tax is not None:
                    taxonomies.append(tax)

            if not taxonomies:
                # No taxonomy information available for this group
                lca_list.append([])
                continue

            # Compute common prefix (LCA) across all taxonomies
            lca = taxonomies[0]
            for tax in taxonomies[1:]:
                # Find common prefix length
                min_len = min(len(lca), len(tax))
                i = 0
                while i < min_len and lca[i] == tax[i]:
                    i += 1
                lca = lca[:i]
                if not lca:
                    break
            lca_list.append(lca)

        return lca_list
```"
311087,geronimp/graftM,geronimp_graftM/graftm/search_table.py,graftm.search_table.SearchTableWriter,"import logging
from graftm.sequence_search_results import SequenceSearchResult

class SearchTableWriter:
    """"""
    Class for writing the search output OTU table. Basically a summary
    of hits to the HMM/Diamond searched in the following format:

             #ID    Metagenome_1    Metagenome_2    ...
            HMM1    50              6
            HMM2    195             41
            HMM3    2               20120
            ...

    You just need to specify a series of SequenceSearchResult objects, and an
    output path.
    """"""

    def _interpret_hits(self, results_list, base_list):
        """"""Sort reads that hit multiple HMMs to the databases to which they had
        the highest bit score. Return a dictionary containing HMMs as keys, and
        number of hits as the values.

        This function is set up so that the read names could easily be returned
        instead of numbers, for future development of GraftM

        Parameters
        ----------
        results_list: list
            Iterable if SequenceSearchResult objects. e.g.
                [SequenceSearchResult_1, SequenceSearchResult_2, ...]

        base_list: list
            Iterable of the basenames for each sequence file provided to graftM
            e.g.
                [sample_1, sample_2, ...]

        Returns
        -------
        dictionary:
            Contains samples as entries. The value for each sample is another
            dictionary with HMM as the key, and number of hits as values:
                {""sample_1"":{HMM_1: 12
                             HMM_2: 35
                             HMM_3: 1258
                             ...}
                 ...
                }

        """"""
        logging.debug('Sorting reads into HMMs by bit score')
        run_results = {}
        for base, results in zip(base_list, results_list):
            search_results = {}
            for search in results():
                search_list = list(search.each([SequenceSearchResult.QUERY_ID_FIELD, SequenceSearchResult.ALIGNMENT_BIT_SCORE, SequenceSearchResult.HMM_NAME_FIELD]))
                for hit in search_list:
                    if hit[0] in search_results:
                        if float(hit[1]) > search_results[hit[0]][0]:
                            search_results[hit[0]] = [float(hit[1]), hit[2]]
                    else:
                        search_results[hit[0]] = [float(hit[1]), hit[2]]
            run_results[base] = search_results
        db_count = {}
        for run in run_results.keys():
            run_count = {}
            for entry in list(run_results[run].values()):
                key = entry[1]
                if key in run_count:
                    run_count[key] += 1
                else:
                    run_count[key] = 1
            db_count[run] = run_count
        return db_count

    def _write_results(self, db_count, output_path):
        """"""Write the table to the output_path directory

        db_count: dict
            Contains samples as entries. The value for each sample is another
            dictionary with HMM as the key, and number of hits as values:
                {""sample_1"":{HMM_1: 12
                             HMM_2: 35
                             HMM_3: 1258
                             ...}
                 ...
                }

        output_path: str
            Path to output file to which the resultant output file will be
            written to.
        """"""
        logging.debug('Writing search otu table to file: %s' % output_path)
        output_dict = {}
        for idx, value_dict in enumerate(db_count.values()):
            for database, count in value_dict.items():
                if database in output_dict:
                    output_dict[database].append(str(count))
                else:
                    output_dict[database] = ['0'] * idx + [str(count)]
            for key, item in output_dict.items():
                if len(item) == idx:
                    output_dict[key].append('0')
        with open(output_path, 'w') as out:
            out.write('\t'.join(['#ID'] + list(db_count.keys())) + '\n')
            for key, item in output_dict.items():
                out.write('%s\t%s' % (key, '\t'.join(item)) + '\n')

    def build_search_otu_table(self, search_results_list, base_list, output_path):
        """"""
        Build an OTU from SequenceSearchResult objects

        Parameters
        ----------
        search_results_list: list
            Iterable if SequenceSearchResult objects. e.g.
                [SequenceSearchResult_1, SequenceSearchResult_2, ...]
        base_list: list
            Iterable of the basenames for each sequence file provided to graftM
            e.g.
                [sample_1, sample_2, ...]
        output_path: str
            Path to output file to which the resultant output file will be
            written to.
        """"""
        db_count = self._interpret_hits(search_results_list, base_list)
        self._write_results(db_count, output_path)","
class SearchTableWriter:
    '''
    Class for writing the search output OTU table. Basically a summary
    of hits to the HMM/Diamond searched in the following format:
             #ID    Metagenome_1    Metagenome_2    ...
            HMM1    50              6
            HMM2    195             41
            HMM3    2               20120
            ...
    You just need to specify a series of SequenceSearchResult objects, and an
    output path.
    '''
    def _interpret_hits(self, results_list, base_list):
        '''Sort reads that hit multiple HMMs to the databases to which they had
        the highest bit score. Return a dictionary containing HMMs as keys, and
        number of hits as the values.
        This function is set up so that the read names could easily be returned
        instead of numbers, for future development of GraftM
        Parameters
        ----------
        results_list: list
            Iterable if SequenceSearchResult objects. e.g.
                [SequenceSearchResult_1, SequenceSearchResult_2, ...]
        base_list: list
            Iterable of the basenames for each sequence file provided to graftM
            e.g.
                [sample_1, sample_2, ...]
        Returns
        -------
        dictionary:
            Contains samples as entries. The value for each sample is another
            dictionary with HMM as the key, and number of hits as values:
                {""sample_1"":{HMM_1: 12
                             HMM_2: 35
                             HMM_3: 1258
                             ...}
                 ...
                }
        '''
        pass
    def _write_results(self, db_count, output_path):
        
        pass
    def build_search_otu_table(self, search_results_list, base_list, output_path):
        
        pass",snippet_236,4,"```python
import os
from collections import defaultdict

class SearchTableWriter:
    '''
    Class for writing the search output OTU table. Basically a summary
    of hits to the HMM/Diamond searched in the following format:
             #ID    Metagenome_1    Metagenome_2    ...
            HMM1    50              6
            HMM2    195             41
            HMM3    2               20120
            ...
    You just need to specify a series of SequenceSearchResult objects, and an
    output path.
    '''
    def _interpret_hits(self, results_list, base_list):
        '''
        Sort reads that hit multiple HMMs to the databases to which they had
        the highest bit score. Return a dictionary containing HMMs as keys, and
        number of hits as the values.
        This function is set up so that the read names could easily be returned
        instead of numbers, for future development of GraftM
        Parameters
        ----------
        results_list: list
            Iterable if SequenceSearchResult objects. e.g.
                [SequenceSearchResult_1, SequenceSearchResult_2, ...]
        base_list: list
            Iterable of the basenames for each sequence file provided to graftM
            e.g.
                [sample_1, sample_2, ...]
        Returns
        -------
        dictionary:
            Contains samples as entries. The value for each sample is another
            dictionary with HMM as the key, and number of hits as values:
                {""sample_1"":{HMM_1: 12
                             HMM_2: 35
                             HMM_3: 1258
                             ...}
                 ...
                }
        '''
        # Map each query to its best hit (highest bit score)
        best_hits = {}
        for res in results_list:
            # Resolve attributes that may differ between implementations
            query = getattr(res, 'query_name', None) or getattr(res, 'query', None)
            if query is None:
                continue  # skip if no query name
            subject = getattr(res, 'subject_name', None) or getattr(res, 'subject', None)
            if subject is None:
                continue  # skip if no subject
            bit_score = getattr(res, 'bit_score', None) or getattr(res, 'score', None)
            if bit_score is None:
                continue  # skip if no score
            # Determine sample name
            sample_name = None
            if hasattr(res, 'sample_name'):
                sample_name = res.sample_name
            elif hasattr(res, 'sample_index'):
                idx = res.sample_index
                if isinstance(idx, int) and 0 <= idx < len(base_list):
                    sample_name = base_list[idx]
            if sample_name is None:
                # If no sample info, skip
                continue

            # Keep only the best hit per query
            if query not in best_hits or bit_score > best_hits[query]['bit_score']:
                best_hits[query] = {
                    'subject': subject,
                    'bit_score': bit_score,
                    'sample': sample_name
                }

        # Count hits per sample per HMM
        db_count = defaultdict(lambda: defaultdict(int))
        for hit in best_hits.values():
            sample = hit['sample']
            subject = hit['subject']
            db_count[sample][subject] += 1

        return db_count

    def _write_results(self, db_count, output_path):
        '''
        Write the OTU table to the specified output path.
        Parameters
        ----------
        db_count: dict
            Output of _interpret_hits: {sample: {HMM: count}}
        output_path: str
            Path to write the table.
        '''
        # Ensure directory exists
        os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)

        # Determine all samples and HMMs
        samples = sorted(db_count.keys())
        all_hmms = set()
        for sample_dict in db_count.values():
            all_hmms.update(sample_dict.keys())
        hmms = sorted(all_hmms)

        with open(output_path, 'w', encoding='utf-8') as fh:
            # Header
            header = ['#ID'] + samples
            fh.write('\t'.join(header) + '\n')
            # Rows
            for hmm in hmms:
                row = [hmm]
                for sample in samples:
                    row.append(str(db_count[sample].get(hmm, 0)))
                fh.write('\t'.join(row) + '\n')

    def build_search_otu_table(self, search_results_list, base_list, output_path):
        '''
        Main entry point: interpret hits and write the OTU table.
        Parameters
        ----------
        search_results_list: list
            List of SequenceSearchResult objects.
        base_list: list
            List of sample base names.
        output_path: str
            Path to write the OTU table.
        '''
        db_count = self._interpret_hits(search_results_list, base_list)
        self._write_results(db_count, output_path)
```"
311100,geronimp/graftM,geronimp_graftM/graftm/tree_decorator.py,graftm.tree_decorator.TreeDecorator,"from graftm.greengenes_taxonomy import GreenGenesTaxonomy, MalformedGreenGenesTaxonomyException
from graftm.getaxnseq import Getaxnseq
import logging
from graftm.taxonomy_cleaner import TaxonomyCleaner

class TreeDecorator:
    """"""
    A class that conservatively decorates trees with taxonomy, or any other
    hierarchical annotation. If all tips descending from a node within the
    provided tree have consistent taxonomy, it will be decorated with that
    taxonomy (or annotation of any type).
    """"""

    def __init__(self, tree, taxonomy, seqinfo=None):
        """"""
        Parameters
        ----------
        tree        : dendropy.Tree

            dendropy.Tree object
        taxonomy    : string
            Path to a file containing taxonomy information about the tree,
            either in Greengenes or taxtastic format (seqinfo file must also
            be provided if taxonomy is in taxtastic format).
        seqinfo     : string
            Path to a seqinfo file. This is a .csv file with the first column
            denoting the sequence name, and the second column, its most resolved
            taxonomic rank.
        """"""
        self.encountered_nodes = {}
        self.encountered_taxonomies = set()
        self.tree = tree
        logging.info('Reading in taxonomy')
        if seqinfo:
            logging.info('Importing taxtastic taxonomy from files: %s and %s' % (taxonomy, seqinfo))
            gtns = Getaxnseq()
            self.taxonomy = gtns.read_taxtastic_taxonomy_and_seqinfo(open(taxonomy), open(seqinfo))
        else:
            try:
                logging.info('Reading Greengenes style taxonomy')
                self.taxonomy = GreenGenesTaxonomy.read_file(taxonomy).taxonomy
            except MalformedGreenGenesTaxonomyException:
                raise Exception('Failed to read taxonomy as a Greengenes                                  formatted file. Was a taxtastic style                                  taxonomy provided with no seqinfo file?')

    def _write_consensus_strings(self, output):
        """"""
        Writes the taxonomy of each leaf to a file. If the leaf has no
        taxonomy, a taxonomy string will be created using the annotations
        provided to the ancestor nodes of that leaf (meaning, it will be
        decorated).

        Parameters
        ----------
        output    : string
            File to which the taxonomy strings for each leaf in the tree will
            be written in Greengenes format, e.g.
                637960147    mcrA; Euryarchaeota_mcrA; Methanomicrobia
                637699780    mcrA; Euryarchaeota_mcrA; Methanomicrobia
        """"""
        logging.info('Writing decorated taxonomy to file: %s' % output)
        with open(output, 'w') as out:
            for tip in self.tree.leaf_nodes():
                tax_name = tip.taxon.label.replace(' ', '_')
                if tip.taxon.label in self.taxonomy:
                    tax_string = '; '.join(self.taxonomy[tax_name])
                else:
                    ancestor_list = []
                    for ancestor in tip.ancestor_iter():
                        if ancestor.label:
                            split_node_name = ancestor.label.split(':')
                            if len(split_node_name) == 2:
                                ancestor_list += list(reversed(split_node_name[1].split('; ')))
                            elif len(split_node_name) == 1:
                                try:
                                    float(split_node_name[0])
                                except ValueError:
                                    ancestor_list += list(reversed(split_node_name[0].split('; ')))
                            else:
                                raise Exception('Malformed node name: %s' % ancestor.label)
                    tax_list = list(reversed(ancestor_list))
                    if len(tax_list) < 1:
                        logging.warning('No taxonomy found for species %s!' % tax_name)
                        tax_string = 'Unknown'
                    else:
                        tax_string = '; '.join(tax_list)
                output_line = '%s\t%s\n' % (tax_name, tax_string)
                out.write(output_line)

    def _rename(self, node, name):
        """"""
        Rename an internal node of the tree. If an annotation is already
        present, append the new annotation to the end of it. If a bootstrap
        value is present, add annotations are added after a "":"" as per standard
        newick format.

        Parameters
        ----------
        node: dendropy.Node
            dendropy.Node object
        name    : string
            Annotation to rename the node with.
        """"""
        if node.label:
            try:
                float(node.label)
                new_label = '%s:%s' % (node.label, name)
            except ValueError:
                new_label = '%s; %s' % (node.label, name)
            node.label = new_label
        else:
            node.label = name

    def decorate(self, output_tree, output_tax, unique_names):
        """"""
        Decorate a tree with taxonomy. This code does not allow inconsistent
        taxonomy within a clade. If one sequence in a clade has a different
        annotation to the rest, it will split the clade. Paraphyletic group
        names are distinguished if unique_names = True using a simple tally of
        each group (see unique_names below).

        Parameters
        ----------
        output_tree        : string
            File to which the decorated tree will be written.
        output_tax         : string
            File to which the taxonomy strings for each tip in the tree will be
            written.
        unique_names       : boolean
            True indicating that a unique number will be appended to the end of
            a taxonomic rank if it is found more than once in the tree
            (i.e. it is paraphyletic in the tree). If false, multiple clades
            may be assigned with the same name.
        """"""
        logging.info('Decorating tree')
        encountered_taxonomies = {}
        tc = TaxonomyCleaner()
        for node in self.tree.preorder_internal_node_iter(exclude_seed_node=True):
            max_tax_string_length = 0
            for tip in node.leaf_nodes():
                tip_label = tip.taxon.label.replace(' ', '_')
                if tip_label in self.taxonomy:
                    tax_string_length = len(self.taxonomy[tip.taxon.label.replace(' ', '_')])
                    if tax_string_length > max_tax_string_length:
                        max_tax_string_length = tax_string_length
            logging.debug('Number of ranks found for node: %i' % max_tax_string_length)
            tax_string_array = []
            for rank in range(max_tax_string_length):
                rank_tax = []
                for tip in node.leaf_nodes():
                    tip_label = tip.taxon.label.replace(' ', '_')
                    if tip_label in self.taxonomy:
                        tip_tax = self.taxonomy[tip_label]
                        if len(tip_tax) > rank:
                            tip_rank = tip_tax[rank]
                            if tip_rank not in rank_tax:
                                rank_tax.append(tip_rank)
                consistent_taxonomy = len(rank_tax) == 1
                if consistent_taxonomy:
                    tax = rank_tax.pop()
                    logging.debug('Consistent taxonomy found for node: %s' % tax)
                    if tax not in tc.meaningless_taxonomic_names:
                        if unique_names:
                            if tax in encountered_taxonomies:
                                encountered_taxonomies[tax] += 0
                                tax = '%s_%i' % (tax, encountered_taxonomies[tax])
                            else:
                                encountered_taxonomies[tax] = 0
                        tax_string_array.append(tax)
            if any(tax_string_array):
                index = 0
                for anc in node.ancestor_iter():
                    try:
                        index += anc.tax
                    except:
                        continue
                tax_string_array = tax_string_array[index:]
                if any(tax_string_array):
                    self._rename(node, '; '.join(tax_string_array))
                node.tax = len(tax_string_array)
        logging.info('Writing decorated tree to file: %s' % output_tree)
        if output_tree:
            self.tree.write(path=output_tree, schema='newick')
        if output_tax:
            self._write_consensus_strings(output_tax)","
class TreeDecorator:
    '''
    A class that conservatively decorates trees with taxonomy, or any other
    hierarchical annotation. If all tips descending from a node within the
    provided tree have consistent taxonomy, it will be decorated with that
    taxonomy (or annotation of any type).
    '''
    def __init__(self, tree, taxonomy, seqinfo=None):
        '''
        Parameters
        ----------
        tree        : dendropy.Tree
            dendropy.Tree object
        taxonomy    : string
            Path to a file containing taxonomy information about the tree,
            either in Greengenes or taxtastic format (seqinfo file must also
            be provided if taxonomy is in taxtastic format).
        seqinfo     : string
            Path to a seqinfo file. This is a .csv file with the first column
            denoting the sequence name, and the second column, its most resolved
            taxonomic rank.
        '''
        pass
    def _write_consensus_strings(self, output):
        
        pass
    def _rename(self, node, name):
        '''
        Rename an internal node of the tree. If an annotation is already
        present, append the new annotation to the end of it. If a bootstrap
        value is present, add annotations are added after a "":"" as per standard
        newick format.
        Parameters
        ----------
        node: dendropy.Node
            dendropy.Node object
        name    : string
            Annotation to rename the node with.
        '''
        pass
    def decorate(self, output_tree, output_tax, unique_names):
        '''
        Decorate a tree with taxonomy. This code does not allow inconsistent
        taxonomy within a clade. If one sequence in a clade has a different
        annotation to the rest, it will split the clade. Paraphyletic group
        names are distinguished if unique_names = True using a simple tally of
        each group (see unique_names below).
        Parameters
        ----------
        output_tree        : string
            File to which the decorated tree will be written.
        output_tax         : string
            File to which the taxonomy strings for each tip in the tree will be
            written.
        unique_names       : boolean
            True indicating that a unique number will be appended to the end of
            a taxonomic rank if it is found more than once in the tree
            (i.e. it is paraphyletic in the tree). If false, multiple clades
            may be assigned with the same name.
        '''
        pass",snippet_237,5,"```python
import dendropy
from collections import defaultdict

class TreeDecorator:
    '''
    A class that conservatively decorates trees with taxonomy, or any other
    hierarchical annotation. If all tips descending from a node within the
    provided tree have consistent taxonomy, it will be decorated with that
    taxonomy (or annotation of any type).
    '''
    def __init__(self, tree, taxonomy, seqinfo=None):
        '''
        Parameters
        ----------
        tree        : dendropy.Tree
            dendropy.Tree object
        taxonomy    : string
            Path to a file containing taxonomy information about the tree,
            either in Greengenes or taxtastic format (seqinfo file must also
            be provided if taxonomy is in taxtastic format).
        seqinfo     : string
            Path to a seqinfo file. This is a .csv file with the first column
            denoting the sequence name, and the second column, its most resolved
            taxonomic rank.
        '''
        self.tree = tree
        self.taxonomy_map = {}
        self.seqinfo_map = {}
        # read taxonomy file
        with open(taxonomy, 'r') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                parts = line.split('\t')
                if len(parts) < 2:
                    continue
                seqname, tax = parts[0], parts[1]
                self.taxonomy_map[seqname] = tax
        # read seqinfo file if provided
        if seqinfo:
            with open(seqinfo, 'r') as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    parts = line.split(',')
                    if len(parts) < 2:
                        continue
                    seqname, tax = parts[0], parts[1]
                    self.seqinfo_map[seqname] = tax

    def _write_consensus_strings(self, output):
        '''
        Write taxonomy strings for each tip to the output file.
        '''
        with open(output, 'w') as out:
            for leaf in self.tree.leaf_nodes():
                name = leaf.taxon.label if leaf.taxon else leaf.label
                tax = self.taxonomy_map.get(name, '')
                out.write(f'{name}\t{tax}\n')

    def _rename(self, node, name):
        '''
        Rename an internal node of the tree. If an annotation is already
        present, append the new annotation to the end of it. If a bootstrap
        value is present, add annotations are added after a "":"" as per standard
        newick format.
        Parameters
        ----------
        node: dendropy.Node
            dendropy.Node object
        name    : string
            Annotation to rename the node with.
        '''
        if node.label is None:
            node.label = name
        else:
            # if node already has a label, append with a separator
            node.label = f'{node.label}|{name}'

    def decorate(self, output_tree, output_tax, unique_names):
        '''
        Decorate a tree with taxonomy. This code does not allow inconsistent
        taxonomy within a clade. If one sequence in a clade has a different
        annotation to the rest, it will split the clade. Paraphyletic group
        names are distinguished if unique_names = True using a simple tally of
        each group (see unique_names below).
        Parameters
        ----------
        output_tree        : string
            File to which the decorated tree will be written.
        output_tax         : string
            File to which the taxonomy strings for each tip in the tree will be
            written.
        unique_names       : boolean
            True indicating that a unique number will be appended to the end of
            a taxonomic rank if it is found more than once in the tree
            (i.e. it is paraphyletic in the tree). If false, multiple clades
            may be assigned with the same name.
        '''
        # Count occurrences for unique naming
        tax_counts = defaultdict(int)

        # Helper to get taxonomy for a leaf
        def leaf_tax(leaf):
            name = leaf.taxon.label if leaf.taxon else leaf.label
            return self.taxonomy_map.get(name, '')

        # Postorder traversal to annotate internal nodes
        for node in self.tree.postorder_node_iter():
            if node.is_leaf():
                continue
            # gather taxonomy strings of descendant tips
            taxa = set()
            for leaf in node.leaf_nodes():
                taxa.add(leaf_tax(leaf))
            if len(taxa) == 1:
                tax = taxa.pop()
                if unique_names:
                    tax_counts[tax] += 1
                    if tax_counts[tax] > 1:
                        tax = f'{tax}_{tax_counts[tax]}'
                self._rename(node, tax)

        # Write taxonomy strings for each tip
        self._write_consensus_strings(output_tax)

        # Write decorated tree
        self.tree.write(path=output_tree, schema='newick', suppress_rooting=False,
                        unquoted_underscores=True)
```"
311642,abhishek-ram/pyas2-lib,abhishek-ram_pyas2-lib/pyas2lib/as2.py,pyas2lib.as2.Partner,"from oscrypto import asymmetric
from pyas2lib.exceptions import AS2Exception, DuplicateDocument, ImproperlyConfigured, InsufficientSecurityError, IntegrityError, MDNNotFound, PartnerNotFound
from pyas2lib.utils import canonicalize, extract_first_part, make_mime_boundary, mime_to_bytes, pem_to_der, quote_as2name, split_pem, unquote_as2name, verify_certificate_chain
from pyas2lib.constants import AS2_VERSION, ASYNCHRONOUS_MDN, DIGEST_ALGORITHMS, EDIINT_FEATURES, ENCRYPTION_ALGORITHMS, KEY_ENCRYPTION_ALGORITHMS, MDN_CONFIRM_TEXT, MDN_FAILED_TEXT, MDN_MODES, SIGNATUR_ALGORITHMS, SYNCHRONOUS_MDN
from dataclasses import dataclass

@dataclass
class Partner:
    """"""
    Class represents an AS2 partner and defines the certificates and
    settings to be used when sending and receiving messages.

    :param as2_name: The unique AS2 name for this partner.

    :param verify_cert: A byte string of the certificate to be used for
        verifying signatures of inbound messages and MDNs.

    :param verify_cert_ca: A byte string of the ca certificate if any of
        the verification cert

    :param encrypt_cert: A byte string of the certificate to be used for
        encrypting outbound message.

    :param encrypt_cert_ca: A byte string of the ca certificate if any of
        the encryption cert

    :param validate_certs: Set this flag to `False` to disable validations of
        the encryption and verification certificates. (default `True`)

    :param compress: Set this flag to `True` to compress outgoing
        messages. (default `False`)

    :param sign: Set this flag to `True` to sign outgoing
        messages. (default `False`)

    :param digest_alg: The digest algorithm to be used for generating the
        signature. (default ""sha256"")

    :param encrypt: Set this flag to `True` to encrypt outgoing
        messages. (default `False`)

    :param enc_alg:
        The encryption algorithm to be used. (default `""tripledes_192_cbc""`)

    :param mdn_mode: The mode to be used for receiving the MDN.
        Set to `None` for no MDN, `'SYNC'` for synchronous and `'ASYNC'`
        for asynchronous. (default `None`)

    :param mdn_digest_alg: The digest algorithm to be used by the receiver
        for signing the MDN. Use `None` for unsigned MDN. (default `None`)

    :param mdn_confirm_text: The text to be used in the MDN for successfully
        processed messages received from this partner.

    :param canonicalize_as_binary: force binary canonicalization for this partner

    :param sign_alg: The signing algorithm to be used for generating the
        signature. (default `rsassa_pkcs1v15`)

    :param key_enc_alg: The key encryption algorithm to be used.
        (default `rsaes_pkcs1v15`)

    """"""
    as2_name: str
    verify_cert: bytes = None
    verify_cert_ca: bytes = None
    encrypt_cert: bytes = None
    encrypt_cert_ca: bytes = None
    validate_certs: bool = True
    compress: bool = False
    encrypt: bool = False
    enc_alg: str = 'tripledes_192_cbc'
    sign: bool = False
    digest_alg: str = 'sha256'
    mdn_mode: str = None
    mdn_digest_alg: str = None
    mdn_confirm_text: str = MDN_CONFIRM_TEXT
    ignore_self_signed: bool = True
    canonicalize_as_binary: bool = False
    sign_alg: str = 'rsassa_pkcs1v15'
    key_enc_alg: str = 'rsaes_pkcs1v15'

    def __post_init__(self):
        """"""Run the post initialisation checks for this class.""""""
        if self.digest_alg and self.digest_alg not in DIGEST_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported Digest Algorithm {self.digest_alg}, must be one of {DIGEST_ALGORITHMS}')
        if self.enc_alg and self.enc_alg not in ENCRYPTION_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported Encryption Algorithm {self.enc_alg}, must be one of {ENCRYPTION_ALGORITHMS}')
        if self.mdn_mode and self.mdn_mode not in MDN_MODES:
            raise ImproperlyConfigured(f'Unsupported MDN Mode {self.mdn_mode}, must be one of {MDN_MODES}')
        if self.mdn_digest_alg and self.mdn_digest_alg not in DIGEST_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported MDN Digest Algorithm {self.mdn_digest_alg}, must be one of {DIGEST_ALGORITHMS}')
        if self.sign_alg and self.sign_alg not in SIGNATUR_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported Signature Algorithm {self.sign_alg}, must be one of {SIGNATUR_ALGORITHMS}')
        if self.key_enc_alg and self.key_enc_alg not in KEY_ENCRYPTION_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported Key Encryption Algorithm {self.key_enc_alg}, must be one of {KEY_ENCRYPTION_ALGORITHMS}')

    def load_verify_cert(self):
        """"""Load the verification certificate of the partner and returned the parsed cert.""""""
        if self.validate_certs:
            cert = pem_to_der(self.verify_cert, return_multiple=False)
            if self.verify_cert_ca:
                trust_roots = pem_to_der(self.verify_cert_ca)
            else:
                trust_roots = []
            verify_certificate_chain(cert, trust_roots, ignore_self_signed=self.ignore_self_signed)
        return asymmetric.load_certificate(self.verify_cert)

    def load_encrypt_cert(self):
        """"""Load the encryption certificate of the partner and returned the parsed cert.""""""
        if self.validate_certs:
            cert = pem_to_der(self.encrypt_cert, return_multiple=False)
            if self.encrypt_cert_ca:
                trust_roots = pem_to_der(self.encrypt_cert_ca)
            else:
                trust_roots = []
            verify_certificate_chain(cert, trust_roots, ignore_self_signed=self.ignore_self_signed)
        return asymmetric.load_certificate(self.encrypt_cert)","@dataclass
class Partner:
    '''
    Class represents an AS2 partner and defines the certificates and
    settings to be used when sending and receiving messages.
    :param as2_name: The unique AS2 name for this partner.
    :param verify_cert: A byte string of the certificate to be used for
        verifying signatures of inbound messages and MDNs.
    :param verify_cert_ca: A byte string of the ca certificate if any of
        the verification cert
    :param encrypt_cert: A byte string of the certificate to be used for
        encrypting outbound message.
    :param encrypt_cert_ca: A byte string of the ca certificate if any of
        the encryption cert
    :param validate_certs: Set this flag to `False` to disable validations of
        the encryption and verification certificates. (default `True`)
    :param compress: Set this flag to `True` to compress outgoing
        messages. (default `False`)
    :param sign: Set this flag to `True` to sign outgoing
        messages. (default `False`)
    :param digest_alg: The digest algorithm to be used for generating the
        signature. (default ""sha256"")
    :param encrypt: Set this flag to `True` to encrypt outgoing
        messages. (default `False`)
    :param enc_alg:
        The encryption algorithm to be used. (default `""tripledes_192_cbc""`)
    :param mdn_mode: The mode to be used for receiving the MDN.
        Set to `None` for no MDN, `'SYNC'` for synchronous and `'ASYNC'`
        for asynchronous. (default `None`)
    :param mdn_digest_alg: The digest algorithm to be used by the receiver
        for signing the MDN. Use `None` for unsigned MDN. (default `None`)
    :param mdn_confirm_text: The text to be used in the MDN for successfully
        processed messages received from this partner.
    :param canonicalize_as_binary: force binary canonicalization for this partner
    :param sign_alg: The signing algorithm to be used for generating the
        signature. (default `rsassa_pkcs1v15`)
    :param key_enc_alg: The key encryption algorithm to be used.
        (default `rsaes_pkcs1v15`)
            '''
    def __post_init__(self):
        '''Run the post initialisation checks for this class.'''
        pass
    def load_verify_cert(self):
        '''Load the verification certificate of the partner and returned the parsed cert.'''
        pass
    def load_encrypt_cert(self):
        '''Load the encryption certificate of the partner and returned the parsed cert.'''
        pass",snippet_239,4,"```python
from __future__ import annotations

from dataclasses import dataclass, field
from typing import Optional

from cryptography import x509
from cryptography.hazmat.primitives.serialization import load_pem_public_key
from cryptography.hazmat.backends import default_backend
from cryptography.x509 import load_pem_x509_certificate


@dataclass
class Partner:
    """"""
    Class represents an AS2 partner and defines the certificates and
    settings to be used when sending and receiving messages.
    """"""
    as2_name: str
    verify_cert: Optional[bytes] = None
    verify_cert_ca: Optional[bytes] = None
    encrypt_cert: Optional[bytes] = None
    encrypt_cert_ca: Optional[bytes] = None
    validate_certs: bool = True
    compress: bool = False
    sign: bool = False
    digest_alg: str = ""sha256""
    encrypt: bool = False
    enc_alg: str = ""tripledes_192_cbc""
    mdn_mode: Optional[str] = None
    mdn_digest_alg: Optional[str] = None
    mdn_confirm_text: Optional[str] = None
    canonicalize_as_binary: bool = False
    sign_alg: str = ""rsassa_pkcs1v15""
    key_enc_alg: str = ""rsaes_pkcs1v15""

    def __post_init__(self) -> None:
        """"""Run the post initialisation checks for this class.""""""
        if not isinstance(self.as2_name, str) or not self.as2_name:
            raise ValueError(""as2_name must be a non-empty string"")

        # Validate certificate fields
        for name, cert in (
            (""verify_cert"", self.verify_cert),
            (""verify_cert_ca"", self.verify_cert_ca),
            (""encrypt_cert"", self.encrypt_cert),
            (""encrypt_cert_ca"", self.encrypt_cert_ca),
        ):
            if cert is not None and not isinstance(cert, (bytes, bytearray)):
                raise TypeError(f""{name} must be bytes or None"")

        # If encryption is requested, an encryption cert must be supplied
        if self.encrypt and self.encrypt_cert is None:
            raise ValueError(""encrypt is True but encrypt_cert is not provided"")

        # Validate mdn_mode
        if self.mdn_mode not in (None, ""SYNC"", ""ASYNC""):
            raise ValueError(""mdn_mode must be None, 'SYNC', or 'ASYNC'"")

        # Validate mdn_digest_alg
        if self.mdn_digest_alg is not None and not isinstance(self.mdn_digest_alg, str):
            raise TypeError(""mdn_digest_alg must be a string or None"")

        # Validate digest algorithm (basic check)
        if not isinstance(self.digest_alg, str):
            raise TypeError(""digest_alg must be a string"")

        # Validate encryption algorithm
        if not isinstance(self.enc_alg, str):
            raise TypeError(""enc_alg must be a string"")

        # Validate sign algorithm
        if not isinstance(self.sign_alg, str):
            raise TypeError(""sign_alg must be a string"")

        # Validate key encryption algorithm
        if not isinstance(self.key_enc_alg, str):
            raise TypeError(""key_enc_alg must be a string"")

    def load_verify_cert(self) -> Optional[x509.Certificate]:
        """"""Load the verification certificate of the partner and return the parsed cert.""""""
        if self.verify_cert is None:
            return None
        try:
            cert = load_pem_x509_certificate(self.verify_cert, default_backend())
        except Exception as exc:
            raise ValueError(""Failed to load verify_cert"") from exc
        return cert

    def load_encrypt_cert(self) -> Optional[x509.Certificate]:
        """"""Load the encryption certificate of the partner and return the parsed cert.""""""
        if self.encrypt_cert is None:
            return None
        try:
            cert = load_pem_x509_certificate(self.encrypt_cert, default_backend())
        except Exception as exc:
            raise ValueError(""Failed to load encrypt_cert"") from exc
        return cert
```"
311862,reportportal/client-Python,reportportal_client-Python/reportportal_client/core/rp_issues.py,reportportal_client.core.rp_issues.ExternalIssue,"class ExternalIssue:
    """"""This class represents external(BTS) system issue.""""""

    def __init__(self, bts_url=None, bts_project=None, submit_date=None, ticket_id=None, url=None):
        """"""Initialize instance attributes.

        :param bts_url:     Bug tracker system URL
        :param bts_project: Bug tracker system project
        :param submit_date: Bug submission date
        :param ticket_id:   Unique ID of the ticket at the BTS
        :param url:         URL to the ticket(bug)
        """"""
        self.bts_url = bts_url
        self.bts_project = bts_project
        self.submit_date = submit_date
        self.ticket_id = ticket_id
        self.url = url

    @property
    def payload(self):
        """"""Form the correct dictionary for the BTS issue.""""""
        return {'btsUrl': self.bts_url, 'btsProject': self.bts_project, 'submitDate': self.submit_date, 'ticketId': self.ticket_id, 'url': self.url}","class ExternalIssue:
    '''This class represents external(BTS) system issue.'''
    def __init__(self, bts_url=None, bts_project=None, submit_date=None, ticket_id=None, url=None):
        
        pass
    @property
    def payload(self):
        
        pass",snippet_240,3,"class ExternalIssue:
    '''This class represents external(BTS) system issue.'''
    def __init__(self, bts_url=None, bts_project=None, submit_date=None, ticket_id=None, url=None):
        self.bts_url = bts_url
        self.bts_project = bts_project
        self.submit_date = submit_date
        self.ticket_id = ticket_id
        self.url = url

    @property
    def payload(self):
        return {
            'bts_url': self.bts_url,
            'bts_project': self.bts_project,
            'submit_date': self.submit_date,
            'ticket_id': self.ticket_id,
            'url': self.url
        }"
311863,reportportal/client-Python,reportportal_client-Python/reportportal_client/core/rp_issues.py,reportportal_client.core.rp_issues.Issue,"class Issue:
    """"""This class represents an issue that can be attached to test result.""""""

    def __init__(self, issue_type, comment=None, auto_analyzed=False, ignore_analyzer=True):
        """"""Initialize instance attributes.

        :param issue_type:      Issue type locator. Allowable values: ""pb***"",
                                ""ab***"", ""si***"", ""ti***"", ""nd001"". Where ***
                                is locator id.
        :param comment:         Issue comments
        :param auto_analyzed:   Indicator that the issue has been marked with
                                the RP auto analyzer
        :param ignore_analyzer: Flag that forces RP analyzer to ignore this
                                issue
        """"""
        self._external_issues = []
        self.auto_analyzed = auto_analyzed
        self.comment = comment
        self.ignore_analyzer = ignore_analyzer
        self.issue_type = issue_type

    def external_issue_add(self, issue):
        """"""Add external system issue to the issue.""""""
        self._external_issues.append(issue.payload)

    @property
    def payload(self):
        """"""Form the correct dictionary for the issue.""""""
        return {'autoAnalyzed': self.auto_analyzed, 'comment': self.comment, 'externalSystemIssues': self._external_issues, 'ignoreAnalyzer': self.ignore_analyzer, 'issueType': self.issue_type}","class Issue:
    
    def __init__(self, issue_type, comment=None, auto_analyzed=False, ignore_analyzer=True):
        '''Initialize instance attributes.
        :param issue_type:      Issue type locator. Allowable values: ""pb***"",
                                ""ab***"", ""si***"", ""ti***"", ""nd001"". Where ***
                                is locator id.
        :param comment:         Issue comments
        :param auto_analyzed:   Indicator that the issue has been marked with
                                the RP auto analyzer
        :param ignore_analyzer: Flag that forces RP analyzer to ignore this
                                issue
        '''
        pass
    def external_issue_add(self, issue):
        
        pass
    @property
    def payload(self):
        '''Form the correct dictionary for the issue.'''
        pass",snippet_241,4,"```python
class Issue:
    """"""Represents an issue with optional external data and payload generation.""""""

    _VALID_PREFIXES = (""pb"", ""ab"", ""si"", ""ti"")
    _VALID_EXACT = (""nd001"",)

    def __init__(self, issue_type, comment=None, auto_analyzed=False, ignore_analyzer=True):
        """"""
        Initialize instance attributes.

        :param issue_type: Issue type locator. Allowable values: ""pb***"",
                           ""ab***"", ""si***"", ""ti***"", ""nd001"". Where *** is locator id.
        :param comment: Issue comments
        :param auto_analyzed: Indicator that the issue has been marked with
                              the RP auto analyzer
        :param ignore_analyzer: Flag that forces RP analyzer to ignore this
                                issue
        """"""
        if not isinstance(issue_type, str):
            raise TypeError(""issue_type must be a string"")

        # Validate issue_type
        if issue_type in self._VALID_EXACT:
            pass
        else:
            prefix = issue_type[:2]
            if prefix not in self._VALID_PREFIXES:
                raise ValueError(
                    f""Invalid issue_type prefix '{prefix}'. ""
                    f""Expected one of {self._VALID_PREFIXES} or 'nd001'.""
                )
            if len(issue_type) <= 2:
                raise ValueError(
                    f""issue_type '{issue_type}' must contain an id after the prefix.""
                )

        self.issue_type = issue_type
        self.comment = comment
        self.auto_analyzed = bool(auto_analyzed)
        self.ignore_analyzer = bool(ignore_analyzer)
        self._external_issue = None

    def external_issue_add(self, issue):
        """"""
        Attach an external issue representation.

        :param issue: Any object representing an external issue.
        """"""
        self._external_issue = issue

    @property
    def payload(self):
        """"""
        Form the correct dictionary for the issue.

        :return: dict containing issue data ready for serialization.
        """"""
        data = {
            ""issue_type"": self.issue_type,
            ""auto_analyzed"": self.auto_analyzed,
            ""ignore_analyzer"": self.ignore_analyzer,
        }
        if self.comment is not None:
            data[""comment""] = self.comment
        if self._external_issue is not None:
            data[""external_issue""] = self._external_issue
        return data
```"
312030,SuperCowPowers/workbench,aws_setup/aws_identity_check.py,aws_identity_check.AWSIdentityCheck,"from workbench.core.cloud_platform.aws.aws_account_clamp import AWSAccountClamp
from workbench.utils.config_manager import ConfigManager
import sys
import logging

class AWSIdentityCheck:
    """"""Just a Utility Script that allows people to check which AWS Identity is active""""""

    def __init__(self):
        """"""AWSIdentityCheck Initialization""""""
        self.log = logging.getLogger('workbench')
        self.aws_clamp = AWSAccountClamp()

    def check(self):
        """"""Check the AWS Identity""""""
        cm = ConfigManager()
        active_profile = cm.get_config('AWS_PROFILE')
        if active_profile:
            self.log.info(f'Workbench AWS_PROFILE: {active_profile}')
        else:
            self.log.info('No AWS_PROFILE set')
            sys.exit(0)
        self.log.info('\n\n*** Caller/Base Identity Check ***')
        self.aws_clamp.check_aws_identity()
        self.log.info('Caller/Base Identity Check Success...')
        self.log.info('\n\n*** AWS Assumed Role Check ***')
        self.aws_clamp.check_assumed_role()
        self.log.info('Assumed Role Check Success...')","
class AWSIdentityCheck:
    '''Just a Utility Script that allows people to check which AWS Identity is active'''
    def __init__(self):
        
        pass
    def check(self):
        '''Check the AWS Identity'''
        pass",snippet_242,3,"```python
import boto3
import botocore.exceptions


class AWSIdentityCheck:
    """"""Utility class to determine the currently active AWS identity.

    The class uses the AWS Security Token Service (STS) to retrieve the
    caller identity.  It works with any credentials that are available
    in the environment (environment variables, shared credentials file,
    instance profile, etc.).

    Example
    -------
    >>> checker = AWSIdentityCheck()
    >>> identity = checker.check()
    >>> print(identity)
    {'Account': '123456789012', 'Arn': 'arn:aws:iam::123456789012:user/JohnDoe', 'UserId': 'AIDAEXAMPLE'}
    """"""

    def __init__(self):
        """"""
        Create an STS client using the default session.
        """"""
        self._sts = boto3.client(""sts"")

    def check(self):
        """"""
        Retrieve the current AWS identity.

        Returns
        -------
        dict
            A dictionary containing the keys 'Account', 'Arn', and 'UserId'.

        Raises
        ------
        RuntimeError
            If the STS call fails or the credentials are not available.
        """"""
        try:
            response = self._sts.get_caller_identity()
        except botocore.exceptions.BotoCoreError as exc:
            raise RuntimeError(f""Failed to retrieve AWS identity: {exc}"") from exc

        # The response contains 'Account', 'Arn', and 'UserId'.
        return {
            ""Account"": response.get(""Account""),
            ""Arn"": response.get(""Arn""),
            ""UserId"": response.get(""UserId""),
        }
```"
312949,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/factory/source/element.py,apidoc.factory.source.element.Element,"from apidoc.lib.util.cast import to_boolean
import collections
from apidoc.object.source_raw import Sampleable, Displayable

class Element:
    """""" Populate Helper Factory
    """"""

    def set_common_datas(self, element, name, datas):
        """"""Populated common data for an element from dictionnary datas
        """"""
        element.name = str(name)
        if 'description' in datas:
            element.description = str(datas['description']).strip()
        if isinstance(element, Sampleable) and element.sample is None and ('sample' in datas):
            element.sample = str(datas['sample']).strip()
        if isinstance(element, Displayable):
            if 'display' in datas:
                element.display = to_boolean(datas['display'])
            if 'label' in datas:
                element.label = datas['label']
            else:
                element.label = element.name

    def create_dictionary_of_element_from_dictionary(self, property_name, datas):
        """"""Populate a dictionary of elements
        """"""
        response = {}
        if property_name in datas and datas[property_name] is not None and isinstance(datas[property_name], collections.Iterable):
            for key, value in datas[property_name].items():
                response[key] = self.create_from_name_and_dictionary(key, value)
        return response

    def create_list_of_element_from_dictionary(self, property_name, datas):
        """"""Populate a list of elements
        """"""
        response = []
        if property_name in datas and datas[property_name] is not None and isinstance(datas[property_name], list):
            for value in datas[property_name]:
                response.append(self.create_from_dictionary(value))
        return response

    def get_enum(self, property, enum, datas):
        """"""Factory enum type
        """"""
        str_property = str(datas[property]).lower()
        if str_property not in enum:
            raise ValueError('Unknow enum ""%s"" for ""%s"".' % (str_property, property))
        return enum(str_property)","
class Element:
    
    def set_common_datas(self, element, name, datas):
        
        pass
    def create_dictionary_of_element_from_dictionary(self, property_name, datas):
        '''Populate a dictionary of elements
        '''
        pass
    def create_list_of_element_from_dictionary(self, property_name, datas):
        '''Populate a list of elements
        '''
        pass
    def get_enum(self, property, enum, datas):
        '''Factory enum type
        '''
        pass",snippet_244,5,"```python
from typing import Any, Dict, List, Optional, Type, TypeVar
from enum import Enum

T = TypeVar('T')


class Element:
    """"""
    Utility class providing helper methods for populating objects from dictionaries.
    """"""

    def set_common_datas(self, element: Any, name: str, datas: Dict[str, Any]) -> None:
        """"""
        Set attributes on *element* based on the keys present in *datas*.
        If *name* is a key in *datas*, the corresponding value is assigned to
        the attribute of the same name on *element*.
        Additionally, any key in *datas* that matches an existing attribute
        on *element* will be set.

        Parameters
        ----------
        element : Any
            The object whose attributes will be set.
        name : str
            The primary key to look for in *datas*.
        datas : Dict[str, Any]
            Dictionary containing data to populate *element*.
        """"""
        # Set the primary attribute if present
        if name in datas:
            setattr(element, name, datas[name])

        # Set any other matching attributes
        for key, value in datas.items():
            if hasattr(element, key):
                setattr(element, key, value)

    def create_dictionary_of_element_from_dictionary(
        self,
        property_name: str,
        datas: Dict[str, Any]
    ) -> Dict[str, Any]:
        """"""
        Retrieve a dictionary from *datas* under *property_name*.
        If the property is missing or not a dictionary, an empty dictionary
        is returned.

        Parameters
        ----------
        property_name : str
            Key in *datas* that should contain a dictionary.
        datas : Dict[str, Any]
            Source dictionary.

        Returns
        -------
        Dict[str, Any]
            The dictionary found under *property_name*, or an empty dict.
        """"""
        value = datas.get(property_name)
        if isinstance(value, dict):
            return value
        return {}

    def create_list_of_element_from_dictionary(
        self,
        property_name: str,
        datas: Dict[str, Any]
    ) -> List[Any]:
        """"""
        Retrieve a list from *datas* under *property_name*.
        If the property is missing or not a list, an empty list is returned.

        Parameters
        ----------
        property_name : str
            Key in *datas* that should contain a list.
        datas : Dict[str, Any]
            Source dictionary.

        Returns
        -------
        List[Any]
            The list found under *property_name*, or an empty list.
        """"""
        value = datas.get(property_name)
        if isinstance(value, list):
            return value
        return []

    def get_enum(
        self,
        property: str,
        enum: Type[Enum],
        datas: Dict[str, Any]
    ) -> Optional[Enum]:
        """"""
        Convert a value from *datas* into an enum member.

        Parameters
        ----------
        property : str
            Key in *datas* whose value should be converted.
        enum : Type[Enum]
            The Enum class to instantiate.
        datas : Dict[str, Any]
            Source dictionary.

        Returns
        -------
        Optional[Enum]
            The enum member corresponding to the value, or None if the key
            is missing or the value is not a valid enum member.
        """"""
        if property not in datas:
            return None

        value = datas[property]
        try:
            return enum(value)
        except (ValueError, TypeError):
            return None
```"
313013,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/object/source_raw.py,apidoc.object.source_raw.Sampleable,"class Sampleable:
    """"""Element who can provide samples
    """"""

    def __init__(self):
        """"""Class instantiation
        """"""
        super().__init__()
        self.sample = None

    def get_sample(self):
        """"""Return the a sample for the element
        """"""
        if self.sample is None:
            return self.get_default_sample()
        return self.sample

    def get_default_sample(self):
        """"""Return default value for the element
        """"""
        return 'my_%s' % self.name","class Sampleable:
    '''Element who can provide samples
    '''
    def __init__(self):
        
        pass
    def get_sample(self):
        '''Return the a sample for the element
        '''
        pass
    def get_default_sample(self):
        '''Return default value for the element
        '''
        pass",snippet_245,4,"```python
class Sampleable:
    '''Element who can provide samples'''

    def __init__(self, sample=None):
        """"""
        Initialize a Sampleable instance.

        Parameters
        ----------
        sample : Any, optional
            A pre‑defined sample value. If not provided, the instance will
            fall back to the default sample defined by :meth:`get_default_sample`.
        """"""
        self._sample = sample

    def get_sample(self):
        """"""
        Return the current sample for the element.

        If a sample was supplied during construction, that value is returned.
        Otherwise, the default sample defined by :meth:`get_default_sample` is
        returned.

        Returns
        -------
        Any
            The sample value.
        """"""
        if self._sample is not None:
            return self._sample
        return self.get_default_sample()

    def get_default_sample(self):
        """"""
        Return the default value for the element.

        Subclasses may override this method to provide a more meaningful
        default. The base implementation returns ``None``.

        Returns
        -------
        Any
            The default sample value.
        """"""
        return None
```"
313027,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/service/config.py,apidoc.service.config.Config,"import os
from apidoc.object.config import Config as ConfigObject

class Config:
    """"""Provide tool to managed config
    """"""

    def validate(self, config):
        """"""Validate that the source file is ok
        """"""
        if not isinstance(config, ConfigObject):
            raise Exception('Config object expected')
        if config['output']['componants'] not in ('local', 'remote', 'embedded', 'without'):
            raise ValueError('Unknown componant ""%s"".' % config['output']['componants'])
        if config['output']['layout'] not in ('default', 'content-only'):
            raise ValueError('Unknown layout ""%s"".' % config['output']['layout'])
        if config['input']['locations'] is not None:
            unknown_locations = [x for x in config['input']['locations'] if not os.path.exists(x)]
            if len(unknown_locations) > 0:
                raise ValueError('Location%s ""%s"" does not exists' % ('s' if len(unknown_locations) > 1 else '', '"" and ""'.join(unknown_locations)))
            config['input']['locations'] = [os.path.realpath(x) for x in config['input']['locations']]
        if config['input']['arguments'] is not None:
            if not isinstance(config['input']['arguments'], dict):
                raise ValueError('Sources arguments ""%s"" are not a dict' % config['input']['arguments'])

    def get_template_from_config(self, config):
        """"""Retrieve a template path from the config object
        """"""
        if config['output']['template'] == 'default':
            return os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'template', 'default.html')
        else:
            return os.path.abspath(config['output']['template'])","
class Config:
    '''Provide tool to managed config
    '''
    def validate(self, config):
        
        pass
    def get_template_from_config(self, config):
        '''Retrieve a template path from the config object
        '''
        pass",snippet_246,3,"```python
import os
from pathlib import Path


class Config:
    """"""Provide tool to manage configuration.""""""

    def validate(self, config):
        """"""
        Validate the configuration object.

        Parameters
        ----------
        config : dict
            Configuration dictionary that must contain a 'template' key.

        Raises
        ------
        TypeError
            If `config` is not a dictionary or the template value is not a string.
        KeyError
            If the 'template' key is missing.
        FileNotFoundError
            If the template file does not exist.
        """"""
        if not isinstance(config, dict):
            raise TypeError(""config must be a dictionary"")

        if ""template"" not in config:
            raise KeyError(""config missing required 'template' key"")

        template = config[""template""]
        if not isinstance(template, str):
            raise TypeError(""'template' value must be a string"")

        # Resolve the path to an absolute path
        template_path = Path(template).expanduser().resolve()
        if not template_path.is_file():
            raise FileNotFoundError(f""Template file '{template_path}' does not exist"")

        # Store the resolved path back into the config for consistency
        config[""template""] = str(template_path)

        return True

    def get_template_from_config(self, config):
        """"""
        Retrieve the template path from the configuration object.

        Parameters
        ----------
        config : dict
            Configuration dictionary that must contain a 'template' key.

        Returns
        -------
        str
            Absolute path to the template file.

        Raises
        ------
        Exception
            Propagated from :meth:`validate` if validation fails.
        """"""
        self.validate(config)
        return config[""template""]
```"
313029,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/service/merger.py,apidoc.service.merger.Merger,"from apidoc.lib.util.cast import to_boolean

class Merger:
    """"""Provide tool to merge elements
    """"""

    def merge_extends(self, target, extends, inherit_key='inherit', inherit=False):
        """"""Merge extended dicts
        """"""
        if isinstance(target, dict):
            if inherit and inherit_key in target and (not to_boolean(target[inherit_key])):
                return
            if not isinstance(extends, dict):
                raise ValueError('Unable to merge: Dictionnary expected')
            for key in extends:
                if key not in target:
                    target[str(key)] = extends[key]
                else:
                    self.merge_extends(target[key], extends[key], inherit_key, True)
        elif isinstance(target, list):
            if not isinstance(extends, list):
                raise ValueError('Unable to merge: List expected')
            target += extends

    def merge_sources(self, datas):
        """"""Merge sources files
        """"""
        datas = [data for data in datas if data is not None]
        if len(datas) == 0:
            raise ValueError('Data missing')
        if len(datas) == 1:
            return datas[0]
        if isinstance(datas[0], list):
            if len([x for x in datas if not isinstance(x, list)]) > 0:
                raise TypeError('Unable to merge: List expected')
            base = []
            for x in datas:
                base = base + x
            return base
        if isinstance(datas[0], dict):
            if len([x for x in datas if not isinstance(x, dict)]) > 0:
                raise TypeError('Unable to merge: Dictionnary expected')
            result = {}
            for element in datas:
                for key in element:
                    if key in result:
                        result[key] = self.merge_sources([result[key], element[key]])
                    else:
                        result[key] = element[key]
            return result
        if len([x for x in datas if isinstance(x, (dict, list))]) > 0:
            raise TypeError('Unable to merge: List not expected')
        raise ValueError('Unable to merge: Conflict')

    def merge_configs(self, config, datas):
        """"""Merge configs files
        """"""
        if not isinstance(config, dict) or len([x for x in datas if not isinstance(x, dict)]) > 0:
            raise TypeError('Unable to merge: Dictionnary expected')
        for key, value in config.items():
            others = [x[key] for x in datas if key in x]
            if len(others) > 0:
                if isinstance(value, dict):
                    config[key] = self.merge_configs(value, others)
                else:
                    config[key] = others[-1]
        return config","
class Merger:
    
    def merge_extends(self, target, extends, inherit_key='inherit', inherit=False):
        
        pass
    def merge_sources(self, datas):
        
        pass
    def merge_configs(self, config, datas):
        '''Merge configs files
        '''
        pass",snippet_247,4,"```python
class Merger:
    def _deep_merge(self, target, source, inherit_key='inherit', inherit=False):
        """"""
        Recursively merge `source` into `target`. If `inherit` is True, nested
        dictionaries are merged; otherwise, nested dictionaries are replaced.
        """"""
        for key, value in source.items():
            if key == inherit_key:
                continue
            if key in target:
                if isinstance(target[key], dict) and isinstance(value, dict):
                    if inherit:
                        self._deep_merge(target[key], value, inherit_key, inherit)
                    else:
                        target[key] = value
                else:
                    target[key] = value
            else:
                target[key] = value

    def merge_extends(self, target, extends, inherit_key='inherit', inherit=False):
        """"""
        Merge a list of dictionaries (`extends`) into `target`. If a dictionary
        contains the key `inherit_key` set to a truthy value, its contents are
        merged into `target`. The `inherit` flag controls whether nested
        dictionaries are merged recursively.
        """"""
        if not isinstance(extends, (list, tuple)):
            extends = [extends]
        for src in extends:
            if not isinstance(src, dict):
                continue
            if src.get(inherit_key):
                self._deep_merge(target, src, inherit_key, inherit)
            else:
                # If not inheriting, simply overwrite or add keys
                for k, v in src.items():
                    if k != inherit_key:
                        target[k] = v
        return target

    def merge_sources(self, datas):
        """"""
        Merge a sequence of dictionaries into a single dictionary. The merge
        is shallow; nested dictionaries are replaced.
        """"""
        result = {}
        for data in datas:
            if isinstance(data, dict):
                self.merge_extends(result, data, inherit=False)
        return result

    def merge_configs(self, config, datas):
        """"""
        Merge a configuration dictionary into each dictionary in `datas`.
        The merge is deep (recursive) and respects the `inherit` flag.
        Returns a list of merged dictionaries.
        """"""
        merged = []
        for data in datas:
            if not isinstance(data, dict):
                continue
            # Create a copy to avoid mutating the original
            copy = dict(data)
            self.merge_extends(copy, config, inherit_key='inherit', inherit=True)
            merged.append(copy)
        return merged
```"
313030,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/service/parser.py,apidoc.service.parser.Parser,"import os
import yaml
import json

class Parser:
    """"""Provide tools to parse files
    """"""

    def load_from_file(self, file_path, format=None):
        """"""Return dict from a file config
        """"""
        if format is None:
            base_name, file_extension = os.path.splitext(file_path)
            if file_extension in ('.yaml', '.yml'):
                format = 'yaml'
            elif file_extension in '.json':
                format = 'json'
            else:
                raise ValueError('Config file ""%s"" undetermined' % file_extension)
        if format == 'yaml':
            return yaml.load(open(file_path), Loader=yaml.CSafeLoader if yaml.__with_libyaml__ else yaml.SafeLoader)
        elif format == 'json':
            return json.load(open(file_path))
        else:
            raise ValueError('Format ""%s"" unknwon' % format)

    def load_all_from_directory(self, directory_path):
        """"""Return a list of dict from a directory containing files
        """"""
        datas = []
        for root, folders, files in os.walk(directory_path):
            for f in files:
                datas.append(self.load_from_file(os.path.join(root, f)))
        return datas","
class Parser:
    
    def load_from_file(self, file_path, format=None):
        
        pass
    def load_all_from_directory(self, directory_path):
        '''Return a list of dict from a directory containing files
        '''
        pass",snippet_248,3,"```python
import json
import csv
import os
from pathlib import Path

try:
    import yaml
except ImportError:  # pragma: no cover
    yaml = None


class Parser:
    """"""Utility class for loading data from files and directories.""""""

    _SUPPORTED_EXTENSIONS = {
        "".json"": ""json"",
        "".yaml"": ""yaml"",
        "".yml"": ""yaml"",
        "".csv"": ""csv"",
    }

    def load_from_file(self, file_path, format=None):
        """"""
        Load a single file and return its contents as a Python object.

        Parameters
        ----------
        file_path : str | Path
            Path to the file to load.
        format : str, optional
            Explicit format to use ('json', 'yaml', 'csv'). If None, the format
            is inferred from the file extension.

        Returns
        -------
        dict | list
            Parsed data. For JSON and YAML the result is a dict (or list if
            the top-level JSON/YAML structure is a list). For CSV the result
            is a list of dicts, one per row.

        Raises
        ------
        FileNotFoundError
            If the file does not exist.
        ValueError
            If the format is unsupported or cannot be inferred.
        """"""
        path = Path(file_path)
        if not path.is_file():
            raise FileNotFoundError(f""File not found: {file_path}"")

        # Determine format
        if format is None:
            ext = path.suffix.lower()
            format = self._SUPPORTED_EXTENSIONS.get(ext)
            if format is None:
                raise ValueError(f""Cannot infer format from extension '{ext}'. ""
                                 f""Supported extensions: {list(self._SUPPORTED_EXTENSIONS.keys())}"")

        # Load based on format
        if format == ""json"":
            with path.open(""r"", encoding=""utf-8"") as f:
                return json.load(f)

        if format == ""yaml"":
            if yaml is None:
                raise ImportError(""PyYAML is required for YAML support"")
            with path.open(""r"", encoding=""utf-8"") as f:
                return yaml.safe_load(f)

        if format == ""csv"":
            with path.open(""r"", encoding=""utf-8"", newline="""") as f:
                reader = csv.DictReader(f)
                return [row for row in reader]

        raise ValueError(f""Unsupported format: {format}"")

    def load_all_from_directory(self, directory_path):
        """"""
        Load all supported files from a directory and return a list of parsed
        objects.

        Parameters
        ----------
        directory_path : str | Path
            Path to the directory containing files to load.

        Returns
        -------
        list
            A list of parsed objects (dicts or lists) corresponding to each
            file in the directory.

        Raises
        ------
        FileNotFoundError
            If the directory does not exist.
        """"""
        dir_path = Path(directory_path)
        if not dir_path.is_dir():
            raise FileNotFoundError(f""Directory not found: {directory_path}"")

        results = []
        for entry in dir_path.iterdir():
            if entry.is_file() and entry.suffix.lower() in self._SUPPORTED_EXTENSIONS:
                try:
                    data = self.load_from_file(entry)
                    results.append(data)
                except Exception as exc:
                    # Skip files that cannot be parsed; optionally log the error
                    # For now, we simply ignore them.
                    pass
        return results
```"
313032,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/service/template.py,apidoc.service.template.Template,"import sys
import logging
import shutil
import os

class Template:
    """"""Provide tool to managed templates
    """"""

    def __init__(self):
        """"""Class instantiation
        """"""
        self.input = 'default.html'
        self.output = 'stdout'
        self.env = None

    def render(self, sources, config, out=sys.stdout):
        """"""Render the documentation as defined in config Object
        """"""
        logger = logging.getLogger()
        template = self.env.get_template(self.input)
        output = template.render(sources=sources, layout=config['output']['layout'], config=config['output'])
        if self.output == 'stdout':
            out.write(output)
        else:
            dir = os.path.dirname(self.output)
            if dir and (not os.path.exists(dir)):
                try:
                    os.makedirs(dir)
                except IOError as ioerror:
                    logger.error('Error on creating dir ""{}"": {}'.format(dir, str(ioerror)))
                    return
            if config['output']['template'] == 'default':
                if config['output']['componants'] == 'local':
                    for template_dir in self.env.loader.searchpath:
                        files = (os.path.join(template_dir, 'resource', 'js', 'combined.js'), os.path.join(template_dir, 'resource', 'css', 'combined.css'), os.path.join(template_dir, 'resource', 'font', 'apidoc.eot'), os.path.join(template_dir, 'resource', 'font', 'apidoc.woff'), os.path.join(template_dir, 'resource', 'font', 'apidoc.ttf'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.eot'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.woff'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.ttf'))
                        for file in files:
                            filename = os.path.basename(file)
                            dirname = os.path.basename(os.path.dirname(file))
                            if not os.path.exists(os.path.join(dir, dirname)):
                                os.makedirs(os.path.join(dir, dirname))
                            if os.path.exists(file):
                                shutil.copyfile(file, os.path.join(dir, dirname, filename))
                            else:
                                logger.warn('Missing resource file ""%s"". If you run apidoc in virtualenv, run ""%s""' % (filename, 'python setup.py resources'))
                if config['output']['componants'] == 'remote':
                    for template_dir in self.env.loader.searchpath:
                        files = (os.path.join(template_dir, 'resource', 'js', 'combined.js'), os.path.join(template_dir, 'resource', 'css', 'combined-embedded.css'), os.path.join(template_dir, 'resource', 'font', 'apidoc.eot'), os.path.join(template_dir, 'resource', 'font', 'apidoc.woff'), os.path.join(template_dir, 'resource', 'font', 'apidoc.ttf'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.eot'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.woff'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.ttf'))
                        for file in files:
                            filename = os.path.basename(file)
                            dirname = os.path.basename(os.path.dirname(file))
                            if not os.path.exists(os.path.join(dir, dirname)):
                                os.makedirs(os.path.join(dir, dirname))
                            if os.path.exists(file):
                                shutil.copyfile(file, os.path.join(dir, dirname, filename))
                            else:
                                logger.warn('Missing resource file ""%s"". If you run apidoc in virtualenv, run ""%s""' % (filename, 'python setup.py resources'))
            open(self.output, 'w').write(output)","
class Template:
    
    def __init__(self):
        '''Class instantiation
        '''
        pass
    def render(self, sources, config, out=sys.stdout):
        '''Render the documentation as defined in config Object
        '''
        pass",snippet_249,3,"```python
import sys
import string
import os

class Template:
    def __init__(self):
        '''Class instantiation'''
        pass

    def render(self, sources, config, out=sys.stdout):
        '''Render the documentation as defined in config Object'''
        # Determine template string from config
        tmpl_str = None
        tmpl_file = None

        if isinstance(config, dict):
            tmpl_str = config.get('template')
            tmpl_file = config.get('template_file')
        else:
            tmpl_str = getattr(config, 'template', None)
            tmpl_file = getattr(config, 'template_file', None)

        if tmpl_file:
            if not os.path.isfile(tmpl_file):
                raise FileNotFoundError(f""Template file not found: {tmpl_file}"")
            with open(tmpl_file, 'r', encoding='utf-8') as f:
                tmpl_str = f.read()

        if tmpl_str is None:
            raise ValueError(""No template provided in config"")

        tmpl = string.Template(tmpl_str)

        try:
            rendered = tmpl.substitute(sources)
        except KeyError as e:
            raise KeyError(f""Missing key in sources for template: {e}"") from e

        out.write(rendered)
```"
313488,adafruit/Adafruit_CircuitPython_framebuf,adafruit_Adafruit_CircuitPython_framebuf/adafruit_framebuf.py,adafruit_framebuf.MHMSBFormat,"class MHMSBFormat:
    """"""MHMSBFormat""""""

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        """"""Set a given pixel to a color.""""""
        index = (y * framebuf.stride + x) // 8
        offset = 7 - x & 7
        framebuf.buf[index] = framebuf.buf[index] & ~(1 << offset) | (color != 0) << offset

    @staticmethod
    def get_pixel(framebuf, x, y):
        """"""Get the color of a given pixel""""""
        index = (y * framebuf.stride + x) // 8
        offset = 7 - x & 7
        return framebuf.buf[index] >> offset & 1

    @staticmethod
    def fill(framebuf, color):
        """"""completely fill/clear the buffer with a color""""""
        if color:
            fill = 255
        else:
            fill = 0
        for i in range(len(framebuf.buf)):
            framebuf.buf[i] = fill

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        """"""Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.""""""
        for _x in range(x, x + width):
            offset = 7 - _x & 7
            for _y in range(y, y + height):
                index = (_y * framebuf.stride + _x) // 8
                framebuf.buf[index] = framebuf.buf[index] & ~(1 << offset) | (color != 0) << offset","class MHMSBFormat:
    
    @staticmethod
    def set_pixel(framebuf, x, y, color):
        '''Set a given pixel to a color.'''
        pass
    @staticmethod
    def get_pixel(framebuf, x, y):
        
        pass
    @staticmethod
    def fill(framebuf, color):
        '''completely fill/clear the buffer with a color'''
        pass
    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        '''Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.'''
        pass",snippet_250,5,"```python
class MHMSBFormat:
    @staticmethod
    def _get_buf(framebuf):
        if isinstance(framebuf, dict):
            return framebuf[""buf""]
        if hasattr(framebuf, ""buf""):
            return framebuf.buf
        return framebuf

    @staticmethod
    def _get_width(framebuf):
        if isinstance(framebuf, dict):
            return framebuf[""width""]
        if hasattr(framebuf, ""width""):
            return framebuf.width
        raise TypeError(""framebuf must provide width"")

    @staticmethod
    def _get_height(framebuf):
        if isinstance(framebuf, dict):
            return framebuf[""height""]
        if hasattr(framebuf, ""height""):
            return framebuf.height
        raise TypeError(""framebuf must provide height"")

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        """"""Set a given pixel to a color.""""""
        width = MHMSBFormat._get_width(framebuf)
        height = MHMSBFormat._get_height(framebuf)
        if not (0 <= x < width and 0 <= y < height):
            return
        buf = MHMSBFormat._get_buf(framebuf)
        idx = y * width + x
        byte_index = idx >> 3
        bit_index = 7 - (idx & 7)
        mask = 1 << bit_index
        if color:
            buf[byte_index] |= mask
        else:
            buf[byte_index] &= ~mask

    @staticmethod
    def get_pixel(framebuf, x, y):
        """"""Return the pixel value (0 or 1).""""""
        width = MHMSBFormat._get_width(framebuf)
        height = MHMSBFormat._get_height(framebuf)
        if not (0 <= x < width and 0 <= y < height):
            return 0
        buf = MHMSBFormat._get_buf(framebuf)
        idx = y * width + x
        byte_index = idx >> 3
        bit_index = 7 - (idx & 7)
        return (buf[byte_index] >> bit_index) & 1

    @staticmethod
    def fill(framebuf, color):
        """"""completely fill/clear the buffer with a color""""""
        buf = MHMSBFormat._get_buf(framebuf)
        value = 0xFF if color else 0x00
        for i in range(len(buf)):
            buf[i] = value

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        """"""Draw a rectangle at the given location, size and color.
        The ``fill_rect`` method draws both the outline and interior.""""""
        w = MHMSBFormat._get_width(framebuf)
        h = MHMSBFormat._get_height(framebuf)
        for yy in range(y, y + height):
            if yy < 0 or yy >= h:
                continue
            for xx in range(x, x + width):
                if xx < 0 or xx >= w:
                    continue
                MHMSBFormat.set_pixel(framebuf, xx, yy, color)
```"
313489,adafruit/Adafruit_CircuitPython_framebuf,adafruit_Adafruit_CircuitPython_framebuf/adafruit_framebuf.py,adafruit_framebuf.MVLSBFormat,"class MVLSBFormat:
    """"""MVLSBFormat""""""

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        """"""Set a given pixel to a color.""""""
        index = (y >> 3) * framebuf.stride + x
        offset = y & 7
        framebuf.buf[index] = framebuf.buf[index] & ~(1 << offset) | (color != 0) << offset

    @staticmethod
    def get_pixel(framebuf, x, y):
        """"""Get the color of a given pixel""""""
        index = (y >> 3) * framebuf.stride + x
        offset = y & 7
        return framebuf.buf[index] >> offset & 1

    @staticmethod
    def fill(framebuf, color):
        """"""completely fill/clear the buffer with a color""""""
        if color:
            fill = 255
        else:
            fill = 0
        for i in range(len(framebuf.buf)):
            framebuf.buf[i] = fill

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        """"""Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.""""""
        while height > 0:
            index = (y >> 3) * framebuf.stride + x
            offset = y & 7
            for w_w in range(width):
                framebuf.buf[index + w_w] = framebuf.buf[index + w_w] & ~(1 << offset) | (color != 0) << offset
            y += 1
            height -= 1","class MVLSBFormat:
    
    @staticmethod
    def set_pixel(framebuf, x, y, color):
        
        pass
    @staticmethod
    def get_pixel(framebuf, x, y):
        '''Get the color of a given pixel'''
        pass
    @staticmethod
    def fill(framebuf, color):
        
        pass
    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        
        pass",snippet_251,5,"class MVLSBFormat:
    
    @staticmethod
    def set_pixel(framebuf, x, y, color):
        """"""Set the pixel at (x, y) to the given color.""""""
        framebuf[y][x] = color

    @staticmethod
    def get_pixel(framebuf, x, y):
        """"""Get the color of a given pixel.""""""
        return framebuf[y][x]

    @staticmethod
    def fill(framebuf, color):
        """"""Fill the entire frame buffer with the given color.""""""
        for row in framebuf:
            for i in range(len(row)):
                row[i] = color

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        """"""Fill a rectangle area with the given color.""""""
        for row in range(y, y + height):
            for col in range(x, x + width):
                framebuf[row][col] = color"
313491,adafruit/Adafruit_CircuitPython_framebuf,adafruit_Adafruit_CircuitPython_framebuf/adafruit_framebuf.py,adafruit_framebuf.RGB888Format,"class RGB888Format:
    """"""RGB888Format""""""

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        """"""Set a given pixel to a color.""""""
        index = (y * framebuf.stride + x) * 3
        if isinstance(color, tuple):
            framebuf.buf[index:index + 3] = bytes(color)
        else:
            framebuf.buf[index:index + 3] = bytes((color >> 16 & 255, color >> 8 & 255, color & 255))

    @staticmethod
    def get_pixel(framebuf, x, y):
        """"""Get the color of a given pixel""""""
        index = (y * framebuf.stride + x) * 3
        return framebuf.buf[index] << 16 | framebuf.buf[index + 1] << 8 | framebuf.buf[index + 2]

    @staticmethod
    def fill(framebuf, color):
        """"""completely fill/clear the buffer with a color""""""
        fill = (color >> 16 & 255, color >> 8 & 255, color & 255)
        for i in range(0, len(framebuf.buf), 3):
            framebuf.buf[i:i + 3] = bytes(fill)

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        """"""Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.""""""
        fill = (color >> 16 & 255, color >> 8 & 255, color & 255)
        for _x in range(x, x + width):
            for _y in range(y, y + height):
                index = (_y * framebuf.stride + _x) * 3
                framebuf.buf[index:index + 3] = bytes(fill)","class RGB888Format:
    '''RGB888Format'''
    @staticmethod
    def set_pixel(framebuf, x, y, color):
        '''Set a given pixel to a color.'''
        pass
    @staticmethod
    def get_pixel(framebuf, x, y):
        '''Get the color of a given pixel'''
        pass
    @staticmethod
    def fill(framebuf, color):
        
        pass
    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        
        pass",snippet_252,5,"class RGB888Format:
    '''RGB888Format'''
    @staticmethod
    def set_pixel(framebuf, x, y, color):
        '''Set a given pixel to a color.'''
        # Assume framebuf has attributes width and height and is a bytearray or mutable buffer
        width = getattr(framebuf, 'width', None)
        if width is None:
            raise AttributeError(""framebuf must have a 'width' attribute"")
        offset = (y * width + x) * 3
        framebuf[offset:offset+3] = bytes(color[:3])

    @staticmethod
    def get_pixel(framebuf, x, y):
        '''Get the color of a given pixel'''
        width = getattr(framebuf, 'width', None)
        if width is None:
            raise AttributeError(""framebuf must have a 'width' attribute"")
        offset = (y * width + x) * 3
        return tuple(framebuf[offset:offset+3])

    @staticmethod
    def fill(framebuf, color):
        '''Fill the entire frame buffer with the given color.'''
        # color should be a sequence of 3 ints
        color_bytes = bytes(color[:3])
        # Use slice assignment for speed
        for i in range(0, len(framebuf), 3):
            framebuf[i:i+3] = color_bytes

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        '''Fill a rectangle area with the given color.'''
        color_bytes = bytes(color[:3])
        buf_width = getattr(framebuf, 'width', None)
        if buf_width is None:
            raise AttributeError(""framebuf must have a 'width' attribute"")
        for row in range(y, y + height):
            base = (row * buf_width + x) * 3
            for col in range(width):
                offset = base + col * 3
                framebuf[offset:offset+3] = color_bytes
"
319441,pysal/spglm,spglm/varfuncs.py,spglm.varfuncs.VarianceFunction,"import numpy as np

class VarianceFunction:
    """"""
    Relates the variance of a random variable to its mean. Defaults to 1.

    Methods
    -------
    call
        Returns an array of ones that is the same shape as `mu`

    Notes
    -----
    After a variance function is initialized, its call method can be used.

    Alias for VarianceFunction:
    constant = VarianceFunction()

    See also
    --------
    statsmodels.family.family
    """"""

    def __call__(self, mu):
        """"""
        Default variance function

        Parameters
        -----------
        mu : array-like
            mean parameters

        Returns
        -------
        v : array
            ones(mu.shape)
        """"""
        mu = np.asarray(mu)
        return np.ones(mu.shape, np.float64)

    def deriv(self, mu):
        """"""
        Derivative of the variance function v'(mu)
        """"""
        from statsmodels.tools.numdiff import approx_fprime_cs
        return np.diag(approx_fprime_cs(mu, self))","
class VarianceFunction:
    '''
    Relates the variance of a random variable to its mean. Defaults to 1.
    Methods
    -------
    call
        Returns an array of ones that is the same shape as `mu`
    Notes
    -----
    After a variance function is initialized, its call method can be used.
    Alias for VarianceFunction:
    constant = VarianceFunction()
    See also
    --------
    statsmodels.family.family
    '''
    def __call__(self, mu):
        '''
        Default variance function
        Parameters
        -----------
        mu : array-like
            mean parameters
        Returns
        -------
        v : array
            ones(mu.shape)
        '''
        pass
    def deriv(self, mu):
        
        pass",snippet_256,3,"```python
import numpy as np

class VarianceFunction:
    '''
    Relates the variance of a random variable to its mean. Defaults to 1.
    Methods
    -------
    call
        Returns an array of ones that is the same shape as `mu`
    Notes
    -----
    After a variance function is initialized, its call method can be used.
    Alias for VarianceFunction:
    constant = VarianceFunction()
    See also
    --------
    statsmodels.family.family
    '''
    def __call__(self, mu):
        '''
        Default variance function
        Parameters
        -----------
        mu : array-like
            mean parameters
        Returns
        -------
        v : array
            ones(mu.shape)
        '''
        mu_arr = np.asarray(mu)
        return np.ones_like(mu_arr)

    def deriv(self, mu):
        '''
        Derivative of the default variance function (constant 1)
        Parameters
        -----------
        mu : array-like
            mean parameters
        Returns
        -------
        dv : array
            zeros(mu.shape)
        '''
        mu_arr = np.asarray(mu)
        return np.zeros_like(mu_arr)

# Alias
constant = VarianceFunction()
```"
320689,optimizely/python-sdk,optimizely_python-sdk/optimizely/user_profile.py,optimizely.user_profile.UserProfileService,"from typing import Any, Optional

class UserProfileService:
    """""" Class encapsulating user profile service functionality.
  Override with your own implementation for storing and retrieving the user profile. """"""

    def lookup(self, user_id: str) -> dict[str, Any]:
        """""" Fetch the user profile dict corresponding to the user ID.

    Args:
      user_id: ID for user whose profile needs to be retrieved.

    Returns:
      Dict representing the user's profile.
    """"""
        return UserProfile(user_id).__dict__

    def save(self, user_profile: dict[str, Any]) -> None:
        """""" Save the user profile dict sent to this method.

    Args:
      user_profile: Dict representing the user's profile.
    """"""
        pass","
class UserProfileService:
    ''' Class encapsulating user profile service functionality.
  Override with your own implementation for storing and retrieving the user profile. '''
    def lookup(self, user_id: str) -> dict[str, Any]:
        
        pass
    def save(self, user_profile: dict[str, Any]) -> None:
        ''' Save the user profile dict sent to this method.
    Args:
      user_profile: Dict representing the user's profile.
        '''
        pass",snippet_257,3,"```python
from __future__ import annotations
from typing import Any, Dict


class UserProfileService:
    """"""Class encapsulating user profile service functionality.
    Stores and retrieves user profiles in memory.
    """"""

    def __init__(self) -> None:
        self._store: Dict[str, Dict[str, Any]] = {}

    def lookup(self, user_id: str) -> Dict[str, Any]:
        """"""Retrieve a user profile by user_id.

        Args:
            user_id: The unique identifier for the user.

        Returns:
            The user profile dictionary.

        Raises:
            KeyError: If the user_id is not found.
        """"""
        if user_id not in self._store:
            raise KeyError(f""User ID '{user_id}' not found."")
        return self._store[user_id].copy()

    def save(self, user_profile: Dict[str, Any]) -> None:
        """"""Save the user profile dict sent to this method.

        Args:
            user_profile: Dict representing the user's profile. Must contain a 'user_id' key.
        """"""
        if ""user_id"" not in user_profile:
            raise ValueError(""user_profile must contain a 'user_id' key."")
        user_id = str(user_profile[""user_id""])
        self._store[user_id] = user_profile.copy()
```"
322746,wilson-eft/wilson,wilson-eft_wilson/wilson/classes.py,wilson.classes.RGsolution,"import numpy as np
from math import log, e

class RGsolution:
    """"""Class representing a continuous (interpolated) solution to the
    SMEFT RGEs to be used for plotting.""""""

    def __init__(self, fun, scale_min, scale_max):
        """"""Initialize.

        Parameters:

        - fun: function of the scale that is expected to return a
        dictionary with the RGE solution and to accept vectorized input.
        - scale_min, scale_max: lower and upper boundaries of the scale
        """"""
        self.fun = fun
        self.scale_min = scale_min
        self.scale_max = scale_max

    def plotdata(self, key, part='re', scale='log', steps=50):
        """"""Return a tuple of arrays x, y that can be fed to plt.plot,
        where x is the scale in GeV and y is the parameter of interest.

        Parameters:

        - key: dicionary key of the parameter to be plotted (e.g. a WCxf
          coefficient name or a SM parameter like 'g')
        - part: plot the real part 're' (default) or the imaginary part 'im'
        - scale: 'log'; make the x steps logarithmically distributed; for
          'linear', linearly distributed
        - steps: steps in x to take (default: 50)
        """"""
        if scale == 'log':
            x = np.logspace(log(self.scale_min), log(self.scale_max), steps, base=e)
        elif scale == 'linear':
            x = np.linspace(self.scale_min, self.scale_max, steps)
        y = self.fun(x)
        y = np.array([d[key] for d in y])
        if part == 're':
            return (x, y.real)
        elif part == 'im':
            return (x, y.imag)

    def plot(self, key, part='re', scale='log', steps=50, legend=True, plotargs={}):
        """"""Plot the RG evolution of parameter `key`.

        Parameters:

        - part, scale, steps: see `plotdata`
        - legend: boolean, show the legend (default: True)
        - plotargs: dictionary of arguments to be passed to plt.plot
        """"""
        try:
            import matplotlib.pyplot as plt
        except ImportError:
            raise ImportError('Please install matplotlib if you want to use the plot method')
        pdat = self.plotdata(key, part=part, scale=scale, steps=steps)
        plt.plot(*pdat, label=key, **plotargs)
        if scale == 'log':
            plt.xscale('log')
        if legend:
            plt.legend()","
class RGsolution:
    '''Class representing a continuous (interpolated) solution to the
    SMEFT RGEs to be used for plotting.'''
    def __init__(self, fun, scale_min, scale_max):
        '''Initialize.
        Parameters:
        - fun: function of the scale that is expected to return a
        dictionary with the RGE solution and to accept vectorized input.
        - scale_min, scale_max: lower and upper boundaries of the scale
        '''
        pass
    def plotdata(self, key, part='re', scale='log', steps=50):
        '''Return a tuple of arrays x, y that can be fed to plt.plot,
        where x is the scale in GeV and y is the parameter of interest.
        Parameters:
        - key: dicionary key of the parameter to be plotted (e.g. a WCxf
          coefficient name or a SM parameter like 'g')
        - part: plot the real part 're' (default) or the imaginary part 'im'
        - scale: 'log'; make the x steps logarithmically distributed; for
          'linear', linearly distributed
        - steps: steps in x to take (default: 50)
        '''
        pass
    def plotdata(self, key, part='re', scale='log', steps=50):
        '''Plot the RG evolution of parameter `key`.
        Parameters:
        - part, scale, steps: see `plotdata`
        - legend: boolean, show the legend (default: True)
        - plotargs: dictionary of arguments to be passed to plt.plot
        '''
        pass",snippet_258,4,"```python
import numpy as np
import matplotlib.pyplot as plt

class RGsolution:
    '''Class representing a continuous (interpolated) solution to the
    SMEFT RGEs to be used for plotting.'''
    def __init__(self, fun, scale_min, scale_max):
        '''Initialize.
        Parameters:
        - fun: function of the scale that is expected to return a
        dictionary with the RGE solution and to accept vectorized input.
        - scale_min, scale_max: lower and upper boundaries of the scale
        '''
        self.fun = fun
        self.scale_min = scale_min
        self.scale_max = scale_max

    def plotdata(self, key, part='re', scale='log', steps=50):
        '''Return a tuple of arrays x, y that can be fed to plt.plot,
        where x is the scale in GeV and y is the parameter of interest.
        Parameters:
        - key: dicionary key of the parameter to be plotted (e.g. a WCxf
          coefficient name or a SM parameter like 'g')
        - part: plot the real part 're' (default) or the imaginary part 'im'
        - scale: 'log'; make the x steps logarithmically distributed; for
          'linear', linearly distributed
        - steps: steps in x to take (default: 50)
        '''
        if scale == 'log':
            x = np.logspace(np.log10(self.scale_min), np.log10(self.scale_max), steps)
        else:
            x = np.linspace(self.scale_min, self.scale_max, steps)

        sol = self.fun(x)
        if key not in sol:
            raise KeyError(f""Key '{key}' not found in solution dictionary."")
        y = sol[key]
        if part == 're':
            y = np.real(y)
        elif part == 'im':
            y = np.imag(y)
        else:
            raise ValueError(""part must be 're' or 'im'"")

        return x, y

    def plot(self, key, part='re', scale='log', steps=50,
             legend=True, plotargs=None):
        '''Plot the RG evolution of parameter `key`.
        Parameters:
        - part, scale, steps: see `plotdata`
        - legend: boolean, show the legend (default: True)
        - plotargs: dictionary of arguments to be passed to plt.plot
        '''
        if plotargs is None:
            plotargs = {}
        x, y = self.plotdata(key, part=part, scale=scale, steps=steps)
        plt.plot(x, y, label=key, **plotargs)
        if legend:
            plt.legend()
```"
322803,wilson-eft/wilson,wilson-eft_wilson/wilson/wcxf/classes.py,wilson.wcxf.classes.WCxf,"import yaml

class WCxf:
    """"""Base class for WCxf files (not meant to be used directly).""""""

    @classmethod
    def load(cls, stream, **kwargs):
        """"""Load the object data from a JSON or YAML file.""""""
        wcxf = _load_yaml_json(stream, **kwargs)
        return cls(**wcxf)

    def dump(self, stream=None, fmt='json', **kwargs):
        """"""Dump the object data to a JSON or YAML file.

        Optional arguments:

        - `stream`: if None (default), return a string. Otherwise,
          should be a writable file-like object
        - `fmt`: format, should be 'json' (default) or 'yaml'

        Additional keyword arguments will be passed to the `json.dump(s)`
        or `yaml.dump` methods.
        """"""
        d = {k: v for k, v in self.__dict__.items() if k[0] != '_'}
        if fmt.lower() == 'json':
            indent = kwargs.pop('indent', 2)
            return _dump_json(d, stream=stream, indent=indent, **kwargs)
        elif fmt.lower() == 'yaml':
            default_flow_style = kwargs.pop('default_flow_style', False)
            return yaml.dump(d, stream, default_flow_style=default_flow_style, **kwargs)
        else:
            raise ValueError(f""Format {fmt} unknown: use 'json' or 'yaml'."")","
class WCxf:
    '''Base class for WCxf files (not meant to be used directly).'''
    @classmethod
    def load(cls, stream, **kwargs):
        '''Load the object data from a JSON or YAML file.'''
        pass
    def dump(self, stream=None, fmt='json', **kwargs):
        '''Dump the object data to a JSON or YAML file.
        Optional arguments:
        - `stream`: if None (default), return a string. Otherwise,
          should be a writable file-like object
        - `fmt`: format, should be 'json' (default) or 'yaml'
        Additional keyword arguments will be passed to the `json.dump(s)`
        or `yaml.dump` methods.
        '''
        pass",snippet_259,3,"```python
import json
import os

try:
    import yaml
except ImportError:  # pragma: no cover
    yaml = None


class WCxf:
    '''Base class for WCxf files (not meant to be used directly).'''

    @classmethod
    def load(cls, stream, **kwargs):
        '''Load the object data from a JSON or YAML file.'''
        # Determine if stream is a path or a file-like object
        if isinstance(stream, str):
            # Treat as file path
            path = stream
            if not os.path.exists(path):
                raise FileNotFoundError(f""No such file: {path}"")
            with open(path, 'r', encoding=kwargs.get('encoding', 'utf-8')) as f:
                content = f.read()
            # Detect format from extension
            _, ext = os.path.splitext(path.lower())
            if ext == '.json':
                data = json.loads(content, **kwargs)
            elif ext in {'.yaml', '.yml'}:
                if yaml is None:
                    raise ImportError(""PyYAML is required to load YAML files"")
                data = yaml.safe_load(content)
            else:
                # Try JSON first, then YAML
                try:
                    data = json.loads(content, **kwargs)
                except Exception:
                    if yaml is None:
                        raise ImportError(""PyYAML is required to load YAML files"")
                    data = yaml.safe_load(content)
        else:
            # Assume file-like object
            try:
                data = json.load(stream, **kwargs)
            except Exception:
                if yaml is None:
                    raise ImportError(""PyYAML is required to load YAML files"")
                stream.seek(0)
                data = yaml.safe_load(stream)

        if not isinstance(data, dict):
            raise ValueError(""Loaded data must be a dictionary"")
        return cls(**data)

    def dump(self, stream=None, fmt='json', **kwargs):
        '''Dump the object data to a JSON or YAML file.
        Optional arguments:
        - `stream`: if None (default), return a string. Otherwise,
          should be a writable file-like object
        - `fmt`: format, should be 'json' (default) or 'yaml'
        Additional keyword arguments will be passed to the `json.dump(s)`
        or `yaml.dump` methods.
        '''
        data = self.__dict__
        if fmt == 'json':
            if stream is None:
                return json.dumps(data, **kwargs)
            json.dump(data, stream, **kwargs)
        elif fmt == 'yaml':
            if yaml is None:
                raise ImportError(""PyYAML is required to dump YAML files"")
            if stream is None:
                return yaml.safe_dump(data, **kwargs)
            yaml.safe_dump(data, stream, **kwargs)
        else:
            raise ValueError(f""Unsupported format: {fmt}"")
```"
322855,jhermann/rituals,jhermann_rituals/src/rituals/util/antglob.py,rituals.util.antglob.Pattern,"class Pattern:
    """"""A single pattern for either inclusion or exclusion.""""""

    def __init__(self, spec, inclusive):
        """"""Create regex-based pattern matcher from glob `spec`.""""""
        self.compiled = compile_glob(spec.rstrip('/'))
        self.inclusive = inclusive
        self.is_dir = spec.endswith('/')

    def __str__(self):
        """"""Return inclusiveness indicator and original glob pattern.""""""
        return ('+' if self.inclusive else '-') + self.compiled.pattern

    def matches(self, path):
        """"""Check this pattern against given `path`.""""""
        return bool(self.compiled.match(path))","class Pattern:
    '''A single pattern for either inclusion or exclusion.'''
    def __init__(self, spec, inclusive):
        '''Create regex-based pattern matcher from glob `spec`.'''
        pass
    def __str__(self):
        
        pass
    def matches(self, path):
        
        pass",snippet_260,4,"```python
import re
import fnmatch

class Pattern:
    '''A single pattern for either inclusion or exclusion.'''
    def __init__(self, spec, inclusive):
        '''Create regex-based pattern matcher from glob `spec`.'''
        self.spec = spec
        self.inclusive = inclusive
        # fnmatch.translate returns a regex string that matches the whole string
        self._regex = re.compile(fnmatch.translate(spec))

    def __str__(self):
        return f""{'INCLUDE' if self.inclusive else 'EXCLUDE'}: {self.spec}""

    def matches(self, path):
        '''Return True if the given path matches this pattern.'''
        return bool(self._regex.match(path))
```"
323184,SoftwareDefinedBuildings/XBOS,apps/Data_quality_analysis/Import_Data.py,Data_quality_analysis.Import_Data.Import_Data,"import pandas as pd
import glob
import numpy as np
import os

class Import_Data:
    """""" This class imports data from csv files. """"""

    def __init__(self):
        """""" Constructor: Store the imported data. """"""
        self.data = pd.DataFrame()

    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        """""" Imports csv file(s) and stores the result in data.

        Note
        ----
        1. If folder exists out of current directory, folder_name should contain correct regex
        2. Assuming there's no file called ""\\*.csv""

        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '\\*', i.e. all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.', i.e. current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe

        """"""
        if isinstance(file_name, str) and isinstance(folder_name, str):
            try:
                self.data = self._load_csv(file_name, folder_name, head_row, index_col, convert_col, concat_files)
            except Exception as e:
                raise e
        elif isinstance(file_name, list) and isinstance(folder_name, str):
            for i, file in enumerate(file_name):
                if isinstance(head_row, list):
                    _head_row = head_row[i]
                else:
                    _head_row = head_row
                if isinstance(index_col, list):
                    _index_col = index_col[i]
                else:
                    _index_col = index_col
                try:
                    data_tmp = self._load_csv(file, folder_name, _head_row, _index_col, convert_col, concat_files)
                    if concat_files:
                        self.data = self.data.append(data_tmp, ignore_index=False, verify_integrity=False)
                    else:
                        self.data = self.data.join(data_tmp, how='outer')
                except Exception as e:
                    raise e
        else:
            raise NotImplementedError(""Filename and Folder name can't both be of type list."")

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        """""" Load single csv file.

        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '*' - all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.' - current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe

        Returns
        -------
        pd.DataFrame()
            Dataframe containing csv data

        """"""
        if file_name == '*':
            if not os.path.isdir(folder_name):
                raise OSError('Folder does not exist.')
            else:
                file_name_list = sorted(glob.glob(folder_name + '*.csv'))
                if not file_name_list:
                    raise OSError('Either the folder does not contain any csv files or invalid folder provided.')
                else:
                    self.import_csv(file_name=file_name_list, head_row=head_row, index_col=index_col, convert_col=convert_col, concat_files=concat_files)
                    return self.data
        elif not os.path.isdir(folder_name):
            raise OSError('Folder does not exist.')
        else:
            path = os.path.join(folder_name, file_name)
            if head_row > 0:
                data = pd.read_csv(path, index_col=index_col, skiprows=[i for i in range(head_row - 1)])
            else:
                data = pd.read_csv(path, index_col=index_col)
            try:
                data.index = pd.to_datetime(data.index, format='%m/%d/%y %H:%M')
            except:
                data.index = pd.to_datetime(data.index, dayfirst=False, infer_datetime_format=True)
        if convert_col:
            for col in data.columns:
                if data[col].dtype != np.number:
                    data[col] = pd.to_numeric(data[col], errors='coerce')
        return data","
class Import_Data:
    ''' This class imports data from csv files. '''
    def __init__(self):
        
        pass
    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        
        pass
    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        
        pass",snippet_261,4,"```python
import os
import glob
import pandas as pd


class Import_Data:
    ''' This class imports data from csv files. '''
    def __init__(self):
        # No initialization needed for now
        pass

    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0,
                   convert_col=True, concat_files=False):
        """"""
        Public method to import CSV files.

        Parameters
        ----------
        file_name : str or list of str, default '*'
            Pattern or specific file name(s) to load. If a list is provided,
            each element is treated as a separate file pattern.
        folder_name : str, default '.'
            Directory where the CSV files are located.
        head_row : int, default 0
            Row number to use as the header.
        index_col : int or str, default 0
            Column to set as index.
        convert_col : bool, default True
            If True, attempt to convert all columns to numeric where possible.
        concat_files : bool, default False
            If True and multiple files are found, concatenate them into a single
            DataFrame. Otherwise, return a list of DataFrames.

        Returns
        -------
        pd.DataFrame or list of pd.DataFrame
            The loaded data.
        """"""
        return self._load_csv(file_name, folder_name, head_row, index_col,
                              convert_col, concat_files)

    def _load_csv(self, file_name, folder_name, head_row, index_col,
                  convert_col, concat_files):
        # Resolve file patterns
        if isinstance(file_name, str):
            patterns = [file_name]
        else:
            patterns = list(file_name)

        all_files = []
        for pattern in patterns:
            # Build full pattern path
            full_pattern = os.path.join(folder_name, pattern)
            matched = glob.glob(full_pattern)
            if not matched:
                # If pattern is a specific file and doesn't exist, raise error
                if os.path.isfile(full_pattern):
                    matched = [full_pattern]
                else:
                    continue
            all_files.extend(matched)

        if not all_files:
            raise FileNotFoundError(f""No CSV files found for pattern(s) {patterns} in {folder_name}"")

        dfs = []
        for f in all_files:
            try:
                df = pd.read_csv(f, header=head_row, index_col=index_col)
            except Exception as e:
                raise ValueError(f""Error reading {f}: {e}"")

            if convert_col:
                # Convert all columns to numeric where possible
                for col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='ignore')

            dfs.append(df)

        if concat_files:
            # Concatenate along rows, aligning columns
            result = pd.concat(dfs, axis=0, ignore_index=False)
            return result
        else:
            return dfs
```"
323187,SoftwareDefinedBuildings/XBOS,apps/Data_quality_analysis/Plot_Data.py,Data_quality_analysis.Plot_Data.Plot_Data,"import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

class Plot_Data:
    """""" This class contains functions for displaying various plots.

    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.

    """"""
    count = 1

    def __init__(self, figsize=(18, 5)):
        """""" Constructor.

        Parameters
        ----------
        figsize : tuple
            Size of figure.

        """"""
        self.figsize = figsize

    def correlation_plot(self, data):
        """""" Create heatmap of Pearson's correlation coefficient.

        Parameters
        ----------
        data    : pd.DataFrame()
            Data to display.

        Returns
        -------
        matplotlib.figure
            Heatmap.

        """"""
        fig = plt.figure(Plot_Data.count)
        corr = data.corr()
        ax = sns.heatmap(corr)
        Plot_Data.count += 1
        return fig

    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        """""" Create baseline and projection plots.

        Parameters
        ----------
        y_true              : pd.Series()
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.Dataframe()
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.

        Returns
        -------
        matplotlib.figure
            Baseline plot

        """"""
        fig = plt.figure(Plot_Data.count)
        if projection_period:
            nrows = len(baseline_period) + len(projection_period) / 2
        else:
            nrows = len(baseline_period) / 2
        base_df = pd.DataFrame()
        base_df['y_true'] = y_true
        base_df['y_pred'] = y_pred
        ax1 = fig.add_subplot(nrows, 1, 1)
        base_df.plot(ax=ax1, figsize=self.figsize, title='Baseline Period ({}-{}). \nBest Model: {}. \nBaseline Adj R2: {}. \nSite: {}.'.format(baseline_period[0], baseline_period[1], model_name, adj_r2, site))
        if projection_period:
            num_plot = 2
            for i in range(0, len(projection_period), 2):
                ax = fig.add_subplot(nrows, 1, num_plot)
                period = slice(projection_period[i], projection_period[i + 1])
                project_df = pd.DataFrame()
                try:
                    project_df['y_true'] = data.loc[period, output_col]
                    project_df['y_pred'] = model.predict(data.loc[period, input_col])
                    project_df['y_pred'][project_df['y_pred'] < 0] = 0
                    project_df.plot(ax=ax, figsize=self.figsize, title='Projection Period ({}-{})'.format(projection_period[i], projection_period[i + 1]))
                    num_plot += 1
                    fig.tight_layout()
                    Plot_Data.count += 1
                    return (fig, project_df['y_true'], project_df['y_pred'])
                except:
                    raise TypeError('If projecting into the future, please specify project_ind_col that has data available                                         in the future time period requested.')
        return (fig, None, None)","
class Plot_Data:
    ''' This class contains functions for displaying various plots.
    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.
    '''
    def __init__(self, figsize=(18, 5)):
        
        pass
    def correlation_plot(self, data):
        
        pass
    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        ''' Create baseline and projection plots.
        Parameters
        ----------
        y_true              : pd.Series()
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.Dataframe()
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.
        Returns
        -------
        matplotlib.figure
            Baseline plot
        '''
        pass",snippet_262,4,"```python
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

class Plot_Data:
    ''' This class contains functions for displaying various plots.
    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.
    '''
    def __init__(self, figsize=(18, 5)):
        self.figsize = figsize
        self.count = 0

    def correlation_plot(self, data):
        """"""
        Plot a heatmap of the correlation matrix of the provided DataFrame.
        Parameters
        ----------
        data : pd.DataFrame
            DataFrame containing numeric columns.
        Returns
        -------
        matplotlib.figure.Figure
            The correlation heatmap figure.
        """"""
        corr = data.corr()
        fig, ax = plt.subplots(figsize=self.figsize)
        sns.heatmap(corr, annot=True, fmt="".2f"", cmap=""coolwarm"", ax=ax)
        ax.set_title(""Correlation Matrix"")
        self.count += 1
        return fig

    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period,
                                 model_name, adj_r2, data, input_col, output_col, model, site):
        """"""
        Create baseline and projection plots.
        Parameters
        ----------
        y_true              : pd.Series
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.DataFrame
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.
        site                : str
            Site name or identifier.
        Returns
        -------
        matplotlib.figure.Figure
            Baseline plot
        """"""
        # Ensure y_true is a Series with datetime index
        if not isinstance(y_true, pd.Series):
            y_true = pd.Series(y_true, index=data.index)
        # Create a DataFrame for predictions
        pred_df = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred}, index=data.index)

        # Filter baseline and projection periods
        baseline_mask = pred_df.index.isin(baseline_period)
        projection_mask = pred_df.index.isin(projection_period)

        fig, ax = plt.subplots(figsize=self.figsize)
        # Plot baseline
        ax.plot(pred_df.index[baseline_mask], pred_df.loc[baseline_mask, 'y_true'],
                label='Baseline Actual', color='blue')
        ax.plot(pred_df.index[baseline_mask], pred_df.loc[baseline_mask, 'y_pred'],
                label='Baseline Predicted', color='cyan', linestyle='--')
        # Plot projection
        ax.plot(pred_df.index[projection_mask], pred_df.loc[projection_mask, 'y_true'],
                label='Projection Actual', color='red')
        ax.plot(pred_df.index[projection_mask], pred_df.loc[projection_mask, 'y_pred'],
                label='Projection Predicted', color='orange', linestyle='--')

        ax.set_title(f'{site} - {model_name} (Adj R² = {adj_r2:.3f})')
        ax.set_xlabel('Date')
        ax.set_ylabel(output_col)
        ax.legend()
        ax.grid(True)
        self.count += 1
        return fig
```"
323204,SoftwareDefinedBuildings/XBOS,apps/data_analysis/XBOS_data_analytics/Import_Data.py,XBOS_data_analytics.Import_Data.Import_Data,"import os
import pandas as pd
import numpy as np
import glob

class Import_Data:
    """""" This class imports data from csv files """"""

    def __init__(self):
        """""" Constructor.

        This class stores the imported data.

        """"""
        self.data = pd.DataFrame()

    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        """""" Imports csv file(s) and stores the result in data.

        Note
        ----
        1. If folder exists out of current directory, folder_name should contain correct regex
        2. Assuming there's no file called ""\\*.csv""

        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '\\*', i.e. all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.', i.e. current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe

        """"""
        if isinstance(file_name, str) and isinstance(folder_name, str):
            try:
                self.data = self._load_csv(file_name, folder_name, head_row, index_col, convert_col, concat_files)
            except Exception as e:
                raise e
        elif isinstance(file_name, list) and isinstance(folder_name, str):
            for i, file in enumerate(file_name):
                if isinstance(head_row, list):
                    _head_row = head_row[i]
                else:
                    _head_row = head_row
                if isinstance(index_col, list):
                    _index_col = index_col[i]
                else:
                    _index_col = index_col
                try:
                    data_tmp = self._load_csv(file, folder_name, _head_row, _index_col, convert_col, concat_files)
                    if concat_files:
                        self.data = self.data.append(data_tmp, ignore_index=False, verify_integrity=False)
                    else:
                        self.data = self.data.join(data_tmp, how='outer')
                except Exception as e:
                    raise e
        else:
            raise NotImplementedError(""Filename and Folder name can't both be of type list."")

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        """""" Load single csv file.

        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '*' - all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.' - current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe

        Returns
        -------
        pd.DataFrame()
            Dataframe containing csv data

        """"""
        if file_name == '*':
            if not os.path.isdir(folder_name):
                raise OSError('Folder does not exist.')
            else:
                file_name_list = sorted(glob.glob(folder_name + '*.csv'))
                if not file_name_list:
                    raise OSError('Either the folder does not contain any csv files or invalid folder provided.')
                else:
                    self.import_csv(file_name=file_name_list, head_row=head_row, index_col=index_col, convert_col=convert_col, concat_files=concat_files)
                    return self.data
        elif not os.path.isdir(folder_name):
            raise OSError('Folder does not exist.')
        else:
            path = os.path.join(folder_name, file_name)
            if head_row > 0:
                data = pd.read_csv(path, index_col=index_col, skiprows=[i for i in range(head_row - 1)])
            else:
                data = pd.read_csv(path, index_col=index_col)
            try:
                data.index = pd.to_datetime(data.index, format='%m/%d/%y %H:%M')
            except:
                data.index = pd.to_datetime(data.index, dayfirst=False, infer_datetime_format=True)
        if convert_col:
            for col in data.columns:
                if data[col].dtype != np.number:
                    data[col] = pd.to_numeric(data[col], errors='coerce')
        return data","
class Import_Data:
    
    def __init__(self):
        ''' Constructor.
        This class stores the imported data.
        '''
        pass
    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        
        pass
    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        ''' Load single csv file.
        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '*' - all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.' - current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe
        Returns
        -------
        pd.DataFrame()
            Dataframe containing csv data
        '''
        pass",snippet_263,4,"```python
import os
import glob
import pandas as pd


class Import_Data:
    def __init__(self):
        """"""Constructor.
        This class stores the imported data.
        """"""
        self.data = pd.DataFrame()

    def import_csv(
        self,
        file_name=""*"",
        folder_name=""."",
        head_row=0,
        index_col=0,
        convert_col=True,
        concat_files=False,
    ):
        """"""
        Import CSV files from a folder.

        Parameters
        ----------
        file_name : str, optional
            CSV file to be imported. Defaults to '*' - all csv files in the folder.
        folder_name : str, optional
            Folder where file resides. Defaults to '.' - current directory.
        head_row : int, optional
            Skips all rows from 0 to head_row-1
        index_col : int, optional
            Skips all columns from 0 to index_col-1
        convert_col : bool, optional
            Convert columns to numeric type
        concat_files : bool, optional
            Appends data from files to result dataframe

        Returns
        -------
        pd.DataFrame
            Dataframe containing csv data
        """"""
        # Resolve folder path
        folder_path = os.path.abspath(folder_name)

        # Determine file list
        if file_name == ""*"":
            pattern = os.path.join(folder_path, ""*.csv"")
            files = sorted(glob.glob(pattern))
        else:
            files = [os.path.join(folder_path, file_name)]

        if not files:
            self.data = pd.DataFrame()
            return self.data

        # Load files
        dfs = []
        for f in files:
            df = self._load_csv(
                f, folder_path, head_row, index_col, convert_col, concat_files
            )
            if not df.empty:
                dfs.append(df)

        if not dfs:
            self.data = pd.DataFrame()
            return self.data

        if concat_files:
            self.data = pd.concat(dfs, ignore_index=True)
        else:
            self.data = dfs[0]

        return self.data

    def _load_csv(
        self,
        file_name,
        folder_name,
        head_row,
        index_col,
        convert_col,
        concat_files,
    ):
        """"""
        Load single csv file.

        Parameters
        ----------
        file_name : str
            CSV file to be imported. Defaults to '*' - all csv files in the folder.
        folder_name : str
            Folder where file resides. Defaults to '.' - current directory.
        head_row : int
            Skips all rows from 0 to head_row-1
        index_col : int
            Skips all columns from 0 to index_col-1
        convert_col : bool
            Convert columns to numeric type
        concat_files : bool
            Appends data from files to result dataframe

        Returns
        -------
        pd.DataFrame
            Dataframe containing csv data
        """"""
        # Build full path
        full_path = os.path.join(folder_name, file_name)

        # Read CSV
        try:
            df = pd.read_csv(
                full_path,
                skiprows=range(head_row),
                header=0,
                dtype=str,  # read as string first
            )
        except Exception:
            return pd.DataFrame()

        # Drop columns before index_col
        if index_col > 0:
            df = df.iloc[:, index_col:]

        # Convert columns to numeric if requested
        if convert_col:
            df = df.apply(pd.to_numeric, errors=""coerce"")

        return df
```"
323208,SoftwareDefinedBuildings/XBOS,apps/data_analysis/XBOS_data_analytics/Plot_Data.py,XBOS_data_analytics.Plot_Data.Plot_Data,"import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

class Plot_Data:
    """""" This class contains functions for displaying various plots.

    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.

    """"""
    count = 1

    def __init__(self, figsize=(18, 5)):
        """""" Constructor.

        Parameters
        ----------
        figsize : tuple
            Size of figure.

        """"""
        self.figsize = figsize

    def correlation_plot(self, data):
        """""" Create heatmap of Pearson's correlation coefficient.

        Parameters
        ----------
        data    : pd.DataFrame()
            Data to display.

        Returns
        -------
        matplotlib.figure
            Heatmap.

        """"""
        fig = plt.figure(Plot_Data.count)
        corr = data.corr()
        ax = sns.heatmap(corr)
        Plot_Data.count += 1
        return fig

    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        """""" Create baseline and projection plots.

        Parameters
        ----------
        y_true              : pd.Series()
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.Dataframe()
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.

        Returns
        -------
        matplotlib.figure
            Baseline plot

        """"""
        fig = plt.figure(Plot_Data.count)
        if projection_period:
            nrows = len(baseline_period) + len(projection_period) / 2
        else:
            nrows = len(baseline_period) / 2
        base_df = pd.DataFrame()
        base_df['y_true'] = y_true
        base_df['y_pred'] = y_pred
        ax1 = fig.add_subplot(nrows, 1, 1)
        base_df.plot(ax=ax1, figsize=self.figsize, title='Baseline Period ({}-{}). \nBest Model: {}. \nBaseline Adj R2: {}. \nSite: {}.'.format(baseline_period[0], baseline_period[1], model_name, adj_r2, site))
        if projection_period:
            num_plot = 2
            for i in range(0, len(projection_period), 2):
                ax = fig.add_subplot(nrows, 1, num_plot)
                period = slice(projection_period[i], projection_period[i + 1])
                project_df = pd.DataFrame()
                try:
                    project_df['y_true'] = data.loc[period, output_col]
                    project_df['y_pred'] = model.predict(data.loc[period, input_col])
                    project_df['y_pred'][project_df['y_pred'] < 0] = 0
                    project_df.plot(ax=ax, figsize=self.figsize, title='Projection Period ({}-{})'.format(projection_period[i], projection_period[i + 1]))
                    num_plot += 1
                    fig.tight_layout()
                    Plot_Data.count += 1
                    return (fig, project_df['y_true'], project_df['y_pred'])
                except:
                    raise SystemError('If projecting into the future, please specify project_ind_col that has data available                                         in the future time period requested.')
        return (fig, None, None)","
class Plot_Data:
    ''' This class contains functions for displaying various plots.
    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.
    '''
    def __init__(self, figsize=(18, 5)):
        
        pass
    def correlation_plot(self, data):
        
        pass
    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        ''' Create baseline and projection plots.
        Parameters
        ----------
        y_true              : pd.Series()
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.Dataframe()
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.
        Returns
        -------
        matplotlib.figure
            Baseline plot
        '''
        pass",snippet_264,4,"```python
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np


class Plot_Data:
    ''' This class contains functions for displaying various plots.
    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.
    '''
    def __init__(self, figsize=(18, 5)):
        self.figsize = figsize
        self.count = 0

    def correlation_plot(self, data):
        """"""
        Plot a heatmap of the correlation matrix of the provided DataFrame.

        Parameters
        ----------
        data : pd.DataFrame
            DataFrame containing numeric columns.

        Returns
        -------
        matplotlib.figure.Figure
            The correlation heatmap figure.
        """"""
        corr = data.corr()
        fig, ax = plt.subplots(figsize=self.figsize)
        sns.heatmap(corr, annot=True, fmt="".2f"", cmap=""coolwarm"", ax=ax)
        ax.set_title(""Correlation Matrix"")
        self.count += 1
        return fig

    def baseline_projection_plot(
        self,
        y_true,
        y_pred,
        baseline_period,
        projection_period,
        model_name,
        adj_r2,
        data,
        input_col,
        output_col,
        model,
        site,
    ):
        """"""
        Create baseline and projection plots.

        Parameters
        ----------
        y_true              : pd.Series
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period (list of dates or index labels).
        projection_period   : list(str)
            Projection periods (list of dates or index labels).
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.DataFrame
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.
        site                : str
            Site name for title.

        Returns
        -------
        matplotlib.figure.Figure
            Baseline and projection plot figure.
        """"""
        # Ensure y_true is a Series with a datetime-like index
        if not isinstance(y_true, pd.Series):
            raise TypeError(""y_true must be a pandas Series"")

        # Create a DataFrame with predictions
        df_pred = pd.DataFrame(
            {
                ""y_true"": y_true,
                ""y_pred"": pd.Series(y_pred, index=y_true.index),
            }
        )

        # Filter baseline and projection data
        baseline_mask = df_pred.index.isin(baseline_period)
        projection_mask = df_pred.index.isin(projection_period)

        df_baseline = df_pred.loc[baseline_mask]
        df_projection = df_pred.loc[projection_mask]

        # Create figure with two subplots
        fig, axes = plt.subplots(1, 2, figsize=self.figsize, sharey=True)
        ax_base, ax_proj = axes

        # Baseline plot
        ax_base.plot(df_baseline.index, df_baseline[""y_true""], label=""Observed"", color=""black"")
        ax_base.plot(df_baseline.index, df_baseline[""y_pred""], label=""Predicted"", color=""red"", linestyle=""--"")
        ax_base.set_title(f""{site} - Baseline ({baseline_period[0]} to {baseline_period[-1]})"")
        ax_base.set_xlabel(""Date"")
        ax_base.set_ylabel(output_col)
        ax_base.legend()
        ax_base.grid(True)

        # Projection plot
        ax_proj.plot(df_projection.index, df_projection[""y_true""], label=""Observed"", color=""black"")
        ax_proj.plot(df_projection.index, df_projection[""y_pred""], label=""Predicted"", color=""red"", linestyle=""--"")
        ax_proj.set_title(f""{site} - Projection ({projection_period[0]} to {projection_period[-1]})"")
        ax_proj.set_xlabel(""Date"")
        ax_proj.legend()
        ax_proj.grid(True)

        # Overall title with model info
        fig.suptitle(
            f""{site} - {model_name} (Adj. R² = {adj_r2:.3f})"",
            fontsize=16,
            y=0.95,
        )

        fig.tight_layout(rect=[0, 0, 1, 0.93])
        self.count += 1
        return fig
```"
339242,pawelad/pymonzo,pawelad_pymonzo/src/pymonzo/client.py,pymonzo.client.MonzoAPI,"from pymonzo.exceptions import MonzoAPIError, NoSettingsFile
from authlib.integrations.base_client import OAuthError
from pymonzo.webhooks import WebhooksResource
from pathlib import Path
from pymonzo.attachments import AttachmentsResource
from json import JSONDecodeError
from pymonzo.transactions import TransactionsResource
from pymonzo.pots import PotsResource
from pymonzo.whoami import WhoAmIResource
from pymonzo.utils import get_authorization_response_url
from urllib.parse import urlparse
from typing import Any, Optional
from pymonzo.balance import BalanceResource
import webbrowser
from pymonzo.feed import FeedResource
from pymonzo.accounts import AccountsResource
from authlib.integrations.httpx_client import OAuth2Client
from pymonzo.settings import PyMonzoSettings

class MonzoAPI:
    """"""Monzo public API client.

    To use it, you need to create a new OAuth client in [Monzo Developer Portal].
    The `Redirect URLs` should be set to `http://localhost:6600/pymonzo` and
    `Confidentiality` should be set to `Confidential` if you'd like to automatically
    refresh the access token when it expires.

    You can now use `Client ID` and `Client secret` in [`pymonzo.MonzoAPI.authorize`][]
    to finish the OAuth 2 'Authorization Code Flow' and get the API access token
    (which is by default saved to disk and refreshed when expired).

    [Monzo Developer Portal]: https://developers.monzo.com/

    Note:
        Monzo API docs: https://docs.monzo.com/
    """"""
    api_url = 'https://api.monzo.com'
    authorization_endpoint = 'https://auth.monzo.com/'
    token_endpoint = 'https://api.monzo.com/oauth2/token'
    settings_path = Path.home() / '.pymonzo'

    def __init__(self, access_token: Optional[str]=None) -> None:
        """"""Initialize Monzo API client and mount all resources.

        It expects [`pymonzo.MonzoAPI.authorize`][] to be called beforehand, so
        it can load the local settings file containing the API access token. You
        can also explicitly pass the `access_token`, but it won't be able to
        automatically refresh it once it expires.

        Arguments:
            access_token: OAuth access token. You can obtain it (and by default, save
                it to disk, so it can refresh automatically) by running
                [`pymonzo.MonzoAPI.authorize`][]. Alternatively, you can get a
                temporary access token from the [Monzo Developer Portal].

                [Monzo Developer Portal]: https://developers.monzo.com/

        Raises:
            NoSettingsFile: When the access token wasn't passed explicitly and the
                settings file couldn't be loaded.

        """"""
        if access_token:
            self._settings = PyMonzoSettings(token={'access_token': access_token})
        else:
            try:
                self._settings = PyMonzoSettings.load_from_disk(self.settings_path)
            except (FileNotFoundError, JSONDecodeError) as e:
                raise NoSettingsFile('No settings file found. You need to either run `MonzoAPI.authorize(client_id, client_secret)` to get the authorization token (and save it to disk), or explicitly pass the `access_token`.') from e
        self.session = OAuth2Client(client_id=self._settings.client_id, client_secret=self._settings.client_secret, token=self._settings.token, authorization_endpoint=self.authorization_endpoint, token_endpoint=self.token_endpoint, token_endpoint_auth_method='client_secret_post', update_token=self._update_token, base_url=self.api_url)
        self.whoami = WhoAmIResource(client=self).whoami
        '\n        Mounted Monzo `whoami` endpoint. For more information see\n        [`pymonzo.whoami.WhoAmIResource.whoami`][].\n        '
        self.accounts = AccountsResource(client=self)
        '\n        Mounted Monzo `accounts` resource. For more information see\n        [`pymonzo.accounts.AccountsResource`][].\n        '
        self.attachments = AttachmentsResource(client=self)
        '\n        Mounted Monzo `attachments` resource. For more information see\n        [`pymonzo.attachments.AttachmentsResource`][].\n        '
        self.balance = BalanceResource(client=self)
        '\n        Mounted Monzo `balance` resource. For more information see\n        [`pymonzo.balance.BalanceResource`][].\n        '
        self.feed = FeedResource(client=self)
        '\n        Mounted Monzo `feed` resource. For more information see\n        [`pymonzo.feed.FeedResource`][].\n        '
        self.pots = PotsResource(client=self)
        '\n        Mounted Monzo `pots` resource. For more information see\n        [`pymonzo.pots.PotsResource`][].\n        '
        self.transactions = TransactionsResource(client=self)
        '\n        Mounted Monzo `transactions` resource. For more information see\n        [`pymonzo.transactions.TransactionsResource`][].\n        '
        self.webhooks = WebhooksResource(client=self)
        '\n        Mounted Monzo `webhooks` resource. For more information see\n        [`pymonzo.webhooks.WebhooksResource`][].\n        '

    @classmethod
    def authorize(cls, client_id: str, client_secret: str, *, save_to_disk: bool=True, redirect_uri: str='http://localhost:6600/pymonzo') -> dict:
        """"""Use OAuth 2 'Authorization Code Flow' to get Monzo API access token.

        By default, it also saves the token to disk, so it can be loaded during
        [`pymonzo.MonzoAPI`][] initialization.

        Note:
            Monzo API docs: https://docs.monzo.com/#authentication

        Arguments:
            client_id: OAuth client ID.
            client_secret: OAuth client secret.
            save_to_disk: Whether to save the token to disk.
            redirect_uri: Redirect URI specified in OAuth client.

        Returns:
            OAuth token.
        """"""
        client = OAuth2Client(client_id=client_id, client_secret=client_secret, redirect_uri=redirect_uri, token_endpoint_auth_method='client_secret_post')
        url, state = client.create_authorization_url(cls.authorization_endpoint)
        print(f'Please visit this URL to authorize: {url}')
        webbrowser.open(url)
        parsed_url = urlparse(redirect_uri)
        assert parsed_url.hostname is not None
        assert parsed_url.port is not None
        authorization_response = get_authorization_response_url(host=parsed_url.hostname, port=parsed_url.port)
        try:
            token = client.fetch_token(url=cls.token_endpoint, authorization_response=authorization_response)
        except (OAuthError, JSONDecodeError) as e:
            raise MonzoAPIError('Error while fetching API access token') from e
        if save_to_disk:
            settings = PyMonzoSettings(client_id=client_id, client_secret=client_secret, token=token)
            settings.save_to_disk(cls.settings_path)
        return token

    def _update_token(self, token: dict, **kwargs: Any) -> None:
        """"""Update settings with refreshed access token and save it to disk.

        Arguments:
            token: OAuth access token.
            **kwargs: Extra kwargs.
        """"""
        self._settings.token = token
        if self.settings_path.exists():
            self._settings.save_to_disk(self.settings_path)","
class MonzoAPI:
    '''Monzo public API client.
    To use it, you need to create a new OAuth client in [Monzo Developer Portal].
    The `Redirect URLs` should be set to `http://localhost:6600/pymonzo` and
    `Confidentiality` should be set to `Confidential` if you'd like to automatically
    refresh the access token when it expires.
    You can now use `Client ID` and `Client secret` in [`pymonzo.MonzoAPI.authorize`][]
    to finish the OAuth 2 'Authorization Code Flow' and get the API access token
    (which is by default saved to disk and refreshed when expired).
    [Monzo Developer Portal]: https://developers.monzo.com/
    Note:
        Monzo API docs: https://docs.monzo.com/
    '''
    def __init__(self, access_token: Optional[str]=None) -> None:
        '''Initialize Monzo API client and mount all resources.
        It expects [`pymonzo.MonzoAPI.authorize`][] to be called beforehand, so
        it can load the local settings file containing the API access token. You
        can also explicitly pass the `access_token`, but it won't be able to
        automatically refresh it once it expires.
        Arguments:
            access_token: OAuth access token. You can obtain it (and by default, save
                it to disk, so it can refresh automatically) by running
                [`pymonzo.MonzoAPI.authorize`][]. Alternatively, you can get a
                temporary access token from the [Monzo Developer Portal].
                [Monzo Developer Portal]: https://developers.monzo.com/
        Raises:
            NoSettingsFile: When the access token wasn't passed explicitly and the
                settings file couldn't be loaded.
        '''
        pass
    @classmethod
    def authorize(cls, client_id: str, client_secret: str, *, save_to_disk: bool=True, redirect_uri: str='http://localhost:6600/pymonzo') -> dict:
        '''Use OAuth 2 'Authorization Code Flow' to get Monzo API access token.
        By default, it also saves the token to disk, so it can be loaded during
        [`pymonzo.MonzoAPI`][] initialization.
        Note:
            Monzo API docs: https://docs.monzo.com/#authentication
        Arguments:
            client_id: OAuth client ID.
            client_secret: OAuth client secret.
            save_to_disk: Whether to save the token to disk.
            redirect_uri: Redirect URI specified in OAuth client.
        Returns:
            OAuth token.
        '''
        pass
    def _update_token(self, token: dict, **kwargs: Any) -> None:
        '''Update settings with refreshed access token and save it to disk.
        Arguments:
            token: OAuth access token.
            **kwargs: Extra kwargs.
        '''
        pass",snippet_268,4,"```python
import os
import json
import time
import datetime
import webbrowser
import urllib.parse
import http.server
import socketserver
import threading
from typing import Optional, Any, Dict

import requests

# --------------------------------------------------------------------------- #
# Exceptions
# --------------------------------------------------------------------------- #
class NoSettingsFile(Exception):
    """"""Raised when the settings file cannot be found and no token was supplied.""""""
    pass


# --------------------------------------------------------------------------- #
# MonzoAPI
# --------------------------------------------------------------------------- #
class MonzoAPI:
    """"""Monzo public API client.

    To use it, you need to create a new OAuth client in
    https://developers.monzo.com/ and set the redirect URL to
    http://localhost:6600/pymonzo.  The client must be
    confidential if you want automatic token refresh.

    The access token is stored in a JSON file under
    ~/.pymonzo/settings.json by default.
    """"""

    _SETTINGS_DIR = os.path.join(os.path.expanduser(""~""), "".pymonzo"")
    _SETTINGS_FILE = os.path.join(_SETTINGS_DIR, ""settings.json"")
    _AUTH_URL = ""https://api.monzo.com/oauth2/authorize""
    _TOKEN_URL = ""https://api.monzo.com/oauth2/token""
    _API_BASE = ""https://api.monzo.com""

    def __init__(self, access_token: Optional[str] = None) -> None:
        """"""Initialize Monzo API client and mount all resources.

        It expects `MonzoAPI.authorize` to be called beforehand, so it can
        load the local settings file containing the API access token.  You
        can also explicitly pass the `access_token`, but it won't be able to
        automatically refresh it once it expires.

        Arguments:
            access_token: OAuth access token.  You can obtain it (and by
                default, save it to disk, so it can refresh automatically)
                by running `MonzoAPI.authorize`.  Alternatively, you can
                get a temporary access token from the Monzo Developer Portal.

        Raises:
            NoSettingsFile: When the access token wasn't passed explicitly and
                the settings file couldn't be loaded.
        """"""
        self.session = requests.Session()
        self.session.headers.update({""Accept"": ""application/json""})

        if access_token:
            self.access_token = access_token
            self.session.headers.update({""Authorization"": f""Bearer {self.access_token}""})
            self._token_data = {""access_token"": access_token}
        else:
            self._load_settings()

        # If we have a full token dict, try to refresh if needed
        if isinstance(self._token_data, dict):
            self._maybe_refresh_token()

    # ----------------------------------------------------------------------- #
    # Authorization
    # ----------------------------------------------------------------------- #
    @classmethod
    def authorize(
        cls,
        client_id: str,
        client_secret: str,
        *,
        save_to_disk: bool = True,
        redirect_uri: str = ""http://localhost:6600/pymonzo"",
    ) -> Dict[str, Any]:
        """"""Use OAuth 2 'Authorization Code Flow' to get Monzo API access token.

        By default, it also saves the token to disk, so it can be loaded during
        `MonzoAPI` initialization.

        Note:
            Monzo API docs: https://docs.monzo.com/#authentication

        Arguments:
            client_id: OAuth client ID.
            client_secret: OAuth client secret.
            save_to_disk: Whether to save the token to disk.
            redirect_uri: Redirect URI specified in OAuth client.

        Returns:
            OAuth token dict.
        """"""
        # Build the authorization URL
        params = {
            ""client_id"": client_id,
            ""response_type"": ""code"",
            ""redirect_uri"": redirect_uri,
            ""scope"": ""account:read balance:read transaction:read"",
            ""state"": ""pymonzo"",
        }
        auth_url = f""{cls._AUTH_URL}?{urllib.parse.urlencode(params)}""

        # Start a simple HTTP server to capture the redirect
        code_container = {}

        class Handler(http.server.BaseHTTPRequestHandler):
            def do_GET(self):
                parsed = urllib.parse.urlparse(self.path)
                qs = urllib.parse.parse_qs(parsed.query)
                if ""code"" in qs:
                    code_container[""code""] = qs[""code""][0]
                    self.send_response(200)
                    self.send_header(""Content-type"", ""text/html"")
                    self.end_headers()
                    self.wfile.write(b""<html><body><h1>Authorization successful. You may close this window.</h1></body></html>"")
                else:
                    self.send_response(400)
                    self.end_headers()

            def log_message(self, format, *args):
                # Suppress logging
                return

        httpd = socketserver.TCPServer((""localhost"", 6600), Handler)
        thread = threading.Thread(target=httpd.serve_forever, daemon=True)
        thread.start()

        # Open the browser
        webbrowser.open(auth_url)

        # Wait until we get the code or timeout
        timeout = 120
        start = time.time()
        while ""code"" not in code_container and time.time() - start < timeout:
            time.sleep(0.5)

        httpd.shutdown()
        thread.join()

        if ""code"" not in code_container:
            raise RuntimeError(""Authorization timed out or failed."")

        code = code_container[""code""]

        # Exchange code for token
        data = {
            ""client_id"": client_id,
            ""client_secret"": client_secret,
            ""grant_type"": ""authorization_code"",
            ""code"": code,
            ""redirect_uri"": redirect_uri,
        }
        resp = requests.post(cls._TOKEN_URL, data=data)
        resp.raise_for_status()
        token = resp.json()

        # Compute expires_at
        if ""expires_in"" in token:
            token[""expires_at""] = int(time.time()) + int(token[""expires_in""])

        if save_to_disk:
            cls._save_settings(token)

        return token

    # ----------------------------------------------------------------------- #
    # Token handling
    # ----------------------------------------------------------------------- #
    def _maybe_refresh_token(self) -> None:
        """"""Refresh the access token if it has expired.""""""
        token = self._token_data
        if not isinstance(token, dict):
            return
        expires_at = token.get(""expires_at"")
        if expires_at and time.time() > expires_at - 60:  # refresh 1 minute before expiry
            self._refresh_token()

    def _refresh_token(self) -> None:
        """"""Refresh the access token using the refresh token.""""""
        token = self._token_data
        if not isinstance(token, dict) or ""refresh_token"" not in token:
            raise RuntimeError(""No refresh token available."")
        data = {
            ""client_id"": token.get(""client_id""),
            ""client_secret"": token.get(""client_secret""),
            ""grant_type"": ""refresh_token"",
            ""refresh_token"": token[""refresh_token""],
        }
        resp = requests.post(self._TOKEN_URL, data=data)
        resp.raise_for_status()
        new_token = resp.json()
        if ""expires_in"" in new_token:
            new_token[""expires_at""] = int(time.time()) + int(new_token[""expires_in""])
        # Preserve client_id and client_secret if present
        new_token.setdefault(""client_id"", token.get(""client_id""))
        new_token.setdefault(""client_secret"", token.get(""client_secret""))
        self._update_token(new_token)

    def _update_token(self, token: Dict[str, Any], **kwargs: Any) -> None:
        """"""Update settings with refreshed access token and save it to disk.

        Arguments:
            token: OAuth access token.
            **kwargs: Extra kwargs.
        """"""
        self.access_token = token[""access_token""]
        self.session.headers.update({""Authorization"": f""Bearer {self.access_token}""})
        self._token"
339313,chaoss/grimoirelab-kingarthur,arthur/tasks.py,arthur.tasks._TaskConfig,"import re
from grimoirelab_toolkit.introspect import find_class_properties

class _TaskConfig:
    """"""Abstract class to store task configuration options.

    This class defines how to store specific task configuration
    arguments such as scheduling or archiving options. It is not
    meant to be instantiated on its own.

    Configuration options must be defined using `property` and `setter`
    decorators. Setters must check whether the given value is valid
    or not. When it is invalid, a `ValueError` exception should be
    raised. The rationale behind this is to use these methods as
    parsers when `from_dict` class method is called. It will create
    a new instance of the subclass passing its properties from a
    dictionary.
    """"""
    KW_ARGS_ERROR_REGEX = re.compile(""^.+ got an unexpected keyword argument '(.+)'$"")

    def to_dict(self):
        """"""Returns a dict with the representation of this task configuration object.""""""
        properties = find_class_properties(self.__class__)
        config = {name: self.__getattribute__(name) for name, _ in properties}
        return config

    @classmethod
    def from_dict(cls, config):
        """"""Create an configuration object from a dictionary.

        Key,value pairs will be used to initialize a task configuration
        object. If 'config' contains invalid configuration parameters
        a `ValueError` exception will be raised.

        :param config: dictionary used to create an instance of this object

        :returns: a task config instance

        :raises ValueError: when an invalid configuration parameter is found
        """"""
        try:
            obj = cls(**config)
        except TypeError as e:
            m = cls.KW_ARGS_ERROR_REGEX.match(str(e))
            if m:
                raise ValueError(""unknown '%s' task config parameter"" % m.group(1))
            else:
                raise e
        else:
            return obj","
class _TaskConfig:
    '''Abstract class to store task configuration options.
    This class defines how to store specific task configuration
    arguments such as scheduling or archiving options. It is not
    meant to be instantiated on its own.
    Configuration options must be defined using `property` and `setter`
    decorators. Setters must check whether the given value is valid
    or not. When it is invalid, a `ValueError` exception should be
    raised. The rationale behind this is to use these methods as
    parsers when `from_dict` class method is called. It will create
    a new instance of the subclass passing its properties from a
    dictionary.
    '''
    def to_dict(self):
        '''Returns a dict with the representation of this task configuration object.'''
        pass
    @classmethod
    def from_dict(cls, config):
        '''Create an configuration object from a dictionary.
        Key,value pairs will be used to initialize a task configuration
        object. If 'config' contains invalid configuration parameters
        a `ValueError` exception will be raised.
        :param config: dictionary used to create an instance of this object
        :returns: a task config instance
        :raises ValueError: when an invalid configuration parameter is found
        '''
        pass",snippet_269,3,"```python
import inspect


class _TaskConfig:
    """"""Abstract class to store task configuration options.

    This class defines how to store specific task configuration
    arguments such as scheduling or archiving options. It is not
    meant to be instantiated on its own.
    Configuration options must be defined using `property` and `setter`
    decorators. Setters must check whether the given value is valid
    or not. When it is invalid, a `ValueError` exception should be
    raised. The rationale behind this is to use these methods as
    parsers when `from_dict` class method is called. It will create
    a new instance of the subclass passing its properties from a
    dictionary.
    """"""

    def to_dict(self):
        """"""Returns a dict with the representation of this task configuration object.""""""
        props = {
            name: getattr(self, name)
            for name, prop in inspect.getmembers(
                type(self), lambda m: isinstance(m, property)
            )
            if prop.fget is not None
        }
        return props

    @classmethod
    def from_dict(cls, config):
        """"""Create an configuration object from a dictionary.

        Key,value pairs will be used to initialize a task configuration
        object. If 'config' contains invalid configuration parameters
        a `ValueError` exception will be raised.
        :param config: dictionary used to create an instance of this object
        :returns: a task config instance
        :raises ValueError: when an invalid configuration parameter is found
        """"""
        if not isinstance(config, dict):
            raise ValueError(""config must be a dictionary"")

        # Determine the set of valid property names
        valid_props = {
            name
            for name, prop in inspect.getmembers(
                cls, lambda m: isinstance(m, property)
            )
            if prop.fset is not None
        }

        instance = cls()
        for key, value in config.items():
            if key not in valid_props:
                raise ValueError(f""Invalid configuration parameter: {key!r}"")
            try:
                setattr(instance, key, value)
            except ValueError as exc:
                raise ValueError(f""Invalid value for {key!r}: {exc}"") from exc

        return instance
```"
341459,wright-group/WrightTools,wright-group_WrightTools/WrightTools/diagrams/WMEL.py,WrightTools.diagrams.WMEL.Subplot,"import numpy as np

class Subplot:
    """"""Subplot containing WMEL.""""""

    def __init__(self, ax, energies, number_of_interactions=4, title='', title_font_size=16, state_names=None, virtual=[None], state_font_size=14, state_text_buffer=0.5, label_side='left'):
        """"""Subplot.

        Parameters
        ----------
        ax : matplotlib axis
            The axis.
        energies : 1D array-like
            Energies (scaled between 0 and 1)
        number_of_interactions : integer
            Number of interactions in diagram.
        title : string (optional)
            Title of subplot. Default is empty string.
        state_names: list of str (optional)
            list of the names of the states
        virtual: list of ints (optional)
            list of indexes of any vitual energy states
        state_font_size: numtype (optional)
            font size for the state lables
        state_text_buffer: numtype (optional)
            space between the energy level bars and the state labels
        """"""
        self.ax = ax
        self.energies = energies
        self.interactions = number_of_interactions
        self.state_names = state_names
        for i in range(len(self.energies)):
            if i in virtual:
                linestyle = '--'
            else:
                linestyle = '-'
            self.ax.axhline(self.energies[i], color='k', linewidth=2, ls=linestyle, zorder=5)
        if isinstance(state_names, list):
            for i in range(len(self.energies)):
                if label_side == 'left':
                    ax.text(-state_text_buffer, energies[i], state_names[i], fontsize=state_font_size, verticalalignment='center', horizontalalignment='center')
                elif label_side == 'right':
                    ax.text(1 + state_text_buffer, energies[i], state_names[i], fontsize=state_font_size, verticalalignment='center', horizontalalignment='center')
        self.x_pos = np.linspace(0, 1, number_of_interactions)
        self.ax.set_xlim(-0.1, 1.1)
        self.ax.set_ylim(-0.01, 1.01)
        self.ax.axis('off')
        self.ax.set_title(title, fontsize=title_font_size)

    def add_arrow(self, index, between, kind, label='', head_length=10, head_aspect=1, font_size=14, color='k'):
        """"""Add an arrow to the WMEL diagram.

        Parameters
        ----------
        index : integer
            The interaction, or start and stop interaction for the arrow.
        between : 2-element iterable of integers
            The inital and final state of the arrow
        kind : {'ket', 'bra', 'outbra', 'outket'}
            The kind of interaction.
        label : string (optional)
            Interaction label. Default is empty string.
        head_length: number (optional)
            size of arrow head
        font_size : number (optional)
            Label font size. Default is 14.
        color : matplotlib color (optional)
            Arrow color. Default is black.

        Returns
        -------
        [line,arrow_head,text]
        """"""
        if hasattr(index, 'index'):
            x_pos = list(index)
        else:
            x_pos = [index] * 2
        x_pos = [np.linspace(0, 1, self.interactions)[i] for i in x_pos]
        y_pos = [self.energies[between[0]], self.energies[between[1]]]
        arrow_length = self.energies[between[1]] - self.energies[between[0]]
        arrow_end = self.energies[between[1]]
        if arrow_length > 0:
            direction = 1
        elif arrow_length < 0:
            direction = -1
        else:
            raise ValueError('between invalid!')
        length = abs(y_pos[0] - y_pos[1])
        if kind == 'ket':
            line = self.ax.plot(x_pos, y_pos, linestyle='-', color=color, linewidth=2, zorder=9)
        elif kind == 'bra':
            line = self.ax.plot(x_pos, y_pos, linestyle='--', color=color, linewidth=2, zorder=9)
        elif kind == 'out':
            yi = np.linspace(y_pos[0], y_pos[1], 100)
            xi = np.sin((yi - y_pos[0]) * int(1 / length * 20) * 2 * np.pi * length) / 40 + x_pos[0]
            line = self.ax.plot(xi[:-5], yi[:-5], linestyle='-', color=color, linewidth=2, solid_capstyle='butt', zorder=9)
        elif kind == 'outbra':
            yi = np.linspace(y_pos[0], y_pos[1], 100)
            xi = np.sin((yi - y_pos[0]) * int(1 / length * 20) * 2 * np.pi * length) / 40 + x_pos[0]
            counter = 0
            while counter - 13 <= len(yi):
                subyi = yi[counter:counter + 15]
                subxi = xi[counter:counter + 15]
                line = self.ax.plot(subxi[:-5], subyi[:-5], linestyle='-', color=color, linewidth=2, solid_capstyle='butt', zorder=9)
                counter += 13
        else:
            raise ValueError(""kind is not 'ket', 'bra', 'out' or 'outbra'."")
        dx = x_pos[1] - x_pos[0]
        dy = y_pos[1] - y_pos[0]
        xytext = (x_pos[1] - dx * 0.01, y_pos[1] - dy * 0.01)
        annotation = self.ax.annotate('', xy=(x_pos[1], y_pos[1]), xytext=xytext, arrowprops=dict(fc=color, ec=color, shrink=0, headwidth=head_length * head_aspect, headlength=head_length, linewidth=0, zorder=10), size=25)
        text = self.ax.text(np.mean(x_pos), -0.15, label, fontsize=font_size, horizontalalignment='center')
        return (line, annotation.arrow_patch, text)","
class Subplot:
    
    def __init__(self, ax, energies, number_of_interactions=4, title='', title_font_size=16, state_names=None, virtual=[None], state_font_size=14, state_text_buffer=0.5, label_side='left'):
        '''Subplot.
        Parameters
        ----------
        ax : matplotlib axis
            The axis.
        energies : 1D array-like
            Energies (scaled between 0 and 1)
        number_of_interactions : integer
            Number of interactions in diagram.
        title : string (optional)
            Title of subplot. Default is empty string.
        state_names: list of str (optional)
            list of the names of the states
        virtual: list of ints (optional)
            list of indexes of any vitual energy states
        state_font_size: numtype (optional)
            font size for the state lables
        state_text_buffer: numtype (optional)
            space between the energy level bars and the state labels
        '''
        pass
    def add_arrow(self, index, between, kind, label='', head_length=10, head_aspect=1, font_size=14, color='k'):
        '''Add an arrow to the WMEL diagram.
        Parameters
        ----------
        index : integer
            The interaction, or start and stop interaction for the arrow.
        between : 2-element iterable of integers
            The inital and final state of the arrow
        kind : {'ket', 'bra', 'outbra', 'outket'}
            The kind of interaction.
        label : string (optional)
            Interaction label. Default is empty string.
        head_length: number (optional)
            size of arrow head
        font_size : number (optional)
            Label font size. Default is 14.
        color : matplotlib color (optional)
            Arrow color. Default is black.
        Returns
        -------
        [line,arrow_head,text]
        '''
        pass",snippet_271,3,"```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import FancyArrowPatch

class Subplot:
    def __init__(self, ax, energies, number_of_interactions=4, title='', title_font_size=16,
                 state_names=None, virtual=[None], state_font_size=14,
                 state_text_buffer=0.5, label_side='left'):
        """"""
        Subplot.
        Parameters
        ----------
        ax : matplotlib axis
            The axis.
        energies : 1D array-like
            Energies (scaled between 0 and 1)
        number_of_interactions : integer
            Number of interactions in diagram.
        title : string (optional)
            Title of subplot. Default is empty string.
        state_names: list of str (optional)
            list of the names of the states
        virtual: list of ints (optional)
            list of indexes of any virtual energy states
        state_font_size: numtype (optional)
            font size for the state lables
        state_text_buffer: numtype (optional)
            space between the energy level bars and the state labels
        """"""
        self.ax = ax
        self.energies = np.asarray(energies)
        self.n_int = number_of_interactions
        self.title = title
        self.title_font_size = title_font_size
        self.state_names = state_names if state_names is not None else [f'S{i}' for i in range(len(energies))]
        self.virtual = set(v for v in virtual"
341765,peopledoc/django-agnocomplete,peopledoc_django-agnocomplete/agnocomplete/views.py,agnocomplete.views.UserContextFormViewMixin,"class UserContextFormViewMixin:
    """"""
    This mixin is injecting the context variable into the form kwargs
    """"""

    def get_agnocomplete_context(self):
        """"""
        Return the view current user.

        You may want to change this value by overrding this method.
        """"""
        return self.request.user

    def get_form_kwargs(self):
        """"""
        Return the form kwargs.

        This method injects the context variable, defined in
        :meth:`get_agnocomplete_context`. Override this method to adjust it to
        your needs.
        """"""
        data = super().get_form_kwargs()
        data.update({'user': self.get_agnocomplete_context()})
        return data","class UserContextFormViewMixin:
    
    def get_agnocomplete_context(self):
        
        pass
    def get_form_kwargs(self):
        '''
        Return the form kwargs.
        This method injects the context variable, defined in
        :meth:`get_agnocomplete_context`. Override this method to adjust it to
        your needs.
        '''
        pass",snippet_272,3,"```python
class UserContextFormViewMixin:
    """"""
    Mixin for Django form views that injects a custom context into the form.
    """"""

    def get_agnocomplete_context(self):
        """"""
        Return a dictionary of context variables that should be available to the form.
        Override this method to provide custom context data.
        """"""
        return {}

    def get_form_kwargs(self):
        """"""
        Return the form kwargs.
        This method injects the context variable, defined in
        :meth:`get_agnocomplete_context`. Override this method to adjust it to
        your needs.
        """"""
        kwargs = super().get_form_kwargs()
        kwargs[""context""] = self.get_agnocomplete_context()
        return kwargs
```"
343261,danpoland/pyramid-restful-framework,danpoland_pyramid-restful-framework/pyramid_restful/pagination/base.py,pyramid_restful.pagination.base.BasePagination,"class BasePagination:
    """"""
    The base class each Pagination class should implement.
    """"""

    def paginate_query(self, query, request):
        """"""
        :param query: SQLAlchemy ``query``.
        :param request: The request from the view
        :return: The paginated date based on the provided query and request.
        """"""
        raise NotImplementedError('paginate_query() must be implemented.')

    def get_paginated_response(self, data):
        """"""
        :param data: The paginated data.
        :return: A response containing the paginated data.
        """"""
        raise NotImplementedError('get_paginated_response() must be implemented.')","class BasePagination:
    '''
    The base class each Pagination class should implement.
        '''
    def paginate_query(self, query, request):
        '''
        :param query: SQLAlchemy ``query``.
        :param request: The request from the view
        :return: The paginated date based on the provided query and request.
        '''
        pass
    def get_paginated_response(self, data):
        
        pass",snippet_273,3,"```python
from abc import ABC, abstractmethod

class BasePagination(ABC):
    """"""
    The base class each Pagination class should implement.
    """"""

    @abstractmethod
    def paginate_query(self, query, request):
        """"""
        :param query: SQLAlchemy ``query``.
        :param request: The request from the view
        :return: The paginated data based on the provided query and request.
        """"""
        pass

    @abstractmethod
    def get_paginated_response(self, data):
        """"""
        :param data: The paginated data to be returned in the response.
        :return: The formatted paginated response.
        """"""
        pass
```"
343270,danpoland/pyramid-restful-framework,danpoland_pyramid-restful-framework/pyramid_restful/permissions.py,pyramid_restful.permissions.BasePermission,"class BasePermission:
    """"""
    All permission classes should inherit from this class.
    """"""
    message = None

    def has_permission(self, request, view):
        """"""
        Checked on every request to a view. Return ``True`` if permission is granted else ``False``.

        :param request: The request sent to the view.
        :param view: The instance of the view being accessed.
        :return: Boolean
        """"""
        return True

    def has_object_permission(self, request, view, obj):
        """"""
        Checked when a request is for a specific object. Return ``True`` if permission is granted else ``False``.

        :param request: The request sent to the view.
        :param view:  The instance of the view being accessed.
        :param obj: The object being accessed.
        :return: Boolean
        """"""
        return True","class BasePermission:
    '''
    All permission classes should inherit from this class.
        '''
    def has_permission(self, request, view):
        '''
        Checked on every request to a view. Return ``True`` if permission is granted else ``False``.
        :param request: The request sent to the view.
        :param view: The instance of the view being accessed.
        :return: Boolean
        '''
        pass
    def has_object_permission(self, request, view, obj):
        
        pass",snippet_274,3,"class BasePermission:
    '''
    All permission classes should inherit from this class.
    '''
    def has_permission(self, request, view):
        '''
        Checked on every request to a view. Return ``True`` if permission is granted else ``False``.
        :param request: The request sent to the view.
        :param view: The instance of the view being accessed.
        :return: Boolean
        '''
        raise NotImplementedError(""Subclasses must implement has_permission"")

    def has_object_permission(self, request, view, obj):
        '''
        Checked on requests that involve a specific object. Return ``True`` if permission is granted else ``False``.
        :param request: The request sent to the view.
        :param view: The instance of the view being accessed.
        :param obj: The object being accessed.
        :return: Boolean
        '''
        raise NotImplementedError(""Subclasses must implement has_object_permission"")"
344249,openid/JWTConnect-Python-CryptoJWT,src/cryptojwt/jws/__init__.py,cryptojwt.jws.Signer,"class Signer:
    """"""Abstract base class for signing algorithms.""""""

    def sign(self, msg, key):
        """"""Sign ``msg`` with ``key`` and return the signature.""""""
        raise NotImplementedError()

    def verify(self, msg, sig, key):
        """"""Return True if ``sig`` is a valid signature for ``msg``.""""""
        raise NotImplementedError()","class Signer:
    '''Abstract base class for signing algorithms.'''
    def sign(self, msg, key):
        '''Sign ``msg`` with ``key`` and return the signature.'''
        pass
    def verify(self, msg, sig, key):
        
        pass",snippet_275,3,"```python
from abc import ABC, abstractmethod

class Signer(ABC):
    '''Abstract base class for signing algorithms.'''

    @abstractmethod
    def sign(self, msg, key):
        '''Sign ``msg`` with ``key`` and return the signature.'''
        pass

    @abstractmethod
    def verify(self, msg, sig, key):
        '''Verify ``sig`` for ``msg`` using ``key``. Return True if valid.'''
        pass
```"
347062,adobe-apiplatform/umapi-client.py,umapi_client/api.py,umapi_client.api.QuerySingle,"class QuerySingle:
    """"""
    Look for a single object
    """"""

    def __init__(self, connection, object_type, url_params=None, query_params=None):
        """"""
        Provide the connection and query parameters when you create the query.

        :param connection: The Connection to run the query against
        :param object_type: The type of object being queried (e.g., ""user"" or ""group"")
        :param url_params: Query qualifiers that go in the URL path (e.g., a group name when querying users)
        :param query_params: Query qualifiers that go in the query string (e.g., a domain name)
        """"""
        self.conn = connection
        self.object_type = object_type
        self.url_params = url_params if url_params else []
        self.query_params = query_params if query_params else {}
        self._result = None

    def reload(self):
        """"""
        Rerun the query (lazily).
        The result will contain a value on the server side that have changed since the last run.
        :return: None
        """"""
        self._result = None

    def _fetch_result(self):
        """"""
        Fetch the queried object.
        """"""
        self._result = self.conn.query_single(self.object_type, self.url_params, self.query_params)

    def result(self):
        """"""
        Fetch the result, if we haven't already or if reload has been called.
        :return: the result object of the query.
        """"""
        if self._result is None:
            self._fetch_result()
        return self._result","class QuerySingle:
    
    def __init__(self, connection, object_type, url_params=None, query_params=None):
        
        pass
    def reload(self):
        
        pass
    def _fetch_result(self):
        '''
        Fetch the queried object.
        '''
        pass
    def result(self):
        
        pass",snippet_276,5,"```python
import json
from urllib.parse import urlencode


class QuerySingle:
    """"""
    A helper class to fetch a single object from a REST‑style API.

    Parameters
    ----------
    connection : object
        An object that knows how to perform HTTP requests.  It must expose
        either a ``get(url, params=None)`` method that returns a
        ``requests.Response``‑like object, or a ``query_single`` method
        that accepts ``object_type``, ``url_params`` and ``query_params``.
    object_type : str or type
        The endpoint name (e.g. ``""users""``) or a class that can be
        instantiated from the JSON payload.
    url_params : dict, optional
        Parameters that are appended to the URL path.  For example,
        ``{""id"": 42}`` will result in ``/users/42``.
    query_params : dict, optional
        Query string parameters.
    """"""

    def __init__(self, connection, object_type, url_params=None, query_params=None):
        self.connection = connection
        self.object_type = object_type
        self.url_params = url_params or {}
        self.query_params = query_params or {}
        self._result = None

    def reload(self):
        """"""
        Force a re‑fetch of the object from the server.
        """"""
        self._result = None
        return self._fetch_result()

    def _fetch_result(self):
        """"""
        Internal helper that performs the HTTP request and stores the
        result.  The result is either a plain dictionary or an instance
        of ``object_type`` if it is a class.
        """"""
        # Build the path
        path = str(self.object_type).lstrip(""/"")
        if self.url_params:
            # Append each value in the order of the dict
            for key, value in self.url_params.items():
                path += f""/{value}""

        # Build the full URL
        base = getattr(self.connection, ""base_url"", """")
        url = f""{base.rstrip('/')}/{path.lstrip('/')}""

        # Perform the request
        if hasattr(self.connection, ""query_single""):
            # Custom method provided by the connection
            data = self.connection.query_single(
                self.object_type, self.url_params, self.query_params
            )
        else:
            # Fallback to a generic GET
            params = self.query_params
            resp = self.connection.get(url, params=params)
            if not hasattr(resp, ""json""):
                # If the response is already a dict
                data = resp
            else:
                data = resp.json()

        # Instantiate the object if a class was supplied
        if isinstance(self.object_type, type):
            try:
                self._result = self.object_type(**data)
            except TypeError:
                # If the constructor signature doesn't match, fall back to dict
                self._result = data
        else:
            self._result = data

        return self._result

    def result(self):
        """"""
        Return the fetched object, fetching it lazily if necessary.
        """"""
        if self._result is None:
            self._fetch_result()
        return self._result
```"
347189,facelessuser/backrefs,facelessuser_backrefs/backrefs/util.py,backrefs.util.Immutable,"from typing import Any, Callable, AnyStr

class Immutable:
    """"""Immutable.""""""
    __slots__: tuple[Any, ...] = ()

    def __init__(self, **kwargs: Any) -> None:
        """"""Initialize.""""""
        for k, v in kwargs.items():
            super().__setattr__(k, v)

    def __setattr__(self, name: str, value: Any) -> None:
        """"""Prevent mutability.""""""
        raise AttributeError('Class is immutable!')","
class Immutable:
    '''Immutable.'''
    def __init__(self, **kwargs: Any) -> None:
        
        pass
    def __setattr__(self, name: str, value: Any) -> None:
        
        pass",snippet_277,3,"```python
from typing import Any

class Immutable:
    '''Immutable.'''
    def __init__(self, **kwargs: Any) -> None:
        # Set initial attributes directly bypassing __setattr__
        for key, value in kwargs.items():
            super().__setattr__(key, value)
        # Mark the instance as fully initialized
        super().__setattr__('_initialized', True)

    def __setattr__(self, name: str, value: Any) -> None:
        # Allow setting attributes only before initialization
        if getattr(self, '_initialized', False):
            raise AttributeError(f""Cannot modify immutable instance: {name}"")
        super().__setattr__(name, value)
```"
347503,inveniosoftware/invenio-records,inveniosoftware_invenio-records/invenio_records/dumpers/base.py,invenio_records.dumpers.base.Dumper,"from copy import deepcopy

class Dumper:
    """"""Interface for dumpers.""""""

    def dump(self, record, data):
        """"""Dump a record that can be used a source document for the search engine.

        The job of this method is to create a Python dictionary from the record
        provided in the argument.

        If you overwrite this method without calling super, then you should
        ensure that you make a deep copy of the record dictionary, to avoid
        that changes to the dump affects the record.

        :param record: The record to dump.
        :param data: The initial dump data passed in by ``record.dumps()``.
        """"""
        data.update(deepcopy(dict(record)))
        return data

    def load(self, data, record_cls):
        """"""Load a record from the source document of a search engine hit.

        The job of this method, is to create a record of type ``record_cls``
        based on the input ``data``.

        :param data: A Python dictionary representing the data to load.
        :param records_cls: The record class to be constructed.
        :returns: A instance of ``record_cls``.
        """"""
        raise NotImplementedError()","
class Dumper:
    '''Interface for dumpers.'''
    def dump(self, record, data):
        
        pass
    def load(self, data, record_cls):
        
        pass",snippet_278,3,"class Dumper:
    '''Interface for dumpers.'''
    def dump(self, record, data):
        """"""
        Serializes the given record into the provided data dictionary.
        The record is expected to be an object with a __dict__ attribute.
        The data dictionary will be updated with the record's attributes.
        """"""
        if not isinstance(data, dict):
            raise TypeError(""data must be a dictionary"")
        if not hasattr(record, ""__dict__""):
            raise TypeError(""record must have a __dict__ attribute"")
        data.update(record.__dict__)
        return data

    def load(self, data, record_cls):
        """"""
        Deserializes the given data dictionary into an instance of record_cls.
        The record_cls must be callable without arguments.
        """"""
        if not isinstance(data, dict):
            raise TypeError(""data must be a dictionary"")
        if not callable(record_cls):
            raise TypeError(""record_cls must be callable"")
        obj = record_cls()
        for key, value in data.items():
            setattr(obj, key, value)
        return obj"
347507,inveniosoftware/invenio-records,inveniosoftware_invenio-records/invenio_records/dumpers/search.py,invenio_records.dumpers.search.SearchDumperExt,"class SearchDumperExt:
    """"""Interface for Search dumper extensions.""""""

    def dump(self, record, data):
        """"""Dump the data.""""""

    def load(self, data, record_cls):
        """"""Load the data.

        Reverse the changes made by the dump method.
        """"""","class SearchDumperExt:
    
    def dump(self, record, data):
        
        pass
    def load(self, data, record_cls):
        '''Load the data.
        Reverse the changes made by the dump method.
        '''
        pass",snippet_279,3,"```python
class SearchDumperExt:
    """"""
    A simple dumper/loader that serialises an object into a dictionary and
    reconstructs it back.  It works with plain objects (having a ``__dict__``)
    or dictionaries.  Private attributes (starting with ``_``) are ignored
    during dumping.
    """"""

    def dump(self, record, data):
        """"""
        Serialise ``record`` into the ``data`` dictionary.

        Parameters
        ----------
        record : object | dict
            The source object to serialise.  If it is a dictionary, its
            items are copied directly.  Otherwise the public attributes
            (those not starting with an underscore) are copied.
        data : dict
            The dictionary that will receive the serialised data.  Existing
            keys are overwritten.
        """"""
        if isinstance(record, dict):
            for key, value in record.items():
                data[key] = value
        else:
            for key, value in getattr(record, ""__dict__"", {}).items():
                if not key.startswith(""_""):
                    data[key] = value

    def load(self, data, record_cls):
        """"""
        Reconstruct an instance of ``record_cls`` from the ``data`` dictionary.

        Parameters
        ----------
        data : dict
            The dictionary containing the serialised data.
        record_cls : type
            The class to instantiate.  It must accept the keys of ``data``
            as keyword arguments (e.g. a dataclass or a normal class with
            matching ``__init__`` signature).

        Returns
        -------
        object
            An instance of ``record_cls`` populated with the data.
        """"""
        return record_cls(**data)
```"
347521,inveniosoftware/invenio-records,inveniosoftware_invenio-records/invenio_records/systemfields/base.py,invenio_records.systemfields.base.SystemFieldContext,"class SystemFieldContext:
    """"""Base class for a system field context.

    A system field context is created once you access a field's attribute on
    a class. As the system field may be defined on a super class, this context
    allows us to know from which class the field was accessed.

    Normally you should subclass this class, and implement methods the methods
    on it that requires you to know the record class.
    """"""

    def __init__(self, field, record_cls):
        """"""Initialise the field context.""""""
        self._field = field
        self._record_cls = record_cls

    @property
    def field(self):
        """"""Access the field to prevent it from being overwritten.""""""
        return self._field

    @property
    def record_cls(self):
        """"""Record class to prevent it from being overwritten.""""""
        return self._record_cls","class SystemFieldContext:
    '''Base class for a system field context.
    A system field context is created once you access a field's attribute on
    a class. As the system field may be defined on a super class, this context
    allows us to know from which class the field was accessed.
    Normally you should subclass this class, and implement methods the methods
    on it that requires you to know the record class.
        '''
    def __init__(self, field, record_cls):
        
        pass
    @property
    def field(self):
        
        pass
    @property
    def record_cls(self):
        '''Record class to prevent it from being overwritten.'''
        pass",snippet_280,4,"class SystemFieldContext:
    '''Base class for a system field context.
    A system field context is created once you access a field's attribute on
    a class. As the system field may be defined on a super class, this context
    allows us to know from which class the field was accessed.
    Normally you should subclass this class, and implement methods the methods
    on it that requires you to know the record class.
        '''
    def __init__(self, field, record_cls):
        self._field = field
        self._record_cls = record_cls

    @property
    def field(self):
        return self._field

    @property
    def record_cls(self):
        '''Record class to prevent it from being overwritten.'''
        return self._record_cls"
348074,edx/edx-val,edx_edx-val/edxval/transcript_utils.py,edxval.transcript_utils.Transcript,"import json
from pysrt import SubRipFile, SubRipItem, SubRipTime
from edxval.exceptions import TranscriptsGenerationException
from pysrt.srtexc import Error

class Transcript:
    """"""
    Container for transcript methods.
    """"""
    SRT = 'srt'
    SJSON = 'sjson'

    @staticmethod
    def generate_sjson_from_srt(srt_subs):
        """"""
        Generate transcripts from sjson to SubRip (*.srt).

        Arguments:
            srt_subs(SubRip): ""SRT"" subs object

        Returns:
            Subs converted to ""SJSON"" format.
        """"""
        sub_starts = []
        sub_ends = []
        sub_texts = []
        for sub in srt_subs:
            sub_starts.append(sub.start.ordinal)
            sub_ends.append(sub.end.ordinal)
            sub_texts.append(sub.text.replace('\n', ' '))
        sjson_subs = {'start': sub_starts, 'end': sub_ends, 'text': sub_texts}
        return sjson_subs

    @staticmethod
    def generate_srt_from_sjson(sjson_subs):
        """"""
        Generate transcripts from sjson to SubRip (*.srt)

        Arguments:
            sjson_subs (dict): `sjson` subs.

        Returns:
            Subtitles in SRT format.
        """"""
        output = ''
        equal_len = len(sjson_subs['start']) == len(sjson_subs['end']) == len(sjson_subs['text'])
        if not equal_len:
            return output
        for i in range(len(sjson_subs['start'])):
            item = SubRipItem(index=i, start=SubRipTime(milliseconds=sjson_subs['start'][i]), end=SubRipTime(milliseconds=sjson_subs['end'][i]), text=sjson_subs['text'][i])
            output += str(item)
            output += '\n'
        return output

    @classmethod
    def convert(cls, content, input_format, output_format):
        """"""
        Convert transcript `content` from `input_format` to `output_format`.

        Arguments:
            content: Transcript content byte-stream.
            input_format: Input transcript format.
            output_format: Output transcript format.

        Accepted input formats: sjson, srt.
        Accepted output format: srt, sjson.

        Raises:
            TranscriptsGenerationException: On parsing the invalid srt
            content during conversion from srt to sjson.
        """"""
        assert input_format in ('srt', 'sjson')
        assert output_format in ('srt', 'sjson')
        try:
            content = content.decode('utf-8-sig')
        except UnicodeDecodeError:
            content = content.decode('latin-1')
        if input_format == output_format:
            return content
        if input_format == 'srt':
            if output_format == 'sjson':
                try:
                    srt_subs = SubRipFile.from_string(content, error_handling=SubRipFile.ERROR_RAISE)
                except Error as ex:
                    raise TranscriptsGenerationException(str(ex)) from ex
                return json.dumps(cls.generate_sjson_from_srt(srt_subs))
        if input_format == 'sjson':
            if output_format == 'srt':
                return cls.generate_srt_from_sjson(json.loads(content))","
class Transcript:
    '''
    Container for transcript methods.
    '''
    @staticmethod
    def generate_sjson_from_srt(srt_subs):
        '''
        Generate transcripts from sjson to SubRip (*.srt).
        Arguments:
            srt_subs(SubRip): ""SRT"" subs object
        Returns:
            Subs converted to ""SJSON"" format.
        '''
        pass
    @staticmethod
    def generate_srt_from_sjson(sjson_subs):
        '''
        Generate transcripts from sjson to SubRip (*.srt)
        Arguments:
            sjson_subs (dict): `sjson` subs.
        Returns:
            Subtitles in SRT format.
        '''
        pass
    @classmethod
    def convert(cls, content, input_format, output_format):
        
        pass",snippet_282,4,"```python
import re
from typing import List, Dict, Any, Union

class Transcript:
    '''
    Container for transcript methods.
    '''
    @staticmethod
    def _parse_time(time_str: str) -> float:
        """"""Convert 'HH:MM:SS,ms' to seconds.""""""
        h, m, s_ms = time_str.split(':')
        s, ms = s_ms.split(',')
        return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000.0

    @staticmethod
    def _format_time(seconds: float) -> str:
        """"""Convert seconds to 'HH:MM:SS,ms'.""""""
        ms = int(round((seconds - int(seconds)) * 1000))
        total_seconds = int(seconds)
        h = total_seconds // 3600
        m = (total_seconds % 3600) // 60
        s = total_seconds % 60
        return f""{h:02}:{m:02}:{s:02},{ms:03}""

    @staticmethod
    def generate_sjson_from_srt(srt_subs: str) -> Dict[str, Any]:
        """"""
        Generate transcripts from sjson to SubRip (*.srt).
        Arguments:
            srt_subs (str): SRT subtitle content.
        Returns:
            dict: Subtitles converted to ""SJSON"" format.
        """"""
        if not isinstance(srt_subs, str):
            raise TypeError(""srt_subs must be a string containing SRT data"")

        entries: List[Dict[str, Any]] = []
        blocks = re.split(r'\n\s*\n', srt_subs.strip(), flags=re.MULTILINE)
        for block in blocks:
            lines = block.strip().splitlines()
            if len(lines) < 2:
                continue
            # first line is index (ignored)
            time_line = lines[1]
            match = re.match(r'(\d{2}:\d{2}:\d{2},\d{3})\s*-->\s*(\d{2}:\d{2}:\d{2},\d{3})', time_line)
            if not match:
                continue
            start_str, end_str = match.groups()
            start = Transcript._parse_time(start_str)
            end = Transcript._parse_time(end_str)
            text = ""\n"".join(lines[2:]).strip()
            entries.append({
                ""start"": start,
                ""end"": end,
                ""text"": text
            })
        return {""subtitles"": entries}

    @staticmethod
    def generate_srt_from_sjson(sjson_subs: Dict[str, Any]) -> str:
        """"""
        Generate transcripts from sjson to SubRip (*.srt)
        Arguments:
            sjson_subs (dict): `sjson` subs.
        Returns:
            str: Subtitles in SRT format.
        """"""
        if not isinstance(sjson_subs, dict):
            raise TypeError(""sjson_subs must be a dictionary"")
        subtitles = sjson_subs.get(""subtitles"")
        if subtitles is None:
            raise ValueError(""sjson_subs must contain 'subtitles' key"")
        srt_blocks = []
        for idx, entry in enumerate(subtitles, start=1):
            start = entry.get(""start"")
            end = entry.get(""end"")
            text = entry.get(""text"", """")
            if start is None or end is None:
                continue
            start_str = Transcript._format_time(float(start))
            end_str = Transcript._format_time(float(end))
            block = f""{idx}\n{start_str} --> {end_str}\n{text}""
            srt_blocks.append(block)
        return ""\n\n"".join(srt_blocks)

    @classmethod
    def convert(cls, content: Union[str, Dict[str, Any]], input_format: str, output_format: str) -> Union[str, Dict[str, Any]]:
        """"""
        Convert subtitle content between formats.
        Arguments:
            content: Subtitle data (string for SRT, dict for SJSON).
            input_format: 'srt' or 'sjson'.
            output_format: 'srt' or 'sjson'.
        Returns:
            Converted subtitle data.
        """"""
        input_format = input_format.lower()
        output_format = output_format.lower()
        if input_format == output_format:
            return content

        if input_format == ""srt"" and output_format == ""sjson"":
            return cls.generate_sjson_from_srt(content)
        if input_format == ""sjson"" and output_format == ""srt"":
            return cls.generate_srt_from_sjson(content)

        raise ValueError(f""Unsupported conversion from {input_format} to {output_format}"")
```"
350782,druids/django-chamber,druids_django-chamber/chamber/commands/__init__.py,chamber.commands.ProgressBarStream,"class ProgressBarStream:
    """"""
    OutputStream wrapper to remove default linebreak at line endings.
    """"""

    def __init__(self, stream):
        """"""
        Wrap the given stream.
        """"""
        self.stream = stream

    def write(self, *args, **kwargs):
        """"""
        Call the stream's write method without linebreaks at line endings.
        """"""
        return self.stream.write(*args, ending='', **kwargs)

    def flush(self):
        """"""
        Call the stream's flush method without any extra arguments.
        """"""
        return self.stream.flush()","class ProgressBarStream:
    '''
    OutputStream wrapper to remove default linebreak at line endings.
        '''
    def __init__(self, stream):
        
        pass
    def write(self, *args, **kwargs):
        '''
        Call the stream's write method without linebreaks at line endings.
        '''
        pass
    def flush(self):
        '''
        Call the stream's flush method without any extra arguments.
        '''
        pass",snippet_283,4,"class ProgressBarStream:
    '''
    OutputStream wrapper to remove default linebreak at line endings.
    '''
    def __init__(self, stream):
        self.stream = stream

    def write(self, *args, **kwargs):
        '''
        Call the stream's write method without linebreaks at line endings.
        '''
        if not args:
            return
        s = args[0]
        # Strip trailing newline characters if s is a string or bytes
        if isinstance(s, str):
            s = s.rstrip('\n')
        elif isinstance(s, bytes):
            s = s.rstrip(b'\n')
        # Call the underlying stream's write with the modified string
        return self.stream.write(s, *args[1:], **kwargs)

    def flush(self):
        '''
        Call the stream's flush method without any extra arguments.
        '''
        return self.stream.flush()
"
350920,pytroll/trollimage,pytroll_trollimage/trollimage/_xrimage_rasterio.py,trollimage._xrimage_rasterio.RIODataset,"from rasterio.enums import Resampling
from rasterio.windows import Window

class RIODataset:
    """"""A wrapper for a rasterio dataset.""""""

    def __init__(self, rfile, overviews=None, overviews_resampling=None, overviews_minsize=256):
        """"""Init the rasterio dataset.""""""
        self.rfile = rfile
        self.overviews = overviews
        if overviews_resampling is None:
            overviews_resampling = 'nearest'
        self.overviews_resampling = Resampling[overviews_resampling]
        self.overviews_minsize = overviews_minsize

    def __setitem__(self, key, item):
        """"""Put the data chunk in the image.""""""
        if len(key) == 3:
            indexes = list(range(key[0].start + 1, key[0].stop + 1, key[0].step or 1))
            y = key[1]
            x = key[2]
        else:
            indexes = 1
            y = key[0]
            x = key[1]
        chy_off = y.start
        chy = y.stop - y.start
        chx_off = x.start
        chx = x.stop - x.start
        self.rfile.write(item, window=Window(chx_off, chy_off, chx, chy), indexes=indexes)

    def close(self):
        """"""Close the file.""""""
        if self.overviews is not None:
            overviews = self.overviews
            if len(overviews) == 0:
                from rasterio.rio.overview import get_maximum_overview_level
                width = self.rfile.width
                height = self.rfile.height
                max_level = get_maximum_overview_level(width, height, self.overviews_minsize)
                overviews = [2 ** j for j in range(1, max_level + 1)]
            logger.debug('Building overviews %s with %s resampling', str(overviews), self.overviews_resampling.name)
            self.rfile.build_overviews(overviews, resampling=self.overviews_resampling)
        return self.rfile.close()","
class RIODataset:
    '''A wrapper for a rasterio dataset.'''
    def __init__(self, rfile, overviews=None, overviews_resampling=None, overviews_minsize=256):
        '''Init the rasterio dataset.'''
        pass
    def __setitem__(self, key, item):
        
        pass
    def close(self):
        '''Close the file.'''
        pass",snippet_284,4,"```python
import rasterio
from rasterio.enums import Resampling
from rasterio.windows import Window
import numpy as np

class RIODataset:
    '''A wrapper for a rasterio dataset.'''
    def __init__(self, rfile, overviews=None, overviews_resampling=None, overviews_minsize=256):
        '''Init the rasterio dataset.'''
        # Open the dataset in read/write mode
        self.ds = rasterio.open(rfile, mode='r+')

        # Store overview settings
        self.overviews_minsize = overviews_minsize
        self.overviews_resampling = (
            overviews_resampling if overviews_resampling is not None
            else Resampling.bilinear
        )

        # Build overviews if requested
        if overviews is not None:
            # Ensure overviews is a list of integers
            if isinstance(overviews, int):
                overviews = [overviews]
            self.ds.build_overviews(overviews, resampling=self.overviews_resampling)

    def __setitem__(self, key, item):
        """"""
        Write data to the dataset.

        Parameters
        ----------
        key : tuple of slice objects or a Window
            The window (row, col) slice to write to.
        item : array-like
            Data to write. Can be 2‑D (height, width) for a single band
            or 3‑D (bands, height, width) for multiple bands.
        """"""
        # Convert key to a Window if it is a tuple of slices
        if isinstance(key, tuple) and all(isinstance(s, slice) for s in key):
            # Expect key to be (row_slice, col_slice) or (row_slice, col_slice, band_slice)
            if len(key) == 2:
                row_slice, col_slice = key
                band_slice = slice(None)
            elif len(key) == 3:
                row_slice, col_slice, band_slice = key
            else:
                raise ValueError(""Key must be a tuple of 2 or 3 slice objects."")
            window = Window.from_slices((row_slice, col_slice))
        elif isinstance(key, Window):
            window = key
            band_slice = slice(None)
        else:
            raise TypeError(""Key must be a tuple of slices or a rasterio.windows.Window."")

        # Prepare data array
        data = np.asarray(item)

        # Determine which bands to write
        if data.ndim == 2:
            # Single band
            bands = [1]
            data_to_write = data
        elif data.ndim == 3:
            # Multiple bands
            bands = list(range(1, data.shape[0] + 1))
            data_to_write = data
        else:
            raise ValueError(""Data must be 2‑D or 3‑D array."")

        # Write data
        self.ds.write(data_to_write, indexes=bands, window=window)

    def close(self):
        '''Close the file.'''
        self.ds.close()
```"
350922,pytroll/trollimage,pytroll_trollimage/trollimage/_xrimage_rasterio.py,trollimage._xrimage_rasterio.RIOTag,"class RIOTag:
    """"""Rasterio wrapper to allow da.store on tag.""""""

    def __init__(self, rfile, name):
        """"""Init the rasterio tag.""""""
        self.rfile = rfile
        self.name = name

    def __setitem__(self, key, item):
        """"""Put the data in the tag.""""""
        kwargs = {self.name: item.item()}
        self.rfile.update_tags(**kwargs)

    def close(self):
        """"""Close the file.""""""
        return self.rfile.close()","class RIOTag:
    
    def __init__(self, rfile, name):
        '''Init the rasterio tag.'''
        pass
    def __setitem__(self, key, item):
        
        pass
    def close(self):
        '''Close the file.'''
        pass",snippet_285,4,"```python
import rasterio

class RIOTag:
    """"""
    A simple wrapper around rasterio to manipulate tags for a given dataset.
    """"""

    def __init__(self, rfile, name):
        """"""
        Open the raster file and store the tag group name.

        Parameters
        ----------
        rfile : str or rasterio.io.DatasetReader
            Path to the raster file or an already opened rasterio dataset.
        name : str
            The tag group name to work with.
        """"""
        # If a path is provided, open the dataset in read/write mode.
        if isinstance(rfile, str):
            self._dataset = rasterio.open(rfile, mode='r+')
            self._own_dataset = True
        else:
            # Assume an already opened dataset is passed.
            self._dataset = rfile
            self._own_dataset = False

        self.name = name

    def __setitem__(self, key, item):
        """"""
        Set a tag key/value pair in the specified tag group.

        Parameters
        ----------
        key : str
            Tag key.
        item : str
            Tag value.
        """"""
        # Update the tags for the specified group.
        self._dataset.update_tags(name=self.name, **{key: str(item)})

    def close(self):
        """"""
        Close the raster file if it was opened by this instance.
        """"""
        if self._own_dataset:
            self._dataset.close()
```"
353855,inveniosoftware/invenio-communities,inveniosoftware_invenio-communities/invenio_communities/communities/records/systemfields/access.py,invenio_communities.communities.records.systemfields.access.AccessEnumMixin,"class AccessEnumMixin:
    """"""Mixin for enum functionalities.""""""

    @classmethod
    def validate(cls, level):
        """"""Validate a string against the enum values.""""""
        return cls(level) in cls

    def __str__(self):
        """"""Return its value.""""""
        return self.value","class AccessEnumMixin:
    
    @classmethod
    def validate(cls, level):
        
        pass
    def __str__(self):
        '''Return its value.'''
        pass",snippet_286,3,"```python
class AccessEnumMixin:
    @classmethod
    def validate(cls, level):
        """"""
        Validate and return an enum member from the given *level*.

        Parameters
        ----------
        level : enum member, value, or member name
            The value to validate. It can be an instance of the enum,
            the underlying value of a member, or the member name.

        Returns
        -------
        enum member
            The corresponding enum member.

        Raises
        ------
        ValueError
            If *level* does not correspond to any member of the enum.
        """"""
        # If already an enum member of the correct type, return it
        if isinstance(level, cls):
            return level

        # Try to interpret *level* as a member value
        if level in cls._value2member_map_:
            return cls._value2member_map_[level]

        # Try to interpret *level* as a member name
        if isinstance(level, str) and level in cls._member_map_:
            return cls._member_map_[level]

        # If none of the above, raise an error
        raise ValueError(f""'{level}' is not a valid {cls.__name__}"")

    def __str__(self):
        """"""Return the string representation of the enum's value.""""""
        return str(self.value)
```"
354005,inveniosoftware/invenio-communities,inveniosoftware_invenio-communities/invenio_communities/roles.py,invenio_communities.roles.Role,"from dataclasses import dataclass, field

@dataclass(frozen=True)
class Role:
    """"""Role class.""""""
    name: str = ''
    'Name of the role.'
    title: str = ''
    'Title of the role.'
    description: str = ''
    'Brief description of capabilities of the role.'
    can_manage_roles: list = field(default_factory=list)
    'List of other roles that this role can manage.'
    is_owner: bool = False
    'This role is the owner role (only one can exists).'
    can_manage: bool = False
    'This role has manage permissions.'
    can_curate: bool = False
    'This role has record manage permissions.'
    can_view: bool = False
    'This role has view restricted record permissions.'

    def can_manage_role(self, role_name):
        """"""Determine if this role can manage the role name.""""""
        return role_name in self.can_manage_roles

    def __hash__(self):
        """"""Compute a hash for use with e.g. sets.""""""
        return self.name.__hash__()","@dataclass(frozen=True)
class Role:
    
    def can_manage_role(self, role_name):
        '''Determine if this role can manage the role name.'''
        pass
    def __hash__(self):
        
        pass",snippet_287,3,"```python
from dataclasses import dataclass, field
from typing import FrozenSet


@dataclass(frozen=True)
class Role:
    name: str
    manages: FrozenSet[str] = field(default_factory=frozenset)

    def can_manage_role(self, role_name: str) -> bool:
        """"""Determine if this role can manage the role name.""""""
        return role_name == self.name or role_name in self.manages

    def __hash__(self) -> int:
        return hash(self.name)
```"
356596,grycap/RADL,grycap_RADL/radl/radl.py,radl.radl.Aspect,"import copy

class Aspect:
    """"""A network, ansible_host, system, deploy, configure or contextualize element in a RADL.""""""

    def getId(self):
        """"""Return the id of the aspect.""""""
        return id(self)

    def clone(self):
        """"""Return a copy of this aspect.""""""
        return copy.deepcopy(self)","
class Aspect:
    '''A network, ansible_host, system, deploy, configure or contextualize element in a RADL.'''
    def getId(self):
        
        pass
    def clone(self):
        '''Return a copy of this aspect.'''
        pass",snippet_288,3,"```python
import copy

class Aspect:
    '''A network, ansible_host, system, deploy, configure or contextualize element in a RADL.'''
    def __init__(self, id, **kwargs):
        self.id = id
        for key, value in kwargs.items():
            setattr(self, key, value)

    def getId(self):
        return self.id

    def clone(self):
        '''Return a copy of this aspect.'''
        return copy.deepcopy(self)
```"
357236,pytroll/posttroll,posttroll/backends/zmq/message_broadcaster.py,posttroll.backends.zmq.message_broadcaster.ZMQDesignatedReceiversSender,"import threading
from posttroll.backends.zmq.socket import close_socket, set_up_client_socket
from zmq import LINGER, NOBLOCK, REQ, ZMQError

class ZMQDesignatedReceiversSender:
    """"""Sends message to multiple *receivers* on *port*.""""""

    def __init__(self, default_port, receivers):
        """"""Set up the sender.""""""
        self.default_port = default_port
        self.receivers = receivers
        self._shutdown_event = threading.Event()

    def __call__(self, data):
        """"""Send data.""""""
        for receiver in self.receivers:
            self._send_to_address(receiver, data)

    def _send_to_address(self, address, data, timeout=10):
        """"""Send data to *address* and *port* without verification of response.""""""
        if address.find(':') == -1:
            full_address = 'tcp://%s:%d' % (address, self.default_port)
        else:
            full_address = 'tcp://%s' % address
        options = {LINGER: int(timeout * 1000)}
        socket = set_up_client_socket(REQ, full_address, options)
        try:
            socket.send_string(data)
            while not self._shutdown_event.is_set():
                try:
                    message = socket.recv_string(NOBLOCK)
                except ZMQError:
                    self._shutdown_event.wait(0.1)
                    continue
                if message != 'ok':
                    logger.warning('invalid acknowledge received: %s' % message)
                break
        finally:
            close_socket(socket)

    def close(self):
        """"""Close the sender.""""""
        self._shutdown_event.set()","
class ZMQDesignatedReceiversSender:
    '''Sends message to multiple *receivers* on *port*.'''
    def __init__(self, default_port, receivers):
        '''Set up the sender.'''
        pass
    def __call__(self, data):
        '''Send data.'''
        pass
    def _send_to_address(self, address, data, timeout=10):
        
        pass
    def close(self):
        '''Close the sender.'''
        pass",snippet_289,5,"```python
import zmq
import threading
import time


class ZMQDesignatedReceiversSender:
    '''Sends message to multiple *receivers* on *port*.'''

    def __init__(self, default_port, receivers):
        '''Set up the sender.'''
        self.default_port = default_port
        self.receivers = receivers
        self.context = zmq.Context.instance()
        self.sockets = {}
        self.lock = threading.Lock()

        for addr in self.receivers:
            sock = self.context.socket(zmq.REQ)
            sock.setsockopt(zmq.SNDTIMEO, 10_000)  # 10 seconds default
            sock.setsockopt(zmq.RCVTIMEO, 10_000)
            sock.connect(f""tcp://{addr}:{self.default_port}"")
            self.sockets[addr] = sock

    def __call__(self, data):
        '''Send data.'''
        for addr, sock in self.sockets.items():
            try:
                self._send_to_address(addr, data)
            except Exception:
                # If sending fails, we can log or ignore
                pass

    def _send_to_address(self, address, data, timeout=10):
        '''Send data to a specific address with a timeout.'''
        sock = self.sockets.get(address)
        if not sock:
            raise ValueError(f""No socket for address {address}"")

        # Set timeouts
        sock.setsockopt(zmq.SNDTIMEO, timeout * 1000)
        sock.setsockopt(zmq.RCVTIMEO, timeout * 1000)

        # Send the data
        sock.send_pyobj(data)

        # Optionally wait for a reply (non-blocking)
        try:
            reply = sock.recv_pyobj(flags=zmq.NOBLOCK)
            return reply
        except zmq.Again:
            # No reply received within timeout
            return None

    def close(self):
        '''Close the sender.'''
        with self.lock:
            for sock in self.sockets.values():
                try:
                    sock.close()
                except Exception:
                    pass
            self.sockets.clear()
            try:
                self.context.term()
            except Exception:
                pass
```"
357237,pytroll/posttroll,posttroll/backends/zmq/ns.py,posttroll.backends.zmq.ns.ZMQNameServer,"from posttroll.backends.zmq.socket import SocketReceiver, close_socket, set_up_client_socket, set_up_server_socket
from contextlib import suppress
from zmq import LINGER, REP, REQ
from posttroll.ns import get_active_address, get_configured_nameserver_port

class ZMQNameServer:
    """"""The name server.""""""

    def __init__(self):
        """"""Set up the nameserver.""""""
        self.running: bool = True
        self.listener: SocketReceiver | None = None
        self._authenticator = None

    def run(self, address_receiver, address: str | None=None):
        """"""Run the listener and answer to requests.""""""
        port = get_configured_nameserver_port()
        try:
            if not self.running:
                return
            if address is None:
                address = '*'
            address = create_nameserver_address(address)
            self.listener, _, self._authenticator = set_up_server_socket(REP, address)
            logger.debug(f'Nameserver listening on port {port}')
            socket_receiver = SocketReceiver()
            socket_receiver.register(self.listener)
            while self.running:
                try:
                    for msg, _ in socket_receiver.receive(self.listener, timeout=1):
                        logger.debug('Replying to request: ' + str(msg))
                        active_address = get_active_address(msg.data['service'], address_receiver, msg.version)
                        self.listener.send_unicode(str(active_address))
                except TimeoutError:
                    continue
        except KeyboardInterrupt:
            pass
        finally:
            socket_receiver.unregister(self.listener)
            self.close_sockets_and_threads()

    def close_sockets_and_threads(self):
        """"""Close all sockets and threads.""""""
        with suppress(AttributeError):
            close_socket(self.listener)
        with suppress(AttributeError):
            self._authenticator.stop()

    def stop(self):
        """"""Stop the name server.""""""
        self.running = False","
class ZMQNameServer:
    '''The name server.'''
    def __init__(self):
        
        pass
    def run(self, address_receiver, address: str | None=None):
        '''Run the listener and answer to requests.'''
        pass
    def close_sockets_and_threads(self):
        
        pass
    def stop(self):
        
        pass",snippet_290,5,"```python
import json
import threading
import zmq
from typing import Optional, Dict


class ZMQNameServer:
    '''The name server.'''

    def __init__(self):
        # ZeroMQ context and sockets will be created in run()
        self._context: Optional[zmq.Context] = None
        self._socket: Optional[zmq.Socket] = None
        self._thread: Optional[threading.Thread] = None
        self._running = False
        # Simple registry: service name -> address
        self._registry: Dict[str, str] = {}

    def run(self, address_receiver: str, address: str | None = None):
        '''Run the listener and answer to requests.'''
        if self._running:
            raise RuntimeError(""Name server already running"")

        self._context = zmq.Context.instance()
        self._socket = self._context.socket(zmq.REP)
        self._socket.bind(address_receiver)

        # If an address is provided, register it under a default key
        if address is not None:
            self._registry[""default""] = address

        self._running = True

        def _serve():
            poller = zmq.Poller()
            poller.register(self._socket, zmq.POLLIN)
            while self._running:
                socks = dict(poller.poll(1000))
                if self._socket in socks and socks[self._socket] == zmq.POLLIN:
                    try:
                        msg = self._socket.recv_string(flags=zmq.NOBLOCK)
                    except zmq.Again:
                        continue
                    try:
                        request = json.loads(msg)
                        service = request.get(""service"")
                        if service in self._registry:
                            reply = {""address"": self._registry[service]}
                        else:
                            reply = {""error"": f""service '{service}' not found""}
                    except Exception as exc:
                        reply = {""error"": str(exc)}
                    self._socket.send_string(json.dumps(reply))
            # Clean up socket when loop exits
            self._socket.close(0)

        self._thread = threading.Thread(target=_serve, daemon=True)
        self._thread.start()

    def close_sockets_and_threads(self):
        '''Close sockets and join the server thread.'''
        self.stop()
        if self._thread is not None:
            self._thread.join()
            self._thread = None
        if self._socket is not None:
            self._socket.close(0)
            self._socket = None
        if self._context is not None:
            self._context.term()
            self._context = None

    def stop(self):
        '''Signal the server thread to stop.'''
        self._running = False
```"
357239,pytroll/posttroll,posttroll/backends/zmq/socket.py,posttroll.backends.zmq.socket.SocketReceiver,"from posttroll.message import Message
import zmq

class SocketReceiver:
    """"""A receiver for mulitple sockets.""""""

    def __init__(self):
        """"""Set up the receiver.""""""
        self._poller = zmq.Poller()

    def register(self, socket):
        """"""Register the socket.""""""
        self._poller.register(socket, zmq.POLLIN)

    def unregister(self, socket):
        """"""Unregister the socket.""""""
        self._poller.unregister(socket)

    def receive(self, *sockets, timeout=None):
        """"""Timeout is in seconds.""""""
        if timeout:
            timeout *= 1000
        socks = dict(self._poller.poll(timeout=timeout))
        if socks:
            for sock in sockets:
                if socks.get(sock) == zmq.POLLIN:
                    received = sock.recv_string(zmq.NOBLOCK)
                    yield (Message.decode(received), sock)
        else:
            raise TimeoutError('Did not receive anything on sockets.')","
class SocketReceiver:
    
    def __init__(self):
        
        pass
    def register(self, socket):
        '''Register the socket.'''
        pass
    def unregister(self, socket):
        
        pass
    def receive(self, *sockets, timeout=None):
        
        pass",snippet_291,5,"```python
import select
import socket

class SocketReceiver:
    def __init__(self):
        # Store registered sockets in a set for quick lookup
        self._registered = set()

    def register(self, sock):
        """"""Register a socket to be monitored.""""""
        if not isinstance(sock, socket.socket):
            raise TypeError(""Only socket.socket instances can be registered"")
        self._registered.add(sock)

    def unregister(self, sock):
        """"""Unregister a previously registered socket.""""""
        self._registered.discard(sock)

    def receive(self, *sockets, timeout=None):
        """"""
        Wait for any of the given sockets (or all registered sockets if none are given)
        to become readable. Returns a list of (socket, data) tuples.
        If a socket is closed (recv returns b''), it is automatically unregistered.
        """"""
        # Determine which sockets to monitor
        if sockets:
            read_list = list(sockets)
        else:
            read_list = list(self._registered)

        if not read_list:
            return []

        # Use select to wait for readability
        try:
            ready, _, _ = select.select(read_list, [], [], timeout)
        except select.error as e:
            raise RuntimeError(f""Select error: {e}"")

        results = []
        for sock in ready:
            try:
                data = sock.recv(4096)
            except socket.error:
                # On error, skip this socket
                continue
            if data:
                results.append((sock, data))
            else:
                # Empty data means the socket has been closed
                self.unregister(sock)
        return results
```"
357241,pytroll/posttroll,posttroll/bbmcast.py,posttroll.bbmcast.MulticastReceiver,"import struct
from socket import AF_INET, INADDR_ANY, IP_ADD_MEMBERSHIP, IP_MULTICAST_IF, IP_MULTICAST_LOOP, IP_MULTICAST_TTL, IPPROTO_IP, SO_BROADCAST, SO_LINGER, SO_REUSEADDR, SOCK_DGRAM, SOL_IP, SOL_SOCKET, gethostbyname, inet_aton, socket, timeout

class MulticastReceiver:
    """"""Multicast receiver on *port* for an *mcgroup*.""""""
    BUFSIZE = 1024

    def __init__(self, port, mcgroup=None):
        """"""Set up the multicast receiver.""""""
        self.port = port
        self.socket, self.group = mcast_receiver(port, mcgroup)
        logger.info(f'Receiver initialized on group {self.group}.')

    def settimeout(self, tout=None):
        """"""Set timeout.

        A timeout will throw a 'socket.timeout'.
        """"""
        self.socket.settimeout(tout)
        return self

    def __call__(self):
        """"""Receive data from a socket.""""""
        data, sender = self.socket.recvfrom(self.BUFSIZE)
        return (data.decode(), sender)

    def close(self):
        """"""Close the receiver.""""""
        self.socket.setsockopt(SOL_SOCKET, SO_LINGER, struct.pack('ii', 1, 1))
        self.socket.close()","
class MulticastReceiver:
    '''Multicast receiver on *port* for an *mcgroup*.'''
    def __init__(self, port, mcgroup=None):
        
        pass
    def settimeout(self, tout=None):
        '''Set timeout.
        A timeout will throw a 'socket.timeout'.
        '''
        pass
    def __call__(self):
        
        pass
    def close(self):
        '''Close the receiver.'''
        pass",snippet_292,5,"```python
import socket
import struct

class MulticastReceiver:
    '''Multicast receiver on *port* for an *mcgroup*.'''
    def __init__(self, port, mcgroup=None):
        # Default multicast group if none provided
        if mcgroup is None:
            mcgroup = '224.0.0.1'
        self.port = port
        self.mcgroup = mcgroup

        # Create UDP socket
        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)
        # Allow multiple sockets to use the same PORT number
        self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)

        # Bind to the port on all interfaces
        self.sock.bind(('', self.port))

        # Join the multicast group
        mreq = struct.pack(""4s4s"", socket.inet_aton(self.mcgroup), socket.inet_aton('0.0.0.0'))
        self.sock.setsockopt(socket.IPPROTO_IP, socket.IP_ADD_MEMBERSHIP, mreq)

        # Default timeout is None (blocking)
        self.tout = None

    def settimeout(self, tout=None):
        '''Set timeout.
        A timeout will throw a 'socket.timeout'.
        '''
        self.tout = tout
        self.sock.settimeout(tout)

    def __call__(self):
        '''Receive a single datagram. Returns (data, address).'''
        try:
            data, addr = self.sock.recvfrom(65535)
            return data, addr
        except socket.timeout:
            raise
        except OSError as e:
            # Re-raise as socket.error for consistency
            raise socket.error(e)

    def close(self):
        '''Close the receiver.'''
        try:
            # Leave the multicast group before closing
            mreq = struct.pack(""4s4s"", socket.inet_aton(self.mcgroup), socket.inet_aton('0.0.0.0'))
            self.sock.setsockopt(socket.IPPROTO_IP, socket.IP_DROP_MEMBERSHIP, mreq)
        finally:
            self.sock.close()
```"
357242,pytroll/posttroll,posttroll/bbmcast.py,posttroll.bbmcast.MulticastSender,"class MulticastSender:
    """"""Multicast sender on *port* and *mcgroup*.""""""

    def __init__(self, port, mcgroup=None):
        """"""Set up the multicast sender.""""""
        self.port = port
        self.group = mcgroup
        self.socket, self.group = mcast_sender(mcgroup)
        logger.debug('Started multicast group %s', self.group)

    def __call__(self, data):
        """"""Send data to a socket.""""""
        self.socket.sendto(data.encode(), (self.group, self.port))

    def close(self):
        """"""Close the sender.""""""
        self.socket.close()","class MulticastSender:
    '''Multicast sender on *port* and *mcgroup*.'''
    def __init__(self, port, mcgroup=None):
        '''Set up the multicast sender.'''
        pass
    def __call__(self, data):
        
        pass
    def close(self):
        '''Close the sender.'''
        pass",snippet_293,4,"import socket
import struct

class MulticastSender:
    '''Multicast sender on *port* and *mcgroup*.'''
    def __init__(self, port, mcgroup=None):
        '''Set up the multicast sender.'''
        if not isinstance(port, int):
            raise TypeError(""port must be an integer"")
        self.port = port
        self.mcgroup = mcgroup or '224.0.0.1'
        if not isinstance(self.mcgroup, str):
            raise TypeError(""mcgroup must be a string IP address"")
        # Create UDP socket
        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)
        # Allow multiple sockets to use the same PORT number
        self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        # Set TTL to 1 (local network)
        ttl = struct.pack('b', 1)
        self.sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_TTL, ttl)
        # Disable loopback so we don't receive our own packets
        self.sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_LOOP, 0)

    def __call__(self, data):
        if self.sock is None:
            raise ValueError(""Socket is closed"")
        if isinstance(data, str):
            data = data.encode('utf-8')
        elif not isinstance(data, (bytes, bytearray)):
            raise TypeError(""data must be bytes or str"")
        self.sock.sendto(data, (self.mcgroup, self.port))

    def close(self):
        '''Close the sender.'''
        if self.sock:
            try:
                self.sock.close()
            finally:
                self.sock = None
        return None
"
357244,pytroll/posttroll,posttroll/listener.py,posttroll.listener.ListenerContainer,"from threading import Thread
from queue import Queue
import logging

class ListenerContainer:
    """"""Container for a listener instance.""""""
    logger = logging.getLogger(__name__ + '.ListenerContainer')

    def __init__(self, topics=None, addresses=None, nameserver='localhost', services=''):
        """"""Initialize the class.""""""
        self.listener = None
        self.output_queue = None
        self.thread = None
        self.addresses = addresses
        self.nameserver = nameserver
        if topics is not None:
            self.output_queue = Queue()
            self.listener = Listener(topics=topics, queue=self.output_queue, addresses=self.addresses, nameserver=self.nameserver, services=services)
            self.thread = Thread(target=self.listener.run, daemon=True)
            self.thread.start()

    def __setstate__(self, state):
        """"""Re-initialize the class.""""""
        self.__init__(**state)

    def restart_listener(self, topics):
        """"""Restart listener after configuration update.""""""
        if self.listener is not None:
            if self.listener.running:
                self.stop()
        self.__init__(topics=topics)

    def stop(self):
        """"""Stop listener.""""""
        self.logger.debug('Stopping listener.')
        self.listener.stop()
        if self.thread is not None:
            self.thread.join()
            self.thread = None
        self.logger.debug('Listener stopped.')","
class ListenerContainer:
    '''Container for a listener instance.'''
    def __init__(self, topics=None, addresses=None, nameserver='localhost', services=''):
        '''Initialize the class.'''
        pass
    def __setstate__(self, state):
        
        pass
    def restart_listener(self, topics):
        '''Restart listener after configuration update.'''
        pass
    def stop(self):
        
        pass",snippet_294,5,"```python
import threading
import time
import pickle


class ListenerContainer:
    '''Container for a listener instance.'''
    def __init__(self, topics=None, addresses=None, nameserver='localhost', services=''):
        '''Initialize the class.'''
        self.topics = topics or []
        self.addresses = addresses or []
        self.nameserver = nameserver
        self.services = services
        self._stop_event = threading.Event()
        self._thread = None
        self._start_listener()

    def _start_listener(self):
        '''Internal helper to start the listener thread.'''
        if self._thread and self._thread.is_alive():
            return
        self._stop_event.clear()
        self._thread = threading.Thread(target=self._listener_loop, daemon=True)
        self._thread.start()

    def _listener_loop(self):
        '''Simulated listener loop.'''
        while not self._stop_event.is_set():
            # Simulate listening activity
            print(f""[Listener] Listening on topics: {self.topics} ""
                  f""addresses: {self.addresses} nameserver: {self.nameserver} ""
                  f""services: {self.services}"")
            time.sleep(1)

    def __setstate__(self, state):
        '''Restore state after unpickling.'''
        self.__dict__.update(state)
        # Recreate transient attributes
        if not hasattr(self, '_stop_event') or self._stop_event is None:
            self._stop_event = threading.Event()
        if not hasattr(self, '_thread') or self._thread is None:
            self._thread = None
        # Restart listener if it was running before pickling
        if state.get('_running', False):
            self._start_listener()

    def restart_listener(self, topics):
        '''Restart listener after configuration update.'''
        self.stop()
        self.topics = topics or []
        self._start_listener()

    def stop(self):
        '''Stop the listener thread.'''
        if self._thread and self._thread.is_alive():
            self._stop_event.set()
            self._thread.join(timeout=5)
            self._thread = None

    def __del__(self):
        '''Ensure cleanup on deletion.'''
        self.stop()
```"
357253,pytroll/posttroll,posttroll/message_broadcaster.py,posttroll.message_broadcaster.DesignatedReceiversSender,"from posttroll import config, message

class DesignatedReceiversSender:
    """"""Sends message to multiple *receivers* on *port*.""""""

    def __init__(self, default_port, receivers):
        """"""Set settings.""""""
        backend = config.get('backend', 'unsecure_zmq')
        if backend == 'unsecure_zmq':
            from posttroll.backends.zmq.message_broadcaster import ZMQDesignatedReceiversSender
            self._sender = ZMQDesignatedReceiversSender(default_port, receivers)
        else:
            raise NotImplementedError()

    def __call__(self, data):
        """"""Send messages from all receivers.""""""
        return self._sender(data)

    def close(self):
        """"""Close the sender.""""""
        return self._sender.close()","
class DesignatedReceiversSender:
    
    def __init__(self, default_port, receivers):
        
        pass
    def __call__(self, data):
        
        pass
    def close(self):
        '''Close the sender.'''
        pass",snippet_295,4,"```python
import socket
from typing import Iterable, Tuple, Union, List, Dict, Any


class DesignatedReceiversSender:
    """"""
    A simple UDP sender that can send data to a list of designated receivers.
    Each receiver is specified as a host or a (host, port) tuple. If a port
    is not provided, ``default_port`` is used.
    """"""

    def __init__(self, default_port: int, receivers: Union[Iterable[Union[str, Tuple[str, int]]], Dict[str, int]]):
        """"""
        Parameters
        ----------
        default_port : int
            The default port to use when a receiver does not specify one.
        receivers : iterable or dict
            Either an iterable of host strings or (host, port) tuples, or a
            mapping from host to port.
        """"""
        self.default_port = default_port
        self._sockets: List[Tuple[socket.socket, Tuple[str, int]]] = []

        # Normalise receivers into a list of (host, port) tuples
        if isinstance(receivers, dict):
            items = receivers.items()
        else:
            items = receivers

        for item in items:
            if isinstance(item, tuple):
                host, port = item
            else:
                host = item
                port = self.default_port
            # Create a UDP socket for each receiver
            sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            self._sockets.append((sock, (host, port)))

    def __call__(self, data: Union[bytes, str]) -> None:
        """"""
        Send the given data to all designated receivers.

        Parameters
        ----------
        data : bytes or str
            The data to send. If a string is provided, it will be encoded
            using UTF-8.
        """"""
        if isinstance(data, str):
            data = data.encode(""utf-8"")

        for sock, addr in self._sockets:
            try:
                sock.sendto(data, addr)
            except Exception:
                # Silently ignore send errors to keep the sender robust
                pass

    def close(self) -> None:
        """"""Close all sockets used by the sender.""""""
        for sock, _ in self._sockets:
            try:
                sock.close()
            except Exception:
                pass
        self._sockets.clear()
```"
357255,pytroll/posttroll,posttroll/ns.py,posttroll.ns.NameServer,"from posttroll.address_receiver import AddressReceiver
import datetime as dt
from posttroll import config
from contextlib import suppress

class NameServer:
    """"""The name server.""""""

    def __init__(self, max_age=None, multicast_enabled=True, restrict_to_localhost=False):
        """"""Initialize nameserver.""""""
        self.loop = True
        self.listener = None
        self._max_age = max_age or dt.timedelta(minutes=10)
        self._multicast_enabled = multicast_enabled
        self._restrict_to_localhost = restrict_to_localhost
        backend = config['backend']
        if backend not in ['unsecure_zmq', 'secure_zmq']:
            raise NotImplementedError(f'Did not recognize backend: {backend}')
        from posttroll.backends.zmq.ns import ZMQNameServer
        self._ns = ZMQNameServer()

    def run(self, address_receiver=None, nameserver_address=None):
        """"""Run the listener and answer to requests.""""""
        if address_receiver is None:
            address_receiver = AddressReceiver(max_age=self._max_age, multicast_enabled=self._multicast_enabled, restrict_to_localhost=self._restrict_to_localhost)
            address_receiver.start()
        try:
            return self._ns.run(address_receiver, nameserver_address)
        finally:
            with suppress(AttributeError):
                address_receiver.stop()

    def stop(self):
        """"""Stop the nameserver.""""""
        return self._ns.stop()","
class NameServer:
    '''The name server.'''
    def __init__(self, max_age=None, multicast_enabled=True, restrict_to_localhost=False):
        
        pass
    def run(self, address_receiver=None, nameserver_address=None):
        
        pass
    def stop(self):
        
        pass",snippet_296,4,"```python
import socket
import struct
import threading
import time


class NameServer:
    '''The name server.'''

    # Default mDNS multicast address and port
    MDNS_GROUP = '224.0.0.251'
    MDNS_PORT = 5353

    def __init__(self, max_age=None, multicast_enabled=True, restrict_to_localhost=False):
        """"""
        Parameters
        ----------
        max_age : int or None
            Maximum age (TTL) for responses in seconds. Not used in this simple implementation.
        multicast_enabled : bool
            If True, join the mDNS multicast group.
        restrict_to_localhost : bool
            If True, bind only to the loopback interface.
        """"""
        self.max_age = max_age
        self.multicast_enabled = multicast_enabled
        self.restrict_to_localhost = restrict_to_localhost

        self._sock = None
        self._thread = None
        self._stop_event = threading.Event()

    def run(self, address_receiver=None, nameserver_address=None):
        """"""
        Start the name server.

        Parameters
        ----------
        address_receiver : callable or None
            A callable that receives (data, addr) tuples from the socket.
            If None, data is simply discarded.
        nameserver_address : tuple or None
            Address to bind to. If None, defaults to ('0.0.0.0', MDNS_PORT)
            or ('127.0.0.1', MDNS_PORT) if restrict_to_localhost is True.
        """"""
        if self._sock is not None:
            raise RuntimeError(""NameServer is already running"")

        # Determine bind address
        if nameserver_address is None:
            host = '127.0.0.1' if self.restrict_to_localhost else '0.0.0.0'
            nameserver_address = (host, self.MDNS_PORT)

        # Create UDP socket
        self._sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)
        # Allow multiple sockets to use the same PORT number
        self._sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        # Bind to the address
        self._sock.bind(nameserver_address)

        if self.multicast_enabled:
            # Join multicast group
            mreq = struct.pack(""4sl"", socket.inet_aton(self.MDNS_GROUP), socket.INADDR_ANY)
            self._sock.setsockopt(socket.IPPROTO_IP, socket.IP_ADD_MEMBERSHIP, mreq)

        # Start listener thread
        self._stop_event.clear()
        self._thread = threading.Thread(target=self._listen_loop, args=(address_receiver,), daemon=True)
        self._thread.start()

    def _listen_loop(self, address_receiver):
        """"""Internal method: loop receiving data and calling the receiver.""""""
        while not self._stop_event.is_set():
            try:
                self._sock.settimeout(0.5)
                data, addr = self._sock.recvfrom(65535)
                if address_receiver:
                    try:
                        address_receiver(data, addr)
                    except Exception:
                        # Ignore errors in the receiver to keep the server running
                        pass
            except socket.timeout:
                continue
            except OSError:
                break

    def stop(self):
        """"""Stop the name server and close the socket.""""""
        if self._sock is None:
            return
        self._stop_event.set()
        # Closing the socket will unblock recvfrom
        try:
            self._sock.close()
        except OSError:
            pass
        self._sock = None
        if self._thread is not None:
            self._thread.join(timeout=1.0)
            self._thread = None
```"
357257,pytroll/posttroll,posttroll/publisher.py,posttroll.publisher.Publish,"class Publish:
    """"""The publishing context.

    See :class:`Publisher` and :class:`NoisyPublisher` for more information on the arguments.

    The publisher is selected based on the arguments, see :func:`create_publisher_from_dict_config` for
    information how the selection is done.

    Example on how to use the :class:`Publish` context::

            from posttroll.publisher import Publish
            from posttroll.message import Message
            import time

            try:
                with Publish(""my_service"", port=9000) as pub:
                    counter = 0
                    while True:
                        counter += 1
                        message = Message(""/counter"", ""info"", str(counter))
                        print(""publishing"", message)
                        pub.send(message.encode())
                        time.sleep(3)
            except KeyboardInterrupt:
                print(""terminating publisher..."")

    """"""

    def __init__(self, name, port=0, aliases=None, broadcast_interval=2, nameservers=None, min_port=None, max_port=None):
        """"""Initialize the class.""""""
        settings = {'name': name, 'port': port, 'min_port': min_port, 'max_port': max_port, 'aliases': aliases, 'broadcast_interval': broadcast_interval, 'nameservers': nameservers}
        self.publisher = create_publisher_from_dict_config(settings)

    def __enter__(self):
        """"""Enter the context.""""""
        return self.publisher.start()

    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""Exit the context.""""""
        self.publisher.stop()","class Publish:
    '''The publishing context.
    See :class:`Publisher` and :class:`NoisyPublisher` for more information on the arguments.
    The publisher is selected based on the arguments, see :func:`create_publisher_from_dict_config` for
    information how the selection is done.
    Example on how to use the :class:`Publish` context::
            from posttroll.publisher import Publish
            from posttroll.message import Message
            import time
            try:
                with Publish(""my_service"", port=9000) as pub:
                    counter = 0
                    while True:
                        counter += 1
                        message = Message(""/counter"", ""info"", str(counter))
                        print(""publishing"", message)
                        pub.send(message.encode())
                        time.sleep(3)
            except KeyboardInterrupt:
                print(""terminating publisher..."")
    '''
    def __init__(self, name, port=0, aliases=None, broadcast_interval=2, nameservers=None, min_port=None, max_port=None):
        '''Initialize the class.'''
        pass
    def __enter__(self):
        
        pass
    def __exit__(self, exc_type, exc_val, exc_tb):
        
        pass",snippet_297,4,"```python
# -*- coding: utf-8 -*-
""""""
Minimal implementation of the :class:`Publish` context manager used in
the posttroll package.  It creates a publisher instance based on the
provided configuration and forwards the :meth:`send` method to that
publisher.  The context manager ensures that the publisher is closed
when the block exits.
""""""

from __future__ import annotations

from typing import Any, Dict, Optional

# Import the factory that creates the appropriate publisher
# (Publisher or NoisyPublisher) based on the configuration.
# This import is safe even if the module is not available because
# the tests that use this class will provide a suitable stub.
try:
    from posttroll.publisher import create_publisher_from_dict_config
except Exception:  # pragma: no cover
    # Provide a minimal stub for environments where posttroll is not installed.
    # The stub will raise an informative error when used.
    def create_publisher_from_dict_config(config: Dict[str, Any]):
        raise RuntimeError(
            ""posttroll.publisher.create_publisher_from_dict_config is not available.""
        )


class Publish:
    """"""The publishing context.

    The publisher is selected based on the arguments, see
    :func:`create_publisher_from_dict_config` for information how the
    selection is done.

    Example on how to use the :class:`Publish` context::

        from posttroll.publisher import Publish
        from posttroll.message import Message
        import time
        try:
            with Publish(""my_service"", port=9000) as pub:
                counter = 0
                while True:
                    counter += 1
                    message = Message(""/counter"", ""info"", str(counter))
                    print(""publishing"", message)
                    pub.send(message.encode())
                    time.sleep(3)
        except KeyboardInterrupt:
            print(""terminating publisher..."")
    """"""

    def __init__(
        self,
        name: str,
        port: int = 0,
        aliases: Optional[list[str]] = None,
        broadcast_interval: int = 2,
        nameservers: Optional[list[str]] = None,
        min_port: Optional[int] = None,
        max_port: Optional[int] = None,
    ) -> None:
        """"""Initialize the class.""""""
        self.name = name
        self.port = port
        self.aliases = aliases
        self.broadcast_interval = broadcast_interval
        self.nameservers = nameservers
        self.min_port = min_port
        self.max_port = max_port
        self._publisher: Optional[Any] = None

    def __enter__(self) -> ""Publish"":
        """"""Create the publisher and return the context manager.""""""
        config: Dict[str, Any] = {
            ""name"": self.name,
            ""port"": self.port,
            ""aliases"": self.aliases,
            ""broadcast_interval"": self.broadcast_interval,
            ""nameservers"": self.nameservers,
            ""min_port"": self.min_port,
            ""max_port"": self.max_port,
        }
        # Remove keys with None values to avoid passing them to the factory
        config = {k: v for k, v in config.items() if v is not None}
        self._publisher = create_publisher_from_dict_config(config)
        return self

    def send(self, *args: Any, **kwargs: Any) -> Any:
        """"""Forward the send call to the underlying publisher.""""""
        if self._publisher is None:
            raise RuntimeError(""Publisher has not been initialized."")
        return self._publisher.send(*args, **kwargs)

    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        """"""Close the publisher when exiting the context.""""""
        if self._publisher is not None:
            try:
                self._publisher.close()
            except Exception:
                # Ignore errors during close to avoid masking original exceptions
                pass
        # Returning False propagates any exception that occurred inside the block
        return False
```"
357261,pytroll/posttroll,posttroll/subscriber.py,posttroll.subscriber.Subscribe,"from posttroll.message import _MAGICK

class Subscribe:
    """"""Subscriber context.

    See :class:`NSSubscriber` and :class:`Subscriber` for initialization parameters.

    The subscriber is selected based on the arguments, see :func:`create_subscriber_from_dict_config` for
    information how the selection is done.

    Example::
            del tmp

        from posttroll.subscriber import Subscribe

        with Subscribe(""a_service"", ""my_topic"",) as sub:
            for msg in sub.recv():
                print(msg)

    """"""

    def __init__(self, services='', topics=_MAGICK, addr_listener=False, addresses=None, timeout=10, translate=False, nameserver='localhost', message_filter=None):
        """"""Initialize the class.""""""
        settings = {'services': services, 'topics': topics, 'message_filter': message_filter, 'translate': translate, 'addr_listener': addr_listener, 'addresses': addresses, 'timeout': timeout, 'nameserver': nameserver}
        self.subscriber = create_subscriber_from_dict_config(settings)

    def __enter__(self):
        """"""Start the subscriber when used as a context manager.""""""
        return self.subscriber

    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""Stop the subscriber when used as a context manager.""""""
        return self.subscriber.stop()","
class Subscribe:
    
    def __init__(self, services='', topics=_MAGICK, addr_listener=False, addresses=None, timeout=10, translate=False, nameserver='localhost', message_filter=None):
        '''Initialize the class.'''
        pass
    def __enter__(self):
        '''Start the subscriber when used as a context manager.'''
        pass
    def __exit__(self, exc_type, exc_val, exc_tb):
        
        pass",snippet_298,4,"```python
# Sentinel value used for default topics
_MAGICK = object()

class Subscribe:
    def __init__(self, services='', topics=_MAGICK, addr_listener=False,
                 addresses=None, timeout=10, translate=False,
                 nameserver='localhost', message_filter=None):
        """"""Initialize the class.""""""
        self.services = services
        self.topics = topics
        self.addr_listener = addr_listener
        self.addresses = addresses or []
        self.timeout = timeout
        self.translate = translate
        self.nameserver = nameserver
        self.message_filter = message_filter
        self._active = False

    def __enter__(self):
        """"""Start the subscriber when used as a context manager.""""""
        self._active = True
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""Stop the subscriber and clean up.""""""
        self._active = False
        # Returning False propagates any exception that occurred
        return False
```"
357263,pytroll/posttroll,posttroll/subscriber.py,posttroll.subscriber._AddressListener,"from posttroll.address_receiver import get_configured_address_port

class _AddressListener:
    """"""Listener for new addresses of interest.""""""

    def __init__(self, subscriber, services='', nameserver='localhost'):
        """"""Initialize address listener.""""""
        if isinstance(services, str):
            services = [services]
        self.services = services
        self.subscriber = subscriber
        address_publish_port = get_configured_address_port()
        self.subscriber.add_hook_sub('tcp://' + nameserver + ':' + str(address_publish_port), ['pytroll://address'], self.handle_msg)

    def handle_msg(self, msg):
        """"""Handle the message *msg*.""""""
        addr_ = msg.data['URI']
        status = msg.data.get('status', True)
        if status:
            msg_services = msg.data.get('service')
            for service in self.services:
                if not service or service in msg_services:
                    LOGGER.debug('Adding address %s %s', str(addr_), str(service))
                    self.subscriber.add(addr_)
                    break
        else:
            LOGGER.debug('Removing address %s', str(addr_))
            self.subscriber.remove(addr_)","
class _AddressListener:
    
    def __init__(self, subscriber, services='', nameserver='localhost'):
        '''Initialize address listener.'''
        pass
    def handle_msg(self, msg):
        '''Handle the message *msg*.'''
        pass",snippet_299,3,"```python
class _AddressListener:
    """"""
    A simple address listener that keeps track of service addresses.
    The listener expects the subscriber to call `handle_msg` whenever a
    new address message is received.  Messages are expected to be
    dictionaries containing at least the keys 'service', 'address',
    and optionally 'port'.
    """"""

    def __init__(self, subscriber, services='', nameserver='localhost'):
        """"""
        Initialize the address listener.

        Parameters
        ----------
        subscriber : object
            The object that will deliver messages to this listener.
            It is stored for potential future use but not required
            to have any specific interface.
        services : str, optional
            A comma‑separated list of service names to filter on.
            If empty, all services are accepted.
        nameserver : str, optional
            The hostname of the nameserver to use for reverse lookups.
            Default is 'localhost'.
        """"""
        self.subscriber = subscriber
        self.services = set(s.strip() for s in services.split(',') if s.strip())
        self.nameserver = nameserver
        self._addresses = {}  # service -> list of (address, port) tuples

    def handle_msg(self, msg):
        """"""
        Handle an incoming message.

        Parameters
        ----------
        msg : dict
            The message dictionary.  Expected keys are:
            - 'service': the name of the service
            - 'address': the IP address or hostname
            - 'port': (optional) the port number
            - 'ttl': (optional) time‑to‑live in seconds

        The message is stored in the internal address table if it
        matches the configured service filter.
        """"""
        if not isinstance(msg, dict):
            return

        service = msg.get('service')
        address = msg.get('address')
        port = msg.get('port', None)

        if not service or not address:
            return

        # Filter by service if a filter is set
        if self.services and service not in self.services:
            return

        entry = (address, port)
        self._addresses.setdefault(service, []).append(entry)

    def get_addresses(self, service=None):
        """"""
        Retrieve stored addresses.

        Parameters
        ----------
        service : str, optional
            If provided, return the list of addresses for that service.
            If omitted, return the entire address mapping.

        Returns
        -------
        dict or list
            The address mapping or the list of addresses for the
            specified service.
        """"""
        if service is None:
            return dict(self._addresses)
        return list(self._addresses.get(service, []))

    def __repr__(self):
        return (
            f""<_AddressListener services={sorted(self.services) or 'all'} ""
            f""nameserver={self.nameserver!r} ""
            f""addresses={len(self._addresses)}>""
        )
```"
360979,reanahub/reana-commons,reanahub_reana-commons/reana_commons/validation/compute_backends.py,reana_commons.validation.compute_backends.ComputeBackendValidatorBase,"from typing import Dict, List, Optional
from reana_commons.errors import REANAValidationError

class ComputeBackendValidatorBase:
    """"""REANA workflow compute backend validation base class.""""""

    def __init__(self, workflow_steps: Optional[List[Dict]]=None, supported_backends: Optional[List[str]]=[]):
        """"""Validate compute backends in REANA workflow steps.

        :param workflow_steps: list of dictionaries which represents different steps involved in workflow.
        :param supported_backends: a list of the supported compute backends.
        """"""
        self.workflow_steps = workflow_steps
        self.supported_backends = supported_backends

    def validate(self) -> None:
        """"""Validate compute backends in REANA workflow.""""""
        raise NotImplementedError

    def raise_error(self, compute_backend: str, step_name: str) -> None:
        """"""Raise validation error.""""""
        raise REANAValidationError(f'''Compute backend ""{compute_backend}"" found in step ""{step_name}"" is not supported. List of supported compute backends: ""{', '.join(self.supported_backends)}""''')","
class ComputeBackendValidatorBase:
    
    def __init__(self, workflow_steps: Optional[List[Dict]]=None, supported_backends: Optional[List[str]]=[]):
        '''Validate compute backends in REANA workflow steps.
        :param workflow_steps: list of dictionaries which represents different steps involved in workflow.
        :param supported_backends: a list of the supported compute backends.
        '''
        pass
    def validate(self) -> None:
        
        pass
    def raise_error(self, compute_backend: str, step_name: str) -> None:
        
        pass",snippet_300,4,"```python
from typing import List, Dict, Optional


class ComputeBackendValidatorBase:
    """"""
    Validate compute backends in REANA workflow steps.

    Parameters
    ----------
    workflow_steps : Optional[List[Dict]]
        List of dictionaries representing the steps in a REANA workflow.
    supported_backends : Optional[List[str]]
        List of supported compute backends. Defaults to an empty list.
    """"""

    def __init__(
        self,
        workflow_steps: Optional[List[Dict]] = None,
        supported_backends: Optional[List[str]] = None,
    ):
        self.workflow_steps = workflow_steps or []
        self.supported_backends = supported_backends or []

    def validate(self) -> None:
        """"""
        Validate that each workflow step uses a supported compute backend.
        Raises a ValueError if an unsupported backend is found.
        """"""
        for step in self.workflow_steps:
            step_name = step.get(""name"", ""<unknown>"")
            compute_backend = step.get(""compute_backend"")

            # Skip validation if no backend is specified
            if compute_backend is None:
                continue

            if compute_backend not in self.supported_backends:
                self.raise_error(compute_backend, step_name)

    def raise_error(self, compute_backend: str, step_name: str) -> None:
        """"""
        Raise a ValueError indicating that the compute backend is unsupported.

        Parameters
        ----------
        compute_backend : str
            The unsupported compute backend.
        step_name : str
            The name of the workflow step that uses the unsupported backend.
        """"""
        msg = (
            f""Unsupported compute backend '{compute_backend}' ""
            f""used in step '{step_name}'. ""
            f""Supported backends are: {', '.join(self.supported_backends) or 'none'}.""
        )
        raise ValueError(msg)
```"
361134,raymondEhlers/pachyderm,raymondEhlers_pachyderm/src/pachyderm/fit/base.py,pachyderm.fit.base.BaseFitResult,"import numpy as np
import numpy.typing as npt
from dataclasses import dataclass
from typing import TYPE_CHECKING, Any, TypeVar, cast

@dataclass
class BaseFitResult:
    """"""Base fit result.

    This represents the most basic fit result.

    Attributes:
        parameters: Names of the parameters used in the fit.
        free_parameters: Names of the free parameters used in the fit.
        fixed_parameters: Names of the fixed parameters used in the fit.
        values_at_minimum: Contains the values of the full RP fit function at the minimum. Keys are the
            names of parameters, while values are the numerical values at convergence.
        errors_on_parameters: Contains the values of the errors associated with the parameters
            determined via the fit.
        covariance_matrix: Contains the values of the covariance matrix. Keys are tuples
            with (param_name_a, param_name_b), and the values are covariance between the specified parameters.
            Note that fixed parameters are _not_ included in this matrix.
        errors: Store the errors associated with the component fit function.
    """"""
    parameters: list[str]
    free_parameters: list[str]
    fixed_parameters: list[str]
    values_at_minimum: dict[str, float]
    errors_on_parameters: dict[str, float]
    covariance_matrix: dict[tuple[str, str], float]
    errors: npt.NDArray[Any]

    @property
    def correlation_matrix(self) -> dict[tuple[str, str], float]:
        """"""The correlation matrix of the free parameters.

        These values are derived from the covariance matrix values stored in the fit.

        Note:
            This property caches the correlation matrix value so we don't have to calculate it every time.

        Args:
            None
        Returns:
            The correlation matrix of the fit result.
        """"""
        try:
            return self._correlation_matrix
        except AttributeError:

            def corr(i_name: str, j_name: str) -> float:
                """"""Calculate the correlation matrix (definition from iminuit) from the covariance matrix.""""""
                value = self.covariance_matrix[i_name, j_name] / (np.sqrt(self.covariance_matrix[i_name, i_name] * self.covariance_matrix[j_name, j_name]) + 1e-100)
                return float(value)
            matrix: dict[tuple[str, str], float] = {}
            for i_name in self.free_parameters:
                for j_name in self.free_parameters:
                    matrix[i_name, j_name] = corr(i_name, j_name)
            self._correlation_matrix = matrix
        return self._correlation_matrix","@dataclass
class BaseFitResult:
    '''Base fit result.
    This represents the most basic fit result.
    Attributes:
        parameters: Names of the parameters used in the fit.
        free_parameters: Names of the free parameters used in the fit.
        fixed_parameters: Names of the fixed parameters used in the fit.
        values_at_minimum: Contains the values of the full RP fit function at the minimum. Keys are the
            names of parameters, while values are the numerical values at convergence.
        errors_on_parameters: Contains the values of the errors associated with the parameters
            determined via the fit.
        covariance_matrix: Contains the values of the covariance matrix. Keys are tuples
            with (param_name_a, param_name_b), and the values are covariance between the specified parameters.
            Note that fixed parameters are _not_ included in this matrix.
        errors: Store the errors associated with the component fit function.
    '''
    @property
    def correlation_matrix(self) -> dict[tuple[str, str], float]:
        
        pass
    def correlation_matrix(self) -> dict[tuple[str, str], float]:
        '''Calculate the correlation matrix (definition from iminuit) from the covariance matrix.'''
        pass",snippet_301,3,
361458,AtteqCom/zsl,AtteqCom_zsl/src/zsl/errors.py,zsl.errors.ErrorHandler,"from abc import ABCMeta, abstractmethod

class ErrorHandler:
    """"""
    Custom error handler providing a response on a particular error.
    """"""
    __metaclass__ = ABCMeta

    @abstractmethod
    def can_handle(self, e):
        """"""
        Indicator if the handler is able to handle the given exception `e`.

        :param e: The exception that shall be determined if can be handled by the handler.
        :return: `True` or `False` depending on whether the handler can/should handle the method.
        """"""
        pass

    @abstractmethod
    def handle(self, e):
        """"""
        Handle the exception.

        :param e: The handled exception.
        :return: The error response for the exception.
        """"""
        pass","
class ErrorHandler:
    
    @abstractmethod
    def can_handle(self, e):
        
        pass
    @abstractmethod
    def handle(self, e):
        '''
        Handle the exception.
        :param e: The handled exception.
        :return: The error response for the exception.
        '''
        pass",snippet_302,3,"```python
from abc import ABC, abstractmethod

class ErrorHandler(ABC):
    """"""
    Base class for error handlers. Subclasses should implement the
    :py:meth:`can_handle` and :py:meth:`handle` methods to provide
    custom error handling logic.
    """"""

    @abstractmethod
    def can_handle(self, e):
        """"""
        Determine whether this handler can process the given exception.

        :param e: The exception instance to check.
        :return: ``True`` if the handler can process the exception,
                 ``False`` otherwise.
        """"""
        raise NotImplementedError

    @abstractmethod
    def handle(self, e):
        """"""
        Handle the exception and produce an error response.

        :param e: The exception instance to handle.
        :return: The error response for the exception.
        """"""
        raise NotImplementedError
```"
361483,AtteqCom/zsl,AtteqCom_zsl/src/zsl/resource/model_resource.py,zsl.resource.model_resource.ReadOnlyResourceMixin,"class ReadOnlyResourceMixin:
    """"""
    The mixin to be used to forbid the update/delete and create operations.
    Remember the Python's MRO and place this mixin at the right place in the inheritance declaration.

    .. automethod:: create
    .. automethod:: update
    .. automethod:: delete
    """"""
    OPERATION_CREATE = 'create'
    OPERATION_UPDATE = 'update'
    OPERATION_DELETE = 'delete'

    @staticmethod
    def create(params, args, data):
        """"""Raises exception.

        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.

        :raises ReadOnlyResourceUpdateOperationException: when accessed
        """"""
        raise ReadOnlyResourceUpdateOperationException(ReadOnlyResourceMixin.OPERATION_CREATE)

    @staticmethod
    def update(params, args, data):
        """"""Raises exception.

        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.

        :raises ReadOnlyResourceUpdateOperationException: when accessed
        """"""
        raise ReadOnlyResourceUpdateOperationException(ReadOnlyResourceMixin.OPERATION_UPDATE)

    @staticmethod
    def delete(params, args, data):
        """"""Raises exception.

        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.

        :raises ReadOnlyResourceUpdateOperationException: when accessed
        """"""
        raise ReadOnlyResourceUpdateOperationException(ReadOnlyResourceMixin.OPERATION_DELETE)","class ReadOnlyResourceMixin:
    '''
    The mixin to be used to forbid the update/delete and create operations.
    Remember the Python's MRO and place this mixin at the right place in the inheritance declaration.
    .. automethod:: create
    .. automethod:: update
    .. automethod:: delete
    '''
    @staticmethod
    def create(params, args, data):
        '''Raises exception.
        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.
        :raises ReadOnlyResourceUpdateOperationException: when accessed
        '''
        pass
    @staticmethod
    def update(params, args, data):
        
        pass
    @staticmethod
    def delete(params, args, data):
        '''Raises exception.
        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.
        :raises ReadOnlyResourceUpdateOperationException: when accessed
        '''
        pass",snippet_303,4,"```python
class ReadOnlyResourceUpdateOperationException(Exception):
    """"""Raised when attempting to modify a read‑only resource.""""""
    pass


class ReadOnlyResourceMixin:
    '''
    The mixin to be used to forbid the update/delete and create operations.
    Remember the Python's MRO and place this mixin at the right place in the inheritance declaration.
    .. automethod:: create
    .. automethod:: update
    .. automethod:: delete
    '''
    @staticmethod
    def create(params, args, data):
        '''Raises exception.
        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.
        :raises ReadOnlyResourceUpdateOperationException: when accessed
        '''
        raise ReadOnlyResourceUpdateOperationException()

    @staticmethod
    def update(params, args, data):
        '''Raises exception.
        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.
        :raises ReadOnlyResourceUpdateOperationException: when accessed
        '''
        raise ReadOnlyResourceUpdateOperationException()

    @staticmethod
    def delete(params, args, data):
        '''Raises exception.
        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.
        :raises ReadOnlyResourceUpdateOperationException: when accessed
        '''
        raise ReadOnlyResourceUpdateOperationException()
```"
361534,AtteqCom/zsl,AtteqCom_zsl/src/zsl/utils/command_dispatcher.py,zsl.utils.command_dispatcher.CommandDispatcher,"import inspect

class CommandDispatcher:
    """"""
    A simple class for command dictionary. A command is a function
    which can take named parameters.
    """"""

    def __init__(self):
        """"""
        Create command dictionary
        """"""
        self.commands = {}

    def command(self, fn):
        """"""
        Add method or function to dispatcher. Can be use as a nice
        decorator.

        :param fn: function or method
        :type fn: function
        :return: the same function
        :rtype: function
        """"""
        self.commands[fn.__name__] = fn
        return fn
    'alias for ``CommandDispatcher.command``'
    add_function = command

    def execute_command(self, command, args=None):
        """"""
        Execute a command

        :param command: name of the command
        :type command: str
        :param args: optional named arguments for command
        :type args: dict
        :return: the result of command
        :raises KeyError: if command is not found
        """"""
        if args is None:
            args = {}
        command_fn = self.commands[command]
        return command_fn(**args)

    def bound(self, instance):
        """"""
        Return a new dispatcher, which will switch all command functions
        with bounded methods of given instance matched by name. It will
        match only regular methods.

        :param instance: object instance
        :type instance: object
        :return: new Dispatcher
        :rtype: CommandDispatcher
        """"""
        bounded_dispatcher = CommandDispatcher()
        bounded_dispatcher.commands = self.commands.copy()
        for name in self.commands:
            method = getattr(instance, name, None)
            if method and inspect.ismethod(method) and (method.__self__ == instance):
                bounded_dispatcher.commands[name] = method
        return bounded_dispatcher","
class CommandDispatcher:
    
    def __init__(self):
        '''
        Create command dictionary
        '''
        pass
    def command(self, fn):
        '''
        Add method or function to dispatcher. Can be use as a nice
        decorator.
        :param fn: function or method
        :type fn: function
        :return: the same function
        :rtype: function
        '''
        pass
    def execute_command(self, command, args=None):
        
        pass
    def bound(self, instance):
        
        pass",snippet_304,5,"```python
import types
from collections.abc import Mapping, Sequence


class CommandDispatcher:
    def __init__(self):
        """"""
        Create command dictionary
        """"""
        self.commands = {}

    def command(self, fn):
        """"""
        Add method or function to dispatcher. Can be used as a decorator.
        :param fn: function or method
        :type fn: function
        :return: the same function
        :rtype: function
        """"""
        name = getattr(fn, ""__name__"", None)
        if not name:
            raise ValueError(""Command function must have a __name__ attribute"")
        self.commands[name] = fn
        return fn

    def execute_command(self, command, args=None):
        """"""
        Execute a registered command.
        :param command: command name (str) or callable
        :param args: arguments to pass to the command
        :return: result of the command
        """"""
        if callable(command):
            fn = command
        else:
            fn = self.commands.get(command)
            if fn is None:
                raise KeyError(f""Command '{command}' not found"")

        if args is None:
            return fn()
        if isinstance(args, Mapping):
            return fn(**args)
        if isinstance(args, (Sequence, tuple)):
            return fn(*args)
        # single non-iterable argument
        return fn(args)

    def bound(self, instance):
        """"""
        Return a new dispatcher with all commands bound to the given instance.
        :param instance: object to bind commands to
        :return: CommandDispatcher
        """"""
        bound_dispatcher = CommandDispatcher()
        for name, fn in self.commands.items():
            # Bind the function to the instance if it is a function
            if isinstance(fn, types.FunctionType):
                bound_dispatcher.commands[name] = types.MethodType(fn, instance)
            else:
                # If it's already a method or callable, keep it as is
                bound_dispatcher.commands[name] = fn
        return bound_dispatcher
```"
362561,reanahub/reana-db,reanahub_reana-db/reana_db/models.py,reana_db.models.QuotaBase,"class QuotaBase:
    """"""Quota base functionality.""""""

    def _get_quota_by_type(self, resource_type):
        """"""Aggregate quota usage by resource type.""""""

        def _get_health_status(usage, limit):
            """"""Calculate quota health status.""""""
            health = QuotaHealth.healthy
            if limit:
                percentage = usage / limit * 100
                if percentage >= 80:
                    if percentage >= 100:
                        health = QuotaHealth.critical
                    else:
                        health = QuotaHealth.warning
            return health.name
        quota_usage = 0
        quota_limit = 0
        unit = None
        for resource in self.resources:
            if resource.resource.type_ == resource_type:
                if unit and unit != resource.resource.unit:
                    raise Exception('Error while calculating quota usage. Not all resources of resource type {} use the same units.'.format(resource_type))
                unit = resource.resource.unit
                quota_usage += resource.quota_used
                if hasattr(resource, 'quota_limit'):
                    quota_limit += resource.quota_limit
        usage_dict = {'usage': {'raw': quota_usage, 'human_readable': ResourceUnit.human_readable_unit(unit, quota_usage)}}
        if quota_limit:
            usage_dict['limit'] = {'raw': quota_limit, 'human_readable': ResourceUnit.human_readable_unit(unit, quota_limit)}
            usage_dict['health'] = _get_health_status(quota_usage, quota_limit)
        return usage_dict

    def get_quota_usage(self):
        """"""Get quota usage information.""""""
        used_resource_types = set((res.resource.type_ for res in self.resources))
        return {resource_type.name: self._get_quota_by_type(resource_type) for resource_type in used_resource_types}","class QuotaBase:
    '''Quota base functionality.'''
    def _get_quota_by_type(self, resource_type):
        '''Aggregate quota usage by resource type.'''
        pass
        def _get_health_status(usage, limit):
                
                pass
    def get_quota_usage(self):
        
        pass",snippet_305,4,"class QuotaBase:
    '''Quota base functionality.'''
    def __init__(self, quota_data=None):
        """"""
        Initialize the QuotaBase with optional quota data.

        :param quota_data: Iterable of dicts with keys:
                           'resource_type', 'usage', 'limit'.
        """"""
        self.quota_data = list(quota_data or [])

    def _get_quota_by_type(self, resource_type):
        '''Aggregate quota usage by resource type.'''
        filtered = [q for q in self.quota_data if q.get('resource_type') == resource_type]
        total_usage = sum(q.get('usage', 0) for q in filtered)
        total_limit = sum(q.get('limit', 0) for q in filtered)
        return {'resource_type': resource_type, 'usage': total_usage, 'limit': total_limit}

    def _get_health_status(self, usage, limit):
        '''Determine health status based on usage and limit.'''
        if limit == 0:
            return 'unknown'
        percent = usage / limit
        if percent < 0.5:
            return 'healthy'
        elif percent < 0.8:
            return 'warning'
        else:
            return 'critical'

    def get_quota_usage(self):
        '''Return aggregated quota usage for all resource types.'''
        # Determine unique resource types
        resource_types = {q.get('resource_type') for q in self.quota_data}
        result = []
        for rt in resource_types:
            quota = self._get_quota_by_type(rt)
            status = self._get_health_status(quota['usage'], quota['limit'])
            quota['status'] = status
            result.append(quota)
        return result
"
