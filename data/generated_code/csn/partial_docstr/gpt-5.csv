id,repository_name,file_path,class_name,human_written_code,class_skeleton,snippet_id,total_program_units,gpt-5_generated_code
17249,jbarlow83/OCRmyPDF,src/ocrmypdf/quality.py,ocrmypdf.quality.OcrQualityDictionary,"import re
from collections.abc import Iterable

class OcrQualityDictionary:
    """"""Manages a dictionary for simple OCR quality checks.""""""

    def __init__(self, *, wordlist: Iterable[str]):
        """"""Construct a dictionary from a list of words.

        Words for which capitalization is important should be capitalized in the
        dictionary. Words that contain spaces or other punctuation will never match.
        """"""
        self.dictionary = set(wordlist)

    def measure_words_matched(self, ocr_text: str) -> float:
        """"""Check how many unique words in the OCR text match a dictionary.

        Words with mixed capitalized are only considered a match if the test word
        matches that capitalization.

        Returns:
            number of words that match / number
        """"""
        text = re.sub('[0-9_]+', ' ', ocr_text)
        text = re.sub('\\W+', ' ', text)
        text_words_list = re.split('\\s+', text)
        text_words = {w for w in text_words_list if len(w) >= 3}
        matches = 0
        for w in text_words:
            if w in self.dictionary or (w != w.lower() and w.lower() in self.dictionary):
                matches += 1
        if matches > 0:
            hit_ratio = matches / len(text_words)
        else:
            hit_ratio = 0.0
        return hit_ratio","
class OcrQualityDictionary:
    '''Manages a dictionary for simple OCR quality checks.'''
    def __init__(self, *, wordlist: Iterable[str]):
        '''Construct a dictionary from a list of words.
        Words for which capitalization is important should be capitalized in the
        dictionary. Words that contain spaces or other punctuation will never match.
        '''
        pass
    def measure_words_matched(self, ocr_text: str) -> float:
        
        pass",snippet_2,3,"from typing import Iterable, Set
import re


class OcrQualityDictionary:
    '''Manages a dictionary for simple OCR quality checks.'''
    _word_re = re.compile(r""[A-Za-z]+"")

    def __init__(self, *, wordlist: Iterable[str]):
        '''Construct a dictionary from a list of words.
        Words for which capitalization is important should be capitalized in the
        dictionary. Words that contain spaces or other punctuation will never match.
        '''
        self._insensitive: Set[str] = set()
        self._sensitive: Set[str] = set()

        for w in wordlist:
            if not isinstance(w, str):
                continue
            if not re.fullmatch(self._word_re, w):
                continue
            if w.islower():
                self._insensitive.add(w)
            else:
                self._sensitive.add(w)

    def measure_words_matched(self, ocr_text: str) -> float:
        tokens = re.findall(self._word_re, ocr_text or """")
        if not tokens:
            return 0.0
        matched = 0
        for t in tokens:
            if t in self._sensitive or t.lower() in self._insensitive:
                matched += 1
        return matched / len(tokens)"
19627,slundberg/shap,slundberg_shap/shap/_serializable.py,shap._serializable.Serializable,"import inspect
import pickle

class Serializable:
    """"""This is the superclass of all serializable objects.""""""

    def save(self, out_file):
        """"""Save the model to the given file stream.""""""
        pickle.dump(type(self), out_file)

    @classmethod
    def load(cls, in_file, instantiate=True):
        """"""This is meant to be overridden by subclasses and called with super.

        We return constructor argument values when not being instantiated. Since there are no
        constructor arguments for the Serializable class we just return an empty dictionary.
        """"""
        if instantiate:
            return cls._instantiated_load(in_file)
        return {}

    @classmethod
    def _instantiated_load(cls, in_file, **kwargs):
        """"""This is meant to be overridden by subclasses and called with super.

        We return constructor argument values (we have no values to load in this abstract class).
        """"""
        obj_type = pickle.load(in_file)
        if obj_type is None:
            return None
        if not inspect.isclass(obj_type) or (not issubclass(obj_type, cls) and obj_type is not cls):
            raise Exception(f'Invalid object type loaded from file. {obj_type} is not a subclass of {cls}.')
        constructor_args = obj_type.load(in_file, instantiate=False, **kwargs)
        used_args = inspect.getfullargspec(obj_type.__init__)[0]
        return obj_type(**{k: constructor_args[k] for k in constructor_args if k in used_args})","
class Serializable:
    '''This is the superclass of all serializable objects.'''
    def save(self, out_file):
        
        pass
    @classmethod
    def load(cls, in_file, instantiate=True):
        
        pass
    @classmethod
    def _instantiated_load(cls, in_file, **kwargs):
        
        pass",snippet_3,4,"class Serializable:
    '''This is the superclass of all serializable objects.'''
    _SERIAL_VERSION = 1
    _KEY_CLASS = ""__class__""
    _KEY_STATE = ""state""
    _KEY_VERSION = ""__version__""

    def _get_state(self):
        if hasattr(self, ""__getstate__""):
            return self.__getstate__()  # type: ignore[attr-defined]
        # Fallback to shallow copy of __dict__
        try:
            return dict(self.__dict__)
        except AttributeError as e:
            raise TypeError(f""{self.__class__.__name__} is not state-serializable"") from e

    @staticmethod
    def _class_path(cls):
        return f""{cls.__module__}.{cls.__qualname__}""

    @staticmethod
    def _resolve_class(path):
        import importlib
        parts = path.split(""."")
        for i in range(len(parts), 0, -1):
            module_name = ""."".join(parts[:i])
            try:
                module = importlib.import_module(module_name)
                attr_path = parts[i:]
                obj = module
                for attr in attr_path:
                    obj = getattr(obj, attr)
                return obj
            except (ModuleNotFoundError, AttributeError):
                continue
        raise ImportError(f""Cannot resolve class path: {path}"")

    @staticmethod
    def _is_path_like(obj):
        try:
            import os
            return isinstance(obj, (str, bytes, os.PathLike))
        except Exception:
            return isinstance(obj, (str, bytes))

    @staticmethod
    def _open_for_write(out_file):
        if Serializable._is_path_like(out_file):
            return open(out_file, ""wb""), True
        return out_file, False

    @staticmethod
    def _open_for_read(in_file):
        if Serializable._is_path_like(in_file):
            return open(in_file, ""rb""), True
        return in_file, False

    def save(self, out_file):
        import pickle
        payload = {
            self._KEY_VERSION: self._SERIAL_VERSION,
            self._KEY_CLASS: self._class_path(self.__class__),
            self._KEY_STATE: self._get_state(),
        }
        fh, must_close = self._open_for_write(out_file)
        try:
            pickle.dump(payload, fh, protocol=pickle.HIGHEST_PROTOCOL)
        finally:
            if must_close:
                fh.close()

    @classmethod
    def load(cls, in_file, instantiate=True):
        import pickle
        fh, must_close = cls._open_for_read(in_file)
        try:
            payload = pickle.load(fh)
        finally:
            if must_close:
                fh.close()

        if not isinstance(payload, dict) or cls._KEY_STATE not in payload:
            raise ValueError(""Invalid serialization payload"")

        # If only raw data desired
        if not instantiate:
            return payload.get(cls._KEY_STATE)

        return cls._instantiated_load(payload)

    @classmethod
    def _apply_state(cls, instance, state):
        if hasattr(instance, ""__setstate__""):
            instance.__setstate__(state)  # type: ignore[attr-defined]
            return
        if hasattr(instance, ""__dict__""):
            instance.__dict__.update(state)
            return
        raise TypeError(f""{instance.__class__.__name__} cannot accept state"")

    @classmethod
    def _instantiated_load(cls, in_file, **kwargs):
        # Accept either already-loaded payload dict or an input file
        if isinstance(in_file, dict) and cls._KEY_STATE in in_file:
            payload = in_file
        else:
            import pickle
            fh, must_close = cls._open_for_read(in_file)
            try:
                payload = pickle.load(fh)
            finally:
                if must_close:
                    fh.close()

        if not isinstance(payload, dict):
            raise ValueError(""Invalid serialization payload"")

        class_path = payload.get(cls._KEY_CLASS)
        state = payload.get(cls._KEY_STATE)

        if class_path is None:
            target_cls = cls
        else:
            target_cls = cls._resolve_class(class_path)

        if not issubclass(target_cls, cls):
            raise TypeError(f""Serialized type {target_cls} is not a subclass of {cls}"")

        # Create instance without calling __init__
        instance = target_cls.__new__(target_cls)

        # Optionally call __init__ with provided kwargs (for subclasses that expect construction)
        if kwargs:
            target_cls.__init__(instance, **kwargs)  # type: ignore[misc]

        cls._apply_state(instance, state)
        return instance"
19720,audreyr/cookiecutter,audreyr_cookiecutter/cookiecutter/environment.py,cookiecutter.environment.ExtensionLoaderMixin,"from cookiecutter.exceptions import UnknownExtension
from typing import Any

class ExtensionLoaderMixin:
    """"""Mixin providing sane loading of extensions specified in a given context.

    The context is being extracted from the keyword arguments before calling
    the next parent class in line of the child.
    """"""

    def __init__(self, *, context: dict[str, Any] | None=None, **kwargs: Any) -> None:
        """"""Initialize the Jinja2 Environment object while loading extensions.

        Does the following:

        1. Establishes default_extensions (currently just a Time feature)
        2. Reads extensions set in the cookiecutter.json _extensions key.
        3. Attempts to load the extensions. Provides useful error if fails.
        """"""
        context = context or {}
        default_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']
        extensions = default_extensions + self._read_extensions(context)
        try:
            super().__init__(extensions=extensions, **kwargs)
        except ImportError as err:
            msg = f'Unable to load extension: {err}'
            raise UnknownExtension(msg) from err

    def _read_extensions(self, context: dict[str, Any]) -> list[str]:
        """"""Return list of extensions as str to be passed on to the Jinja2 env.

        If context does not contain the relevant info, return an empty
        list instead.
        """"""
        try:
            extensions = context['cookiecutter']['_extensions']
        except KeyError:
            return []
        else:
            return [str(ext) for ext in extensions]","
class ExtensionLoaderMixin:
    '''Mixin providing sane loading of extensions specified in a given context.
    The context is being extracted from the keyword arguments before calling
    the next parent class in line of the child.
        '''
    def __init__(self, *, context: dict[str, Any] | None=None, **kwargs: Any) -> None:
        '''Initialize the Jinja2 Environment object while loading extensions.
        Does the following:
        1. Establishes default_extensions (currently just a Time feature)
        2. Reads extensions set in the cookiecutter.json _extensions key.
        3. Attempts to load the extensions. Provides useful error if fails.
        '''
        pass
    def _read_extensions(self, context: dict[str, Any]) -> list[str]:
        '''Return list of extensions as str to be passed on to the Jinja2 env.
        If context does not contain the relevant info, return an empty
        list instead.
        '''
        pass",snippet_5,3,"from __future__ import annotations

import importlib
from typing import Any


class ExtensionLoaderMixin:
    '''Mixin providing sane loading of extensions specified in a given context.
    The context is being extracted from the keyword arguments before calling
    the next parent class in line of the child.
        '''
    def __init__(self, *, context: dict[str, Any] | None = None, **kwargs: Any) -> None:
        '''Initialize the Jinja2 Environment object while loading extensions.
        Does the following:
        1. Establishes default_extensions (currently just a Time feature)
        2. Reads extensions set in the cookiecutter.json _extensions key.
        3. Attempts to load the extensions. Provides useful error if fails.
        '''
        # default extensions
        default_extensions: list[str] = ['jinja2_time.TimeExtension']

        # pop possible stray context from kwargs to avoid passing it upstream
        kwargs.pop('context', None)

        # read additional extensions from context
        ctx = context or {}
        ctx_extensions = self._read_extensions(ctx)

        # combine, de-duplicate while preserving order
        combined: list[str] = []
        seen: set[str] = set()
        for ext in (*default_extensions, *ctx_extensions):
            if ext not in seen:
                seen.add(ext)
                combined.append(ext)

        # Validate that extensions can be imported, but keep passing strings to super
        for ext in combined:
            try:
                self._ensure_importable(ext)
            except Exception as exc:  # provide a helpful error message
                raise RuntimeError(
                    f""Failed loading Jinja2 extension '{ext}'. ""
                    ""Ensure it is installed and the import path is correct.""
                ) from exc

        # hand off to the next class in MRO with extensions set
        super().__init__(extensions=combined, **kwargs)

    def _read_extensions(self, context: dict[str, Any]) -> list[str]:
        '''Return list of extensions as str to be passed on to the Jinja2 env.
        If context does not contain the relevant info, return an empty
        list instead.
        '''
        raw = context.get('_extensions', [])
        if raw is None:
            return []
        if isinstance(raw, str):
            return [raw] if raw else []
        if isinstance(raw, (list, tuple)):
            result: list[str] = []
            for item in raw:
                if not item:
                    continue
                if not isinstance(item, str):
                    raise TypeError(
                        f""Invalid extension entry {item!r} of type {type(item).__name__}; expected str.""
                    )
                result.append(item)
            return result
        raise TypeError(
            f""Invalid _extensions value of type {type(raw).__name__}; expected str or list/tuple of str.""
        )

    @staticmethod
    def _ensure_importable(path: str) -> None:
        # Accept paths like ""pkg.mod:Class"" or ""pkg.mod.Class""
        module_name: str
        attr_name: str | None = None

        if ':' in path:
            module_name, attr_name = path.split(':', 1)
        elif '.' in path:
            # Try to split on last dot to separate module and attribute/class
            parts = path.rsplit('.', 1)
            if len(parts) == 2:
                module_name, attr_name = parts
            else:
                module_name = path
                attr_name = None
        else:
            # simple module import
            module_name = path

        module = importlib.import_module(module_name)
        if attr_name:
            # attribute may be a class symbol; ensure it exists
            getattr(module, attr_name)  # will raise AttributeError if missing"
21340,Delgan/loguru,loguru/_recattrs.py,loguru._recattrs.RecordFile,"class RecordFile:
    """"""A class representing a file record with name and path.

    Attributes
    ----------
    name : str
        The name of the file
    path : str
        The path to the file
    """"""
    __slots__ = ('name', 'path')

    def __init__(self, name, path):
        """"""Initialize a RecordFile instance.

        Parameters
        ----------
        name : str
            The name of the file
        path : str
            The path to the file
        """"""
        self.name = name
        self.path = path

    def __repr__(self):
        """"""Return string representation of RecordFile.

        Returns
        -------
        str
            Formatted string with name and path
        """"""
        return '(name=%r, path=%r)' % (self.name, self.path)

    def __format__(self, spec):
        """"""Format the RecordFile instance.

        Parameters
        ----------
        spec : str
            Format specification

        Returns
        -------
        str
            Formatted name according to specification
        """"""
        return self.name.__format__(spec)","class RecordFile:
    
    def __init__(self, name, path):
        '''Initialize a RecordFile instance.
        Parameters
        ----------
        name : str
            The name of the file
        path : str
            The path to the file
        '''
        pass
    def __repr__(self):
        
        pass
    def __format__(self, spec):
        
        pass",snippet_6,4,"import os


class RecordFile:
    
    def __init__(self, name, path):
        '''Initialize a RecordFile instance.
        Parameters
        ----------
        name : str
            The name of the file
        path : str
            The path to the file
        '''
        if not isinstance(name, str):
            raise TypeError(""name must be a str"")
        if not isinstance(path, str):
            raise TypeError(""path must be a str"")
        self.name = name
        self.path = path

    def __repr__(self):
        return f""RecordFile(name={self.name!r}, path={self.path!r})""

    def __format__(self, spec):
        if not spec:
            return repr(self)
        key = spec.lower()
        if key in (""n"", ""name""):
            return self.name
        if key in (""p"", ""path""):
            return self.path
        if key in (""f"", ""full"", ""filepath"", ""fullpath""):
            return os.path.normpath(os.path.join(self.path, self.name))
        return format(repr(self), spec)"
21341,Delgan/loguru,loguru/_recattrs.py,loguru._recattrs.RecordLevel,"class RecordLevel:
    """"""A class representing the logging level record with name, number and icon.

    Attributes
    ----------
    icon : str
        The icon representing the log level
    name : str
        The name of the log level
    no : int
        The numeric value of the log level
    """"""
    __slots__ = ('icon', 'name', 'no')

    def __init__(self, name, no, icon):
        """"""Initialize a RecordLevel instance.

        Parameters
        ----------
        name : str
            The name of the log level
        no : int
            The numeric value of the log level
        icon : str
            The icon representing the log level
        """"""
        self.name = name
        self.no = no
        self.icon = icon

    def __repr__(self):
        """"""Return string representation of RecordLevel.

        Returns
        -------
        str
            Formatted string with name, number and icon
        """"""
        return '(name=%r, no=%r, icon=%r)' % (self.name, self.no, self.icon)

    def __format__(self, spec):
        """"""Format the RecordLevel instance.

        Parameters
        ----------
        spec : str
            Format specification

        Returns
        -------
        str
            Formatted name according to specification
        """"""
        return self.name.__format__(spec)","class RecordLevel:
    '''A class representing the logging level record with name, number and icon.
    Attributes
    ----------
    icon : str
        The icon representing the log level
    name : str
        The name of the log level
    no : int
        The numeric value of the log level
    '''
    def __init__(self, name, no, icon):
        
        pass
    def __repr__(self):
        '''Return string representation of RecordLevel.
        Returns
        -------
        str
            Formatted string with name, number and icon
        '''
        pass
    def __format__(self, spec):
        '''Format the RecordLevel instance.
        Parameters
        ----------
        spec : str
            Format specification
        Returns
        -------
        str
            Formatted name according to specification
        '''
        pass",snippet_7,4,"class RecordLevel:
    '''A class representing the logging level record with name, number and icon.
    Attributes
    ----------
    icon : str
        The icon representing the log level
    name : str
        The name of the log level
    no : int
        The numeric value of the log level
    '''
    def __init__(self, name, no, icon):
        self.name = str(name)
        self.no = int(no)
        self.icon = str(icon)

    def __repr__(self):
        '''Return string representation of RecordLevel.
        Returns
        -------
        str
            Formatted string with name, number and icon
        '''
        return f""{self.__class__.__name__}(name={self.name!r}, no={self.no!r}, icon={self.icon!r})""

    def __format__(self, spec):
        '''Format the RecordLevel instance.
        Parameters
        ----------
        spec : str
            Format specification
        Returns
        -------
        str
            Formatted name according to specification
        '''
        if spec is None:
            spec = ''
        return format(self.name, spec)"
21342,Delgan/loguru,loguru/_recattrs.py,loguru._recattrs.RecordProcess,"class RecordProcess:
    """"""A class representing a process record with ID and name.

    Attributes
    ----------
    id : int
        The process ID
    name : str
        The process name
    """"""
    __slots__ = ('id', 'name')

    def __init__(self, id_, name):
        """"""Initialize a RecordProcess instance.

        Parameters
        ----------
        id_ : int
            The process ID
        name : str
            The process name
        """"""
        self.id = id_
        self.name = name

    def __repr__(self):
        """"""Return string representation of RecordProcess.

        Returns
        -------
        str
            Formatted string with id and name
        """"""
        return '(id=%r, name=%r)' % (self.id, self.name)

    def __format__(self, spec):
        """"""Format the RecordProcess instance.

        Parameters
        ----------
        spec : str
            Format specification

        Returns
        -------
        str
            Formatted ID according to specification
        """"""
        return self.id.__format__(spec)","class RecordProcess:
    '''A class representing a process record with ID and name.
    Attributes
    ----------
    id : int
        The process ID
    name : str
        The process name
    '''
    def __init__(self, id_, name):
        '''Initialize a RecordProcess instance.
        Parameters
        ----------
        id_ : int
            The process ID
        name : str
            The process name
        '''
        pass
    def __repr__(self):
        
        pass
    def __format__(self, spec):
        '''Format the RecordProcess instance.
        Parameters
        ----------
        spec : str
            Format specification
        Returns
        -------
        str
            Formatted ID according to specification
        '''
        pass",snippet_8,4,"class RecordProcess:
    '''A class representing a process record with ID and name.
    Attributes
    ----------
    id : int
        The process ID
    name : str
        The process name
    '''
    def __init__(self, id_, name):
        '''Initialize a RecordProcess instance.
        Parameters
        ----------
        id_ : int
            The process ID
        name : str
            The process name
        '''
        if not isinstance(id_, int):
            raise TypeError(""id_ must be an int"")
        if not isinstance(name, str):
            raise TypeError(""name must be a str"")
        self.id = id_
        self.name = name

    def __repr__(self):
        return f""RecordProcess(id={self.id!r}, name={self.name!r})""

    def __format__(self, spec):
        '''Format the RecordProcess instance.
        Parameters
        ----------
        spec : str
            Format specification
        Returns
        -------
        str
            Formatted ID according to specification
        '''
        if spec is None or spec == """":
            return str(self.id)
        return format(self.id, spec)"
21343,Delgan/loguru,loguru/_recattrs.py,loguru._recattrs.RecordThread,"class RecordThread:
    """"""A class representing a thread record with ID and name.

    Attributes
    ----------
    id : int
        The thread ID
    name : str
        The thread name
    """"""
    __slots__ = ('id', 'name')

    def __init__(self, id_, name):
        """"""Initialize a RecordThread instance.

        Parameters
        ----------
        id_ : int
            The thread ID
        name : str
            The thread name
        """"""
        self.id = id_
        self.name = name

    def __repr__(self):
        """"""Return string representation of RecordThread.

        Returns
        -------
        str
            Formatted string with id and name
        """"""
        return '(id=%r, name=%r)' % (self.id, self.name)

    def __format__(self, spec):
        """"""Format the RecordThread instance.

        Parameters
        ----------
        spec : str
            Format specification

        Returns
        -------
        str
            Formatted ID according to specification
        """"""
        return self.id.__format__(spec)","class RecordThread:
    '''A class representing a thread record with ID and name.
    Attributes
    ----------
    id : int
        The thread ID
    name : str
        The thread name
    '''
    def __init__(self, id_, name):
        '''Initialize a RecordThread instance.
        Parameters
        ----------
        id_ : int
            The thread ID
        name : str
            The thread name
        '''
        pass
    def __repr__(self):
        
        pass
    def __format__(self, spec):
        '''Format the RecordThread instance.
        Parameters
        ----------
        spec : str
            Format specification
        Returns
        -------
        str
            Formatted ID according to specification
        '''
        pass",snippet_9,4,"class RecordThread:
    '''A class representing a thread record with ID and name.
    Attributes
    ----------
    id : int
        The thread ID
    name : str
        The thread name
    '''
    def __init__(self, id_, name):
        '''Initialize a RecordThread instance.
        Parameters
        ----------
        id_ : int
            The thread ID
        name : str
            The thread name
        '''
        if not isinstance(id_, int):
            raise TypeError(f""id_ must be int, got {type(id_).__name__}"")
        if not isinstance(name, str):
            raise TypeError(f""name must be str, got {type(name).__name__}"")
        self.id = id_
        self.name = name

    def __repr__(self):
        return f""{self.__class__.__name__}(id={self.id!r}, name={self.name!r})""

    def __format__(self, spec):
        '''Format the RecordThread instance.
        Parameters
        ----------
        spec : str
            Format specification
        Returns
        -------
        str
            Formatted ID according to specification
        '''
        return format(self.id, spec)"
33971,Miserlou/Zappa,Miserlou_Zappa/zappa/asynchronous.py,zappa.asynchronous.LambdaAsyncResponse,"import json
import uuid

class LambdaAsyncResponse:
    """"""
    Base Response Dispatcher class
    Can be used directly or subclassed if the method to send the message is changed.
    """"""

    def __init__(self, lambda_function_name=None, aws_region=None, capture_response=False, **kwargs):
        """""" """"""
        if kwargs.get('boto_session'):
            self.client = kwargs.get('boto_session').client('lambda')
        else:
            self.client = LAMBDA_CLIENT
        self.lambda_function_name = lambda_function_name
        self.aws_region = aws_region
        if capture_response:
            if ASYNC_RESPONSE_TABLE is None:
                print(""Warning! Attempted to capture a response without async_response_table configured in settings (you won't capture async responses)."")
                capture_response = False
                self.response_id = 'MISCONFIGURED'
            else:
                self.response_id = str(uuid.uuid4())
        else:
            self.response_id = None
        self.capture_response = capture_response

    def send(self, task_path, args, kwargs):
        """"""
        Create the message object and pass it to the actual sender.
        """"""
        message = {'task_path': task_path, 'capture_response': self.capture_response, 'response_id': self.response_id, 'args': args, 'kwargs': kwargs}
        self._send(message)
        return self

    def _send(self, message):
        """"""
        Given a message, directly invoke the lamdba function for this task.
        """"""
        message['command'] = 'zappa.asynchronous.route_lambda_task'
        payload = json.dumps(message).encode('utf-8')
        if len(payload) > LAMBDA_ASYNC_PAYLOAD_LIMIT:
            raise AsyncException('Payload too large for async Lambda call')
        self.response = self.client.invoke(FunctionName=self.lambda_function_name, InvocationType='Event', Payload=payload)
        self.sent = self.response.get('StatusCode', 0) == 202","
class LambdaAsyncResponse:
    
    def __init__(self, lambda_function_name=None, aws_region=None, capture_response=False, **kwargs):
        
        pass
    def send(self, task_path, args, kwargs):
        '''
        Create the message object and pass it to the actual sender.
        '''
        pass
    def _send(self, message):
        '''
        Given a message, directly invoke the lamdba function for this task.
        '''
        pass",snippet_10,4,"class LambdaAsyncResponse:
    
    def __init__(self, lambda_function_name=None, aws_region=None, capture_response=False, **kwargs):
        self.lambda_function_name = lambda_function_name
        self.capture_response = bool(capture_response)

        self._client = kwargs.pop(""lambda_client"", None)
        self._client_kwargs = kwargs

        if self._client is None:
            try:
                import boto3
            except ImportError as e:
                raise RuntimeError(""boto3 is required to use LambdaAsyncResponse without providing a lambda_client"") from e
            client_kwargs = dict(self._client_kwargs)
            if aws_region:
                client_kwargs.setdefault(""region_name"", aws_region)
            self._client = boto3.client(""lambda"", **client_kwargs)

        if not self.lambda_function_name:
            raise ValueError(""lambda_function_name must be provided"")

    def send(self, task_path, args, kwargs):
        '''
        Create the message object and pass it to the actual sender.
        '''
        if not isinstance(task_path, str) or not task_path:
            raise ValueError(""task_path must be a non-empty string"")
        if args is None:
            args = ()
        if kwargs is None:
            kwargs = {}

        if not isinstance(args, (list, tuple)):
            raise TypeError(""args must be a list or tuple"")
        if not isinstance(kwargs, dict):
            raise TypeError(""kwargs must be a dict"")

        message = {
            ""task_path"": task_path,
            ""args"": list(args),
            ""kwargs"": dict(kwargs),
        }
        return self._send(message)

    def _send(self, message):
        '''
        Given a message, directly invoke the lamdba function for this task.
        '''
        try:
            import json
        except ImportError as e:
            raise RuntimeError(""json module is required"") from e

        invocation_type = ""RequestResponse"" if self.capture_response else ""Event""
        payload_bytes = json.dumps(message, ensure_ascii=False, separators=("","", "":"")).encode(""utf-8"")

        response = self._client.invoke(
            FunctionName=self.lambda_function_name,
            InvocationType=invocation_type,
            Payload=payload_bytes,
        )

        result = {
            ""status_code"": response.get(""StatusCode""),
            ""request_id"": (response.get(""ResponseMetadata"") or {}).get(""RequestId""),
            ""function_error"": response.get(""FunctionError""),
            ""log_result"": response.get(""LogResult""),
            ""payload"": None,
        }

        payload_stream = response.get(""Payload"")
        if payload_stream is not None:
            try:
                raw = payload_stream.read()
                if isinstance(raw, bytes):
                    raw = raw.decode(""utf-8"", errors=""replace"")
                try:
                    parsed = json.loads(raw) if raw else None
                    result[""payload""] = parsed
                except Exception:
                    result[""payload""] = raw
            finally:
                try:
                    payload_stream.close()
                except Exception:
                    pass

        return result"
44118,LonamiWebs/Telethon,LonamiWebs_Telethon/telethon/crypto/aesctr.py,telethon.crypto.aesctr.AESModeCTR,"import pyaes

class AESModeCTR:
    """"""Wrapper around pyaes.AESModeOfOperationCTR mode with custom IV""""""

    def __init__(self, key, iv):
        """"""
        Initializes the AES CTR mode with the given key/iv pair.

        :param key: the key to be used as bytes.
        :param iv: the bytes initialization vector. Must have a length of 16.
        """"""
        assert isinstance(key, bytes)
        self._aes = pyaes.AESModeOfOperationCTR(key)
        assert isinstance(iv, bytes)
        assert len(iv) == 16
        self._aes._counter._counter = list(iv)

    def encrypt(self, data):
        """"""
        Encrypts the given plain text through AES CTR.

        :param data: the plain text to be encrypted.
        :return: the encrypted cipher text.
        """"""
        return self._aes.encrypt(data)

    def decrypt(self, data):
        """"""
        Decrypts the given cipher text through AES CTR

        :param data: the cipher text to be decrypted.
        :return: the decrypted plain text.
        """"""
        return self._aes.decrypt(data)","
class AESModeCTR:
    '''Wrapper around pyaes.AESModeOfOperationCTR mode with custom IV'''
    def __init__(self, key, iv):
        '''
        Initializes the AES CTR mode with the given key/iv pair.
        :param key: the key to be used as bytes.
        :param iv: the bytes initialization vector. Must have a length of 16.
        '''
        pass
    def encrypt(self, data):
        
        pass
    def decrypt(self, data):
        '''
        Decrypts the given cipher text through AES CTR
        :param data: the cipher text to be decrypted.
        :return: the decrypted plain text.
        '''
        pass",snippet_11,4,"import pyaes

class AESModeCTR:
    '''Wrapper around pyaes.AESModeOfOperationCTR mode with custom IV'''
    def __init__(self, key, iv):
        '''
        Initializes the AES CTR mode with the given key/iv pair.
        :param key: the key to be used as bytes.
        :param iv: the bytes initialization vector. Must have a length of 16.
        '''
        if not isinstance(key, (bytes, bytearray, memoryview)):
            raise TypeError(""key must be bytes-like"")
        if not isinstance(iv, (bytes, bytearray, memoryview)):
            raise TypeError(""iv must be bytes-like"")
        iv_bytes = bytes(iv)
        if len(iv_bytes) != 16:
            raise ValueError(""iv must be 16 bytes long"")
        self._key = bytes(key)
        self._iv_bytes = iv_bytes
        self._ctr_initial = int.from_bytes(self._iv_bytes, byteorder=""big"", signed=False)

    def _new_ctr_cipher(self):
        counter = pyaes.Counter(self._ctr_initial)  # big-endian by default
        return pyaes.AESModeOfOperationCTR(self._key, counter=counter)

    def encrypt(self, data):
        if not isinstance(data, (bytes, bytearray, memoryview)):
            raise TypeError(""data must be bytes-like"")
        cipher = self._new_ctr_cipher()
        return cipher.encrypt(bytes(data))

    def decrypt(self, data):
        '''
        Decrypts the given cipher text through AES CTR
        :param data: the cipher text to be decrypted.
        :return: the decrypted plain text.
        '''
        if not isinstance(data, (bytes, bytearray, memoryview)):
            raise TypeError(""data must be bytes-like"")
        cipher = self._new_ctr_cipher()
        return cipher.decrypt(bytes(data))"
44121,LonamiWebs/Telethon,LonamiWebs_Telethon/telethon/crypto/factorization.py,telethon.crypto.factorization.Factorization,"from random import randint

class Factorization:
    """"""
    Simple module to factorize large numbers really quickly.
    """"""

    @classmethod
    def factorize(cls, pq):
        """"""
        Factorizes the given large integer.

        Implementation from https://comeoncodeon.wordpress.com/2010/09/18/pollard-rho-brent-integer-factorization/.

        :param pq: the prime pair pq.
        :return: a tuple containing the two factors p and q.
        """"""
        if pq % 2 == 0:
            return (2, pq // 2)
        y, c, m = (randint(1, pq - 1), randint(1, pq - 1), randint(1, pq - 1))
        g = r = q = 1
        x = ys = 0
        while g == 1:
            x = y
            for i in range(r):
                y = (pow(y, 2, pq) + c) % pq
            k = 0
            while k < r and g == 1:
                ys = y
                for i in range(min(m, r - k)):
                    y = (pow(y, 2, pq) + c) % pq
                    q = q * abs(x - y) % pq
                g = cls.gcd(q, pq)
                k += m
            r *= 2
        if g == pq:
            while True:
                ys = (pow(ys, 2, pq) + c) % pq
                g = cls.gcd(abs(x - ys), pq)
                if g > 1:
                    break
        p, q = (g, pq // g)
        return (p, q) if p < q else (q, p)

    @staticmethod
    def gcd(a, b):
        """"""
        Calculates the Greatest Common Divisor.

        :param a: the first number.
        :param b: the second number.
        :return: GCD(a, b)
        """"""
        while b:
            a, b = (b, a % b)
        return a","
class Factorization:
    '''
    Simple module to factorize large numbers really quickly.
    '''
    @classmethod
    def factorize(cls, pq):
        
        pass
    @staticmethod
    def gcd(a, b):
        
        pass",snippet_12,3,"class Factorization:
    '''
    Simple module to factorize large numbers really quickly.
    '''
    @classmethod
    def factorize(cls, n):
        if n <= 1:
            return (n, 1)
        if n % 2 == 0:
            return (2, n // 2)
        # Trial divide small primes first
        small_primes = [
            3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47,
            53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107,
            109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167,
            173, 179, 181, 191, 193, 197, 199
        ]
        for p in small_primes:
            if n % p == 0:
                return (p, n // p)

        def is_probable_prime(a):
            if a < 2:
                return False
            # Deterministic Miller-Rabin for 64-bit, extended bases cover large ints reasonably
            # but Python ints are arbitrary precision; we'll use a common deterministic set for 128-bit
            # For general use, this is a robust probable-prime check.
            d = a - 1
            s = 0
            while d % 2 == 0:
                d //= 2
                s += 1

            def check(witness):
                x = pow(witness, d, a)
                if x == 1 or x == a - 1:
                    return True
                for _ in range(s - 1):
                    x = (x * x) % a
                    if x == a - 1:
                        return True
                return False

            # Small primes check
            for sp in [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37]:
                if a == sp:
                    return True
                if a % sp == 0:
                    return a == sp
            # Miller-Rabin bases sufficient for 128-bit integers; for larger, still very strong
            bases = [2, 3, 5, 7, 11, 13, 17]
            for b in bases:
                if b % a == 0:
                    continue
                if not check(b):
                    return False
            return True

        import random

        def pollards_rho(num):
            if num % 2 == 0:
                return 2
            if is_probable_prime(num):
                return num
            while True:
                x = random.randrange(2, num - 1)
                y = x
                c = random.randrange(1, num - 1)
                d = 1
                # Floyd's cycle detection
                while d == 1:
                    x = (x * x + c) % num
                    y = (y * y + c) % num
                    y = (y * y + c) % num
                    d = cls.gcd(abs(x - y), num)
                if d != num:
                    return d

        def factor_one(num):
            if num == 1:
                return 1
            if is_probable_prime(num):
                return num
            d = pollards_rho(num)
            if d == num:
                return num
            # Ensure we get a prime factor
            f1 = factor_one(d)
            return f1

        d = factor_one(n)
        if d == 1 or d == n:
            return (d, n // max(d, 1) if d != 0 else 0)
        return (d, n // d)

    @staticmethod
    def gcd(a, b):
        a = abs(a)
        b = abs(b)
        while b:
            a, b = b, a % b
        return a"
49235,blue-yonder/tsfresh,blue-yonder_tsfresh/tsfresh/examples/driftbif_simulation.py,tsfresh.examples.driftbif_simulation.velocity,"import numpy as np

class velocity:
    """"""
    Simulates the velocity of a dissipative soliton (kind of self organized particle) [6]_.
    The equilibrium velocity without noise R=0 for
    $	au>1.0/\\kappa_3$ is $\\kappa_3 \\sqrt{(tau - 1.0/\\kappa_3)/Q}.
    Before the drift-bifurcation $	au \\le 1.0/\\kappa_3$ the velocity is zero.

    References
    ----------

    .. [6] Andreas Kempa-Liehr (2013, p. 159-170)
        Dynamics of Dissipative Soliton
        Dissipative Solitons in Reaction Diffusion Systems.
        Springer: Berlin


    >>> ds = velocity(tau=3.5) # Dissipative soliton with equilibrium velocity 1.5e-3
    >>> print(ds.label) # Discriminating before or beyond Drift-Bifurcation
    1

    # Equilibrium velocity
    >>> print(ds.deterministic)
    0.0015191090506254991

    # Simulated velocity as a time series with 20000 time steps being disturbed by Gaussian white noise
    >>> v = ds.simulate(20000)
    """"""

    def __init__(self, tau=3.8, kappa_3=0.3, Q=1950.0, R=0.0003, delta_t=0.05, seed=None):
        """"""
        :param tau: Bifurcation parameter determining the intrinsic velocity of the dissipative soliton,
                    which is zero for tau<=1.0/kappa_3 and np.sqrt(kappa_3**3/Q * (tau - 1.0/kappa_3)) otherwise
        :type tau: float
        :param kappa_3: Inverse bifurcation point.
        :type kappa_3:
        :param Q: Shape parameter of dissipative soliton
        :type Q: float
        :param R: Noise amplitude
        :type R: float
        :param delta_t: temporal discretization
        :type delta_t: float
        """"""
        self.delta_t = delta_t
        self.kappa_3 = kappa_3
        self.Q = Q
        self.tau = tau
        self.a = self.delta_t * kappa_3 ** 2 * (tau - 1.0 / kappa_3)
        self.b = self.delta_t * Q / kappa_3
        self.label = int(tau > 1.0 / kappa_3)
        self.c = np.sqrt(self.delta_t) * R
        self.delta_t = self.delta_t
        if seed is not None:
            np.random.seed(seed)
        if tau <= 1.0 / kappa_3:
            self.deterministic = 0.0
        else:
            self.deterministic = kappa_3 ** 1.5 * np.sqrt((tau - 1.0 / kappa_3) / Q)

    def __call__(self, v):
        """"""
        returns deterministic dynamic = acceleration (without noise)

        :param v: initial velocity vector
        :rtype v: ndarray
        :return: velocity vector of next time step
        :return type: ndarray
        """"""
        return v * (1.0 + self.a - self.b * np.dot(v, v))

    def simulate(self, N, v0=np.zeros(2)):
        """"""

        :param N: number of time steps
        :type N: int
        :param v0: initial velocity vector
        :type v0: ndarray
        :return: time series of velocity vectors with shape (N, v0.shape[0])
        :rtype: ndarray
        """"""
        v = [v0]
        n = N - 1
        gamma = np.random.randn(n, v0.size)
        for i in range(n):
            next_v = self.__call__(v[i]) + self.c * gamma[i]
            v.append(next_v)
        v_vec = np.array(v)
        return v_vec","
class velocity:
    '''
    Simulates the velocity of a dissipative soliton (kind of self organized particle) [6]_.
    The equilibrium velocity without noise R=0 for
    $    au>1.0/\kappa_3$ is $\kappa_3 \sqrt{(tau - 1.0/\kappa_3)/Q}.
    Before the drift-bifurcation $    au \le 1.0/\kappa_3$ the velocity is zero.
    References
    ----------
    .. [6] Andreas Kempa-Liehr (2013, p. 159-170)
        Dynamics of Dissipative Soliton
        Dissipative Solitons in Reaction Diffusion Systems.
        Springer: Berlin
    >>> ds = velocity(tau=3.5) # Dissipative soliton with equilibrium velocity 1.5e-3
    >>> print(ds.label) # Discriminating before or beyond Drift-Bifurcation
    1
    # Equilibrium velocity
    >>> print(ds.deterministic)
    0.0015191090506254991
    # Simulated velocity as a time series with 20000 time steps being disturbed by Gaussian white noise
    >>> v = ds.simulate(20000)
    '''
    def __init__(self, tau=3.8, kappa_3=0.3, Q=1950.0, R=0.0003, delta_t=0.05, seed=None):
        
        pass
    def __call__(self, v):
        '''
        returns deterministic dynamic = acceleration (without noise)
        :param v: initial velocity vector
        :rtype v: ndarray
        :return: velocity vector of next time step
        :return type: ndarray
        '''
        pass
    def simulate(self, N, v0=np.zeros(2)):
        
        pass",snippet_13,4,"import numpy as np

class velocity:
    '''
    Simulates the velocity of a dissipative soliton (kind of self organized particle) [6]_.
    The equilibrium velocity without noise R=0 for
    $    au>1.0/\kappa_3$ is $\kappa_3 \sqrt{(tau - 1.0/\kappa_3)/Q}.
    Before the drift-bifurcation $    au \le 1.0/\kappa_3$ the velocity is zero.
    References
    ----------
    .. [6] Andreas Kempa-Liehr (2013, p. 159-170)
        Dynamics of Dissipative Soliton
        Dissipative Solitons in Reaction Diffusion Systems.
        Springer: Berlin
    >>> ds = velocity(tau=3.5) # Dissipative soliton with equilibrium velocity 1.5e-3
    >>> print(ds.label) # Discriminating before or beyond Drift-Bifurcation
    1
    # Equilibrium velocity
    >>> print(ds.deterministic)
    0.0015191090506254991
    # Simulated velocity as a time series with 20000 time steps being disturbed by Gaussian white noise
    >>> v = ds.simulate(20000)
    '''
    def __init__(self, tau=3.8, kappa_3=0.3, Q=1950.0, R=0.0003, delta_t=0.05, seed=None):
        self.tau = float(tau)
        self.kappa_3 = float(kappa_3)
        self.Q = float(Q)
        self.R = float(R)
        self.delta_t = float(delta_t)

        self._rng = np.random.default_rng(seed)

        threshold = 1.0 / self.kappa_3
        self.label = 1 if self.tau > threshold else 0

        mu = (self.tau - threshold) / self.Q
        mu = max(mu, 0.0)
        self.deterministic = self.kappa_3 * np.sqrt(mu)

    def __call__(self, v):
        '''
        returns deterministic dynamic = acceleration (without noise)
        :param v: initial velocity vector
        :rtype v: ndarray
        :return: velocity vector of next time step
        :return type: ndarray
        '''
        v = np.asarray(v, dtype=float)
        mu = (self.tau - 1.0 / self.kappa_3) / self.Q
        a = mu
        b = 1.0 / (self.kappa_3 ** 2)
        v_norm_sq = np.dot(v, v)
        return a * v - b * v_norm_sq * v

    def simulate(self, N, v0=np.zeros(2)):
        v = np.asarray(v0, dtype=float).copy()
        traj = np.empty((N, v.shape[0]), dtype=float)
        dt = self.delta_t
        noise_scale = np.sqrt(2.0 * self.R * dt) if self.R > 0.0 else 0.0

        for i in range(N):
            det = self.__call__(v)
            if noise_scale > 0.0:
                eta = self._rng.normal(0.0, 1.0, size=v.shape)
                v = v + det * dt + noise_scale * eta
            else:
                v = v + det * dt
            traj[i] = v

        return traj"
98891,peterbrittain/asciimatics,peterbrittain_asciimatics/asciimatics/effects.py,asciimatics.effects._Flake,"from random import randint, random, choice
from asciimatics.screen import Screen

class _Flake:
    """"""
    Track a single snow flake.
    """"""
    _snow_chars = '.+*'
    _drift_chars = ' ,;#@'

    def __init__(self, screen: Screen):
        """"""
        :param screen: The Screen being used for the Scene.
        """"""
        self._screen = screen
        self._x = 0
        self._y = 0
        self._rate = 0
        self._char = ''
        self._reseed()

    def _reseed(self):
        """"""
        Randomly create a new snowflake once this one is finished.
        """"""
        self._char = choice(self._snow_chars)
        self._rate = randint(1, 3)
        self._x = randint(0, self._screen.width - 1)
        self._y = self._screen.start_line + randint(0, self._rate)

    def update(self, reseed: bool):
        """"""
        Update that snowflake!

        :param reseed: Whether we are in the normal reseed cycle or not.
        """"""
        self._screen.print_at(' ', self._x, self._y)
        cell = None
        for _ in range(self._rate):
            self._y += 1
            cell = self._screen.get_from(self._x, self._y)
            if cell is None or cell[0] != 32:
                break
        if (cell is not None and cell[0] in [ord(x) for x in self._snow_chars + ' ']) and self._y < self._screen.start_line + self._screen.height:
            self._screen.print_at(self._char, self._x, self._y)
        else:
            self._y = min(self._y, self._screen.start_line + self._screen.height)
            drift_index = -1
            if cell:
                drift_index = self._drift_chars.find(chr(cell[0]))
            if 0 <= drift_index < len(self._drift_chars) - 1:
                drift_char = self._drift_chars[drift_index + 1]
                self._screen.print_at(drift_char, self._x, self._y)
            else:
                self._screen.print_at(',', self._x, self._y - 1)
            if reseed:
                self._reseed()","
class _Flake:
    '''
    Track a single snow flake.
    '''
    def __init__(self, screen: Screen):
        
        pass
    def _reseed(self):
        
        pass
    def update(self, reseed: bool):
        '''
        Update that snowflake!
        :param reseed: Whether we are in the normal reseed cycle or not.
        '''
        pass",snippet_14,4,"import random
from typing import Optional

class _Flake:
    '''
    Track a single snow flake.
    '''
    def __init__(self, screen):
        self._screen = screen
        self._width = getattr(screen, ""width"", None)
        self._height = getattr(screen, ""height"", None)
        if self._width is None or self._height is None:
            # Try curses-like api
            try:
                self._height, self._width = screen.getmaxyx()
            except Exception:
                raise ValueError(""Screen must provide width/height or getmaxyx()"")

        self._x: float = 0.0
        self._y: float = 0.0
        self._prev_x: Optional[int] = None
        self._prev_y: Optional[int] = None
        self._dx: float = 0.0
        self._speed: float = 0.0
        self._glyph: str = ""*""
        self._reseed()

    def _reseed(self):
        self._x = float(random.randrange(0, max(1, self._width)))
        # Start slightly above the top to stagger arrival
        self._y = float(-random.randrange(0, max(1, self._height // 4 + 1)))
        self._dx = random.choice([-0.5, -0.25, 0.0, 0.25, 0.5])
        # Speed biases toward slower flakes for better density
        self._speed = random.choice([0.25, 0.25, 0.5, 0.5, 0.75, 1.0])
        # Choose glyph by ""size""/speed
        if self._speed <= 0.25:
            self._glyph = "".""
        elif self._speed <= 0.5:
            self._glyph = """"
        elif self._speed <= 0.75:
            self._glyph = ""o""
        else:
            self._glyph = ""*""
        self._prev_x = None
        self._prev_y = None

    def update(self, reseed: bool):
        '''
        Update that snowflake!
        :param reseed: Whether we are in the normal reseed cycle or not.
        '''
        # Erase previous position
        if self._prev_x is not None and self._prev_y is not None:
            if 0 <= self._prev_x < self._width and 0 <= self._prev_y < self._height:
                try:
                    self._screen.print_at("" "", self._prev_x, self._prev_y)
                except Exception:
                    pass

        # Occasionally vary horizontal drift
        if random.random() < 0.1:
            self._dx += random.choice([-0.25, 0.0, 0.25])
            self._dx = max(-0.75, min(0.75, self._dx))

        # Apply movement
        self._x += self._dx
        self._y += self._speed

        # Handle edges
        out_of_bounds = (
            self._y >= self._height or
            self._x < 0 or
            self._x >= self._width
        )

        if out_of_bounds:
            if reseed:
                self._reseed()
            else:
                # Clamp within screen without reseeding
                self._x = max(0.0, min(float(self._width - 1), self._x))
                self._y = max(0.0, min(float(self._height - 1), self._y))

        # Draw current position
        ix = int(self._x)
        iy = int(self._y)
        if 0 <= ix < self._width and 0 <= iy < self._height:
            try:
                self._screen.print_at(self._glyph, ix, iy)
            except Exception:
                pass

        # Remember for erase next frame
        self._prev_x = ix
        self._prev_y = iy"
98892,peterbrittain/asciimatics,peterbrittain_asciimatics/asciimatics/effects.py,asciimatics.effects._Star,"from random import randint, random, choice
from asciimatics.screen import Screen

class _Star:
    """"""
    Simple class to represent a single star for the Stars special effect.
    """"""

    def __init__(self, screen: Screen, pattern: str):
        """"""
        :param screen: The Screen being used for the Scene.
        :param pattern: The pattern to loop through
        """"""
        self._screen = screen
        self._star_chars = pattern
        self._cycle = 0
        self._old_char = ''
        self._respawn()

    def _respawn(self):
        """"""
        Pick a random location for the star making sure it does
        not overwrite an existing piece of text.
        """"""
        self._cycle = randint(0, len(self._star_chars))
        height, width = self._screen.dimensions
        while True:
            self._x = randint(0, width - 1)
            self._y = self._screen.start_line + randint(0, height - 1)
            c = self._screen.get_from(self._x, self._y)
            if c is not None and c[0] == 32:
                break
        self._old_char = ' '

    def update(self):
        """"""
        Draw the star.
        """"""
        if not self._screen.is_visible(self._x, self._y):
            self._respawn()
        c = self._screen.get_from(self._x, self._y)
        if c is not None and c[0] not in (ord(self._old_char), 32):
            self._respawn()
        self._cycle += 1
        if self._cycle >= len(self._star_chars):
            self._cycle = 0
        new_char = self._star_chars[self._cycle]
        if new_char == self._old_char:
            return
        self._screen.print_at(new_char, self._x, self._y)
        self._old_char = new_char","
class _Star:
    
    def __init__(self, screen: Screen, pattern: str):
        '''
        :param screen: The Screen being used for the Scene.
        :param pattern: The pattern to loop through
        '''
        pass
    def _respawn(self):
        '''
        Pick a random location for the star making sure it does
        not overwrite an existing piece of text.
        '''
        pass
    def update(self):
        
        pass",snippet_15,4,"import random

class _Star:
    
    def __init__(self, screen, pattern: str):
        '''
        :param screen: The Screen being used for the Scene.
        :param pattern: The pattern to loop through
        '''
        self._screen = screen
        self._pattern = pattern or ""*""
        self._i = random.randrange(len(self._pattern))
        self._x = None
        self._y = None
        self._last_drawn = None
        self._respawn()

    def _get_char_at(self, x, y):
        try:
            cell = self._screen.get_from(x, y)
        except Exception:
            return "" ""
        # Try to handle both tuple and object with .char
        if cell is None:
            return "" ""
        ch = None
        if hasattr(cell, ""char""):
            ch = cell.char
        else:
            try:
                ch = cell[0]
            except Exception:
                ch = str(cell)
        if ch is None:
            return "" ""
        # Normalize to a single character if possible
        if isinstance(ch, (list, tuple)) and ch:
            ch = ch[0]
        return ch if isinstance(ch, str) and len(ch) > 0 else "" ""

    def _is_empty(self, x, y):
        ch = self._get_char_at(x, y)
        return ch == "" "" or ch == ""\x00""

    def _respawn(self):
        '''
        Pick a random location for the star making sure it does
        not overwrite an existing piece of text.
        '''
        width = getattr(self._screen, ""width"", None)
        height = getattr(self._screen, ""height"", None)
        if not isinstance(width, int) or not isinstance(height, int) or width <= 0 or height <= 0:
            # Fallback if screen doesn't expose width/height
            width = 80
            height = 24

        for _ in range(200):
            x = random.randrange(0, max(1, width))
            y = random.randrange(0, max(1, height))
            if self._is_empty(x, y):
                self._x, self._y = x, y
                self._last_drawn = None
                return

        # If we couldn't find an empty spot, place it anywhere and allow overwrite
        self._x = random.randrange(0, max(1, width))
        self._y = random.randrange(0, max(1, height))
        self._last_drawn = None

    def update(self):
        if self._x is None or self._y is None:
            self._respawn()

        # Advance pattern
        self._i = (self._i + 1) % len(self._pattern)
        ch = self._pattern[self._i]

        # If target cell has been occupied by something else, respawn
        current = self._get_char_at(self._x, self._y)
        if current not in ("" "", ""\x00"") and current != self._last_drawn:
            self._respawn()

        # Draw at current location
        try:
            self._screen.print_at(ch, self._x, self._y)
            self._last_drawn = ch
        except Exception:
            # If draw fails, try to respawn next time
            self._last_drawn = None
            self._x = None
            self._y = None"
98893,peterbrittain/asciimatics,peterbrittain_asciimatics/asciimatics/effects.py,asciimatics.effects._Trail,"from random import randint, random, choice
from asciimatics.screen import Screen

class _Trail:
    """"""
    Track a single trail  for a falling character effect (a la Matrix).
    """"""

    def __init__(self, screen: Screen, x: int):
        """"""
        :param screen: The Screen being used for the Scene.
        :param x: The column (y coordinate) for this trail to use.
        """"""
        self._screen = screen
        self._x = x
        self._y = 0
        self._life = 0
        self._rate = 0
        self._clear = True
        self._maybe_reseed(True)

    def _maybe_reseed(self, normal: bool):
        """"""
        Randomly create a new column once this one is finished.
        """"""
        self._y += self._rate
        self._life -= 1
        if self._life <= 0:
            self._clear = not self._clear if normal else True
            self._rate = randint(1, 2)
            if self._clear:
                self._y = 0
                self._life = self._screen.height // self._rate
            else:
                self._y = randint(0, self._screen.height // 2) - self._screen.height // 4
                self._life = randint(1, self._screen.height - self._y) // self._rate

    def update(self, reseed: bool):
        """"""
        Update that trail!

        :param reseed: Whether we are in the normal reseed cycle or not.
        """"""
        if self._clear:
            for i in range(0, 3):
                self._screen.print_at(' ', self._x, self._screen.start_line + self._y + i)
            self._maybe_reseed(reseed)
        else:
            for i in range(0, 3):
                self._screen.print_at(chr(randint(32, 126)), self._x, self._screen.start_line + self._y + i, Screen.COLOUR_GREEN)
            for i in range(4, 6):
                self._screen.print_at(chr(randint(32, 126)), self._x, self._screen.start_line + self._y + i, Screen.COLOUR_GREEN, Screen.A_BOLD)
            self._maybe_reseed(reseed)","
class _Trail:
    '''
    Track a single trail  for a falling character effect (a la Matrix).
    '''
    def __init__(self, screen: Screen, x: int):
        
        pass
    def _maybe_reseed(self, normal: bool):
        
        pass
    def update(self, reseed: bool):
        '''
        Update that trail!
        :param reseed: Whether we are in the normal reseed cycle or not.
        '''
        pass",snippet_16,4,"class _Trail:
    '''
    Track a single trail  for a falling character effect (a la Matrix).
    '''
    def __init__(self, screen, x: int):
        self.screen = screen
        self.x = int(x)

        # Determine screen height as flexibly as possible
        height = None
        for getter in (
            lambda s: getattr(s, ""height""),
            lambda s: getattr(s, ""rows""),
            lambda s: s.get_height(),
            lambda s: s.getmaxyx()[0],
        ):
            try:
                height = getter(screen)
                if isinstance(height, int) and height > 0:
                    break
            except Exception:
                continue
        if not isinstance(height, int) or height <= 0:
            height = 24
        self.height = height

        # Characters and RNG
        self._chars = ""01abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789""
        try:
            import random as _random
        except Exception:  # pragma: no cover
            _random = None
        self._random = _random

        # Trail state
        self.length = self._randint(6, max(8, self.height // 2))
        self.y = -self._randint(0, self.height)  # start off-screen above
        self.speed = 1  # cells per update
        self._last_drawn = []

    def _randint(self, a: int, b: int) -> int:
        if self._random is None:
            # fallback deterministic
            return (a + b) // 2
        return self._random.randint(a, b)

    def _choice(self, seq):
        if self._random is None:
            return seq[0]
        return self._random.choice(seq)

    def _maybe_reseed(self, normal: bool):
        # normal indicates whether we are in the normal reseed cycle
        # Reseed if trail fully past bottom, or with a small random chance
        should = False
        if normal:
            if self.y - self.length > self.height:
                should = True
            else:
                # small chance to refresh variety
                if self._random and self._random.random() < 0.005:
                    should = True
        else:
            should = True

        if should:
            self.length = self._randint(6, max(8, self.height // 2))
            self.y = -self._randint(0, self.height)
            self.speed = 1
            return True
        return False

    def _draw_cell(self, x: int, y: int, ch: str, head: bool):
        # Try a variety of possible screen APIs
        s = self.screen
        # Prefer curses-like addch(y, x, ch)
        try:
            if hasattr(s, ""addch""):
                return s.addch(y, x, ch)
        except Exception:
            pass
        # Generic draw(x, y, ch)
        try:
            if hasattr(s, ""draw""):
                return s.draw(x, y, ch)
        except Exception:
            pass
        # set_cell(x, y, ch)
        try:
            if hasattr(s, ""set_cell""):
                return s.set_cell(x, y, ch)
        except Exception:
            pass
        # putch(x, y, ch)
        try:
            if hasattr(s, ""putch""):
                return s.putch(x, y, ch)
        except Exception:
            pass
        # write_at(x, y, ch)
        try:
            if hasattr(s, ""write_at""):
                return s.write_at(x, y, ch)
        except Exception:
            pass
        # set(x, y, ch)
        try:
            if hasattr(s, ""set""):
                return s.set(x, y, ch)
        except Exception:
            pass
        # put(x, y, ch)
        try:
            if hasattr(s, ""put""):
                return s.put(x, y, ch)
        except Exception:
            pass
        # If none available, just record the last drawn for external consumption
        self._last_drawn.append((x, y, ch, head))

    def update(self, reseed: bool):
        '''
        Update that trail!
        :param reseed: Whether we are in the normal reseed cycle or not.
        '''
        self._last_drawn = []
        self._maybe_reseed(bool(reseed))

        # Advance head
        self.y += self.speed

        # Draw visible part of the trail
        head_ch = self._choice(self._chars)
        for i in range(self.length):
            yy = self.y - i
            if 0 <= yy < self.height:
                ch = head_ch if i == 0 else self._choice(self._chars)
                self._draw_cell(self.x, yy, ch, head=(i == 0))

        # Optionally return what was drawn for screens that don't implement drawing
        return list(self._last_drawn) if self._last_drawn else None"
99108,ConsenSys/mythril-classic,ConsenSys_mythril-classic/mythril/analysis/ops.py,mythril.analysis.ops.Variable,"class Variable:
    """"""The representation of a variable with value and type.""""""

    def __init__(self, val, _type):
        """"""

        :param val:
        :param _type:
        """"""
        self.val = val
        self.type = _type

    def __str__(self):
        """"""

        :return:
        """"""
        return str(self.val)","class Variable:
    '''The representation of a variable with value and type.'''
    def __init__(self, val, _type):
        '''
        :param val:
        :param _type:
        '''
        pass
    def __str__(self):
        
        pass",snippet_17,3,"class Variable:
    '''The representation of a variable with value and type.'''

    _TYPE_MAP = {
        'int': int,
        'float': float,
        'str': str,
        'bool': bool,
        'list': list,
        'dict': dict,
        'tuple': tuple,
        'set': set,
        'bytes': bytes,
    }

    def __init__(self, val, _type):
        '''
        :param val:
        :param _type:
        '''
        self.type = self._resolve_type(_type)
        self.value = self._coerce_value(val, self.type)

    def _resolve_type(self, _type):
        if isinstance(_type, type):
            return _type
        if isinstance(_type, str):
            t = self._TYPE_MAP.get(_type.strip().lower())
            if t is not None:
                return t
        raise TypeError(""Unsupported type specification"")

    def _coerce_value(self, val, tp):
        if isinstance(val, tp):
            return val
        if tp is bool:
            if isinstance(val, str):
                v = val.strip().lower()
                if v in ('true', '1', 'yes', 'y', 't'):
                    return True
                if v in ('false', '0', 'no', 'n', 'f'):
                    return False
                raise ValueError(""Cannot convert string to bool"")
            if isinstance(val, (int, float)):
                return bool(val)
        try:
            return tp(val)
        except Exception as e:
            raise ValueError(f""Cannot convert value to {tp.__name__}"") from e

    def __str__(self):
        return f""{self.value} ({self.type.__name__})"""
100863,bigchaindb/bigchaindb,bigchaindb_bigchaindb/bigchaindb/validation.py,bigchaindb.validation.BaseValidationRules,"class BaseValidationRules:
    """"""Base validation rules for BigchainDB.

    A validation plugin must expose a class inheriting from this one via an entry_point.

    All methods listed below must be implemented.
    """"""

    @staticmethod
    def validate_transaction(bigchaindb, transaction):
        """"""See :meth:`bigchaindb.models.Transaction.validate`
        for documentation.
        """"""
        return transaction.validate(bigchaindb)

    @staticmethod
    def validate_block(bigchaindb, block):
        """"""See :meth:`bigchaindb.models.Block.validate` for documentation.""""""
        return block.validate(bigchaindb)","class BaseValidationRules:
    
    @staticmethod
    def validate_transaction(bigchaindb, transaction):
        '''See :meth:`bigchaindb.models.Transaction.validate`
        for documentation.
        '''
        pass
    @staticmethod
    def validate_block(bigchaindb, block):
        
        pass",snippet_22,3,"class BaseValidationRules:
    
    @staticmethod
    def validate_transaction(bigchaindb, transaction):
        '''See :meth:`bigchaindb.models.Transaction.validate`
        for documentation.
        '''
        return transaction.validate(bigchaindb)

    @staticmethod
    def validate_block(bigchaindb, block):
        return block.validate(bigchaindb)"
100865,bigchaindb/bigchaindb,bigchaindb_bigchaindb/bigchaindb/web/strip_content_type_middleware.py,bigchaindb.web.strip_content_type_middleware.StripContentTypeMiddleware,"class StripContentTypeMiddleware:
    """"""WSGI middleware to strip Content-Type header for GETs.""""""

    def __init__(self, app):
        """"""Create the new middleware.

        Args:
            app: a flask application
        """"""
        self.app = app

    def __call__(self, environ, start_response):
        """"""Run the middleware and then call the original WSGI application.""""""
        if environ['REQUEST_METHOD'] == 'GET':
            try:
                del environ['CONTENT_TYPE']
            except KeyError:
                pass
            else:
                logger.debug('Remove header ""Content-Type"" from GET request')
        return self.app(environ, start_response)","class StripContentTypeMiddleware:
    
    def __init__(self, app):
        '''Create the new middleware.
        Args:
            app: a flask application
        '''
        pass
    def __call__(self, environ, start_response):
        
        pass",snippet_23,3,"class StripContentTypeMiddleware:
    
    def __init__(self, app):
        '''Create the new middleware.
        Args:
            app: a flask application
        '''
        self.app = app

    def __call__(self, environ, start_response):
        method = environ.get('REQUEST_METHOD', 'GET').upper()
        content_length = environ.get('CONTENT_LENGTH')

        if not content_length or content_length == '0' or method in ('GET', 'HEAD', 'DELETE', 'OPTIONS'):
            environ.pop('CONTENT_TYPE', None)
            environ.pop('HTTP_CONTENT_TYPE', None)

        return self.app(environ, start_response)"
100895,ansible/molecule,src/molecule/app.py,molecule.app.App,"from pathlib import Path
from subprocess import CalledProcessError, CompletedProcess
from molecule.console import original_stderr
from ansible_compat.runtime import Runtime
from molecule.ansi_output import CommandBorders
from molecule.util import print_environment_vars

class App:
    """"""App class that keep runtime status.""""""

    def __init__(self, path: Path) -> None:
        """"""Create a new app instance.

        Args:
            path: The path to the project.
        """"""
        self.runtime = Runtime(project_dir=path, isolated=False)

    def run_command(self, cmd: str | list[str], env: dict[str, str] | None=None, cwd: Path | None=None, *, debug: bool=False, echo: bool=False, quiet: bool=False, check: bool=False, command_borders: bool=False) -> CompletedProcess[str]:
        """"""Execute the given command and returns None.

        Args:
            cmd: A list of strings containing the command to run.
            env: A dict containing the shell's environment.
            cwd: An optional Path to the working directory.
            debug: An optional bool to toggle debug output.
            echo: An optional bool to toggle command echo.
            quiet: An optional bool to toggle command output.
            check: An optional bool to toggle command error checking.
            command_borders: An optional bool to enable borders around command output.

        Returns:
            A completed process object.

        Raises:
            CalledProcessError: If return code is nonzero and check is True.
        """"""
        if debug:
            print_environment_vars(env)
        borders = None
        if command_borders:
            borders = CommandBorders(cmd=cmd, original_stderr=original_stderr)
        result = self.runtime.run(args=cmd, env=env, cwd=cwd, tee=True, set_acp=False)
        if borders:
            borders.finalize(result.returncode)
        if result.returncode != 0 and check:
            raise CalledProcessError(returncode=result.returncode, cmd=result.args, output=result.stdout, stderr=result.stderr)
        return result","
class App:
    
    def __init__(self, path: Path) -> None:
        
        pass
    def run_command(self, cmd: str | list[str], env: dict[str, str] | None=None, cwd: Path | None=None, *, debug: bool=False, echo: bool=False, quiet: bool=False, check: bool=False, command_borders: bool=False) -> CompletedProcess[str]:
        '''Execute the given command and returns None.
        Args:
            cmd: A list of strings containing the command to run.
            env: A dict containing the shell's environment.
            cwd: An optional Path to the working directory.
            debug: An optional bool to toggle debug output.
            echo: An optional bool to toggle command echo.
            quiet: An optional bool to toggle command output.
            check: An optional bool to toggle command error checking.
            command_borders: An optional bool to enable borders around command output.
        Returns:
            A completed process object.
        Raises:
            CalledProcessError: If return code is nonzero and check is True.
        '''
        pass",snippet_24,3,"from __future__ import annotations

import os
import shlex
import subprocess
import sys
from pathlib import Path
from subprocess import CalledProcessError, CompletedProcess


class App:
    def __init__(self, path: Path) -> None:
        self.path = Path(path)

    def run_command(
        self,
        cmd: str | list[str],
        env: dict[str, str] | None = None,
        cwd: Path | None = None,
        *,
        debug: bool = False,
        echo: bool = False,
        quiet: bool = False,
        check: bool = False,
        command_borders: bool = False,
    ) -> CompletedProcess[str]:
        if isinstance(cmd, str):
            cmd_for_display = cmd
            use_shell = True
            run_cmd: str | list[str] = cmd
        else:
            try:
                cmd_for_display = shlex.join(cmd)
            except Exception:
                cmd_for_display = "" "".join(shlex.quote(c) for c in cmd)
            use_shell = False
            run_cmd = cmd

        run_cwd = Path(cwd) if cwd is not None else self.path

        base_env = os.environ.copy()
        if env:
            base_env.update(env)

        if echo:
            print(f""$ {cmd_for_display}"", file=sys.stderr if quiet else sys.stdout)

        if debug:
            print(f""[debug] cwd: {run_cwd}"", file=sys.stderr)
            if env:
                print(f""[debug] env overrides: {', '.join(sorted(env.keys()))}"", file=sys.stderr)

        # If we want borders, we must capture output to print inside borders, and
        # then optionally raise if check=True.
        if command_borders and not quiet:
            proc = subprocess.run(
                run_cmd,
                shell=use_shell,
                cwd=str(run_cwd) if run_cwd is not None else None,
                env=base_env,
                text=True,
                capture_output=True,
                check=False,
            )
            border = ""="" * 8
            print(f""{border} BEGIN: {cmd_for_display} {border}"")
            if proc.stdout:
                print(proc.stdout, end="""" if proc.stdout.endswith(""\n"") else ""\n"")
            if proc.stderr:
                # Label stderr to distinguish, only if non-empty
                print(""--- stderr ---"")
                print(proc.stderr, end="""" if proc.stderr.endswith(""\n"") else ""\n"")
            print(f""{border} END ({proc.returncode}): {cmd_for_display} {border}"")

            if check and proc.returncode != 0:
                raise CalledProcessError(proc.returncode, run_cmd, output=proc.stdout, stderr=proc.stderr)

            return proc

        # Without borders, decide output handling based on quiet
        stdout_setting = subprocess.DEVNULL if quiet else None
        stderr_setting = subprocess.DEVNULL if quiet else None

        proc = subprocess.run(
            run_cmd,
            shell=use_shell,
            cwd=str(run_cwd) if run_cwd is not None else None,
            env=base_env,
            text=True,
            stdout=stdout_setting,
            stderr=stderr_setting,
            check=check,
        )
        return proc"
104788,autokey/autokey,autokey_autokey/lib/autokey/scripting/system.py,autokey.scripting.system.System,"import subprocess

class System:
    """"""
    Simplified access to some system commands.
    """"""

    @staticmethod
    def exec_command(command, getOutput=True):
        """"""
        Execute a shell command

        Usage: C{system.exec_command(command, getOutput=True)}

        Set getOutput to False if the command does not exit and return immediately. Otherwise
        AutoKey will not respond to any hotkeys/abbreviations etc until the process started
        by the command exits.

        @param command: command to be executed (including any arguments) - e.g. ""ls -l""
        @param getOutput: whether to capture the (stdout) output of the command
        @raise subprocess.CalledProcessError: if the command returns a non-zero exit code
        """"""
        if getOutput:
            with subprocess.Popen(command, shell=True, bufsize=-1, stdout=subprocess.PIPE, universal_newlines=True) as p:
                output = p.communicate()[0]
                output = output.rstrip('\n')
                if p.returncode:
                    raise subprocess.CalledProcessError(p.returncode, output)
                return output
        else:
            subprocess.Popen(command, shell=True, bufsize=-1)

    @staticmethod
    def create_file(file_name, contents=''):
        """"""
        Create a file with contents

        Usage: C{system.create_file(fileName, contents="""")}

        @param fileName: full path to the file to be created
        @param contents: contents to insert into the file
        """"""
        with open(file_name, 'w') as written_file:
            written_file.write(contents)","
class System:
    
    @staticmethod
    def exec_command(command, getOutput=True):
        
        pass
    @staticmethod
    def create_file(file_name, contents=''):
        '''
        Create a file with contents
        Usage: C{system.create_file(fileName, contents="""")}
        @param fileName: full path to the file to be created
        @param contents: contents to insert into the file
        '''
        pass",snippet_26,3,"class System:
    
    @staticmethod
    def exec_command(command, getOutput=True):
        import subprocess
        import shlex

        try:
            if isinstance(command, (list, tuple)):
                cmd = list(command)
                shell = False
            else:
                cmd = command
                shell = True

            result = subprocess.run(
                cmd if shell else cmd,
                shell=shell,
                capture_output=getOutput,
                text=True
            )

            if getOutput:
                return (result.stdout, result.stderr, result.returncode)
            else:
                return result.returncode
        except Exception as e:
            if getOutput:
                return ("""", str(e), -1)
            else:
                return -1

    @staticmethod
    def create_file(file_name, contents=''):
        '''
        Create a file with contents
        Usage: C{system.create_file(fileName, contents="""")}
        @param fileName: full path to the file to be created
        @param contents: contents to insert into the file
        '''
        import os

        directory = os.path.dirname(os.path.abspath(file_name))
        if directory and not os.path.exists(directory):
            os.makedirs(directory, exist_ok=True)

        with open(file_name, 'w', encoding='utf-8') as f:
            f.write(contents if contents is not None else '')

        return True"
106092,mikedh/trimesh,trimesh/collision.py,trimesh.collision.DistanceData,"class DistanceData:
    """"""
    Data structure for holding information about a distance query.
    """"""

    def __init__(self, names, result):
        """"""
        Initialize a DistanceData.

        Parameters
        ----------
        names : list of str
          The names of the two objects in order.
        contact : fcl.DistanceResult
          The distance query result.
        """"""
        self.names = set(names)
        self._inds = {names[0]: result.b1, names[1]: result.b2}
        self._points = {names[0]: result.nearest_points[0], names[1]: result.nearest_points[1]}
        self._distance = result.min_distance

    @property
    def distance(self):
        """"""
        Returns the distance between the two objects.

        Returns
        -------
        distance : float
          The euclidean distance between the objects.
        """"""
        return self._distance

    def index(self, name):
        """"""
        Returns the index of the closest face for the mesh with
        the given name.

        Parameters
        ----------
        name : str
          The name of the target object.

        Returns
        -------
        index : int
          The index of the face in collisoin.
        """"""
        return self._inds[name]

    def point(self, name):
        """"""
        The 3D point of closest distance on the mesh with the given name.

        Parameters
        ----------
        name : str
          The name of the target object.

        Returns
        -------
        point : (3,) float
          The closest point.
        """"""
        return self._points[name]","class DistanceData:
    
    def __init__(self, names, result):
        '''
        Initialize a DistanceData.
        Parameters
        ----------
        names : list of str
          The names of the two objects in order.
        contact : fcl.DistanceResult
          The distance query result.
        '''
        pass
    @property
    def distance(self):
        '''
        Returns the distance between the two objects.
        Returns
        -------
        distance : float
          The euclidean distance between the objects.
        '''
        pass
    def index(self, name):
        '''
        Returns the index of the closest face for the mesh with
        the given name.
        Parameters
        ----------
        name : str
          The name of the target object.
        Returns
        -------
        index : int
          The index of the face in collisoin.
        '''
        pass
    def point(self, name):
        '''
        The 3D point of closest distance on the mesh with the given name.
        Parameters
        ----------
        name : str
          The name of the target object.
        Returns
        -------
        point : (3,) float
          The closest point.
        '''
        pass",snippet_28,5,"class DistanceData:
    
    def __init__(self, names, result):
        '''
        Initialize a DistanceData.
        Parameters
        ----------
        names : list of str
          The names of the two objects in order.
        contact : fcl.DistanceResult
          The distance query result.
        '''
        if not isinstance(names, (list, tuple)) or len(names) != 2:
            raise ValueError(""names must be a list/tuple of two strings"")
        self._names = (str(names[0]), str(names[1]))
        self._result = result
        self._name_to_idx = {self._names[0]: 0, self._names[1]: 1}

    @property
    def distance(self):
        '''
        Returns the distance between the two objects.
        Returns
        -------
        distance : float
          The euclidean distance between the objects.
        '''
        # Prefer library-provided min_distance when available
        if hasattr(self._result, ""min_distance""):
            d = self._result.min_distance
            if d is not None:
                try:
                    return float(d)
                except (TypeError, ValueError):
                    pass
        # Fallback: compute from nearest points
        p1, p2 = self._get_points_pair()
        dx = p1[0] - p2[0]
        dy = p1[1] - p2[1]
        dz = p1[2] - p2[2]
        return (dx * dx + dy * dy + dz * dz) ** 0.5

    def index(self, name):
        '''
        Returns the index of the closest face for the mesh with
        the given name.
        Parameters
        ----------
        name : str
          The name of the target object.
        Returns
        -------
        index : int
          The index of the face in collisoin.
        '''
        idx = self._name_to_idx.get(name)
        if idx is None:
            raise ValueError(f""Unknown object name: {name!r}. Expected one of {self._names}"")
        # Try common attribute names used by python-fcl / FCL
        # Primary: id1/id2
        if hasattr(self._result, ""id1"") and hasattr(self._result, ""id2""):
            return int(self._result.id1 if idx == 0 else self._result.id2)
        # Alternative: b1/b2 (primitive indices)
        if hasattr(self._result, ""b1"") and hasattr(self._result, ""b2""):
            return int(self._result.b1 if idx == 0 else self._result.b2)
        raise AttributeError(""DistanceResult does not provide primitive/face indices (id1/id2 or b1/b2)"")

    def point(self, name):
        '''
        The 3D point of closest distance on the mesh with the given name.
        Parameters
        ----------
        name : str
          The name of the target object.
        Returns
        -------
        point : (3,) float
          The closest point.
        '''
        idx = self._name_to_idx.get(name)
        if idx is None:
            raise ValueError(f""Unknown object name: {name!r}. Expected one of {self._names}"")
        p1, p2 = self._get_points_pair()
        p = p1 if idx == 0 else p2
        return (float(p[0]), float(p[1]), float(p[2]))

    # Internal helpers
    def _get_points_pair(self):
        # Try result.nearest_points as ((x1,y1,z1), (x2,y2,z2))
        if hasattr(self._result, ""nearest_points"") and self._result.nearest_points is not None:
            npnts = self._result.nearest_points
            # Some bindings store as tuple/list of two 3D sequences
            if isinstance(npnts, (list, tuple)):
                if len(npnts) == 2 and all(isinstance(v, (list, tuple)) or hasattr(v, ""__getitem__"") for v in npnts):
                    p1 = self._to_xyz(npnts[0])
                    p2 = self._to_xyz(npnts[1])
                    return p1, p2
                # Or flat length-6 sequence
                if len(npnts) == 6:
                    p1 = self._to_xyz(npnts[0:3])
                    p2 = self._to_xyz(npnts[3:6])
                    return p1, p2
        # Alternative attribute names occasionally used
        for a1, a2 in ((""nearest_points1"", ""nearest_points2""),
                       (""p1"", ""p2""),
                       (""point1"", ""point2"")):
            if hasattr(self._result, a1) and hasattr(self._result, a2):
                p1 = self._to_xyz(getattr(self._result, a1))
                p2 = self._to_xyz(getattr(self._result, a2))
                return p1, p2
        raise AttributeError(""DistanceResult does not provide nearest points"")

    @staticmethod
    def _to_xyz(seq):
        # Convert a 3D sequence (or numpy array) to a 3-tuple of floats
        if hasattr(seq, ""tolist""):
            seq = seq.tolist()
        if not hasattr(seq, ""__getitem__"") or len(seq) < 3:
            raise ValueError(""Expected a 3D coordinate sequence"")
        return (float(seq[0]), float(seq[1]), float(seq[2]))"
106111,mikedh/trimesh,trimesh/path/packing.py,trimesh.path.packing.RectangleBin,"import numpy as np

class RectangleBin:
    """"""
    An N-dimensional binary space partition tree for packing
    hyper-rectangles. Split logic is pure `numpy` but behaves
    similarly to `scipy.spatial.Rectangle`.

    Mostly useful for packing 2D textures and 3D boxes and
    has not been tested outside of 2 and 3 dimensions.

    Original article about using this for packing textures:
    http://www.blackpawn.com/texts/lightmaps/
    """"""

    def __init__(self, bounds):
        """"""
        Create a rectangular bin.

        Parameters
        ------------
        bounds : (2, dimension *) float
          Bounds array are `[mins, maxes]`
        """"""
        self.child = []
        self.occupied = False
        self.bounds = np.array(bounds, dtype=np.float64)

    @property
    def extents(self):
        """"""
        Bounding box size.

        Returns
        ----------
        extents : (dimension,) float
          Edge lengths of bounding box
        """"""
        bounds = self.bounds
        return bounds[1] - bounds[0]

    def insert(self, size, rotate=True):
        """"""
        Insert a rectangle into the bin.

        Parameters
        -------------
        size : (dimension,) float
          Size of rectangle to insert/

        Returns
        ----------
        inserted : (2,) float or None
          Position of insertion in the tree or None
          if the insertion was unsuccessful.
        """"""
        for child in self.child:
            attempt = child.insert(size=size, rotate=rotate)
            if attempt is not None:
                return attempt
        if self.occupied:
            return None
        bounds = self.bounds.copy()
        extents = bounds[1] - bounds[0]
        if rotate:
            for roll in range(len(size)):
                size_test = extents - _roll(size, roll)
                fits = (size_test > -_TOL_ZERO).all()
                if fits:
                    size = _roll(size, roll)
                    break
            if not fits:
                return None
        else:
            size_test = extents - size
            if (size_test < -_TOL_ZERO).any():
                return None
        self.occupied = True
        if (size_test < _TOL_ZERO).all():
            return bounds
        axis = size_test.argmax()
        splits = np.vstack((bounds, bounds))
        splits[1:3, axis] = bounds[0][axis] + size[axis]
        self.child[:] = (RectangleBin(splits[:2]), RectangleBin(splits[2:]))
        return self.child[0].insert(size, rotate=rotate)","
class RectangleBin:
    '''
    An N-dimensional binary space partition tree for packing
    hyper-rectangles. Split logic is pure `numpy` but behaves
    similarly to `scipy.spatial.Rectangle`.
    Mostly useful for packing 2D textures and 3D boxes and
    has not been tested outside of 2 and 3 dimensions.
    Original article about using this for packing textures:
    http://www.blackpawn.com/texts/lightmaps/
    '''
    def __init__(self, bounds):
        '''
        Create a rectangular bin.
        Parameters
        ------------
        bounds : (2, dimension *) float
          Bounds array are `[mins, maxes]`
        '''
        pass
    @property
    def extents(self):
        
        pass
    def insert(self, size, rotate=True):
        
        pass",snippet_29,4,"import numpy as np
import itertools


class RectangleBin:
    '''
    An N-dimensional binary space partition tree for packing
    hyper-rectangles. Split logic is pure `numpy` but behaves
    similarly to `scipy.spatial.Rectangle`.
    Mostly useful for packing 2D textures and 3D boxes and
    has not been tested outside of 2 and 3 dimensions.
    Original article about using this for packing textures:
    http://www.blackpawn.com/texts/lightmaps/
    '''
    def __init__(self, bounds):
        '''
        Create a rectangular bin.
        Parameters
        ------------
        bounds : (2, dimension *) float
          Bounds array are `[mins, maxes]`
        '''
        b = np.asarray(bounds, dtype=float)
        if b.ndim != 2 or b.shape[0] != 2:
            raise ValueError(""bounds must have shape (2, dim)"")
        if not np.all(b[1] >= b[0]):
            raise ValueError(""bounds must satisfy maxes >= mins"")
        self.bounds = b.copy()
        self._children = None
        self._used = False

    @property
    def extents(self):
        return self.bounds[1] - self.bounds[0]

    def insert(self, size, rotate=True):
        size = np.asarray(size, dtype=float)
        if size.ndim != 1 or size.shape[0] != self.extents.shape[0]:
            raise ValueError(""size must be a 1D array with same dimension as bounds"")
        if np.any(size < 0):
            return None

        eps = 1e-12

        def _fits(sz):
            return np.all(sz <= self.extents + eps)

        def _best_permutation():
            if not rotate:
                return np.arange(size.size), size
            best = None
            for perm in itertools.permutations(range(size.size)):
                perm = np.asarray(perm, dtype=int)
                szp = size[perm]
                if _fits(szp):
                    slack = self.extents - szp
                    # prefer tighter fits: minimize L-infinity, then L1, then lexicographic
                    key = (np.max(slack), np.sum(slack), tuple(slack))
                    if best is None or key < best[0]:
                        best = (key, perm, szp)
            return (best[1], best[2]) if best is not None else (None, None)

        # If this node is split, try children
        if self._children is not None:
            res = self._children[0].insert(size, rotate=rotate)
            if res is not None:
                return res
            return self._children[1].insert(size, rotate=rotate)

        # Leaf node
        if self._used:
            return None

        perm, sz = _best_permutation()
        if perm is None:
            return None

        slack = self.extents - sz
        if np.all(slack <= eps):
            self._used = True
            placed_mins = self.bounds[0].copy()
            placed_maxs = placed_mins + sz
            return np.stack([placed_mins, placed_maxs], axis=0)

        # Choose axis with largest slack to split
        axis = int(np.argmax(slack))
        split = self.bounds[0, axis] + sz[axis]

        # Create children
        b0 = self.bounds.copy()
        b1 = self.bounds.copy()
        b0[1, axis] = split
        b1[0, axis] = split

        left = RectangleBin(b0)
        right = RectangleBin(b1)
        self._children = (left, right)

        # Insert into left child; it will further split as needed
        return left.insert(sz, rotate=False)  # size already oriented"
107609,weld-project/weld,python/grizzly/grizzly/lazy_op.py,grizzly.lazy_op.LazyOpResult,"class LazyOpResult:
    """"""Wrapper class around as yet un-evaluated Weld computation results

    Attributes:
        dim (int): Dimensionality of the output
        expr (WeldObject / Numpy.ndarray): The expression that needs to be
            evaluated
        weld_type (WeldType): Type of the output object
    """"""

    def __init__(self, expr, weld_type, dim):
        """"""Summary

        Args:
            expr (TYPE): Description
            weld_type (TYPE): Description
            dim (TYPE): Description
        """"""
        self.expr = expr
        self.weld_type = weld_type
        self.dim = dim

    def evaluate(self, verbose=True, decode=True, passes=None, num_threads=1, apply_experimental_transforms=False):
        """"""Summary

        Args:
            verbose (bool, optional): Description
            decode (bool, optional): Description

        Returns:
            TYPE: Description
        """"""
        if isinstance(self.expr, WeldObject):
            return self.expr.evaluate(to_weld_type(self.weld_type, self.dim), verbose, decode, passes=passes, num_threads=num_threads, apply_experimental_transforms=apply_experimental_transforms)
        return self.expr","class LazyOpResult:
    '''Wrapper class around as yet un-evaluated Weld computation results
    Attributes:
        dim (int): Dimensionality of the output
        expr (WeldObject / Numpy.ndarray): The expression that needs to be
            evaluated
        weld_type (WeldType): Type of the output object
    '''
    def __init__(self, expr, weld_type, dim):
        
        pass
    def evaluate(self, verbose=True, decode=True, passes=None, num_threads=1, apply_experimental_transforms=False):
        '''Summary
        Args:
            verbose (bool, optional): Description
            decode (bool, optional): Description
        Returns:
            TYPE: Description
        '''
        pass",snippet_30,3,"class LazyOpResult:
    '''Wrapper class around as yet un-evaluated Weld computation results
    Attributes:
        dim (int): Dimensionality of the output
        expr (WeldObject / Numpy.ndarray): The expression that needs to be
            evaluated
        weld_type (WeldType): Type of the output object
    '''
    def __init__(self, expr, weld_type, dim):
        self.expr = expr
        self.weld_type = weld_type
        self.dim = dim

    def evaluate(self, verbose=True, decode=True, passes=None, num_threads=1, apply_experimental_transforms=False):
        '''Summary
        Args:
            verbose (bool, optional): Description
            decode (bool, optional): Description
        Returns:
            TYPE: Description
        '''
        obj = self.expr
        if hasattr(obj, ""evaluate"") and callable(getattr(obj, ""evaluate"")):
            try:
                import inspect
                sig = inspect.signature(obj.evaluate)
                kwargs = {
                    ""verbose"": verbose,
                    ""decode"": decode,
                    ""passes"": passes,
                    ""num_threads"": num_threads,
                    ""apply_experimental_transforms"": apply_experimental_transforms,
                }
                filtered_kwargs = {k: v for k, v in kwargs.items() if k in sig.parameters}
                return obj.evaluate(**filtered_kwargs)
            except Exception:
                # Best-effort fallback with common kwargs
                try:
                    return obj.evaluate(verbose=verbose, decode=decode)
                except Exception:
                    return obj.evaluate()
        return obj"
113819,abseil/abseil-py,abseil_abseil-py/absl/app.py,absl.app.ExceptionHandler,"class ExceptionHandler:
    """"""Base exception handler from which other may inherit.""""""

    def wants(self, exc):
        """"""Returns whether this handler wants to handle the exception or not.

        This base class returns True for all exceptions by default. Override in
        subclass if it wants to be more selective.

        Args:
          exc: Exception, the current exception.
        """"""
        del exc
        return True

    def handle(self, exc):
        """"""Do something with the current exception.

        Args:
          exc: Exception, the current exception

        This method must be overridden.
        """"""
        raise NotImplementedError()","class ExceptionHandler:
    '''Base exception handler from which other may inherit.'''
    def wants(self, exc):
        '''Returns whether this handler wants to handle the exception or not.
        This base class returns True for all exceptions by default. Override in
        subclass if it wants to be more selective.
        Args:
          exc: Exception, the current exception.
        '''
        pass
    def handle(self, exc):
        '''Do something with the current exception.
        Args:
          exc: Exception, the current exception
        This method must be overridden.
        '''
        pass",snippet_33,3,"class ExceptionHandler:
    '''Base exception handler from which other may inherit.'''

    def wants(self, exc):
        '''Returns whether this handler wants to handle the exception or not.
        This base class returns True for all exceptions by default. Override in
        subclass if it wants to be more selective.
        Args:
          exc: Exception, the current exception.
        '''
        return True

    def handle(self, exc):
        '''Do something with the current exception.
        Args:
          exc: Exception, the current exception
        This method must be overridden.
        '''
        raise NotImplementedError(""Subclasses must implement the handle method."")"
116413,QuantEcon/QuantEcon.py,quantecon/util/timing.py,quantecon.util.timing.__Timer__,"import time
import numpy as np

class __Timer__:
    """"""Computes elapsed time, between tic, tac, and toc.

    Methods
    -------
    tic :
        Resets timer.
    toc :
        Returns and prints time elapsed since last tic().
    tac :
        Returns and prints time elapsed since last
             tic(), tac() or toc() whichever occured last.
    loop_timer :
        Returns and prints the total and average time elapsed for n runs
        of a given function.

    """"""
    start = None
    last = None

    def tic(self):
        """"""
        Save time for future use with `tac()` or `toc()`.

        Returns
        -------
        None
            This function doesn't return a value.
        """"""
        t = time.time()
        self.start = t
        self.last = t

    def tac(self, verbose=True, digits=2):
        """"""
        Return and print elapsed time since last `tic()`, `tac()`, or
        `toc()`.

        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.

        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.

        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`, `tac()`, or `toc()`.

        """"""
        if self.start is None:
            raise Exception('tac() without tic()')
        t = time.time()
        elapsed = t - self.last
        self.last = t
        if verbose:
            m, s = divmod(elapsed, 60)
            h, m = divmod(m, 60)
            print('TAC: Elapsed: %d:%02d:%0d.%0*d' % (h, m, s, digits, s % 1 * 10 ** digits))
        return elapsed

    def toc(self, verbose=True, digits=2):
        """"""
        Return and print time elapsed since last `tic()`.

        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.

        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.

        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`.

        """"""
        if self.start is None:
            raise Exception('toc() without tic()')
        t = time.time()
        self.last = t
        elapsed = t - self.start
        if verbose:
            m, s = divmod(elapsed, 60)
            h, m = divmod(m, 60)
            print('TOC: Elapsed: %d:%02d:%0d.%0*d' % (h, m, s, digits, s % 1 * 10 ** digits))
        return elapsed

    def loop_timer(self, n, function, args=None, verbose=True, digits=2, best_of=3):
        """"""
        Return and print the total and average time elapsed for n runs
        of function.

        Parameters
        ----------
        n : scalar(int)
            Number of runs.

        function : function
            Function to be timed.

        args : list, optional(default=None)
            Arguments of the function.

        verbose : bool, optional(default=True)
            If True, then prints average time.

        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.

        best_of : scalar(int), optional(default=3)
            Average time over best_of runs.

        Returns
        -------
        average_time : scalar(float)
            Average time elapsed for n runs of function.

        average_of_best : scalar(float)
            Average of best_of times for n runs of function.

        """"""
        tic()
        all_times = np.empty(n)
        for run in range(n):
            if hasattr(args, '__iter__'):
                function(*args)
            elif args is None:
                function()
            else:
                function(args)
            all_times[run] = tac(verbose=False, digits=digits)
        elapsed = toc(verbose=False, digits=digits)
        m, s = divmod(elapsed, 60)
        h, m = divmod(m, 60)
        print('Total run time: %d:%02d:%0d.%0*d' % (h, m, s, digits, s % 1 * 10 ** digits))
        average_time = all_times.mean()
        average_of_best = np.sort(all_times)[:best_of].mean()
        if verbose:
            m, s = divmod(average_time, 60)
            h, m = divmod(m, 60)
            print('Average time for %d runs: %d:%02d:%0d.%0*d' % (n, h, m, s, digits, s % 1 * 10 ** digits))
            m, s = divmod(average_of_best, 60)
            h, m = divmod(m, 60)
            print('Average of %d best times: %d:%02d:%0d.%0*d' % (best_of, h, m, s, digits, s % 1 * 10 ** digits))
        return (average_time, average_of_best)","
class __Timer__:
    
    def tic(self):
        
        pass
    def tac(self, verbose=True, digits=2):
        '''
        Return and print elapsed time since last `tic()`, `tac()`, or
        `toc()`.
        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.
        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.
        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`, `tac()`, or `toc()`.
        '''
        pass
    def toc(self, verbose=True, digits=2):
        '''
        Return and print time elapsed since last `tic()`.
        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.
        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.
        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`.
        '''
        pass
    def loop_timer(self, n, function, args=None, verbose=True, digits=2, best_of=3):
        '''
        Return and print the total and average time elapsed for n runs
        of function.
        Parameters
        ----------
        n : scalar(int)
            Number of runs.
        function : function
            Function to be timed.
        args : list, optional(default=None)
            Arguments of the function.
        verbose : bool, optional(default=True)
            If True, then prints average time.
        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.
        best_of : scalar(int), optional(default=3)
            Average time over best_of runs.
        Returns
        -------
        average_time : scalar(float)
            Average time elapsed for n runs of function.
        average_of_best : scalar(float)
            Average of best_of times for n runs of function.
        '''
        pass",snippet_35,5,"import time
from typing import Callable, Iterable, Optional, Tuple


class __Timer__:
    def __init__(self):
        self._tic_time: Optional[float] = None
        self._last_time: Optional[float] = None

    def tic(self):
        now = time.perf_counter()
        self._tic_time = now
        self._last_time = now

    def tac(self, verbose=True, digits=2):
        '''
        Return and print elapsed time since last `tic()`, `tac()`, or
        `toc()`.
        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.
        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.
        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`, `tac()`, or `toc()`.
        '''
        now = time.perf_counter()
        if self._last_time is None:
            # Initialize if not started
            self._tic_time = now
            self._last_time = now
            elapsed = 0.0
        else:
            elapsed = now - self._last_time
            self._last_time = now
        if verbose:
            print(f""{elapsed:.{digits}f}s"")
        return elapsed

    def toc(self, verbose=True, digits=2):
        '''
        Return and print time elapsed since last `tic()`.
        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.
        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.
        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`.
        '''
        now = time.perf_counter()
        if self._tic_time is None:
            # Initialize if not started
            self._tic_time = now
            self._last_time = now
            elapsed = 0.0
        else:
            elapsed = now - self._tic_time
            # Update last_time to align with tac() doc stating it measures since last tic/tac/toc
            self._last_time = now
        if verbose:
            print(f""{elapsed:.{digits}f}s"")
        return elapsed

    def loop_timer(self, n, function: Callable, args: Optional[Iterable] = None, verbose=True, digits=2, best_of=3):
        '''
        Return and print the total and average time elapsed for n runs
        of function.
        Parameters
        ----------
        n : scalar(int)
            Number of runs.
        function : function
            Function to be timed.
        args : list, optional(default=None)
            Arguments of the function.
        verbose : bool, optional(default=True)
            If True, then prints average time.
        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.
        best_of : scalar(int), optional(default=3)
            Average time over best_of runs.
        Returns
        -------
        average_time : scalar(float)
            Average time elapsed for n runs of function.
        average_of_best : scalar(float)
            Average of best_of times for n runs of function.
        '''
        if n <= 0:
            if verbose:
                print(f""0.00s total, 0.00s avg, 0.00s best-of"")
            return 0.0, 0.0

        if args is None:
            call_args: Tuple = ()
        elif isinstance(args, (list, tuple)):
            call_args = tuple(args)
        else:
            call_args = (args,)

        times = []
        for _ in range(n):
            t0 = time.perf_counter()
            function(*call_args)
            t1 = time.perf_counter()
            times.append(t1 - t0)

        total_time = sum(times)
        average_time = total_time / n

        k = min(best_of, n) if best_of is not None else 1
        best_k_avg = sum(sorted(times)[:k]) / k

        if verbose:
            print(f""{total_time:.{digits}f}s total, {average_time:.{digits}f}s avg, {best_k_avg:.{digits}f}s best-of-{k}"")

        return average_time, best_k_avg"
116421,DerwenAI/pytextrank,DerwenAI_pytextrank/pytextrank/base.py,pytextrank.base.Sentence,"import typing
from spacy.tokens import Doc, Span, Token
from dataclasses import dataclass

@dataclass
class Sentence:
    """"""
A data class representing the distance measure for one sentence.
    """"""
    start: int
    end: int
    sent_id: int
    phrases: typing.Set[int]
    distance: float

    def empty(self) -> bool:
        """"""
Test whether this sentence includes any ranked phrases.

    returns:
`True` if the `phrases` is not empty.
        """"""
        return len(self.phrases) == 0

    def text(self, doc: Doc) -> str:
        """"""
Accessor for the text slice of the `spaCy` [`Doc`](https://spacy.io/api/doc)
document represented by this sentence.

    doc:
source document

    returns:
the sentence text
        """"""
        return doc[self.start:self.end]","@dataclass
class Sentence:
    
    def empty(self) -> bool:
        '''
Test whether this sentence includes any ranked phrases.
    returns:
`True` if the `phrases` is not empty.
        '''
        pass
    def text(self, doc: Doc) -> str:
        
        pass",snippet_36,3,"from dataclasses import dataclass, field
from typing import Any, List
from spacy.tokens import Doc

@dataclass
class Sentence:
    start: int = 0
    end: int = 0
    phrases: List[Any] = field(default_factory=list)
    
    def empty(self) -> bool:
        '''
Test whether this sentence includes any ranked phrases.
    returns:
`True` if the `phrases` is not empty.
        '''
        return bool(self.phrases)

    def text(self, doc: Doc) -> str:
        s = max(0, min(self.start, len(doc)))
        e = max(s, min(self.end, len(doc)))
        return doc[s:e].text"
128871,arviz-devs/arviz,arviz-devs_arviz/arviz/utils.py,arviz.utils.interactive_backend,"import matplotlib.pyplot as plt

class interactive_backend:
    """"""Context manager to change backend temporarily in ipython sesson.

    It uses ipython magic to change temporarily from the ipython inline backend to
    an interactive backend of choice. It cannot be used outside ipython sessions nor
    to change backends different than inline -> interactive.

    Notes
    -----
    The first time ``interactive_backend`` context manager is called, any of the available
    interactive backends can be chosen. The following times, this same backend must be used
    unless the kernel is restarted.

    Parameters
    ----------
    backend : str, optional
        Interactive backend to use. It will be passed to ``%matplotlib`` magic, refer to
        its docs to see available options.

    Examples
    --------
    Inside an ipython session (i.e. a jupyter notebook) with the inline backend set:

    .. code::

        >>> import arviz as az
        >>> idata = az.load_arviz_data(""centered_eight"")
        >>> az.plot_posterior(idata) # inline
        >>> with az.interactive_backend():
        ...     az.plot_density(idata) # interactive
        >>> az.plot_trace(idata) # inline

    """"""

    def __init__(self, backend=''):
        """"""Initialize context manager.""""""
        try:
            from IPython import get_ipython
        except ImportError as err:
            raise ImportError(f'The exception below was risen while importing Ipython, this context manager can only be used inside ipython sessions:\n{err}') from err
        self.ipython = get_ipython()
        if self.ipython is None:
            raise EnvironmentError('This context manager can only be used inside ipython sessions')
        self.ipython.magic(f'matplotlib {backend}')

    def __enter__(self):
        """"""Enter context manager.""""""
        return self

    def __exit__(self, exc_type, exc_value, exc_tb):
        """"""Exit context manager.""""""
        plt.show(block=True)
        self.ipython.magic('matplotlib inline')","
class interactive_backend:
    '''Context manager to change backend temporarily in ipython sesson.
    It uses ipython magic to change temporarily from the ipython inline backend to
    an interactive backend of choice. It cannot be used outside ipython sessions nor
    to change backends different than inline -> interactive.
    Notes
    -----
    The first time ``interactive_backend`` context manager is called, any of the available
    interactive backends can be chosen. The following times, this same backend must be used
    unless the kernel is restarted.
    Parameters
    ----------
    backend : str, optional
        Interactive backend to use. It will be passed to ``%matplotlib`` magic, refer to
        its docs to see available options.
    Examples
    --------
    Inside an ipython session (i.e. a jupyter notebook) with the inline backend set:
    .. code::
        >>> import arviz as az
        >>> idata = az.load_arviz_data(""centered_eight"")
        >>> az.plot_posterior(idata) # inline
        >>> with az.interactive_backend():
        ...     az.plot_density(idata) # interactive
        >>> az.plot_trace(idata) # inline
    '''
    def __init__(self, backend=''):
        
        pass
    def __enter__(self):
        '''Enter context manager.'''
        pass
    def __exit__(self, exc_type, exc_value, exc_tb):
        '''Exit context manager.'''
        pass",snippet_40,4,"class interactive_backend:
    '''Context manager to change backend temporarily in ipython sesson.
    It uses ipython magic to change temporarily from the ipython inline backend to
    an interactive backend of choice. It cannot be used outside ipython sessions nor
    to change backends different than inline -> interactive.
    Notes
    -----
    The first time ``interactive_backend`` context manager is called, any of the available
    interactive backends can be chosen. The following times, this same backend must be used
    unless the kernel is restarted.
    Parameters
    ----------
    backend : str, optional
        Interactive backend to use. It will be passed to ``%matplotlib`` magic, refer to
        its docs to see available options.
    Examples
    --------
    Inside an ipython session (i.e. a jupyter notebook) with the inline backend set:
    .. code::
        >>> import arviz as az
        >>> idata = az.load_arviz_data(""centered_eight"")
        >>> az.plot_posterior(idata) # inline
        >>> with az.interactive_backend():
        ...     az.plot_density(idata) # interactive
        >>> az.plot_trace(idata) # inline
    '''
    _chosen_backend = None

    def __init__(self, backend=''):
        self._requested_backend = backend.strip()
        self._enabled = False

    def __enter__(self):
        '''Enter context manager.'''
        try:
            from IPython import get_ipython
        except Exception as err:
            raise RuntimeError(""interactive_backend can only be used inside an IPython session"") from err

        ip = get_ipython()
        if ip is None:
            raise RuntimeError(""interactive_backend can only be used inside an IPython session"")

        try:
            import matplotlib
        except Exception as err:
            raise RuntimeError(""matplotlib is required to use interactive_backend"") from err

        current_backend = str(matplotlib.get_backend()).lower()
        if ""inline"" not in current_backend:
            raise RuntimeError(
                ""interactive_backend can only switch from the inline backend to an interactive backend""
            )

        # Determine which interactive backend to use for this and subsequent uses
        if interactive_backend._chosen_backend is None:
            chosen = self._requested_backend if self._requested_backend else ""auto""
            interactive_backend._chosen_backend = chosen
        else:
            chosen = interactive_backend._chosen_backend
            if self._requested_backend and self._requested_backend != chosen:
                raise RuntimeError(
                    f""interactive_backend already initialized with backend '{chosen}'. ""
                    f""Requested '{self._requested_backend}' does not match.""
                )

        # Switch to interactive backend using IPython magic
        try:
            ip.run_line_magic(""matplotlib"", interactive_backend._chosen_backend)
        except Exception as err:
            # If switching fails, reset chosen backend so future attempts can try again
            if interactive_backend._chosen_backend == self._requested_backend or (
                not self._requested_backend and interactive_backend._chosen_backend == ""auto""
            ):
                interactive_backend._chosen_backend = None
            raise RuntimeError(
                f""Failed to switch to interactive backend '{interactive_backend._chosen_backend}' via %matplotlib""
            ) from err

        self._enabled = True
        return self

    def __exit__(self, exc_type, exc_value, exc_tb):
        '''Exit context manager.'''
        if not self._enabled:
            return False
        try:
            from IPython import get_ipython
            ip = get_ipython()
            if ip is None:
                return False
            ip.run_line_magic(""matplotlib"", ""inline"")
        except Exception:
            # Do not suppress exceptions
            return False
        finally:
            self._enabled = False
        return False"
128872,arviz-devs/arviz,arviz-devs_arviz/arviz/utils.py,arviz.utils.lazy_property,"import functools

class lazy_property:
    """"""Used to load numba first time it is needed.""""""

    def __init__(self, fget):
        """"""Lazy load a property with `fget`.""""""
        self.fget = fget
        functools.update_wrapper(self, fget)

    def __get__(self, obj, cls):
        """"""Call the function, set the attribute.""""""
        if obj is None:
            return self
        value = self.fget(obj)
        setattr(obj, self.fget.__name__, value)
        return value","
class lazy_property:
    '''Used to load numba first time it is needed.'''
    def __init__(self, fget):
        '''Lazy load a property with `fget`.'''
        pass
    def __get__(self, obj, cls):
        
        pass",snippet_41,3,"class lazy_property:
    '''Used to load numba first time it is needed.'''
    def __init__(self, fget):
        '''Lazy load a property with `fget`.'''
        self.fget = fget
        self.__doc__ = getattr(fget, '__doc__', None)
        self._name = fget.__name__

    def __get__(self, obj, cls):
        if obj is None:
            return self
        value = self.fget(obj)
        setattr(obj, self._name, value)
        return value"
129413,dtmilano/AndroidViewClient,dtmilano_AndroidViewClient/src/com/dtmilano/android/adb/adbclient.py,com.dtmilano.android.adb.adbclient.WifiManager,"import re
import sys

class WifiManager:
    """"""
    Simulates Android WifiManager.

    @see: http://developer.android.com/reference/android/net/wifi/WifiManager.html
    """"""
    WIFI_STATE_DISABLING = 0
    WIFI_STATE_DISABLED = 1
    WIFI_STATE_ENABLING = 2
    WIFI_STATE_ENABLED = 3
    WIFI_STATE_UNKNOWN = 4
    WIFI_IS_ENABLED_RE = re.compile('Wi-Fi is enabled')
    WIFI_IS_DISABLED_RE = re.compile('Wi-Fi is disabled')

    def __init__(self, device):
        """"""
        Constructor.
        :param device:
        :type device:
        """"""
        self.device = device

    def getWifiState(self):
        """"""
        Gets the Wi-Fi enabled state.

        @return: One of WIFI_STATE_DISABLED, WIFI_STATE_DISABLING, WIFI_STATE_ENABLED, WIFI_STATE_ENABLING, WIFI_STATE_UNKNOWN
        """"""
        result = self.device.shell('dumpsys wifi')
        if result:
            state = result.splitlines()[0]
            if self.WIFI_IS_ENABLED_RE.match(state):
                return self.WIFI_STATE_ENABLED
            elif self.WIFI_IS_DISABLED_RE.match(state):
                return self.WIFI_STATE_DISABLED
        print('UNKNOWN WIFI STATE:', state, file=sys.stderr)
        return self.WIFI_STATE_UNKNOWN","
class WifiManager:
    '''
    Simulates Android WifiManager.
    @see: http://developer.android.com/reference/android/net/wifi/WifiManager.html
    '''
    def __init__(self, device):
        
        pass
    def getWifiState(self):
        
        pass",snippet_42,3,"class WifiManager:
    '''
    Simulates Android WifiManager.
    @see: http://developer.android.com/reference/android/net/wifi/WifiManager.html
    '''
    # Wi-Fi state constants (mirroring Android WifiManager)
    WIFI_STATE_DISABLING = 0
    WIFI_STATE_DISABLED = 1
    WIFI_STATE_ENABLING = 2
    WIFI_STATE_ENABLED = 3
    WIFI_STATE_UNKNOWN = 4

    def __init__(self, device):
        self._device = device
        self._state = getattr(device, ""wifi_state"", self.WIFI_STATE_UNKNOWN)

    def getWifiState(self):
        return self._state"
129620,neovim/pynvim,neovim_pynvim/pynvim/msgpack_rpc/async_session.py,pynvim.msgpack_rpc.async_session.Response,"from pynvim.msgpack_rpc.msgpack_stream import MsgpackStream

class Response:
    """"""Response to a msgpack-rpc request that came from Nvim.

    When Nvim sends a msgpack-rpc request, an instance of this class is
    created for remembering state required to send a response.
    """"""

    def __init__(self, msgpack_stream: MsgpackStream, request_id: int):
        """"""Initialize the Response instance.""""""
        self._msgpack_stream = msgpack_stream
        self._request_id = request_id

    def send(self, value, error=False):
        """"""Send the response.

        If `error` is True, it will be sent as an error.
        """"""
        if error:
            resp = [1, self._request_id, value, None]
        else:
            resp = [1, self._request_id, None, value]
        debug('sending response to request %d: %s', self._request_id, resp)
        self._msgpack_stream.send(resp)","
class Response:
    
    def __init__(self, msgpack_stream: MsgpackStream, request_id: int):
        '''Initialize the Response instance.'''
        pass
    def send(self, value, error=False):
        '''Send the response.
        If `error` is True, it will be sent as an error.
        '''
        pass",snippet_43,3,"class Response:
    
    def __init__(self, msgpack_stream, request_id: int):
        self._stream = msgpack_stream
        self._id = request_id
        self._sent = False

    def send(self, value, error=False):
        if self._sent:
            raise RuntimeError(""Response already sent"")
        self._sent = True

        if error:
            err = value
            res = None
        else:
            err = None
            res = value

        msg = [1, self._id, err, res]

        send_method = getattr(self._stream, ""send"", None) or getattr(self._stream, ""write"", None) or getattr(self._stream, ""send_msg"", None)
        if send_method is None:
            raise AttributeError(""MsgpackStream does not have a send/write method"")

        send_method(msg)
        return True"
129967,sendgrid/sendgrid-python,sendgrid_sendgrid-python/sendgrid/helpers/eventwebhook/__init__.py,sendgrid.helpers.eventwebhook.EventWebhook,"from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.asymmetric import ec
from cryptography.exceptions import InvalidSignature
from cryptography.hazmat.primitives.serialization import load_pem_public_key
import base64

class EventWebhook:
    """"""
    This class allows you to use the Event Webhook feature. Read the docs for
    more details: https://sendgrid.com/docs/for-developers/tracking-events/event
    """"""

    def __init__(self, public_key=None):
        """"""
        Construct the Event Webhook verifier object
        :param public_key: verification key under Mail Settings
        :type public_key: string
        """"""
        self.public_key = self.convert_public_key_to_ecdsa(public_key) if public_key else public_key

    def convert_public_key_to_ecdsa(self, public_key):
        """"""
        Convert the public key string to an EllipticCurvePublicKey object.

        :param public_key: verification key under Mail Settings
        :type public_key string
        :return: An EllipticCurvePublicKey object using the ECDSA algorithm
        :rtype cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey
        """"""
        pem_key = '-----BEGIN PUBLIC KEY-----\n' + public_key + '\n-----END PUBLIC KEY-----'
        return load_pem_public_key(pem_key.encode('utf-8'))

    def verify_signature(self, payload, signature, timestamp, public_key=None):
        """"""
        Verify signed event webhook requests.

        :param payload: event payload in the request body
        :type payload: string
        :param signature: value obtained from the 'X-Twilio-Email-Event-Webhook-Signature' header
        :type signature: string
        :param timestamp: value obtained from the 'X-Twilio-Email-Event-Webhook-Timestamp' header
        :type timestamp: string
        :param public_key: elliptic curve public key
        :type public_key: cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey
        :return: true or false if signature is valid
        """"""
        timestamped_payload = (timestamp + payload).encode('utf-8')
        decoded_signature = base64.b64decode(signature)
        key = public_key or self.public_key
        try:
            key.verify(decoded_signature, timestamped_payload, ec.ECDSA(hashes.SHA256()))
            return True
        except InvalidSignature:
            return False","
class EventWebhook:
    
    def __init__(self, public_key=None):
        
        pass
    def convert_public_key_to_ecdsa(self, public_key):
        
        pass
    def verify_signature(self, payload, signature, timestamp, public_key=None):
        '''
        Verify signed event webhook requests.
        :param payload: event payload in the request body
        :type payload: string
        :param signature: value obtained from the 'X-Twilio-Email-Event-Webhook-Signature' header
        :type signature: string
        :param timestamp: value obtained from the 'X-Twilio-Email-Event-Webhook-Timestamp' header
        :type timestamp: string
        :param public_key: elliptic curve public key
        :type public_key: cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey
        :return: true or false if signature is valid
        '''
        pass",snippet_44,4,"import base64
from typing import Optional, Union

from cryptography.exceptions import InvalidSignature
from cryptography.hazmat.primitives import hashes, serialization
from cryptography.hazmat.primitives.asymmetric import ec
from cryptography.hazmat.primitives.serialization import load_der_public_key, load_pem_public_key


class EventWebhook:
    
    def __init__(self, public_key=None):
        if public_key is None:
            self.public_key = None
        else:
            self.public_key = self.convert_public_key_to_ecdsa(public_key)

    def convert_public_key_to_ecdsa(self, public_key):
        if public_key is None:
            return None

        if isinstance(public_key, ec.EllipticCurvePublicKey):
            return public_key

        key_bytes: Optional[bytes] = None

        if isinstance(public_key, str):
            pem_marker = ""-----BEGIN PUBLIC KEY-----"" in public_key
            if pem_marker:
                key_bytes = public_key.encode(""utf-8"")
                try:
                    loaded = load_pem_public_key(key_bytes)
                except Exception as e:
                    raise ValueError(""Invalid PEM public key"") from e
                if not isinstance(loaded, ec.EllipticCurvePublicKey):
                    raise ValueError(""Public key is not an elliptic curve public key"")
                return loaded
            else:
                # Try base64 DER
                try:
                    der = base64.b64decode(public_key)
                    loaded = load_der_public_key(der)
                    if not isinstance(loaded, ec.EllipticCurvePublicKey):
                        raise ValueError(""Public key is not an elliptic curve public key"")
                    return loaded
                except Exception:
                    # Try treating as raw bytes string (hex not supported)
                    try:
                        loaded = load_der_public_key(public_key.encode(""utf-8""))
                        if not isinstance(loaded, ec.EllipticCurvePublicKey):
                            raise ValueError(""Public key is not an elliptic curve public key"")
                        return loaded
                    except Exception as e:
                        raise ValueError(""Unable to load public key from provided string"") from e

        if isinstance(public_key, (bytes, bytearray)):
            # Try PEM first
            try:
                loaded = load_pem_public_key(public_key)
                if isinstance(loaded, ec.EllipticCurvePublicKey):
                    return loaded
            except Exception:
                pass
            # Then DER
            try:
                loaded = load_der_public_key(public_key)
                if isinstance(loaded, ec.EllipticCurvePublicKey):
                    return loaded
            except Exception as e:
                raise ValueError(""Unable to load public key from bytes"") from e

        raise ValueError(""Unsupported public_key type"")

    def verify_signature(self, payload, signature, timestamp, public_key=None):
        '''
        Verify signed event webhook requests.
        :param payload: event payload in the request body
        :type payload: string
        :param signature: value obtained from the 'X-Twilio-Email-Event-Webhook-Signature' header
        :type signature: string
        :param timestamp: value obtained from the 'X-Twilio-Email-Event-Webhook-Timestamp' header
        :type timestamp: string
        :param public_key: elliptic curve public key
        :type public_key: cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey
        :return: true or false if signature is valid
        '''
        key_obj: Optional[ec.EllipticCurvePublicKey]

        if public_key is not None:
            key_obj = self.convert_public_key_to_ecdsa(public_key)
        else:
            if self.public_key is None:
                raise ValueError(""Public key is required to verify signature"")
            key_obj = self.public_key

        if isinstance(payload, str):
            payload_bytes = payload.encode(""utf-8"")
        elif isinstance(payload, (bytes, bytearray)):
            payload_bytes = bytes(payload)
        else:
            raise ValueError(""payload must be str or bytes"")

        if isinstance(timestamp, (bytes, bytearray)):
            timestamp_str = bytes(timestamp).decode(""utf-8"")
        else:
            timestamp_str = str(timestamp)

        message = (timestamp_str.encode(""utf-8"") + payload_bytes)

        if isinstance(signature, str):
            try:
                signature_bytes = base64.b64decode(signature, validate=True)
            except Exception:
                # Some environments may send URL-safe base64
                try:
                    signature_bytes = base64.urlsafe_b64decode(signature)
                except Exception as e:
                    raise ValueError(""Invalid signature encoding"") from e
        elif isinstance(signature, (bytes, bytearray)):
            signature_bytes = bytes(signature)
        else:
            raise ValueError(""signature must be str or bytes"")

        try:
            key_obj.verify(signature_bytes, message, ec.ECDSA(hashes.SHA256()))
            return True
        except InvalidSignature:
            return False
        except Exception:
            return False"
131054,SoCo/SoCo,SoCo_SoCo/soco/music_services/token_store.py,soco.music_services.token_store.TokenStoreBase,"class TokenStoreBase:
    """"""Token store base class""""""

    def __init__(self, token_collection='default'):
        """"""Instantiate instance variables

        Args:
            token_collection (str): The name of the token collection to use. This may be
                used to store different token collections for different client programs.
        """"""
        self.token_collection = token_collection

    def save_token_pair(self, music_service_id, household_id, token_pair):
        """"""Save a token value pair (token, key) which is a 2 item sequence""""""
        raise NotImplementedError

    def load_token_pair(self, music_service_id, household_id):
        """"""Load a token pair (token, key) which is a 2 item sequence""""""
        raise NotImplementedError

    def has_token(self, music_service_id, household_id):
        """"""Return True if a token is stored for the music service and household ID""""""
        raise NotImplementedError","class TokenStoreBase:
    '''Token store base class'''
    def __init__(self, token_collection='default'):
        '''Instantiate instance variables
        Args:
            token_collection (str): The name of the token collection to use. This may be
                used to store different token collections for different client programs.
        '''
        pass
    def save_token_pair(self, music_service_id, household_id, token_pair):
        
        pass
    def load_token_pair(self, music_service_id, household_id):
        
        pass
    def has_token(self, music_service_id, household_id):
        '''Return True if a token is stored for the music service and household ID'''
        pass",snippet_45,5,"class TokenStoreBase:
    '''Token store base class'''
    def __init__(self, token_collection='default'):
        '''Instantiate instance variables
        Args:
            token_collection (str): The name of the token collection to use. This may be
                used to store different token collections for different client programs.
        '''
        self.token_collection = token_collection
        self._tokens = {}  # key: (music_service_id, household_id) -> value: token_pair

    def save_token_pair(self, music_service_id, household_id, token_pair):
        self._tokens[(music_service_id, household_id)] = token_pair

    def load_token_pair(self, music_service_id, household_id):
        return self._tokens.get((music_service_id, household_id))

    def has_token(self, music_service_id, household_id):
        '''Return True if a token is stored for the music service and household ID'''
        return (music_service_id, household_id) in self._tokens"
131060,SoCo/SoCo,SoCo_SoCo/soco/plugins/sharelink.py,soco.plugins.sharelink.ShareClass,"class ShareClass:
    """"""Base class for supported services.""""""

    def canonical_uri(self, uri):
        """"""Recognize a share link and return its canonical representation.

        Args:
            uri (str): A URI like ""https://tidal.com/browse/album/157273956"".

        Returns:
            str: The canonical URI or None if not recognized.
        """"""
        raise NotImplementedError

    def service_number(self):
        """"""Return the service number.

        Returns:
            int: A number identifying the supported music service.
        """"""
        raise NotImplementedError

    @staticmethod
    def magic():
        """"""Return magic.

        Returns:
            dict: Magic prefix/key/class values for each share type.
        """"""
        return {'album': {'prefix': 'x-rincon-cpcontainer:1004206c', 'key': '00040000', 'class': 'object.container.album.musicAlbum'}, 'episode': {'prefix': '', 'key': '00032020', 'class': 'object.item.audioItem.musicTrack'}, 'track': {'prefix': '', 'key': '00032020', 'class': 'object.item.audioItem.musicTrack'}, 'show': {'prefix': 'x-rincon-cpcontainer:1006206c', 'key': '1006206c', 'class': 'object.container.playlistContainer'}, 'song': {'prefix': '', 'key': '10032020', 'class': 'object.item.audioItem.musicTrack'}, 'playlist': {'prefix': 'x-rincon-cpcontainer:1006206c', 'key': '1006206c', 'class': 'object.container.playlistContainer'}}

    def extract(self, uri):
        """"""Extract the share type and encoded URI from a share link.

        Returns:
            share_type: The shared type, like ""album"" or ""track"".
            encoded_uri: An escaped URI with a service-specific format.
        """"""
        raise NotImplementedError","class ShareClass:
    
    def canonical_uri(self, uri):
        
        pass
    def service_number(self):
        '''Return the service number.
        Returns:
            int: A number identifying the supported music service.
        '''
        pass
    @staticmethod
    def magic():
        
        pass
    def extract(self, uri):
        '''Extract the share type and encoded URI from a share link.
        Returns:
            share_type: The shared type, like ""album"" or ""track"".
            encoded_uri: An escaped URI with a service-specific format.
        '''
        pass",snippet_46,5,"class ShareClass:
    
    def canonical_uri(self, uri):
        share_type, encoded_uri = self.extract(uri)
        if not share_type or not encoded_uri:
            raise ValueError(""Unable to derive canonical URI from the given input"")
        return f""share:{share_type}:{encoded_uri}""

    def service_number(self):
        '''Return the service number.
        Returns:
            int: A number identifying the supported music service.
        '''
        return 0

    @staticmethod
    def magic():
        return (
            r'^share:(album|track|playlist|artist):.+$',
            r'^https?://.+/(album|track|playlist|artist)/[^/?#]+',
            r'^https?://.+\?.*\buri=[^&#]+',
        )

    def extract(self, uri):
        '''Extract the share type and encoded URI from a share link.
        Returns:
            share_type: The shared type, like ""album"" or ""track"".
            encoded_uri: An escaped URI with a service-specific format.
        '''
        if not isinstance(uri, str) or not uri:
            raise ValueError(""URI must be a non-empty string"")

        # Case 1: share:<type>:<encoded>
        if uri.startswith(""share:""):
            parts = uri.split("":"", 2)
            if len(parts) >= 3:
                share_type = parts[1].lower()
                encoded_uri = parts[2]
                if share_type in {""album"", ""track"", ""playlist"", ""artist""} and encoded_uri:
                    return share_type, encoded_uri
            raise ValueError(""Invalid share URI format"")

        # Case 2: URL formats
        from urllib.parse import urlparse, parse_qs

        parsed = urlparse(uri)
        if not parsed.scheme or not parsed.netloc:
            raise ValueError(""Invalid URI"")

        # Try query param ""uri"" first (commonly percent-encoded full service URI)
        qs = parse_qs(parsed.query)
        if ""uri"" in qs and qs[""uri""]:
            encoded_uri = qs[""uri""][0]
            # Attempt to infer share_type from encoded service URI (e.g., spotify:track:...)
            # We do minimal parsing and keep encoded form as-is.
            maybe_decoded = encoded_uri
            # Determine share_type heuristically
            share_type = None
            # Try to detect pattern like scheme:type:...
            # We avoid unquoting to preserve ""escaped URI"" nature.
            colon_parts = maybe_decoded.split("":"")
            if len(colon_parts) >= 2:
                inferred = colon_parts[1].lower()
                if inferred in {""album"", ""track"", ""playlist"", ""artist""}:
                    share_type = inferred
            # Fallback: derive from path segments
            if not share_type:
                segs = [s for s in parsed.path.split(""/"") if s]
                for seg in segs:
                    low = seg.lower()
                    if low in {""album"", ""track"", ""playlist"", ""artist""}:
                        share_type = low
                        break
            if not share_type:
                raise ValueError(""Unable to determine share type from URI"")
            return share_type, encoded_uri

        # Else derive from path: .../<type>/<id>(?...)?(#...)?
        segments = [s for s in parsed.path.split(""/"") if s]
        share_type = None
        encoded_uri = None
        for i, seg in enumerate(segments):
            low = seg.lower()
            if low in {""album"", ""track"", ""playlist"", ""artist""}:
                share_type = low
                # Next segment is typically the identifier
                if i + 1 < len(segments):
                    candidate = segments[i + 1]
                else:
                    candidate = """"
                # Strip query-like leftovers if any (shouldn't be present as we used path)
                # Keep as-is (already ""encoded"" enough for our purposes)
                encoded_uri = candidate
                break

        if not share_type or not encoded_uri:
            raise ValueError(""Unsupported or unrecognized share URI format"")

        return share_type, encoded_uri"
131928,mjg59/python-broadlink,mjg59_python-broadlink/broadlink/protocol.py,broadlink.protocol.Datetime,"import time
import datetime as dt

class Datetime:
    """"""Helps to pack and unpack datetime objects for the Broadlink protocol.""""""

    @staticmethod
    def pack(datetime: dt.datetime) -> bytes:
        """"""Pack the timestamp to be sent over the Broadlink protocol.""""""
        data = bytearray(12)
        utcoffset = int(datetime.utcoffset().total_seconds() / 3600)
        data[:4] = utcoffset.to_bytes(4, 'little', signed=True)
        data[4:6] = datetime.year.to_bytes(2, 'little')
        data[6] = datetime.minute
        data[7] = datetime.hour
        data[8] = int(datetime.strftime('%y'))
        data[9] = datetime.isoweekday()
        data[10] = datetime.day
        data[11] = datetime.month
        return data

    @staticmethod
    def unpack(data: bytes) -> dt.datetime:
        """"""Unpack a timestamp received over the Broadlink protocol.""""""
        utcoffset = int.from_bytes(data[0:4], 'little', signed=True)
        year = int.from_bytes(data[4:6], 'little')
        minute = data[6]
        hour = data[7]
        subyear = data[8]
        isoweekday = data[9]
        day = data[10]
        month = data[11]
        tz_info = dt.timezone(dt.timedelta(hours=utcoffset))
        datetime = dt.datetime(year, month, day, hour, minute, 0, 0, tz_info)
        if datetime.isoweekday() != isoweekday:
            raise ValueError('isoweekday does not match')
        if int(datetime.strftime('%y')) != subyear:
            raise ValueError('subyear does not match')
        return datetime

    @staticmethod
    def now() -> dt.datetime:
        """"""Return the current date and time with timezone info.""""""
        tz_info = dt.timezone(dt.timedelta(seconds=-time.timezone))
        return dt.datetime.now(tz_info)","
class Datetime:
    
    @staticmethod
    def pack(datetime: dt.datetime) -> bytes:
        
        pass
    @staticmethod
    def unpack(data: bytes) -> dt.datetime:
        '''Unpack a timestamp received over the Broadlink protocol.'''
        pass
    @staticmethod
    def now() -> dt.datetime:
        
        pass",snippet_48,4,"class Datetime:
    
    @staticmethod
    def pack(datetime) -> bytes:
        import struct, datetime as dt
        if not isinstance(datetime, dt.datetime):
            raise TypeError(""datetime must be a datetime.datetime instance"")
        # Use Unix time (seconds) as 4-byte little-endian
        ts = int(datetime.timestamp())
        if ts < 0:
            raise ValueError(""datetime is before Unix epoch"")
        return struct.pack(""<I"", ts)

    @staticmethod
    def unpack(data: bytes):
        '''Unpack a timestamp received over the Broadlink protocol.'''
        import struct, datetime as dt

        if not isinstance(data, (bytes, bytearray)):
            raise TypeError(""data must be bytes"")

        n = len(data)

        # 4 bytes: Unix epoch seconds, little-endian
        if n == 4:
            ts = struct.unpack(""<I"", data)[0]
            return dt.datetime.fromtimestamp(ts)

        # 6 bytes: [year-2000, month, day, hour, minute, second]
        if n == 6:
            y = 2000 + data[0]
            m, d, hh, mm, ss = data[1], data[2], data[3], data[4], data[5]
            return dt.datetime(y, m, d, hh, mm, ss)

        # 7 bytes: [year_low, year_high, month, day, hour, minute, second]
        if n == 7:
            y = data[0] | (data[1] << 8)
            m, d, hh, mm, ss = data[2], data[3], data[4], data[5], data[6]
            return dt.datetime(y, m, d, hh, mm, ss)

        # 8 bytes: try unsigned 64-bit epoch
        if n == 8:
            val = struct.unpack(""<Q"", data)[0]
            # Heuristic: if value looks like milliseconds, convert; else treat as seconds
            if val > 10**12:
                return dt.datetime.fromtimestamp(val / 1000.0)
            return dt.datetime.fromtimestamp(val)

        raise ValueError(f""Unsupported timestamp length: {n}"")

    @staticmethod
    def now():
        import datetime as dt
        return dt.datetime.now()"
132032,rigetti/pyquil,pyquil/quilatom.py,pyquil.quilatom.QuilAtom,"class QuilAtom:
    """"""Abstract class for atomic elements of Quil.""""""

    def out(self) -> str:
        """"""Return the element as a valid Quil string.""""""
        raise NotImplementedError()

    def __str__(self) -> str:
        """"""Get a string representation of the element, possibly not valid Quil.""""""
        raise NotImplementedError()

    def __eq__(self, other: object) -> bool:
        """"""Return True if the other object is equal to this one.""""""
        raise NotImplementedError()

    def __hash__(self) -> int:
        """"""Return a hash of the object.""""""
        raise NotImplementedError()","class QuilAtom:
    
    def out(self) -> str:
        '''Return the element as a valid Quil string.'''
        pass
    def __str__(self) -> str:
        
        pass
    def __eq__(self, other: object) -> bool:
        
        pass
    def __hash__(self) -> int:
        
        pass",snippet_49,5,"class QuilAtom:
    
    def out(self) -> str:
        '''Return the element as a valid Quil string.'''
        raise NotImplementedError(""Subclasses must implement out()"")
    def __str__(self) -> str:
        return self.out()
    def __eq__(self, other: object) -> bool:
        if not isinstance(other, QuilAtom):
            return NotImplemented
        return type(self) is type(other) and self.out() == other.out()
    def __hash__(self) -> int:
        return hash((type(self), self.out()))"
134597,Unidata/MetPy,Unidata_MetPy/tools/flake8-metpy/flake8_metpy.py,flake8_metpy.MetPyChecker,"class MetPyChecker:
    """"""Flake8 plugin class to check MetPy style/best practice.""""""
    name = __name__
    version = '1.0'

    def __init__(self, tree):
        """"""Initialize the plugin.""""""
        self.tree = tree

    def run(self):
        """"""Run the plugin and yield errors.""""""
        visitor = MetPyVisitor()
        visitor.visit(self.tree)
        for err in visitor.errors:
            yield self.error(err)

    def error(self, err):
        """"""Format errors into Flake8's required format.""""""
        return (err.lineno, err.col, f'MPY{err.code:03d}: Multiplying/dividing by units--use units.Quantity()', type(self))","class MetPyChecker:
    
    def __init__(self, tree):
        '''Initialize the plugin.'''
        pass
    def run(self):
        
        pass
    def error(self, err):
        
        pass",snippet_50,4,"class MetPyChecker:
    name = ""metpy-checker""
    version = ""0.1.0""

    CODE_WILDCARD_IMPORT = ""MPY100""
    MSG_WILDCARD_IMPORT = ""MPY100 Avoid 'import *' from MetPy; import explicit names.""

    def __init__(self, tree):
        '''Initialize the plugin.'''
        self.tree = tree
        self._errors = []

    def run(self):
        if self.tree is None:
            return
        import ast

        for node in ast.walk(self.tree):
            if isinstance(node, ast.ImportFrom):
                mod = node.module or """"
                if mod.startswith(""metpy""):
                    for alias in node.names:
                        if alias.name == ""*"":
                            yield self.error(
                                {
                                    ""line"": getattr(node, ""lineno"", 1),
                                    ""col"": getattr(node, ""col_offset"", 0),
                                    ""msg"": self.MSG_WILDCARD_IMPORT,
                                }
                            )

    def error(self, err):
        return (err.get(""line"", 1), err.get(""col"", 0), err.get(""msg"", """"), type(self))"
134620,Unidata/MetPy,Unidata_MetPy/src/metpy/io/_tools.py,metpy.io._tools.Array,"from struct import Struct

class Array:
    """"""Use a Struct as a callable to unpack a bunch of bytes as a list.""""""

    def __init__(self, fmt):
        """"""Initialize the Struct unpacker.""""""
        self._struct = Struct(fmt)

    def __call__(self, buf):
        """"""Perform the actual unpacking.""""""
        return list(self._struct.unpack(buf))","
class Array:
    '''Use a Struct as a callable to unpack a bunch of bytes as a list.'''
    def __init__(self, fmt):
        
        pass
    def __call__(self, buf):
        '''Perform the actual unpacking.'''
        pass",snippet_51,3,"import struct

class Array:
    '''Use a Struct as a callable to unpack a bunch of bytes as a list.'''
    def __init__(self, fmt):
        self._struct = struct.Struct(fmt)
        # Determine number of items produced per unpack
        self._items_per = len(self._struct.unpack(b'\x00' * self._struct.size))

    def __call__(self, buf):
        '''Perform the actual unpacking.'''
        mv = memoryview(buf)
        if len(mv) % self._struct.size != 0:
            raise ValueError(""Buffer size must be a multiple of the struct size"")
        out = []
        for i in range(0, len(mv), self._struct.size):
            chunk = mv[i:i + self._struct.size]
            vals = self._struct.unpack(chunk)
            if self._items_per == 1:
                out.append(vals[0])
            else:
                out.append(vals)
        return out"
134621,Unidata/MetPy,Unidata_MetPy/src/metpy/io/_tools.py,metpy.io._tools.BitField,"class BitField:
    """"""Convert an integer to a string for each bit.""""""

    def __init__(self, *names):
        """"""Initialize the list of named bits.""""""
        self._names = names

    def __call__(self, val):
        """"""Return a list with a string for each True bit in the integer.""""""
        if not val:
            return None
        bits = []
        for n in self._names:
            if val & 1:
                bits.append(n)
            val >>= 1
            if not val:
                break
        return bits[0] if len(bits) == 1 else bits","class BitField:
    '''Convert an integer to a string for each bit.'''
    def __init__(self, *names):
        '''Initialize the list of named bits.'''
        pass
    def __call__(self, val):
        
        pass",snippet_52,3,"class BitField:
    '''Convert an integer to a string for each bit.'''
    def __init__(self, *names):
        '''Initialize the list of named bits.'''
        # Allow either varargs of names or a single iterable of names
        if len(names) == 1 and isinstance(names[0], (list, tuple)):
            self.names = list(names[0])
        else:
            self.names = list(names)

    def __call__(self, val):
        try:
            val = int(val)
        except (TypeError, ValueError):
            raise ValueError(""Value must be convertible to int"")
        if val < 0:
            raise ValueError(""Value must be non-negative"")

        result = []
        i = 0
        v = val
        while v:
            if v & 1:
                if i < len(self.names):
                    result.append(str(self.names[i]))
                else:
                    result.append(str(i))
            v >>= 1
            i += 1

        return ', '.join(result)"
134622,Unidata/MetPy,Unidata_MetPy/src/metpy/io/_tools.py,metpy.io._tools.Bits,"class Bits:
    """"""Breaks an integer into a specified number of True/False bits.""""""

    def __init__(self, num_bits):
        """"""Initialize the number of bits.""""""
        self._bits = range(num_bits)

    def __call__(self, val):
        """"""Convert the integer to the list of True/False values.""""""
        return [bool(val >> i & 1) for i in self._bits]","class Bits:
    
    def __init__(self, num_bits):
        '''Initialize the number of bits.'''
        pass
    def __call__(self, val):
        '''Convert the integer to the list of True/False values.'''
        pass",snippet_53,3,"class Bits:
    
    def __init__(self, num_bits):
        '''Initialize the number of bits.'''
        if not isinstance(num_bits, int):
            raise TypeError(""num_bits must be an integer"")
        if num_bits <= 0:
            raise ValueError(""num_bits must be a positive integer"")
        self.num_bits = num_bits

    def __call__(self, val):
        '''Convert the integer to the list of True/False values.'''
        if not isinstance(val, int):
            raise TypeError(""val must be an integer"")
        mask = (1 << self.num_bits) - 1
        v = val & mask
        return [bool((v >> (self.num_bits - 1 - i)) & 1) for i in range(self.num_bits)]"
134624,Unidata/MetPy,Unidata_MetPy/src/metpy/io/_tools.py,metpy.io._tools.Enum,"class Enum:
    """"""Map values to specific strings.""""""

    def __init__(self, *args, **kwargs):
        """"""Initialize the mapping.""""""
        self.val_map = dict(enumerate(args))
        self.val_map.update(zip(kwargs.values(), kwargs.keys(), strict=False))

    def __call__(self, val):
        """"""Map an integer to the string representation.""""""
        return self.val_map.get(val, f'Unknown ({val})')","class Enum:
    
    def __init__(self, *args, **kwargs):
        
        pass
    def __call__(self, val):
        '''Map an integer to the string representation.'''
        pass",snippet_54,3,"class Enum:
    
    def __init__(self, *args, **kwargs):
        self._name_to_value = {}
        self._value_to_name = {}
        if args and kwargs:
            raise TypeError(""Provide either positional names or keyword mappings, not both."")
        if len(args) == 1 and isinstance(args[0], (list, tuple)):
            args = tuple(args[0])
        if kwargs:
            items = list(kwargs.items())
        else:
            items = [(name, idx) for idx, name in enumerate(args)]
        for name, value in items:
            if not isinstance(name, str):
                raise TypeError(""Enum name must be a string."")
            if not isinstance(value, int):
                raise TypeError(""Enum value must be an int."")
            if name in self._name_to_value:
                raise ValueError(f""Duplicate enum name: {name}"")
            if value in self._value_to_name:
                raise ValueError(f""Duplicate enum value: {value}"")
            self._name_to_value[name] = value
            self._value_to_name[value] = name
            setattr(self, name, value)

    def __call__(self, val):
        '''Map an integer to the string representation.'''
        if not isinstance(val, int):
            raise TypeError(""Enum value must be an int."")
        try:
            return self._value_to_name[val]
        except KeyError:
            raise ValueError(f""{val} is not a valid enum value."")"
134657,Unidata/MetPy,Unidata_MetPy/src/metpy/package_tools.py,metpy.package_tools.Exporter,"class Exporter:
    """"""Manages exporting of symbols from the module.

    Grabs a reference to `globals()` for a module and provides a decorator to add
    functions and classes to `__all__` rather than requiring a separately maintained list.
    Also provides a context manager to do this for instances by adding all instances added
    within a block to `__all__`.
    """"""

    def __init__(self, globls):
        """"""Initialize the Exporter.""""""
        self.globls = globls
        self.exports = globls.setdefault('__all__', [])

    def export(self, defn):
        """"""Declare a function or class as exported.""""""
        self.exports.append(defn.__name__)
        return defn

    def __enter__(self):
        """"""Start a block tracking all instances created at global scope.""""""
        self.start_vars = set(self.globls)

    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""Exit the instance tracking block.""""""
        self.exports.extend(set(self.globls) - self.start_vars)
        del self.start_vars","class Exporter:
    
    def __init__(self, globls):
        
        pass
    def export(self, defn):
        
        pass
    def __enter__(self):
        '''Start a block tracking all instances created at global scope.'''
        pass
    def __exit__(self, exc_type, exc_val, exc_tb):
        '''Exit the instance tracking block.'''
        pass",snippet_55,5,"class Exporter:
    def __init__(self, globls):
        if not isinstance(globls, dict):
            raise TypeError(""globls must be a dict of globals()"")
        self._g = globls
        self._pre_keys = None
        self._ensure_all()

    def _ensure_all(self):
        allv = self._g.get('__all__')
        if allv is None:
            self._g['__all__'] = []
        elif not isinstance(allv, list):
            # Coerce to list for consistent behavior
            self._g['__all__'] = list(allv)

    def _add_to_all(self, name):
        if not name or not isinstance(name, str):
            return
        if name == '__all__' or name.startswith('_'):
            return
        allv = self._g['__all__']
        if name not in allv:
            allv.append(name)

    def export(self, defn):
        import types

        def _export_one(item):
            # If string, just export by name (must already exist in globals)
            if isinstance(item, str):
                if item in self._g:
                    self._add_to_all(item)
                else:
                    # Still add to __all__ to allow forward declaration style
                    self._add_to_all(item)
                return item

            # If it looks like a definition (function/class/object with __name__)
            name = getattr(item, '__name__', None)
            if not name:
                raise ValueError(""Cannot export object without a __name__"")
            # Place into globals if not already there
            if self._g.get(name) is not item:
                self._g[name] = item
            # Ensure module name matches current module if applicable
            if hasattr(item, '__module__'):
                try:
                    modname = self._g.get('__name__')
                    if isinstance(modname, str):
                        item.__module__ = modname
                except Exception:
                    pass
            self._add_to_all(name)
            return item

        if isinstance(defn, (list, tuple, set)):
            return [self.export(x) for x in defn]
        return _export_one(defn)

    def __enter__(self):
        '''Start a block tracking all instances created at global scope.'''
        self._ensure_all()
        self._pre_keys = set(self._g.keys())
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        '''Exit the instance tracking block.'''
        if exc_type is not None:
            # Do not modify exports if the block failed
            self._pre_keys = None
            return False

        import types

        if self._pre_keys is None:
            return False

        new_keys = set(self._g.keys()) - self._pre_keys
        for name in sorted(new_keys):
            if name == '__all__' or name.startswith('_'):
                continue
            val = self._g.get(name)
            if isinstance(val, types.ModuleType):
                continue
            self._add_to_all(name)

        self._pre_keys = None
        return False"
137119,jpype-project/jpype,jpype-project_jpype/doc/java/util.py,java.util.Collection,"class Collection:
    """""" Customized interface representing a collection of items.

    JPype wraps ``java.util.Collection`` as a Python collection.
    """"""

    def __len__(self) -> int:
        """""" Get the length of this collection.

        Use ``len(collection)`` to find the number of items in this
        collection.

        """"""
        ...

    def __delitem__(self, item):
        """""" Collections do not support remove by index. """"""
        ...

    def __contains__(self, item) -> bool:
        """""" Check if this collection contains this item.

        Use ``item in collection`` to check if the item is 
        present.

        Args:
           item: is the item to check for.  This must be a Java
           object or an object which can be automatically converted
           such as a string.

        Returns:
           bool: True if the item is in the collection.
        """"""
        ...","class Collection:
    ''' Customized interface representing a collection of items.
    JPype wraps ``java.util.Collection`` as a Python collection.
    '''
    def __len__(self) -> int:
        ''' Get the length of this collection.
        Use ``len(collection)`` to find the number of items in this
        collection.
        '''
        pass
    def __delitem__(self, item):
        
        pass
    def __contains__(self, item) -> bool:
        ''' Check if this collection contains this item.
        Use ``item in collection`` to check if the item is 
        present.
        Args:
           item: is the item to check for.  This must be a Java
           object or an object which can be automatically converted
           such as a string.
        Returns:
           bool: True if the item is in the collection.
        '''
        pass",snippet_56,4,"class Collection:
    ''' Customized interface representing a collection of items.
    JPype wraps ``java.util.Collection`` as a Python collection.
    '''
    def __init__(self, iterable=()):
        self._items = list(iterable)

    def __len__(self) -> int:
        ''' Get the length of this collection.
        Use ``len(collection)`` to find the number of items in this
        collection.
        '''
        return len(self._items)

    def __delitem__(self, item):
        if isinstance(item, slice):
            del self._items[item]
            return
        if isinstance(item, int):
            del self._items[item]
            return
        try:
            idx = self._items.index(item)
        except ValueError:
            raise ValueError(f""Item not found: {item!r}"")
        del self._items[idx]

    def __contains__(self, item) -> bool:
        ''' Check if this collection contains this item.
        Use ``item in collection`` to check if the item is 
        present.
        Args:
           item: is the item to check for.  This must be a Java
           object or an object which can be automatically converted
           such as a string.
        Returns:
           bool: True if the item is in the collection.
        '''
        return item in self._items"
138700,sebp/scikit-survival,sebp_scikit-survival/sksurv/util.py,sksurv.util.Surv,"from sklearn.utils.validation import check_array, check_consistent_length
import numpy as np
import pandas as pd

class Surv:
    """"""A helper class to create a structured array for survival analysis.

    This class provides helper functions to create a structured array that
    encapsulates the event indicator and the observed time. The resulting
    structured array is the recommended format for the ``y`` argument in
    scikit-survival's estimators.
    """"""

    @staticmethod
    def from_arrays(event, time, name_event=None, name_time=None):
        """"""Create structured array from event indicator and time arrays.

        Parameters
        ----------
        event : array-like, shape=(n_samples,)
            Event indicator. A boolean array or array with values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : array-like, shape=(n_samples,)
            Observed time. Time to event or time of censoring.
        name_event : str, optional, default: 'event'
            Name of the event field in the structured array.
        name_time : str, optional, default: 'time'
            Name of the observed time field in the structured array.

        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are set to the values of `name_event` and `name_time`.

        Examples
        --------
        >>> from sksurv.util import Surv
        >>>
        >>> y = Surv.from_arrays(event=[True, False, True],
        ...                      time=[10, 25, 15])
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('event', '?'), ('time', '<f8')])
        >>> y['event']
        array([ True, False,  True])
        >>> y['time']
        array([10., 25., 15.])
        """"""
        name_event = name_event or 'event'
        name_time = name_time or 'time'
        if name_time == name_event:
            raise ValueError('name_time must be different from name_event')
        time = np.asanyarray(time, dtype=float)
        y = np.empty(time.shape[0], dtype=[(name_event, bool), (name_time, float)])
        y[name_time] = time
        event = np.asanyarray(event)
        check_consistent_length(time, event)
        if np.issubdtype(event.dtype, np.bool_):
            y[name_event] = event
        else:
            events = np.unique(event)
            events.sort()
            if len(events) != 2:
                raise ValueError('event indicator must be binary')
            if np.all(events == np.array([0, 1], dtype=events.dtype)):
                y[name_event] = event.astype(bool)
            else:
                raise ValueError('non-boolean event indicator must contain 0 and 1 only')
        return y

    @staticmethod
    def from_dataframe(event, time, data):
        """"""Create structured array from columns in a pandas DataFrame.

        Parameters
        ----------
        event : str
            Name of the column in ``data`` containing the event indicator.
            It must be a boolean or have values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : str
            Name of the column in ``data`` containing the observed time
            (time to event or time of censoring).
        data : pandas.DataFrame
            A DataFrame with columns for event and time.

        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are the respective column names.

        Examples
        --------
        >>> import pandas as pd
        >>> from sksurv.util import Surv
        >>>
        >>> df = pd.DataFrame({
        ...     'status': [True, False, True],
        ...     'followup_time': [10, 25, 15],
        ... })
        >>> y = Surv.from_dataframe(
        ...     event='status', time='followup_time', data=df,
        ... )
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('status', '?'), ('followup_time', '<f8')])
        >>> y['status']
        array([ True, False,  True])
        >>> y['followup_time']
        array([10., 25., 15.])
        """"""
        if not isinstance(data, pd.DataFrame):
            raise TypeError(f'expected pandas.DataFrame, but got {type(data)!r}')
        return Surv.from_arrays(data.loc[:, event].values, data.loc[:, time].values, name_event=str(event), name_time=str(time))","
class Surv:
    '''A helper class to create a structured array for survival analysis.
    This class provides helper functions to create a structured array that
    encapsulates the event indicator and the observed time. The resulting
    structured array is the recommended format for the ``y`` argument in
    scikit-survival's estimators.
    '''
    @staticmethod
    def from_arrays(event, time, name_event=None, name_time=None):
        '''Create structured array from event indicator and time arrays.
        Parameters
        ----------
        event : array-like, shape=(n_samples,)
            Event indicator. A boolean array or array with values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : array-like, shape=(n_samples,)
            Observed time. Time to event or time of censoring.
        name_event : str, optional, default: 'event'
            Name of the event field in the structured array.
        name_time : str, optional, default: 'time'
            Name of the observed time field in the structured array.
        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are set to the values of `name_event` and `name_time`.
        Examples
        --------
        >>> from sksurv.util import Surv
        >>>
        >>> y = Surv.from_arrays(event=[True, False, True],
        ...                      time=[10, 25, 15])
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('event', '?'), ('time', '<f8')])
        >>> y['event']
        array([ True, False,  True])
        >>> y['time']
        array([10., 25., 15.])
        '''
        pass
    @staticmethod
    def from_dataframe(event, time, data):
        '''Create structured array from columns in a pandas DataFrame.
        Parameters
        ----------
        event : str
            Name of the column in ``data`` containing the event indicator.
            It must be a boolean or have values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : str
            Name of the column in ``data`` containing the observed time
            (time to event or time of censoring).
        data : pandas.DataFrame
            A DataFrame with columns for event and time.
        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are the respective column names.
        Examples
        --------
        >>> import pandas as pd
        >>> from sksurv.util import Surv
        >>>
        >>> df = pd.DataFrame({
        ...     'status': [True, False, True],
        ...     'followup_time': [10, 25, 15],
        ... })
        >>> y = Surv.from_dataframe(
        ...     event='status', time='followup_time', data=df,
        ... )
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('status', '?'), ('followup_time', '<f8')])
        >>> y['status']
        array([ True, False,  True])
        >>> y['followup_time']
        array([10., 25., 15.])
        '''
        pass",snippet_57,3,"class Surv:
    '''A helper class to create a structured array for survival analysis.
    This class provides helper functions to create a structured array that
    encapsulates the event indicator and the observed time. The resulting
    structured array is the recommended format for the ``y`` argument in
    scikit-survival's estimators.
    '''
    @staticmethod
    def from_arrays(event, time, name_event=None, name_time=None):
        '''Create structured array from event indicator and time arrays.
        Parameters
        ----------
        event : array-like, shape=(n_samples,)
            Event indicator. A boolean array or array with values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : array-like, shape=(n_samples,)
            Observed time. Time to event or time of censoring.
        name_event : str, optional, default: 'event'
            Name of the event field in the structured array.
        name_time : str, optional, default: 'time'
            Name of the observed time field in the structured array.
        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are set to the values of `name_event` and `name_time`.
        Examples
        --------
        >>> from sksurv.util import Surv
        >>>
        >>> y = Surv.from_arrays(event=[True, False, True],
        ...                      time=[10, 25, 15])
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('event', '?'), ('time', '<f8')])
        >>> y['event']
        array([ True, False,  True])
        >>> y['time']
        array([10., 25., 15.])
        '''
        import numpy as np

        if name_event is None:
            name_event = ""event""
        if name_time is None:
            name_time = ""time""

        ev = np.asarray(event)
        tm = np.asarray(time)

        if ev.shape[0] != tm.shape[0]:
            raise ValueError(""event and time must have the same number of samples"")

        ev_bool = ev.astype(bool, copy=False)
        tm_float = tm.astype(float, copy=False)

        dtype = [(name_event, np.bool_), (name_time, np.float64)]
        y = np.empty(ev_bool.shape[0], dtype=dtype)
        y[name_event] = ev_bool
        y[name_time] = tm_float
        return y

    @staticmethod
    def from_dataframe(event, time, data):
        '''Create structured array from columns in a pandas DataFrame.
        Parameters
        ----------
        event : str
            Name of the column in ``data`` containing the event indicator.
            It must be a boolean or have values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : str
            Name of the column in ``data`` containing the observed time
            (time to event or time of censoring).
        data : pandas.DataFrame
            A DataFrame with columns for event and time.
        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are the respective column names.
        Examples
        --------
        >>> import pandas as pd
        >>> from sksurv.util import Surv
        >>>
        >>> df = pd.DataFrame({
        ...     'status': [True, False, True],
        ...     'followup_time': [10, 25, 15],
        ... })
        >>> y = Surv.from_dataframe(
        ...     event='status', time='followup_time', data=df,
        ... )
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('status', '?'), ('followup_time', '<f8')])
        >>> y['status']
        array([ True, False,  True])
        >>> y['followup_time']
        array([10., 25., 15.])
        '''
        ev = data[event].to_numpy()
        tm = data[time].to_numpy()
        return Surv.from_arrays(ev, tm, name_event=event, name_time=time)"
139652,sentinel-hub/eo-learn,sentinel-hub_eo-learn/eolearn/geometry/morphology.py,eolearn.geometry.morphology.MorphologicalStructFactory,"import cv2
import numpy as np

class MorphologicalStructFactory:
    """"""
    Factory methods for generating morphological structuring elements
    """"""

    @staticmethod
    def get_disk(radius: int) -> np.ndarray:
        """"""
        :param radius: Radius of disk
        :return: The structuring element where elements of the neighborhood are 1 and 0 otherwise.
        """"""
        return cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (radius, radius))

    @staticmethod
    def get_rectangle(width: int, height: int) -> np.ndarray:
        """"""
        :param width: Width of rectangle
        :param height: Height of rectangle
        :return: A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.
        """"""
        return cv2.getStructuringElement(cv2.MORPH_RECT, (height, width))

    @staticmethod
    def get_square(width: int) -> np.ndarray:
        """"""
        :param width: Size of square
        :return: A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.
        """"""
        return cv2.getStructuringElement(cv2.MORPH_RECT, (width, width))","
class MorphologicalStructFactory:
    
    @staticmethod
    def get_disk(radius: int) -> np.ndarray:
        '''
        :param radius: Radius of disk
        :return: The structuring element where elements of the neighborhood are 1 and 0 otherwise.
        '''
        pass
    @staticmethod
    def get_rectangle(width: int, height: int) -> np.ndarray:
        '''
        :param width: Width of rectangle
        :param height: Height of rectangle
        :return: A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.
        '''
        pass
    @staticmethod
    def get_square(width: int) -> np.ndarray:
        '''
        :param width: Size of square
        :return: A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.
        '''
        pass",snippet_58,4,"import numpy as np

class MorphologicalStructFactory:
    
    @staticmethod
    def get_disk(radius: int) -> np.ndarray:
        '''
        :param radius: Radius of disk
        :return: The structuring element where elements of the neighborhood are 1 and 0 otherwise.
        '''
        if not isinstance(radius, int):
            raise TypeError(""radius must be an integer"")
        if radius < 0:
            raise ValueError(""radius must be >= 0"")
        size = 2 * radius + 1
        y, x = np.ogrid[-radius:radius + 1, -radius:radius + 1]
        mask = (x * x + y * y) <= radius * radius
        return mask.astype(np.uint8)

    @staticmethod
    def get_rectangle(width: int, height: int) -> np.ndarray:
        '''
        :param width: Width of rectangle
        :param height: Height of rectangle
        :return: A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.
        '''
        if not isinstance(width, int) or not isinstance(height, int):
            raise TypeError(""width and height must be integers"")
        if width <= 0 or height <= 0:
            raise ValueError(""width and height must be > 0"")
        return np.ones((height, width), dtype=np.uint8)

    @staticmethod
    def get_square(width: int) -> np.ndarray:
        '''
        :param width: Size of square
        :return: A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.
        '''
        if not isinstance(width, int):
            raise TypeError(""width must be an integer"")
        if width <= 0:
            raise ValueError(""width must be > 0"")
        return np.ones((width, width), dtype=np.uint8)"
139672,sentinel-hub/eo-learn,sentinel-hub_eo-learn/eolearn/visualization/eoexecutor.py,eolearn.visualization.eoexecutor._ErrorSummary,"from dataclasses import dataclass

@dataclass()
class _ErrorSummary:
    """"""Contains data for errors of a node.""""""
    origin: str
    example_message: str
    failed_indexed_executions: list[tuple[int, str]]

    def add_execution(self, index: int, name: str) -> None:
        """"""Adds an execution to the summary.""""""
        self.failed_indexed_executions.append((index, name))

    @property
    def num_failed(self) -> int:
        """"""Helps with jinja""""""
        return len(self.failed_indexed_executions)","@dataclass()
class _ErrorSummary:
    '''Contains data for errors of a node.'''
    def add_execution(self, index: int, name: str) -> None:
        '''Adds an execution to the summary.'''
        pass
    @property
    def num_failed(self) -> int:
        
        pass",snippet_59,3,"from dataclasses import dataclass, field
from typing import List, Tuple

@dataclass()
class _ErrorSummary:
    '''Contains data for errors of a node.'''
    executions: List[Tuple[int, str]] = field(default_factory=list)

    def add_execution(self, index: int, name: str) -> None:
        '''Adds an execution to the summary.'''
        self.executions.append((index, name))

    @property
    def num_failed(self) -> int:
        return len(self.executions)"
139835,python-escpos/python-escpos,python-escpos_python-escpos/src/escpos/codepages.py,escpos.codepages.CodePageManager,"class CodePageManager:
    """"""Holds information about all the code pages.

    Information as defined in escpos-printer-db.
    """"""

    def __init__(self, data):
        """"""Initialize code page manager.""""""
        self.data = data

    @staticmethod
    def get_encoding_name(encoding):
        """"""Get encoding name.

        .. todo:: Resolve the encoding alias.
        """"""
        return encoding.upper()

    def get_encoding(self, encoding):
        """"""Return the encoding data.""""""
        return self.data[encoding]","class CodePageManager:
    '''Holds information about all the code pages.
    Information as defined in escpos-printer-db.
        '''
    def __init__(self, data):
        
        pass
    @staticmethod
    def get_encoding_name(encoding):
        
        pass
    def get_encoding_name(encoding):
        '''Return the encoding data.'''
        pass",snippet_60,4,"class CodePageManager:
    '''Holds information about all the code pages.
    Information as defined in escpos-printer-db.
        '''
    def __init__(self, data):
        self._raw = data
        self._encodings = {}

        if isinstance(data, dict):
            items = data.items()
        else:
            try:
                items = enumerate(data)
            except TypeError:
                items = ()

        for key, info in items:
            if isinstance(info, dict):
                # Primary identifier from dict key if it's a str, else from info
                identifiers = []
                if isinstance(key, str):
                    identifiers.append(key)
                enc = info.get(""encoding"")
                if isinstance(enc, str):
                    identifiers.append(enc)
                # also common aliases inside info
                for alias_key in (""aliases"", ""alias"", ""names"", ""name""):
                    aliases = info.get(alias_key)
                    if isinstance(aliases, str):
                        identifiers.append(aliases)
                    elif isinstance(aliases, (list, tuple, set)):
                        identifiers.extend([a for a in aliases if isinstance(a, str)])

                # Build normalized map
                for ident in identifiers:
                    norm = self._normalize_encoding(ident)
                    if norm and norm not in self._encodings:
                        self._encodings[norm] = info
                        # also store canonical python codec name if present
                        if isinstance(enc, str):
                            pyname = self._normalize_encoding(enc)
                            if pyname and pyname not in self._encodings:
                                self._encodings[pyname] = info
            else:
                # if info is not dict, try to treat as string encoding name
                if isinstance(info, str):
                    norm = self._normalize_encoding(info)
                    if norm and norm not in self._encodings:
                        self._encodings[norm] = {""encoding"": info}

                if isinstance(key, str):
                    normk = self._normalize_encoding(key)
                    if normk and normk not in self._encodings:
                        self._encodings[normk] = {""encoding"": key}

    @staticmethod
    def _normalize_encoding(encoding):
        if encoding is None:
            return None
        if not isinstance(encoding, str):
            encoding = str(encoding)
        s = encoding.strip().lower()

        # Remove common separators
        for ch in [' ', '-', '_', '.', '/']:
            s = s.replace(ch, '')

        # Normalize common vendor prefixes
        if s.startswith('windows'):
            s = 'cp' + s[len('windows'):]
        if s.startswith('ibm'):
            s = 'cp' + s[len('ibm'):]
        if s.startswith('ms'):
            s = 'cp' + s[len('ms'):]
        if s.startswith('cp-'):
            s = 'cp' + s[3:]

        # Trim 'cs' registry prefix
        if s.startswith('cs') and s not in ('cskoi8r',):
            s = s[2:]

        # Common special cases
        if s in ('utf8', 'utf'):
            s = 'utf8'
        elif s in ('utf16', 'utf16le', 'utf16be'):
            s = s
        elif s in ('utf32', 'utf32le', 'utf32be'):
            s = s
        elif s in ('latin1', 'latin'):
            s = 'iso88591'
        elif s.startswith('iso8859'):
            s = s

        return s or None

    def get_encoding_name(*args):
        '''Return the encoding data.'''
        # Support both instance and class-style calls.
        if len(args) == 2:
            self, encoding = args
            norm = CodePageManager._normalize_encoding(encoding)
            if norm is None:
                return None
            # Direct hit
            info = getattr(self, ""_encodings"", {}).get(norm)
            if info is not None:
                return info
            # Try alternative fallbacks
            # utf-8 variants
            if norm in ('utf8', 'utf_8'):
                return getattr(self, ""_encodings"", {}).get('utf8')
            # cp vs windows prefixes
            if norm.startswith('cp'):
                alt = 'windows' + norm[2:]
                info = getattr(self, ""_encodings"", {}).get(CodePageManager._normalize_encoding(alt))
                if info is not None:
                    return info
            if norm.startswith('windows'):
                alt = 'cp' + norm[len('windows'):]
                info = getattr(self, ""_encodings"", {}).get(CodePageManager._normalize_encoding(alt))
                if info is not None:
                    return info
            return None
        elif len(args) == 1:
            # Called as CodePageManager.get_encoding_name(encoding): return normalized name
            encoding = args[0]
            return CodePageManager._normalize_encoding(encoding)
        else:
            raise TypeError(""get_encoding_name expected 1 or 2 arguments"")"
141706,mar10/wsgidav,mar10_wsgidav/wsgidav/rw_lock.py,wsgidav.rw_lock.ReadWriteLock,"from threading import Condition, Lock, current_thread
from time import time

class ReadWriteLock:
    """"""Read-Write lock class. A read-write lock differs from a standard
    threading.RLock() by allowing multiple threads to simultaneously hold a
    read lock, while allowing only a single thread to hold a write lock at the
    same point of time.

    When a read lock is requested while a write lock is held, the reader
    is blocked; when a write lock is requested while another write lock is
    held or there are read locks, the writer is blocked.

    Writers are always preferred by this implementation: if there are blocked
    threads waiting for a write lock, current readers may request more read
    locks (which they eventually should free, as they starve the waiting
    writers otherwise), but a new thread requesting a read lock will not
    be granted one, and block. This might mean starvation for readers if
    two writer threads interweave their calls to acquire_write() without
    leaving a window only for readers.

    In case a current reader requests a write lock, this can and will be
    satisfied without giving up the read locks first, but, only one thread
    may perform this kind of lock upgrade, as a deadlock would otherwise
    occur. After the write lock has been granted, the thread will hold a
    full write lock, and not be downgraded after the upgrading call to
    acquire_write() has been match by a corresponding release().
    """"""

    def __init__(self):
        """"""Initialize this read-write lock.""""""
        self.__condition = Condition(Lock())
        self.__writer = None
        self.__upgradewritercount = 0
        self.__pendingwriters = []
        self.__readers = {}

    def acquire_read(self, *, timeout=None):
        """"""Acquire a read lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.

        In case timeout is None, the call to acquire_read blocks until the
        lock request can be serviced.

        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.""""""
        if timeout is not None:
            endtime = time() + timeout
        me = current_thread()
        self.__condition.acquire()
        try:
            if self.__writer is me:
                self.__writercount += 1
                return
            while True:
                if self.__writer is None:
                    if self.__upgradewritercount or self.__pendingwriters:
                        if me in self.__readers:
                            self.__readers[me] += 1
                            return
                    else:
                        self.__readers[me] = self.__readers.get(me, 0) + 1
                        return
                if timeout is not None:
                    remaining = endtime - time()
                    if remaining <= 0:
                        raise RuntimeError('Acquiring read lock timed out')
                    self.__condition.wait(remaining)
                else:
                    self.__condition.wait()
        finally:
            self.__condition.release()

    def acquire_write(self, *, timeout=None):
        """"""Acquire a write lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.

        In case the write lock cannot be serviced due to the deadlock
        condition mentioned above, a ValueError is raised.

        In case timeout is None, the call to acquire_write blocks until the
        lock request can be serviced.

        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.""""""
        if timeout is not None:
            endtime = time() + timeout
        me, upgradewriter = (current_thread(), False)
        self.__condition.acquire()
        try:
            if self.__writer is me:
                self.__writercount += 1
                return
            elif me in self.__readers:
                if self.__upgradewritercount:
                    raise ValueError('Inevitable dead lock, denying write lock')
                upgradewriter = True
                self.__upgradewritercount = self.__readers.pop(me)
            else:
                self.__pendingwriters.append(me)
            while True:
                if not self.__readers and self.__writer is None:
                    if self.__upgradewritercount:
                        if upgradewriter:
                            self.__writer = me
                            self.__writercount = self.__upgradewritercount + 1
                            self.__upgradewritercount = 0
                            return
                    elif self.__pendingwriters[0] is me:
                        self.__writer = me
                        self.__writercount = 1
                        self.__pendingwriters = self.__pendingwriters[1:]
                        return
                if timeout is not None:
                    remaining = endtime - time()
                    if remaining <= 0:
                        if upgradewriter:
                            self.__readers[me] = self.__upgradewritercount
                            self.__upgradewritercount = 0
                        else:
                            self.__pendingwriters.remove(me)
                        raise RuntimeError('Acquiring write lock timed out')
                    self.__condition.wait(remaining)
                else:
                    self.__condition.wait()
        finally:
            self.__condition.release()

    def release(self):
        """"""Release the currently held lock.

        In case the current thread holds no lock, a ValueError is thrown.""""""
        me = current_thread()
        self.__condition.acquire()
        try:
            if self.__writer is me:
                self.__writercount -= 1
                if not self.__writercount:
                    self.__writer = None
                    self.__condition.notify_all()
            elif me in self.__readers:
                self.__readers[me] -= 1
                if not self.__readers[me]:
                    del self.__readers[me]
                    if not self.__readers:
                        self.__condition.notify_all()
            else:
                raise ValueError('Trying to release unheld lock')
        finally:
            self.__condition.release()","
class ReadWriteLock:
    '''Read-Write lock class. A read-write lock differs from a standard
    threading.RLock() by allowing multiple threads to simultaneously hold a
    read lock, while allowing only a single thread to hold a write lock at the
    same point of time.
    When a read lock is requested while a write lock is held, the reader
    is blocked; when a write lock is requested while another write lock is
    held or there are read locks, the writer is blocked.
    Writers are always preferred by this implementation: if there are blocked
    threads waiting for a write lock, current readers may request more read
    locks (which they eventually should free, as they starve the waiting
    writers otherwise), but a new thread requesting a read lock will not
    be granted one, and block. This might mean starvation for readers if
    two writer threads interweave their calls to acquire_write() without
    leaving a window only for readers.
    In case a current reader requests a write lock, this can and will be
    satisfied without giving up the read locks first, but, only one thread
    may perform this kind of lock upgrade, as a deadlock would otherwise
    occur. After the write lock has been granted, the thread will hold a
    full write lock, and not be downgraded after the upgrading call to
    acquire_write() has been match by a corresponding release().
    '''
    def __init__(self):
        '''Initialize this read-write lock.'''
        pass
    def acquire_read(self, *, timeout=None):
        '''Acquire a read lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.
        In case timeout is None, the call to acquire_read blocks until the
        lock request can be serviced.
        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.'''
        pass
    def acquire_write(self, *, timeout=None):
        '''Acquire a write lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.
        In case the write lock cannot be serviced due to the deadlock
        condition mentioned above, a ValueError is raised.
        In case timeout is None, the call to acquire_write blocks until the
        lock request can be serviced.
        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.'''
        pass
    def release(self):
        '''Release the currently held lock.
        In case the current thread holds no lock, a ValueError is thrown.'''
        pass",snippet_62,5,"import threading
import time

class ReadWriteLock:
    '''Read-Write lock class. A read-write lock differs from a standard
    threading.RLock() by allowing multiple threads to simultaneously hold a
    read lock, while allowing only a single thread to hold a write lock at the
    same point of time.
    When a read lock is requested while a write lock is held, the reader
    is blocked; when a write lock is requested while another write lock is
    held or there are read locks, the writer is blocked.
    Writers are always preferred by this implementation: if there are blocked
    threads waiting for a write lock, current readers may request more read
    locks (which they eventually should free, as they starve the waiting
    writers otherwise), but a new thread requesting a read lock will not
    be granted one, and block. This might mean starvation for readers if
    two writer threads interweave their calls to acquire_write() without
    leaving a window only for readers.
    In case a current reader requests a write lock, this can and will be
    satisfied without giving up the read locks first, but, only one thread
    may perform this kind of lock upgrade, as a deadlock would otherwise
    occur. After the write lock has been granted, the thread will hold a
    full write lock, and not be downgraded after the upgrading call to
    acquire_write() has been match by a corresponding release().
    '''
    def __init__(self):
        '''Initialize this read-write lock.'''
        self._cond = threading.Condition(threading.RLock())
        self._readers = {}  # tid -> count
        self._writer = None
        self._write_count = 0
        self._waiting_writers = 0
        self._upgrader = None  # tid of thread attempting upgrade (reader->writer), at most one

    def _tid(self):
        return threading.get_ident()

    def _now(self):
        return time.monotonic()

    def _has_other_readers(self, tid):
        # any readers other than tid?
        for t, c in self._readers.items():
            if t != tid and c > 0:
                return True
        return False

    def acquire_read(self, *, timeout=None):
        '''Acquire a read lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.
        In case timeout is None, the call to acquire_read blocks until the
        lock request can be serviced.
        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.'''
        tid = self._tid()
        with self._cond:
            # Fast path: if this thread already holds the write lock, allow read reentrantly
            if self._writer == tid:
                self._readers[tid] = self._readers.get(tid, 0) + 1
                return

            deadline = None
            if timeout is not None:
                if timeout <= 0:
                    deadline = self._now()  # immediate check
                else:
                    deadline = self._now() + timeout

            while True:
                writer_active = self._writer is not None
                already_reader = self._readers.get(tid, 0) > 0
                writers_waiting = self._waiting_writers > 0
                upgrader_present = self._upgrader is not None and self._upgrader != tid

                can_read = False
                if not writer_active:
                    if already_reader:
                        # Current readers may acquire more reads even if writers are waiting.
                        # But block if a different upgrader is in progress to avoid starvation.
                        can_read = not upgrader_present
                    else:
                        # New readers are blocked if writers are waiting or an upgrader exists.
                        can_read = (not writers_waiting) and (self._upgrader is None)

                if can_read:
                    self._readers[tid] = self._readers.get(tid, 0) + 1
                    return

                if deadline is not None:
                    remaining = deadline - self._now()
                    if remaining <= 0:
                        raise RuntimeError(""Timeout while acquiring read lock"")
                    self._cond.wait(remaining)
                else:
                    self._cond.wait()

    def acquire_write(self, *, timeout=None):
        '''Acquire a write lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.
        In case the write lock cannot be serviced due to the deadlock
        condition mentioned above, a ValueError is raised.
        In case timeout is None, the call to acquire_write blocks until the
        lock request can be serviced.
        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.'''
        tid = self._tid()
        with self._cond:
            # Re-entrant write
            if self._writer == tid:
                self._write_count += 1
                return

            is_reader = self._readers.get(tid, 0) > 0
            # Deadlock prevention: only one upgrading reader allowed
            if is_reader and (self._upgrader is not None) and (self._upgrader != tid):
                raise ValueError(""Another reader is already upgrading to a write lock"")

            deadline = None
            if timeout is not None:
                if timeout <= 0:
                    deadline = self._now()
                else:
                    deadline = self._now() + timeout

            self._waiting_writers += 1
            try:
                if is_reader:
                    # mark as upgrader
                    if self._upgrader is None:
                        self._upgrader = tid
                    elif self._upgrader != tid:
                        # Shouldn't happen due to earlier check, but be safe
                        raise ValueError(""Another reader is already upgrading to a write lock"")

                    while True:
                        no_active_writer = (self._writer is None)
                        no_other_readers = not self._has_other_readers(tid)
                        if no_active_writer and no_other_readers:
                            # acquire write
                            self._writer = tid
                            self._write_count = 1
                            # upgrade complete
                            self._upgrader = None
                            return

                        if deadline is not None:
                            remaining = deadline - self._now()
                            if remaining <= 0:
                                raise RuntimeError(""Timeout while upgrading to write lock"")
                            self._cond.wait(remaining)
                        else:
                            self._cond.wait()
                else:
                    # pure writer (not a reader)
                    while True:
                        no_active_writer = (self._writer is None)
                        no_readers = (len(self._readers) == 0)
                        if no_active_writer and no_readers:
                            self._writer = tid
                            self._write_count = 1
                            return

                        if deadline is not None:
                            remaining = deadline - self._now()
                            if remaining <= 0:
                                raise RuntimeError(""Timeout while acquiring write lock"")
                            self._cond.wait(remaining)
                        else:
                            self._cond.wait()
            finally:
                # waiting writers count reduced only when successfully acquired, or on exception
                # If we successfully acquired, we'll decrement here? We need to decrement in both success and failure.
                # If we returned successfully earlier, we wouldn't reach here; thus adjust:
                if self._writer != tid:
                    # didn't acquire
                    self._waiting_writers -= 1
                else:
                    # acquired; decrement the waiting counter as well
                    self._waiting_writers -= 1

    def release(self):
        '''Release the currently held lock.
        In case the current thread holds no lock, a ValueError is thrown.'''
        tid = self._tid()
        with self._cond:
            if self._writer == tid and self._write_count > 0:
                self._write_count -= 1
                if self._write_count == 0:
                    self._writer = None
                    # Wake up everyone: writers and readers
                    self._cond.notify_all()
                return

            rc = self._readers.get(tid, 0)
            if rc > 0:
                if rc == 1:
                    del self._readers[tid]
                else:
                    self._readers[tid] = rc - 1
                # If no readers remain, wake waiting writers
                if not self._readers:
                    self._cond.notify_all()
                return

            raise ValueError(""Current thread holds no lock to release"")"
141733,mar10/wsgidav,mar10_wsgidav/wsgidav/stream_tools.py,wsgidav.stream_tools.StreamingFile,"class StreamingFile:
    """"""A file object wrapped around an iterator / data stream.""""""

    def __init__(self, data_stream):
        """"""Initialise the object with the data stream.""""""
        self.data_stream = data_stream
        self.buffer = ''

    def read(self, size=None):
        """"""Read bytes from an iterator.""""""
        while size is None or len(self.buffer) < size:
            try:
                self.buffer += next(self.data_stream)
            except StopIteration:
                break
        sized_chunk = self.buffer[:size]
        if size is None:
            self.buffer = ''
        else:
            self.buffer = self.buffer[size:]
        return sized_chunk","class StreamingFile:
    '''A file object wrapped around an iterator / data stream.'''
    def __init__(self, data_stream):
        '''Initialise the object with the data stream.'''
        pass
    def read(self, size=None):
        
        pass",snippet_63,3,"class StreamingFile:
    '''A file object wrapped around an iterator / data stream.'''
    def __init__(self, data_stream):
        '''Initialise the object with the data stream.'''
        self._iter = iter(data_stream)
        self._buffer = None
        self._eof = False

    def _empty_like(self):
        if self._buffer is not None:
            return self._buffer[:0]
        return ''

    def read(self, size=None):
        if size is None or (isinstance(size, int) and size < 0):
            if self._eof:
                return self._empty_like()
            chunks = []
            if self._buffer is not None:
                chunks.append(self._buffer)
            try:
                for chunk in self._iter:
                    chunks.append(chunk)
            finally:
                self._eof = True
            if not chunks:
                # unknown type and empty
                return ''
            first = chunks[0]
            if isinstance(first, (bytes, bytearray)):
                out = b''.join(chunks)
                self._buffer = first[:0]
                return out
            else:
                out = ''.join(chunks)
                self._buffer = first[:0]
                return out

        if size == 0:
            return self._empty_like()

        if self._eof and (self._buffer is None or len(self._buffer) == 0):
            return self._empty_like()

        # Ensure buffer has at least 'size' bytes/chars, or stream exhausted
        current_len = 0 if self._buffer is None else len(self._buffer)
        while (size is None or current_len < size) and not self._eof:
            try:
                chunk = next(self._iter)
                if self._buffer is None:
                    self._buffer = chunk
                else:
                    self._buffer += chunk
                current_len = len(self._buffer)
            except StopIteration:
                self._eof = True
                break

        if self._buffer is None or len(self._buffer) == 0:
            return self._empty_like()

        # Slice out the requested amount
        if size is None:
            result = self._buffer
            self._buffer = self._buffer[:0]
            return result

        result = self._buffer[:size]
        self._buffer = self._buffer[size:]
        return result"
142723,PyThaiNLP/pythainlp,PyThaiNLP_pythainlp/pythainlp/augment/lm/fasttext.py,pythainlp.augment.lm.fasttext.FastTextAug,"from gensim.models.fasttext import FastText as FastText_gensim
from typing import List, Tuple
import itertools
from gensim.models.keyedvectors import KeyedVectors
from pythainlp.tokenize import word_tokenize

class FastTextAug:
    """"""
    Text Augment from fastText

    :param str model_path: path of model file
    """"""

    def __init__(self, model_path: str):
        """"""
        :param str model_path: path of model file
        """"""
        if model_path.endswith('.bin'):
            self.model = FastText_gensim.load_facebook_vectors(model_path)
        elif model_path.endswith('.vec'):
            self.model = KeyedVectors.load_word2vec_format(model_path)
        else:
            self.model = FastText_gensim.load(model_path)
        self.dict_wv = list(self.model.key_to_index.keys())

    def tokenize(self, text: str) -> List[str]:
        """"""
        Thai text tokenization for fastText

        :param str text: Thai text

        :return: list of words
        :rtype: List[str]
        """"""
        return word_tokenize(text, engine='icu')

    def modify_sent(self, sent: str, p: float=0.7) -> List[List[str]]:
        """"""
        :param str sent: text of sentence
        :param float p: probability
        :rtype: List[List[str]]
        """"""
        list_sent_new = []
        for i in sent:
            if i in self.dict_wv:
                w = [j for j, v in self.model.most_similar(i) if v >= p]
                if w == []:
                    list_sent_new.append([i])
                else:
                    list_sent_new.append(w)
            else:
                list_sent_new.append([i])
        return list_sent_new

    def augment(self, sentence: str, n_sent: int=1, p: float=0.7) -> List[Tuple[str]]:
        """"""
        Text Augment from fastText

        You may want to download the Thai model
        from https://fasttext.cc/docs/en/crawl-vectors.html.

        :param str sentence: Thai sentence
        :param int n_sent: number of sentences
        :param float p: probability of word

        :return: list of synonyms
        :rtype: List[Tuple[str]]
        """"""
        self.sentence = self.tokenize(sentence)
        self.list_synonym = self.modify_sent(self.sentence, p=p)
        new_sentences = []
        for x in list(itertools.product(*self.list_synonym))[0:n_sent]:
            new_sentences.append(x)
        return new_sentences","
class FastTextAug:
    
    def __init__(self, model_path: str):
        
        pass
    def tokenize(self, text: str) -> List[str]:
        '''
        Thai text tokenization for fastText
        :param str text: Thai text
        :return: list of words
        :rtype: List[str]
        '''
        pass
    def modify_sent(self, sent: str, p: float=0.7) -> List[List[str]]:
        
        pass
    def augment(self, sentence: str, n_sent: int=1, p: float=0.7) -> List[Tuple[str]]:
        
        pass",snippet_64,5,"from typing import List, Tuple, Optional
import random
import warnings

class FastTextAug:
    
    def __init__(self, model_path: str):
        self.model = None
        self._use_gensim = False
        self._thai_tokenizer = None
        # Load tokenizer if available
        try:
            from pythainlp.tokenize import word_tokenize  # type: ignore
            self._thai_tokenizer = word_tokenize
        except Exception:
            self._thai_tokenizer = None
        # Load fastText model via gensim if available
        if model_path:
            try:
                from gensim.models import KeyedVectors  # type: ignore
                self._use_gensim = True
                # Try native load
                try:
                    self.model = KeyedVectors.load(model_path, mmap='r')
                except Exception:
                    # Try word2vec format
                    binary = model_path.lower().endswith('.bin')
                    self.model = KeyedVectors.load_word2vec_format(model_path, binary=binary)
            except Exception as e:
                warnings.warn(f""Could not load model at '{model_path}': {e}. Augmentation will be limited."")
                self.model = None
                self._use_gensim = False
        
        # Determine vocabulary accessor if model is loaded
        if self.model is not None and self._use_gensim:
            # Gensim 4.x uses key_to_index
            if hasattr(self.model, 'key_to_index'):
                self._in_vocab = lambda w: w in self.model.key_to_index
            else:
                self._in_vocab = lambda w: w in self.model.vocab
        else:
            self._in_vocab = lambda w: False

    def tokenize(self, text: str) -> List[str]:
        '''
        Thai text tokenization for fastText
        :param str text: Thai text
        :return: list of words
        :rtype: List[str]
        '''
        if not text:
            return []
        # Prefer Thai tokenizer if available
        if self._thai_tokenizer is not None:
            try:
                tokens = self._thai_tokenizer(text, engine='newmm')
                # Filter out pure whitespace tokens
                return [t for t in tokens if t and not t.isspace()]
            except Exception:
                pass
        # Fallback: if there are spaces, split on whitespace; else character-level
        if any(ch.isspace() for ch in text):
            toks = text.split()
            return [t for t in toks if t]
        # No whitespace: return characters as tokens (crude fallback for Thai)
        return [ch for ch in text if not ch.isspace()]

    def _get_similar(self, token: str, topn: int = 10) -> List[str]:
        if self.model is None or not self._use_gensim:
            return []
        if not self._in_vocab(token):
            return []
        try:
            sims = self.model.most_similar(token, topn=topn)
            # sims is list of tuples (word, score)
            candidates = [w for w, _ in sims if w != token]
            return candidates
        except Exception:
            return []

    def modify_sent(self, sent: str, p: float = 0.7) -> List[List[str]]:
        tokens = self.tokenize(sent)
        if not tokens:
            return []
        variants: List[List[str]] = []
        # Option 1: create variants by replacing each eligible token independently
        for idx, tok in enumerate(tokens):
            if random.random() <= max(0.0, min(1.0, p)):
                cands = self._get_similar(tok, topn=10)
                if cands:
                    rep = random.choice(cands)
                    new_tokens = list(tokens)
                    new_tokens[idx] = rep
                    variants.append(new_tokens)
        # If no single-replacement variants and we have a model, try one random multi replacement attempt
        if not variants and self.model is not None:
            new_tokens = list(tokens)
            changed = False
            for idx, tok in enumerate(tokens):
                if random.random() <= max(0.0, min(1.0, p)):
                    cands = self._get_similar(tok, topn=10)
                    if cands:
                        new_tokens[idx] = random.choice(cands)
                        changed = True
            if changed:
                variants.append(new_tokens)
        # Fallback: keep original tokens if nothing changed
        if not variants:
            variants.append(tokens)
        return variants

    def augment(self, sentence: str, n_sent: int = 1, p: float = 0.7) -> List[Tuple[str]]:
        if n_sent <= 0:
            return []
        variants = self.modify_sent(sentence, p=p)
        if not variants:
            variants = [self.tokenize(sentence)]
        # Determine join rule: if original sentence contains spaces, join with space; else join without
        join_with_space = any(ch.isspace() for ch in sentence.strip())
        def detok(toks: List[str]) -> str:
            if join_with_space:
                return "" "".join(toks)
            return """".join(toks)
        # Prepare outputs
        outputs: List[Tuple[str]] = []
        # Shuffle to get diversity
        random.shuffle(variants)
        # If we have fewer variants than requested, sample with replacement
        if len(variants) >= n_sent:
            chosen = variants[:n_sent]
        else:
            chosen = variants + [random.choice(variants) for _ in range(n_sent - len(variants))]
        for toks in chosen:
            outputs.append((detok(toks),))
        return outputs"
142777,PyThaiNLP/pythainlp,PyThaiNLP_pythainlp/pythainlp/tokenize/core.py,pythainlp.tokenize.core.Tokenizer,"from typing import Iterable, List, Union
from pythainlp.tokenize import DEFAULT_SENT_TOKENIZE_ENGINE, DEFAULT_SUBWORD_TOKENIZE_ENGINE, DEFAULT_SYLLABLE_DICT_TRIE, DEFAULT_SYLLABLE_TOKENIZE_ENGINE, DEFAULT_WORD_DICT_TRIE, DEFAULT_WORD_TOKENIZE_ENGINE
from pythainlp.util.trie import Trie, dict_trie

class Tokenizer:
    """"""
    Tokenizer class for a custom tokenizer.

    This class allows users to pre-define custom dictionary along with
    tokenizer and encapsulate them into one single object.
    It is an wrapper for both functions, that are
    :func:`pythainlp.tokenize.word_tokenize`,
    and :func:`pythainlp.util.dict_trie`

    :Example:

    Tokenizer object instantiated with :class:`pythainlp.util.Trie`::

        from pythainlp.tokenize import Tokenizer
        from pythainlp.corpus.common import thai_words
        from pythainlp.util import dict_trie

        custom_words_list = set(thai_words())
        custom_words_list.add('')
        custom_words_list.add('Aphasia')
        trie = dict_trie(dict_source=custom_words_list)

        text = "" (Aphasia*) ""
        _tokenizer = Tokenizer(custom_dict=trie, engine='newmm')
        _tokenizer.word_tokenize(text)
        # output: ['', ' ', '(', 'Aphasia', ')', ' ', '', '',
        '', '', '', '']

    Tokenizer object instantiated with a list of words::

        text = "" (Aphasia) ""
        _tokenizer = Tokenizer(custom_dict=list(thai_words()), engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['', '', ' ', '(', 'Aphasia', ')', ' ', '', '',
        #   '', '', '', '']

    Tokenizer object instantiated with a file path containing a list of
    words separated with *newline* and explicitly setting a new tokenizer
    after initiation::

        PATH_TO_CUSTOM_DICTIONARY = './custom_dictionary.txtt'

        # write a file
        with open(PATH_TO_CUSTOM_DICTIONARY, 'w', encoding='utf-8') as f:
            f.write('\\nAphasia\\n\\n')

        text = "" (Aphasia) ""

        # initiate an object from file with `attacut` as tokenizer
        _tokenizer = Tokenizer(custom_dict=PATH_TO_CUSTOM_DICTIONARY, \\
            engine='attacut')

        _tokenizer.word_tokenize(text)
        # output:
        # ['', ' ', '(', 'Aphasia', ')', ' ', '', '', '',
        #   '', '', '', '']

        # change tokenizer to `newmm`
        _tokenizer.set_tokenizer_engine(engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['', ' ', '(', 'Aphasia', ')', ' ', '', '',
        #   '', '']
    """"""

    def __init__(self, custom_dict: Union[Trie, Iterable[str], str]=[], engine: str='newmm', keep_whitespace: bool=True, join_broken_num: bool=True):
        """"""
        Initialize tokenizer object.

        :param str custom_dict: a file path, a list of vocaburaies* to be
                    used to create a trie, or an instantiated
                    :class:`pythainlp.util.Trie` object.
        :param str engine: choose between different options of tokenizer engines
                            (i.e.  *newmm*, *mm*, *longest*, *deepcut*)
        :param bool keep_whitespace: True to keep whitespace, a common mark
                                     for end of phrase in Thai
        """"""
        self.__trie_dict = Trie([])
        if custom_dict:
            self.__trie_dict = dict_trie(custom_dict)
        else:
            self.__trie_dict = DEFAULT_WORD_DICT_TRIE
        self.__engine = engine
        if self.__engine not in ['newmm', 'mm', 'longest', 'deepcut']:
            raise NotImplementedError('\n                The Tokenizer class is not support %s for custom tokenizer\n                ' % self.__engine)
        self.__keep_whitespace = keep_whitespace
        self.__join_broken_num = join_broken_num

    def word_tokenize(self, text: str) -> List[str]:
        """"""
        Main tokenization function.

        :param str text: text to be tokenized
        :return: list of words, tokenized from the text
        :rtype: list[str]
        """"""
        return word_tokenize(text, custom_dict=self.__trie_dict, engine=self.__engine, keep_whitespace=self.__keep_whitespace, join_broken_num=self.__join_broken_num)

    def set_tokenize_engine(self, engine: str) -> None:
        """"""
        Set the tokenizer's engine.

        :param str engine: choose between different options of tokenizer engines
                           (i.e. *newmm*, *mm*, *longest*, *deepcut*)
        """"""
        self.__engine = engine","
class Tokenizer:
    '''
    Tokenizer class for a custom tokenizer.
    This class allows users to pre-define custom dictionary along with
    tokenizer and encapsulate them into one single object.
    It is an wrapper for both functions, that are
    :func:`pythainlp.tokenize.word_tokenize`,
    and :func:`pythainlp.util.dict_trie`
    :Example:
    Tokenizer object instantiated with :class:`pythainlp.util.Trie`::
        from pythainlp.tokenize import Tokenizer
        from pythainlp.corpus.common import thai_words
        from pythainlp.util import dict_trie
        custom_words_list = set(thai_words())
        custom_words_list.add('')
        custom_words_list.add('Aphasia')
        trie = dict_trie(dict_source=custom_words_list)
        text = "" (Aphasia*) ""
        _tokenizer = Tokenizer(custom_dict=trie, engine='newmm')
        _tokenizer.word_tokenize(text)
        # output: ['', ' ', '(', 'Aphasia', ')', ' ', '', '',
        '', '', '', '']
    Tokenizer object instantiated with a list of words::
        text = "" (Aphasia) ""
        _tokenizer = Tokenizer(custom_dict=list(thai_words()), engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['', '', ' ', '(', 'Aphasia', ')', ' ', '', '',
        #   '', '', '', '']
    Tokenizer object instantiated with a file path containing a list of
    words separated with *newline* and explicitly setting a new tokenizer
    after initiation::
        PATH_TO_CUSTOM_DICTIONARY = './custom_dictionary.txtt'
        # write a file
        with open(PATH_TO_CUSTOM_DICTIONARY, 'w', encoding='utf-8') as f:
            f.write('\nAphasia\n\n')
        text = "" (Aphasia) ""
        # initiate an object from file with `attacut` as tokenizer
        _tokenizer = Tokenizer(custom_dict=PATH_TO_CUSTOM_DICTIONARY, \
            engine='attacut')
        _tokenizer.word_tokenize(text)
        # output:
        # ['', ' ', '(', 'Aphasia', ')', ' ', '', '', '',
        #   '', '', '', '']
        # change tokenizer to `newmm`
        _tokenizer.set_tokenizer_engine(engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['', ' ', '(', 'Aphasia', ')', ' ', '', '',
        #   '', '']
    '''
    def __init__(self, custom_dict: Union[Trie, Iterable[str], str]=[], engine: str='newmm', keep_whitespace: bool=True, join_broken_num: bool=True):
        '''
        Initialize tokenizer object.
        :param str custom_dict: a file path, a list of vocaburaies* to be
                    used to create a trie, or an instantiated
                    :class:`pythainlp.util.Trie` object.
        :param str engine: choose between different options of tokenizer engines
                            (i.e.  *newmm*, *mm*, *longest*, *deepcut*)
        :param bool keep_whitespace: True to keep whitespace, a common mark
                                     for end of phrase in Thai
        '''
        pass
    def word_tokenize(self, text: str) -> List[str]:
        
        pass
    def set_tokenize_engine(self, engine: str) -> None:
        '''
        Set the tokenizer's engine.
        :param str engine: choose between different options of tokenizer engines
                           (i.e. *newmm*, *mm*, *longest*, *deepcut*)
        '''
        pass",snippet_65,4,"from typing import Iterable, List, Optional, Union
from pythainlp.tokenize import word_tokenize as th_word_tokenize
from pythainlp.util import Trie, dict_trie


class Tokenizer:
    '''
    Tokenizer class for a custom tokenizer.
    This class allows users to pre-define custom dictionary along with
    tokenizer and encapsulate them into one single object.
    It is an wrapper for both functions, that are
    :func:`pythainlp.tokenize.word_tokenize`,
    and :func:`pythainlp.util.dict_trie`
    :Example:
    Tokenizer object instantiated with :class:`pythainlp.util.Trie`::
        from pythainlp.tokenize import Tokenizer
        from pythainlp.corpus.common import thai_words
        from pythainlp.util import dict_trie
        custom_words_list = set(thai_words())
        custom_words_list.add('')
        custom_words_list.add('Aphasia')
        trie = dict_trie(dict_source=custom_words_list)
        text = "" (Aphasia*) ""
        _tokenizer = Tokenizer(custom_dict=trie, engine='newmm')
        _tokenizer.word_tokenize(text)
        # output: ['', ' ', '(', 'Aphasia', ')', ' ', '', '',
        '', '', '', '']
    Tokenizer object instantiated with a list of words::
        text = "" (Aphasia) ""
        _tokenizer = Tokenizer(custom_dict=list(thai_words()), engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['', '', ' ', '(', 'Aphasia', ')', ' ', '', '',
        #   '', '', '', '']
    Tokenizer object instantiated with a file path containing a list of
    words separated with *newline* and explicitly setting a new tokenizer
    after initiation::
        PATH_TO_CUSTOM_DICTIONARY = './custom_dictionary.txtt'
        # write a file
        with open(PATH_TO_CUSTOM_DICTIONARY, 'w', encoding='utf-8') as f:
            f.write('\nAphasia\n\n')
        text = "" (Aphasia) ""
        # initiate an object from file with `attacut` as tokenizer
        _tokenizer = Tokenizer(custom_dict=PATH_TO_CUSTOM_DICTIONARY, \
            engine='attacut')
        _tokenizer.word_tokenize(text)
        # output:
        # ['', ' ', '(', 'Aphasia', ')', ' ', '', '', '',
        #   '', '', '', '']
        # change tokenizer to `newmm`
        _tokenizer.set_tokenizer_engine(engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['', ' ', '(', 'Aphasia', ')', ' ', '', '',
        #   '', '']
    '''
    def __init__(
        self,
        custom_dict: Union[Trie, Iterable[str], str] = [],
        engine: str = 'newmm',
        keep_whitespace: bool = True,
        join_broken_num: bool = True
    ):
        '''
        Initialize tokenizer object.
        :param str custom_dict: a file path, a list of vocaburaies* to be
                    used to create a trie, or an instantiated
                    :class:`pythainlp.util.Trie` object.
        :param str engine: choose between different options of tokenizer engines
                            (i.e.  *newmm*, *mm*, *longest*, *deepcut*)
        :param bool keep_whitespace: True to keep whitespace, a common mark
                                     for end of phrase in Thai
        '''
        self.engine: str = engine
        self.keep_whitespace: bool = keep_whitespace
        self.join_broken_num: bool = join_broken_num

        self.custom_dict: Optional[Trie] = None
        if isinstance(custom_dict, Trie):
            self.custom_dict = custom_dict
        elif isinstance(custom_dict, str):
            words: List[str] = []
            with open(custom_dict, 'r', encoding='utf-8') as f:
                for line in f:
                    w = line.strip()
                    if w:
                        words.append(w)
            self.custom_dict = dict_trie(dict_source=words) if words else None
        else:
            try:
                iter(custom_dict)  # type: ignore
                words = [w for w in custom_dict if isinstance(w, str)]  # type: ignore
                self.custom_dict = dict_trie(dict_source=words) if words else None
            except TypeError:
                self.custom_dict = None

    def word_tokenize(self, text: str) -> List[str]:
        return th_word_tokenize(
            text,
            engine=self.engine,
            keep_whitespace=self.keep_whitespace,
            custom_dict=self.custom_dict,
            join_broken_num=self.join_broken_num
        )

    def set_tokenize_engine(self, engine: str) -> None:
        '''
        Set the tokenizer's engine.
        :param str engine: choose between different options of tokenizer engines
                           (i.e. *newmm*, *mm*, *longest*, *deepcut*)
        '''
        self.engine = engine

    # Backward-compatible alias
    def set_tokenizer_engine(self, engine: str) -> None:
        self.set_tokenize_engine(engine)"
145446,TomasTomecek/sen,TomasTomecek_sen/sen/tui/commands/base.py,sen.tui.commands.base.ArgumentProcessor,"class ArgumentProcessor:
    """"""
    responsible for parsing given list of arguments
    """"""

    def __init__(self, options, arguments):
        """"""
        :param options: list of options
        :param arguments: list of arguments
        """"""
        self.given_arguments = {}
        self.options = {}
        for a in options:
            self.options[a.name] = a
            self.given_arguments[normalize_arg_name(a.name)] = a.default
            for alias in a.aliases:
                self.options[alias] = a
        for o in arguments:
            self.given_arguments[normalize_arg_name(o.name)] = o.default
        self.arguments = arguments
        logger.info('arguments = %s', arguments)
        logger.info('options = %s', options)

    def process(self, argument_list):
        """"""
        :param argument_list: list of str, input from user
        :return: dict:
            {""cleaned_arg_name"": ""value""}
        """"""
        arg_index = 0
        for a in argument_list:
            opt_and_val = a.split('=', 1)
            opt_name = opt_and_val[0]
            try:
                argument = self.options[opt_name]
            except KeyError:
                try:
                    argument = self.arguments[arg_index]
                except IndexError:
                    logger.error('option/argument %r not specified', a)
                    raise NoSuchOptionOrArgument('No such option or argument: %r' % opt_name)
            logger.info('argument found: %s', argument)
            safe_arg_name = normalize_arg_name(argument.name)
            logger.info('argument is available under name %r', safe_arg_name)
            if isinstance(argument, Argument):
                arg_index += 1
                value = (a,)
            else:
                try:
                    value = (opt_and_val[1],)
                except IndexError:
                    value = tuple()
            arg_val = argument.action(*value)
            logger.info('argument %r has value %r', safe_arg_name, arg_val)
            self.given_arguments[safe_arg_name] = arg_val
        return self.given_arguments","class ArgumentProcessor:
    
    def __init__(self, options, arguments):
        
        pass
    def process(self, argument_list):
        '''
        :param argument_list: list of str, input from user
        :return: dict:
            {""cleaned_arg_name"": ""value""}
        '''
        pass",snippet_66,3,"class ArgumentProcessor:
    def __init__(self, options, arguments):
        """"""
        options: dict or list
          - If dict: {
                ""name"": {
                    ""aliases"": [""-n"", ""--name""],
                    ""type"": callable (default str),
                    ""default"": any,
                    ""required"": bool (default False),
                    ""action"": ""store"", ""store_true"", ""store_false"", ""count"", ""append"",
                    ""choices"": iterable
                },
                ...
            }
          - If list: [""flag1"", ""flag2""] -> treated as boolean flags (store_true)
        arguments: list defining positional arguments
          Accepts:
            - [""arg1"", ""arg2""]
            - [(""arg1"", default), (""arg2"", default)]
            - [{""name"": ""arg"", ""default"": ..., ""required"": True, ""multiple"": True}, ...]
        """"""
        self.options_raw = options
        self.arguments_raw = arguments

        self.options = {}
        if isinstance(options, dict):
            for name, conf in options.items():
                self.options[self._clean_name(name)] = self._normalize_option_conf(name, conf or {})
        elif isinstance(options, (list, tuple)):
            for name in options:
                cname = self._clean_name(name)
                self.options[cname] = self._normalize_option_conf(
                    cname,
                    {""aliases"": [f""--{cname.replace('_','-')}""], ""action"": ""store_true""},
                )
        else:
            raise TypeError(""options must be dict or list"")

        self.flag_to_name = {}
        for cname, conf in self.options.items():
            for flag in conf[""aliases""]:
                if not isinstance(flag, str) or not flag.startswith(""-""):
                    raise ValueError(f""Invalid alias '{flag}' for option '{cname}'"")
                if flag in self.flag_to_name and self.flag_to_name[flag] != cname:
                    raise ValueError(f""Alias '{flag}' is duplicated for different options"")
                self.flag_to_name[flag] = cname

        self.positionals = []
        self.positional_index = []
        if not isinstance(arguments, (list, tuple)):
            raise TypeError(""arguments must be list/tuple"")
        for idx, spec in enumerate(arguments):
            if isinstance(spec, str):
                conf = {""name"": self._clean_name(spec), ""default"": None, ""required"": True, ""multiple"": False}
            elif isinstance(spec, tuple) and len(spec) == 2:
                conf = {""name"": self._clean_name(spec[0]), ""default"": spec[1], ""required"": spec[1] is None, ""multiple"": False}
            elif isinstance(spec, dict) and ""name"" in spec:
                conf = {
                    ""name"": self._clean_name(spec[""name""]),
                    ""default"": spec.get(""default"", None),
                    ""required"": bool(spec.get(""required"", spec.get(""default"", None) is None and not spec.get(""multiple"", False))),
                    ""multiple"": bool(spec.get(""multiple"", False)),
                    ""type"": spec.get(""type"", str),
                    ""choices"": spec.get(""choices""),
                }
            else:
                raise ValueError(f""Invalid positional spec at index {idx}: {spec!r}"")
            if conf.get(""multiple"", False) and idx != len(arguments) - 1:
                raise ValueError(""Only the last positional argument may have multiple=True"")
            self.positionals.append(conf)
            self.positional_index.append(conf[""name""])

    def process(self, argument_list):
        '''
        :param argument_list: list of str, input from user
        :return: dict:
            {""cleaned_arg_name"": ""value""}
        '''
        if not isinstance(argument_list, (list, tuple)):
            raise TypeError(""argument_list must be list/tuple of strings"")
        tokens = list(argument_list)
        result = {}

        # Initialize with defaults for options
        for name, conf in self.options.items():
            if conf[""action""] == ""store_true"":
                result[name] = conf.get(""default"", False)
            elif conf[""action""] == ""store_false"":
                result[name] = conf.get(""default"", True)
            elif conf[""action""] == ""count"":
                result[name] = conf.get(""default"", 0) or 0
            elif conf[""action""] == ""append"":
                result[name] = list(conf.get(""default"", []))
            else:
                result[name] = conf.get(""default"", None)

        # Initialize positionals with defaults
        for conf in self.positionals:
            if conf.get(""multiple"", False):
                result[conf[""name""]] = list(conf.get(""default"", []))
            else:
                result[conf[""name""]] = conf.get(""default"", None)

        # Parse options and collect positional tokens
        pos_values = []
        i = 0
        while i < len(tokens):
            tok = tokens[i]
            if isinstance(tok, str) and tok.startswith(""-"") and tok != ""-"":
                # Long with = : --key=value
                if tok.startswith(""--"") and ""="" in tok:
                    flag, val = tok.split(""="", 1)
                    self._apply_option(flag, val, result)
                    i += 1
                    continue

                # Handle --no-<flag> for booleans
                if tok.startswith(""--no-""):
                    base = ""--"" + tok[5:]
                    cname = self.flag_to_name.get(base)
                    if not cname:
                        raise ValueError(f""Unknown option '{tok}'"")
                    conf = self.options[cname]
                    if conf[""action""] == ""store_true"":
                        result[cname] = False
                        i += 1
                        continue
                    elif conf[""action""] == ""store_false"":
                        result[cname] = True
                        i += 1
                        continue
                    else:
                        raise ValueError(f""Option '{base}' does not support negation form '{tok}'"")

                # Regular flag possibly expecting value
                cname = self.flag_to_name.get(tok)
                if not cname:
                    raise ValueError(f""Unknown option '{tok}'"")
                conf = self.options[cname]
                action = conf[""action""]

                if action in (""store_true"", ""store_false""):
                    result[cname] = True if action == ""store_true"" else False
                    i += 1
                    continue
                if action == ""count"":
                    result[cname] = (result.get(cname, 0) or 0) + 1
                    i += 1
                    continue

                # Needs a value
                if i + 1 >= len(tokens):
                    raise ValueError(f""Option '{tok}' requires a value"")
                value_token = tokens[i + 1]
                self._apply_option(tok, value_token, result)
                i += 2
            else:
                pos_values.append(tok)
                i += 1

        # Assign positional values
        pi = 0
        for conf in self.positionals:
            name = conf[""name""]
            multiple = conf.get(""multiple"", False)
            ptype = conf.get(""type"", str)
            choices = conf.get(""choices"")

            if multiple:
                rest = pos_values[pi:]
                converted = [self._convert_value(v, ptype, f""positional '{name}'"") for v in rest]
                if choices is not None:
                    for v in converted:
                        if v not in choices:
                            raise ValueError(f""Invalid value for positional '{name}': {v!r}. Allowed: {list(choices)!r}"")
                result[name] = converted
                pi = len(pos_values)
            else:
                if pi >= len(pos_values):
                    if conf.get(""required"", True) and result.get(name) is None:
                        raise ValueError(f""Missing required positional argument '{name}'"")
                    # keep default
                else:
                    val = self._convert_value(pos_values[pi], ptype, f""positional '{name}'"")
                    if choices is not None and val not in choices:
                        raise ValueError(f""Invalid value for positional '{name}': {val!r}. Allowed: {list(choices)!r}"")
                    result[name] = val
                    pi += 1

        if pi < len(pos_values):
            extra = pos_values[pi:]
            raise ValueError(f""Unexpected positional arguments: {extra!r}"")

        # Validate required options
        for name, conf in self.options.items():
            if conf.get(""required"") and (result.get(name) is None or (conf[""action""] == ""append"" and not result.get(name))):
                raise ValueError(f""Missing required option '--{name.replace('_','-')}'"")

        return result

    def _clean_name(self, name):
        if not isinstance(name, str):
            raise TypeError(""name must be string"")
        name = name.strip()
        if name.startswith(""-""):
            name = name.lstrip(""-"")
        return name.replace(""-"", ""_"")

    def _normalize_option_conf(self, name, conf):
        cname = self._clean_name(name)
        norm = {}
        norm[""aliases""] = list(conf.get(""aliases"") or [f""--{cname.replace('_','-')}""])
        # Ensure canonical long alias exists
        long_alias = f""--{cname.replace('_','-')}""
        if long_alias not in norm[""aliases""]:
            norm[""aliases""].append(long_alias)

        action = conf.get(""action"")
        if action not in (None, ""store"", ""store_true"", ""store_false"", ""count"", ""append""):
            raise ValueError(f""Unsupported action for option '{cname}': {action}"")
        if action is None:
            if conf.get(""type"", str) is bool or isinstance(conf.get(""default"", None), bool):
                action = ""store_true""
            else:
                action = ""store""
        norm[""action""] = action
        norm[""type""] = conf.get(""type"", (None if action in (""store_true"", ""store_false"", ""count"") else str))
        norm[""default""] = conf.get(""default"", (False if action == ""store_true"" else True if action == ""store_false"" else 0 if action == ""count"" else ([] if action == ""append"" else None)))
        norm[""required""] = bool(conf.get(""required"", False))
        norm[""choices""] = conf.get(""choices"")
        return norm

    def _convert_value(self, value, typ, ctx):
        if typ is None:
            return value
        try:
            return typ(value)
        except Exception as e:
            raise ValueError(f""Invalid value for {ctx}: {value!r} ({e})"") from None

    def _apply_option(self, flag, raw_value, result_dict):
        cname = self.flag_to_name.get(flag)
        if not cname:
            raise ValueError(f""Unknown option '{flag}'"")
        conf = self.options[cname]
        action = conf[""action""]

        if action in (""store_true"", ""store_false"", ""count""):
            raise ValueError(f""Option '{flag}' does not take a value"")

        if action == ""append"":
            val = self._convert_value(raw_value, conf.get(""type"", str), f""option '{flag}'"")
            if conf.get(""choices"") is not None and val not in conf[""choices""]:
                raise ValueError(f""Invalid value for option '{flag}': {val!r}. Allowed: {list(conf['choices'])!r}"")
            result_dict[cname].append(val)
            return

        # action == ""store""
        val = self._convert_value(raw_value, conf.get(""type"", str), f""option '{flag}'"")
        if conf.get(""choices"") is not None and val not in conf[""choices""]:
            raise ValueError(f""Invalid value for option '{flag}': {val!r}. Allowed: {list(conf['choices'])!r}"")
        result_dict[cname] = val"
152154,hellock/icrawler,hellock_icrawler/icrawler/storage/base.py,icrawler.storage.base.BaseStorage,"from abc import ABCMeta, abstractmethod

class BaseStorage:
    """"""Base class of backend storage""""""
    __metaclass__ = ABCMeta

    @abstractmethod
    def write(self, id, data):
        """"""Abstract interface of writing data

        Args:
            id (str): unique id of the data in the storage.
            data (bytes or str): data to be stored.
        """"""
        return

    @abstractmethod
    def exists(self, id):
        """"""Check the existence of some data

        Args:
            id (str): unique id of the data in the storage

        Returns:
            bool: whether the data exists
        """"""
        return False

    @abstractmethod
    def max_file_idx(self):
        """"""Get the max existing file index

        Returns:
            int: the max index
        """"""
        return 0","
class BaseStorage:
    '''Base class of backend storage'''
    @abstractmethod
    def write(self, id, data):
        
        pass
    @abstractmethod
    def exists(self, id):
        '''Check the existence of some data
        Args:
            id (str): unique id of the data in the storage
        Returns:
            bool: whether the data exists
        '''
        pass
    @abstractmethod
    def max_file_idx(self):
        
        pass",snippet_69,4,"from abc import ABC, abstractmethod
from typing import Any


class BaseStorage(ABC):
    '''Base class of backend storage'''

    @abstractmethod
    def write(self, id: str, data: Any) -> None:
        raise NotImplementedError

    @abstractmethod
    def exists(self, id: str) -> bool:
        '''Check the existence of some data
        Args:
            id (str): unique id of the data in the storage
        Returns:
            bool: whether the data exists
        '''
        raise NotImplementedError

    @abstractmethod
    def max_file_idx(self) -> int:
        raise NotImplementedError"
155190,sentinel-hub/sentinelhub-py,sentinelhub/download/rate_limit.py,sentinelhub.download.rate_limit.SentinelHubRateLimit,"import time

class SentinelHubRateLimit:
    """"""Class implementing rate limiting logic of Sentinel Hub service

    It has 2 public methods:

    - register_next - tells if next download can start or if not, what is the wait before it can be asked again
    - update - updates expectations according to headers obtained from download

    The rate limiting object is collecting information about the status of rate limiting policy buckets from
    Sentinel Hub service. According to this information and a feedback from download requests it adapts expectations
    about when the next download attempt will be possible.
    """"""
    RETRY_HEADER = 'Retry-After'
    UNITS_SPENT_HEADER = 'X-ProcessingUnits-Spent'

    def __init__(self, num_processes: int=1, minimum_wait_time: float=0.05, maximum_wait_time: float=60.0):
        """"""
        :param num_processes: Number of parallel download processes running.
        :param minimum_wait_time: Minimum wait time between two consecutive download requests in seconds.
        :param maximum_wait_time: Maximum wait time between two consecutive download requests in seconds.
        """"""
        self.wait_time = min(num_processes * minimum_wait_time, maximum_wait_time)
        self.next_download_time = time.monotonic()

    def register_next(self) -> float:
        """"""Determines if next download request can start or not by returning the waiting time in seconds.""""""
        current_time = time.monotonic()
        wait_time = max(self.next_download_time - current_time, 0)
        if wait_time == 0:
            self.next_download_time = max(current_time + self.wait_time, self.next_download_time)
        return wait_time

    def update(self, headers: dict, *, default: float) -> None:
        """"""Update the next possible download time if the service has responded with the rate limit.

        :param headers: The headers that (may) contain information about waiting times.
        :param default: The default waiting time (in milliseconds) when retrying after getting a
            TOO_MANY_REQUESTS response without appropriate retry headers.
        """"""
        retry_after: float = int(headers.get(self.RETRY_HEADER, default))
        retry_after = retry_after / 1000
        if retry_after:
            self.next_download_time = max(time.monotonic() + retry_after, self.next_download_time)","
class SentinelHubRateLimit:
    
    def __init__(self, num_processes: int=1, minimum_wait_time: float=0.05, maximum_wait_time: float=60.0):
        '''
        :param num_processes: Number of parallel download processes running.
        :param minimum_wait_time: Minimum wait time between two consecutive download requests in seconds.
        :param maximum_wait_time: Maximum wait time between two consecutive download requests in seconds.
        '''
        pass
    def register_next(self) -> float:
        
        pass
    def update(self, headers: dict, *, default: float) -> None:
        '''Update the next possible download time if the service has responded with the rate limit.
        :param headers: The headers that (may) contain information about waiting times.
        :param default: The default waiting time (in milliseconds) when retrying after getting a
            TOO_MANY_REQUESTS response without appropriate retry headers.
        '''
        pass",snippet_73,4,"import threading
import time
from typing import Optional, Dict, Any


class SentinelHubRateLimit:
    
    def __init__(self, num_processes: int = 1, minimum_wait_time: float = 0.05, maximum_wait_time: float = 60.0):
        '''
        :param num_processes: Number of parallel download processes running.
        :param minimum_wait_time: Minimum wait time between two consecutive download requests in seconds.
        :param maximum_wait_time: Maximum wait time between two consecutive download requests in seconds.
        '''
        if num_processes <= 0:
            raise ValueError(""num_processes must be >= 1"")
        if minimum_wait_time < 0:
            raise ValueError(""minimum_wait_time must be >= 0"")
        if maximum_wait_time <= 0:
            raise ValueError(""maximum_wait_time must be > 0"")
        if minimum_wait_time > maximum_wait_time:
            raise ValueError(""minimum_wait_time cannot be greater than maximum_wait_time"")

        self._num_processes = int(num_processes)
        self._min_wait = float(minimum_wait_time)
        self._max_wait = float(maximum_wait_time)

        self._lock = threading.Lock()
        self._next_time_monotonic = 0.0

    def register_next(self) -> float:
        now = time.monotonic()
        with self._lock:
            wait = max(0.0, self._next_time_monotonic - now)
            # Enforce minimum spacing between consecutive requests globally.
            step = self._min_wait
            self._next_time_monotonic = max(self._next_time_monotonic, now) + step
        return wait

    def update(self, headers: Dict[str, Any], *, default: float) -> None:
        '''Update the next possible download time if the service has responded with the rate limit.
        :param headers: The headers that (may) contain information about waiting times.
        :param default: The default waiting time (in milliseconds) when retrying after getting a
            TOO_MANY_REQUESTS response without appropriate retry headers.
        '''
        if headers is None:
            headers = {}
        lower = {str(k).lower(): v for k, v in headers.items()}
        wait_seconds: Optional[float] = None

        # 1) Retry-After header (seconds or HTTP-date). We handle numeric only.
        ra = lower.get('retry-after')
        if ra is not None:
            try:
                wait_seconds = float(ra)
            except (TypeError, ValueError):
                wait_seconds = None

        # 2) Millisecond-based retry headers
        if wait_seconds is None:
            ra_ms = lower.get('retry-after-ms') or lower.get('x-retry-after-ms')
            if ra_ms is not None:
                try:
                    wait_seconds = float(ra_ms) / 1000.0
                except (TypeError, ValueError):
                    wait_seconds = None

        # 3) Rate limit reset semantics
        if wait_seconds is None:
            remaining = lower.get('x-ratelimit-remaining') or lower.get('ratelimit-remaining')
            reset = lower.get('x-ratelimit-reset') or lower.get('ratelimit-reset')
            if reset is not None:
                now_wall = time.time()
                # Try to parse remaining
                rem_val: Optional[int] = None
                try:
                    rem_val = int(str(remaining)) if remaining is not None else None
                except (TypeError, ValueError):
                    rem_val = None

                # Parse reset: could be epoch seconds, epoch ms, or delta seconds
                reset_val: Optional[float] = None
                try:
                    r = float(str(reset))
                    # Heuristics for units
                    if r > 1e12:   # microseconds epoch
                        reset_val = r / 1e6
                    elif r > 1e11: # milliseconds epoch
                        reset_val = r / 1e3
                    elif r > 1e9:  # seconds epoch
                        reset_val = r
                    else:
                        # Treat as delta seconds
                        wait_seconds = r
                except (TypeError, ValueError):
                    reset_val = None

                if wait_seconds is None and reset_val is not None:
                    # We have an absolute reset time
                    delta = max(0.0, reset_val - now_wall)
                    # Only apply if we are out of quota or delta is meaningful
                    if rem_val is None or rem_val <= 0 or delta > 0:
                        wait_seconds = delta

        # 4) Default if still unknown (default is provided in milliseconds)
        if wait_seconds is None:
            try:
                wait_seconds = float(default) / 1000.0
            except (TypeError, ValueError):
                wait_seconds = self._min_wait

        # Clamp to configured bounds
        if self._max_wait is not None:
            wait_seconds = min(wait_seconds, self._max_wait)
        wait_seconds = max(wait_seconds, self._min_wait)

        target = time.monotonic() + wait_seconds
        with self._lock:
            if target > self._next_time_monotonic:
                self._next_time_monotonic = target"
156520,textX/textX,textx/scoping/providers.py,textx.scoping.providers.PlainName,"from textx.exceptions import TextXSemanticError

class PlainName:
    """"""
    plain name scope provider
    """"""

    def __init__(self, multi_metamodel_support=True):
        """"""
        the default scope provider constructor

        Args:
            multi_metamodel_support: enable a AST based search, instead
            of using the parser._instances
        """"""
        self.multi_metamodel_support = multi_metamodel_support
        pass

    def __call__(self, obj, attr, obj_ref):
        """"""
        the default scope provider

        Args:
            obj: unused (used for multi_metamodel_support)
            attr: unused
            obj_ref: the cross reference to be resolved

        Returns:
            the resolved reference or None
        """"""
        from textx.const import RULE_ABSTRACT, RULE_COMMON
        from textx.model import ObjCrossRef
        from textx.scoping.tools import get_parser
        if obj_ref is None:
            return None
        assert type(obj_ref) is ObjCrossRef, type(obj_ref)
        if get_parser(obj).debug:
            get_parser(obj).dprint(f'Resolving obj crossref: {obj_ref.cls}:{obj_ref.obj_name}')

        def _inner_resolve_link_rule_ref(cls, obj_name):
            """"""
            Depth-first resolving of link rule reference.
            """"""
            if cls._tx_type is RULE_ABSTRACT:
                for inherited in cls._tx_inh_by:
                    result = _inner_resolve_link_rule_ref(inherited, obj_name)
                    if result:
                        return result
            elif cls._tx_type == RULE_COMMON and id(cls) in get_parser(obj)._instances:
                objs = get_parser(obj)._instances[id(cls)]
                return objs.get(obj_name)
        if self.multi_metamodel_support:
            from textx import get_children, get_model, textx_isinstance
            result_lst = get_children(lambda x: hasattr(x, 'name') and x.name == obj_ref.obj_name and textx_isinstance(x, obj_ref.cls), get_model(obj))
            if len(result_lst) == 1:
                result = result_lst[0]
            elif len(result_lst) > 1:
                line, col = get_parser(obj).pos_to_linecol(obj_ref.position)
                raise TextXSemanticError(f'name {obj_ref.obj_name} is not unique.', line=line, col=col, filename=get_model(obj)._tx_filename)
            else:
                result = None
        else:
            result = _inner_resolve_link_rule_ref(obj_ref.cls, obj_ref.obj_name)
        return result","
class PlainName:
    
    def __init__(self, multi_metamodel_support=True):
        '''
        the default scope provider constructor
        Args:
            multi_metamodel_support: enable a AST based search, instead
            of using the parser._instances
        '''
        pass
    def __call__(self, obj, attr, obj_ref):
        '''
        the default scope provider
        Args:
            obj: unused (used for multi_metamodel_support)
            attr: unused
            obj_ref: the cross reference to be resolved
        Returns:
            the resolved reference or None
        '''
        pass
        def _inner_resolve_link_rule_ref(cls, obj_name):
            '''
            Depth-first resolving of link rule reference.
                '''
                pass",snippet_74,4,"class PlainName:
    
    def __init__(self, multi_metamodel_support=True):
        '''
        the default scope provider constructor
        Args:
            multi_metamodel_support: enable a AST based search, instead
            of using the parser._instances
        '''
        self.multi_metamodel_support = multi_metamodel_support

    def __call__(self, obj, attr, obj_ref):
        '''
        the default scope provider
        Args:
            obj: unused (used for multi_metamodel_support)
            attr: unused
            obj_ref: the cross reference to be resolved
        Returns:
            the resolved reference or None
        '''
        def extract_name(ref):
            if ref is None:
                return None
            # textX style Reference has obj_name
            n = getattr(ref, 'obj_name', None)
            if n is not None:
                return n
            # direct string reference
            if isinstance(ref, str):
                return ref
            # fallbacks
            n = getattr(ref, 'name', None)
            if isinstance(n, str):
                return n
            return None

        def get_root(node):
            cur = node
            # textX sets _tx_parent on contained objects
            while True:
                parent = getattr(cur, '_tx_parent', None)
                if parent is None:
                    break
                cur = parent
            return cur

        def is_primitive(v):
            return isinstance(v, (str, bytes, int, float, bool))

        def iter_children(node):
            if node is None:
                return
            # Iterate over attributes except private/textX meta ones
            for k, v in vars(node).items():
                if k.startswith('_tx_'):
                    continue
                if is_primitive(v) or v is None:
                    continue
                if isinstance(v, (list, tuple)):
                    for it in v:
                        if not is_primitive(it) and hasattr(it, '__dict__'):
                            yield it
                else:
                    if hasattr(v, '__dict__'):
                        yield v

        def get_name(o):
            for candidate in ('name', 'id'):
                if hasattr(o, candidate):
                    n = getattr(o, candidate)
                    if isinstance(n, str):
                        return n
            return None

        def find_by_name(root_obj, target_name):
            # Simple DFS search by name
            visited = set()
            stack = [root_obj]
            while stack:
                cur = stack.pop()
                oid = id(cur)
                if oid in visited:
                    continue
                visited.add(oid)
                n = get_name(cur)
                if n == target_name:
                    return cur
                # push children
                for ch in iter_children(cur):
                    stack.append(ch)
            return None

        def find_qualified(root_obj, parts):
            # Resolve a qualified name like A.B.C by descending containment
            # Start with all candidates matching first part anywhere, then descend.
            first = parts[0]
            visited = set()
            # Gather all with name == first
            candidates = []
            stack = [root_obj]
            while stack:
                cur = stack.pop()
                oid = id(cur)
                if oid in visited:
                    continue
                visited.add(oid)
                if get_name(cur) == first:
                    candidates.append(cur)
                for ch in iter_children(cur):
                    stack.append(ch)
            if not candidates:
                return None
            # Descend for remaining parts
            for p in parts[1:]:
                next_candidates = []
                for c in candidates:
                    for ch in iter_children(c):
                        if get_name(ch) == p:
                            next_candidates.append(ch)
                if not next_candidates:
                    return None
                candidates = next_candidates
            return candidates[0] if candidates else None

        def search_using_instances(model_like, target_name):
            # model_like may expose parser._instances as {rule: [objs]}
            try:
                parser = getattr(model_like, 'parser', None)
                if parser is None:
                    return None
                instances = getattr(parser, '_instances', None)
                if not instances:
                    return None
                # Flatten and search
                for coll in instances.values():
                    for inst in coll:
                        if get_name(inst) == target_name:
                            return inst
                return None
            except Exception:
                return None

        def _inner_resolve_link_rule_ref(cls, obj_name):
            '''
            Depth-first resolving of link rule reference.
            '''
            if not obj_name:
                return None
            parts = obj_name.split('.')
            if not parts:
                return None
            # If cls is provided as a root to start from, use it directly.
            # Otherwise, fall back to searching from the overall root.
            start = cls if cls is not None else get_root(obj)
            if len(parts) == 1:
                return find_by_name(start, parts[0])
            return find_qualified(start, parts)

        name = extract_name(obj_ref)
        if not name:
            return None

        # Resolve qualified names via inner link-rule resolver
        if '.' in name:
            return _inner_resolve_link_rule_ref(obj, name)

        # Try fast path using instances when multi_metamodel_support is False
        if not self.multi_metamodel_support:
            # Attempt to locate model's parser instances
            tx_model = getattr(obj, '_tx_model', None)
            resolved = None
            if tx_model is not None:
                resolved = search_using_instances(tx_model, name)
            if resolved is not None:
                return resolved
            # Fallback to root search
            return find_by_name(get_root(obj), name)

        # multi_metamodel_support: AST-based search from root
        root = get_root(obj)
        return find_by_name(root, name)"
156861,csparpa/pyowm,csparpa_pyowm/pyowm/utils/geo.py,pyowm.utils.geo.Geometry,"class Geometry:
    """"""
    Abstract parent class for geotypes

    """"""

    def geojson(self):
        """"""
        Returns a GeoJSON string representation of this geotype, compliant to
        RFC 7946 (https://tools.ietf.org/html/rfc7946)
        :return: str
        """"""
        raise NotImplementedError()

    def to_dict(self):
        """"""
        Returns a dict representation of this geotype
        :return: dict
        """"""
        raise NotImplementedError()","class Geometry:
    '''
    Abstract parent class for geotypes
        '''
    def geojson(self):
        
        pass
    def to_dict(self):
        '''
        Returns a dict representation of this geotype
        :return: dict
        '''
        pass",snippet_75,3,"import abc
import json
from typing import Dict, Any


class Geometry(metaclass=abc.ABCMeta):
    '''
    Abstract parent class for geotypes
    '''
    def geojson(self) -> str:
        return json.dumps(self.to_dict(), separators=(',', ':'))

    @abc.abstractmethod
    def to_dict(self) -> Dict[str, Any]:
        '''
        Returns a dict representation of this geotype
        :return: dict
        '''
        raise NotImplementedError"
157718,jwkvam/bowtie,bowtie/_cache.py,bowtie._cache._Cache,"from eventlet.queue import LightQueue
from flask_socketio import emit
import eventlet
import flask
import msgpack
from bowtie._component import pack

class _Cache:
    """"""Store data in the browser.

    This cache uses session storage so data will stay
    in the browser until the tab is closed.
    All data must be serializable, which means if the
    serialization transforms the data it won't be the same
    when it is fetched.

    Examples
    --------
    >>> from bowtie import cache
    >>> cache['a'] = True  # doctest: +SKIP
    >>> cache['a']  # doctest: +SKIP
    True
    >>> cache['b'] = np.arange(5)  # doctest: +SKIP
    >>> cache['b']  # doctest: +SKIP
    [1, 2, 3, 4, 5]

    """"""

    def __getitem__(self, key):
        """"""Load the value stored with the key.

        Parameters
        ----------
        key : str
            The key to lookup the value stored.

        Returns
        -------
        object
            The value if the key exists in the cache, otherwise None.

        """"""
        validate(key)
        signal = 'cache_load'
        event = LightQueue(1)
        if flask.has_request_context():
            emit(signal, {'data': pack(key)}, callback=event.put)
        else:
            sio = flask.current_app.extensions['socketio']
            sio.emit(signal, {'data': pack(key)}, callback=event.put)
        return msgpack.unpackb(bytes(event.get(timeout=10)), encoding='utf8')

    def __setitem__(self, key, value):
        """"""Store the key value pair.

        Parameters
        ----------
        key : str
            The key to determine where it's stored, you'll need this to load the value later.
        value : object
            The value to store in the cache.

        Returns
        -------
        None

        """"""
        validate(key)
        signal = 'cache_save'
        if flask.has_request_context():
            emit(signal, {'key': pack(key), 'data': pack(value)})
        else:
            sio = flask.current_app.extensions['socketio']
            sio.emit(signal, {'key': pack(key), 'data': pack(value)})
        eventlet.sleep()","
class _Cache:
    '''Store data in the browser.
    This cache uses session storage so data will stay
    in the browser until the tab is closed.
    All data must be serializable, which means if the
    serialization transforms the data it won't be the same
    when it is fetched.
    Examples
    --------
    >>> from bowtie import cache
    >>> cache['a'] = True  # doctest: +SKIP
    >>> cache['a']  # doctest: +SKIP
    True
    >>> cache['b'] = np.arange(5)  # doctest: +SKIP
    >>> cache['b']  # doctest: +SKIP
    [1, 2, 3, 4, 5]
    '''
    def __getitem__(self, key):
        '''Load the value stored with the key.
        Parameters
        ----------
        key : str
            The key to lookup the value stored.
        Returns
        -------
        object
            The value if the key exists in the cache, otherwise None.
        '''
        pass
    def __setitem__(self, key, value):
        
        pass",snippet_78,3,"class _Cache:
    '''Store data in the browser.
    This cache uses session storage so data will stay
    in the browser until the tab is closed.
    All data must be serializable, which means if the
    serialization transforms the data it won't be the same
    when it is fetched.
    Examples
    --------
    >>> from bowtie import cache
    >>> cache['a'] = True  # doctest: +SKIP
    >>> cache['a']  # doctest: +SKIP
    True
    >>> cache['b'] = np.arange(5)  # doctest: +SKIP
    >>> cache['b']  # doctest: +SKIP
    [1, 2, 3, 4, 5]
    '''
    _storage = {}

    def __getitem__(self, key):
        '''Load the value stored with the key.
        Parameters
        ----------
        key : str
            The key to lookup the value stored.
        Returns
        -------
        object
            The value if the key exists in the cache, otherwise None.
        '''
        import json
        if not isinstance(key, str):
            raise TypeError(""key must be a string"")
        raw = self._storage.get(key)
        if raw is None:
            return None
        try:
            return json.loads(raw)
        except Exception:
            return None

    def __setitem__(self, key, value):
        import json

        if not isinstance(key, str):
            raise TypeError(""key must be a string"")

        def _default(obj):
            try:
                import numpy as np  # noqa
                import numpy
                if isinstance(obj, numpy.generic):
                    return obj.item()
                if hasattr(obj, ""tolist""):
                    return obj.tolist()
            except Exception:
                pass
            if isinstance(obj, (set, frozenset)):
                return list(obj)
            if isinstance(obj, bytes):
                return obj.decode(""utf-8"", errors=""replace"")
            raise TypeError(f""Object of type {type(obj).__name__} is not JSON serializable"")

        serialized = json.dumps(value, default=_default)
        self._storage[key] = serialized"
159496,Erotemic/ubelt,Erotemic_ubelt/ubelt/util_mixins.py,ubelt.util_mixins.NiceRepr,"import warnings

class NiceRepr:
    """"""
    Inherit from this class and define ``__nice__`` to ""nicely"" print your
    objects.

    Defines ``__str__`` and ``__repr__`` in terms of ``__nice__`` function
    Classes that inherit from :class:`NiceRepr` should redefine ``__nice__``.
    If the inheriting class has a ``__len__``, method then the default
    ``__nice__`` method will return its length.

    Example:
        >>> import ubelt as ub
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        return 'info'
        >>> foo = Foo()
        >>> assert str(foo) == '<Foo(info)>'
        >>> assert repr(foo).startswith('<Foo(info) at ')

    Example:
        >>> import ubelt as ub
        >>> class Bar(ub.NiceRepr):
        ...    pass
        >>> bar = Bar()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     assert 'object at' in str(bar)
        >>>     assert 'object at' in repr(bar)

    Example:
        >>> import ubelt as ub
        >>> class Baz(ub.NiceRepr):
        ...    def __len__(self):
        ...        return 5
        >>> baz = Baz()
        >>> assert str(baz) == '<Baz(5)>'

    Example:
        >>> import ubelt as ub
        >>> # If your nice message has a bug, it shouldn't bring down the house
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        assert False
        >>> foo = Foo()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     print('foo = {!r}'.format(foo))
        foo = <...Foo ...>

    Example:
        >>> import ubelt as ub
        >>> class Animal(ub.NiceRepr):
        ...    def __init__(self):
        ...        ...
        ...    def __nice__(self):
        ...        return ''
        >>> class Cat(Animal):
        >>>     ...
        >>> class Dog(Animal):
        >>>     ...
        >>> class Beagle(Dog):
        >>>     ...
        >>> class Ragdoll(Cat):
        >>>     ...
        >>> instances = [Animal(), Cat(), Dog(), Beagle(), Ragdoll()]
        >>> for inst in instances:
        >>>     print(str(inst))
        <Animal()>
        <Cat()>
        <Dog()>
        <Beagle()>
        <Ragdoll()>

    In the case where you cant or dont want to use ubelt.NiceRepr you can get
    similar behavior by pasting the methods from the following snippet into
    your class:

    .. code:: python

        class MyClass:

            def __nice__(self):
                return 'your concise information'

            def __repr__(self):
                nice = self.__nice__()
                classname = self.__class__.__name__
                return '<{0}({1}) at {2}>'.format(classname, nice, hex(id(self)))

            def __str__(self):
                classname = self.__class__.__name__
                nice = self.__nice__()
                return '<{0}({1})>'.format(classname, nice)
    """"""

    def __nice__(self):
        """"""
        Returns:
            str
        """"""
        if hasattr(self, '__len__'):
            return str(len(self))
        else:
            raise NotImplementedError('Define the __nice__ method for {!r}'.format(self.__class__))

    def __repr__(self):
        """"""
        Returns:
            str
        """"""
        try:
            nice = self.__nice__()
            classname = self.__class__.__name__
            return '<{0}({1}) at {2}>'.format(classname, nice, hex(id(self)))
        except Exception as ex:
            warnings.warn(str(ex), category=RuntimeWarning)
            return object.__repr__(self)

    def __str__(self):
        """"""
        Returns:
            str
        """"""
        try:
            classname = self.__class__.__name__
            nice = self.__nice__()
            return '<{0}({1})>'.format(classname, nice)
        except Exception as ex:
            warnings.warn(str(ex), category=RuntimeWarning)
            return object.__repr__(self)","
class NiceRepr:
    '''
    Inherit from this class and define ``__nice__`` to ""nicely"" print your
    objects.
    Defines ``__str__`` and ``__repr__`` in terms of ``__nice__`` function
    Classes that inherit from :class:`NiceRepr` should redefine ``__nice__``.
    If the inheriting class has a ``__len__``, method then the default
    ``__nice__`` method will return its length.
    Example:
        >>> import ubelt as ub
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        return 'info'
        >>> foo = Foo()
        >>> assert str(foo) == '<Foo(info)>'
        >>> assert repr(foo).startswith('<Foo(info) at ')
    Example:
        >>> import ubelt as ub
        >>> class Bar(ub.NiceRepr):
        ...    pass
        >>> bar = Bar()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     assert 'object at' in str(bar)
        >>>     assert 'object at' in repr(bar)
    Example:
        >>> import ubelt as ub
        >>> class Baz(ub.NiceRepr):
        ...    def __len__(self):
        ...        return 5
        >>> baz = Baz()
        >>> assert str(baz) == '<Baz(5)>'
    Example:
        >>> import ubelt as ub
        >>> # If your nice message has a bug, it shouldn't bring down the house
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        assert False
        >>> foo = Foo()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     print('foo = {!r}'.format(foo))
        foo = <...Foo ...>
    Example:
        >>> import ubelt as ub
        >>> class Animal(ub.NiceRepr):
        ...    def __init__(self):
        ...        ...
        ...    def __nice__(self):
        ...        return ''
        >>> class Cat(Animal):
        >>>     ...
        >>> class Dog(Animal):
        >>>     ...
        >>> class Beagle(Dog):
        >>>     ...
        >>> class Ragdoll(Cat):
        >>>     ...
        >>> instances = [Animal(), Cat(), Dog(), Beagle(), Ragdoll()]
        >>> for inst in instances:
        >>>     print(str(inst))
        <Animal()>
        <Cat()>
        <Dog()>
        <Beagle()>
        <Ragdoll()>
    In the case where you cant or dont want to use ubelt.NiceRepr you can get
    similar behavior by pasting the methods from the following snippet into
    your class:
    .. code:: python
        class MyClass:
            def __nice__(self):
                return 'your concise information'
            def __repr__(self):
                nice = self.__nice__()
                classname = self.__class__.__name__
                return '<{0}({1}) at {2}>'.format(classname, nice, hex(id(self)))
            def __str__(self):
                classname = self.__class__.__name__
                nice = self.__nice__()
                return '<{0}({1})>'.format(classname, nice)
                    '''
            def __nice__(self):
                '''
        Returns:
            str
                        '''
                        pass
            def __repr__(self):
                '''
        Returns:
            str
                        '''
                        pass
            def __str__(self):
                '''
        Returns:
            str
                        '''
                        pass",snippet_80,4,"class NiceRepr:
    '''
    Inherit from this class and define ``__nice__`` to ""nicely"" print your
    objects.
    Defines ``__str__`` and ``__repr__`` in terms of ``__nice__`` function
    Classes that inherit from :class:`NiceRepr` should redefine ``__nice__``.
    If the inheriting class has a ``__len__``, method then the default
    ``__nice__`` method will return its length.
    Example:
        >>> import ubelt as ub
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        return 'info'
        >>> foo = Foo()
        >>> assert str(foo) == '<Foo(info)>'
        >>> assert repr(foo).startswith('<Foo(info) at ')
    Example:
        >>> import ubelt as ub
        >>> class Bar(ub.NiceRepr):
        ...    pass
        >>> bar = Bar()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     assert 'object at' in str(bar)
        >>>     assert 'object at' in repr(bar)
    Example:
        >>> import ubelt as ub
        >>> class Baz(ub.NiceRepr):
        ...    def __len__(self):
        ...        return 5
        >>> baz = Baz()
        >>> assert str(baz) == '<Baz(5)>'
    Example:
        >>> import ubelt as ub
        >>> # If your nice message has a bug, it shouldn't bring down the house
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        assert False
        >>> foo = Foo()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     print('foo = {!r}'.format(foo))
        foo = <...Foo ...>
    Example:
        >>> import ubelt as ub
        >>> class Animal(ub.NiceRepr):
        ...    def __init__(self):
        ...        ...
        ...    def __nice__(self):
        ...        return ''
        >>> class Cat(Animal):
        >>>     ...
        >>> class Dog(Animal):
        >>>     ...
        >>> class Beagle(Dog):
        >>>     ...
        >>> class Ragdoll(Cat):
        >>>     ...
        >>> instances = [Animal(), Cat(), Dog(), Beagle(), Ragdoll()]
        >>> for inst in instances:
        >>>     print(str(inst))
        <Animal()>
        <Cat()>
        <Dog()>
        <Beagle()>
        <Ragdoll()>
    In the case where you cant or dont want to use ubelt.NiceRepr you can get
    similar behavior by pasting the methods from the following snippet into
    your class:
    .. code:: python
        class MyClass:
            def __nice__(self):
                return 'your concise information'
            def __repr__(self):
                nice = self.__nice__()
                classname = self.__class__.__name__
                return '<{0}({1}) at {2}>'.format(classname, nice, hex(id(self)))
            def __str__(self):
                classname = self.__class__.__name__
                nice = self.__nice__()
                return '<{0}({1})>'.format(classname, nice)
    '''
    def __nice__(self):
        '''
        Returns:
            str
        '''
        import warnings
        try:
            if hasattr(self, '__len__'):
                return len(self)
        except Exception:
            pass
        warnings.warn(
            'NiceRepr default __nice__ used without __len__; falling back to object repr',
            RuntimeWarning
        )
        return object.__repr__(self)

    def __repr__(self):
        '''
        Returns:
            str
        '''
        import warnings
        classname = self.__class__.__name__
        try:
            nice = self.__nice__()
            return '<{0}({1}) at {2}>'.format(classname, nice, hex(id(self)))
        except Exception:
            warnings.warn(
                'Error in __nice__ for {}'.format(classname),
                RuntimeWarning
            )
            return '<...{} ...>'.format(classname)

    def __str__(self):
        '''
        Returns:
            str
        '''
        import warnings
        classname = self.__class__.__name__
        try:
            nice = self.__nice__()
            return '<{0}({1})>'.format(classname, nice)
        except Exception:
            warnings.warn(
                'Error in __nice__ for {}'.format(classname),
                RuntimeWarning
            )
            return '<...{} ...>'.format(classname)"
159497,Erotemic/ubelt,Erotemic_ubelt/ubelt/util_path.py,ubelt.util_path.ChDir,"import os

class ChDir:
    """"""
    Context manager that changes the current working directory and then
    returns you to where you were.

    This is nearly the same as the stdlib :func:`contextlib.chdir`, with the
    exception that it will do nothing if the input path is None (i.e. the user
    did not want to change directories).

    SeeAlso:
        :func:`contextlib.chdir`

    Example:
        >>> import ubelt as ub
        >>> dpath = ub.Path.appdir('ubelt/tests/chdir').ensuredir()
        >>> dir1 = (dpath / 'dir1').ensuredir()
        >>> dir2 = (dpath / 'dir2').ensuredir()
        >>> with ChDir(dpath):
        >>>     assert ub.Path.cwd() == dpath
        >>>     # change to the given directory, and then returns back
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(dir2):
        >>>             assert ub.Path.cwd() == dir2
        >>>             # changes inside the context manager will be reset
        >>>             os.chdir(dpath)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(None):
        >>>             assert ub.Path.cwd() == dir1
        >>>             # When disabled, the cwd does *not* reset at context exit
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir2
        >>>         os.chdir(dir1)
        >>>         # Dont change dirs, but reset to your cwd at context end
        >>>         with ChDir('.'):
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
    """"""

    def __init__(self, dpath):
        """"""
        Args:
            dpath (str | PathLike | None):
                The new directory to work in.
                If None, then the context manager is disabled.
        """"""
        self._context_dpath = dpath
        self._orig_dpath = None

    def __enter__(self):
        """"""
        Returns:
            ChDir: self
        """"""
        if self._context_dpath is not None:
            self._orig_dpath = os.getcwd()
            os.chdir(self._context_dpath)
        return self

    def __exit__(self, ex_type, ex_value, ex_traceback):
        """"""
        Args:
            ex_type (Type[BaseException] | None):
            ex_value (BaseException | None):
            ex_traceback (TracebackType | None):

        Returns:
            bool | None
        """"""
        if self._context_dpath is not None:
            os.chdir(self._orig_dpath)","
class ChDir:
    '''
    Context manager that changes the current working directory and then
    returns you to where you were.
    This is nearly the same as the stdlib :func:`contextlib.chdir`, with the
    exception that it will do nothing if the input path is None (i.e. the user
    did not want to change directories).
    SeeAlso:
        :func:`contextlib.chdir`
    Example:
        >>> import ubelt as ub
        >>> dpath = ub.Path.appdir('ubelt/tests/chdir').ensuredir()
        >>> dir1 = (dpath / 'dir1').ensuredir()
        >>> dir2 = (dpath / 'dir2').ensuredir()
        >>> with ChDir(dpath):
        >>>     assert ub.Path.cwd() == dpath
        >>>     # change to the given directory, and then returns back
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(dir2):
        >>>             assert ub.Path.cwd() == dir2
        >>>             # changes inside the context manager will be reset
        >>>             os.chdir(dpath)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(None):
        >>>             assert ub.Path.cwd() == dir1
        >>>             # When disabled, the cwd does *not* reset at context exit
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir2
        >>>         os.chdir(dir1)
        >>>         # Dont change dirs, but reset to your cwd at context end
        >>>         with ChDir('.'):
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
    '''
    def __init__(self, dpath):
        '''
        Args:
            dpath (str | PathLike | None):
                The new directory to work in.
                If None, then the context manager is disabled.
        '''
        pass
    def __enter__(self):
        '''
        Returns:
            ChDir: self
        '''
        pass
    def __exit__(self, ex_type, ex_value, ex_traceback):
        '''
        Args:
            ex_type (Type[BaseException] | None):
            ex_value (BaseException | None):
            ex_traceback (TracebackType | None):
        Returns:
            bool | None
        '''
        pass",snippet_81,4,"class ChDir:
    '''
    Context manager that changes the current working directory and then
    returns you to where you were.
    This is nearly the same as the stdlib :func:`contextlib.chdir`, with the
    exception that it will do nothing if the input path is None (i.e. the user
    did not want to change directories).
    SeeAlso:
        :func:`contextlib.chdir`
    Example:
        >>> import ubelt as ub
        >>> dpath = ub.Path.appdir('ubelt/tests/chdir').ensuredir()
        >>> dir1 = (dpath / 'dir1').ensuredir()
        >>> dir2 = (dpath / 'dir2').ensuredir()
        >>> with ChDir(dpath):
        >>>     assert ub.Path.cwd() == dpath
        >>>     # change to the given directory, and then returns back
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(dir2):
        >>>             assert ub.Path.cwd() == dir2
        >>>             # changes inside the context manager will be reset
        >>>             os.chdir(dpath)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(None):
        >>>             assert ub.Path.cwd() == dir1
        >>>             # When disabled, the cwd does *not* reset at context exit
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir2
        >>>         os.chdir(dir1)
        >>>         # Dont change dirs, but reset to your cwd at context end
        >>>         with ChDir('.'):
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
    '''
    def __init__(self, dpath):
        '''
        Args:
            dpath (str | PathLike | None):
                The new directory to work in.
                If None, then the context manager is disabled.
        '''
        import os
        self._os = os
        self._dpath = dpath
        self._disabled = dpath is None
        self._prevdir = None

    def __enter__(self):
        '''
        Returns:
            ChDir: self
        '''
        if not self._disabled:
            self._prevdir = self._os.getcwd()
            self._os.chdir(self._os.fspath(self._dpath))
        return self

    def __exit__(self, ex_type, ex_value, ex_traceback):
        '''
        Args:
            ex_type (Type[BaseException] | None):
            ex_value (BaseException | None):
            ex_traceback (TracebackType | None):
        Returns:
            bool | None
        '''
        if not self._disabled and self._prevdir is not None:
            self._os.chdir(self._prevdir)
        return False"
159980,pazz/alot,alot/completion/completer.py,alot.completion.completer.Completer,"import abc

class Completer:
    """"""base class for completers""""""
    __metaclass__ = abc.ABCMeta

    @abc.abstractmethod
    def complete(self, original, pos):
        """"""returns a list of completions and cursor positions for the string
        `original` from position `pos` on.

        :param original: the string to complete
        :type original: str
        :param pos: starting position to complete from
        :type pos: int
        :returns: pairs of completed string and cursor position in the
                  new string
        :rtype: list of (str, int)
        :raises: :exc:`CompletionError`
        """"""
        pass

    def relevant_part(self, original, pos):
        """"""
        Calculate the subword in a ' '-separated list of substrings of
        `original` that `pos` is in.
        """"""
        start = original.rfind(' ', 0, pos) + 1
        end = original.find(' ', pos - 1)
        if end == -1:
            end = len(original)
        return (original[start:end], start, end, pos - start)","
class Completer:
    
    @abc.abstractmethod
    def complete(self, original, pos):
        
        pass
    def relevant_part(self, original, pos):
        '''
        Calculate the subword in a ' '-separated list of substrings of
        `original` that `pos` is in.
        '''
        pass",snippet_82,3,"import abc

class Completer(abc.ABC):
    
    @abc.abstractmethod
    def complete(self, original, pos):
        raise NotImplementedError

    def relevant_part(self, original, pos):
        '''
        Calculate the subword in a ' '-separated list of substrings of
        `original` that `pos` is in.
        '''
        if original is None:
            return ''
        if pos is None:
            pos = 0
        # Clamp pos to valid range
        pos = max(0, min(len(original), pos))

        # Start is one char after the last space before pos (or 0 if none)
        last_space = original.rfind(' ', 0, pos)
        start = last_space + 1

        # End is the next space at/after pos (or end of string if none)
        next_space = original.find(' ', pos)
        end = len(original) if next_space == -1 else next_space

        return original[start:end]"
160363,splunk/splunk-sdk-python,splunk_splunk-sdk-python/splunklib/modularinput/argument.py,splunklib.modularinput.argument.Argument,"import xml.etree.ElementTree as ET

class Argument:
    """"""Class representing an argument to a modular input kind.

    ``Argument`` is meant to be used with ``Scheme`` to generate an XML
    definition of the modular input kind that Splunk understands.

    ``name`` is the only required parameter for the constructor.

        **Example with least parameters**::

            arg1 = Argument(name=""arg1"")

        **Example with all parameters**::

            arg2 = Argument(
                name=""arg2"",
                description=""This is an argument with lots of parameters"",
                validation=""is_pos_int('some_name')"",
                data_type=Argument.data_type_number,
                required_on_edit=True,
                required_on_create=True
            )
    """"""
    data_type_boolean = 'BOOLEAN'
    data_type_number = 'NUMBER'
    data_type_string = 'STRING'

    def __init__(self, name, description=None, validation=None, data_type=data_type_string, required_on_edit=False, required_on_create=False, title=None):
        """"""
        :param name: ``string``, identifier for this argument in Splunk.
        :param description: ``string``, human-readable description of the argument.
        :param validation: ``string`` specifying how the argument should be validated, if using internal validation.
               If using external validation, this will be ignored.
        :param data_type: ``string``, data type of this field; use the class constants.
               ""data_type_boolean"", ""data_type_number"", or ""data_type_string"".
        :param required_on_edit: ``Boolean``, whether this arg is required when editing an existing modular input of this kind.
        :param required_on_create: ``Boolean``, whether this arg is required when creating a modular input of this kind.
        :param title: ``String``, a human-readable title for the argument.
        """"""
        self.name = name
        self.description = description
        self.validation = validation
        self.data_type = data_type
        self.required_on_edit = required_on_edit
        self.required_on_create = required_on_create
        self.title = title

    def add_to_document(self, parent):
        """"""Adds an ``Argument`` object to this ElementTree document.

        Adds an <arg> subelement to the parent element, typically <args>
        and sets up its subelements with their respective text.

        :param parent: An ``ET.Element`` to be the parent of a new <arg> subelement
        :returns: An ``ET.Element`` object representing this argument.
        """"""
        arg = ET.SubElement(parent, 'arg')
        arg.set('name', self.name)
        if self.title is not None:
            ET.SubElement(arg, 'title').text = self.title
        if self.description is not None:
            ET.SubElement(arg, 'description').text = self.description
        if self.validation is not None:
            ET.SubElement(arg, 'validation').text = self.validation
        subelements = [('data_type', self.data_type), ('required_on_edit', self.required_on_edit), ('required_on_create', self.required_on_create)]
        for name, value in subelements:
            ET.SubElement(arg, name).text = str(value).lower()
        return arg","
class Argument:
    '''Class representing an argument to a modular input kind.
    ``Argument`` is meant to be used with ``Scheme`` to generate an XML
    definition of the modular input kind that Splunk understands.
    ``name`` is the only required parameter for the constructor.
        **Example with least parameters**::
            arg1 = Argument(name=""arg1"")
        **Example with all parameters**::
            arg2 = Argument(
                name=""arg2"",
                description=""This is an argument with lots of parameters"",
                validation=""is_pos_int('some_name')"",
                data_type=Argument.data_type_number,
                required_on_edit=True,
                required_on_create=True
            )
    '''
    def __init__(self, name, description=None, validation=None, data_type=data_type_string, required_on_edit=False, required_on_create=False, title=None):
        '''
        :param name: ``string``, identifier for this argument in Splunk.
        :param description: ``string``, human-readable description of the argument.
        :param validation: ``string`` specifying how the argument should be validated, if using internal validation.
               If using external validation, this will be ignored.
        :param data_type: ``string``, data type of this field; use the class constants.
               ""data_type_boolean"", ""data_type_number"", or ""data_type_string"".
        :param required_on_edit: ``Boolean``, whether this arg is required when editing an existing modular input of this kind.
        :param required_on_create: ``Boolean``, whether this arg is required when creating a modular input of this kind.
        :param title: ``String``, a human-readable title for the argument.
        '''
        pass
    def add_to_document(self, parent):
        '''Adds an ``Argument`` object to this ElementTree document.
        Adds an <arg> subelement to the parent element, typically <args>
        and sets up its subelements with their respective text.
        :param parent: An ``ET.Element`` to be the parent of a new <arg> subelement
        :returns: An ``ET.Element`` object representing this argument.
        '''
        pass",snippet_83,3,"import xml.etree.ElementTree as ET


class Argument:
    '''Class representing an argument to a modular input kind.
    ``Argument`` is meant to be used with ``Scheme`` to generate an XML
    definition of the modular input kind that Splunk understands.
    ``name`` is the only required parameter for the constructor.
        **Example with least parameters**::
            arg1 = Argument(name=""arg1"")
        **Example with all parameters**::
            arg2 = Argument(
                name=""arg2"",
                description=""This is an argument with lots of parameters"",
                validation=""is_pos_int('some_name')"",
                data_type=Argument.data_type_number,
                required_on_edit=True,
                required_on_create=True
            )
    '''

    data_type_boolean = ""boolean""
    data_type_number = ""number""
    data_type_string = ""string""

    def __init__(self, name, description=None, validation=None, data_type=data_type_string, required_on_edit=False, required_on_create=False, title=None):
        '''
        :param name: ``string``, identifier for this argument in Splunk.
        :param description: ``string``, human-readable description of the argument.
        :param validation: ``string`` specifying how the argument should be validated, if using internal validation.
               If using external validation, this will be ignored.
        :param data_type: ``string``, data type of this field; use the class constants.
               ""data_type_boolean"", ""data_type_number"", or ""data_type_string"".
        :param required_on_edit: ``Boolean``, whether this arg is required when editing an existing modular input of this kind.
        :param required_on_create: ``Boolean``, whether this arg is required when creating a modular input of this kind.
        :param title: ``String``, a human-readable title for the argument.
        '''
        if not isinstance(name, str) or not name:
            raise ValueError(""Argument 'name' must be a non-empty string"")
        allowed_types = {self.data_type_boolean, self.data_type_number, self.data_type_string}
        if data_type not in allowed_types:
            raise ValueError(f""Argument 'data_type' must be one of {allowed_types}"")
        if not isinstance(required_on_create, bool):
            raise ValueError(""Argument 'required_on_create' must be a boolean"")
        if not isinstance(required_on_edit, bool):
            raise ValueError(""Argument 'required_on_edit' must be a boolean"")

        self.name = name
        self.description = description
        self.validation = validation
        self.data_type = data_type
        self.required_on_edit = required_on_edit
        self.required_on_create = required_on_create
        self.title = title

    def add_to_document(self, parent):
        '''Adds an ``Argument`` object to this ElementTree document.
        Adds an <arg> subelement to the parent element, typically <args>
        and sets up its subelements with their respective text.
        :param parent: An ``ET.Element`` to be the parent of a new <arg> subelement
        :returns: An ``ET.Element`` object representing this argument.
        '''
        if parent is None or not isinstance(parent, ET.Element):
            raise ValueError(""parent must be an xml.etree.ElementTree.Element"")

        arg_el = ET.SubElement(parent, ""arg"")
        arg_el.set(""name"", self.name)

        if self.title is not None:
            title_el = ET.SubElement(arg_el, ""title"")
            title_el.text = str(self.title)

        if self.description is not None:
            desc_el = ET.SubElement(arg_el, ""description"")
            desc_el.text = str(self.description)

        if self.validation is not None:
            val_el = ET.SubElement(arg_el, ""validation"")
            val_el.text = str(self.validation)

        dt_el = ET.SubElement(arg_el, ""data_type"")
        dt_el.text = self.data_type

        roc_el = ET.SubElement(arg_el, ""required_on_create"")
        roc_el.text = ""true"" if self.required_on_create else ""false""

        roe_el = ET.SubElement(arg_el, ""required_on_edit"")
        roe_el.text = ""true"" if self.required_on_edit else ""false""

        return arg_el"
160367,splunk/splunk-sdk-python,splunk_splunk-sdk-python/splunklib/modularinput/scheme.py,splunklib.modularinput.scheme.Scheme,"import xml.etree.ElementTree as ET

class Scheme:
    """"""Class representing the metadata for a modular input kind.

    A ``Scheme`` specifies a title, description, several options of how Splunk should run modular inputs of this
    kind, and a set of arguments which define a particular modular input's properties.

    The primary use of ``Scheme`` is to abstract away the construction of XML to feed to Splunk.
    """"""
    streaming_mode_simple = 'SIMPLE'
    streaming_mode_xml = 'XML'

    def __init__(self, title):
        """"""
        :param title: ``string`` identifier for this Scheme in Splunk.
        """"""
        self.title = title
        self.description = None
        self.use_external_validation = True
        self.use_single_instance = False
        self.streaming_mode = Scheme.streaming_mode_xml
        self.arguments = []

    def add_argument(self, arg):
        """"""Add the provided argument, ``arg``, to the ``self.arguments`` list.

        :param arg: An ``Argument`` object to add to ``self.arguments``.
        """"""
        self.arguments.append(arg)

    def to_xml(self):
        """"""Creates an ``ET.Element`` representing self, then returns it.

        :returns: an ``ET.Element`` representing this scheme.
        """"""
        root = ET.Element('scheme')
        ET.SubElement(root, 'title').text = self.title
        if self.description is not None:
            ET.SubElement(root, 'description').text = self.description
        subelements = [('use_external_validation', self.use_external_validation), ('use_single_instance', self.use_single_instance), ('streaming_mode', self.streaming_mode)]
        for name, value in subelements:
            ET.SubElement(root, name).text = str(value).lower()
        endpoint = ET.SubElement(root, 'endpoint')
        args = ET.SubElement(endpoint, 'args')
        for arg in self.arguments:
            arg.add_to_document(args)
        return root","
class Scheme:
    '''Class representing the metadata for a modular input kind.
    A ``Scheme`` specifies a title, description, several options of how Splunk should run modular inputs of this
    kind, and a set of arguments which define a particular modular input's properties.
    The primary use of ``Scheme`` is to abstract away the construction of XML to feed to Splunk.
    '''
    def __init__(self, title):
        
        pass
    def add_argument(self, arg):
        '''Add the provided argument, ``arg``, to the ``self.arguments`` list.
        :param arg: An ``Argument`` object to add to ``self.arguments``.
        '''
        pass
    def to_xml(self):
        '''Creates an ``ET.Element`` representing self, then returns it.
        :returns: an ``ET.Element`` representing this scheme.
        '''
        pass",snippet_84,4,"import xml.etree.ElementTree as ET

class Scheme:
    '''Class representing the metadata for a modular input kind.
    A ``Scheme`` specifies a title, description, several options of how Splunk should run modular inputs of this
    kind, and a set of arguments which define a particular modular input's properties.
    The primary use of ``Scheme`` is to abstract away the construction of XML to feed to Splunk.
    '''
    def __init__(self, title):
        if not title:
            raise ValueError(""title must be a non-empty string"")
        self.title = title
        self.description = None
        self.use_external_validation = False
        self.streaming_mode = 'xml'
        self.use_single_instance = False
        self.arguments = []

    def add_argument(self, arg):
        '''Add the provided argument, ``arg``, to the ``self.arguments`` list.
        :param arg: An ``Argument`` object to add to ``self.arguments``.
        '''
        if arg is None:
            raise ValueError(""arg cannot be None"")
        self.arguments.append(arg)
        return self

    def to_xml(self):
        '''Creates an ``ET.Element`` representing self, then returns it.
        :returns: an ``ET.Element`` representing this scheme.
        '''
        scheme = ET.Element('scheme')

        title_el = ET.SubElement(scheme, 'title')
        title_el.text = str(self.title)

        if self.description is not None:
            desc_el = ET.SubElement(scheme, 'description')
            desc_el.text = str(self.description)

        ext_val_el = ET.SubElement(scheme, 'use_external_validation')
        ext_val_el.text = 'true' if self.use_external_validation else 'false'

        streaming_el = ET.SubElement(scheme, 'streaming_mode')
        streaming_el.text = str(self.streaming_mode)

        single_inst_el = ET.SubElement(scheme, 'use_single_instance')
        single_inst_el.text = 'true' if self.use_single_instance else 'false'

        endpoint_el = ET.SubElement(scheme, 'endpoint')
        args_el = ET.SubElement(endpoint_el, 'args')

        for arg in self.arguments:
            if hasattr(arg, 'to_xml') and callable(arg.to_xml):
                arg_el = arg.to_xml()
                args_el.append(arg_el)
            else:
                # Fallback minimal representation if no to_xml available
                ET.SubElement(args_el, 'arg', {'name': str(arg)})

        return scheme"
160991,ncclient/ncclient,ncclient_ncclient/ncclient/transport/session.py,ncclient.transport.session.SessionListener,"class SessionListener:
    """"""Base class for :class:`Session` listeners, which are notified when a new
    NETCONF message is received or an error occurs.

    .. note::
        Avoid time-intensive tasks in a callback's context.
    """"""

    def callback(self, root, raw):
        """"""Called when a new XML document is received. The *root* argument allows the callback to determine whether it wants to further process the document.

        Here, *root* is a tuple of *(tag, attributes)* where *tag* is the qualified name of the root element and *attributes* is a dictionary of its attributes (also qualified names).

        *raw* will contain the XML document as a string.
        """"""
        raise NotImplementedError

    def errback(self, ex):
        """"""Called when an error occurs.

        :type ex: :exc:`Exception`
        """"""
        raise NotImplementedError","class SessionListener:
    '''Base class for :class:`Session` listeners, which are notified when a new
    NETCONF message is received or an error occurs.
    .. note::
        Avoid time-intensive tasks in a callback's context.
    '''
    def callback(self, root, raw):
        
        pass
    def errback(self, ex):
        '''Called when an error occurs.
        :type ex: :exc:`Exception`
        '''
        pass",snippet_85,3,"from abc import ABC, abstractmethod
from typing import Any


class SessionListener(ABC):
    '''Base class for :class:`Session` listeners, which are notified when a new
    NETCONF message is received or an error occurs.
    .. note::
        Avoid time-intensive tasks in a callback's context.
    '''

    @abstractmethod
    def callback(self, root: Any, raw: Any) -> None:
        pass

    @abstractmethod
    def errback(self, ex: Exception) -> None:
        '''Called when an error occurs.
        :type ex: :exc:`Exception`
        '''
        pass"
163149,python-cmd2/cmd2,python-cmd2_cmd2/cmd2/argparse_custom.py,cmd2.argparse_custom.Cmd2AttributeWrapper,"from typing import TYPE_CHECKING, Any, ClassVar, NoReturn, Protocol, cast, runtime_checkable

class Cmd2AttributeWrapper:
    """"""Wraps a cmd2-specific attribute added to an argparse Namespace.

    This makes it easy to know which attributes in a Namespace are
    arguments from a parser and which were added by cmd2.
    """"""

    def __init__(self, attribute: Any) -> None:
        """"""Initialize Cmd2AttributeWrapper instances.""""""
        self.__attribute = attribute

    def get(self) -> Any:
        """"""Get the value of the attribute.""""""
        return self.__attribute

    def set(self, new_val: Any) -> None:
        """"""Set the value of the attribute.""""""
        self.__attribute = new_val","
class Cmd2AttributeWrapper:
    '''Wraps a cmd2-specific attribute added to an argparse Namespace.
    This makes it easy to know which attributes in a Namespace are
    arguments from a parser and which were added by cmd2.
    '''
    def __init__(self, attribute: Any) -> None:
        
        pass
    def get(self) -> Any:
        '''Get the value of the attribute.'''
        pass
    def set(self, new_val: Any) -> None:
        
        pass",snippet_86,4,"from typing import Any

class Cmd2AttributeWrapper:
    '''Wraps a cmd2-specific attribute added to an argparse Namespace.
    This makes it easy to know which attributes in a Namespace are
    arguments from a parser and which were added by cmd2.
    '''
    def __init__(self, attribute: Any) -> None:
        self._value = attribute

    def get(self) -> Any:
        '''Get the value of the attribute.'''
        return self._value

    def set(self, new_val: Any) -> None:
        self._value = new_val"
163184,python-cmd2/cmd2,python-cmd2_cmd2/cmd2/utils.py,cmd2.utils.ContextFlag,"class ContextFlag:
    """"""A context manager which is also used as a boolean flag value within the default sigint handler.

    Its main use is as a flag to prevent the SIGINT handler in cmd2 from raising a KeyboardInterrupt
    while a critical code section has set the flag to True. Because signal handling is always done on the
    main thread, this class is not thread-safe since there is no need.
    """"""

    def __init__(self) -> None:
        """"""When this flag has a positive value, it is considered set. When it is 0, it is not set.

        It should never go below 0.
        """"""
        self.__count = 0

    def __bool__(self) -> bool:
        """"""Define the truth value of an object when it is used in a boolean context.""""""
        return self.__count > 0

    def __enter__(self) -> None:
        """"""When a with block is entered, the __enter__ method of the context manager is called.""""""
        self.__count += 1

    def __exit__(self, *args: object) -> None:
        """"""When the execution flow exits a with statement block this is called, regardless of whether an exception occurred.""""""
        self.__count -= 1
        if self.__count < 0:
            raise ValueError('count has gone below 0')","class ContextFlag:
    '''A context manager which is also used as a boolean flag value within the default sigint handler.
    Its main use is as a flag to prevent the SIGINT handler in cmd2 from raising a KeyboardInterrupt
    while a critical code section has set the flag to True. Because signal handling is always done on the
    main thread, this class is not thread-safe since there is no need.
        '''
    def __init__(self) -> None:
        
        pass
    def __bool__(self) -> bool:
        
        pass
    def __enter__(self) -> None:
        '''When a with block is entered, the __enter__ method of the context manager is called.'''
        pass
    def __exit__(self, *args: object) -> None:
        '''When the execution flow exits a with statement block this is called, regardless of whether an exception occurred.'''
        pass",snippet_87,5,"class ContextFlag:
    '''A context manager which is also used as a boolean flag value within the default sigint handler.
    Its main use is as a flag to prevent the SIGINT handler in cmd2 from raising a KeyboardInterrupt
    while a critical code section has set the flag to True. Because signal handling is always done on the
    main thread, this class is not thread-safe since there is no need.
        '''
    def __init__(self) -> None:
        self._depth = 0

    def __bool__(self) -> bool:
        return self._depth > 0

    def __enter__(self) -> None:
        '''When a with block is entered, the __enter__ method of the context manager is called.'''
        self._depth += 1

    def __exit__(self, *args: object) -> None:
        '''When the execution flow exits a with statement block this is called, regardless of whether an exception occurred.'''
        if self._depth > 0:
            self._depth -= 1
        else:
            self._depth = 0"
166548,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/cache/cache.py,pypyr.cache.cache.Cache,"import threading
from pypyr.config import config

class Cache:
    """"""Thread-safe general purpose cache for objects.

    Add things to the cache by calling get(key, creator). If the requested key
    doesn't exist, will add the item to the cache for you.
    """"""

    def __init__(self):
        """"""Instantiate the cache.""""""
        self._lock = threading.Lock()
        self._cache = {}

    def clear(self):
        """"""Clear the cache of all objects.""""""
        with self._lock:
            self._cache.clear()

    def get(self, key, creator):
        """"""Get key from cache. If key not exist, call creator and cache result.

        Looks for key in cache and returns object for that key.

        If key is not found, call creator and save the result to cache for that
        key.

        Be warned that get happens under the context of a Lock. . . so if
        creator takes a long time you might well be blocking.

        If config no_cache is True, bypasses cache entirely - will call
        creator each time and also not save the result to cache.

        Args:
            key: key (unique id) of cached item
            creator: callable that will create cached object if key not found

        Returns:
            Cached item at key or the result of creator()
        """"""
        if config.no_cache:
            logger.debug('no cache mode enabled. creating `%s` sans cache', key)
            return creator()
        with self._lock:
            if key in self._cache:
                logger.debug('`%s` loading from cache', key)
                obj = self._cache[key]
            else:
                logger.debug('`%s` not found in cache. . . creating', key)
                obj = creator()
                self._cache[key] = obj
        return obj","
class Cache:
    '''Thread-safe general purpose cache for objects.
    Add things to the cache by calling get(key, creator). If the requested key
    doesn't exist, will add the item to the cache for you.
    '''
    def __init__(self):
        
        pass
    def clear(self):
        '''Clear the cache of all objects.'''
        pass
    def get(self, key, creator):
        '''Get key from cache. If key not exist, call creator and cache result.
        Looks for key in cache and returns object for that key.
        If key is not found, call creator and save the result to cache for that
        key.
        Be warned that get happens under the context of a Lock. . . so if
        creator takes a long time you might well be blocking.
        If config no_cache is True, bypasses cache entirely - will call
        creator each time and also not save the result to cache.
        Args:
            key: key (unique id) of cached item
            creator: callable that will create cached object if key not found
        Returns:
            Cached item at key or the result of creator()
        '''
        pass",snippet_88,4,"import threading

class Cache:
    '''Thread-safe general purpose cache for objects.
    Add things to the cache by calling get(key, creator). If the requested key
    doesn't exist, will add the item to the cache for you.
    '''
    def __init__(self):
        self._cache = {}
        self._lock = threading.Lock()
        self.no_cache = False

    def clear(self):
        '''Clear the cache of all objects.'''
        with self._lock:
            self._cache.clear()

    def get(self, key, creator):
        '''Get key from cache. If key not exist, call creator and cache result.
        Looks for key in cache and returns object for that key.
        If key is not found, call creator and save the result to cache for that
        key.
        Be warned that get happens under the context of a Lock. . . so if
        creator takes a long time you might well be blocking.
        If config no_cache is True, bypasses cache entirely - will call
        creator each time and also not save the result to cache.
        Args:
            key: key (unique id) of cached item
            creator: callable that will create cached object if key not found
        Returns:
            Cached item at key or the result of creator()
        '''
        if self.no_cache:
            return creator()

        with self._lock:
            if key in self._cache:
                return self._cache[key]
            value = creator()
            self._cache[key] = value
            return value"
166549,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/cache/loadercache.py,pypyr.cache.loadercache.Loader,"from pypyr.pipedef import PipelineDefinition, PipelineInfo
from pypyr.cache.cache import Cache
from collections.abc import Mapping
from pypyr.errors import PipelineDefinitionError

class Loader:
    """"""A single pipeline loader & the cache for all pipelines it has loaded.

    It loads pipelines using the get_pipeline_definition you assign to the
    loader at initialization.

    Attributes:
        name (str): Absolute module name of loader.
    """"""
    __slots__ = ['name', '_get_pipeline_definition', '_pipeline_cache']

    def __init__(self, name, get_pipeline_definition):
        """"""Initialize the loader and its pipeline cache.

        The expected function signature is:
        get_pipeline_definition(name: str,
                                parent: any) -> PipelineDefinition | Mapping

        Args:
            name: Absolute name of loader
            get_pipeline_definition: Reference to the function to call when
                loading a pipeline with this Loader.
        """"""
        self.name = name
        self._get_pipeline_definition = get_pipeline_definition
        self._pipeline_cache = Cache()

    def clear(self):
        """"""Clear all the pipelines in this Loader's cache.""""""
        self._pipeline_cache.clear()

    def get_pipeline(self, name, parent):
        """"""Get cached PipelineDefinition. Adds it to cache if it doesn't exist.

        The cache is local to this Loader instance.

        The combination of parent+name must be unique for this Loader. Parent
        should therefore have a sensible __str__ implementation because it
        forms part of the pipeline's identifying str key in the cache.

        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.

        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        """"""
        normalized_name = f'{parent}+{name}' if parent else name
        return self._pipeline_cache.get(normalized_name, lambda: self._load_pipeline(name, parent))

    def _load_pipeline(self, name, parent):
        """"""Execute get_pipeline_definition(name, parent) for this loader.

        If the loader get_pipeline_definition does not return a
        PipelineDefinition, this method will wrap the payload inside a
        PipelineDefinition for you.

        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.

        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        """"""
        logger.debug('starting')
        logger.debug('loading the pipeline definition with %s', self.name)
        pipeline_definition = self._get_pipeline_definition(pipeline_name=name, parent=parent)
        if not isinstance(pipeline_definition, PipelineDefinition):
            pipeline_definition = PipelineDefinition(pipeline=pipeline_definition, info=PipelineInfo(pipeline_name=name, loader=self.name, parent=parent))
        if not isinstance(pipeline_definition.pipeline, Mapping):
            raise PipelineDefinitionError(""A pipeline must be a mapping at the top level. Does your top-level yaml have a 'steps:' key? For example:\n\nsteps:\n  - name: pypyr.steps.echo\n    in:\n      echoMe: this is a bare bones pipeline example.\n"")
        logger.debug('done')
        return pipeline_definition","
class Loader:
    '''A single pipeline loader & the cache for all pipelines it has loaded.
    It loads pipelines using the get_pipeline_definition you assign to the
    loader at initialization.
    Attributes:
        name (str): Absolute module name of loader.
    '''
    def __init__(self, name, get_pipeline_definition):
        '''Initialize the loader and its pipeline cache.
        The expected function signature is:
        get_pipeline_definition(name: str,
                                parent: any) -> PipelineDefinition | Mapping
        Args:
            name: Absolute name of loader
            get_pipeline_definition: Reference to the function to call when
                loading a pipeline with this Loader.
        '''
        pass
    def clear(self):
        '''Clear all the pipelines in this Loader's cache.'''
        pass
    def get_pipeline(self, name, parent):
        '''Get cached PipelineDefinition. Adds it to cache if it doesn't exist.
        The cache is local to this Loader instance.
        The combination of parent+name must be unique for this Loader. Parent
        should therefore have a sensible __str__ implementation because it
        forms part of the pipeline's identifying str key in the cache.
        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.
        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        '''
        pass
    def _load_pipeline(self, name, parent):
        '''Execute get_pipeline_definition(name, parent) for this loader.
        If the loader get_pipeline_definition does not return a
        PipelineDefinition, this method will wrap the payload inside a
        PipelineDefinition for you.
        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.
        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        '''
        pass",snippet_89,5,"class Loader:
    '''A single pipeline loader & the cache for all pipelines it has loaded.
    It loads pipelines using the get_pipeline_definition you assign to the
    loader at initialization.
    Attributes:
        name (str): Absolute module name of loader.
    '''
    def __init__(self, name, get_pipeline_definition):
        '''Initialize the loader and its pipeline cache.
        The expected function signature is:
        get_pipeline_definition(name: str,
                                parent: any) -> PipelineDefinition | Mapping
        Args:
            name: Absolute name of loader
            get_pipeline_definition: Reference to the function to call when
                loading a pipeline with this Loader.
        '''
        if not isinstance(name, str) or not name:
            raise ValueError('name must be a non-empty string.')
        if not callable(get_pipeline_definition):
            raise TypeError('get_pipeline_definition must be callable.')
        self.name = name
        self.get_pipeline_definition = get_pipeline_definition
        self._cache = {}

    def clear(self):
        '''Clear all the pipelines in this Loader's cache.'''
        self._cache.clear()

    def get_pipeline(self, name, parent):
        '''Get cached PipelineDefinition. Adds it to cache if it doesn't exist.
        The cache is local to this Loader instance.
        The combination of parent+name must be unique for this Loader. Parent
        should therefore have a sensible __str__ implementation because it
        forms part of the pipeline's identifying str key in the cache.
        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.
        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        '''
        key = f'{str(parent)}:{name}'
        if key in self._cache:
            return self._cache[key]

        pipe = self._load_pipeline(name, parent)
        self._cache[key] = pipe
        return pipe

    def _load_pipeline(self, name, parent):
        '''Execute get_pipeline_definition(name, parent) for this loader.
        If the loader get_pipeline_definition does not return a
        PipelineDefinition, this method will wrap the payload inside a
        PipelineDefinition for you.
        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.
        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        '''
        from pypyr.pipedef import PipelineDefinition

        result = self.get_pipeline_definition(name, parent)
        if isinstance(result, PipelineDefinition):
            return result

        return PipelineDefinition(result, self, name, parent)"
166587,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/pipedef.py,pypyr.pipedef.PipelineDefinition,"class PipelineDefinition:
    """"""The pipeline body and its metadata.

    A loader creates the PipelineDefinition and sets the metadata in .info.

    The PipelineDefinition is a globally shared cache of the pipeline body &
    meta-data.

    Attributes:
        pipeline (dict-like): The pipeline yaml body.
        info (PipelineInfo): Meta-data set by the loader for the pipeline.
    """"""
    __slots__ = ['pipeline', 'info']

    def __init__(self, pipeline, info):
        """"""Initialize a pipeline definition.

        Args:
            pipeline (dict-like): The pipeline yaml body itself.
            info (PipelineInfo): Meta-data set by the loader for the pipeline.
        """"""
        self.pipeline = pipeline
        self.info = info

    def __eq__(self, other):
        """"""Equality comparison checks Pipeline and info objects are equal.""""""
        type_self = type(self)
        if type_self is type(other):
            all_slots = [p for c in type_self.__mro__ for p in getattr(c, '__slots__', [])]
            return all((getattr(self, s, id(self)) == getattr(other, s, id(other)) for s in all_slots))
        else:
            return False","class PipelineDefinition:
    '''The pipeline body and its metadata.
    A loader creates the PipelineDefinition and sets the metadata in .info.
    The PipelineDefinition is a globally shared cache of the pipeline body &
    meta-data.
    Attributes:
        pipeline (dict-like): The pipeline yaml body.
        info (PipelineInfo): Meta-data set by the loader for the pipeline.
    '''
    def __init__(self, pipeline, info):
        '''Initialize a pipeline definition.
        Args:
            pipeline (dict-like): The pipeline yaml body itself.
            info (PipelineInfo): Meta-data set by the loader for the pipeline.
        '''
        pass
    def __eq__(self, other):
        '''Equality comparison checks Pipeline and info objects are equal.'''
        pass",snippet_92,3,"class PipelineDefinition:
    '''The pipeline body and its metadata.
    A loader creates the PipelineDefinition and sets the metadata in .info.
    The PipelineDefinition is a globally shared cache of the pipeline body &
    meta-data.
    Attributes:
        pipeline (dict-like): The pipeline yaml body.
        info (PipelineInfo): Meta-data set by the loader for the pipeline.
    '''
    def __init__(self, pipeline, info):
        '''Initialize a pipeline definition.
        Args:
            pipeline (dict-like): The pipeline yaml body itself.
            info (PipelineInfo): Meta-data set by the loader for the pipeline.
        '''
        self.pipeline = pipeline
        self.info = info

    def __eq__(self, other):
        '''Equality comparison checks Pipeline and info objects are equal.'''
        if not isinstance(other, PipelineDefinition):
            return NotImplemented
        return self.pipeline == other.pipeline and self.info == other.info"
166589,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/pipedef.py,pypyr.pipedef.PipelineInfo,"class PipelineInfo:
    """"""The common attributes that every pipeline loader should set.

    Custom loaders that want to add more properties to a pipeline's meta-data
    should probably derive from this class.

    Attributes:
        pipeline_name (str): Name of pipeline, as set by the loader.
        loader (str): Absolute module name of the pipeline loader.
        parent (any): pipeline_name resolves from parent. The parent can be any
            type - it is up to the loader to interpret the parent property.
        is_loader_cascading (bool): Loader cascades to child pipelines if not
            otherwise set on pype. Default True.
        is_parent_cascading (bool): Parent cascades to child pipelines if not
            otherwise set on pype. Default True.
    """"""
    __slots__ = ['pipeline_name', 'loader', 'parent', 'is_loader_cascading', 'is_parent_cascading']

    def __init__(self, pipeline_name, loader, parent, is_parent_cascading=True, is_loader_cascading=True):
        """"""Initialize PipelineInfo.

        Args:
            pipeline_name (str): name of pipeline, as set by the loader.
            loader (str): absolute module name of pypeloader.
            parent (any): pipeline_name resolves from parent.
            is_loader_cascading (bool): Loader cascades to child pipelines if
                not otherwise set on pype. Default True.
            is_parent_cascading (bool): Parent cascades to child pipelines if
                not otherwise set on pype. Default True.
        """"""
        self.pipeline_name = pipeline_name
        self.loader = loader
        self.parent = parent
        self.is_loader_cascading = is_loader_cascading
        self.is_parent_cascading = is_parent_cascading

    def __eq__(self, other):
        """"""Check all instance attributes are equal.""""""
        type_self = type(self)
        if type_self is type(other):
            all_slots = [p for c in type_self.__mro__ for p in getattr(c, '__slots__', [])]
            return all((getattr(self, s, id(self)) == getattr(other, s, id(other)) for s in all_slots))
        else:
            return False","class PipelineInfo:
    '''The common attributes that every pipeline loader should set.
    Custom loaders that want to add more properties to a pipeline's meta-data
    should probably derive from this class.
    Attributes:
        pipeline_name (str): Name of pipeline, as set by the loader.
        loader (str): Absolute module name of the pipeline loader.
        parent (any): pipeline_name resolves from parent. The parent can be any
            type - it is up to the loader to interpret the parent property.
        is_loader_cascading (bool): Loader cascades to child pipelines if not
            otherwise set on pype. Default True.
        is_parent_cascading (bool): Parent cascades to child pipelines if not
            otherwise set on pype. Default True.
    '''
    def __init__(self, pipeline_name, loader, parent, is_parent_cascading=True, is_loader_cascading=True):
        
        pass
    def __eq__(self, other):
        
        pass",snippet_93,3,"class PipelineInfo:
    '''The common attributes that every pipeline loader should set.
    Custom loaders that want to add more properties to a pipeline's meta-data
    should probably derive from this class.
    Attributes:
        pipeline_name (str): Name of pipeline, as set by the loader.
        loader (str): Absolute module name of the pipeline loader.
        parent (any): pipeline_name resolves from parent. The parent can be any
            type - it is up to the loader to interpret the parent property.
        is_loader_cascading (bool): Loader cascades to child pipelines if not
            otherwise set on pype. Default True.
        is_parent_cascading (bool): Parent cascades to child pipelines if not
            otherwise set on pype. Default True.
    '''
    def __init__(self, pipeline_name, loader, parent, is_parent_cascading=True, is_loader_cascading=True):
        self.pipeline_name = pipeline_name
        self.loader = loader
        self.parent = parent
        self.is_parent_cascading = bool(is_parent_cascading)
        self.is_loader_cascading = bool(is_loader_cascading)

    def __eq__(self, other):
        if not isinstance(other, PipelineInfo):
            return NotImplemented
        return (
            self.pipeline_name == other.pipeline_name and
            self.loader == other.loader and
            self.parent == other.parent and
            self.is_parent_cascading == other.is_parent_cascading and
            self.is_loader_cascading == other.is_loader_cascading
        )"
166604,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/steps/dsl/cmd.py,pypyr.steps.dsl.cmd.CmdStep,"import pypyr.utils.types
from pypyr.errors import ContextError, KeyInContextHasNoValueError, KeyNotInContextError
from collections.abc import Mapping, Sequence
from pypyr.context import Context
import logging
from pypyr.subproc import Command, SimpleCommandTypes

class CmdStep:
    """"""A pypyr step to run an executable or command as a subprocess.

    This models a step that takes config like this:
        cmd: <<cmd string>>

    OR, expanded syntax is as a dict
        cmd:
            run: str. mandatory. command + args to execute.
            save: bool. defaults False. save output to cmdOut. Treats output
                as text in the system's encoding and removes newlines at end.
            cwd: str/Pathlike. optional. Working directory for this command.
            bytes (bool): Default False. When `save` return output bytes from
                cmd unaltered, without applying any encoding & text newline
                processing.
            encoding (str): Default None. When `save`, decode cmd output with
                this encoding. The default of None uses the system encoding and
                should ""just work"".
            stdout (str | Path): Default None. Write stdout to this file path.
                Special value `/dev/null` writes to the system null device.
            stderr (str | Path): Default None. Write stderr to this file path.
                Special value `/dev/null` writes to the system null device.
                Special value `/dev/stdout` redirects err output to stdout.
            append (bool): Default False. When stdout/stderr a file, append
                rather than overwrite. Default is to overwrite.

    In expanded syntax, `run` can be a simple string or a list:
        cmd:
          run:
            - my-executable --arg
            - cmd here
          save: False
          cwd: ./path/here

    OR, as a list in simplified syntax:
        cmd:
          - my-executable --arg
          - ./another-executable --arg

    Any or all of the list items can use expanded syntax:
        cmd:
          - ./simple-cmd-here --arg1 value
          - run: cmd here
            save: False
            cwd: ./path/here
          - run:
              - my-executable --arg
              - ./another-executable --arg
            save: True cwd: ./path/here

    If save is True, will save the output to context as follows:
        cmdOut:
            returncode: 0
            stdout: 'stdout str here. None if empty.'
            stderr: 'stderr str here. None if empty.'

    If the cmd input contains a list of executables, cmdOut will be a list of
    cmdOut objects, in order executed.

    cmdOut.returncode is the exit status of the called process. Typically 0
    means OK. A negative value -N indicates that the child was terminated by
    signal N (POSIX only).

    The run_step method does the actual work. init parses the input yaml.

    Attributes:
        logger (logger): Logger instantiated by name of calling step.
        context: (pypyr.context.Context): The current pypyr Context.
        commands (list[pypyr.subproc.Command]): Commands to run as subprocess.
        is_shell (bool): True if subprocess should run through default shell.
        name (str): Name of calling step. Used for logging output & error
            messages.
    """"""

    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        """"""Initialize the CmdStep.

        The step config in the context dict in simplified syntax:
            cmd: <<cmd string>>

        OR, as a dict in expanded syntax:
            cmd:
                run: str. mandatory. command + args to execute.
                save: bool. optional. defaults False. save output to cmdOut.
                cwd: str/path. optional. if specified, change the working
                     directory just for the duration of the command.

        `run` can be a single string, or it can be a list of string if there
        are multiple commands to execute with the same settings.

        OR, as a list:
            cmd:
                - my-executable --arg
                - ./another-executable --arg

        Any or all of the list items can be in expanded syntax.

        Args:
            name (str): Unique name for step. Likely __name__ of calling step.
            context (pypyr.context.Context): Look for step config in this
                context instance.
            is_shell (bool): Set to true to execute cmd through the default
                shell.
        """"""
        assert name, 'name parameter must exist for CmdStep.'
        assert context, 'context param must exist for CmdStep.'
        self.name = name
        self.logger = logging.getLogger(name)
        context.assert_key_has_value(key='cmd', caller=name)
        self.context = context
        self.is_shell = is_shell
        cmd_config = context.get_formatted('cmd')
        commands: list[Command] = []
        if isinstance(cmd_config, SimpleCommandTypes):
            commands.append(Command(cmd_config, is_shell=is_shell))
        elif isinstance(cmd_config, Mapping):
            commands.append(self.create_command(cmd_config))
        elif isinstance(cmd_config, Sequence):
            for cmd in cmd_config:
                if isinstance(cmd, SimpleCommandTypes):
                    commands.append(Command(cmd, is_shell=is_shell))
                elif isinstance(cmd, Mapping):
                    commands.append(self.create_command(cmd))
                else:
                    raise ContextError(f'{cmd} in {name} cmd config is wrong.\nEach list item should be either a simple string or a dict for expanded syntax:\ncmd:\n  - my-executable --arg\n  - run: another-executable --arg value\n    cwd: ../mydir/subdir\n  - run:\n      - arb-executable1 --arg value1\n      - arb-executable2 --arg value2\n    cwd: ../mydir/arbdir')
        else:
            raise ContextError(f'{name} cmd config should be either a simple string:\ncmd: my-executable --arg\n\nor a dictionary:\ncmd:\n  run: subdir/my-executable --arg\n  cwd: ./mydir\n\nor a list of commands:\ncmd:\n  - my-executable --arg\n  - run: another-executable --arg value\n    cwd: ../mydir/subdir')
        self.commands: list[Command] = commands

    def create_command(self, cmd_input: Mapping) -> Command:
        """"""Create a pypyr.subproc.Command object from expanded step input.""""""
        try:
            cmd = cmd_input['run']
            if not cmd:
                raise KeyInContextHasNoValueError(f'cmd.run must have a value for {self.name}.\nThe `run` input should look something like this:\ncmd:\n  run: my-executable-here --arg1\n  cwd: ./mydir/subdir\n\nOr, `run` could be a list of commands:\ncmd:\n  run:\n    - arb-executable1 --arg value1\n    - arb-executable2 --arg value2\n  cwd: ../mydir/arbdir')
        except KeyError as err:
            raise KeyNotInContextError(f""cmd.run doesn't exist for {self.name}.\nThe input should look like this in the simplified syntax:\ncmd: my-executable-here --arg1\n\nOr in the expanded syntax:\ncmd:\n  run: my-executable-here --arg1\n\nIf you're passing in a list of commands, each command should be a simple string,\nor a dict with a `run` entry:\ncmd:\n  - my-executable --arg\n  - run: another-executable --arg value\n    cwd: ../mydir/subdir\n  - run:\n      - arb-executable1 --arg value1\n      - arb-executable2 --arg value2\n    cwd: ../mydir/arbdir"") from err
        is_save = pypyr.utils.types.cast_to_bool(cmd_input.get('save', False))
        cwd = cmd_input.get('cwd')
        is_bytes = cmd_input.get('bytes')
        is_text = not is_bytes if is_save else False
        stdout = cmd_input.get('stdout')
        stderr = cmd_input.get('stderr')
        if is_save:
            if stderr or stderr:
                raise ContextError(""You can't set `stdout` or `stderr` when `save` is True."")
        encoding = cmd_input.get('encoding')
        append = cmd_input.get('append', False)
        is_shell_override = cmd_input.get('shell', None)
        is_shell = self.is_shell if is_shell_override is None else is_shell_override
        return Command(cmd=cmd, is_shell=is_shell, cwd=cwd, is_save=is_save, is_text=is_text, stdout=stdout, stderr=stderr, encoding=encoding, append=append)

    def run_step(self) -> None:
        """"""Spawn a subprocess to run the command or program.

        If cmd.is_save==True, save result of each command to context 'cmdOut'.
        """"""
        results = []
        try:
            for cmd in self.commands:
                try:
                    cmd.run()
                finally:
                    if cmd.results:
                        results.extend(cmd.results)
        finally:
            if results:
                if len(results) == 1:
                    self.context['cmdOut'] = results[0]
                else:
                    self.context['cmdOut'] = results","
class CmdStep:
    '''A pypyr step to run an executable or command as a subprocess.
    This models a step that takes config like this:
        cmd: <<cmd string>>
    OR, expanded syntax is as a dict
        cmd:
            run: str. mandatory. command + args to execute.
            save: bool. defaults False. save output to cmdOut. Treats output
                as text in the system's encoding and removes newlines at end.
            cwd: str/Pathlike. optional. Working directory for this command.
            bytes (bool): Default False. When `save` return output bytes from
                cmd unaltered, without applying any encoding & text newline
                processing.
            encoding (str): Default None. When `save`, decode cmd output with
                this encoding. The default of None uses the system encoding and
                should ""just work"".
            stdout (str | Path): Default None. Write stdout to this file path.
                Special value `/dev/null` writes to the system null device.
            stderr (str | Path): Default None. Write stderr to this file path.
                Special value `/dev/null` writes to the system null device.
                Special value `/dev/stdout` redirects err output to stdout.
            append (bool): Default False. When stdout/stderr a file, append
                rather than overwrite. Default is to overwrite.
    In expanded syntax, `run` can be a simple string or a list:
        cmd:
          run:
            - my-executable --arg
            - cmd here
          save: False
          cwd: ./path/here
    OR, as a list in simplified syntax:
        cmd:
          - my-executable --arg
          - ./another-executable --arg
    Any or all of the list items can use expanded syntax:
        cmd:
          - ./simple-cmd-here --arg1 value
          - run: cmd here
            save: False
            cwd: ./path/here
          - run:
              - my-executable --arg
              - ./another-executable --arg
            save: True cwd: ./path/here
    If save is True, will save the output to context as follows:
        cmdOut:
            returncode: 0
            stdout: 'stdout str here. None if empty.'
            stderr: 'stderr str here. None if empty.'
    If the cmd input contains a list of executables, cmdOut will be a list of
    cmdOut objects, in order executed.
    cmdOut.returncode is the exit status of the called process. Typically 0
    means OK. A negative value -N indicates that the child was terminated by
    signal N (POSIX only).
    The run_step method does the actual work. init parses the input yaml.
    Attributes:
        logger (logger): Logger instantiated by name of calling step.
        context: (pypyr.context.Context): The current pypyr Context.
        commands (list[pypyr.subproc.Command]): Commands to run as subprocess.
        is_shell (bool): True if subprocess should run through default shell.
        name (str): Name of calling step. Used for logging output & error
            messages.
    '''
    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        '''Initialize the CmdStep.
        The step config in the context dict in simplified syntax:
            cmd: <<cmd string>>
        OR, as a dict in expanded syntax:
            cmd:
                run: str. mandatory. command + args to execute.
                save: bool. optional. defaults False. save output to cmdOut.
                cwd: str/path. optional. if specified, change the working
                     directory just for the duration of the command.
        `run` can be a single string, or it can be a list of string if there
        are multiple commands to execute with the same settings.
        OR, as a list:
            cmd:
                - my-executable --arg
                - ./another-executable --arg
        Any or all of the list items can be in expanded syntax.
        Args:
            name (str): Unique name for step. Likely __name__ of calling step.
            context (pypyr.context.Context): Look for step config in this
                context instance.
            is_shell (bool): Set to true to execute cmd through the default
                shell.
        '''
        pass
    def create_command(self, cmd_input: Mapping) -> Command:
        '''Create a pypyr.subproc.Command object from expanded step input.'''
        pass
    def run_step(self) -> None:
        '''Spawn a subprocess to run the command or program.
        If cmd.is_save==True, save result of each command to context 'cmdOut'.
        '''
        pass",snippet_94,4,"from __future__ import annotations

import logging
import os
import shlex
import subprocess
from pathlib import Path
from typing import Any, Mapping, Optional, Union, List


try:
    from pypyr.context import Context  # type: ignore
except Exception:  # pragma: no cover
    # Minimal fallback to allow type hints without dependency at runtime.
    class Context(dict):  # type: ignore
        pass


class CmdStep:
    '''A pypyr step to run an executable or command as a subprocess.
    This models a step that takes config like this:
        cmd: <<cmd string>>
    OR, expanded syntax is as a dict
        cmd:
            run: str. mandatory. command + args to execute.
            save: bool. defaults False. save output to cmdOut. Treats output
                as text in the system's encoding and removes newlines at end.
            cwd: str/Pathlike. optional. Working directory for this command.
            bytes (bool): Default False. When `save` return output bytes from
                cmd unaltered, without applying any encoding & text newline
                processing.
            encoding (str): Default None. When `save`, decode cmd output with
                this encoding. The default of None uses the system encoding and
                should ""just work"".
            stdout (str | Path): Default None. Write stdout to this file path.
                Special value `/dev/null` writes to the system null device.
            stderr (str | Path): Default None. Write stderr to this file path.
                Special value `/dev/null` writes to the system null device.
                Special value `/dev/stdout` redirects err output to stdout.
            append (bool): Default False. When stdout/stderr a file, append
                rather than overwrite. Default is to overwrite.
    In expanded syntax, `run` can be a simple string or a list:
        cmd:
          run:
            - my-executable --arg
            - cmd here
          save: False
          cwd: ./path/here
    OR, as a list in simplified syntax:
        cmd:
          - my-executable --arg
          - ./another-executable --arg
    Any or all of the list items can use expanded syntax:
        cmd:
          - ./simple-cmd-here --arg1 value
          - run: cmd here
            save: False
            cwd: ./path/here
          - run:
              - my-executable --arg
              - ./another-executable --arg
            save: True cwd: ./path/here
    If save is True, will save the output to context as follows:
        cmdOut:
            returncode: 0
            stdout: 'stdout str here. None if empty.'
            stderr: 'stderr str here. None if empty.'
    If the cmd input contains a list of executables, cmdOut will be a list of
    cmdOut objects, in order executed.
    cmdOut.returncode is the exit status of the called process. Typically 0
    means OK. A negative value -N indicates that the child was terminated by
    signal N (POSIX only).
    The run_step method does the actual work. init parses the input yaml.
    Attributes:
        logger (logger): Logger instantiated by name of calling step.
        context: (pypyr.context.Context): The current pypyr Context.
        commands (list[pypyr.subproc.Command]): Commands to run as subprocess.
        is_shell (bool): True if subprocess should run through default shell.
        name (str): Name of calling step. Used for logging output & error
            messages.
    '''
    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        '''Initialize the CmdStep.
        The step config in the context dict in simplified syntax:
            cmd: <<cmd string>>
        OR, as a dict in expanded syntax:
            cmd:
                run: str. mandatory. command + args to execute.
                save: bool. optional. defaults False. save output to cmdOut.
                cwd: str/path. optional. if specified, change the working
                     directory just for the duration of the command.
        `run` can be a single string, or it can be a list of string if there
        are multiple commands to execute with the same settings.
        OR, as a list:
            cmd:
                - my-executable --arg
                - ./another-executable --arg
        Any or all of the list items can be in expanded syntax.
        Args:
            name (str): Unique name for step. Likely __name__ of calling step.
            context (pypyr.context.Context): Look for step config in this
                context instance.
            is_shell (bool): Set to true to execute cmd through the default
                shell.
        '''
        self.name = name
        self.logger = logging.getLogger(name)
        self.context = context
        self.is_shell = is_shell

        if 'cmd' not in context:
            raise KeyError(""Required key not found in context: 'cmd'"")

        self._normalized: List[Mapping[str, Any]] = self._normalize_cmd(context['cmd'])

    def create_command(self, cmd_input: Mapping) -> Mapping[str, Any]:
        '''Create a pypyr.subproc.Command object from expanded step input.'''
        # This implementation normalizes to an internal dict describing the command.
        # Mapping keys:
        # - run: str
        # - save: bool
        # - cwd: Optional[Path]
        # - bytes: bool
        # - encoding: Optional[str]
        # - stdout: Optional[str | Path] (special: '/dev/null')
        # - stderr: Optional[str | Path] (special: '/dev/null', '/dev/stdout')
        # - append: bool
        run = cmd_input.get('run')
        if not run:
            raise ValueError(""cmd item must have 'run'."")

        save = bool(cmd_input.get('save', False))
        cwd = cmd_input.get('cwd', None)
        cwd_path: Optional[Path] = Path(cwd) if cwd is not None else None
        out_bytes = bool(cmd_input.get('bytes', False))
        encoding = cmd_input.get('encoding', None)
        stdout = cmd_input.get('stdout', None)
        stderr = cmd_input.get('stderr', None)
        append = bool(cmd_input.get('append', False))

        base = {
            'save': save,
            'cwd': cwd_path,
            'bytes': out_bytes,
            'encoding': encoding,
            'stdout': stdout,
            'stderr': stderr,
            'append': append,
        }

        normalized: List[Mapping[str, Any]] = []
        if isinstance(run, list):
            for r in run:
                if not isinstance(r, str):
                    raise ValueError(""Each 'run' item in list must be a string."")
                entry = dict(base)
                entry['run'] = r
                normalized.append(entry)
        elif isinstance(run, str):
            entry = dict(base)
            entry['run'] = run
            normalized.append(entry)
        else:
            raise ValueError(""'run' must be a string or list of strings."")

        # Return the first if single, else return list. But method contract says single Command.
        # For compatibility with the rest of this class, return dict for a single command.
        if len(normalized) == 1:
            return normalized[0]
        else:
            # If caller uses create_command directly with a list run, return the first.
            return normalized[0]

    def run_step(self) -> None:
        '''Spawn a subprocess to run the command or program.
        If cmd.is_save==True, save result of each command to context 'cmdOut'.
        '''
        outputs: List[Mapping[str, Any]] = []

        for cmd in self._normalized:
            run_str: str = cmd['run']
            save: bool = bool(cmd.get('save', False))
            cwd: Optional[Path] = cmd.get('cwd')
            out_bytes: bool = bool(cmd.get('bytes', False))
            encoding: Optional[str] = cmd.get('encoding')
            stdout_tgt = cmd.get('stdout')
            stderr_tgt = cmd.get('stderr')
            append: bool = bool(cmd.get('append', False))

            # Prepare subprocess parameters
            kwargs: dict[str, Any] = {}
            kwargs['shell'] = self.is_shell
            if cwd is not None:
                kwargs['cwd'] = os.fspath(cwd)

            # Determine if stderr should redirect to stdout
            stderr_to_stdout = (isinstance(stderr_tgt, str) and stderr_tgt == '/dev/stdout')

            # Handle capture when saving
            if save:
                kwargs['stdout'] = subprocess.PIPE
                kwargs['stderr'] = subprocess.STDOUT if stderr_to_stdout else subprocess.PIPE
            else:
                # stdout handling when not saving
                stdout_handle = None
                if stdout_tgt is None:
                    kwargs['stdout'] = None  # inherit
                elif isinstance(stdout_tgt, str) and stdout_tgt == '/dev/null':
                    kwargs['stdout'] = subprocess.DEVNULL
                else:
                    path = Path(stdout_tgt)
                    mode = 'ab' if append else 'wb'
                    stdout_handle = open(path, mode)
                    kwargs['stdout'] = stdout_handle

                # stderr handling when not saving
                stderr_handle = None
                if stderr_to_stdout:
                    kwargs['stderr'] = subprocess.STDOUT
                elif stderr_tgt is None:
                    kwargs['stderr'] = None  # inherit
                elif isinstance(stderr_tgt, str) and stderr_tgt == '/dev/null':
                    kwargs['stderr'] = subprocess.DEVNULL
                else:
                    path = Path(stderr_tgt)
                    mode = 'ab' if append else 'wb'
                    stderr_handle = open(path, mode)
                    kwargs['stderr'] = stderr_handle

            # Build args
            if self.is_shell:
                args: Union[str, List[str]] = run_str
            else:
                # Split into argv list
                args = shlex.split(run_str, posix=os.name != 'nt')

            # Run the subprocess
            try:
                completed = subprocess.run(args, check=False, **kwargs)
            finally:
                # Close file handles if opened for non-save
                if not save:
                    fh = kwargs.get('stdout')
                    if hasattr(fh, 'close'):
                        try:
                            fh.close()
                        except Exception:
                            pass
                    fh = kwargs.get('stderr')
                    if hasattr(fh, 'close'):
                        try:
                            fh.close()
                        except Exception:
                            pass

            # Collect and/or write outputs when saving
            if save:
                std_bytes = completed.stdout if completed.stdout is not None else b''
                err_bytes: Optional[bytes]
                if stderr_to_stdout:
                    err_bytes = None
                else:
                    err_bytes = completed.stderr if completed.stderr is not None else b''

                # If file targets specified, write the captured bytes to files.
                if stdout_tgt is not None:
                    if isinstance(stdout_tgt, str) and stdout_tgt == '/dev/null':
                        pass
                    else:
                        path = Path(stdout_tgt)
                        mode = 'ab' if append else 'wb'
                        with open(path, mode) as f:
                            f.write(std_bytes)

                if not stderr_to_stdout and (stderr_tgt is not None):
                    if isinstance(stderr_tgt, str) and stderr_tgt == '/dev/null':
                        pass
                    else:
                        path = Path(stderr_tgt)
                        mode = 'ab' if append else 'wb'
                        with open(path, mode) as f:
                            f.write(err_bytes or b'')

                # Prepare saved output
                if out_bytes:
                    out_stdout: Optional[bytes] = std_bytes if std_bytes else None
                    out_stderr: Optional[bytes] = (None if stderr_to_stdout else (err_bytes if err_bytes else None))
                else:
                    sys_enc = encoding or (None)
                    if sys_enc:
                        try:
                            s_stdout = std_bytes.decode(sys_enc)
                        except Exception:
                            s_stdout = std_bytes.decode(sys_enc, errors='replace')
                    else:
                        s_stdout = std_bytes.decode(errors='replace')
                    s_stdout = s_stdout.rstrip('\r\n') if s_stdout else ''
                    out_stdout = s_stdout if s_stdout else None

                    if stderr_to_stdout:
                        out_stderr = None
                    else:
                        if sys_enc:
                            try:
                                s_stderr = (err_bytes or b'').decode(sys_enc)
                            except Exception:
                                s_stderr = (err_bytes or b'').decode(sys_enc, errors='replace')
                        else:
                            s_stderr = (err_bytes or b'').decode(errors='replace')
                        s_stderr = s_stderr.rstrip('\r\n') if s_stderr else ''
                        out_stderr = s_stderr if s_stderr else None

                outputs.append({
                    'returncode': completed.returncode,
                    'stdout': out_stdout,
                    'stderr': out_stderr
                })

        # Save to context if any outputs recorded
        if outputs:
            self.context['cmdOut'] = outputs[0] if len(outputs) == 1 else outputs

    # Internal normalization helpers

    def _normalize_cmd(self, cmd_cfg: Any) -> List[Mapping[str, Any]]:
        items: List[Mapping[str, Any]] = []

        def expand_item(item: Any) -> List[Mapping[str, Any]]:
            if isinstance(item, str):
                return [{
                    'run': item,
                    'save': False,
                    'cwd': None,
                    'bytes': False,
                    'encoding': None,
                    'stdout': None,
                    'stderr': None,
                    'append': False
                }]
            elif isinstance(item, Mapping):
                # If run is list, expand to multiple
                run_val = item.get('run')
                if isinstance(run_val, list):
                    merged: List[Mapping[str, Any]] = []
                    for r in run_val:
                        itm = dict(item)
                        itm['run'] = r
                        merged.extend(expand_item(itm))
                    return merged
                else:
                    # Create a single normalized dict
                    norm = self.create_command(item)
                    return [norm]
            else:
                raise ValueError(""cmd items must be string or mapping."")

        if isinstance(cmd_cfg, str):
            items.extend(expand_item(cmd_cfg))
        elif isinstance(cmd_cfg, Mapping):
            # Top-level dict
            run_val = cmd_cfg.get('run')
            if run_val is None:
                raise ValueError(""cmd dict must contain 'run'."")
            items.extend(expand_item(cmd_cfg))
        elif isinstance(cmd_cfg, list):
            for entry in cmd_cfg:
                items.extend(expand_item(entry))
        else:
            raise ValueError(""cmd must be string, mapping or list."")

        return items"
166605,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/steps/dsl/cmdasync.py,pypyr.steps.dsl.cmdasync.AsyncCmdStep,"import logging
from pypyr.aio.subproc import Command, Commands
from pypyr.context import Context
import pypyr.utils.types
from collections.abc import Mapping, Sequence
from pypyr.errors import ContextError, KeyInContextHasNoValueError, KeyNotInContextError
from pypyr.subproc import SimpleCommandTypes

class AsyncCmdStep:
    """"""A pypyr step to run executables/commands concurrently as a subprocess.

    This models a step that takes config like this in simple syntax:
        cmds:
            - <<cmd string 1>>
            - <<cmd string 2>>

    All the commands will run concurrently, in parallel.

    OR, expanded syntax is as a dict
        cmds:
            run: list[str | list[str]]. mandatory. command + args to execute.
                If list entry is another list[str], the sub-list will run in
                serial.
            save: bool. defaults False. save output to cmdOut. Treats output
                as text in the system's encoding and removes newlines at end.
            cwd: str/Pathlike. optional. Working directory for these commands.
            bytes (bool): Default False. When `save` return output bytes from
                cmds unaltered, without applying any encoding & text newline
                processing.
            encoding (str): Default None. When `save`, decode output with
                this encoding. The default of None uses the system encoding and
                should ""just work"".
            stdout (str | Path): Default None. Write stdout to this file path.
                Special value `/dev/null` writes to the system null device.
            stderr (str | Path): Default None. Write stderr to this file path.
                Special value `/dev/null` writes to the system null device.
                Special value `/dev/stdout` redirects err output to stdout.
            append (bool): Default False. When stdout/stderr a file, append
                rather than overwrite. Default is to overwrite.

    In expanded syntax, `run` can be a simple string or a list:
        cmds:
          run:
            - ./my-executable --arg
            - [./another-executable --arg, ./arb-executable arghere]
          save: False
          cwd: ./path/here

    As a list in simplified syntax:
        cmds:
          - my-executable --arg
          - ./another-executable --arg

    Any or all of the list items can use expanded syntax:
        cmds:
          - ./simple-cmd-here --arg1 value
          - run: cmd here
            save: False cwd: ./path/here
          - run:
              - my-executable --arg
              - ./another-executable --arg
            save: True
            cwd: ./path/here

    Any of the list items can in turn be a list. A sub-list will run in serial.

    In this example A, B.1 & C will start concurrently. B.2 will only run once
    B.1 is finished.

        cmds:
            - A
            - [B.1, B.2]
            - C

    If save is True, will save the output to context as cmdOut.

    cmdOut will be a list of pypyr.subproc.SubprocessResult objects, in order
    executed.

    SubprocessResult has the following properties:
    cmd: the cmd/args executed
    returncode: 0
    stdout: 'stdout str here. None if empty.'
    stderr: 'stderr str here. None if empty.'

    cmdOut.returncode is the exit status of the called process. Typically 0
    means OK. A negative value -N indicates that the child was terminated by
    signal N (POSIX only).

    The run_step method does the actual work. init parses the input yaml.

    Attributes:
        logger (logger): Logger instantiated by name of calling step.
        context: (pypyr.context.Context): The current pypyr Context.
        commands (pypyr.subproc.Commands): Commands to run as subprocess.
        is_shell (bool): True if subprocess should run through default shell.
        name (str): Name of calling step. Used for logging output & error
            messages.
    """"""

    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        """"""Initialize the CmdStep.

        The step config in the context dict in simplified syntax:
            cmd: <<cmd string>>

        OR, as a dict in expanded syntax:
            cmd:
                run: str. mandatory. command + args to execute.
                save: bool. optional. defaults False. save output to cmdOut.
                cwd: str/path. optional. if specified, change the working
                     directory just for the duration of the command.

        `run` can be a single string, or it can be a list of string if there
        are multiple commands to execute with the same settings.

        OR, as a list:
            cmd:
                - my-executable --arg
                - ./another-executable --arg

        Any or all of the list items can be in expanded syntax.

        Args:
            name (str): Unique name for step. Likely __name__ of calling step.
            context (pypyr.context.Context): Look for step config in this
                context instance.
            is_shell (bool): Set to true to execute cmd through the default
                shell.
        """"""
        assert name, 'name parameter must exist for CmdStep.'
        assert context, 'context param must exist for CmdStep.'
        self.name = name
        self.logger = logging.getLogger(name)
        context.assert_key_has_value(key='cmds', caller=name)
        self.context = context
        self.is_shell = is_shell
        cmd_config = context.get_formatted('cmds')
        commands = Commands()
        if isinstance(cmd_config, SimpleCommandTypes):
            commands.append(Command(cmd_config, is_shell=is_shell))
        elif isinstance(cmd_config, Mapping):
            commands.append(self.create_command(cmd_config))
        elif isinstance(cmd_config, Sequence):
            for cmd in cmd_config:
                if isinstance(cmd, SimpleCommandTypes):
                    commands.append(Command(cmd, is_shell=is_shell))
                elif isinstance(cmd, Sequence):
                    commands.append(Command([cmd], is_shell=is_shell))
                elif isinstance(cmd, Mapping):
                    commands.append(self.create_command(cmd))
                else:
                    raise ContextError(f'{cmd} in {name} cmds config is wrong.\nEach list item should be either a simple string, or a list to run in serial,\nor a dict for expanded syntax:\ncmds:\n  - ./my-executable --arg\n  - run:\n      - ./another-executable --arg value\n      - ./another-executable --arg value2\n    cwd: ../mydir/subdir\n  - run:\n      - ./arb-executable1 --arg value1\n      - [./arb-executable2.1, ./arb-executable2.2]\n    cwd: ../mydir/arbdir\n  - [./arb-executable3.1, ./arb-executable3.2]')
        else:
            raise ContextError(f'{name} cmds config should be either a list:\ncmds:\n  - ./my-executable --arg\n  - subdir/executable --arg1\n\nor a dictionary with a `run` sub-key:\ncmds:\n  run:\n    - ./my-executable --arg\n    - subdir/executable --arg1\n  cwd: ./mydir\n\nAny of the list items in root can be in expanded syntax:\ncmds:\n  - ./my-executable --arg\n  - subdir/executable --arg1\n  - run:\n      - ./arb-executable1 --arg value1\n      - [./arb-executable2.1, ./arb-executable2.2]\n    cwd: ../mydir/subdir\n  - [./arb-executable3.1, ./arb-executable3.2]')
        self.commands: Commands = commands

    def create_command(self, cmd_input: Mapping) -> Command:
        """"""Create pypyr.aio.subproc.Command object from expanded step input.""""""
        try:
            cmd = cmd_input['run']
            if not cmd:
                raise KeyInContextHasNoValueError(f'cmds.run must have a value for {self.name}.\nThe `run` input should look something like this:\ncmds:\n  run:\n    - ./arb-executable1 --arg value1\n    - ./arb-executable2 --arg value2\n  cwd: ../mydir/arbdir')
        except KeyError as err:
            raise KeyNotInContextError(f""cmds.run doesn't exist for {self.name}.\nThe input should look like this in expanded syntax:\ncmds:\n  run:\n    - ./my-executable --arg\n    - subdir/executable --arg1\n  cwd: ./mydir\n\nIf you're passing in a list of commands, each command should be a simple string,\nor a sub-list of commands to run in serial,\nor a dict with a `run` entry:\ncmds:\n  - ./my-executable --arg\n  - run: ./another-executable --arg value\n    cwd: ../mydir/subdir\n  - run:\n      - ./arb-executable1 --arg value1\n      - [./arb-executable2.1, ./arb-executable2.2]\n    cwd: ../mydir/arbdir\n  - [./arb-executable3.1, ./arb-executable3.2]"") from err
        is_save = pypyr.utils.types.cast_to_bool(cmd_input.get('save', False))
        cwd = cmd_input.get('cwd')
        is_bytes = cmd_input.get('bytes')
        is_text = not is_bytes if is_save else False
        stdout = cmd_input.get('stdout')
        stderr = cmd_input.get('stderr')
        if is_save:
            if stderr or stderr:
                raise ContextError(""You can't set `stdout` or `stderr` when `save` is True."")
        encoding = cmd_input.get('encoding')
        append = cmd_input.get('append', False)
        is_shell_override = cmd_input.get('shell', None)
        is_shell = self.is_shell if is_shell_override is None else is_shell_override
        return Command(cmd=cmd, is_shell=is_shell, cwd=cwd, is_save=is_save, is_text=is_text, stdout=stdout, stderr=stderr, encoding=encoding, append=append)

    def run_step(self) -> None:
        """"""Spawn subprocesses to run the commands asynchronously.

        If cmd.is_save==True, save aggregate result of all commands to context
        'cmdOut'.

        cmdOut will be a list of pypyr.subproc.SubprocessResult or Exception
        objects, in order executed.

        SubprocessResult has the following properties:
        cmd: the cmd/args executed
        returncode: 0
        stdout: 'stdout str here. None if empty.'
        stderr: 'stderr str here. None if empty.'
        """"""
        try:
            self.commands.run()
        finally:
            if self.commands.is_save:
                self.logger.debug('saving results to cmdOut')
                self.context['cmdOut'] = self.commands.results
            else:
                self.logger.debug('save is False: not saving results to cmdOut')","
class AsyncCmdStep:
    '''A pypyr step to run executables/commands concurrently as a subprocess.
    This models a step that takes config like this in simple syntax:
        cmds:
            - <<cmd string 1>>
            - <<cmd string 2>>
    All the commands will run concurrently, in parallel.
    OR, expanded syntax is as a dict
        cmds:
            run: list[str | list[str]]. mandatory. command + args to execute.
                If list entry is another list[str], the sub-list will run in
                serial.
            save: bool. defaults False. save output to cmdOut. Treats output
                as text in the system's encoding and removes newlines at end.
            cwd: str/Pathlike. optional. Working directory for these commands.
            bytes (bool): Default False. When `save` return output bytes from
                cmds unaltered, without applying any encoding & text newline
                processing.
            encoding (str): Default None. When `save`, decode output with
                this encoding. The default of None uses the system encoding and
                should ""just work"".
            stdout (str | Path): Default None. Write stdout to this file path.
                Special value `/dev/null` writes to the system null device.
            stderr (str | Path): Default None. Write stderr to this file path.
                Special value `/dev/null` writes to the system null device.
                Special value `/dev/stdout` redirects err output to stdout.
            append (bool): Default False. When stdout/stderr a file, append
                rather than overwrite. Default is to overwrite.
    In expanded syntax, `run` can be a simple string or a list:
        cmds:
          run:
            - ./my-executable --arg
            - [./another-executable --arg, ./arb-executable arghere]
          save: False
          cwd: ./path/here
    As a list in simplified syntax:
        cmds:
          - my-executable --arg
          - ./another-executable --arg
    Any or all of the list items can use expanded syntax:
        cmds:
          - ./simple-cmd-here --arg1 value
          - run: cmd here
            save: False cwd: ./path/here
          - run:
              - my-executable --arg
              - ./another-executable --arg
            save: True
            cwd: ./path/here
    Any of the list items can in turn be a list. A sub-list will run in serial.
    In this example A, B.1 & C will start concurrently. B.2 will only run once
    B.1 is finished.
        cmds:
            - A
            - [B.1, B.2]
            - C
    If save is True, will save the output to context as cmdOut.
    cmdOut will be a list of pypyr.subproc.SubprocessResult objects, in order
    executed.
    SubprocessResult has the following properties:
    cmd: the cmd/args executed
    returncode: 0
    stdout: 'stdout str here. None if empty.'
    stderr: 'stderr str here. None if empty.'
    cmdOut.returncode is the exit status of the called process. Typically 0
    means OK. A negative value -N indicates that the child was terminated by
    signal N (POSIX only).
    The run_step method does the actual work. init parses the input yaml.
    Attributes:
        logger (logger): Logger instantiated by name of calling step.
        context: (pypyr.context.Context): The current pypyr Context.
        commands (pypyr.subproc.Commands): Commands to run as subprocess.
        is_shell (bool): True if subprocess should run through default shell.
        name (str): Name of calling step. Used for logging output & error
            messages.
    '''
    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        '''Initialize the CmdStep.
        The step config in the context dict in simplified syntax:
            cmd: <<cmd string>>
        OR, as a dict in expanded syntax:
            cmd:
                run: str. mandatory. command + args to execute.
                save: bool. optional. defaults False. save output to cmdOut.
                cwd: str/path. optional. if specified, change the working
                     directory just for the duration of the command.
        `run` can be a single string, or it can be a list of string if there
        are multiple commands to execute with the same settings.
        OR, as a list:
            cmd:
                - my-executable --arg
                - ./another-executable --arg
        Any or all of the list items can be in expanded syntax.
        Args:
            name (str): Unique name for step. Likely __name__ of calling step.
            context (pypyr.context.Context): Look for step config in this
                context instance.
            is_shell (bool): Set to true to execute cmd through the default
                shell.
        '''
        pass
    def create_command(self, cmd_input: Mapping) -> Command:
        '''Create pypyr.aio.subproc.Command object from expanded step input.'''
        pass
    def run_step(self) -> None:
        '''Spawn subprocesses to run the commands asynchronously.
        If cmd.is_save==True, save aggregate result of all commands to context
        'cmdOut'.
        cmdOut will be a list of pypyr.subproc.SubprocessResult or Exception
        objects, in order executed.
        SubprocessResult has the following properties:
        cmd: the cmd/args executed
        returncode: 0
        stdout: 'stdout str here. None if empty.'
        stderr: 'stderr str here. None if empty.'
        '''
        pass",snippet_95,4,"import asyncio
import logging
import os
import shlex
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Iterable, List, Mapping, Optional, Tuple, Union


@dataclass
class _CmdSpec:
    cmd: Union[str, List[str]]
    save: bool = False
    cwd: Optional[Union[str, os.PathLike]] = None
    is_bytes: bool = False
    encoding: Optional[str] = None
    stdout_path: Optional[Union[str, os.PathLike]] = None
    stderr_path: Optional[Union[str, os.PathLike]] = None
    append: bool = False


@dataclass
class _SubprocessResult:
    cmd: Union[str, List[str]]
    returncode: int
    stdout: Optional[Union[str, bytes]]
    stderr: Optional[Union[str, bytes]]


class AsyncCmdStep:
    '''A pypyr step to run executables/commands concurrently as a subprocess.
    This models a step that takes config like this in simple syntax:
        cmds:
            - <<cmd string 1>>
            - <<cmd string 2>>
    All the commands will run concurrently, in parallel.
    OR, expanded syntax is as a dict
        cmds:
            run: list[str | list[str]]. mandatory. command + args to execute.
                If list entry is another list[str], the sub-list will run in
                serial.
            save: bool. defaults False. save output to cmdOut. Treats output
                as text in the system's encoding and removes newlines at end.
            cwd: str/Pathlike. optional. Working directory for these commands.
            bytes (bool): Default False. When `save` return output bytes from
                cmds unaltered, without applying any encoding & text newline
                processing.
            encoding (str): Default None. When `save`, decode output with
                this encoding. The default of None uses the system encoding and
                should ""just work"".
            stdout (str | Path): Default None. Write stdout to this file path.
                Special value `/dev/null` writes to the system null device.
            stderr (str | Path): Default None. Write stderr to this file path.
                Special value `/dev/null` writes to the system null device.
                Special value `/dev/stdout` redirects err output to stdout.
            append (bool): Default False. When stdout/stderr a file, append
                rather than overwrite. Default is to overwrite.
    In expanded syntax, `run` can be a simple string or a list:
        cmds:
          run:
            - ./my-executable --arg
            - [./another-executable --arg, ./arb-executable arghere]
          save: False
          cwd: ./path/here
    As a list in simplified syntax:
        cmds:
          - my-executable --arg
          - ./another-executable --arg
    Any or all of the list items can use expanded syntax:
        cmds:
          - ./simple-cmd-here --arg1 value
          - run: cmd here
            save: False cwd: ./path/here
          - run:
              - my-executable --arg
              - ./another-executable --arg
            save: True
            cwd: ./path/here
    Any of the list items can in turn be a list. A sub-list will run in serial.
    In this example A, B.1 & C will start concurrently. B.2 will only run once
    B.1 is finished.
        cmds:
            - A
            - [B.1, B.2]
            - C
    If save is True, will save the output to context as cmdOut.
    cmdOut will be a list of pypyr.subproc.SubprocessResult objects, in order
    executed.
    SubprocessResult has the following properties:
    cmd: the cmd/args executed
    returncode: 0
    stdout: 'stdout str here. None if empty.'
    stderr: 'stderr str here. None if empty.'
    cmdOut.returncode is the exit status of the called process. Typically 0
    means OK. A negative value -N indicates that the child was terminated by
    signal N (POSIX only).
    The run_step method does the actual work. init parses the input yaml.
    Attributes:
        logger (logger): Logger instantiated by name of calling step.
        context: (pypyr.context.Context): The current pypyr Context.
        commands (pypyr.subproc.Commands): Commands to run as subprocess.
        is_shell (bool): True if subprocess should run through default shell.
        name (str): Name of calling step. Used for logging output & error
            messages.
    '''
    def __init__(self, name: str, context: Mapping[str, Any], is_shell: bool=False) -> None:
        '''Initialize the CmdStep.
        The step config in the context dict in simplified syntax:
            cmd: <<cmd string>>
        OR, as a dict in expanded syntax:
            cmd:
                run: str. mandatory. command + args to execute.
                save: bool. optional. defaults False. save output to cmdOut.
                cwd: str/path. optional. if specified, change the working
                     directory just for the duration of the command.
        `run` can be a single string, or it can be a list of string if there
        are multiple commands to execute with the same settings.
        OR, as a list:
            cmd:
                - my-executable --arg
                - ./another-executable --arg
        Any or all of the list items can be in expanded syntax.
        Args:
            name (str): Unique name for step. Likely __name__ of calling step.
            context (pypyr.context.Context): Look for step config in this
                context instance.
            is_shell (bool): Set to true to execute cmd through the default
                shell.
        '''
        self.name = name
        self.context = context
        self.is_shell = is_shell
        self.logger = logging.getLogger(name)

        cfg = context.get('cmds')
        if cfg is None:
            # fallback to older/alternate key
            cfg = context.get('cmd')
        if cfg is None:
            raise KeyError(""AsyncCmdStep requires 'cmds' (or 'cmd') in context."")

        # Parse configuration into groups of commands
        # Each element in self._groups is a list[_CmdSpec] to run in serial.
        # Groups run concurrently with each other.
        self._groups: List[List[_CmdSpec]] = self._parse_cfg(cfg)

    def create_command(self, cmd_input: Mapping) -> _CmdSpec:
        '''Create pypyr.aio.subproc.Command object from expanded step input.'''
        return self._spec_from_mapping(cmd_input)

    def run_step(self) -> None:
        '''Spawn subprocesses to run the commands asynchronously.
        If cmd.is_save==True, save aggregate result of all commands to context
        'cmdOut'.
        cmdOut will be a list of pypyr.subproc.SubprocessResult or Exception
        objects, in order executed.
        SubprocessResult has the following properties:
        cmd: the cmd/args executed
        returncode: 0
        stdout: 'stdout str here. None if empty.'
        stderr: 'stderr str here. None if empty.'
        '''
        results = asyncio.run(self._run_all_groups())
        # Flatten results preserving group order then serial order within group
        flat: List[Union[_SubprocessResult, Exception]] = []
        for grp in results:
            flat.extend(grp)
        # Save if any command had save=True
        if any(spec.save for grp in self._groups for spec in grp):
            # Transform to expected shape: keep attributes as in docstring
            self.context['cmdOut'] = flat

    # ---------------- internal helpers ----------------

    def _parse_cfg(self, cfg: Any) -> List[List[_CmdSpec]]:
        # Top-level can be:
        # - string: single command
        # - list: items of string | list (serial group) | mapping (expanded)
        # - mapping: expanded syntax with run
        if isinstance(cfg, str):
            return [[self._spec_from_mapping({'run': cfg})]]

        if isinstance(cfg, Mapping):
            return self._groups_from_expanded(cfg)

        if isinstance(cfg, Iterable):
            groups: List[List[_CmdSpec]] = []
            for item in cfg:
                if isinstance(item, str):
                    groups.append([self._spec_from_mapping({'run': item})])
                elif isinstance(item, Mapping):
                    groups.extend(self._groups_from_expanded(item))
                elif isinstance(item, Iterable):
                    # serial group: list of items, each can be str or mapping
                    serial_specs: List[_CmdSpec] = []
                    for sub in item:
                        if isinstance(sub, str):
                            serial_specs.append(self._spec_from_mapping({'run': sub}))
                        elif isinstance(sub, Mapping):
                            # allow per-item expanded
                            item_groups = self._groups_from_expanded(sub)
                            # flatten: if groups_from_expanded returns multiple groups,
                            # treat them as serially concatenated here
                            for g in item_groups:
                                serial_specs.extend(g)
                        else:
                            raise TypeError(f""Unsupported sub-item type in serial list: {type(sub)}"")
                    if serial_specs:
                        groups.append(serial_specs)
                else:
                    raise TypeError(f""Unsupported item type in cmds: {type(item)}"")
            return groups

        raise TypeError(f""Unsupported cmds config type: {type(cfg)}"")

    def _groups_from_expanded(self, m: Mapping) -> List[List[_CmdSpec]]:
        if 'run' not in m:
            raise KeyError(""Expanded syntax requires 'run'"")
        base = {k: v for k, v in m.items() if k != 'run'}
        run = m['run']
        groups: List[List[_CmdSpec]] = []
        if isinstance(run, str):
            groups.append([self._spec_from_mapping({'run': run, **base})])
        elif isinstance(run, Iterable):
            # Each element of run can be:
            # - str: a single cmd (concurrent group)
            # - list: serial group of strings or mappings
            # - mapping: a single cmd with overrides
            for elem in run:
                if isinstance(elem, str):
                    groups.append([self._spec_from_mapping({'run': elem, **base})])
                elif isinstance(elem, Mapping):
                    # this allows per-elem overrides
                    merged = dict(base)
                    merged.update(elem)
                    sub_groups = self._groups_from_expanded(merged)
                    groups.extend(sub_groups)
                elif isinstance(elem, Iterable):
                    serial_specs: List[_CmdSpec] = []
                    for sub in elem:
                        if isinstance(sub, str):
                            serial_specs.append(self._spec_from_mapping({'run': sub, **base}))
                        elif isinstance(sub, Mapping):
                            merged = dict(base)
                            merged.update(sub)
                            sub_groups2 = self._groups_from_expanded(merged)
                            for g in sub_groups2:
                                serial_specs.extend(g)
                        else:
                            raise TypeError(f""Unsupported sub-item type in serial list: {type(sub)}"")
                    if serial_specs:
                        groups.append(serial_specs)
                else:
                    raise TypeError(f""Unsupported element type in run: {type(elem)}"")
        else:
            raise TypeError(f""Unsupported type for 'run': {type(run)}"")
        return groups

    def _spec_from_mapping(self, m: Mapping) -> _CmdSpec:
        cmd = m.get('run')
        if cmd is None or not isinstance(cmd, (str, list)):
            raise ValueError(""Command 'run' must be a string or argv list."")
        save = bool(m.get('save', False))
        cwd = m.get('cwd')
        is_bytes = bool(m.get('bytes', False))
        encoding = m.get('encoding')
        stdout_path = m.get('stdout')
        stderr_path = m.get('stderr')
        append = bool(m.get('append', False))
        return _CmdSpec(
            cmd=cmd,
            save=save,
            cwd=cwd,
            is_bytes=is_bytes,
            encoding=encoding,
            stdout_path=stdout_path,
            stderr_path=stderr_path,
            append=append,
        )

    async def _run_all_groups(self) -> List[List[Union[_SubprocessResult, Exception]]]:
        tasks = [asyncio.create_task(self._run_serial_group(group)) for group in self._groups]
        results = await asyncio.gather(*tasks, return_exceptions=False)
        return results

    async def _run_serial_group(self, group: List[_CmdSpec]) -> List[Union[_SubprocessResult, Exception]]:
        out: List[Union[_SubprocessResult, Exception]] = []
        for spec in group:
            try:
                res = await self._exec_one(spec)
            except Exception as ex:
                out.append(ex)
            else:
                out.append(res)
        return out

    def _open_redir(self, path: Optional[Union[str, os.PathLike]], append: bool):
        if not path:
            return None
        if isinstance(path, (str, Path)):
            s = str(path)
            if s == '/dev/null':
                return open(os.devnull, 'ab')
            # '/dev/stdout' handled separately for stderr mapping to stdout
            mode = 'ab' if append else 'wb'
            p = Path(s)
            p.parent.mkdir(parents=True, exist_ok=True)
            return open(p, mode)
        return None

    def _build_argv(self, cmd: Union[str, List[str]]) -> Tuple[Union[str, List[str]], bool]:
        if self.is_shell:
            if isinstance(cmd, list):
                # if shell=True but argv list provided, join into a string
                return ' '.join(shlex.quote(x) for x in cmd), True
            return cmd, True
        else:
            if isinstance(cmd, list):
                return cmd, False
            return shlex.split(cmd), False

    async def _exec_one(self, spec: _CmdSpec) -> _SubprocessResult:
        argv, use_shell = self._build_argv(spec.cmd)
        # stdout/err handling
        # Determine whether to capture for save
        capture_stdout = spec.save or (spec.stderr_path == '/dev/stdout')
        capture_stderr = spec.save

        stdout_file = None
        stderr_file = None
        to_stdout = asyncio.subprocess.PIPE if capture_stdout else None
        to_stderr = asyncio.subprocess.PIPE if capture_stderr else None

        # Special redirects
        if spec.stderr_path == '/dev/stdout':
            to_stderr = asyncio.subprocess.STDOUT
            capture_stderr = False  # combined in stdout

        # File outputs
        if spec.stdout_path and spec.stdout_path != '/dev/null':
            # We will tee: capture if needed, and also write to file after
            stdout_file = self._open_redir(spec.stdout_path, spec.append)
            if not capture_stdout:
                # direct write to file
                to_stdout = stdout_file

        if spec.stderr_path and spec.stderr_path not in ('/dev/null', '/dev/stdout'):
            stderr_file = self._open_redir(spec.stderr_path, spec.append)
            if not capture_stderr:
                to_stderr = stderr_file

        if spec.stdout_path == '/dev/null':
            to_stdout = asyncio.subprocess.DEVNULL
        if spec.stderr_path == '/dev/null':
            to_stderr = asyncio.subprocess.DEVNULL

        proc = await asyncio.create_subprocess_shell(
            argv if use_shell else self._argv_to_str(argv),
            stdout=to_stdout,
            stderr=to_stderr,
            cwd=str(spec.cwd) if spec.cwd else None,
        ) if use_shell else await asyncio.create_subprocess_exec(
            *argv, stdout=to_stdout, stderr=to_stderr,
            cwd=str(spec.cwd) if spec.cwd else None,
        )

        stdout_bytes, stderr_bytes = await proc.communicate()

        # If we directed to files, but also captured, tee now
        if stdout_file and stdout_bytes is not None:
            try:
                stdout_file.write(stdout_bytes)
                stdout_file.flush()
            finally:
                stdout_file.close()
        elif stdout_file:
            stdout_file.close()

        if stderr_file and stderr_bytes is not None:
            try:
                stderr_file.write(stderr_bytes)
                stderr_file.flush()
            finally:
                stderr_file.close()
        elif stderr_file:
            stderr_file.close()

        # Prepare result
        if spec.save:
            if spec.is_bytes:
                out_stdout = stdout_bytes if stdout_bytes not in (None, b'') else None
                out_stderr = None
                if spec.stderr_path != '/dev/stdout':
                    out_stderr = stderr_bytes if stderr_bytes not in (None, b'') else None
            else:
                enc = spec.encoding or sys.getdefaultencoding()
                out_stdout = None
                if stdout_bytes is not None:
                    out_stdout = stdout_bytes.decode(enc, errors='replace')
                    out_stdout = out_stdout.rstrip('\r\n') if out_stdout != '' else None
                out_stderr = None
                if spec.stderr_path != '/dev/stdout':
                    if stderr_bytes is not None:
                        out_stderr = stderr_bytes.decode(enc, errors='replace')
                        out_stderr = out_stderr.rstrip('\r\n') if out_stderr != '' else None
        else:
            out_stdout = None
            out_stderr = None

        return _SubprocessResult(
            cmd=spec.cmd,
            returncode=proc.returncode,
            stdout=out_stdout,
            stderr=out_stderr
        )

    def _argv_to_str(self, argv: Union[str, List[str]]) -> str:
        if isinstance(argv, str):
            return argv
        return ' '.join(shlex.quote(x) for x in argv)"
167547,automl/HpBandSter,automl_HpBandSter/hpbandster/optimizers/learning_curve_models/base.py,hpbandster.optimizers.learning_curve_models.base.LCModel,"class LCModel:
    """"""
        base class for simple learning curve models
    """"""

    def fit(self, times, losses, configs=None):
        """"""
            function to train the model on the observed data

            Parameters:
            -----------

            times: list
                list of numpy arrays of the timesteps for each curve
            losses: list
                list of numpy arrays of the loss (the actual learning curve)
            configs: list or None
                list of the configurations for each sample. Each element
                has to be a numpy array. Set to None, if no configuration
                information is available.
        """"""
        raise NotImplementedError()

    def predict_unseen(self, times, config):
        """"""
            predict the loss of an unseen configuration

            Parameters:
            -----------

            times: numpy array
                times where to predict the loss
            config: numpy array
                the numerical representation of the config

            Returns:
            --------

            mean and variance prediction at input times for the given config
        """"""
        raise NotImplementedError()

    def extend_partial(self, times, obs_times, obs_losses, config=None):
        """"""
            extends a partially observed curve

            Parameters:
            -----------

            times: numpy array
                times where to predict the loss
            obs_times: numpy array
                times where the curve has already been observed
            obs_losses: numpy array
                corresponding observed losses
            config: numpy array
                numerical reperesentation of the config; None if no config
                information is available

            Returns:
            --------

            mean and variance prediction at input times


        """"""","class LCModel:
    
    def fit(self, times, losses, configs=None):
        
        pass
    def predict_unseen(self, times, config):
        '''
            predict the loss of an unseen configuration
            Parameters:
            -----------
            times: numpy array
                times where to predict the loss
            config: numpy array
                the numerical representation of the config
            Returns:
            --------
            mean and variance prediction at input times for the given config
        '''
        pass
    def extend_partial(self, times, obs_times, obs_losses, config=None):
        
        pass",snippet_98,4,"import numpy as np

class LCModel:
    def __init__(self, degree=3, reg=1e-6):
        self.degree = int(degree)
        self.reg = float(reg)
        self._fitted = False
        self._w = None
        self._sigma2 = None
        self._L = None  # Cholesky factor of (X^T X + lambda I)
        self._A = None  # (X^T X + lambda I)
        self._Xty = None
        self.config_dim = 0

    def _as_1d(self, x):
        x = np.asarray(x)
        if x.ndim == 0:
            x = x[None]
        return x.reshape(-1)

    def _time_features(self, t):
        t = self._as_1d(t)
        # Polynomial basis 0..degree
        powers = [np.ones_like(t)]
        for k in range(1, self.degree + 1):
            powers.append(t ** k)
        return np.vstack(powers).T  # shape (n, degree+1)

    def _build_z(self, configs):
        if configs is None:
            if self.config_dim == 0:
                return np.ones((1, 1))  # will be repeated via kron with broadcasting logic
            else:
                raise ValueError(""Model expects configs of dim {} but got None"".format(self.config_dim))
        c = np.asarray(configs)
        if c.ndim == 1:
            c = c[None, :]
        if self._fitted and c.shape[1] != self.config_dim:
            raise ValueError(""Config dim mismatch: expected {}, got {}"".format(self.config_dim, c.shape[1]))
        # z = [1, c...]
        ones = np.ones((c.shape[0], 1))
        return np.hstack([ones, c])

    def _design_matrix(self, times, configs):
        t = self._as_1d(times)
        Phi_t = self._time_features(t)  # (n, p)
        p = Phi_t.shape[1]
        if configs is None and self.config_dim == 0:
            # Only time features
            return Phi_t
        # Ensure configs row-wise align with times
        cfg = np.asarray(configs) if configs is not None else None
        if cfg is None:
            raise ValueError(""configs must be provided"")
        if cfg.ndim == 1:
            # single config for all times
            cfg = np.tile(cfg, (len(t), 1))
        if cfg.shape[0] == 1 and len(t) > 1:
            cfg = np.tile(cfg, (len(t), 1))
        if cfg.shape[0] != len(t):
            raise ValueError(""configs rows ({}) must match times length ({}) or be 1"".format(cfg.shape[0], len(t)))
        Z = self._build_z(cfg)  # (n, d+1)
        # Kronecker row-wise: for each i, kron(Z[i], Phi_t[i])
        # Efficient construction using outer then reshaping
        n, dz = Z.shape
        X = np.einsum('ij,ik->ijk', Z, Phi_t).reshape(n, dz * p)
        return X

    def fit(self, times, losses, configs=None):
        times = self._as_1d(times)
        y = self._as_1d(losses)
        if times.shape[0] != y.shape[0]:
            raise ValueError(""times and losses must have the same length"")
        if configs is None:
            self.config_dim = 0
        else:
            c = np.asarray(configs)
            if c.ndim == 1:
                self.config_dim = c.shape[0]
            else:
                self.config_dim = c.shape[1]
        X = self._design_matrix(times, configs)
        n_features = X.shape[1]
        A = X.T @ X
        A_reg = A + self.reg * np.eye(n_features)
        Xty = X.T @ y
        # Solve for weights
        try:
            L = np.linalg.cholesky(A_reg)
            w = np.linalg.solve(L.T, np.linalg.solve(L, Xty))
        except np.linalg.LinAlgError:
            w = np.linalg.solve(A_reg, Xty)
            # Recompute a stable Cholesky if possible by adding jitter
            jitter = 1e-10
            for _ in range(5):
                try:
                    L = np.linalg.cholesky(A_reg + jitter * np.eye(n_features))
                    break
                except np.linalg.LinAlgError:
                    jitter *= 10
            else:
                L = None
            A_reg = A_reg + (jitter if L is not None else 0.0) * np.eye(n_features)
        # Residual variance
        y_hat = X @ w
        dof = max(1, X.shape[0] - n_features)
        sigma2 = float(np.sum((y - y_hat) ** 2) / dof)
        self._w = w
        self._sigma2 = sigma2 if sigma2 > 0 else 1e-12
        self._A = A_reg
        self._Xty = Xty
        self._L = L
        self._fitted = True
        return self

    def _predict_core(self, X, w, L, sigma2):
        mean = X @ w
        if L is None:
            # Fallback using solve on A
            A_inv_XT = np.linalg.solve(self._A, X.T)
            q = np.sum(X * A_inv_XT.T, axis=1)
        else:
            # var = sigma2 * (1 + x^T A^{-1} x)
            # compute v = solve(L, x^T), q = sum(v^2)
            v = np.linalg.solve(L, X.T)
            q = np.sum(v * v, axis=0)
        var = sigma2 * (1.0 + q)
        return mean, var

    def predict_unseen(self, times, config):
        '''
            predict the loss of an unseen configuration
            Parameters:
            -----------
            times: numpy array
                times where to predict the loss
            config: numpy array
                the numerical representation of the config
            Returns:
            --------
            mean and variance prediction at input times for the given config
        '''
        if not self._fitted:
            raise RuntimeError(""Model is not fitted yet."")
        times = self._as_1d(times)
        cfg = np.asarray(config)
        if cfg.ndim == 0:
            cfg = np.array([cfg])
        if self.config_dim == 0 and cfg.size != 0:
            raise ValueError(""Model was fit without configs; config should be empty."")
        if self.config_dim != 0 and cfg.size != self.config_dim:
            raise ValueError(""Config dim mismatch: expected {}, got {}"".format(self.config_dim, cfg.size))
        X = self._design_matrix(times, cfg)
        mean, var = self._predict_core(X, self._w, self._L, self._sigma2)
        return mean, var

    def extend_partial(self, times, obs_times, obs_losses, config=None):
        if not self._fitted:
            raise RuntimeError(""Model is not fitted yet."")
        times = self._as_1d(times)
        obs_times = self._as_1d(obs_times)
        obs_losses = self._as_1d(obs_losses)
        if obs_times.shape[0] != obs_losses.shape[0]:
            raise ValueError(""obs_times and obs_losses must have the same length"")
        # Build design matrices
        if self.config_dim == 0:
            cfg = None
        else:
            if config is None:
                raise ValueError(""config must be provided for extend_partial when model uses configs"")
            cfg = np.asarray(config)
            if cfg.ndim == 0:
                cfg = np.array([cfg])
            if cfg.size != self.config_dim:
                raise ValueError(""Config dim mismatch: expected {}, got {}"".format(self.config_dim, cfg.size))
        X_new = self._design_matrix(times, cfg)
        X_obs = self._design_matrix(obs_times, cfg if cfg is None else cfg)
        # Posterior update with partial observations:
        # Prec_post = A + X_obs^T X_obs
        # mean_post = Prec_post^{-1} (Xty + X_obs^T y_obs)
        A_post = self._A + (X_obs.T @ X_obs)
        b_post = self._Xty + (X_obs.T @ obs_losses)
        # Solve for posterior mean and its Cholesky
        try:
            L_post = np.linalg.cholesky(A_post)
            w_post = np.linalg.solve(L_post.T, np.linalg.solve(L_post, b_post))
        except np.linalg.LinAlgError:
            w_post = np.linalg.solve(A_post, b_post)
            L_post = None
        mean, var = self._predict_core(X_new, w_post, L_post if L_post is not None else None, self._sigma2)
        return mean, var"
168574,romanz/trezor-agent,romanz_trezor-agent/libagent/age/client.py,libagent.age.client.Client,"from cryptography.hazmat.primitives.kdf.hkdf import HKDF
from cryptography.hazmat.primitives import hashes

class Client:
    """"""Sign messages and get public keys from a hardware device.""""""

    def __init__(self, device):
        """"""C-tor.""""""
        self.device = device

    def pubkey(self, identity, ecdh=False):
        """"""Return public key as VerifyingKey object.""""""
        with self.device:
            pubkey = bytes(self.device.pubkey(ecdh=ecdh, identity=identity))
            assert len(pubkey) == 32
            return pubkey

    def ecdh(self, identity, peer_pubkey):
        """"""Derive shared secret using ECDH from peer public key.""""""
        log.info('please confirm AGE decryption on %s for ""%s""...', self.device, identity.to_string())
        with self.device:
            assert len(peer_pubkey) == 32
            result, self_pubkey = self.device.ecdh_with_pubkey(pubkey=b'@' + peer_pubkey, identity=identity)
            assert result[:1] == b'\x04'
            hkdf = HKDF(algorithm=hashes.SHA256(), length=32, salt=peer_pubkey + self_pubkey, info=b'age-encryption.org/v1/X25519')
            return hkdf.derive(result[1:])","
class Client:
    
    def __init__(self, device):
        '''C-tor.'''
        pass
    def pubkey(self, identity, ecdh=False):
        '''Return public key as VerifyingKey object.'''
        pass
    def ecdh(self, identity, peer_pubkey):
        
        pass",snippet_100,4,"import ecdsa

class Client:
    
    def __init__(self, device):
        '''C-tor.'''
        self.device = device
        self.curve = ecdsa.NIST256p

    def _get_sk(self, identity, ecdh=False):
        keyspace = 'ecdh' if ecdh else 'sign'
        sk = None
        stored = None

        # Try dict-like storage with composite key
        if hasattr(self.device, 'get'):
            try:
                stored = self.device.get((keyspace, identity))
            except TypeError:
                stored = None

        if isinstance(stored, ecdsa.SigningKey):
            sk = stored
        elif isinstance(stored, (bytes, bytearray)):
            sk = ecdsa.SigningKey.from_string(bytes(stored), curve=self.curve)

        # Try device methods
        if sk is None:
            for name in ('get_key', 'get_sk', 'get_signing_key'):
                if hasattr(self.device, name):
                    meth = getattr(self.device, name)
                    try:
                        maybe = meth(identity, ecdh=ecdh)
                    except TypeError:
                        try:
                            maybe = meth(identity)
                        except Exception:
                            continue
                    if isinstance(maybe, ecdsa.SigningKey):
                        sk = maybe
                        break
                    if isinstance(maybe, (bytes, bytearray)):
                        sk = ecdsa.SigningKey.from_string(bytes(maybe), curve=self.curve)
                        break

        # Generate and store if still missing
        if sk is None:
            sk = ecdsa.SigningKey.generate(curve=self.curve)
            # Try dict-like storage
            try:
                if hasattr(self.device, '__setitem__'):
                    self.device[(keyspace, identity)] = sk.to_string()
            except Exception:
                pass
            # Try device setter methods
            for name in ('set_key', 'store_key', 'put_key'):
                if hasattr(self.device, name):
                    setter = getattr(self.device, name)
                    try:
                        setter(identity, sk.to_string(), ecdh=ecdh)
                        break
                    except TypeError:
                        try:
                            setter(identity, sk.to_string())
                            break
                        except Exception:
                            continue

        return sk

    def pubkey(self, identity, ecdh=False):
        '''Return public key as VerifyingKey object.'''
        sk = self._get_sk(identity, ecdh=ecdh)
        return sk.get_verifying_key()

    def ecdh(self, identity, peer_pubkey):
        sk = self._get_sk(identity, ecdh=True)

        # Normalize peer_pubkey to VerifyingKey
        if isinstance(peer_pubkey, ecdsa.VerifyingKey):
            vk = peer_pubkey
        elif isinstance(peer_pubkey, (bytes, bytearray)):
            try:
                vk = ecdsa.VerifyingKey.from_string(bytes(peer_pubkey), curve=self.curve)
            except Exception:
                # Attempt hex-decoding if it failed
                s = bytes(peer_pubkey).decode()
                vk = ecdsa.VerifyingKey.from_string(bytes.fromhex(s), curve=self.curve)
        elif isinstance(peer_pubkey, str):
            s = peer_pubkey.strip()
            if ""BEGIN PUBLIC KEY"" in s or ""BEGIN EC PUBLIC KEY"" in s:
                vk = ecdsa.VerifyingKey.from_pem(s)
            else:
                vk = ecdsa.VerifyingKey.from_string(bytes.fromhex(s), curve=self.curve)
        else:
            raise TypeError(""peer_pubkey must be VerifyingKey, bytes, or str"")

        priv = sk.privkey.secret_multiplier
        point = vk.pubkey.point * priv
        x = point.x()
        size = self.curve.baselen
        return x.to_bytes(size, 'big')"
168596,romanz/trezor-agent,romanz_trezor-agent/libagent/ssh/__init__.py,libagent.ssh.ClosableNamedTemporaryFile,"import tempfile
import os

class ClosableNamedTemporaryFile:
    """"""Creates a temporary file that is not deleted when the file is closed.

    This allows the file to be opened with an exclusive lock, but used by other programs before
    it is deleted
    """"""

    def __init__(self):
        """"""Create a temporary file.""""""
        self.file = tempfile.NamedTemporaryFile(prefix='trezor-ssh-pubkey-', mode='w', delete=False)
        self.name = self.file.name

    def write(self, buf):
        """"""Write `buf` to the file.""""""
        self.file.write(buf)

    def close(self):
        """"""Closes the file, allowing it to be opened by other programs. Does not delete the file.""""""
        self.file.close()

    def __del__(self):
        """"""Deletes the temporary file.""""""
        try:
            os.unlink(self.file.name)
        except OSError:
            log.warning('Failed to delete temporary file: %s', self.file.name)","
class ClosableNamedTemporaryFile:
    
    def __init__(self):
        '''Create a temporary file.'''
        pass
    def write(self, buf):
        '''Write `buf` to the file.'''
        pass
    def close(self):
        
        pass
    def __del__(self):
        
        pass",snippet_101,5,"import os
import tempfile

class ClosableNamedTemporaryFile:
    
    def __init__(self):
        '''Create a temporary file.'''
        self._tmp = tempfile.NamedTemporaryFile(delete=False, mode='wb+')
        self.name = self._tmp.name
        self._closed = False
        self._unlinked = False

    def write(self, buf):
        '''Write `buf` to the file.'''
        if self._closed:
            raise ValueError(""I/O operation on closed file."")
        if isinstance(buf, str):
            buf = buf.encode('utf-8')
        self._tmp.write(buf)
        self._tmp.flush()

    def close(self):
        if not self._closed:
            try:
                self._tmp.close()
            finally:
                self._closed = True

    def __del__(self):
        try:
            if not self._closed:
                try:
                    self._tmp.close()
                except Exception:
                    pass
                self._closed = True
        except Exception:
            pass
        try:
            if not self._unlinked and hasattr(self, 'name') and self.name:
                try:
                    os.unlink(self.name)
                except FileNotFoundError:
                    pass
                except Exception:
                    pass
                finally:
                    self._unlinked = True
        except Exception:
            pass"
168603,romanz/trezor-agent,romanz_trezor-agent/libagent/util.py,libagent.util.ExpiringCache,"import time

class ExpiringCache:
    """"""Simple cache with a deadline.""""""

    def __init__(self, seconds, timer=time.time):
        """"""C-tor.""""""
        self.duration = seconds
        self.timer = timer
        self.value = None
        self.set(None)

    def get(self):
        """"""Returns existing value, or None if deadline has expired.""""""
        if self.timer() > self.deadline:
            self.value = None
        return self.value

    def set(self, value):
        """"""Set new value and reset the deadline for expiration.""""""
        self.deadline = self.timer() + self.duration
        self.value = value","
class ExpiringCache:
    '''Simple cache with a deadline.'''
    def __init__(self, seconds, timer=time.time):
        '''C-tor.'''
        pass
    def get(self):
        '''Returns existing value, or None if deadline has expired.'''
        pass
    def set(self, value):
        
        pass",snippet_102,4,"import time

class ExpiringCache:
    '''Simple cache with a deadline.'''
    def __init__(self, seconds, timer=time.time):
        '''C-tor.'''
        self._seconds = float(seconds)
        self._timer = timer
        self._value = None
        self._deadline = None

    def get(self):
        '''Returns existing value, or None if deadline has expired.'''
        if self._value is None or self._deadline is None:
            return None
        if self._timer() <= self._deadline:
            return self._value
        # Expired
        self._value = None
        self._deadline = None
        return None

    def set(self, value):
        self._value = value
        self._deadline = self._timer() + self._seconds"
168604,romanz/trezor-agent,romanz_trezor-agent/libagent/util.py,libagent.util.Reader,"import struct
import contextlib

class Reader:
    """"""Read basic type objects out of given stream.""""""

    def __init__(self, stream):
        """"""Create a non-capturing reader.""""""
        self.s = stream
        self._captured = None

    def readfmt(self, fmt):
        """"""Read a specified object, using a struct format string.""""""
        size = struct.calcsize(fmt)
        blob = self.read(size)
        obj, = struct.unpack(fmt, blob)
        return obj

    def read(self, size=None):
        """"""Read `size` bytes from stream.""""""
        blob = self.s.read(size)
        if size is not None and len(blob) < size:
            raise EOFError
        if self._captured:
            self._captured.write(blob)
        return blob

    @contextlib.contextmanager
    def capture(self, stream):
        """"""Capture all data read during this context.""""""
        self._captured = stream
        try:
            yield
        finally:
            self._captured = None","
class Reader:
    
    def __init__(self, stream):
        '''Create a non-capturing reader.'''
        pass
    def readfmt(self, fmt):
        '''Read a specified object, using a struct format string.'''
        pass
    def readfmt(self, fmt):
        
        pass
    @contextlib.contextmanager
    def capture(self, stream):
        
        pass",snippet_103,5,"import struct
import contextlib

class Reader:
    
    def __init__(self, stream):
        '''Create a non-capturing reader.'''
        self.stream = stream
        self._capture_stack = []

    def readfmt(self, fmt):
        '''Read a specified object, using a struct format string.'''
        return self._readfmt(fmt)

    def _readfmt(self, fmt):
        size = struct.calcsize(fmt)
        data = self.stream.read(size)
        if data is None or len(data) != size:
            raise EOFError(""Unexpected end of stream while reading formatted data"")
        for cap in self._capture_stack:
            cap.write(data)
        values = struct.unpack(fmt, data)
        return values[0] if len(values) == 1 else values

    def readfmt(self, fmt):
        return self._readfmt(fmt)

    @contextlib.contextmanager
    def capture(self, stream):
        self._capture_stack.append(stream)
        try:
            yield
        finally:
            self._capture_stack.pop()"
171012,indygreg/python-zstandard,indygreg_python-zstandard/zstandard/backend_cffi.py,zstandard.backend_cffi.BufferSegment,"class BufferSegment:
    """"""Represents a segment within a ``BufferWithSegments``.

    This type is essentially a reference to N bytes within a
    ``BufferWithSegments``.

    The object conforms to the buffer protocol.
    """"""

    @property
    def offset(self):
        """"""The byte offset of this segment within its parent buffer.""""""
        raise NotImplementedError()

    def __len__(self):
        """"""Obtain the length of the segment, in bytes.""""""
        raise NotImplementedError()

    def tobytes(self):
        """"""Obtain bytes copy of this segment.""""""
        raise NotImplementedError()","class BufferSegment:
    
    @property
    def offset(self):
        
        pass
    def __len__(self):
        
        pass
    def tobytes(self):
        '''Obtain bytes copy of this segment.'''
        pass",snippet_104,4,"class BufferSegment:
    def __init__(self, data, offset=0, length=None):
        mv = memoryview(data)
        if offset < 0:
            raise ValueError(""offset must be non-negative"")
        if length is None:
            length = len(mv) - offset
        if length < 0:
            raise ValueError(""length must be non-negative"")
        end = offset + length
        if end > len(mv):
            raise ValueError(""offset + length exceeds buffer size"")
        self._base_view = mv
        self._offset = offset
        self._length = length
        self._view = mv[offset:end]
    
    @property
    def offset(self):
        return self._offset

    def __len__(self):
        return self._length

    def tobytes(self):
        '''Obtain bytes copy of this segment.'''
        return self._view.tobytes()"
171015,indygreg/python-zstandard,indygreg_python-zstandard/zstandard/backend_cffi.py,zstandard.backend_cffi.BufferWithSegmentsCollection,"class BufferWithSegmentsCollection:
    """"""A virtual spanning view over multiple BufferWithSegments.

    Instances are constructed from 1 or more :py:class:`BufferWithSegments`
    instances. The resulting object behaves like an ordered sequence whose
    members are the segments within each ``BufferWithSegments``.

    If the object is composed of 2 ``BufferWithSegments`` instances with the
    first having 2 segments and the second have 3 segments, then ``b[0]``
    and ``b[1]`` access segments in the first object and ``b[2]``, ``b[3]``,
    and ``b[4]`` access segments from the second.
    """"""

    def __len__(self):
        """"""The number of segments within all ``BufferWithSegments``.""""""
        raise NotImplementedError()

    def __getitem__(self, i):
        """"""Obtain the ``BufferSegment`` at an offset.""""""
        raise NotImplementedError()","class BufferWithSegmentsCollection:
    '''A virtual spanning view over multiple BufferWithSegments.
    Instances are constructed from 1 or more :py:class:`BufferWithSegments`
    instances. The resulting object behaves like an ordered sequence whose
    members are the segments within each ``BufferWithSegments``.
    If the object is composed of 2 ``BufferWithSegments`` instances with the
    first having 2 segments and the second have 3 segments, then ``b[0]``
    and ``b[1]`` access segments in the first object and ``b[2]``, ``b[3]``,
    and ``b[4]`` access segments from the second.
    '''
    def __len__(self):
        
        pass
    def __getitem__(self, i):
        
        pass",snippet_105,3,"class BufferWithSegmentsCollection:
    '''A virtual spanning view over multiple BufferWithSegments.
    Instances are constructed from 1 or more :py:class:`BufferWithSegments`
    instances. The resulting object behaves like an ordered sequence whose
    members are the segments within each ``BufferWithSegments``.
    If the object is composed of 2 ``BufferWithSegments`` instances with the
    first having 2 segments and the second have 3 segments, then ``b[0]``
    and ``b[1]`` access segments in the first object and ``b[2]``, ``b[3]``,
    and ``b[4]`` access segments from the second.
    '''
    def __init__(self, *buffers):
        if len(buffers) == 1 and hasattr(buffers[0], ""__iter__"") and not hasattr(buffers[0], ""read""):
            self._buffers = list(buffers[0])
        else:
            self._buffers = list(buffers)
        self._starts = [0]
        total = 0
        for b in self._buffers:
            total += len(b)
            self._starts.append(total)
        self._total = total

    def __len__(self):
        return self._total

    def __getitem__(self, i):
        if isinstance(i, slice):
            start, stop, step = i.indices(len(self))
            return [self[j] for j in range(start, stop, step)]
        n = len(self)
        if i < 0:
            i += n
        if i < 0 or i >= n:
            raise IndexError(""index out of range"")
        # binary search over starts
        lo, hi = 0, len(self._starts) - 1
        while lo < hi:
            mid = (lo + hi) // 2
            if self._starts[mid + 1] <= i:
                lo = mid + 1
            elif self._starts[mid] > i:
                hi = mid
            else:
                lo = mid
                break
        buf_idx = lo
        local_idx = i - self._starts[buf_idx]
        return self._buffers[buf_idx][local_idx]"
171185,ssalentin/plip,ssalentin_plip/plip/exchange/xml.py,plip.exchange.xml.XMLStorage,"class XMLStorage:
    """"""Generic class for storing XML data from PLIP XML files.""""""

    @staticmethod
    def getdata(tree, location, force_string=False):
        """"""Gets XML data from a specific element and handles types.""""""
        found = tree.xpath('%s/text()' % location)
        if not found:
            return None
        else:
            data = found[0]
        if force_string:
            return data
        if data == 'True':
            return True
        elif data == 'False':
            return False
        else:
            try:
                return int(data)
            except ValueError:
                try:
                    return float(data)
                except ValueError:
                    return data

    @staticmethod
    def getcoordinates(tree, location):
        """"""Gets coordinates from a specific element in PLIP XML""""""
        return tuple((float(x) for x in tree.xpath('.//%s/*/text()' % location)))","class XMLStorage:
    '''Generic class for storing XML data from PLIP XML files.'''
    @staticmethod
    def getdata(tree, location, force_string=False):
        
        pass
    @staticmethod
    def getcoordinates(tree, location):
        '''Gets coordinates from a specific element in PLIP XML'''
        pass",snippet_106,3,"class XMLStorage:
    '''Generic class for storing XML data from PLIP XML files.'''
    @staticmethod
    def _get_root(tree):
        try:
            # xml.etree.ElementTree.ElementTree
            return tree.getroot()
        except AttributeError:
            # Already an Element
            return tree

    @staticmethod
    def _find(root, location):
        # Accept ElementPath string or a list/tuple of tags
        if isinstance(location, (list, tuple)):
            elem = root
            for tag in location:
                if elem is None:
                    return None
                elem = elem.find(tag)
            return elem
        else:
            return root.find(location)

    @staticmethod
    def _cast_value(text, force_string=False):
        if force_string:
            return '' if text is None else str(text)

        if text is None:
            return None

        s = str(text).strip()
        if s == '':
            return ''

        sl = s.lower()
        if sl in ('true', 'false'):
            return sl == 'true'

        # Try int
        try:
            if s.isdigit() or (s.startswith(('+', '-')) and s[1:].isdigit()):
                return int(s)
        except Exception:
            pass

        # Try float
        try:
            return float(s)
        except Exception:
            pass

        return s

    @staticmethod
    def getdata(tree, location, force_string=False):
        root = XMLStorage._get_root(tree)

        # Support attribute selection if location ends with '@attr' or contains '/@attr'
        attr_name = None
        elem = None

        if isinstance(location, str) and '@' in location:
            # Split at last '/@' or at '@' if no slash
            if '/@' in location:
                path, attr_name = location.rsplit('/@', 1)
            else:
                path, attr_name = location.split('@', 1)
            elem = XMLStorage._find(root, path)
            if elem is None:
                return None
            val = elem.get(attr_name)
            return XMLStorage._cast_value(val, force_string=force_string)
        else:
            elem = XMLStorage._find(root, location)

        if elem is None:
            return None

        # If element has no text but has exactly one attribute, return that attribute
        text = elem.text
        if (text is None or str(text).strip() == '') and len(elem.attrib) == 1 and not force_string:
            only_attr_val = next(iter(elem.attrib.values()))
            return XMLStorage._cast_value(only_attr_val, force_string=force_string)

        return XMLStorage._cast_value(text, force_string=force_string)

    @staticmethod
    def getcoordinates(tree, location):
        '''Gets coordinates from a specific element in PLIP XML'''
        root = XMLStorage._get_root(tree)
        elem = XMLStorage._find(root, location)
        if elem is None:
            return None

        def _to_float(v):
            try:
                return float(str(v).strip())
            except Exception:
                return None

        # Case 1: attributes x, y, z on the element
        if all(k in elem.attrib for k in ('x', 'y', 'z')):
            x = _to_float(elem.attrib.get('x'))
            y = _to_float(elem.attrib.get('y'))
            z = _to_float(elem.attrib.get('z'))
            if None not in (x, y, z):
                return (x, y, z)

        # Case 2: child elements <x>, <y>, <z>
        cx = elem.find('x')
        cy = elem.find('y')
        cz = elem.find('z')
        if all(c is not None for c in (cx, cy, cz)):
            x = _to_float(cx.text)
            y = _to_float(cy.text)
            z = _to_float(cz.text)
            if None not in (x, y, z):
                return (x, y, z)

        # Case 3: text content with three numbers (comma or space separated)
        if elem.text:
            raw = str(elem.text).strip()
            if raw:
                # Try comma-separated first
                if ',' in raw:
                    parts = [p.strip() for p in raw.split(',')]
                else:
                    parts = raw.split()
                if len(parts) == 3:
                    x = _to_float(parts[0])
                    y = _to_float(parts[1])
                    z = _to_float(parts[2])
                    if None not in (x, y, z):
                        return (x, y, z)

        # Case 4: nested element <coord x="""" y="""" z=""""> or similar common naming
        for tag in ('coord', 'center', 'coords', 'position'):
            sub = elem.find(tag)
            if sub is not None:
                # recur once on the subelement
                result = XMLStorage.getcoordinates(sub, '.')
                if result is not None:
                    return result

        return None"
172857,cogeotiff/rio-tiler,rio_tiler/colormap.py,rio_tiler.colormap.ColorMaps,"import attr
from rio_tiler.errors import ColorMapAlreadyRegistered, InvalidColorFormat, InvalidColorMapName, InvalidFormat
import json
import numpy
from typing import Dict, List, Sequence, Tuple, Union
import pathlib
from rio_tiler.types import ColorMapType, DataMaskType, DiscreteColorMapType, GDALColorMapType, IntervalColorMapType

@attr.s(frozen=True)
class ColorMaps:
    """"""Default Colormaps holder.

    Attributes:
        data (dict): colormaps. Defaults to `rio_tiler.colormap.DEFAULTS_CMAPS`.

    """"""
    data: Dict[str, Union[str, pathlib.Path, ColorMapType]] = attr.ib(default=attr.Factory(lambda: DEFAULT_CMAPS_FILES))

    def get(self, name: str) -> ColorMapType:
        """"""Fetch a colormap.

        Args:
            name (str): colormap name.

        Returns
            dict: colormap dictionary.

        """"""
        cmap = self.data.get(name, None)
        if cmap is None:
            raise InvalidColorMapName(f'Invalid colormap name: {name}')
        if isinstance(cmap, (pathlib.Path, str)):
            if isinstance(cmap, str):
                cmap = pathlib.Path(cmap)
            if cmap.suffix == '.npy':
                colormap = numpy.load(cmap)
                assert colormap.shape == (256, 4)
                assert colormap.dtype == numpy.uint8
                cmap_data = {idx: tuple(value) for idx, value in enumerate(colormap)}
            elif cmap.suffix == '.json':
                with cmap.open() as f:
                    cmap_data = json.load(f, object_hook=lambda x: {int(k): parse_color(v) for k, v in x.items()})
                if isinstance(cmap_data, Sequence):
                    cmap_data = [(tuple(inter), parse_color(v)) for inter, v in cmap_data]
            else:
                raise ValueError(f'Not supported {cmap.suffix} extension for ColorMap')
            self.data[name] = cmap_data
            return cmap_data
        return cmap

    def list(self) -> List[str]:
        """"""List registered Colormaps.

        Returns
            list: list of colormap names.

        """"""
        return list(self.data)

    def register(self, custom_cmap: Dict[str, Union[str, pathlib.Path, ColorMapType]], overwrite: bool=False) -> 'ColorMaps':
        """"""Register a custom colormap.

        Args:
            custom_cmap (dict): custom colormap(s) to register.
            overwrite (bool): Overwrite existing colormap with same key. Defaults to False.

        Examples:
            >>> cmap = cmap.register({""acmap"": {0: (0, 0, 0, 0), ...}})

            >>> cmap = cmap.register({""acmap"": ""acmap.npy""})

        """"""
        for name, _ in custom_cmap.items():
            if not overwrite and name in self.data:
                raise ColorMapAlreadyRegistered(f'{name} is already registered. Use force=True to overwrite.')
        return ColorMaps({**self.data, **custom_cmap})","@attr.s(frozen=True)
class ColorMaps:
    '''Default Colormaps holder.
    Attributes:
        data (dict): colormaps. Defaults to `rio_tiler.colormap.DEFAULTS_CMAPS`.
    '''
    def get(self, name: str) -> ColorMapType:
        
        pass
    def list(self) -> List[str]:
        '''List registered Colormaps.
        Returns
            list: list of colormap names.
        '''
        pass
    def register(self, custom_cmap: Dict[str, Union[str, pathlib.Path, ColorMapType]], overwrite: bool=False) -> 'ColorMaps':
        '''Register a custom colormap.
        Args:
            custom_cmap (dict): custom colormap(s) to register.
            overwrite (bool): Overwrite existing colormap with same key. Defaults to False.
        Examples:
            >>> cmap = cmap.register({""acmap"": {0: (0, 0, 0, 0), ...}})
            >>> cmap = cmap.register({""acmap"": ""acmap.npy""})
        '''
        pass",snippet_107,4,"from typing import Dict, List, Union, Mapping, Tuple
import pathlib
import json

import attr
import numpy as np

try:
    # rio-tiler >= 6
    from rio_tiler.colormap import DEFAULTS_CMAPS, ColorMapType
except Exception:  # Fallback typing if rio-tiler types are unavailable at runtime
    ColorMapType = Dict[int, Tuple[int, int, int, int]]  # type: ignore
    DEFAULTS_CMAPS = {}  # type: ignore


@attr.s(frozen=True)
class ColorMaps:
    '''Default Colormaps holder.
    Attributes:
        data (dict): colormaps. Defaults to `rio_tiler.colormap.DEFAULTS_CMAPS`.
    '''
    data: Dict[str, ColorMapType] = attr.ib(factory=lambda: dict(DEFAULTS_CMAPS))

    def get(self, name: str) -> ColorMapType:
        try:
            return self.data[name]
        except KeyError as exc:
            raise KeyError(f""Colormap '{name}' is not registered."") from exc

    def list(self) -> List[str]:
        '''List registered Colormaps.
        Returns
            list: list of colormap names.
        '''
        return sorted(self.data.keys())

    def register(self, custom_cmap: Dict[str, Union[str, pathlib.Path, ColorMapType]], overwrite: bool=False) -> 'ColorMaps':
        '''Register a custom colormap.
        Args:
            custom_cmap (dict): custom colormap(s) to register.
            overwrite (bool): Overwrite existing colormap with same key. Defaults to False.
        Examples:
            >>> cmap = cmap.register({""acmap"": {0: (0, 0, 0, 0), ...}})
            >>> cmap = cmap.register({""acmap"": ""acmap.npy""})
        '''
        new_data: Dict[str, ColorMapType] = dict(self.data)

        for name, value in custom_cmap.items():
            if not overwrite and name in new_data:
                raise KeyError(f""Colormap '{name}' already exists. Use overwrite=True to replace it."")

            cmap: ColorMapType
            if isinstance(value, (str, pathlib.Path)):
                cmap = self._load_colormap_from_path(pathlib.Path(value))
            else:
                cmap = self._normalize_colormap_mapping(value)

            new_data[name] = cmap

        return attr.evolve(self, data=new_data)

    @staticmethod
    def _normalize_colormap_mapping(mapping: Mapping[int, Union[Tuple[int, int, int, int], List[int]]]) -> ColorMapType:
        norm: Dict[int, Tuple[int, int, int, int]] = {}
        for k, v in mapping.items():
            key = int(k)
            if isinstance(v, (list, tuple)) and len(v) == 4:
                r, g, b, a = (int(v[0]), int(v[1]), int(v[2]), int(v[3]))
                norm[key] = (r, g, b, a)
            else:
                raise ValueError(f""Invalid color tuple for index {k}: {v}"")
        return norm

    @staticmethod
    def _load_colormap_from_path(path: pathlib.Path) -> ColorMapType:
        if not path.exists():
            raise FileNotFoundError(f""Colormap file not found: {path}"")

        suffix = path.suffix.lower()
        if suffix == "".npy"":
            arr = np.load(path)
            if arr.ndim != 2 or arr.shape[1] != 4:
                raise ValueError(f""Invalid .npy colormap shape {arr.shape}; expected (N, 4)."")
            arr = np.asarray(arr, dtype=np.uint8)
            return {int(i): (int(r), int(g), int(b), int(a)) for i, (r, g, b, a) in enumerate(arr)}
        elif suffix == "".json"":
            with path.open(""r"", encoding=""utf-8"") as f:
                data = json.load(f)
            if not isinstance(data, dict):
                raise ValueError(""JSON colormap must be a mapping of index to RGBA."")
            # Keys might be strings in JSON; convert to int.
            return ColorMaps._normalize_colormap_mapping({int(k): v for k, v in data.items()})
        else:
            raise ValueError(f""Unsupported colormap file format: {path.suffix}"")"
174678,ipinfo/python,ipinfo_python/ipinfo/details.py,ipinfo.details.Details,"class Details:
    """"""Encapsulates data for single IP address.""""""

    def __init__(self, details):
        """"""Initialize by settings `details` attribute.""""""
        self.details = details

    def __getattr__(self, attr):
        """"""Return attribute if it exists in details array, else return error.""""""
        if attr not in self.details:
            raise AttributeError(f'{attr} is not a valid attribute of Details')
        return self.details[attr]

    @property
    def all(self):
        """"""Return all details as dict.""""""
        return self.details","class Details:
    '''Encapsulates data for single IP address.'''
    def __init__(self, details):
        
        pass
    def __getattr__(self, attr):
        '''Return attribute if it exists in details array, else return error.'''
        pass
    @property
    def all(self):
        '''Return all details as dict.'''
        pass",snippet_108,4,"class Details:
    '''Encapsulates data for single IP address.'''
    def __init__(self, details):
        if details is None:
            details = {}
        elif not isinstance(details, dict):
            try:
                details = dict(details)
            except Exception as e:
                raise TypeError(""details must be a mapping or convertible to dict"") from e
        self._details = details

    def __getattr__(self, attr):
        '''Return attribute if it exists in details array, else return error.'''
        try:
            return self._details[attr]
        except KeyError:
            raise AttributeError(f""'Details' object has no attribute '{attr}'"")

    @property
    def all(self):
        '''Return all details as dict.'''
        return dict(self._details)"
176091,opencobra/cobrapy,opencobra_cobrapy/src/cobra/util/context.py,cobra.util.context.HistoryManager,"from typing import TYPE_CHECKING, Any, Callable, Optional

class HistoryManager:
    """"""
    Define a base context manager.

    It records a list of actions to be taken at a later time.
    This is used to implement context managers that allow temporary
    changes to a `cobra.core.Model`.

    """"""

    def __init__(self, **kwargs) -> None:
        """"""Initialize the class.""""""
        super().__init__(**kwargs)
        self._history = []

    def __call__(self, operation: Callable[[Any], Any]) -> None:
        """"""Add the corresponding operation to the history stack.

        Parameters
        ----------
        operation : callable
            A function to be called at a later time.

        """"""
        self._history.append(operation)

    def reset(self) -> None:
        """"""Trigger executions for all items in the stack in reverse order.""""""
        while self._history:
            entry = self._history.pop()
            entry()

    def size(self) -> int:
        """"""Calculate number of operations on the stack.""""""
        return len(self._history)","
class HistoryManager:
    
    def __init__(self, **kwargs) -> None:
        '''Initialize the class.'''
        pass
    def __call__(self, operation: Callable[[Any], Any]) -> None:
        
        pass
    def reset(self) -> None:
        '''Trigger executions for all items in the stack in reverse order.'''
        pass
    def size(self) -> int:
        '''Calculate number of operations on the stack.'''
        pass",snippet_109,5,"from typing import Any, Callable, List

class HistoryManager:
    
    def __init__(self, **kwargs) -> None:
        '''Initialize the class.'''
        self._stack: List[Callable[[Any], Any]] = []
        self._context: Any = kwargs.get(""context"", None)
        self._raise_on_error: bool = bool(kwargs.get(""raise_on_error"", False))

    def __call__(self, operation: Callable[[Any], Any]) -> None:
        if not callable(operation):
            raise TypeError(""operation must be callable"")
        self._stack.append(operation)

    def reset(self) -> None:
        '''Trigger executions for all items in the stack in reverse order.'''
        while self._stack:
            op = self._stack.pop()
            try:
                op(self._context)
            except Exception:
                if self._raise_on_error:
                    raise

    def size(self) -> int:
        '''Calculate number of operations on the stack.'''
        return len(self._stack)"
176713,adafruit/Adafruit_Blinka,adafruit_Adafruit_Blinka/src/adafruit_blinka/__init__.py,adafruit_blinka.Enum,"class Enum:
    """"""
    Object supporting CircuitPython-style of static symbols
    as seen with Direction.OUTPUT, Pull.UP
    """"""

    def __repr__(self):
        """"""
        Assumes instance will be found as attribute of own class.
        Returns dot-subscripted path to instance
        (assuming absolute import of containing package)
        """"""
        cls = type(self)
        for key in dir(cls):
            if getattr(cls, key) is self:
                return '{}.{}.{}'.format(cls.__module__, cls.__qualname__, key)
        return repr(self)

    @classmethod
    def iteritems(cls):
        """"""
        Inspects attributes of the class for instances of the class
        and returns as key,value pairs mirroring dict#iteritems
        """"""
        for key in dir(cls):
            val = getattr(cls, key)
            if isinstance(cls, val):
                yield (key, val)","class Enum:
    '''
    Object supporting CircuitPython-style of static symbols
    as seen with Direction.OUTPUT, Pull.UP
    '''
    def __repr__(self):
        '''
        Assumes instance will be found as attribute of own class.
        Returns dot-subscripted path to instance
        (assuming absolute import of containing package)
        '''
        pass
    @classmethod
    def iteritems(cls):
        
        pass",snippet_110,3,"class Enum:
    '''
    Object supporting CircuitPython-style of static symbols
    as seen with Direction.OUTPUT, Pull.UP
    '''
    def __repr__(self):
        '''
        Assumes instance will be found as attribute of own class.
        Returns dot-subscripted path to instance
        (assuming absolute import of containing package)
        '''
        # Search for a class (subclass of Enum) that has this instance bound as an attribute
        def all_subclasses(cls):
            seen = set()
            stack = [cls]
            while stack:
                c = stack.pop()
                for sc in c.__subclasses__():
                    if sc not in seen:
                        seen.add(sc)
                        stack.append(sc)
                        yield sc

        # Prefer subclasses; also consider the base Enum in case attributes are placed directly on it
        candidate_classes = list(all_subclasses(Enum)) + [Enum]

        for owner in candidate_classes:
            for name, value in vars(owner).items():
                if value is self:
                    return f""{owner.__module__}.{owner.__name__}.{name}""
        return object.__repr__(self)

    @classmethod
    def iteritems(cls):
        for name, value in vars(cls).items():
            if not name.startswith('_') and isinstance(value, Enum):
                yield name, value"
176756,adafruit/Adafruit_Blinka,adafruit_Adafruit_Blinka/src/adafruit_blinka/microcontroller/nova/__init__.py,adafruit_blinka.microcontroller.nova.Connection,"class Connection:
    """"""Connection class""""""
    __instance = None

    @staticmethod
    def getInstance():
        """"""Static access method.""""""
        if Connection.__instance is None:
            Connection()
        return Connection.__instance

    def __init__(self):
        """"""Virtually private constructor.""""""
        if Connection.__instance is not None:
            raise Exception('This class is a singleton!')
        from binhoHostAdapter import binhoHostAdapter
        from binhoHostAdapter import binhoUtilities
        devices = binhoUtilities.listAvailableDevices()
        if len(devices) > 0:
            Connection.__instance = binhoHostAdapter.binhoHostAdapter(devices[0])
        else:
            raise RuntimeError('No Binho Nova found!')","class Connection:
    '''Connection class'''
    @staticmethod
    def getInstance():
        
        pass
    def __init__(self):
        
        pass",snippet_111,3,"class Connection:
    '''Connection class'''
    _instance = None

    @staticmethod
    def getInstance():
        if Connection._instance is None:
            Connection._instance = Connection()
        return Connection._instance

    def __init__(self):
        if getattr(self, '_initialized', False):
            return
        self.connected = False
        self.params = {}
        self._initialized = True"
177578,ottogroup/palladium,ottogroup_palladium/palladium/persistence.py,palladium.persistence.FileLikeIO,"from abc import abstractmethod

class FileLikeIO:
    """"""Used by :class:`FileLike` to access low level file handle
    operations.
    """"""

    @abstractmethod
    def open(self, path, mode='r'):
        """"""Return a file handle

        For normal files, the implementation is:

        ```python
        return open(path, mode)
        ```
        """"""

    @abstractmethod
    def exists(self, path):
        """"""Test whether a path exists

        For normal files, the implementation is:

        ```python
        return os.path.exists(path)
        ```
        """"""

    @abstractmethod
    def remove(self, path):
        """"""Remove a file

        For normal files, the implementation is:

        ```python
        os.remove(path)
        ```
        """"""","
class FileLikeIO:
    '''Used by :class:`FileLike` to access low level file handle
    operations.
    '''
    @abstractmethod
    def open(self, path, mode='r'):
        '''Return a file handle
        For normal files, the implementation is:
        ```python
        return open(path, mode)
        ```
        '''
        pass
    @abstractmethod
    def exists(self, path):
        
        pass
    @abstractmethod
    def remove(self, path):
        
        pass",snippet_113,4,"import os
import builtins

class FileLikeIO:
    '''Used by :class:`FileLike` to access low level file handle
    operations.
    '''
    def open(self, path, mode='r'):
        '''Return a file handle
        For normal files, the implementation is:
        ```python
        return open(path, mode)
        ```
        '''
        return builtins.open(path, mode)

    def exists(self, path):
        return os.path.exists(path)

    def remove(self, path):
        os.remove(path)"
179419,python-constraint/python-constraint,python-constraint_python-constraint/constraint/domain.py,constraint.domain.Variable,"class Variable:
    """"""Helper class for variable definition.

    Using this class is optional, since any hashable object,
    including plain strings and integers, may be used as variables.
    """"""

    def __init__(self, name):
        """"""Initialization method.

        Args:
            name (string): Generic variable name for problem-specific
                purposes
        """"""
        self.name = name

    def __repr__(self):
        """"""Represents itself with the name attribute.""""""
        return self.name","class Variable:
    '''Helper class for variable definition.
    Using this class is optional, since any hashable object,
    including plain strings and integers, may be used as variables.
    '''
    def __init__(self, name):
        '''Initialization method.
        Args:
            name (string): Generic variable name for problem-specific
                purposes
        '''
        pass
    def __repr__(self):
        '''Represents itself with the name attribute.'''
        pass",snippet_115,3,"class Variable:
    '''Helper class for variable definition.
    Using this class is optional, since any hashable object,
    including plain strings and integers, may be used as variables.
    '''
    def __init__(self, name):
        '''Initialization method.
        Args:
            name (string): Generic variable name for problem-specific
                purposes
        '''
        self.name = name

    def __repr__(self):
        '''Represents itself with the name attribute.'''
        return str(self.name)"
179425,python-constraint/python-constraint,python-constraint_python-constraint/constraint/solvers.py,constraint.solvers.Solver,"class Solver:
    """"""Abstract base class for solvers.""""""
    requires_pickling = False

    def getSolution(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Return one solution for the given problem.

        Args:
            domains (dict): Dictionary mapping variables to their domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        """"""
        msg = f'{self.__class__.__name__} is an abstract class'
        raise NotImplementedError(msg)

    def getSolutions(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Return all solutions for the given problem.

        Args:
            domains (dict): Dictionary mapping variables to domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        """"""
        msg = f'{self.__class__.__name__} provides only a single solution'
        raise NotImplementedError(msg)

    def getSolutionIter(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Return an iterator for the solutions of the given problem.

        Args:
            domains (dict): Dictionary mapping variables to domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        """"""
        msg = f""{self.__class__.__name__} doesn't provide iteration""
        raise NotImplementedError(msg)","class Solver:
    
    def getSolution(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        '''Return one solution for the given problem.
        Args:
            domains (dict): Dictionary mapping variables to their domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        '''
        pass
    def getSolutions(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        
        pass
    def getSolutionIter(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        
        pass",snippet_116,4,"class Solver:
    
    def getSolution(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        for sol in self.getSolutionIter(domains, constraints, vconstraints):
            return sol
        return None

    def getSolutions(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        return list(self.getSolutionIter(domains, constraints, vconstraints))

    def getSolutionIter(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        vars_list = list(domains.keys())
        doms = {v: list(domains[v]) for v in vars_list}

        def is_consistent(var, value, assignment):
            related = vconstraints.get(var, [])
            for constraint, cvars in related:
                vals = []
                all_assigned = True
                for cv in cvars:
                    if cv == var:
                        vals.append(value)
                    elif cv in assignment:
                        vals.append(assignment[cv])
                    else:
                        all_assigned = False
                        break
                if all_assigned:
                    try:
                        ok = constraint(*vals)
                    except TypeError:
                        ok = constraint(vals)
                    if not ok:
                        return False
            return True

        def select_unassigned_variable(assignment):
            unassigned = [v for v in vars_list if v not in assignment]
            # MRV heuristic
            return min(unassigned, key=lambda v: len(doms[v]))

        def backtrack(assignment):
            if len(assignment) == len(vars_list):
                yield dict(assignment)
                return
            var = select_unassigned_variable(assignment)
            for value in doms[var]:
                if is_consistent(var, value, assignment):
                    assignment[var] = value
                    yield from backtrack(assignment)
                    del assignment[var]

        return backtrack({})"
179445,maroba/findiff,findiff/pde.py,findiff.pde.PDE,"import numpy as np
from scipy.sparse.linalg import spsolve
import scipy.sparse as sparse

class PDE:
    """"""
    Representation of a partial differential equation.
    """"""

    def __init__(self, lhs, rhs, bcs):
        """"""
        Initializes the PDE.

        You need to specify the left hand side (lhs) in terms of derivatives
        as well as the right hand side in terms of an array.

        Parameters
        ----------
        lhs: FinDiff object or combination of FinDiff objects
            the left hand side of the PDE
        rhs: numpy.ndarray
            the right hand side of the PDE
        bcs: BoundaryConditions
            the boundary conditions for the PDE

        """"""
        self.lhs = lhs
        self.rhs = rhs
        self.bcs = bcs
        self._L = None

    def solve(self):
        """"""
        Solves the PDE.

        Returns
        -------
        out: numpy.ndarray
            Array with the solution of the PDE.
        """"""
        shape = self.bcs.shape
        if self._L is None:
            self._L = self.lhs.matrix(shape)
        L = sparse.lil_matrix(self._L)
        f = self.rhs.reshape(-1, 1)
        nz = list(self.bcs.row_inds())
        L[nz, :] = self.bcs.lhs[nz, :]
        f[nz] = np.array(self.bcs.rhs[nz].toarray()).reshape(-1, 1)
        L = sparse.csr_matrix(L)
        return spsolve(L, f).reshape(shape)","
class PDE:
    
    def __init__(self, lhs, rhs, bcs):
        '''
        Initializes the PDE.
        You need to specify the left hand side (lhs) in terms of derivatives
        as well as the right hand side in terms of an array.
        Parameters
        ----------
        lhs: FinDiff object or combination of FinDiff objects
            the left hand side of the PDE
        rhs: numpy.ndarray
            the right hand side of the PDE
        bcs: BoundaryConditions
            the boundary conditions for the PDE
        '''
        pass
    def solve(self):
        
        pass",snippet_117,3,"import numpy as np

try:
    import scipy.sparse as sp
    import scipy.sparse.linalg as spla
except Exception:  # allow operation without scipy at import time
    sp = None
    spla = None


class PDE:
    
    def __init__(self, lhs, rhs, bcs):
        '''
        Initializes the PDE.
        You need to specify the left hand side (lhs) in terms of derivatives
        as well as the right hand side in terms of an array.
        Parameters
        ----------
        lhs: FinDiff object or combination of FinDiff objects
            the left hand side of the PDE
        rhs: numpy.ndarray
            the right hand side of the PDE
        bcs: BoundaryConditions
            the boundary conditions for the PDE
        '''
        self.lhs = lhs
        self.rhs = np.asarray(rhs)
        self.shape = self.rhs.shape
        self.size = int(np.prod(self.shape)) if self.shape else 1
        self.bcs = bcs
        self.A = None
        self.b = None
        self.u = None

    def _build_matrix(self):
        lhs = self.lhs
        shape = self.shape

        # Try common APIs to obtain a system matrix
        A = None
        # scipy sparse preferred if available
        if hasattr(lhs, ""sparse_matrix""):
            try:
                A = lhs.sparse_matrix(shape)
            except TypeError:
                # some implementations expect unpacked shape
                A = lhs.sparse_matrix(*shape)
        elif hasattr(lhs, ""matrix""):
            try:
                A = lhs.matrix(shape)
            except TypeError:
                A = lhs.matrix(*shape)
        elif hasattr(lhs, ""to_matrix""):
            try:
                A = lhs.to_matrix(shape)
            except TypeError:
                A = lhs.to_matrix(*shape)

        if A is None:
            raise TypeError(""Cannot build system matrix from lhs; expected an object with sparse_matrix/matrix/to_matrix methods"")

        # Normalize matrix type
        if sp is not None and sp.issparse(A):
            A = A.tocsr()
        else:
            A = np.array(A)

        return A

    def _apply_bcs(self, A, b):
        bcs = self.bcs
        if bcs is None:
            return A, b

        # Support different styles of boundary condition applicators
        # Expected to return possibly modified (A, b)
        if hasattr(bcs, ""apply""):
            try:
                out = bcs.apply(A, b, rhs_shape=self.shape)
            except TypeError:
                out = bcs.apply(A, b)
        elif hasattr(bcs, ""apply_to""):
            out = bcs.apply_to(A, b)
        elif isinstance(bcs, (list, tuple)):
            outA, outb = A, b
            for bc in bcs:
                if hasattr(bc, ""apply""):
                    try:
                        outA, outb = bc.apply(outA, outb, rhs_shape=self.shape)
                    except TypeError:
                        outA, outb = bc.apply(outA, outb)
                elif hasattr(bc, ""apply_to""):
                    outA, outb = bc.apply_to(outA, outb)
                else:
                    raise TypeError(""Boundary condition element does not provide apply/apply_to"")
            out = (outA, outb)
        else:
            raise TypeError(""Unsupported BoundaryConditions object; expected apply/apply_to or list of such"")

        if not isinstance(out, tuple) or len(out) != 2:
            raise ValueError(""BoundaryConditions.apply/apply_to must return (A, b)"")
        return out

    def _solve_linear_system(self, A, b):
        # Prefer sparse solvers when possible
        if sp is not None and sp.issparse(A):
            if spla is None:
                raise ImportError(""scipy is required to solve sparse systems"")
            return spla.spsolve(A, b)
        # Support LinearOperator (iterative)
        if spla is not None and isinstance(A, spla.LinearOperator):
            x, info = spla.gmres(A, b)
            if info != 0:
                raise RuntimeError(f""Iterative solver did not converge, info={info}"")
            return x
        # Dense solve
        return np.linalg.solve(np.asarray(A), b)

    def solve(self):
        A = self._build_matrix()
        b = np.asarray(self.rhs, dtype=float).reshape(-1)

        A, b = self._apply_bcs(A, b)

        x = self._solve_linear_system(A, b)

        self.A = A
        self.b = b
        self.u = x.reshape(self.shape)
        return self.u"
186773,sphinx-gallery/sphinx-gallery,sphinx_gallery/_dummy/__init__.py,sphinx_gallery._dummy.DummyClass,"class DummyClass:
    """"""Dummy class for testing method resolution.""""""

    def run(self):
        """"""Do nothing.""""""
        pass

    @property
    def prop(self):
        """"""Property.""""""
        return 'Property'","class DummyClass:
    
    def run(self):
        '''Do nothing.'''
        pass
    @property
    def prop(self):
        '''Property.'''
        pass",snippet_119,3,"class DummyClass:
    def __init__(self, value=None):
        self._prop = value

    def run(self):
        """"""Do nothing.""""""
        return None

    @property
    def prop(self):
        """"""Property.""""""
        return self._prop"
186774,sphinx-gallery/sphinx-gallery,sphinx_gallery/_dummy/nested.py,sphinx_gallery._dummy.nested.NestedDummyClass,"class NestedDummyClass:
    """"""Nested dummy class for testing method resolution.""""""

    def run(self):
        """"""Do nothing.""""""
        pass

    @property
    def prop(self):
        """"""Property.""""""
        return 'Property'","class NestedDummyClass:
    
    def run(self):
        '''Do nothing.'''
        pass
    @property
    def prop(self):
        
        pass",snippet_120,3,"class NestedDummyClass:
    def __init__(self, value=None):
        self._prop = value

    def run(self):
        '''Do nothing.'''
        return None

    @property
    def prop(self):
        return self._prop

    @prop.setter
    def prop(self, value):
        self._prop = value

    @prop.deleter
    def prop(self):
        self._prop = None

    def __repr__(self):
        return f""{self.__class__.__name__}(value={self._prop!r})"""
190420,JamesPHoughton/pysd,JamesPHoughton_pysd/pysd/py_backend/external.py,pysd.py_backend.external.Excels,"import numpy as np
from openpyxl import load_workbook
import pandas as pd

class Excels:
    """"""
    Class to save the read Excel files and thus avoid double reading
    """"""
    _Excels, _Excels_opyxl = ({}, {})

    @classmethod
    def read(cls, file_name, tab):
        """"""
        Read the Excel file or return the previously read one
        """"""
        if file_name.joinpath(tab) in cls._Excels:
            return cls._Excels[file_name.joinpath(tab)]
        else:
            read_kwargs = {}
            ext = file_name.suffix.lower()
            if ext in _SPREADSHEET_EXTS:
                read_func = pd.read_excel
                read_kwargs['sheet_name'] = tab
            elif ext == '.csv':
                read_func = pd.read_csv
                if tab and (not tab[0].isalnum()):
                    read_kwargs['sep'] = tab
            else:
                read_func = pd.read_table
                if tab and (not tab[0].isalnum()):
                    read_kwargs['sep'] = tab
            excel = np.array([pd.to_numeric(ex, errors='coerce') for ex in read_func(file_name, header=None, **read_kwargs).values])
            cls._Excels[file_name.joinpath(tab)] = excel
            return excel

    @classmethod
    def read_opyxl(cls, file_name):
        """"""
        Read the Excel file using OpenPyXL or return the previously read one
        """"""
        if file_name in cls._Excels_opyxl:
            return cls._Excels_opyxl[file_name]
        else:
            excel = load_workbook(file_name, read_only=True, data_only=True)
            cls._Excels_opyxl[file_name] = excel
            return excel

    @classmethod
    def clean(cls):
        """"""
        Clean the dictionary of read files
        """"""
        for file in cls._Excels_opyxl.values():
            file.close()
        cls._Excels, cls._Excels_opyxl = ({}, {})","
class Excels:
    
    @classmethod
    def read(cls, file_name, tab):
        
        pass
    @classmethod
    def read_opyxl(cls, file_name):
        
        pass
    @classmethod
    def clean(cls):
        '''
        Clean the dictionary of read files
        '''
        pass",snippet_121,4,"class Excels:
    _workbooks = {}

    @classmethod
    def read(cls, file_name, tab):
        wb = cls.read_opyxl(file_name)
        if isinstance(tab, int):
            try:
                sheet_name = wb.sheetnames[tab]
            except Exception as e:
                raise IndexError(f""Tab index out of range: {tab}"") from e
        else:
            sheet_name = str(tab)
        if sheet_name not in wb.sheetnames:
            raise KeyError(f""Sheet not found: {sheet_name}"")
        ws = wb[sheet_name]
        return [list(row) for row in ws.iter_rows(values_only=True)]

    @classmethod
    def read_opyxl(cls, file_name):
        from openpyxl import load_workbook
        key = str(file_name)
        if key not in cls._workbooks:
            wb = load_workbook(filename=file_name, read_only=True, data_only=True)
            cls._workbooks[key] = wb
        return cls._workbooks[key]

    @classmethod
    def clean(cls):
        '''
        Clean the dictionary of read files
        '''
        for wb in cls._workbooks.values():
            try:
                wb.close()
            except Exception:
                pass
        cls._workbooks.clear()"
222265,box/flaky,box_flaky/flaky/multiprocess_string_io.py,flaky.multiprocess_string_io.MultiprocessingStringIO,"import multiprocessing

class MultiprocessingStringIO:
    """"""
    Provide a StringIO-like interface to the multiprocessing ListProxy. The
    multiprocessing ListProxy needs to be instantiated before the flaky plugin
    is configured, so the list is created as a class variable.
    """"""
    _manager = multiprocessing.Manager()
    proxy = _manager.list()

    def getvalue(self):
        """"""
        Shadow the StringIO.getvalue method.
        """"""
        return ''.join((i for i in self.proxy))

    def writelines(self, content_list):
        """"""
        Shadow the StringIO.writelines method. Ingests a list and
        translates that to a string
        """"""
        for item in content_list:
            self.write(item)

    def write(self, content):
        """"""
        Shadow the StringIO.write method.
        """"""
        content.strip('\n')
        self.proxy.append(content)","
class MultiprocessingStringIO:
    
    def getvalue(self):
        
        pass
    def writelines(self, content_list):
        '''
        Shadow the StringIO.writelines method. Ingests a list and
        translates that to a string
        '''
        pass
    def writelines(self, content_list):
        
        pass",snippet_123,4,"class MultiprocessingStringIO:
    def __init__(self, initial_value=None):
        self._chunks = []
        if initial_value is not None:
            self._chunks.append(str(initial_value))

    def getvalue(self):
        return ''.join(self._chunks)

    def writelines(self, content_list):
        '''
        Shadow the StringIO.writelines method. Ingests a list and
        translates that to a string
        '''
        if content_list is None:
            return
        if isinstance(content_list, (str, bytes)):
            self._chunks.append(content_list.decode() if isinstance(content_list, bytes) else content_list)
            return
        try:
            joined = ''.join(str(item) for item in content_list)
        except TypeError:
            raise TypeError(""writelines expects an iterable of items to be converted to string"")
        self._chunks.append(joined)"
222348,joshspeagle/dynesty,joshspeagle_dynesty/py/dynesty/utils.py,dynesty.utils.DelayTimer,"import time

class DelayTimer:
    """""" Utility class that allows us to detect a certain
    time has passed""""""

    def __init__(self, delay):
        """""" Initialise the time with delay of dt seconds

        Parameters
        ----------

        delay: float
            The number of seconds in the timer
        """"""
        self.delay = delay
        self.last_time = time.time()

    def is_time(self):
        """"""
        Returns true if more than self.dt seconds has passed
        since the initialization or last call of successful is_time()

        Returns
        -------
        ret: bool
             True if specified amout of time has passed since the
             initialization or last successful is_time() call
        """"""
        curt = time.time()
        if curt - self.last_time > self.delay:
            self.last_time = curt
            return True
        return False","
class DelayTimer:
        ''' Utility class that allows us to detect a certain
            time has passed'''
    def __init__(self, delay):
        
        pass
    def is_time(self):
        
        pass",snippet_124,3,"import time

class DelayTimer:
    ''' Utility class that allows us to detect a certain
        time has passed'''
    def __init__(self, delay):
        self.delay = float(delay)
        self._next = time.monotonic() + self.delay

    def is_time(self):
        now = time.monotonic()
        if now >= self._next:
            self._next = now + self.delay
            return True
        return False"
223977,jilljenn/tryalgo,jilljenn_tryalgo/tryalgo/fenwick.py,tryalgo.fenwick.FenwickMin,"class FenwickMin:
    """"""maintains a tree to allow quick updates and queries
    of a virtual table t
    """"""

    def __init__(self, size):
        """"""stores a table t and allows updates and queries
        of prefix sums in logarithmic time.

        :param size: length of the table
        """"""
        self.s = [float('+inf')] * (size + 1)

    def prefixMin(self, a):
        """"""
        :param int a: index in t, negative a will return infinity
        :returns: min(t[0], ... ,t[a])
        """"""
        i = a + 1
        retval = float('+inf')
        while i > 0:
            retval = min(retval, self.s[i])
            i -= i & -i
        return retval

    def update(self, a, val):
        """"""
        :param int a: index in t
        :param val: a value
        :modifies: sets t[a] to the minimum of t[a] and val
        """"""
        i = a + 1
        while i < len(self.s):
            self.s[i] = min(self.s[i], val)
            i += i & -i","class FenwickMin:
    
    def __init__(self, size):
        
        pass
    def prefixMin(self, a):
        '''
        :param int a: index in t, negative a will return infinity
        :returns: min(t[0], ... ,t[a])
        '''
        pass
    def update(self, a, val):
        '''
        :param int a: index in t
        :param val: a value
        :modifies: sets t[a] to the minimum of t[a] and val
        '''
        pass",snippet_125,4,"class FenwickMin:
    
    def __init__(self, size):
        self.n = int(size)
        self.bit = [float('inf')] * self.n

    def prefixMin(self, a):
        '''
        :param int a: index in t, negative a will return infinity
        :returns: min(t[0], ... ,t[a])
        '''
        if a < 0:
            return float('inf')
        if self.n == 0:
            return float('inf')
        if a >= self.n:
            a = self.n - 1
        res = float('inf')
        i = a
        while i >= 0:
            if self.bit[i] < res:
                res = self.bit[i]
            i = (i & (i + 1)) - 1
        return res

    def update(self, a, val):
        '''
        :param int a: index in t
        :param val: a value
        :modifies: sets t[a] to the minimum of t[a] and val
        '''
        if a < 0 or a >= self.n:
            return
        i = a
        while i < self.n:
            if val < self.bit[i]:
                self.bit[i] = val
            i = i | (i + 1)"
223981,jilljenn/tryalgo,jilljenn_tryalgo/tryalgo/lowest_common_ancestor.py,tryalgo.lowest_common_ancestor.LowestCommonAncestorRMQ,"from tryalgo.range_minimum_query import RangeMinQuery

class LowestCommonAncestorRMQ:
    """"""Lowest common ancestor data structure using a reduction to
       range minimum query
    """"""

    def __init__(self, graph):
        """"""builds the structure from a given tree

        :param graph: adjacency matrix of a tree
        :complexity: O(n log n), with n = len(graph)
        """"""
        n = len(graph)
        dfs_trace = []
        self.last = [None] * n
        to_visit = [(0, 0, None)]
        succ = [0] * n
        while to_visit:
            level, node, father = to_visit[-1]
            self.last[node] = len(dfs_trace)
            dfs_trace.append((level, node))
            if succ[node] < len(graph[node]) and graph[node][succ[node]] == father:
                succ[node] += 1
            if succ[node] == len(graph[node]):
                to_visit.pop()
            else:
                neighbor = graph[node][succ[node]]
                succ[node] += 1
                to_visit.append((level + 1, neighbor, node))
        self.rmq = RangeMinQuery(dfs_trace, (float('inf'), None))

    def query(self, u, v):
        """""":returns: the lowest common ancestor of u and v
        :complexity: O(log n)
        """"""
        lu = self.last[u]
        lv = self.last[v]
        if lu > lv:
            lu, lv = (lv, lu)
        return self.rmq.range_min(lu, lv + 1)[1]","
class LowestCommonAncestorRMQ:
    '''Lowest common ancestor data structure using a reduction to
       range minimum query
    '''
    def __init__(self, graph):
        '''builds the structure from a given tree
        :param graph: adjacency matrix of a tree
        :complexity: O(n log n), with n = len(graph)
        '''
        pass
    def query(self, u, v):
        
        pass",snippet_126,3,"class LowestCommonAncestorRMQ:
    '''Lowest common ancestor data structure using a reduction to
       range minimum query
    '''
    def __init__(self, graph):
        '''builds the structure from a given tree
        :param graph: adjacency matrix of a tree
        :complexity: O(n log n), with n = len(graph)
        '''
        n = len(graph)
        self.n = n
        # Build adjacency list from adjacency matrix
        adj = [[] for _ in range(n)]
        for i in range(n):
            row = graph[i]
            for j in range(n):
                if i != j and row[j]:
                    adj[i].append(j)
        self.adj = adj

        if n == 0:
            # Empty structure
            self.euler = []
            self.depths = []
            self.first = []
            self.log = [0]
            self.st = [[]]
            return

        # Euler tour and depths
        euler = []
        depths = []
        first = [-1] * n
        depth = [0] * n

        # Iterative DFS with explicit enter/back actions to build Euler tour
        stack = [('visit', 0, -1)]
        depth[0] = 0
        while stack:
            typ, u, p = stack.pop()
            if typ == 'visit':
                if first[u] == -1:
                    first[u] = len(euler)
                euler.append(u)
                depths.append(depth[u])
                # Push children in reverse order so traversal is in original order
                for v in reversed(adj[u]):
                    if v == p:
                        continue
                    stack.append(('back', u, p))
                    depth[v] = depth[u] + 1
                    stack.append(('visit', v, u))
            else:  # 'back' action
                euler.append(u)
                depths.append(depth[u])

        self.euler = euler
        self.depths = depths
        self.first = first

        m = len(euler)
        # Precompute logs
        log = [0] * (m + 1)
        for i in range(2, m + 1):
            log[i] = log[i // 2] + 1
        self.log = log

        # Build Sparse Table over depths; store indices into euler
        kmax = log[m]
        st = [[0] * m for _ in range(kmax + 1)]
        for i in range(m):
            st[0][i] = i
        j = 1
        while (1 << j) <= m:
            span = 1 << j
            half = span >> 1
            prev = st[j - 1]
            cur = st[j]
            for i in range(m - span + 1):
                left = prev[i]
                right = prev[i + half]
                cur[i] = left if depths[left] <= depths[right] else right
            j += 1
        self.st = st

    def query(self, u, v):
        if self.n == 0:
            return None
        if not (0 <= u < self.n and 0 <= v < self.n):
            raise IndexError(""query nodes out of range"")
        l = self.first[u]
        r = self.first[v]
        if l > r:
            l, r = r, l
        length = r - l + 1
        k = self.log[length]
        i1 = self.st[k][l]
        i2 = self.st[k][r - (1 << k) + 1]
        idx = i1 if self.depths[i1] <= self.depths[i2] else i2
        return self.euler[idx]"
223982,jilljenn/tryalgo,jilljenn_tryalgo/tryalgo/lowest_common_ancestor.py,tryalgo.lowest_common_ancestor.LowestCommonAncestorShortcuts,"class LowestCommonAncestorShortcuts:
    """"""Lowest common ancestor data structure using shortcuts to ancestors
    """"""

    def __init__(self, prec):
        """"""builds the structure from a given tree

        :param prec: father for every node, with prec[0] = 0
        :assumes: prec[node] < node
        :complexity: O(n log n), with n = len(nodes)
        """"""
        n = len(prec)
        self.level = [None] * n
        self.level[0] = 0
        for u in range(1, n):
            self.level[u] = 1 + self.level[prec[u]]
        depth = log2ceil(max((self.level[u] for u in range(n)))) + 1
        self.anc = [[0] * n for _ in range(depth)]
        for u in range(n):
            self.anc[0][u] = prec[u]
        for k in range(1, depth):
            for u in range(n):
                self.anc[k][u] = self.anc[k - 1][self.anc[k - 1][u]]

    def query(self, u, v):
        """""":returns: the lowest common ancestor of u and v
        :complexity: O(log n)
        """"""
        if self.level[u] > self.level[v]:
            u, v = (v, u)
        depth = len(self.anc)
        for k in range(depth - 1, -1, -1):
            if self.level[u] <= self.level[v] - (1 << k):
                v = self.anc[k][v]
        assert self.level[u] == self.level[v]
        if u == v:
            return u
        for k in range(depth - 1, -1, -1):
            if self.anc[k][u] != self.anc[k][v]:
                u = self.anc[k][u]
                v = self.anc[k][v]
        assert self.anc[0][u] == self.anc[0][v]
        return self.anc[0][u]","class LowestCommonAncestorShortcuts:
    '''Lowest common ancestor data structure using shortcuts to ancestors
    '''
    def __init__(self, prec):
        '''builds the structure from a given tree
        :param prec: father for every node, with prec[0] = 0
        :assumes: prec[node] < node
        :complexity: O(n log n), with n = len(nodes)
        '''
        pass
    def query(self, u, v):
        
        pass",snippet_127,3,"class LowestCommonAncestorShortcuts:
    '''Lowest common ancestor data structure using shortcuts to ancestors
    '''
    def __init__(self, prec):
        '''builds the structure from a given tree
        :param prec: father for every node, with prec[0] = 0
        :assumes: prec[node] < node
        :complexity: O(n log n), with n = len(nodes)
        '''
        import math
        self.prec = list(prec)
        self.n = len(self.prec)
        self.depth = [0] * self.n
        for v in range(1, self.n):
            self.depth[v] = self.depth[self.prec[v]] + 1

        self.LOG = max(1, math.ceil(math.log2(self.n + 1)))
        self.up = [[0] * self.n for _ in range(self.LOG)]
        for v in range(self.n):
            self.up[0][v] = self.prec[v]
        for k in range(1, self.LOG):
            up_prev = self.up[k - 1]
            up_curr = self.up[k]
            for v in range(self.n):
                up_curr[v] = up_prev[up_prev[v]]

    def query(self, u, v):
        if u == v:
            return u
        if self.depth[u] < self.depth[v]:
            u, v = v, u
        diff = self.depth[u] - self.depth[v]
        k = 0
        while diff:
            if diff & 1:
                u = self.up[k][u]
            diff >>= 1
            k += 1
        if u == v:
            return u
        for k in range(self.LOG - 1, -1, -1):
            if self.up[k][u] != self.up[k][v]:
                u = self.up[k][u]
                v = self.up[k][v]
        return self.up[0][u]"
223988,jilljenn/tryalgo,jilljenn_tryalgo/tryalgo/partition_refinement.py,tryalgo.partition_refinement.PartitionRefinement,"class PartitionRefinement:
    """"""This data structure implements an order preserving
    partition with refinements.
    """"""

    def __init__(self, n):
        """"""Start with the partition consisting of the unique class {0,1,..,n-1}
        complexity: O(n) both in time and space
        """"""
        c = PartitionClass()
        self.classes = c
        self.items = [PartitionItem(i, c) for i in range(n)]

    def refine(self, pivot):
        """"""Split every class C in the partition into C intersection pivot
        and C setminus pivot complexity: linear in size of pivot
        """"""
        has_split = []
        for i in pivot:
            if 0 <= i < len(self.items):
                x = self.items[i]
                c = x.theclass
                if not c.split:
                    c.split = PartitionClass(c)
                    if self.classes is c:
                        self.classes = c.split
                    has_split.append(c)
                x.remove()
                x.theclass = c.split
                c.split.append(x)
        for c in has_split:
            c.split = None
            if not c.items:
                c.remove()
                del c

    def tolist(self):
        """"""produce a list representation of the partition
        """"""
        return [[x.val for x in theclass.items] for theclass in self.classes]

    def order(self):
        """"""Produce a flatten list of the partition, ordered by classes
        """"""
        return [x.val for theclass in self.classes for x in theclass.items]","class PartitionRefinement:
    
    def __init__(self, n):
        
        pass
    def refine(self, pivot):
        '''Split every class C in the partition into C intersection pivot
        and C setminus pivot complexity: linear in size of pivot
        '''
        pass
    def tolist(self):
        
        pass
    def order(self):
        
        pass",snippet_128,5,"class PartitionRefinement:
    
    def __init__(self, n):
        self.n = n
        self.elems = list(range(n))          # current order of elements
        self.pos = list(range(n))            # position of each element in elems
        self.block_start = [0]               # start index (inclusive) for each block
        self.block_end = [n]                 # end index (exclusive) for each block
        self.split = [0]                     # current split pointer for each block (used during refine)
        self.block_id = [0] * n              # current block index for each element

    def _swap_positions(self, i, j):
        if i == j:
            return
        a, b = self.elems[i], self.elems[j]
        self.elems[i], self.elems[j] = b, a
        self.pos[a], self.pos[b] = j, i

    def refine(self, pivot):
        '''Split every class C in the partition into C intersection pivot
        and C setminus pivot complexity: linear in size of pivot
        '''
        num_blocks = len(self.block_start)
        touched = []
        touched_flag = [False] * num_blocks

        for x in pivot:
            b = self.block_id[x]
            # mark block as touched (once)
            if not touched_flag[b]:
                touched_flag[b] = True
                touched.append(b)
            # pack x into the front segment of its block if not already
            s = self.block_start[b]
            sp = self.split[b]
            i = self.pos[x]
            if i >= sp:  # not yet in the marked prefix
                self._swap_positions(i, sp)
                self.split[b] = sp + 1

        # perform splits on touched blocks
        for b in touched:
            s = self.block_start[b]
            e = self.block_end[b]
            m = self.split[b] - s          # number of marked in block
            size = e - s

            if m == 0 or m == size:
                # no split needed, just reset split pointer
                self.split[b] = s
                continue

            # create a new block for the smaller side to keep work linear
            if m <= size - m:
                # smaller side is the marked prefix [s, s+m)
                new_start, new_end = s, s + m
                # old block becomes unmarked suffix [s+m, e)
                self.block_start[b] = s + m
                # new block appended
                new_b = len(self.block_start)
                self.block_start.append(new_start)
                self.block_end.append(new_end)
                self.split.append(new_start)
                # update end of old block
                self.block_end[b] = e
                # update block_id for elements in new small block
                for idx in range(new_start, new_end):
                    self.block_id[self.elems[idx]] = new_b
                # reset split for old block
                self.split[b] = self.block_start[b]
            else:
                # smaller side is the unmarked suffix [s+m, e)
                new_start, new_end = s + m, e
                # old block becomes marked prefix [s, s+m)
                self.block_end[b] = s + m
                # new block appended
                new_b = len(self.block_start)
                self.block_start.append(new_start)
                self.block_end.append(new_end)
                self.split.append(new_start)
                # update block_id for elements in new small block
                for idx in range(new_start, new_end):
                    self.block_id[self.elems[idx]] = new_b
                # reset split for old block
                self.split[b] = self.block_start[b]

    def tolist(self):
        res = []
        for s, e in zip(self.block_start, self.block_end):
            res.append(self.elems[s:e].copy())
        return res

    def order(self):
        return self.elems.copy()"
224105,rigetti/grove,rigetti_grove/grove/alpha/fermion_transforms/fenwick_tree.py,grove.alpha.fermion_transforms.fenwick_tree.FenwickNode,"class FenwickNode:
    """"""Fenwick Tree node.""""""
    parent = None
    children = None
    index = None

    def __init__(self, parent, children, index=None):
        """"""Fenwick Tree node. Single parent and multiple children.

        :param FenwickNode parent: a parent node
        :param list(FenwickNode) children: a list of children nodes
        :param int index: node label
        """"""
        self.children = children
        self.parent = parent
        self.index = index

    def get_ancestors(self):
        """"""Returns a list of ancestors of the node. Ordered from the earliest.

        :return: node's ancestors, ordered from most recent
        :rtype: list(FenwickNode)
        """"""
        node = self
        ancestor_list = []
        while node.parent is not None:
            ancestor_list.append(node.parent)
            node = node.parent
        return ancestor_list","class FenwickNode:
    '''Fenwick Tree node.'''
    def __init__(self, parent, children, index=None):
        '''Fenwick Tree node. Single parent and multiple children.
        :param FenwickNode parent: a parent node
        :param list(FenwickNode) children: a list of children nodes
        :param int index: node label
        '''
        pass
    def get_ancestors(self):
        
        pass",snippet_129,3,"class FenwickNode:
    '''Fenwick Tree node.'''
    def __init__(self, parent, children, index=None):
        '''Fenwick Tree node. Single parent and multiple children.
        :param FenwickNode parent: a parent node
        :param list(FenwickNode) children: a list of children nodes
        :param int index: node label
        '''
        self.parent = parent
        self.children = list(children) if children is not None else []
        self.index = index

    def get_ancestors(self):
        ancestors = []
        current = self.parent
        while current is not None:
            ancestors.append(current)
            current = current.parent
        return ancestors"
227838,econ-ark/HARK,HARK/utilities.py,HARK.utilities.NullFunc,"import numpy as np

class NullFunc:
    """"""
    A trivial class that acts as a placeholder ""do nothing"" function.
    """"""

    def __call__(self, *args):
        """"""
        Returns meaningless output no matter what the input(s) is.  If no input,
        returns None.  Otherwise, returns an array of NaNs (or a single NaN) of
        the same size as the first input.
        """"""
        if len(args) == 0:
            return None
        else:
            arg = args[0]
            if hasattr(arg, 'shape'):
                return np.zeros_like(arg) + np.nan
            else:
                return np.nan

    def distance(self, other):
        """"""
        Trivial distance metric that only cares whether the other object is also
        an instance of NullFunc.  Intentionally does not inherit from HARKobject
        as this might create dependency problems.

        Parameters
        ----------
        other : any
            Any object for comparison to this instance of NullFunc.

        Returns
        -------
        (unnamed) : float
            The distance between self and other.  Returns 0 if other is also a
            NullFunc; otherwise returns an arbitrary high number.
        """"""
        try:
            if other.__class__ is self.__class__:
                return 0.0
            else:
                return 1000.0
        except:
            return 10000.0","
class NullFunc:
    
    def __call__(self, *args):
        
        pass
    def distance(self, other):
        '''
        Trivial distance metric that only cares whether the other object is also
        an instance of NullFunc.  Intentionally does not inherit from HARKobject
        as this might create dependency problems.
        Parameters
        ----------
        other : any
            Any object for comparison to this instance of NullFunc.
        Returns
        -------
        (unnamed) : float
            The distance between self and other.  Returns 0 if other is also a
            NullFunc; otherwise returns an arbitrary high number.
        '''
        pass",snippet_131,3,"class NullFunc:
    
    def __call__(self, *args):
        return 0.0

    def distance(self, other):
        '''
        Trivial distance metric that only cares whether the other object is also
        an instance of NullFunc.  Intentionally does not inherit from HARKobject
        as this might create dependency problems.
        Parameters
        ----------
        other : any
            Any object for comparison to this instance of NullFunc.
        Returns
        -------
        (unnamed) : float
            The distance between self and other.  Returns 0 if other is also a
            NullFunc; otherwise returns an arbitrary high number.
        '''
        return 0.0 if isinstance(other, NullFunc) else 1e9"
228145,jcrobak/parquet-python,jcrobak_parquet-python/parquet/__init__.py,parquet.JsonWriter,"import json

class JsonWriter:
    """"""Utility for dumping rows as JSON objects.""""""

    def __init__(self, out):
        """"""Initialize with output destination.""""""
        self._out = out

    def writerow(self, row):
        """"""Write a single row.""""""
        json_text = json.dumps(row)
        if isinstance(json_text, bytes):
            json_text = json_text.decode('utf-8')
        self._out.write(json_text)
        self._out.write(u'\n')","
class JsonWriter:
    '''Utility for dumping rows as JSON objects.'''
    def __init__(self, out):
        
        pass
    def writerow(self, row):
        '''Write a single row.'''
        pass",snippet_132,3,"import json

class JsonWriter:
    '''Utility for dumping rows as JSON objects.'''
    def __init__(self, out):
        self._out = out

    def writerow(self, row):
        '''Write a single row.'''
        self._out.write(json.dumps(row, ensure_ascii=False))
        self._out.write('\n')"
228682,common-workflow-language/cwltool,common-workflow-language_cwltool/cwltool/software_requirements.py,cwltool.software_requirements.DependenciesConfiguration,"import os
from typing import TYPE_CHECKING, Any, Optional, Union, cast
import argparse

class DependenciesConfiguration:
    """"""Dependency configuration class, for RuntimeContext.job_script_provider.""""""

    def __init__(self, args: argparse.Namespace) -> None:
        """"""Initialize.""""""
        self.tool_dependency_dir: Optional[str] = None
        self.dependency_resolvers_config_file: Optional[str] = None
        conf_file = getattr(args, 'beta_dependency_resolvers_configuration', None)
        tool_dependency_dir = getattr(args, 'beta_dependencies_directory', None)
        conda_dependencies = getattr(args, 'beta_conda_dependencies', None)
        if conf_file is not None and os.path.exists(conf_file):
            self.use_tool_dependencies = True
            if tool_dependency_dir is None:
                tool_dependency_dir = os.path.abspath(os.path.dirname(conf_file))
            self.tool_dependency_dir = tool_dependency_dir
            self.dependency_resolvers_config_file = os.path.abspath(conf_file)
        elif conda_dependencies is not None:
            if tool_dependency_dir is None:
                tool_dependency_dir = os.path.abspath('./cwltool_deps')
            self.tool_dependency_dir = tool_dependency_dir
            self.use_tool_dependencies = True
            self.dependency_resolvers_config_file = None
        else:
            self.use_tool_dependencies = False
        if self.tool_dependency_dir and (not os.path.exists(self.tool_dependency_dir)):
            os.makedirs(self.tool_dependency_dir)

    def build_job_script(self, builder: 'Builder', command: list[str]) -> str:
        """"""Use the galaxy-tool-util library to construct a build script.""""""
        ensure_galaxy_lib_available()
        resolution_config_dict = {'use': self.use_tool_dependencies, 'default_base_path': self.tool_dependency_dir}
        app_config = {'conda_auto_install': True, 'conda_auto_init': True, 'debug': builder.debug}
        tool_dependency_manager: 'deps.DependencyManager' = deps.build_dependency_manager(app_config_dict=app_config, resolution_config_dict=resolution_config_dict, conf_file=self.dependency_resolvers_config_file)
        handle_dependencies: str = ''
        if (dependencies := get_dependencies(builder)):
            handle_dependencies = '\n'.join(tool_dependency_manager.dependency_shell_commands(dependencies, job_directory=builder.tmpdir))
        template_kwds: dict[str, str] = dict(handle_dependencies=handle_dependencies)
        job_script = COMMAND_WITH_DEPENDENCIES_TEMPLATE.substitute(template_kwds)
        return job_script","
class DependenciesConfiguration:
    '''Dependency configuration class, for RuntimeContext.job_script_provider.'''
    def __init__(self, args: argparse.Namespace) -> None:
        '''Initialize.'''
        pass
    def build_job_script(self, builder: 'Builder', command: list[str]) -> str:
        
        pass",snippet_133,3,"import argparse
import shlex
from typing import Any


class DependenciesConfiguration:
    '''Dependency configuration class, for RuntimeContext.job_script_provider.'''

    def __init__(self, args: argparse.Namespace) -> None:
        '''Initialize.'''
        self.python: str = getattr(args, ""python"", ""python3"")
        self.venv: str | None = getattr(args, ""venv"", None)
        self.requirements: str | None = getattr(args, ""requirements"", None)
        self.packages: list[str] = list(getattr(args, ""packages"", []) or [])
        self.pre_commands: list[str] = list(getattr(args, ""pre_commands"", []) or [])
        self.post_commands: list[str] = list(getattr(args, ""post_commands"", []) or [])
        self.upgrade_pip: bool = bool(getattr(args, ""upgrade_pip"", True))
        self.install_command: str = getattr(args, ""install_command"", ""pip install"")

    def build_job_script(self, builder: 'Builder', command: list[str]) -> str:
        lines: list[str] = []
        lines.append(""set -euo pipefail"")

        # Pre-commands
        for cmd in self.pre_commands:
            if cmd.strip():
                lines.append(cmd)

        # Virtual environment setup
        if self.venv:
            venv_bin = f""{self.venv}/bin""
            lines.append(f'{shlex.quote(self.python)} -m venv {shlex.quote(self.venv)} || true')
            lines.append(f'. {shlex.quote(venv_bin)}/activate')
            if self.upgrade_pip:
                lines.append(""python -m pip install --upgrade pip setuptools wheel"")
            pip_prefix = """"
        else:
            # No venv; still optionally upgrade pip
            if self.upgrade_pip:
                lines.append(f'{shlex.quote(self.python)} -m pip install --upgrade pip setuptools wheel')
            pip_prefix = f'{shlex.quote(self.python)} -m '

        # Dependency installation
        if self.requirements:
            lines.append(f'{pip_prefix}{self.install_command} -r {shlex.quote(self.requirements)}')

        if self.packages:
            pkg_args = "" "".join(shlex.quote(p) for p in self.packages)
            lines.append(f'{pip_prefix}{self.install_command} {pkg_args}')

        # Main command
        if command:
            lines.append(shlex.join(command))

        # Post-commands
        for cmd in self.post_commands:
            if cmd.strip():
                lines.append(cmd)

        return ""\n"".join(lines) + ""\n"""
229565,sourceperl/pyModbusTCP,sourceperl_pyModbusTCP/examples/client_serial_gw.py,client_serial_gw.Serial2ModbusClient,"from pyModbusTCP.constants import EXP_GATEWAY_TARGET_DEVICE_FAILED_TO_RESPOND
import struct

class Serial2ModbusClient:
    """""" Customize a slave serial worker for map a modbus TCP client. """"""

    def __init__(self, serial_w, mbus_cli, slave_addr=1, allow_bcast=False):
        """"""Serial2ModbusClient constructor.

        :param serial_w: a SlaveSerialWorker instance
        :type serial_w: SlaveSerialWorker
        :param mbus_cli: a ModbusClient instance
        :type mbus_cli: ModbusClient
        :param slave_addr: modbus slave address
        :type slave_addr: int
        :param allow_bcast: allow processing broadcast frames (slave @0)
        :type allow_bcast: bool
        """"""
        self.serial_w = serial_w
        self.mbus_cli = mbus_cli
        self.slave_addr = slave_addr
        self.allow_bcast = allow_bcast
        self.serial_w.handle_request = self._handle_request

    def _handle_request(self):
        """"""Request handler for SlaveSerialWorker""""""
        if self.serial_w.request.slave_addr == 0 and self.allow_bcast:
            self.mbus_cli.custom_request(self.serial_w.request.pdu)
        elif self.serial_w.request.slave_addr == self.slave_addr:
            resp_pdu = self.mbus_cli.custom_request(self.serial_w.request.pdu)
            if resp_pdu:
                self.serial_w.response.build(raw_pdu=resp_pdu, slave_addr=self.serial_w.request.slave_addr)
            else:
                exp_pdu = struct.pack('BB', self.serial_w.request.function_code + 128, EXP_GATEWAY_TARGET_DEVICE_FAILED_TO_RESPOND)
                self.serial_w.response.build(raw_pdu=exp_pdu, slave_addr=self.serial_w.request.slave_addr)

    def run(self):
        """"""Start serial processing.""""""
        self.serial_w.run()","
class Serial2ModbusClient:
    
    def __init__(self, serial_w, mbus_cli, slave_addr=1, allow_bcast=False):
        
        pass
    def _handle_request(self):
        '''Request handler for SlaveSerialWorker'''
        pass
    def run(self):
        
        pass",snippet_134,4,"class Serial2ModbusClient:
    
    def __init__(self, serial_w, mbus_cli, slave_addr=1, allow_bcast=False):
        if serial_w is None:
            raise ValueError(""serial_w is required"")
        if mbus_cli is None:
            raise ValueError(""mbus_cli is required"")
        if not isinstance(slave_addr, int) or not (0 <= slave_addr <= 247):
            raise ValueError(""slave_addr must be an integer between 0 and 247"")
        self.serial_w = serial_w
        self.mbus_cli = mbus_cli
        self.slave_addr = slave_addr
        self.allow_bcast = allow_bcast
        self._running = False

    def _handle_request(self, req=None):
        '''Request handler for SlaveSerialWorker'''
        if req is None:
            return None
        if not isinstance(req, (bytes, bytearray)):
            # Some workers pass (addr, pdu) tuple
            if isinstance(req, tuple) and len(req) == 2:
                addr, pdu = req
                if not isinstance(addr, int) or not isinstance(pdu, (bytes, bytearray)):
                    return None
            else:
                return None
        else:
            if len(req) < 1:
                return None
            addr = req[0]
            pdu = req[1:]

        # Address filtering
        if addr == 0:
            # Broadcast - no response should be sent back
            if not self.allow_bcast:
                return None
            # Forward to client if allowed but do not return a response
            try:
                self._forward_pdu(pdu, unit=0)
            except Exception:
                pass
            return None

        if addr != self.slave_addr:
            return None

        # Forward to Modbus client and return encapsulated ADU
        resp_pdu = self._forward_pdu(pdu, unit=addr)
        if resp_pdu is None:
            return None
        return bytes([addr]) + bytes(resp_pdu)

    def _forward_pdu(self, pdu, unit):
        # Try a variety of client interfaces to send raw PDU
        cli = self.mbus_cli

        # Preferred: execute_pdu(pdu: bytes, unit: int) -> bytes
        execute_pdu = getattr(cli, ""execute_pdu"", None)
        if callable(execute_pdu):
            return execute_pdu(bytes(pdu), unit=unit)

        # Fallback: execute(pdu, unit=...) -> bytes
        execute = getattr(cli, ""execute"", None)
        if callable(execute):
            return execute(bytes(pdu), unit=unit)

        # Fallback: transport-like object
        transport = getattr(cli, ""transport"", None)
        if transport is not None:
            send_pdu = getattr(transport, ""send_pdu"", None)
            if callable(send_pdu):
                return send_pdu(bytes(pdu), unit=unit)

        raise RuntimeError(""mbus_cli does not support raw PDU execution"")

    def run(self):
        self._running = True
        # Register request handler with various possible APIs
        registered = False
        try:
            if hasattr(self.serial_w, ""register_request_handler""):
                self.serial_w.register_request_handler(self._handle_request)
                registered = True
            elif hasattr(self.serial_w, ""set_request_handler""):
                self.serial_w.set_request_handler(self._handle_request)
                registered = True
            elif hasattr(self.serial_w, ""on_request""):
                # Some APIs expose a property/callback
                try:
                    self.serial_w.on_request = self._handle_request
                    registered = True
                except Exception:
                    registered = False

            # Start/Run the worker
            if hasattr(self.serial_w, ""serve_forever""):
                self.serial_w.serve_forever()
            elif hasattr(self.serial_w, ""run""):
                self.serial_w.run()
            elif hasattr(self.serial_w, ""start""):
                self.serial_w.start()
                if hasattr(self.serial_w, ""join""):
                    self.serial_w.join()
            else:
                # Polling fallback API: expect recv()->request, send(response)
                # This is a minimal synchronous loop.
                while self._running:
                    if not hasattr(self.serial_w, ""recv""):
                        break
                    req = self.serial_w.recv()
                    if req is None:
                        continue
                    resp = self._handle_request(req)
                    if resp is not None and hasattr(self.serial_w, ""send""):
                        self.serial_w.send(resp)
        finally:
            self._running = False
            # Cleanup/close serial worker if possible
            for attr in (""close"", ""stop"", ""shutdown""):
                fn = getattr(self.serial_w, attr, None)
                if callable(fn):
                    try:
                        fn()
                    except Exception:
                        pass"
230113,aws/aws-xray-sdk-python,aws_aws-xray-sdk-python/aws_xray_sdk/core/models/noop_traceid.py,aws_aws-xray-sdk-python.aws_xray_sdk.core.models.noop_traceid.NoOpTraceId,"class NoOpTraceId:
    """"""
    A trace ID tracks the path of a request through your application.
    A trace collects all the segments generated by a single request.
    A trace ID is required for a segment.
    """"""
    VERSION = '1'
    DELIMITER = '-'

    def __init__(self):
        """"""
        Generate a no-op trace id.
        """"""
        self.start_time = '00000000'
        self.__number = '000000000000000000000000'

    def to_id(self):
        """"""
        Convert TraceId object to a string.
        """"""
        return '%s%s%s%s%s' % (NoOpTraceId.VERSION, NoOpTraceId.DELIMITER, self.start_time, NoOpTraceId.DELIMITER, self.__number)","class NoOpTraceId:
    
    def __init__(self):
        
        pass
    def to_id(self):
        '''
        Convert TraceId object to a string.
        '''
        pass",snippet_135,3,"class NoOpTraceId:
    
    def __init__(self):
        self._id = """"
        
    def to_id(self):
        '''
        Convert TraceId object to a string.
        '''
        return self._id"
230120,aws/aws-xray-sdk-python,aws_aws-xray-sdk-python/aws_xray_sdk/core/models/traceid.py,aws_aws-xray-sdk-python.aws_xray_sdk.core.models.traceid.TraceId,"import binascii
import os
import time

class TraceId:
    """"""
    A trace ID tracks the path of a request through your application.
    A trace collects all the segments generated by a single request.
    A trace ID is required for a segment.
    """"""
    VERSION = '1'
    DELIMITER = '-'

    def __init__(self):
        """"""
        Generate a random trace id.
        """"""
        self.start_time = int(time.time())
        self.__number = binascii.b2a_hex(os.urandom(12)).decode('utf-8')

    def to_id(self):
        """"""
        Convert TraceId object to a string.
        """"""
        return '%s%s%s%s%s' % (TraceId.VERSION, TraceId.DELIMITER, format(self.start_time, 'x'), TraceId.DELIMITER, self.__number)","
class TraceId:
    
    def __init__(self):
        '''
        Generate a random trace id.
        '''
        pass
    def to_id(self):
        '''
        Convert TraceId object to a string.
        '''
        pass",snippet_136,3,"import secrets

class TraceId:
    
    def __init__(self):
        '''
        Generate a random trace id.
        '''
        self._id = secrets.token_hex(16)
    def to_id(self):
        '''
        Convert TraceId object to a string.
        '''
        return self._id"
230123,aws/aws-xray-sdk-python,aws_aws-xray-sdk-python/aws_xray_sdk/core/sampling/local/reservoir.py,aws_aws-xray-sdk-python.aws_xray_sdk.core.sampling.local.reservoir.Reservoir,"import time
import threading

class Reservoir:
    """"""
    Keeps track of the number of sampled segments within
    a single second. This class is implemented to be
    thread-safe to achieve accurate sampling.
    """"""

    def __init__(self, traces_per_sec=0):
        """"""
        :param int traces_per_sec: number of guranteed
            sampled segments.
        """"""
        self._lock = threading.Lock()
        self.traces_per_sec = traces_per_sec
        self.used_this_sec = 0
        self.this_sec = int(time.time())

    def take(self):
        """"""
        Returns True if there are segments left within the
        current second, otherwise return False.
        """"""
        with self._lock:
            now = int(time.time())
            if now != self.this_sec:
                self.used_this_sec = 0
                self.this_sec = now
            if self.used_this_sec >= self.traces_per_sec:
                return False
            self.used_this_sec = self.used_this_sec + 1
            return True","
class Reservoir:
    '''
    Keeps track of the number of sampled segments within
    a single second. This class is implemented to be
    thread-safe to achieve accurate sampling.
    '''
    def __init__(self, traces_per_sec=0):
        
        pass
    def take(self):
        
        pass",snippet_137,3,"import time
from threading import Lock

class Reservoir:
    '''
    Keeps track of the number of sampled segments within
    a single second. This class is implemented to be
    thread-safe to achieve accurate sampling.
    '''
    def __init__(self, traces_per_sec=0):
        self._limit = int(traces_per_sec) if traces_per_sec is not None else 0
        if self._limit < 0:
            self._limit = 0
        self._lock = Lock()
        self._second = None
        self._count = 0

    def take(self):
        now_sec = int(time.time())
        with self._lock:
            if self._limit <= 0:
                return False
            if self._second != now_sec:
                self._second = now_sec
                self._count = 0
            if self._count < self._limit:
                self._count += 1
                return True
            return False"
230719,django-salesforce/django-salesforce,django-salesforce_django-salesforce/salesforce/backend/indep.py,salesforce.backend.indep.LazyField,"from typing import Any, Callable, Dict, Tuple, Type
from inspect import signature

class LazyField:
    """"""A Field that can be later customized until it is binded to the final Model""""""
    counter = 0

    def __init__(self, klass: 'Type[Field[Any, Any]]') -> None:
        """"""Instantiate the field type""""""
        self.klass = klass
        self.kw = {}
        self.args = ()
        self.called = False
        self.counter = self.counter

    def __call__(self, *args: Any, **kwargs: Any) -> 'LazyField':
        """"""Instantiate a new field with options""""""
        assert not self.called
        bound_args = signature(self.klass.__init__).bind(self, *args, **kwargs)
        obj = type(self)(self.klass)
        obj.args = bound_args.args[1:]
        obj.kw = bound_args.kwargs
        setattr(type(self), 'counter', getattr(type(self), 'counter') + 1)
        return obj

    def update(self, **kwargs: Any) -> 'LazyField':
        """"""Customize the lazy field""""""
        assert not self.called
        self.kw.update(kwargs)
        return self

    def create(self) -> 'Field[Any, Any]':
        """"""Create a normal field from the lazy field""""""
        assert not self.called
        return self.klass(*self.args, **self.kw)","
class LazyField:
    
    def __init__(self, klass: 'Type[Field[Any, Any]]') -> None:
        
        pass
    def __call__(self, *args: Any, **kwargs: Any) -> 'LazyField':
        '''Instantiate a new field with options'''
        pass
    def update(self, **kwargs: Any) -> 'LazyField':
        '''Customize the lazy field'''
        pass
    def create(self) -> 'Field[Any, Any]':
        
        pass",snippet_139,5,"from typing import Any, Type, Tuple, Dict


class LazyField:
    def __init__(self, klass: 'Type[Field[Any, Any]]') -> None:
        self._klass = klass
        self._args: Tuple[Any, ...] = ()
        self._kwargs: Dict[str, Any] = {}

    def __call__(self, *args: Any, **kwargs: Any) -> 'LazyField':
        '''Instantiate a new field with options'''
        new = LazyField(self._klass)
        new._args = self._args + args
        merged_kwargs = dict(self._kwargs)
        merged_kwargs.update(kwargs)
        new._kwargs = merged_kwargs
        return new

    def update(self, **kwargs: Any) -> 'LazyField':
        '''Customize the lazy field'''
        new = LazyField(self._klass)
        new._args = self._args
        merged_kwargs = dict(self._kwargs)
        merged_kwargs.update(kwargs)
        new._kwargs = merged_kwargs
        return new

    def create(self) -> 'Field[Any, Any]':
        return self._klass(*self._args, **self._kwargs)"
231223,weblyzard/inscriptis,benchmarking/run_benchmarking.py,run_benchmarking.AbstractHtmlConverter,"from time import time

class AbstractHtmlConverter:
    """"""
    An abstract HTML convert class.
    """"""

    def get_text(self, html):
        """"""
        Returns:
            a text representation of the given HTML snippet.
        """"""
        raise NotImplementedError

    def benchmark(self, html):
        """"""
        Benchmarks the classes HTML to text converter.

        Returns:
            A tuple of the required time and the obtained text representation.
        """"""
        start_time = time()
        for _ in range(TRIES):
            text = self.get_text(html)
        return (time() - start_time, text)","
class AbstractHtmlConverter:
    
    def get_text(self, html):
        '''
        Returns:
            a text representation of the given HTML snippet.
        '''
        pass
    def benchmark(self, html):
        
        pass",snippet_140,3,"class AbstractHtmlConverter:
    def get_text(self, html):
        '''
        Returns:
            a text representation of the given HTML snippet.
        '''
        from html.parser import HTMLParser
        from html import unescape
        import re

        if html is None:
            return """"

        class _TextExtractor(HTMLParser):
            def __init__(self):
                super().__init__(convert_charrefs=False)
                self._buf = []
                self._suppress = 0  # depth counter for script/style
                self._pre = 0       # depth counter for preformatted blocks

            def _newline(self):
                if not self._buf:
                    self._buf.append(""\n"")
                    return
                if self._buf[-1].endswith(""\n""):
                    return
                self._buf.append(""\n"")

            def handle_starttag(self, tag, attrs):
                t = tag.lower()
                if t in (""script"", ""style""):
                    self._suppress += 1
                    return

                if t in (""br"",):
                    self._newline()
                elif t in (""p"", ""div"", ""section"", ""article"", ""header"", ""footer"", ""aside"",
                           ""ul"", ""ol"", ""li"", ""table"", ""tr"", ""td"", ""th"", ""thead"", ""tbody"",
                           ""h1"", ""h2"", ""h3"", ""h4"", ""h5"", ""h6"", ""blockquote"", ""hr""):
                    self._newline()
                elif t in (""pre"",):
                    self._pre += 1
                    self._newline()

                if t == ""li"":
                    # bullet prefix
                    self._buf.append(""- "")

            def handle_endtag(self, tag):
                t = tag.lower()
                if t in (""script"", ""style""):
                    if self._suppress > 0:
                        self._suppress -= 1
                    return

                if t in (""p"", ""div"", ""section"", ""article"", ""header"", ""footer"", ""aside"",
                         ""ul"", ""ol"", ""li"", ""table"", ""tr"", ""td"", ""th"", ""thead"", ""tbody"",
                         ""h1"", ""h2"", ""h3"", ""h4"", ""h5"", ""h6"", ""blockquote"", ""pre""):
                    self._newline()

                if t == ""pre"" and self._pre > 0:
                    self._pre -= 1

            def handle_data(self, data):
                if self._suppress:
                    return
                if not data:
                    return
                if self._pre:
                    self._buf.append(data)
                else:
                    self._buf.append(data)

            def handle_entityref(self, name):
                if self._suppress:
                    return
                self._buf.append(unescape(f""&{name};""))

            def handle_charref(self, name):
                if self._suppress:
                    return
                self._buf.append(unescape(f""&#{name};""))

            def get_text(self):
                text = """".join(self._buf)
                text = unescape(text)

                # Normalize whitespace outside of pre blocks effect, but since we can't
                # reconstruct exact ranges post-parse, we approximate:
                # - collapse spaces/tabs
                # - normalize line endings
                text = text.replace(""\r\n"", ""\n"").replace(""\r"", ""\n"")

                # Collapse spaces around newlines
                text = re.sub(r""[ \t]+\n"", ""\n"", text)
                text = re.sub(r""\n[ \t]+"", ""\n"", text)

                # Collapse multiple spaces
                text = re.sub(r""[ \t]{2,}"", "" "", text)

                # Collapse excessive blank lines to at most two newlines
                text = re.sub(r""\n{3,}"", ""\n\n"", text)

                # Trim leading/trailing whitespace
                return text.strip()

        parser = _TextExtractor()
        try:
            parser.feed(str(html))
            parser.close()
        except Exception:
            # Fallback: very naive strip tags
            import re as _re
            raw = _re.sub(r""(?is)<(script|style).*?</\1>"", """", str(html))
            raw = _re.sub(r""(?is)<br\s*/?>"", ""\n"", raw)
            raw = _re.sub(r""(?is)</?(p|div|li|tr|h[1-6]|blockquote|pre|section|article|header|footer|aside)[^>]*>"", ""\n"", raw)
            raw = _re.sub(r""(?is)<.*?>"", """", raw)
            text = unescape(raw)
            text = _re.sub(r""[ \t]+"", "" "", text)
            text = _re.sub(r""\n{3,}"", ""\n\n"", text)
            return text.strip()

        return parser.get_text()

    def benchmark(self, html):
        import time

        # Warm-up
        self.get_text(html)

        min_runtime = 0.2
        iterations = 1
        total = 0.0

        while total < min_runtime and iterations < 100000:
            start = time.perf_counter()
            for _ in range(iterations):
                self.get_text(html)
            end = time.perf_counter()
            total = end - start
            if total < min_runtime:
                iterations *= 2

        per_call = total / iterations if iterations else float(""inf"")
        ips = (1.0 / per_call) if per_call > 0 else float(""inf"")

        return {
            ""iterations"": iterations,
            ""total_seconds"": total,
            ""seconds_per_call"": per_call,
            ""iterations_per_second"": ips,
        }"
236113,richardchien/python-aiocqhttp,richardchien_python-aiocqhttp/aiocqhttp/api.py,aiocqhttp.api.Api,"from typing import Callable, Any, Union, Awaitable
import functools
import abc

class Api:
    """"""
    API 

     `call_action` 
    """"""

    @abc.abstractmethod
    def call_action(self, action: str, **params) -> Union[Awaitable[Any], Any]:
        """"""
         OneBot API`action`  API `**params`
         API 

        
        """"""
        pass

    def __getattr__(self, item: str) -> Callable[..., Union[Awaitable[Any], Any]]:
        """""" API""""""
        return functools.partial(self.call_action, item)","
class Api:
    
    @abc.abstractmethod
    def call_action(self, action: str, **params) -> Union[Awaitable[Any], Any]:
        '''
         OneBot API`action`  API `**params`
         API 
        
        '''
        pass
    def __getattr__(self, item: str) -> Callable[..., Union[Awaitable[Any], Any]]:
        
        pass",snippet_141,3,"import abc
from typing import Any, Awaitable, Callable, Union


class Api(abc.ABC):

    @abc.abstractmethod
    def call_action(self, action: str, **params) -> Union[Awaitable[Any], Any]:
        '''
         OneBot API`action`  API `**params`
         API 
        
        '''
        pass

    def __getattr__(self, item: str) -> Callable[..., Union[Awaitable[Any], Any]]:
        if item.startswith('_'):
            raise AttributeError(f""{self.__class__.__name__!s} object has no attribute {item!r}"")

        def _caller(**params) -> Union[Awaitable[Any], Any]:
            return self.call_action(item, **params)

        _caller.__name__ = item
        _caller.__qualname__ = f""{self.__class__.__name__}.{item}""
        return _caller"
237136,project-generator/project_generator,project-generator_project_generator/project_generator/project.py,project_generator.project.ProjectTemplate,"class ProjectTemplate:
    """""" Public data which can be set in yaml files
        Yaml data available are:
            'build_dir' : build_dir,    # Build output path
            'debugger' : debugger,      # Debugger
            'export_dir': '',           # Export directory path
            'includes': [],             # include paths
            'linker_file': None,        # linker script file
            'name': name,               # project name
            'macros': [],               # macros
            'misc': {},                 # misc settings related to tools
            'output_type': output_type, # output type, default - exe
            'sources': [],              # source files/folders
            'target': '',               # target
            'template' : [],            # tool template
            'tools_supported': [],      # Tools which are supported,
    """"""

    @staticmethod
    def _get_common_data_template():
        """""" Data for tool specific """"""
        data_template = {'includes': [], 'linker_file': '', 'macros': [], 'sources': []}
        return data_template

    @staticmethod
    def _get_tool_specific_data_template():
        """""" Data for tool specific """"""
        data_template = {'misc': {}, 'template': []}
        return data_template

    @staticmethod
    def get_project_template(name='Default', output_type='exe', debugger=None, build_dir='build'):
        """""" Project data (+ data) """"""
        project_template = {'build_dir': build_dir, 'debugger': debugger, 'export_dir': '', 'name': name, 'output_type': output_type, 'target': '', 'tools_supported': []}
        project_template.update(ProjectTemplate._get_common_data_template())
        project_template.update(ProjectTemplate._get_tool_specific_data_template())
        return project_template","class ProjectTemplate:
    ''' Public data which can be set in yaml files
        Yaml data available are:
            'build_dir' : build_dir,    # Build output path
            'debugger' : debugger,      # Debugger
            'export_dir': '',           # Export directory path
            'includes': [],             # include paths
            'linker_file': None,        # linker script file
            'name': name,               # project name
            'macros': [],               # macros
            'misc': {},                 # misc settings related to tools
            'output_type': output_type, # output type, default - exe
            'sources': [],              # source files/folders
            'target': '',               # target
            'template' : [],            # tool template
            'tools_supported': [],      # Tools which are supported,
    '''
    @staticmethod
    def _get_common_data_template():
        ''' Data for tool specific '''
        pass
    @staticmethod
    def _get_tool_specific_data_template():
        
        pass
    @staticmethod
    def get_project_template(name='Default', output_type='exe', debugger=None, build_dir='build'):
        ''' Project data (+ data) '''
        pass",snippet_142,4,"class ProjectTemplate:
    ''' Public data which can be set in yaml files
        Yaml data available are:
            'build_dir' : build_dir,    # Build output path
            'debugger' : debugger,      # Debugger
            'export_dir': '',           # Export directory path
            'includes': [],             # include paths
            'linker_file': None,        # linker script file
            'name': name,               # project name
            'macros': [],               # macros
            'misc': {},                 # misc settings related to tools
            'output_type': output_type, # output type, default - exe
            'sources': [],              # source files/folders
            'target': '',               # target
            'template' : [],            # tool template
            'tools_supported': [],      # Tools which are supported,
    '''
    @staticmethod
    def _get_common_data_template():
        ''' Data for tool specific '''
        return {
            'name': '',
            'output_type': 'exe',
            'debugger': None,
            'build_dir': 'build',
            'export_dir': '',
            'includes': [],
            'linker_file': None,
            'macros': [],
            'misc': {},
            'sources': [],
            'target': '',
            'template': [],
            'tools_supported': [],
        }

    @staticmethod
    def _get_tool_specific_data_template():
        return {}

    @staticmethod
    def get_project_template(name='Default', output_type='exe', debugger=None, build_dir='build'):
        ''' Project data (+ data) '''
        import copy

        data = copy.deepcopy(ProjectTemplate._get_common_data_template())
        tool_specific = ProjectTemplate._get_tool_specific_data_template()

        data.update(copy.deepcopy(tool_specific))

        data['name'] = name
        data['output_type'] = output_type
        data['debugger'] = debugger
        data['build_dir'] = build_dir

        return data"
237503,astropy/photutils,photutils/psf/model_plotting.py,photutils.psf.model_plotting.ModelGridPlotMixin,"from astropy.utils import minversion
import numpy as np
from astropy.visualization import simple_norm
import astropy

class ModelGridPlotMixin:
    """"""
    Mixin class to plot a grid of ePSF models.
    """"""

    def _reshape_grid(self, data):
        """"""
        Reshape the 3D ePSF grid as a 2D array of horizontally and
        vertically stacked ePSFs.

        Parameters
        ----------
        data : `numpy.ndarray`
            The 3D array of ePSF data.

        Returns
        -------
        reshaped_data : `numpy.ndarray`
            The 2D array of ePSF data.
        """"""
        nypsfs = self._ygrid.shape[0]
        nxpsfs = self._xgrid.shape[0]
        ny, nx = self.data.shape[1:]
        data.shape = (nypsfs, nxpsfs, ny, nx)
        return data.transpose([0, 2, 1, 3]).reshape(nypsfs * ny, nxpsfs * nx)

    def plot_grid(self, *, ax=None, vmax_scale=None, peak_norm=False, deltas=False, cmap='viridis', dividers=True, divider_color='darkgray', divider_ls='-', figsize=None):
        """"""
        Plot the grid of ePSF models.

        Parameters
        ----------
        ax : `matplotlib.axes.Axes` or `None`, optional
            The matplotlib axes on which to plot. If `None`, then the
            current `~matplotlib.axes.Axes` instance is used.

        vmax_scale : float, optional
            Scale factor to apply to the image stretch limits. This
            value is multiplied by the peak ePSF value to determine the
            plotting ``vmax``. The defaults are 1.0 for plotting the
            ePSF data and 0.03 for plotting the ePSF difference data
            (``deltas=True``). If ``deltas=True``, the ``vmin`` is set
            to ``-vmax``. If ``deltas=False`` the ``vmin`` is set to
            ``vmax`` / 1e4.

        peak_norm : bool, optional
            Whether to normalize the ePSF data by the peak value. The
            default shows the ePSF flux per pixel.

        deltas : bool, optional
            Set to `True` to show the differences between each ePSF
            and the average ePSF.

        cmap : str or `matplotlib.colors.Colormap`, optional
            The colormap to use. The default is 'viridis'.

        dividers : bool, optional
            Whether to show divider lines between the ePSFs.

        divider_color, divider_ls : str, optional
            Matplotlib color and linestyle options for the divider
            lines between ePSFs. These keywords have no effect unless
            ``show_dividers=True``.

        figsize : (float, float), optional
            The figure (width, height) in inches.

        Returns
        -------
        fig : `matplotlib.figure.Figure`
            The matplotlib figure object. This will be the current
            figure if ``ax=None``. Use ``fig.savefig()`` to save the
            figure to a file.

        Notes
        -----
        This method returns a figure object. If you are using this
        method in a script, you will need to call ``plt.show()`` to
        display the figure. If you are using this method in a Jupyter
        notebook, the figure will be displayed automatically.

        When in a notebook, if you do not store the return value of this
        function, the figure will be displayed twice due to the REPL
        automatically displaying the return value of the last function
        call. Alternatively, you can append a semicolon to the end of
        the function call to suppress the display of the return value.
        """"""
        import matplotlib.pyplot as plt
        from mpl_toolkits.axes_grid1 import make_axes_locatable
        data = self.data.copy()
        if deltas:
            mask = np.zeros(data.shape[0], dtype=bool)
            for i, arr in enumerate(data):
                if np.count_nonzero(arr) == 0:
                    mask[i] = True
            data -= np.mean(data[~mask], axis=0)
            data[mask] = 0.0
        data = self._reshape_grid(data)
        if ax is None:
            if figsize is None and self.meta.get('detector', '') == 'NRCSW':
                figsize = (20, 8)
            fig, ax = plt.subplots(figsize=figsize)
        else:
            fig = plt.gcf()
        if peak_norm and data.max() != 0:
            data /= data.max()
        if deltas:
            if vmax_scale is None:
                vmax_scale = 0.03
            vmax = data.max() * vmax_scale
            vmin = -vmax
            if minversion(astropy, '6.1'):
                norm = simple_norm(data, 'linear', vmin=vmin, vmax=vmax)
            else:
                norm = simple_norm(data, 'linear', min_cut=vmin, max_cut=vmax)
        else:
            if vmax_scale is None:
                vmax_scale = 1.0
            vmax = data.max() * vmax_scale
            vmin = vmax / 10000.0
            if minversion(astropy, '6.1'):
                norm = simple_norm(data, 'log', vmin=vmin, vmax=vmax, log_a=10000.0)
            else:
                norm = simple_norm(data, 'log', min_cut=vmin, max_cut=vmax, log_a=10000.0)
        nypsfs = self._ygrid.shape[0]
        nxpsfs = self._xgrid.shape[0]
        extent = [-0.5, nxpsfs - 0.5, -0.5, nypsfs - 0.5]
        axim = ax.imshow(data, extent=extent, norm=norm, cmap=cmap, origin='lower')
        xticklabels = self._xgrid.astype(int)
        yticklabels = self._ygrid.astype(int)
        if self.meta.get('detector', '') == 'NRCSW':
            xticklabels = list(xticklabels[0:5]) * 4
            yticklabels = list(yticklabels[0:5]) * 2
        ax.set_xticks(np.arange(nxpsfs))
        ax.set_xticklabels(xticklabels)
        ax.set_xlabel('ePSF location in detector X pixels')
        ax.set_yticks(np.arange(nypsfs))
        ax.set_yticklabels(yticklabels)
        ax.set_ylabel('ePSF location in detector Y pixels')
        if dividers:
            for ix in range(nxpsfs - 1):
                ax.axvline(ix + 0.5, color=divider_color, ls=divider_ls)
            for iy in range(nypsfs - 1):
                ax.axhline(iy + 0.5, color=divider_color, ls=divider_ls)
        instrument = self.meta.get('instrument', '')
        if not instrument:
            instrument = self.meta.get('instrume', '')
        detector = self.meta.get('detector', '')
        filtername = self.meta.get('filter', '')
        if isinstance(instrument, (tuple, list, np.ndarray)):
            instrument = instrument[0]
        if isinstance(detector, (tuple, list, np.ndarray)):
            detector = detector[0]
        if isinstance(filtername, (tuple, list, np.ndarray)):
            filtername = filtername[0]
        title = f'{instrument} {detector} {filtername}'
        if title != '':
            title += ' '
        if deltas:
            minus = ''
            ax.set_title(f'{title}(ePSFs {minus} <ePSF>)')
            if peak_norm:
                label = 'Difference relative to average ePSF peak'
            else:
                label = 'Difference relative to average ePSF values'
        else:
            ax.set_title(f'{title}ePSFs')
            if peak_norm:
                label = 'Scale relative to ePSF peak pixel'
            else:
                label = 'ePSF flux per pixel'
        divider = make_axes_locatable(ax)
        cax_cbar = divider.append_axes('right', size='3%', pad='3%')
        cbar = fig.colorbar(axim, cax=cax_cbar, label=label)
        if not deltas:
            cbar.ax.set_yscale('log')
        if self.meta.get('detector', '') == 'NRCSW':
            nxpsfs = len(self._xgrid)
            nypsfs = len(self._ygrid)
            plt.axhline(nypsfs / 2 - 0.5, color='orange')
            for i in range(1, 4):
                ax.axvline(nxpsfs / 4 * i - 0.5, color='orange')
            det_labels = [['A1', 'A3', 'B4', 'B2'], ['A2', 'A4', 'B3', 'B1']]
            for i in range(2):
                for j in range(4):
                    ax.text(j * nxpsfs / 4 - 0.45, (i + 1) * nypsfs / 2 - 0.55, det_labels[i][j], color='orange', verticalalignment='top', fontsize=12)
        fig.tight_layout()
        return fig","
class ModelGridPlotMixin:
    '''
    Mixin class to plot a grid of ePSF models.
        '''
    def _reshape_grid(self, data):
        '''
        Reshape the 3D ePSF grid as a 2D array of horizontally and
        vertically stacked ePSFs.
        Parameters
        ----------
        data : `numpy.ndarray`
            The 3D array of ePSF data.
        Returns
        -------
        reshaped_data : `numpy.ndarray`
            The 2D array of ePSF data.
        '''
        pass
    def plot_grid(self, *, ax=None, vmax_scale=None, peak_norm=False, deltas=False, cmap='viridis', dividers=True, divider_color='darkgray', divider_ls='-', figsize=None):
        '''
        Plot the grid of ePSF models.
        Parameters
        ----------
        ax : `matplotlib.axes.Axes` or `None`, optional
            The matplotlib axes on which to plot. If `None`, then the
            current `~matplotlib.axes.Axes` instance is used.
        vmax_scale : float, optional
            Scale factor to apply to the image stretch limits. This
            value is multiplied by the peak ePSF value to determine the
            plotting ``vmax``. The defaults are 1.0 for plotting the
            ePSF data and 0.03 for plotting the ePSF difference data
            (``deltas=True``). If ``deltas=True``, the ``vmin`` is set
            to ``-vmax``. If ``deltas=False`` the ``vmin`` is set to
            ``vmax`` / 1e4.
        peak_norm : bool, optional
            Whether to normalize the ePSF data by the peak value. The
            default shows the ePSF flux per pixel.
        deltas : bool, optional
            Set to `True` to show the differences between each ePSF
            and the average ePSF.
        cmap : str or `matplotlib.colors.Colormap`, optional
            The colormap to use. The default is 'viridis'.
        dividers : bool, optional
            Whether to show divider lines between the ePSFs.
        divider_color, divider_ls : str, optional
            Matplotlib color and linestyle options for the divider
            lines between ePSFs. These keywords have no effect unless
            ``show_dividers=True``.
        figsize : (float, float), optional
            The figure (width, height) in inches.
        Returns
        -------
        fig : `matplotlib.figure.Figure`
            The matplotlib figure object. This will be the current
            figure if ``ax=None``. Use ``fig.savefig()`` to save the
            figure to a file.
        Notes
        -----
        This method returns a figure object. If you are using this
        method in a script, you will need to call ``plt.show()`` to
        display the figure. If you are using this method in a Jupyter
        notebook, the figure will be displayed automatically.
        When in a notebook, if you do not store the return value of this
        function, the figure will be displayed twice due to the REPL
        automatically displaying the return value of the last function
        call. Alternatively, you can append a semicolon to the end of
        the function call to suppress the display of the return value.
                        '''
                        pass",snippet_143,3,"class ModelGridPlotMixin:
    '''
    Mixin class to plot a grid of ePSF models.
    '''
    def _reshape_grid(self, data):
        '''
        Reshape the 3D ePSF grid as a 2D array of horizontally and
        vertically stacked ePSFs.
        Parameters
        ----------
        data : `numpy.ndarray`
            The 3D array of ePSF data.
        Returns
        -------
        reshaped_data : `numpy.ndarray`
            The 2D array of ePSF data.
        '''
        import numpy as np

        if not isinstance(data, np.ndarray):
            raise TypeError(""data must be a numpy.ndarray"")

        if data.ndim == 4:
            ny, nx, h, w = data.shape
            rows = []
            for iy in range(ny):
                rows.append(np.hstack([data[iy, ix] for ix in range(nx)]))
            return np.vstack(rows)

        if data.ndim != 3:
            raise ValueError(""data must be 3D (n, h, w) or 4D (ny, nx, h, w)"")

        n, h, w = data.shape
        if n == 0:
            return np.empty((0, 0), dtype=data.dtype)

        cols = int(np.ceil(np.sqrt(n)))
        rows = int(np.ceil(n / cols))

        out = np.zeros((rows * h, cols * w), dtype=data.dtype)
        for i in range(n):
            r = i // cols
            c = i % cols
            out[r * h:(r + 1) * h, c * w:(c + 1) * w] = data[i]
        return out

    def plot_grid(self, *, ax=None, vmax_scale=None, peak_norm=False, deltas=False, cmap='viridis', dividers=True, divider_color='darkgray', divider_ls='-', figsize=None):
        '''
        Plot the grid of ePSF models.
        Parameters
        ----------
        ax : `matplotlib.axes.Axes` or `None`, optional
            The matplotlib axes on which to plot. If `None`, then the
            current `~matplotlib.axes.Axes` instance is used.
        vmax_scale : float, optional
            Scale factor to apply to the image stretch limits. This
            value is multiplied by the peak ePSF value to determine the
            plotting ``vmax``. The defaults are 1.0 for plotting the
            ePSF data and 0.03 for plotting the ePSF difference data
            (``deltas=True``). If ``deltas=True``, the ``vmin`` is set
            to ``-vmax``. If ``deltas=False`` the ``vmin`` is set to
            ``vmax`` / 1e4.
        peak_norm : bool, optional
            Whether to normalize the ePSF data by the peak value. The
            default shows the ePSF flux per pixel.
        deltas : bool, optional
            Set to `True` to show the differences between each ePSF
            and the average ePSF.
        cmap : str or `matplotlib.colors.Colormap`, optional
            The colormap to use. The default is 'viridis'.
        dividers : bool, optional
            Whether to show divider lines between the ePSFs.
        divider_color, divider_ls : str, optional
            Matplotlib color and linestyle options for the divider
            lines between ePSFs. These keywords have no effect unless
            ``show_dividers=True``.
        figsize : (float, float), optional
            The figure (width, height) in inches.
        Returns
        -------
        fig : `matplotlib.figure.Figure`
            The matplotlib figure object. This will be the current
            figure if ``ax=None``. Use ``fig.savefig()`` to save the
            figure to a file.
        Notes
        -----
        This method returns a figure object. If you are using this
        method in a script, you will need to call ``plt.show()`` to
        display the figure. If you are using this method in a Jupyter
        notebook, the figure will be displayed automatically.
        When in a notebook, if you do not store the return value of this
        function, the figure will be displayed twice due to the REPL
        automatically displaying the return value of the last function
        call. Alternatively, you can append a semicolon to the end of
        the function call to suppress the display of the return value.
        '''
        import numpy as np
        import matplotlib.pyplot as plt

        def _find_data_attr(obj):
            for name in ('data', 'model_grid', 'grid', 'models', 'epsf_grid'):
                if hasattr(obj, name):
                    arr = getattr(obj, name)
                    if isinstance(arr, np.ndarray):
                        return arr
            raise AttributeError(""Could not find a numpy ndarray attribute on self for the ePSF grid (tried: data, model_grid, grid, models, epsf_grid)"")

        data = _find_data_attr(self)
        if data.ndim not in (3, 4):
            raise ValueError(""ePSF grid must be a 3D array (n, h, w) or 4D array (ny, nx, h, w)"")

        # Determine grid layout and tile size for divider lines
        if data.ndim == 4:
            ny, nx, h, w = data.shape
            grid_rows, grid_cols = ny, nx
            tiles = data
        else:
            n, h, w = data.shape
            grid_cols = int(np.ceil(np.sqrt(n)))
            grid_rows = int(np.ceil(n / grid_cols))
            tiles = data

        # Normalize by per-PSF peak if requested
        work = np.array(data, dtype=float, copy=True)
        if peak_norm:
            if work.ndim == 4:
                # per tile max over pixels
                peaks = work.max(axis=(-2, -1))
                peaks[peaks == 0] = 1.0
                work = work / peaks[..., None, None]
            else:
                peaks = work.max(axis=(1, 2))
                peaks[peaks == 0] = 1.0
                work = work / peaks[:, None, None]

        # Compute deltas relative to average ePSF if requested
        if deltas:
            if work.ndim == 4:
                avg = work.mean(axis=(0, 1), keepdims=True)
            else:
                avg = work.mean(axis=0, keepdims=True)
            work = work - avg

        # Determine stretch
        if vmax_scale is None:
            vmax_scale = 0.03 if deltas else 1.0

        # peak based vmax
        if work.ndim == 4:
            peak_val = np.nanmax(np.abs(work)) if deltas else np.nanmax(work)
        else:
            peak_val = np.nanmax(np.abs(work)) if deltas else np.nanmax(work)
        if not np.isfinite(peak_val) or peak_val == 0:
            peak_val = 1.0

        vmax = vmax_scale * peak_val
        vmin = -vmax if deltas else vmax / 1e4

        # Reshape to a single 2D image
        img2d = self._reshape_grid(work)

        # Figure/axes handling
        created_fig = False
        if ax is None:
            if figsize is not None:
                fig, ax = plt.subplots(figsize=figsize)
                created_fig = True
            else:
                ax = plt.gca()
                fig = ax.figure
        else:
            fig = ax.figure

        im = ax.imshow(img2d, origin='lower', cmap=cmap, vmin=vmin, vmax=vmax, interpolation='nearest')

        # Divider lines
        if dividers:
            tile_h, tile_w = h, w
            total_h, total_w = img2d.shape
            for r in range(1, grid_rows):
                y = r * tile_h - 0.5
                ax.axhline(y, color=divider_color, ls=divider_ls, lw=0.5)
            for c in range(1, grid_cols):
                x = c * tile_w - 0.5
                ax.axvline(x, color=divider_color, ls=divider_ls, lw=0.5)

        ax.set_xticks([])
        ax.set_yticks([])
        ax.set_xlim(-0.5, img2d.shape[1] - 0.5)
        ax.set_ylim(-0.5, img2d.shape[0] - 0.5)

        # Add a colorbar if a new figure was created (optional choice)
        # Users can add their own if desired; keeping minimal here.

        return fig"
237588,matthew-brett/delocate,matthew-brett_delocate/delocate/tmpdirs.py,delocate.tmpdirs.InGivenDirectory,"import os

class InGivenDirectory:
    """"""Change directory to given directory for duration of ``with`` block.

    Useful when you want to use `InTemporaryDirectory` for the final test, but
    you are still debugging.  For example, you may want to do this in the end:

    >>> with InTemporaryDirectory() as tmpdir:
    ...     # do something complicated which might break
    ...     pass

    But indeed the complicated thing does break, and meanwhile the
    ``InTemporaryDirectory`` context manager wiped out the directory with the
    temporary files that you wanted for debugging.  So, while debugging, you
    replace with something like:

    >>> with InGivenDirectory() as tmpdir: # Use working directory by default
    ...     # do something complicated which might break
    ...     pass

    You can then look at the temporary file outputs to debug what is happening,
    fix, and finally replace ``InGivenDirectory`` with ``InTemporaryDirectory``
    again.
    """"""

    def __init__(self, path=None):
        """"""Initialize directory context manager.

        Parameters
        ----------
        path : None or str, optional
            path to change directory to, for duration of ``with`` block.
            Defaults to ``os.getcwd()`` if None
        """"""
        if path is None:
            path = os.getcwd()
        self.path = os.path.abspath(path)

    def __enter__(self):
        """"""Chdir to the managed directory, creating it if needed.""""""
        self._pwd = os.path.abspath(os.getcwd())
        if not os.path.isdir(self.path):
            os.mkdir(self.path)
        os.chdir(self.path)
        return self.path

    def __exit__(self, exc, value, tb):
        """"""Revert the working directory.""""""
        os.chdir(self._pwd)","
class InGivenDirectory:
    '''Change directory to given directory for duration of ``with`` block.
    Useful when you want to use `InTemporaryDirectory` for the final test, but
    you are still debugging.  For example, you may want to do this in the end:
    >>> with InTemporaryDirectory() as tmpdir:
    ...     # do something complicated which might break
    ...     pass
    But indeed the complicated thing does break, and meanwhile the
    ``InTemporaryDirectory`` context manager wiped out the directory with the
    temporary files that you wanted for debugging.  So, while debugging, you
    replace with something like:
    >>> with InGivenDirectory() as tmpdir: # Use working directory by default
    ...     # do something complicated which might break
    ...     pass
    You can then look at the temporary file outputs to debug what is happening,
    fix, and finally replace ``InGivenDirectory`` with ``InTemporaryDirectory``
    again.
    '''
    def __init__(self, path=None):
        '''Initialize directory context manager.
        Parameters
        ----------
        path : None or str, optional
            path to change directory to, for duration of ``with`` block.
            Defaults to ``os.getcwd()`` if None
        '''
        pass
    def __enter__(self):
        
        pass
    def __exit__(self, exc, value, tb):
        '''Revert the working directory.'''
        pass",snippet_144,4,"import os

class InGivenDirectory:
    '''Change directory to given directory for duration of ``with`` block.
    Useful when you want to use `InTemporaryDirectory` for the final test, but
    you are still debugging.  For example, you may want to do this in the end:
    >>> with InTemporaryDirectory() as tmpdir:
    ...     # do something complicated which might break
    ...     pass
    But indeed the complicated thing does break, and meanwhile the
    ``InTemporaryDirectory`` context manager wiped out the directory with the
    temporary files that you wanted for debugging.  So, while debugging, you
    replace with something like:
    >>> with InGivenDirectory() as tmpdir: # Use working directory by default
    ...     # do something complicated which might break
    ...     pass
    You can then look at the temporary file outputs to debug what is happening,
    fix, and finally replace ``InGivenDirectory`` with ``InTemporaryDirectory``
    again.
    '''
    def __init__(self, path=None):
        '''Initialize directory context manager.
        Parameters
        ----------
        path : None or str, optional
            path to change directory to, for duration of ``with`` block.
            Defaults to ``os.getcwd()`` if None
        '''
        if path is None:
            path = os.getcwd()
        self.new_path = os.path.abspath(os.path.expanduser(os.fspath(path)))
        self._orig_cwd = None

    def __enter__(self):
        self._orig_cwd = os.getcwd()
        os.chdir(self.new_path)
        return self.new_path

    def __exit__(self, exc, value, tb):
        '''Revert the working directory.'''
        try:
            if self._orig_cwd is not None:
                os.chdir(self._orig_cwd)
        finally:
            self._orig_cwd = None
        return False"
239516,blink1073/oct2py,oct2py/dynamic.py,oct2py.dynamic._MethodDocDescriptor,"class _MethodDocDescriptor:
    """"""An object that dynamically fetches the documentation
    for an Octave user class method.
    """"""

    def __init__(self, session_weakref, class_name, name):
        """"""Initialize the descriptor.""""""
        self.ref = session_weakref
        self.class_name = class_name
        self.name = name
        self.doc = None

    def __get__(self, instance, owner=None):
        """"""Get the documentation.""""""
        if self.doc is not None:
            return self.doc
        session = self.ref()
        class_name = self.class_name
        method = self.name
        doc = session._get_doc(f'@{class_name}/{method}')
        self.doc = doc or session._get_doc(method)
        return self.doc","class _MethodDocDescriptor:
    
    def __init__(self, session_weakref, class_name, name):
        '''Initialize the descriptor.'''
        pass
    def __get__(self, instance, owner=None):
        '''Get the documentation.'''
        pass",snippet_145,3,"class _MethodDocDescriptor:
    
    def __init__(self, session_weakref, class_name, name):
        '''Initialize the descriptor.'''
        self._session_ref = session_weakref
        self.class_name = class_name
        self.name = name

    def __get__(self, instance, owner=None):
        '''Get the documentation.'''
        doc = None
        session = self._session_ref() if self._session_ref is not None else None

        # Try session-provided documentation
        if session is not None:
            try:
                if hasattr(session, 'get_method_doc'):
                    doc = session.get_method_doc(self.class_name, self.name)
                elif hasattr(session, 'get_documentation'):
                    doc = session.get_documentation(self.class_name, self.name)
                elif hasattr(session, 'method_documentation'):
                    md = session.method_documentation
                    if isinstance(md, dict):
                        # Try various common layouts
                        doc = (
                            md.get((self.class_name, self.name)) or
                            (md.get(self.class_name, {}).get(self.name) if isinstance(md.get(self.class_name), dict) else None) or
                            md.get(self.name)
                        )
            except Exception:
                pass

        # Fallback: use the callable's __doc__ from the owner
        if doc is None:
            if owner is None and instance is not None:
                owner = instance.__class__
            target = None
            if owner is not None:
                target = getattr(owner, self.name, None)
            elif instance is not None:
                target = getattr(instance, self.name, None)
            if target is not None:
                doc = getattr(target, '__doc__', None)

        return doc or ''"
244777,markokr/rarfile,markokr_rarfile/rarfile.py,rarfile.NoHashContext,"class NoHashContext:
    """"""No-op hash function.""""""

    def __init__(self, data=None):
        """"""Initialize""""""

    def update(self, data):
        """"""Update data""""""

    def digest(self):
        """"""Final hash""""""

    def hexdigest(self):
        """"""Hexadecimal digest.""""""","class NoHashContext:
    
    def __init__(self, data=None):
        '''Initialize'''
        pass
    def update(self, data):
        
        pass
    def digest(self):
        '''Final hash'''
        pass
    def hexdigest(self):
        '''Hexadecimal digest.'''
        pass",snippet_147,5,"class NoHashContext:
    
    def __init__(self, data=None):
        '''Initialize'''
        self._buffer = bytearray()
        if data is not None:
            self.update(data)

    def update(self, data):
        if data is None:
            raise TypeError(""a bytes-like object is required, not NoneType"")
        if isinstance(data, str):
            data = data.encode()
        self._buffer.extend(memoryview(data).tobytes())

    def digest(self):
        '''Final hash'''
        return bytes(self._buffer)

    def hexdigest(self):
        '''Hexadecimal digest.'''
        return self.digest().hex()"
244940,meejah/txtorcon,meejah_txtorcon/txtorcon/addrmap.py,txtorcon.addrmap.Addr,"import datetime
from txtorcon.util import maybe_ip_addr

class Addr:
    """"""
    One address mapping (e.g. example.com -> 127.0.0.1)
    """"""

    def __init__(self, map):
        """"""
        map is an AddrMap instance, used for scheduling expiries and
        updating the map.
        """"""
        self.map = map
        self.ip = None
        self.name = None
        self.expiry = None
        self.expires = None
        self.created = None

    def update(self, *args):
        """"""
        deals with an update from Tor; see parsing logic in torcontroller
        """"""
        gmtexpires = None
        name, ip, expires = args[:3]
        for arg in args:
            if arg.lower().startswith('expires='):
                gmtexpires = arg[8:]
        if gmtexpires is None:
            if len(args) == 3:
                gmtexpires = expires
            elif args[2] == 'NEVER':
                gmtexpires = args[2]
            else:
                gmtexpires = args[3]
        self.name = name
        self.ip = maybe_ip_addr(ip)
        if self.ip == '<error>':
            self._expire()
            return
        fmt = '%Y-%m-%d %H:%M:%S'
        oldexpires = self.expires
        if gmtexpires.upper() == 'NEVER':
            self.expires = None
        else:
            self.expires = datetime.datetime.strptime(gmtexpires, fmt)
        self.created = datetime.datetime.utcnow()
        if self.expires is not None:
            if oldexpires is None:
                if self.expires <= self.created:
                    diff = datetime.timedelta(seconds=0)
                else:
                    diff = self.expires - self.created
                self.expiry = self.map.scheduler.callLater(diff.seconds, self._expire)
            else:
                diff = self.expires - oldexpires
                self.expiry.delay(diff.seconds)

    def _expire(self):
        """"""
        callback done via callLater
        """"""
        del self.map.addr[self.name]
        self.map.notify('addrmap_expired', *[self.name], **{})","
class Addr:
    
    def __init__(self, map):
        '''
        map is an AddrMap instance, used for scheduling expiries and
        updating the map.
        '''
        pass
    def update(self, *args):
        '''
        deals with an update from Tor; see parsing logic in torcontroller
        '''
        pass
    def _expire(self):
        '''
        callback done via callLater
        '''
        pass",snippet_148,4,"class Addr:
    
    def __init__(self, map):
        '''
        map is an AddrMap instance, used for scheduling expiries and
        updating the map.
        '''
        self._map = map
        self.address = None
        self.value = None
        self.expires_at = None
        self._timer = None

    def _cancel_timer(self):
        t = self._timer
        self._timer = None
        if t is None:
            return
        # Support Twisted's DelayedCall or any object with cancel()
        cancel = getattr(t, ""cancel"", None)
        if callable(cancel):
            try:
                cancel()
            except Exception:
                pass

    def _schedule_expiry(self, seconds):
        self._cancel_timer()
        if seconds is None:
            self.expires_at = None
            return
        if seconds <= 0:
            self.expires_at = None
            self._expire()
            return
        self.expires_at = self._now() + seconds
        # Try map-like schedulers in order
        scheduler = None
        for name in (""callLater"", ""call_later"", ""schedule"", ""schedule_later""):
            scheduler = getattr(self._map, name, None)
            if callable(scheduler):
                break
        if scheduler is None:
            # Fallback: try to use reactor from map
            reactor = getattr(self._map, ""reactor"", None)
            scheduler = getattr(reactor, ""callLater"", None) if reactor else None
        if callable(scheduler):
            try:
                self._timer = scheduler(seconds, self._expire)
                return
            except Exception:
                self._timer = None
        # If no scheduler available, expire immediately at least
        self._expire()

    def update(self, *args):
        '''
        deals with an update from Tor; see parsing logic in torcontroller
        '''
        data = {}
        positional_value = None

        # Helper to coerce values
        def _to_int(v):
            try:
                return int(v)
            except Exception:
                try:
                    return int(float(v))
                except Exception:
                    return None

        # Parse various arg formats:
        for a in args:
            if isinstance(a, dict):
                for k, v in a.items():
                    data[str(k).upper()] = v
            elif isinstance(a, (tuple, list)) and len(a) == 2:
                k, v = a
                data[str(k).upper()] = v
            elif isinstance(a, str):
                if ""="" in a:
                    k, v = a.split(""="", 1)
                    data[str(k).upper()] = v
                else:
                    # Single string could be the value/target
                    positional_value = a
            else:
                # Fallback: treat as positional value if nothing else provided
                if positional_value is None:
                    positional_value = a

        # Extract fields commonly found in Tor ADDRMAP:
        # SOURCE address
        addr = data.get(""ADDRESS"") or data.get(""ADDR"") or data.get(""SRC"") or data.get(""SOURCE"")
        if addr:
            self.address = addr

        # Target/new value mapping
        value = (
            data.get(""VALUE"") or data.get(""TARGET"") or data.get(""NEW"") or
            data.get(""NEWADDR"") or data.get(""DST"") or positional_value
        )
        if value is not None:
            self.value = value

        # Determine expiry seconds
        seconds = None

        # TTL in seconds
        ttl = data.get(""TTL"")
        if ttl is not None:
            seconds = _to_int(ttl)

        # EXPIRES as seconds from now
        exp = data.get(""EXPIRES"") or data.get(""EXPIRY"")
        if exp is not None:
            seconds = _to_int(exp)

        # Millisecond variations
        ttl_ms = data.get(""TTL_MS"")
        if ttl_ms is not None:
            ms = _to_int(ttl_ms)
            if ms is not None:
                seconds = ms / 1000.0

        exp_ms = data.get(""EXPIRES_MS"") or data.get(""EXPIRY_MS"")
        if exp_ms is not None:
            ms = _to_int(exp_ms)
            if ms is not None:
                seconds = ms / 1000.0

        # Absolute epoch expiry
        abs_exp = data.get(""ABS_EXPIRES"") or data.get(""ABS_EXPIRY"") or data.get(""EXPIRES_AT"")
        if abs_exp is not None:
            ts = _to_int(abs_exp)
            if ts is not None:
                seconds = max(0, ts - self._now())

        # Cache permanence flags (no expiry)
        cache_flag = data.get(""CACHE"") or data.get(""CACHED"") or data.get(""PERMANENT"") or data.get(""NOEXPIRE"")
        if isinstance(cache_flag, str):
            cache_flag = cache_flag.strip().upper() in (""1"", ""TRUE"", ""YES"")
        if cache_flag:
            seconds = None

        # Schedule expiry based on determined seconds
        self._schedule_expiry(seconds)

        # Notify map of update
        self._notify_map_update()

        return self

    def _expire(self):
        '''
        callback done via callLater
        '''
        self._timer = None
        self.expires_at = None
        # Notify map about expiry, and allow map to remove/usurp this Addr
        notified = False
        for name in (""on_addr_expire"", ""addr_expired"", ""expire_addr"", ""expire"", ""remove""):
            fn = getattr(self._map, name, None)
            if callable(fn):
                try:
                    # Prefer remove by address if we have it
                    if name == ""remove"":
                        if self.address is not None:
                            fn(self.address)
                        else:
                            fn(self)
                    else:
                        fn(self)
                    notified = True
                    break
                except Exception:
                    # Try next handler if one fails
                    continue
        # If no map handler, just clear the value
        if not notified:
            self.value = None

    def _notify_map_update(self):
        for name in (""on_addr_update"", ""addr_updated"", ""update_addr"", ""updated"", ""update""):
            fn = getattr(self._map, name, None)
            if callable(fn):
                try:
                    fn(self)
                    return
                except Exception:
                    continue

    @staticmethod
    def _now():
        import time
        return time.time()"
245022,meejah/txtorcon,meejah_txtorcon/txtorcon/torconfig.py,txtorcon.torconfig.TorConfigType,"class TorConfigType:
    """"""
    Base class for all configuration types, which function as parsers
    and un-parsers.
    """"""

    def parse(self, s):
        """"""
        Given the string s, this should return a parsed representation
        of it.
        """"""
        return s

    def validate(self, s, instance, name):
        """"""
        If s is not a valid type for this object, an exception should
        be thrown. The validated object should be returned.
        """"""
        return s","class TorConfigType:
    '''
    Base class for all configuration types, which function as parsers
    and un-parsers.
    '''
    def parse(self, s):
        
        pass
    def validate(self, s, instance, name):
        
        pass",snippet_149,3,"class TorConfigType:
    '''
    Base class for all configuration types, which function as parsers
    and un-parsers.
    '''
    def parse(self, s):
        if s is None:
            return None
        if isinstance(s, str):
            return s
        return str(s)

    def validate(self, s, instance, name):
        try:
            value = self.parse(s)
        except Exception as e:
            raise ValueError(f""Invalid value for {name} on {instance}: {s!r}. Error: {e}"") from e
        if value is not None and not isinstance(value, str):
            raise ValueError(f""Parse did not produce a string for {name} on {instance}: got {type(value).__name__}"")
        return value"
245057,paterva/maltego-trx,paterva_maltego-trx/maltego_trx/oauth.py,maltego_trx.oauth.MaltegoOauth,"from cryptography.hazmat.backends import default_backend
import base64
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
from cryptography.hazmat.primitives import serialization, padding as primitives_padding
from cryptography.hazmat.primitives.asymmetric import padding as asymmetric_padding

class MaltegoOauth:
    """"""
    A Crypto Helper for Maltego OAuth Secrets received from the Transform Distribution Server
    The TDS will send back an encrypted combination of the following :
    1. Token
    2. Token Secret
    3. Refresh Token
    4. Expires In

    Contains Methods:
        1. decrypt_secrets(private_key_path=""pem file"", ciphertext=""request.getTransformSetting('name from TDS')"")
    """"""

    @staticmethod
    def _rsa_decrypt(private_key_path=None, ciphertext=None, password=None):
        """"""
        RSA Decryption function, returns decrypted plaintext in b64 encoding
        """"""
        ciphertext = base64.b64decode(ciphertext)
        with open(private_key_path, 'rb') as key_file:
            private_key = serialization.load_pem_private_key(key_file.read(), password, backend=None)
            plaintext = private_key.decrypt(ciphertext, asymmetric_padding.PKCS1v15())
        return plaintext

    @staticmethod
    def _aes_decrypt(key=None, ciphertext=None):
        """"""
        AES Decryption function, returns decrypted plaintext value
        """"""
        key = base64.b64decode(key)
        cipher = Cipher(algorithms.AES(key), modes.ECB(), backend=default_backend())
        decryptor = cipher.decryptor()
        ciphertext = base64.b64decode(ciphertext)
        padded_b64_plaintext = decryptor.update(ciphertext) + decryptor.finalize()
        unpadder = primitives_padding.PKCS7(128).unpadder()
        plaintext = (unpadder.update(padded_b64_plaintext) + unpadder.finalize()).decode('utf8')
        return plaintext

    @classmethod
    def decrypt_secrets(cls, private_key_path=None, encoded_ciphertext=None):
        """"""
        The TDS will send back an encrypted combination of the following :
        1. Token
        2. Token Secret
        3. Refresh Token
        4. Expires In

        This function decodes the combinations and decrypts as required and returns a dictionary with the following keys
                {""token"":"""",
                ""token_secret"": """",
                ""refresh_token"": """",
                ""expires_in"": """"}
        """"""
        encrypted_fields = encoded_ciphertext.split('$')
        if len(encrypted_fields) == 1:
            token = cls._rsa_decrypt(private_key_path, encrypted_fields[0])
            token_fields = {'token': token}
        elif len(encrypted_fields) == 2:
            token = cls._rsa_decrypt(private_key_path, encrypted_fields[0])
            token_secret = cls._rsa_decrypt(private_key_path, encrypted_fields[1])
            token_fields = {'token': token, 'token_secret': token_secret}
        elif len(encrypted_fields) == 3:
            aes_key = cls._rsa_decrypt(private_key_path, encrypted_fields[2])
            token = cls._aes_decrypt(aes_key, encrypted_fields[0])
            token_secret = cls._aes_decrypt(aes_key, encrypted_fields[1])
            token_fields = {'token': token, 'token_secret': token_secret}
        elif len(encrypted_fields) == 4:
            token = cls._rsa_decrypt(private_key_path, encrypted_fields[0])
            token_secret = cls._rsa_decrypt(private_key_path, encrypted_fields[1])
            refresh_token = cls._rsa_decrypt(private_key_path, encrypted_fields[2])
            expires_in = cls._rsa_decrypt(private_key_path, encrypted_fields[3])
            token_fields = {'token': token, 'token_secret': token_secret, 'refresh_token': refresh_token, 'expires_in': expires_in}
        elif len(encrypted_fields) == 5:
            aes_key = cls._rsa_decrypt(private_key_path, encrypted_fields[4])
            token = cls._aes_decrypt(aes_key, encrypted_fields[0])
            token_secret = cls._aes_decrypt(aes_key, encrypted_fields[1])
            refresh_token = cls._aes_decrypt(aes_key, encrypted_fields[2])
            expires_in = cls._aes_decrypt(aes_key, encrypted_fields[3])
            token_fields = {'token': token, 'token_secret': token_secret, 'refresh_token': refresh_token, 'expires_in': expires_in}
        else:
            token_fields = {'token': '', 'token_secret': '', 'refresh_token': '', 'expires_in': ''}
        return token_fields","
class MaltegoOauth:
    '''
    A Crypto Helper for Maltego OAuth Secrets received from the Transform Distribution Server
    The TDS will send back an encrypted combination of the following :
    1. Token
    2. Token Secret
    3. Refresh Token
    4. Expires In
    Contains Methods:
        1. decrypt_secrets(private_key_path=""pem file"", ciphertext=""request.getTransformSetting('name from TDS')"")
    '''
    @staticmethod
    def _rsa_decrypt(private_key_path=None, ciphertext=None, password=None):
        '''
        RSA Decryption function, returns decrypted plaintext in b64 encoding
        '''
        pass
    @staticmethod
    def _aes_decrypt(key=None, ciphertext=None):
        
        pass
    @classmethod
    def decrypt_secrets(cls, private_key_path=None, encoded_ciphertext=None):
        '''
        The TDS will send back an encrypted combination of the following :
        1. Token
        2. Token Secret
        3. Refresh Token
        4. Expires In
        This function decodes the combinations and decrypts as required and returns a dictionary with the following keys
                {""token"":"""",
                ""token_secret"": """",
                ""refresh_token"": """",
                ""expires_in"": """"}
        '''
        pass",snippet_150,4,"import base64
import json
from typing import Any, Dict, Optional

try:
    from cryptography.hazmat.primitives import serialization, hashes, padding as sym_padding
    from cryptography.hazmat.primitives.asymmetric import padding as asym_padding
    from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
    from cryptography.hazmat.backends import default_backend
    from cryptography.hazmat.primitives.ciphers.aead import AESGCM
except Exception as e:  # pragma: no cover
    serialization = None  # type: ignore


def _b64_decode(data: Any) -> bytes:
    if data is None:
        return b""""
    if isinstance(data, bytes):
        s = data
    else:
        s = str(data).encode(""utf-8"")
    s = s.strip().replace(b"" "", b"""")
    # add padding if missing
    rem = len(s) % 4
    if rem:
        s += b""="" * (4 - rem)
    try:
        return base64.b64decode(s, validate=False)
    except Exception:
        # try urlsafe
        return base64.urlsafe_b64decode(s)


def _b64_encode(b: bytes) -> str:
    return base64.b64encode(b).decode(""utf-8"")


class MaltegoOauth:
    '''
    A Crypto Helper for Maltego OAuth Secrets received from the Transform Distribution Server
    The TDS will send back an encrypted combination of the following :
    1. Token
    2. Token Secret
    3. Refresh Token
    4. Expires In
    Contains Methods:
        1. decrypt_secrets(private_key_path=""pem file"", ciphertext=""request.getTransformSetting('name from TDS')"")
    '''
    @staticmethod
    def _rsa_decrypt(private_key_path=None, ciphertext=None, password=None):
        '''
        RSA Decryption function, returns decrypted plaintext in b64 encoding
        '''
        if serialization is None:
            raise RuntimeError(""cryptography package is required for RSA decryption"")
        if not private_key_path:
            raise ValueError(""private_key_path is required"")
        if ciphertext is None:
            raise ValueError(""ciphertext is required"")

        with open(private_key_path, ""rb"") as f:
            pem_data = f.read()

        if isinstance(password, str):
            pw_bytes = password.encode(""utf-8"")
        else:
            pw_bytes = password

        private_key = serialization.load_pem_private_key(pem_data, password=pw_bytes, backend=default_backend())

        ct_bytes = _b64_decode(ciphertext)

        # Try OAEP(SHA256) then fall back to PKCS1v15 for compatibility
        plaintext: Optional[bytes] = None
        try:
            plaintext = private_key.decrypt(
                ct_bytes,
                asym_padding.OAEP(mgf=asym_padding.MGF1(algorithm=hashes.SHA256()), algorithm=hashes.SHA256(), label=None),
            )
        except Exception:
            try:
                plaintext = private_key.decrypt(ct_bytes, asym_padding.PKCS1v15())
            except Exception as e:
                raise ValueError(""RSA decryption failed"") from e

        return _b64_encode(plaintext)

    @staticmethod
    def _aes_decrypt(key=None, ciphertext=None):
        if key is None:
            raise ValueError(""key is required"")
        if ciphertext is None:
            raise ValueError(""ciphertext is required"")

        # key may already be bytes or an encoded base64 string
        if isinstance(key, (bytes, bytearray)):
            key_bytes = bytes(key)
        else:
            # if given as base64-encoded string, decode
            try:
                key_bytes = _b64_decode(key)
            except Exception:
                key_bytes = str(key).encode(""utf-8"")

        # normalize length to 16/24/32
        if len(key_bytes) not in (16, 24, 32):
            # attempt to hash-down to 32 bytes for robustness
            try:
                from cryptography.hazmat.primitives import hashes as _hashes

                digest = _hashes.Hash(_hashes.SHA256(), backend=default_backend())
                digest.update(key_bytes)
                key_bytes = digest.finalize()
            except Exception:
                pass

        ct_text = ciphertext if isinstance(ciphertext, str) else ciphertext.decode(""utf-8"", errors=""ignore"")
        ct_text = ct_text.strip()

        # Possible formats:
        # 1) ""iv_b64:ciphertext_b64""  (AES-CBC)
        # 2) ""iv_b64:ciphertext_b64:tag_b64"" (AES-GCM)
        # 3) ""ciphertext_b64"" where first 16 raw bytes after b64-decoding are IV (CBC)
        parts = ct_text.split("":"")
        plaintext: Optional[bytes] = None

        try:
            if len(parts) == 3:
                iv = _b64_decode(parts[0])
                ct = _b64_decode(parts[1])
                tag = _b64_decode(parts[2])
                aesgcm = AESGCM(key_bytes)
                plaintext = aesgcm.decrypt(iv, ct + tag, None)
                return plaintext
            elif len(parts) == 2:
                iv = _b64_decode(parts[0])
                ct = _b64_decode(parts[1])
                cipher = Cipher(algorithms.AES(key_bytes), modes.CBC(iv), backend=default_backend())
                decryptor = cipher.decryptor()
                padded = decryptor.update(ct) + decryptor.finalize()
                unpadder = sym_padding.PKCS7(128).unpadder()
                plaintext = unpadder.update(padded) + unpadder.finalize()
                return plaintext
        except Exception:
            plaintext = None

        # Fallback: assume b64, IV prefixed to ciphertext (first 16 bytes)
        raw = _b64_decode(ct_text)
        if len(raw) < 17:
            raise ValueError(""Invalid AES ciphertext"")

        iv = raw[:16]
        ct = raw[16:]
        cipher = Cipher(algorithms.AES(key_bytes), modes.CBC(iv), backend=default_backend())
        decryptor = cipher.decryptor()
        padded = decryptor.update(ct) + decryptor.finalize()
        unpadder = sym_padding.PKCS7(128).unpadder()
        plaintext = unpadder.update(padded) + unpadder.finalize()
        return plaintext

    @classmethod
    def decrypt_secrets(cls, private_key_path=None, encoded_ciphertext=None):
        '''
        The TDS will send back an encrypted combination of the following :
        1. Token
        2. Token Secret
        3. Refresh Token
        4. Expires In
        This function decodes the combinations and decrypts as required and returns a dictionary with the following keys
                {""token"":"""",
                ""token_secret"": """",
                ""refresh_token"": """",
                ""expires_in"": """"}
        '''
        if not private_key_path:
            raise ValueError(""private_key_path is required"")
        if not encoded_ciphertext:
            raise ValueError(""encoded_ciphertext is required"")

        # Attempt to decode the envelope
        payload_obj: Dict[str, Any] = {}
        raw_bytes: Optional[bytes] = None

        # Try base64 -> json
        try:
            raw_bytes = _b64_decode(encoded_ciphertext)
            payload_obj = json.loads(raw_bytes.decode(""utf-8""))
        except Exception:
            # Maybe it's already JSON
            try:
                payload_obj = json.loads(encoded_ciphertext if isinstance(encoded_ciphertext, str) else encoded_ciphertext.decode(""utf-8""))
            except Exception:
                payload_obj = {}

        rsa_ct_b64: Optional[str] = None
        aes_ct_repr: Optional[str] = None

        # Common key names
        for k in (""key"", ""aes_key"", ""encrypted_key"", ""rsa"", ""rsa_key""):
            if k in payload_obj:
                rsa_ct_b64 = payload_obj[k]
                break
        for k in (""ciphertext"", ""secrets"", ""payload"", ""data"", ""ct""):
            if k in payload_obj:
                aes_ct_repr = payload_obj[k]
                break

        # If not JSON, attempt delimited format: ""<rsa_b64>|<aes_ct_repr>""
        if rsa_ct_b64 is None or aes_ct_repr is None:
            s = encoded_ciphertext if isinstance(encoded_ciphertext, str) else encoded_ciphertext.decode(""utf-8"", errors=""ignore"")
            # Try to decode base64 first to see if it's ""rsa:...:aes:...""
            if ""|"" in s:
                parts = s.split(""|"", 1)
                if len(parts) == 2:
                    rsa_ct_b64 = parts[0].strip()
                    aes_ct_repr = parts[1].strip()
            elif "":"" in s and len(s.split("":"")) >= 4:
                # Heuristic: rsa_b64 : aes_iv_b64 : aes_ct_b64 [ : gcm_tag_b64 ]
                # So first token is rsa, remainder join as aes repr
                parts = s.split("":"")
                rsa_ct_b64 = parts[0].strip()
                aes_ct_repr = "":"".join(parts[1:]).strip()

        if rsa_ct_b64 is None or aes_ct_repr is None:
            raise ValueError(""Unable to parse encoded_ciphertext payload"")

        # 1) Decrypt RSA to get AES key (returned in b64 per _rsa_decrypt)
        aes_key_b64 = cls._rsa_decrypt(private_key_path=private_key_path, ciphertext=rsa_ct_b64)
        aes_key = _b64_decode(aes_key_b64)

        # 2) AES decrypt secrets payload
        secrets_bytes = cls._aes_decrypt(key=aes_key, ciphertext=aes_ct_repr)
        secrets_text = secrets_bytes.decode(""utf-8"", errors=""ignore"").strip()

        # 3) Parse secrets as JSON or delimited string
        result = {
            ""token"": """",
            ""token_secret"": """",
            ""refresh_token"": """",
            ""expires_in"": """",
        }

        parsed = None
        try:
            parsed = json.loads(secrets_text)
        except Exception:
            parsed = None

        if isinstance(parsed, dict):
            result[""token""] = str(parsed.get(""token"", """")) if parsed.get(""token"") is not None else """"
            result[""token_secret""] = str(parsed.get(""token_secret"", """")) if parsed.get(""token_secret"") is not None else """"
            result[""refresh_token""] = str(parsed.get(""refresh_token"", """")) if parsed.get(""refresh_token"") is not None else """"
            result[""expires_in""] = str(parsed.get(""expires_in"", """")) if parsed.get(""expires_in"") is not None else """"
            # Accept alternative keys
            if not result[""token""] and ""access_token"" in parsed:
                result[""token""] = str(parsed.get(""access_token"", """"))
            if not result[""token_secret""] and ""secret"" in parsed:
                result[""token_secret""] = str(parsed.get(""secret"", """"))
            if not result[""expires_in""] and ""expiry"" in parsed:
                result[""expires_in""] = str(parsed.get(""expiry"", """"))
            return result

        # Fallback: pipe/comma separated order: token|token_secret|refresh_token|expires_in
        delim = ""|""
        if delim not in secrets_text:
            delim = "","" if "","" in secrets_text else None
        if delim:
            parts = [p.strip() for p in secrets_text.split(delim)]
            if len(parts) >= 1:
                result[""token""] = parts[0]
            if len(parts) >= 2:
                result[""token_secret""] = parts[1]
            if len(parts) >= 3:
                result[""refresh_token""] = parts[2]
            if len(parts) >= 4:
                result[""expires_in""] = parts[3]
        else:
            # If we cannot parse, return plaintext as token
            result[""token""] = secrets_text

        return result"
247043,CiscoDevNet/webexteamssdk,src/webexpythonsdk/models/cards/adaptive_card_component.py,webexpythonsdk.models.cards.adaptive_card_component.AdaptiveCardComponent,"import json
import enum

class AdaptiveCardComponent:
    """"""
    Base class for all Adaptive Card elements.

    Each element should inherit from this class and specify which of its
    properties fall into the following two categories:

    * Simple properties are basic types (int, float, str, etc.).

    * Serializable properties are properties that can themselves be serialized.
      This includes lists of items (i.e. the 'body' field of the adaptive card)
      or single objects that also inherit from Serializable
    """"""

    def __init__(self, serializable_properties, simple_properties):
        """"""
        Initialize a serializable object.

        Args:
            serializable_properties(list): List of all serializable properties
            simple_properties(list): List of all simple properties.
        """"""
        self.serializable_properties = serializable_properties
        self.simple_properties = simple_properties

    def to_dict(self):
        """"""
        Serialize the element into a Python dictionary.

        The to_dict() method recursively serializes the object's data into
        a Python dictionary.

        Returns:
            dict: Dictionary representation of this element.
        """"""
        serialized_data = {}
        for property_name in self.simple_properties:
            property_value = getattr(self, property_name, None)
            if property_value is not None:
                if isinstance(property_value, enum.Enum):
                    property_value = str(property_value)
                serialized_data[property_name] = property_value
        for property_name in self.serializable_properties:
            property_value = getattr(self, property_name, None)
            if property_value is not None:
                if isinstance(property_value, list):
                    serialized_data[property_name] = [item.to_dict() if hasattr(item, 'to_dict') else item for item in property_value]
                else:
                    serialized_data[property_name] = property_value.to_dict()
        return serialized_data

    def to_json(self, **kwargs):
        """"""
        Serialize the element into JSON text.

        Any keyword arguments provided are passed through the Python JSON
        encoder.
        """"""
        return json.dumps(self.to_dict(), **kwargs)","
class AdaptiveCardComponent:
    '''
    Base class for all Adaptive Card elements.
    Each element should inherit from this class and specify which of its
    properties fall into the following two categories:
    * Simple properties are basic types (int, float, str, etc.).
    * Serializable properties are properties that can themselves be serialized.
      This includes lists of items (i.e. the 'body' field of the adaptive card)
      or single objects that also inherit from Serializable
    '''
    def __init__(self, serializable_properties, simple_properties):
        
        pass
    def to_dict(self):
        
        pass
    def to_json(self, **kwargs):
        '''
        Serialize the element into JSON text.
        Any keyword arguments provided are passed through the Python JSON
        encoder.
        '''
        pass",snippet_151,4,"import json
from collections.abc import Mapping, Sequence

class AdaptiveCardComponent:
    '''
    Base class for all Adaptive Card elements.
    Each element should inherit from this class and specify which of its
    properties fall into the following two categories:
    * Simple properties are basic types (int, float, str, etc.).
    * Serializable properties are properties that can themselves be serialized.
      This includes lists of items (i.e. the 'body' field of the adaptive card)
      or single objects that also inherit from Serializable
    '''
    def __init__(self, serializable_properties, simple_properties):
        if serializable_properties is None:
            serializable_properties = []
        if simple_properties is None:
            simple_properties = []
        self._serializable_properties = tuple(serializable_properties)
        self._simple_properties = tuple(simple_properties)

    def _serialize_value(self, value):
        if value is None:
            return None

        if hasattr(value, ""to_dict"") and callable(getattr(value, ""to_dict"")):
            return value.to_dict()

        if isinstance(value, Mapping):
            out = {}
            for k, v in value.items():
                sv = self._serialize_value(v)
                out[k] = sv
            return out

        # Treat sequences but not strings/bytes as collections
        if isinstance(value, Sequence) and not isinstance(value, (str, bytes, bytearray)):
            out_list = []
            for item in value:
                out_list.append(self._serialize_value(item))
            return out_list

        return value

    def to_dict(self):
        result = {}

        # Simple properties: add if not None
        for name in self._simple_properties:
            if not hasattr(self, name):
                continue
            val = getattr(self, name)
            if val is not None:
                result[name] = val

        # Serializable properties: serialize and add if not None
        for name in self._serializable_properties:
            if not hasattr(self, name):
                continue
            val = getattr(self, name)
            if val is None:
                continue
            serialized = self._serialize_value(val)
            if serialized is not None:
                result[name] = serialized

        return result

    def to_json(self, **kwargs):
        '''
        Serialize the element into JSON text.
        Any keyword arguments provided are passed through the Python JSON
        encoder.
        '''
        return json.dumps(self.to_dict(), **kwargs)"
248349,facelessuser/soupsieve,facelessuser_soupsieve/soupsieve/css_match.py,soupsieve.css_match._FakeParent,"import bs4

class _FakeParent:
    """"""
    Fake parent class.

    When we have a fragment with no `BeautifulSoup` document object,
    we can't evaluate `nth` selectors properly.  Create a temporary
    fake parent so we can traverse the root element as a child.
    """"""

    def __init__(self, element: bs4.Tag) -> None:
        """"""Initialize.""""""
        self.contents = [element]

    def __len__(self) -> int:
        """"""Length.""""""
        return len(self.contents)","
class _FakeParent:
    
    def __init__(self, element: bs4.Tag) -> None:
        
        pass
    def __len__(self) -> int:
        '''Length.'''
        pass",snippet_152,3,"class _FakeParent:
    
    def __init__(self, element: bs4.Tag) -> None:
        self.element = element

    def __len__(self) -> int:
        '''Length.'''
        return 1"
248351,facelessuser/soupsieve,facelessuser_soupsieve/soupsieve/css_parser.py,soupsieve.css_parser.SelectorPattern,"from typing import Match, Any, Iterator, cast
import re

class SelectorPattern:
    """"""Selector pattern.""""""

    def __init__(self, name: str, pattern: str) -> None:
        """"""Initialize.""""""
        self.name = name
        self.re_pattern = re.compile(pattern, re.I | re.X | re.U)

    def get_name(self) -> str:
        """"""Get name.""""""
        return self.name

    def match(self, selector: str, index: int, flags: int) -> Match[str] | None:
        """"""Match the selector.""""""
        return self.re_pattern.match(selector, index)","
class SelectorPattern:
    '''Selector pattern.'''
    def __init__(self, name: str, pattern: str) -> None:
        '''Initialize.'''
        pass
    def get_name(self) -> str:
        
        pass
    def match(self, selector: str, index: int, flags: int) -> Match[str] | None:
        
        pass",snippet_153,4,"import re
from typing import Match

class SelectorPattern:
    '''Selector pattern.'''
    def __init__(self, name: str, pattern: str) -> None:
        '''Initialize.'''
        self._name = name
        self._pattern = pattern

    def get_name(self) -> str:
        return self._name

    def match(self, selector: str, index: int, flags: int) -> Match[str] | None:
        regex = re.compile(self._pattern, flags)
        return regex.match(selector, pos=index)"
250153,Unidata/siphon,src/siphon/catalog.py,siphon.catalog.CatalogRef,"from urllib.parse import urljoin, urlparse

class CatalogRef:
    """"""
    An object for holding catalog references obtained from a THREDDS Client Catalog.

    Attributes
    ----------
    name : str
        The name of the :class:`CatalogRef` element
    href : str
        url to the :class:`CatalogRef`'s THREDDS Client Catalog
    title : str
        Title of the :class:`CatalogRef` element

    """"""

    def __init__(self, base_url, element_node):
        """"""
        Initialize the catalogRef object.

        Parameters
        ----------
        base_url : str
            URL to the base catalog that owns this reference
        element_node : :class:`~xml.etree.ElementTree.Element`
            An :class:`~xml.etree.ElementTree.Element` representing a catalogRef node

        """"""
        self.title = element_node.attrib['{http://www.w3.org/1999/xlink}title']
        self.name = element_node.attrib.get('name', self.title)
        href = element_node.attrib['{http://www.w3.org/1999/xlink}href']
        self.href = urljoin(base_url, href)

    def __str__(self):
        """"""Return a string representation of the catalog reference.""""""
        return str(self.title)

    def follow(self):
        """"""Follow the catalog reference and return a new :class:`TDSCatalog`.

        Returns
        -------
        TDSCatalog
            The referenced catalog

        """"""
        return TDSCatalog(self.href)
    __repr__ = __str__","
class CatalogRef:
    
    def __init__(self, base_url, element_node):
        '''
        Initialize the catalogRef object.
        Parameters
        ----------
        base_url : str
            URL to the base catalog that owns this reference
        element_node : :class:`~xml.etree.ElementTree.Element`
            An :class:`~xml.etree.ElementTree.Element` representing a catalogRef node
        '''
        pass
    def __str__(self):
        
        pass
    def follow(self):
        
        pass",snippet_154,4,"class CatalogRef:
    
    def __init__(self, base_url, element_node):
        '''
        Initialize the catalogRef object.
        Parameters
        ----------
        base_url : str
            URL to the base catalog that owns this reference
        element_node : :class:`~xml.etree.ElementTree.Element`
            An :class:`~xml.etree.ElementTree.Element` representing a catalogRef node
        '''
        from urllib.parse import urljoin

        if not isinstance(base_url, str) or not base_url:
            raise ValueError(""base_url must be a non-empty string"")
        self.base_url = base_url
        self.element = element_node

        # Common xlink namespace attr names
        xlink_href = '{http://www.w3.org/1999/xlink}href'
        xlink_title = '{http://www.w3.org/1999/xlink}title'

        # Extract href, accommodating both xlink and non-namespaced variants
        self.href = (
            (element_node.get(xlink_href) if element_node is not None else None)
            or (element_node.get('href') if element_node is not None else None)
        )

        # Extract a human-readable title or name
        self.title = (
            (element_node.get(xlink_title) if element_node is not None else None)
            or (element_node.get('title') if element_node is not None else None)
            or (element_node.get('name') if element_node is not None else None)
        )

        if not self.href:
            raise ValueError(""catalogRef element is missing href/xlink:href attribute"")

        # Precompute absolute URL
        self._resolved_url = urljoin(self.base_url, self.href)

    def __str__(self):
        return self.title or self._resolved_url

    def follow(self):
        return self._resolved_url"
250158,Unidata/siphon,src/siphon/catalog.py,siphon.catalog.SimpleService,"class SimpleService:
    """"""Hold information about an access service enabled on a dataset.

    Attributes
    ----------
    name : str
        The name of the service
    service_type : str
        The service type (i.e. ""OPENDAP"", ""NetcdfSubset"", ""WMS"", etc.)
    access_urls : dict[str, str]
        A dictionary of access urls whose keywords are the access service
        types defined in the catalog (for example, ""OPENDAP"", ""NetcdfSubset"",
        ""WMS"", etc.)

    """"""

    def __init__(self, service_node):
        """"""Initialize the Dataset object.

        Parameters
        ----------
        service_node : :class:`~xml.etree.ElementTree.Element`
            An :class:`~xml.etree.ElementTree.Element` representing a service node

        """"""
        self.name = service_node.attrib['name']
        self.service_type = CaseInsensitiveStr(service_node.attrib['serviceType'])
        self.base = service_node.attrib['base']
        self.access_urls = {}

    def is_resolver(self):
        """"""Return whether the service is a resolver service.""""""
        return self.service_type == 'Resolver'","class SimpleService:
    '''Hold information about an access service enabled on a dataset.
    Attributes
    ----------
    name : str
        The name of the service
    service_type : str
        The service type (i.e. ""OPENDAP"", ""NetcdfSubset"", ""WMS"", etc.)
    access_urls : dict[str, str]
        A dictionary of access urls whose keywords are the access service
        types defined in the catalog (for example, ""OPENDAP"", ""NetcdfSubset"",
        ""WMS"", etc.)
    '''
    def __init__(self, service_node):
        
        pass
    def is_resolver(self):
        '''Return whether the service is a resolver service.'''
        pass",snippet_156,3,"class SimpleService:
    '''Hold information about an access service enabled on a dataset.
    Attributes
    ----------
    name : str
        The name of the service
    service_type : str
        The service type (i.e. ""OPENDAP"", ""NetcdfSubset"", ""WMS"", etc.)
    access_urls : dict[str, str]
        A dictionary of access urls whose keywords are the access service
        types defined in the catalog (for example, ""OPENDAP"", ""NetcdfSubset"",
        ""WMS"", etc.)
    '''
    def __init__(self, service_node):
        self.name = None
        self.service_type = None
        self.access_urls = {}

        # Support mapping-like inputs
        try:
            from collections.abc import Mapping
            is_mapping = isinstance(service_node, Mapping)
        except Exception:
            is_mapping = False

        # Support xml.etree.ElementTree Element inputs
        try:
            from xml.etree.ElementTree import Element
            is_xml_element = isinstance(service_node, Element)
        except Exception:
            is_xml_element = False

        if is_mapping:
            self._init_from_mapping(service_node)
        elif is_xml_element:
            self._init_from_xml(service_node)
        else:
            # Fallback: attempt to coerce basic tuple/list or object with attributes
            self._init_from_generic(service_node)

        # Normalize keys of access_urls to strings and strip whitespace
        self.access_urls = {
            (str(k).strip() if k is not None else ''): (v.strip() if isinstance(v, str) else v)
            for k, v in self.access_urls.items()
            if k is not None and v is not None
        }

        # Normalize name and service_type
        if isinstance(self.name, str):
            self.name = self.name.strip()
        if isinstance(self.service_type, str):
            self.service_type = self.service_type.strip()

    def _init_from_mapping(self, m):
        # Direct fields
        self.name = m.get('name') or m.get('Name') or m.get('id') or m.get('ID')
        self.service_type = (
            m.get('service_type')
            or m.get('serviceType')
            or m.get('type')
            or m.get('Type')
        )
        # Access URLs directly provided
        access = (
            m.get('access_urls')
            or m.get('accessUrls')
            or m.get('access')
            or m.get('urls')
        )
        if isinstance(access, dict):
            self.access_urls.update(access)
        elif isinstance(access, (list, tuple)):
            # if list of (type, url) pairs
            for item in access:
                if isinstance(item, (list, tuple)) and len(item) >= 2:
                    key, val = item[0], item[1]
                    if key is not None and val is not None:
                        self.access_urls[str(key)] = str(val)
        # Look for flat keys like 'OPENDAP', 'WMS', etc.
        for k, v in m.items():
            if k and isinstance(k, str) and v and isinstance(v, str):
                upper_k = k.upper()
                if upper_k in ('OPENDAP', 'DAP', 'WMS', 'WCS', 'HTTPServer', 'HTTP', 'NetcdfSubset', 'NCSS', 'CDMRemote', 'Resolver', 'RESOLVER'):
                    self.access_urls[upper_k] = v

        # Sometimes nested children describing services
        children = m.get('children') or m.get('services') or []
        if isinstance(children, dict):
            children = [children]
        if isinstance(children, (list, tuple)):
            for child in children:
                if isinstance(child, dict):
                    st = child.get('serviceType') or child.get('type') or child.get('service_type')
                    url = child.get('url') or child.get('urlPath') or child.get('href') or child.get('link')
                    if st and url:
                        self.access_urls[str(st)] = str(url)

    def _init_from_xml(self, elem):
        # Attributes on the element
        self.name = elem.attrib.get('name') or elem.attrib.get('id') or elem.attrib.get('ID')
        self.service_type = elem.attrib.get('serviceType') or elem.attrib.get('type')

        # Direct URL attributes
        direct_url = elem.attrib.get('url') or elem.attrib.get('href')
        if direct_url and self.service_type:
            self.access_urls[self.service_type] = direct_url

        # Handle THREDDS-like ""access"" child elements or nested services
        for child in list(elem):
            tag = child.tag.split('}')[-1]  # strip namespace if present
            if tag.lower() in ('access', 'service'):
                st = child.attrib.get('serviceType') or child.attrib.get('type')
                url = child.attrib.get('url') or child.attrib.get('urlPath') or child.attrib.get('href')
                if st and url:
                    self.access_urls[st] = url

            # Generic pattern: any child with serviceType and url/href/urlPath
            st = child.attrib.get('serviceType') or child.attrib.get('type')
            if st:
                url = child.attrib.get('url') or child.attrib.get('href') or child.attrib.get('urlPath')
                if url:
                    self.access_urls[st] = url

    def _init_from_generic(self, obj):
        # Try attribute access
        self.name = getattr(obj, 'name', None) or getattr(obj, 'id', None)
        self.service_type = getattr(obj, 'service_type', None) or getattr(obj, 'serviceType', None) or getattr(obj, 'type', None)

        access = getattr(obj, 'access_urls', None) or getattr(obj, 'access', None) or getattr(obj, 'urls', None)
        if isinstance(access, dict):
            self.access_urls.update(access)
        elif isinstance(access, (list, tuple)):
            for item in access:
                if isinstance(item, (list, tuple)) and len(item) >= 2:
                    k, v = item[0], item[1]
                    if k is not None and v is not None:
                        self.access_urls[str(k)] = str(v)

        # If object has iterable children
        children = getattr(obj, 'children', None) or getattr(obj, 'services', None)
        if isinstance(children, (list, tuple)):
            for child in children:
                st = getattr(child, 'service_type', None) or getattr(child, 'serviceType', None) or getattr(child, 'type', None)
                url = getattr(child, 'url', None) or getattr(child, 'href', None) or getattr(child, 'urlPath', None)
                if st and url:
                    self.access_urls[str(st)] = str(url)

    def is_resolver(self):
        '''Return whether the service is a resolver service.'''
        if not self.service_type:
            return False
        return str(self.service_type).strip().lower() == 'resolver'"
250272,incuna/django-pgcrypto-fields,incuna_django-pgcrypto-fields/pgcrypto/mixins.py,pgcrypto.mixins.HashMixin,"class HashMixin:
    """"""Keyed hash mixin.

    `HashMixin` uses 'pgcrypto' to encrypt data in a postgres database.
    """"""
    encrypt_sql = None

    def __init__(self, original=None, *args, **kwargs):
        """"""Tells the init the original attr.""""""
        self.original = original
        super(HashMixin, self).__init__(*args, **kwargs)

    def pre_save(self, model_instance, add):
        """"""Save the original_value.""""""
        if self.original:
            original_value = getattr(model_instance, self.original)
            setattr(model_instance, self.attname, original_value)
        return super(HashMixin, self).pre_save(model_instance, add)

    def get_placeholder(self, value=None, compiler=None, connection=None):
        """"""
        Tell postgres to encrypt this field with a hashing function.

        The `value` string is checked to determine if we need to hash or keep
        the current value.

        `compiler` and `connection` is ignored here as we don't need custom operators.
        """"""
        if value is None or value.startswith('\\x'):
            return '%s'
        return self.get_encrypt_sql(connection)

    def get_encrypt_sql(self, connection):
        """"""Get encrypt sql. This may be overidden by some implementations.""""""
        return self.encrypt_sql","class HashMixin:
    '''Keyed hash mixin.
    `HashMixin` uses 'pgcrypto' to encrypt data in a postgres database.
    '''
    def __init__(self, original=None, *args, **kwargs):
        
        pass
    def pre_save(self, model_instance, add):
        '''Save the original_value.'''
        pass
    def get_placeholder(self, value=None, compiler=None, connection=None):
        
        pass
    def get_encrypt_sql(self, connection):
        '''Get encrypt sql. This may be overidden by some implementations.'''
        pass",snippet_157,5,"class HashMixin:
    '''Keyed hash mixin.
    `HashMixin` uses 'pgcrypto' to encrypt data in a postgres database.
    '''
    def __init__(self, original=None, *args, **kwargs):
        self.original = original or 'original_value'
        self.algorithm = kwargs.pop('algorithm', 'sha256')
        self.key = kwargs.pop('key', None)
        # Allow SECRET_KEY fallback if Django settings are available.
        if self.key is None:
            try:
                from django.conf import settings
                self.key = getattr(settings, 'SECRET_KEY', '')
            except Exception:
                self.key = ''
        super().__init__(*args, **kwargs)

    def pre_save(self, model_instance, add):
        '''Save the original_value.'''
        # Attempt to fetch the raw/original value from the instance using the field's attname
        value = None
        attname = getattr(self, 'attname', None)
        if attname:
            value = getattr(model_instance, attname, None)
        # Store original value on the model instance if configured
        if self.original:
            try:
                setattr(model_instance, self.original, value)
            except Exception:
                pass
        return value

    def get_placeholder(self, value=None, compiler=None, connection=None):
        placeholder = '%s'
        if connection is not None:
            try:
                placeholder = connection.ops.placeholder(value)
            except Exception:
                placeholder = '%s'
        # Return a PostgreSQL expression that computes a keyed hash using pgcrypto's hmac()
        return self.get_encrypt_sql(connection).format(placeholder=placeholder)

    def get_encrypt_sql(self, connection):
        '''Get encrypt sql. This may be overidden by some implementations.'''
        # Safely inline the key and algorithm as SQL string literals
        key = self.key or ''
        algo = self.algorithm or 'sha256'
        key_sql = ""'"" + key.replace(""'"", ""''"") + ""'""
        algo_sql = ""'"" + algo.replace(""'"", ""''"") + ""'""
        # Use a format placeholder for the parameter placeholder to be supplied by get_placeholder
        # hmac(data bytea, key text, type text) returns bytea
        return f""hmac({{placeholder}}::bytea, {key_sql}, {algo_sql})"""
251421,Yelp/py_zipkin,Yelp_py_zipkin/py_zipkin/encoding/_encoders.py,py_zipkin.encoding._encoders.IEncoder,"from typing import Union
from py_zipkin.encoding._helpers import Span
from typing import List

class IEncoder:
    """"""Encoder interface.""""""

    def fits(self, current_count: int, current_size: int, max_size: int, new_span: Union[str, bytes]) -> bool:
        """"""Returns whether the new span will fit in the list.

        :param current_count: number of spans already in the list.
        :type current_count: int
        :param current_size: sum of the sizes of all the spans already in the list.
        :type current_size: int
        :param max_size: max supported transport payload size.
        :type max_size: int
        :param new_span: encoded span object that we want to add the the list.
        :type new_span: str or bytes
        :return: True if the new span can be added to the list, False otherwise.
        :rtype: bool
        """"""
        raise NotImplementedError()

    def encode_span(self, span: Span) -> Union[str, bytes]:
        """"""Encodes a single span.

        :param span: Span object representing the span.
        :type span: Span
        :return: encoded span.
        :rtype: str or bytes
        """"""
        raise NotImplementedError()

    def encode_queue(self, queue: List[Union[str, bytes]]) -> Union[str, bytes]:
        """"""Encodes a list of pre-encoded spans.

        :param queue: list of encoded spans.
        :type queue: list
        :return: encoded list, type depends on the encoding.
        :rtype: str or bytes
        """"""
        raise NotImplementedError()","
class IEncoder:
    
    def fits(self, current_count: int, current_size: int, max_size: int, new_span: Union[str, bytes]) -> bool:
        
        pass
    def encode_span(self, span: Span) -> Union[str, bytes]:
        '''Encodes a single span.
        :param span: Span object representing the span.
        :type span: Span
        :return: encoded span.
        :rtype: str or bytes
        '''
        pass
    def encode_queue(self, queue: List[Union[str, bytes]]) -> Union[str, bytes]:
        
        pass",snippet_158,4,"from __future__ import annotations

import json
from typing import Union, List, Any


class IEncoder:
    def fits(self, current_count: int, current_size: int, max_size: int, new_span: Union[str, bytes]) -> bool:
        if not isinstance(new_span, (str, bytes)):
            raise TypeError(""new_span must be str or bytes"")

        if max_size < 0 or current_count < 0 or current_size < 0:
            return False

        if isinstance(new_span, bytes):
            new_size = len(new_span)
        else:
            new_size = len(new_span.encode(""utf-8""))

        # account for a single-byte separator (newline) if this won't be the first item
        sep_size = 1 if current_count > 0 else 0
        return (current_size + sep_size + new_size) <= max_size

    def encode_span(self, span: 'Span') -> Union[str, bytes]:
        # Prefer bytes if a clear bytes representation is available
        if hasattr(span, ""to_bytes"") and callable(getattr(span, ""to_bytes"")):
            res = span.to_bytes()
            if not isinstance(res, bytes):
                raise TypeError(""to_bytes() must return bytes"")
            return res

        if hasattr(span, ""__bytes__""):
            res = bytes(span)  # type: ignore[arg-type]
            if not isinstance(res, bytes):
                raise TypeError(""__bytes__ must return bytes"")
            return res

        # Next, try JSON/string encodings
        if hasattr(span, ""to_json"") and callable(getattr(span, ""to_json"")):
            res = span.to_json()
            if not isinstance(res, str):
                raise TypeError(""to_json() must return str"")
            return res

        if hasattr(span, ""to_str"") and callable(getattr(span, ""to_str"")):
            res = span.to_str()
            if not isinstance(res, str):
                raise TypeError(""to_str() must return str"")
            return res

        # Fallbacks: dict-like -> JSON string; else str()
        try:
            if hasattr(span, ""to_dict"") and callable(getattr(span, ""to_dict"")):
                as_dict = span.to_dict()
                return json.dumps(as_dict, separators=("","", "":""), ensure_ascii=False)
        except Exception:
            pass

        if isinstance(span, (dict, list, tuple)):
            return json.dumps(span, separators=("","", "":""), ensure_ascii=False)

        return str(span)

    def encode_queue(self, queue: List[Union[str, bytes]]) -> Union[str, bytes]:
        if not isinstance(queue, list):
            raise TypeError(""queue must be a list"")

        if not queue:
            return b""""

        has_bytes = any(isinstance(item, bytes) for item in queue)
        has_str = any(isinstance(item, str) for item in queue)

        if not (has_bytes or has_str):
            raise TypeError(""queue must contain str or bytes items"")

        if has_bytes and has_str:
            raise TypeError(""queue contains a mix of str and bytes"")

        if has_bytes:
            # join bytes with newline
            for item in queue:
                if not isinstance(item, bytes):
                    raise TypeError(""All items must be bytes when queue is bytes"")
            return b""\n"".join(queue)

        # join strings with newline
        for item in queue:
            if not isinstance(item, str):
                raise TypeError(""All items must be str when queue is str"")
        return ""\n"".join(queue)"
251832,glut23/webvtt-py,glut23_webvtt-py/webvtt/models.py,webvtt.models.Style,"import typing

class Style:
    """"""Representation of a style.""""""

    def __init__(self, text: typing.Union[str, typing.List[str]]):
        """"""Initialize.

        :param: text: the style text
        """"""
        self.lines = text.splitlines() if isinstance(text, str) else text
        self.comments: typing.List[str] = []

    @property
    def text(self):
        """"""Return the text of the style.""""""
        return '\n'.join(self.lines)","
class Style:
    '''Representation of a style.'''
    def __init__(self, text: typing.Union[str, typing.List[str]]):
        '''Initialize.
        :param: text: the style text
        '''
        pass
    @property
    def text(self):
        
        pass",snippet_159,3,"import typing


class Style:
    '''Representation of a style.'''
    def __init__(self, text: typing.Union[str, typing.List[str]]):
        '''Initialize.
        :param: text: the style text
        '''
        if isinstance(text, str):
            self._text = text
        elif isinstance(text, list):
            if not all(isinstance(item, str) for item in text):
                raise TypeError(""All items in the list must be strings."")
            self._text = ""\n"".join(text)
        else:
            raise TypeError(""text must be a string or a list of strings."")

    @property
    def text(self):
        return self._text"
251834,glut23/webvtt-py,glut23_webvtt-py/webvtt/sbv.py,webvtt.sbv.SBVCueBlock,"import typing
import re

class SBVCueBlock:
    """"""Representation of a cue timing block.""""""
    CUE_TIMINGS_PATTERN = re.compile('\\s*(\\d{1,2}:\\d{1,2}:\\d{1,2}.\\d{3}),(\\d{1,2}:\\d{1,2}:\\d{1,2}.\\d{3})')

    def __init__(self, start: str, end: str, payload: typing.Sequence[str]):
        """"""
        Initialize.

        :param start: start time
        :param end: end time
        :param payload: caption text
        """"""
        self.start = start
        self.end = end
        self.payload = payload

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a cue time block.

        :param lines: the lines to be validated
        :returns: true for a matching cue time block
        """"""
        return bool(len(lines) >= 2 and re.match(cls.CUE_TIMINGS_PATTERN, lines[0]) and lines[1].strip())

    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SBVCueBlock':
        """"""
        Create a `SBVCueBlock` from lines of text.

        :param lines: the lines of text
        :returns: `SBVCueBlock` instance
        """"""
        match = re.match(cls.CUE_TIMINGS_PATTERN, lines[0])
        assert match is not None
        payload = lines[1:]
        return cls(match.group(1), match.group(2), payload)","
class SBVCueBlock:
    '''Representation of a cue timing block.'''
    def __init__(self, start: str, end: str, payload: typing.Sequence[str]):
        
        pass
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        
        pass
    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SBVCueBlock':
        '''
        Create a `SBVCueBlock` from lines of text.
        :param lines: the lines of text
        :returns: `SBVCueBlock` instance
        '''
        pass",snippet_160,4,"import re
import typing


class SBVCueBlock:
    '''Representation of a cue timing block.'''
    _TIMING_RE = re.compile(
        r'^\s*(?P<start>\d+:\d{2}:\d{2}\.\d{3})\s*,\s*(?P<end>\d+:\d{2}:\d{2}\.\d{3})\s*$'
    )

    def __init__(self, start: str, end: str, payload: typing.Sequence[str]):
        self.start = start
        self.end = end
        self.payload = list(payload)

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        if not lines:
            return False
        first = lines[0].lstrip(""\ufeff"")  # handle BOM if present
        return cls._TIMING_RE.match(first) is not None

    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SBVCueBlock':
        if not lines:
            raise ValueError(""No lines provided for SBV cue block"")
        timing_line = lines[0].lstrip(""\ufeff"")
        m = cls._TIMING_RE.match(timing_line)
        if not m:
            raise ValueError(f""Invalid SBV timing line: {timing_line!r}"")
        start = m.group('start')
        end = m.group('end')
        payload = list(lines[1:])
        # Trim trailing empty lines from payload
        while payload and payload[-1].strip() == '':
            payload.pop()
        return cls(start, end, payload)"
251835,glut23/webvtt-py,glut23_webvtt-py/webvtt/srt.py,webvtt.srt.SRTCueBlock,"import typing
import re

class SRTCueBlock:
    """"""Representation of a cue timing block.""""""
    CUE_TIMINGS_PATTERN = re.compile('\\s*(\\d+:\\d{2}:\\d{2},\\d{3})\\s*-->\\s*(\\d+:\\d{2}:\\d{2},\\d{3})')

    def __init__(self, index: str, start: str, end: str, payload: typing.Sequence[str]):
        """"""
        Initialize.

        :param start: start time
        :param end: end time
        :param payload: caption text
        """"""
        self.index = index
        self.start = start
        self.end = end
        self.payload = payload

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a cue time block.

        :param lines: the lines to be validated
        :returns: true for a matching cue time block
        """"""
        return bool(len(lines) >= 3 and lines[0].isdigit() and re.match(cls.CUE_TIMINGS_PATTERN, lines[1]))

    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SRTCueBlock':
        """"""
        Create a `SRTCueBlock` from lines of text.

        :param lines: the lines of text
        :returns: `SRTCueBlock` instance
        """"""
        index = lines[0]
        match = re.match(cls.CUE_TIMINGS_PATTERN, lines[1])
        assert match is not None
        payload = lines[2:]
        return cls(index, match.group(1), match.group(2), payload)","
class SRTCueBlock:
    
    def __init__(self, index: str, start: str, end: str, payload: typing.Sequence[str]):
        
        pass
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        '''
        Validate the lines for a match of a cue time block.
        :param lines: the lines to be validated
        :returns: true for a matching cue time block
        '''
        pass
    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SRTCueBlock':
        
        pass",snippet_161,4,"import re
import typing

class SRTCueBlock:
    _TIMELINE_RE = re.compile(
        r'^\s*(\d{2}:\d{2}:\d{2},\d{3})\s*-->\s*(\d{2}:\d{2}:\d{2},\d{3})(?:\s*.*)?$'
    )

    def __init__(self, index: str, start: str, end: str, payload: typing.Sequence[str]):
        self.index = str(index).strip()
        self.start = str(start).strip()
        self.end = str(end).strip()
        self.payload = list(payload)

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        if not lines or len(lines) < 3:
            return False
        idx = lines[0].strip()
        if not idx.isdigit():
            return False
        m = cls._TIMELINE_RE.match(lines[1].rstrip(""\n""))
        if not m:
            return False
        return True

    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SRTCueBlock':
        if not cls.is_valid(lines):
            raise ValueError(""Invalid SRT cue block lines"")
        index = lines[0].strip()
        m = cls._TIMELINE_RE.match(lines[1].rstrip(""\n""))
        start, end = m.group(1), m.group(2)
        payload = list(lines[2:])
        # Remove a trailing empty line if present (common separator)
        while payload and payload[-1].strip() == '':
            payload.pop()
        return cls(index=index, start=start, end=end, payload=payload)"
251838,glut23/webvtt-py,glut23_webvtt-py/webvtt/vtt.py,webvtt.vtt.WebVTTCommentBlock,"import typing
import re

class WebVTTCommentBlock:
    """"""Representation of a comment block.""""""
    COMMENT_PATTERN = re.compile('NOTE\\s(.*?)\\Z', re.DOTALL)

    def __init__(self, text: str):
        """"""
        Initialize.

        :param text: comment text
        """"""
        self.text = text

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a comment block.

        :param lines: the lines to be validated
        :returns: true for a matching comment block
        """"""
        return bool(lines and lines[0].startswith('NOTE'))

    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTCommentBlock':
        """"""
        Create a `WebVTTCommentBlock` from lines of text.

        :param lines: the lines of text
        :returns: `WebVTTCommentBlock` instance
        """"""
        match = cls.COMMENT_PATTERN.match('\n'.join(lines))
        return cls(text=match.group(1).strip() if match else '')

    @staticmethod
    def format_lines(lines: str) -> typing.List[str]:
        """"""
        Return the lines for a comment block.

        :param lines: comment lines
        :returns: list of lines for a comment block
        """"""
        list_of_lines = lines.split('\n')
        if len(list_of_lines) == 1:
            return [f'NOTE {lines}']
        return ['NOTE', *list_of_lines]","
class WebVTTCommentBlock:
    
    def __init__(self, text: str):
        
        pass
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        '''
        Validate the lines for a match of a comment block.
        :param lines: the lines to be validated
        :returns: true for a matching comment block
        '''
        pass
    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTCommentBlock':
        '''
        Create a `WebVTTCommentBlock` from lines of text.
        :param lines: the lines of text
        :returns: `WebVTTCommentBlock` instance
        '''
        pass
    @staticmethod
    def format_lines(lines: str) -> typing.List[str]:
        '''
        Return the lines for a comment block.
        :param lines: comment lines
        :returns: list of lines for a comment block
        '''
        pass",snippet_162,5,"import typing

class WebVTTCommentBlock:
    
    def __init__(self, text: str):
        self.text = text or """"
    
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        '''
        Validate the lines for a match of a comment block.
        :param lines: the lines to be validated
        :returns: true for a matching comment block
        '''
        if not lines:
            return False
        first = lines[0].rstrip(""\n\r"")
        if not first.startswith(""NOTE""):
            return False
        if len(first) == 4:
            return True
        return first[4].isspace()
    
    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTCommentBlock':
        '''
        Create a `WebVTTCommentBlock` from lines of text.
        :param lines: the lines of text
        :returns: `WebVTTCommentBlock` instance
        '''
        collected = [l.rstrip(""\n\r"") for l in lines]
        if not cls.is_valid(collected):
            raise ValueError(""Lines do not represent a valid WebVTT comment block"")
        first = collected[0]
        rest = collected[1:]
        if len(first) == 4:
            text = ""\n"".join(rest)
        else:
            initial = first[4:].lstrip()
            if rest:
                text = initial + (""\n"" if initial else """") + ""\n"".join(rest) if any(rest) else initial
            else:
                text = initial
        return cls(text)
    
    @staticmethod
    def format_lines(lines: str) -> typing.List[str]:
        '''
        Return the lines for a comment block.
        :param lines: comment lines
        :returns: list of lines for a comment block
        '''
        if lines is None:
            return [""NOTE""]
        parts = str(lines).splitlines()
        if not parts:
            return [""NOTE""]
        first = parts[0]
        if first:
            header = ""NOTE "" + first
        else:
            header = ""NOTE""
        return [header] + parts[1:]"
251840,glut23/webvtt-py,glut23_webvtt-py/webvtt/vtt.py,webvtt.vtt.WebVTTStyleBlock,"import re
import typing

class WebVTTStyleBlock:
    """"""Representation of a style block.""""""
    STYLE_PATTERN = re.compile('STYLE\\s(.*?)\\Z', re.DOTALL)

    def __init__(self, text: str):
        """"""
        Initialize.

        :param text: style text
        """"""
        self.text = text

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a style block.

        :param lines: the lines to be validated
        :returns: true for a matching style block
        """"""
        return len(lines) >= 2 and lines[0] == 'STYLE' and (not any((line.strip() == '' or '-->' in line for line in lines)))

    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTStyleBlock':
        """"""
        Create a `WebVTTStyleBlock` from lines of text.

        :param lines: the lines of text
        :returns: `WebVTTStyleBlock` instance
        """"""
        match = cls.STYLE_PATTERN.match('\n'.join(lines))
        return cls(text=match.group(1).strip() if match else '')

    @staticmethod
    def format_lines(lines: typing.List[str]) -> typing.List[str]:
        """"""
        Return the lines for a style block.

        :param lines: style lines
        :returns: list of lines for a style block
        """"""
        return ['STYLE', *lines]","
class WebVTTStyleBlock:
    
    def __init__(self, text: str):
        
        pass
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        
        pass
    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTStyleBlock':
        '''
        Create a `WebVTTStyleBlock` from lines of text.
        :param lines: the lines of text
        :returns: `WebVTTStyleBlock` instance
        '''
        pass
    @staticmethod
    def format_lines(lines: typing.List[str]) -> typing.List[str]:
        
        pass",snippet_163,5,"import typing

class WebVTTStyleBlock:
    
    def __init__(self, text: str):
        if not isinstance(text, str):
            raise TypeError(""text must be a string"")
        self.text = text

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        if not lines:
            return False
        # Normalize lines: strip line endings
        norm = [("""" if s is None else str(s)).rstrip(""\r\n"") for s in lines]
        # Skip leading empty lines
        i = 0
        while i < len(norm) and norm[i].strip() == """":
            i += 1
        if i >= len(norm):
            return False
        if norm[i].strip() != ""STYLE"":
            return False
        # A STYLE block should have at least the header and can have zero or more CSS lines
        return True

    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTStyleBlock':
        raw_lines = [("""" if s is None else str(s)) for s in lines]
        if not cls.is_valid(raw_lines):
            raise ValueError(""Invalid WebVTT STYLE block lines"")
        formatted = cls.format_lines(raw_lines)
        text = ""\n"".join(formatted)
        return cls(text)

    @staticmethod
    def format_lines(lines: typing.List[str]) -> typing.List[str]:
        # Normalize to strings without CR/LF
        norm = [("""" if s is None else str(s)).rstrip(""\r\n"") for s in lines]

        # Remove leading empty lines
        while norm and norm[0].strip() == """":
            norm.pop(0)

        if not norm:
            return [""STYLE"", """"]

        # Ensure first non-empty is exactly ""STYLE""
        if norm[0].strip() != ""STYLE"":
            # If the first line contains extra spaces around, normalize to exact ""STYLE""
            norm[0] = ""STYLE""
        else:
            norm[0] = ""STYLE""

        # Strip trailing whitespace on all subsequent lines (keep internal spaces)
        for i in range(1, len(norm)):
            norm[i] = norm[i].rstrip()

        # Remove trailing empty lines, then ensure a single empty line terminator
        while len(norm) > 1 and norm[-1].strip() == """":
            norm.pop()
        norm.append("""")

        return norm"
251866,Shoobx/xmldiff,Shoobx_xmldiff/xmldiff/diff_match_patch.py,xmldiff.diff_match_patch.patch_obj,"import urllib.parse

class patch_obj:
    """"""Class representing one patch operation.""""""

    def __init__(self):
        """"""Initializes with an empty list of diffs.""""""
        self.diffs = []
        self.start1 = None
        self.start2 = None
        self.length1 = 0
        self.length2 = 0

    def __str__(self):
        """"""Emulate GNU diff's format.
        Header: @@ -382,8 +481,9 @@
        Indices are printed as 1-based, not 0-based.

        Returns:
          The GNU diff string.
        """"""
        if self.length1 == 0:
            coords1 = str(self.start1) + ',0'
        elif self.length1 == 1:
            coords1 = str(self.start1 + 1)
        else:
            coords1 = str(self.start1 + 1) + ',' + str(self.length1)
        if self.length2 == 0:
            coords2 = str(self.start2) + ',0'
        elif self.length2 == 1:
            coords2 = str(self.start2 + 1)
        else:
            coords2 = str(self.start2 + 1) + ',' + str(self.length2)
        text = ['@@ -', coords1, ' +', coords2, ' @@\n']
        for op, data in self.diffs:
            if op == diff_match_patch.DIFF_INSERT:
                text.append('+')
            elif op == diff_match_patch.DIFF_DELETE:
                text.append('-')
            elif op == diff_match_patch.DIFF_EQUAL:
                text.append(' ')
            data = data.encode('utf-8')
            text.append(urllib.parse.quote(data, ""!~*'();/?:@&=+$,# "") + '\n')
        return ''.join(text)","
class patch_obj:
    
    def __init__(self):
        '''Initializes with an empty list of diffs.'''
        pass
    def __str__(self):
        '''Emulate GNU diff's format.
        Header: @@ -382,8 +481,9 @@
        Indices are printed as 1-based, not 0-based.
        Returns:
          The GNU diff string.
        '''
        pass",snippet_164,3,"class patch_obj:
    
    def __init__(self):
        '''Initializes with an empty list of diffs.'''
        self.diffs = []
        self.start1 = 0
        self.start2 = 0
        self.length1 = 0
        self.length2 = 0

    def __str__(self):
        '''Emulate GNU diff's format.
        Header: @@ -382,8 +481,9 @@
        Indices are printed as 1-based, not 0-based.
        Returns:
          The GNU diff string.
        '''
        if self.length1 == 0:
            coords1 = f'{self.start1 + 1},0'
        elif self.length1 == 1:
            coords1 = f'{self.start1 + 1}'
        else:
            coords1 = f'{self.start1 + 1},{self.length1}'

        if self.length2 == 0:
            coords2 = f'{self.start2 + 1},0'
        elif self.length2 == 1:
            coords2 = f'{self.start2 + 1}'
        else:
            coords2 = f'{self.start2 + 1},{self.length2}'

        text = [f'@@ -{coords1} +{coords2} @@\n']
        for (op, data) in self.diffs:
            safe = data.replace('%', '%25').replace('\n', '%0A').replace('\r', '%0D')
            if op == 1:
                text.append('+' + safe + '\n')
            elif op == -1:
                text.append('-' + safe + '\n')
            else:
                text.append(' ' + safe + '\n')
        return ''.join(text)"
252656,happyleavesaoc/aoc-mgz,happyleavesaoc_aoc-mgz/mgz/model/inputs.py,mgz.model.inputs.Inputs,"from mgz.model.definitions import Input
from mgz.fast import Action as ActionEnum

class Inputs:
    """"""Normalize player inputs.""""""

    def __init__(self, gaia):
        """"""Initialize.""""""
        self._gaia = gaia
        self._buildings = {}
        self._oid_cache = {}
        self.inputs = []

    def add_chat(self, chat):
        """"""Add chat input.""""""
        self.inputs.append(Input(chat.timestamp, 'Chat', None, dict(message=chat.message), chat.player, None))

    def add_action(self, action):
        """"""Add action input.""""""
        if action.type in (ActionEnum.DE_TRANSFORM, ActionEnum.POSTGAME):
            return
        name = ACTION_TRANSLATE.get(action.type, action.type.name).replace('_', ' ').title()
        param = None
        if 'object_ids' in action.payload and action.payload['object_ids']:
            self._oid_cache[action.type] = action.payload['object_ids']
        elif action.type in self._oid_cache:
            action.payload['object_ids'] = self._oid_cache[action.type]
        if action.type is ActionEnum.SPECIAL:
            name = action.payload['order']
        elif action.type is ActionEnum.GAME:
            name = action.payload['command']
            if name == 'Speed':
                param = action.payload['speed']
        elif action.type is ActionEnum.STANCE:
            name = 'Stance'
            param = action.payload['stance']
        elif action.type is ActionEnum.FORMATION:
            name = 'Formation'
            param = action.payload['formation']
        elif action.type is ActionEnum.ORDER and action.payload['target_id'] in self._gaia:
            name = 'Gather'
            param = self._gaia[action.payload['target_id']]
        elif action.type is ActionEnum.ORDER and action.position and (action.position.hash() in self._buildings):
            name = 'Target'
            param = self._buildings[action.position.hash()]
        elif action.type is ActionEnum.GATHER_POINT:
            if action.payload['target_id'] in self._gaia:
                param = self._gaia[action.payload['target_id']]
            elif action.position and action.position.hash() in self._buildings:
                if len(action.payload['object_ids']) == 1 and action.payload['object_ids'][0] == action.payload['target_id']:
                    name = 'Spawn'
                param = self._buildings[action.position.hash()]
        elif action.type in (ActionEnum.BUY, ActionEnum.SELL):
            action.payload['amount'] *= 100
        elif action.type is ActionEnum.BUILD:
            param = action.payload['building']
            if action.position.hash() in self._buildings:
                if self._buildings[action.position.hash()] == 'Farm' and action.payload['building'] == 'Farm':
                    name = 'Reseed'
            self._buildings[action.position.hash()] = action.payload['building']
        elif action.type in (ActionEnum.QUEUE, ActionEnum.DE_QUEUE):
            param = action.payload['unit']
        elif action.type is ActionEnum.RESEARCH:
            param = action.payload['technology']
        new_input = Input(action.timestamp, name, param, action.payload, action.player, action.position)
        self.inputs.append(new_input)
        return new_input","
class Inputs:
    
    def __init__(self, gaia):
        '''Initialize.'''
        pass
    def add_chat(self, chat):
        
        pass
    def add_action(self, action):
        
        pass",snippet_165,4,"class Inputs:
    
    def __init__(self, gaia):
        '''Initialize.'''
        self.gaia = gaia
        self.chats = []
        self.actions = []

    def add_chat(self, chat):
        if chat is None:
            raise ValueError(""chat cannot be None"")
        self.chats.append(chat)
        return self

    def add_action(self, action):
        if action is None:
            raise ValueError(""action cannot be None"")
        self.actions.append(action)
        return self"
256858,cherrypy/cheroot,cheroot/server.py,cheroot.server.Gateway,"class Gateway:
    """"""Base class to interface HTTPServer with other systems, such as WSGI.""""""

    def __init__(self, req):
        """"""Initialize Gateway instance with request.

        Args:
            req (HTTPRequest): current HTTP request
        """"""
        self.req = req

    def respond(self):
        """"""Process the current request. Must be overridden in a subclass.""""""
        raise NotImplementedError","class Gateway:
    '''Base class to interface HTTPServer with other systems, such as WSGI.'''
    def __init__(self, req):
        
        pass
    def respond(self):
        
        pass",snippet_168,3,"class Gateway:
    '''Base class to interface HTTPServer with other systems, such as WSGI.'''
    def __init__(self, req):
        if req is None:
            raise ValueError(""req must not be None"")
        self.req = req

    def respond(self):
        raise NotImplementedError(""Subclasses must implement respond()"")"
258632,chovanecm/sacredboard,chovanecm_sacredboard/sacredboard/app/data/datastorage.py,sacredboard.app.data.datastorage.Cursor,"class Cursor:
    """"""Interface that abstracts the cursor object returned from databases.""""""

    def __init__(self):
        """"""Declare a new cursor to iterate over runs.""""""
        pass

    def count(self):
        """"""Return the number of items in this cursor.""""""
        raise NotImplementedError()

    def __iter__(self):
        """"""Iterate over elements.""""""
        raise NotImplementedError()","class Cursor:
    
    def __init__(self):
        
        pass
    def count(self):
        '''Return the number of items in this cursor.'''
        pass
    def __iter__(self):
        
        pass",snippet_169,4,"class Cursor:
    
    def __init__(self):
        self._items = []
        
    def count(self):
        '''Return the number of items in this cursor.'''
        return len(self._items)
    
    def __iter__(self):
        return iter(self._items)"
258643,chovanecm/sacredboard,chovanecm_sacredboard/sacredboard/app/data/metricsdao.py,sacredboard.app.data.metricsdao.MetricsDAO,"class MetricsDAO:
    """"""
    Interface for accessing Sacred metrics.

    Issue: https://github.com/chovanecm/sacredboard/issues/58

    Extended because of: https://github.com/chovanecm/sacredboard/issues/66
    """"""

    def get(self, run_id, metric_id):
        """"""
        Read a metric of the given id and run.

        The returned object has the following format (timestamps are datetime
         objects).

        .. code::

            {""steps"": [0,1,20,40,...],
            ""timestamps"": [timestamp1,timestamp2,timestamp3,...],
            ""values"": [0,1 2,3,4,5,6,...],
            ""name"": ""name of the metric"",
            ""metric_id"": ""metric_id"",
            ""run_id"": ""run_id""}

        :param run_id: ID of the Run that the metric belongs to.
        :param metric_id: The ID fo the metric.
        :return: The whole metric as specified.

        :raise NotFoundError
        """"""
        raise NotImplementedError('The MetricsDAO class is abstract.')

    def delete(self, run_id):
        """"""
        Delete all metrics belonging to the given run.

        :param run_id: ID of the Run that the metric belongs to.
        """"""
        raise NotImplementedError('The MetricsDAO class is abstract.')","class MetricsDAO:
    
    def get(self, run_id, metric_id):
        
        pass
    def delete(self, run_id):
        '''
        Delete all metrics belonging to the given run.
        :param run_id: ID of the Run that the metric belongs to.
        '''
        pass",snippet_170,3,"import threading
import copy

class MetricsDAO:
    def __init__(self):
        self._data = {}
        self._lock = threading.RLock()
    
    def get(self, run_id, metric_id):
        if run_id is None:
            raise ValueError(""run_id must not be None"")
        if metric_id is None:
            raise ValueError(""metric_id must not be None"")
        with self._lock:
            run_metrics = self._data.get(run_id, {})
            value = run_metrics.get(metric_id)
            return copy.deepcopy(value)
    
    def delete(self, run_id):
        '''
        Delete all metrics belonging to the given run.
        :param run_id: ID of the Run that the metric belongs to.
        '''
        if run_id is None:
            raise ValueError(""run_id must not be None"")
        with self._lock:
            if run_id in self._data:
                del self._data[run_id]
                return True
            return False"
258651,chovanecm/sacredboard,chovanecm_sacredboard/sacredboard/app/data/rundao.py,sacredboard.app.data.rundao.RunDAO,"class RunDAO:
    """"""
    Interface for accessing Runs.

    Issue: https://github.com/chovanecm/sacredboard/issues/69
    """"""

    def get(self, run_id):
        """"""
        Return the run associated with the id.

        :raise NotFoundError when not found
        """"""
        raise NotImplementedError('RunDAO is abstract.')

    def get_runs(self, sort_by=None, sort_direction=None, start=0, limit=None, query={'type': 'and', 'filters': []}):
        """"""Return all runs that match the query.""""""
        raise NotImplementedError('RunDAO is abstract.')

    def delete(self, run_id):
        """"""
        Delete run with the given id from the backend.

        :param run_id: Id of the run to delete.
        :raise NotImplementedError If not supported by the backend.
        :raise DataSourceError General data source error.
        :raise NotFoundError The run was not found. (Some backends may succeed
        even if the run does not exist.
        """"""
        raise NotImplementedError('This database data source does not currently support this operation.')","class RunDAO:
    
    def get(self, run_id):
        '''
        Return the run associated with the id.
        :raise NotFoundError when not found
        '''
        pass
    def get_runs(self, sort_by=None, sort_direction=None, start=0, limit=None, query={'type': 'and', 'filters': []}):
        
        pass
    def delete(self, run_id):
        '''
        Delete run with the given id from the backend.
        :param run_id: Id of the run to delete.
        :raise NotImplementedError If not supported by the backend.
        :raise DataSourceError General data source error.
        :raise NotFoundError The run was not found. (Some backends may succeed
        even if the run does not exist.
        '''
        pass",snippet_171,4,"class NotFoundError(Exception):
    pass


class DataSourceError(Exception):
    pass


class RunDAO:
    def __init__(self, runs=None, id_field=""id""):
        self._id_field = id_field
        self._runs = {}
        if runs:
            for run in runs:
                rid = self._extract_id(run)
                self._runs[rid] = run

    def _extract_id(self, run):
        try:
            return run[self._id_field]
        except Exception:
            raise DataSourceError(f""Run missing id field '{self._id_field}'"")

    def get(self, run_id):
        '''
        Return the run associated with the id.
        :raise NotFoundError when not found
        '''
        try:
            return self._runs[run_id]
        except KeyError:
            raise NotFoundError(f""Run '{run_id}' not found"")

    def get_runs(self, sort_by=None, sort_direction=None, start=0, limit=None, query={'type': 'and', 'filters': []}):
        try:
            items = list(self._runs.values())
            items = self._apply_query(items, query or {'type': 'and', 'filters': []})
            items = self._apply_sort(items, sort_by, sort_direction)
            if start is None or start < 0:
                start = 0
            if limit is None:
                return items[start:]
            if limit < 0:
                return []
            return items[start:start + limit]
        except NotFoundError:
            raise
        except Exception as e:
            raise DataSourceError(str(e)) from e

    def delete(self, run_id):
        '''
        Delete run with the given id from the backend.
        :param run_id: Id of the run to delete.
        :raise NotImplementedError If not supported by the backend.
        :raise DataSourceError General data source error.
        :raise NotFoundError The run was not found. (Some backends may succeed
        even if the run does not exist.
        '''
        try:
            del self._runs[run_id]
        except KeyError:
            raise NotFoundError(f""Run '{run_id}' not found"")
        except Exception as e:
            raise DataSourceError(str(e)) from e

    def _apply_query(self, items, query):
        qtype = (query.get('type') or 'and').lower()
        filters = query.get('filters') or []

        if not filters:
            return items

        def match(item, flt):
            field = flt.get('field')
            op = (flt.get('op') or 'eq').lower()
            value = flt.get('value')

            present = self._get_field(item, field, missing_sentinel=object())
            exists = present is not object()

            if op == 'exists':
                return bool(value) is True and exists or (bool(value) is False and not exists)

            if not exists:
                return False

            left = present
            right = value

            try:
                if op in ('eq', '=='):
                    return left == right
                if op in ('ne', '!='):
                    return left != right
                if op in ('lt', '<'):
                    return left < right
                if op in ('lte', '<='):
                    return left <= right
                if op in ('gt', '>'):
                    return left > right
                if op in ('gte', '>='):
                    return left >= right
                if op == 'in':
                    try:
                        return left in right
                    except TypeError:
                        return False
                if op == 'nin':
                    try:
                        return left not in right
                    except TypeError:
                        return True
                if op == 'contains':
                    try:
                        return right in left
                    except TypeError:
                        return False
                if op == 'icontains':
                    try:
                        return str(right).lower() in str(left).lower()
                    except Exception:
                        return False
                if op == 'startswith':
                    try:
                        return str(left).startswith(str(right))
                    except Exception:
                        return False
                if op == 'istartswith':
                    try:
                        return str(left).lower().startswith(str(right).lower())
                    except Exception:
                        return False
                if op == 'endswith':
                    try:
                        return str(left).endswith(str(right))
                    except Exception:
                        return False
                if op == 'iendswith':
                    try:
                        return str(left).lower().endswith(str(right).lower())
                    except Exception:
                        return False
            except Exception:
                return False
            return False

        def combine(flags):
            return all(flags) if qtype == 'and' else any(flags)

        if qtype == 'not':
            return [it for it in items if not combine([match(it, f) for f in filters])]

        return [it for it in items if combine([match(it, f) for f in filters])]

    def _apply_sort(self, items, sort_by, sort_direction):
        if not sort_by:
            return items

        if isinstance(sort_by, (list, tuple)):
            fields = list(sort_by)
        else:
            fields = [sort_by]

        if sort_direction is None:
            directions = ['asc'] * len(fields)
        elif isinstance(sort_direction, (list, tuple)):
            directions = [str(d or 'asc').lower() for d in sort_direction]
            if len(directions) < len(fields):
                directions += ['asc'] * (len(fields) - len(directions))
        else:
            directions = [str(sort_direction or 'asc').lower()] * len(fields)

        def keyfunc(item):
            keys = []
            for f in fields:
                val = self._get_field(item, f, None)
                keys.append(val)
            return tuple(keys)

        # Python can't sort with per-field directions directly; apply stable sorts in reverse order
        sorted_items = items
        for field, direction in reversed(list(zip(fields, directions))):
            reverse = direction.startswith('d')  # desc
            sorted_items = sorted(sorted_items, key=lambda it: self._get_field(it, field, None), reverse=reverse)
        return sorted_items

    def _get_field(self, item, field, missing_sentinel=None):
        if field is None:
            return missing_sentinel
        cur = item
        try:
            for part in str(field).split('.'):
                if isinstance(cur, dict):
                    if part in cur:
                        cur = cur[part]
                    else:
                        return missing_sentinel
                else:
                    if hasattr(cur, part):
                        cur = getattr(cur, part)
                    elif isinstance(cur, (list, tuple)) and part.isdigit():
                        idx = int(part)
                        try:
                            cur = cur[idx]
                        except Exception:
                            return missing_sentinel
                    else:
                        return missing_sentinel
            return cur
        except Exception:
            return missing_sentinel"
259541,mozilla-iot/webthing-python,mozilla-iot_webthing-python/webthing/server.py,webthing.server.MultipleThings,"class MultipleThings:
    """"""A container for multiple things.""""""

    def __init__(self, things, name):
        """"""
        Initialize the container.

        things -- the things to store
        name -- the mDNS server name
        """"""
        self.things = things
        self.name = name

    def get_thing(self, idx):
        """"""
        Get the thing at the given index.

        idx -- the index
        """"""
        try:
            idx = int(idx)
        except ValueError:
            return None
        if idx < 0 or idx >= len(self.things):
            return None
        return self.things[idx]

    def get_things(self):
        """"""Get the list of things.""""""
        return self.things

    def get_name(self):
        """"""Get the mDNS server name.""""""
        return self.name","class MultipleThings:
    
    def __init__(self, things, name):
        '''
        Initialize the container.
        things -- the things to store
        name -- the mDNS server name
        '''
        pass
    def get_thing(self, idx):
        
        pass
    def get_things(self):
        '''Get the list of things.'''
        pass
    def get_name(self):
        
        pass",snippet_172,5,"class MultipleThings:
    
    def __init__(self, things, name):
        '''
        Initialize the container.
        things -- the things to store
        name -- the mDNS server name
        '''
        self._things = list(things) if things is not None else []
        self._name = name

    def get_thing(self, idx):
        return self._things[idx]

    def get_things(self):
        '''Get the list of things.'''
        return list(self._things)

    def get_name(self):
        return self._name"
259544,mozilla-iot/webthing-python,mozilla-iot_webthing-python/webthing/server.py,webthing.server.SingleThing,"class SingleThing:
    """"""A container for a single thing.""""""

    def __init__(self, thing):
        """"""
        Initialize the container.

        thing -- the thing to store
        """"""
        self.thing = thing

    def get_thing(self, _=None):
        """"""Get the thing at the given index.""""""
        return self.thing

    def get_things(self):
        """"""Get the list of things.""""""
        return [self.thing]

    def get_name(self):
        """"""Get the mDNS server name.""""""
        return self.thing.title","class SingleThing:
    '''A container for a single thing.'''
    def __init__(self, thing):
        '''
        Initialize the container.
        thing -- the thing to store
        '''
        pass
    def get_thing(self, _=None):
        
        pass
    def get_things(self):
        '''Get the list of things.'''
        pass
    def get_name(self):
        '''Get the mDNS server name.'''
        pass",snippet_173,5,"class SingleThing:
    '''A container for a single thing.'''
    def __init__(self, thing):
        '''
        Initialize the container.
        thing -- the thing to store
        '''
        self._thing = thing

    def get_thing(self, _=None):
        return self._thing

    def get_things(self):
        '''Get the list of things.'''
        return [self._thing]

    def get_name(self):
        '''Get the mDNS server name.'''
        # Prefer common naming/title conventions if available
        if hasattr(self._thing, 'get_title') and callable(getattr(self._thing, 'get_title')):
            return self._thing.get_title()
        if hasattr(self._thing, 'title'):
            return getattr(self._thing, 'title')
        if hasattr(self._thing, 'get_name') and callable(getattr(self._thing, 'get_name')):
            return self._thing.get_name()
        if hasattr(self._thing, 'name'):
            return getattr(self._thing, 'name')
        return type(self._thing).__name__"
259548,mozilla-iot/webthing-python,mozilla-iot_webthing-python/webthing/subscriber.py,webthing.subscriber.Subscriber,"class Subscriber:
    """"""Abstract Subscriber class.""""""

    def update_property(self, property_):
        """"""
        Send an update about a Property.

        :param property_: Property
        """"""
        raise NotImplementedError

    def update_action(self, action):
        """"""
        Send an update about an Action.

        :param action: Action
        """"""
        raise NotImplementedError

    def update_event(self, event):
        """"""
        Send an update about an Event.

        :param event: Event
        """"""
        raise NotImplementedError","class Subscriber:
    
    def update_property(self, property_):
        
        pass
    def update_action(self, action):
        '''
        Send an update about an Action.
        :param action: Action
        '''
        pass
    def update_event(self, event):
        '''
        Send an update about an Event.
        :param event: Event
        '''
        pass",snippet_174,4,"class Subscriber:
    def __init__(self):
        self._properties = []
        self._actions = []
        self._events = []

    def update_property(self, property_):
        self._properties.append(property_)

    def update_action(self, action):
        '''
        Send an update about an Action.
        :param action: Action
        '''
        self._actions.append(action)

    def update_event(self, event):
        '''
        Send an update about an Event.
        :param event: Event
        '''
        self._events.append(event)

    @property
    def properties(self):
        return list(self._properties)

    @property
    def actions(self):
        return list(self._actions)

    @property
    def events(self):
        return list(self._events)

    def last_property(self):
        return self._properties[-1] if self._properties else None

    def last_action(self):
        return self._actions[-1] if self._actions else None

    def last_event(self):
        return self._events[-1] if self._events else None"
261819,user-cont/conu,user-cont_conu/conu/apidefs/image.py,conu.apidefs.image.S2Image,"class S2Image:
    """"""
    Additional functionality related to s2i-enabled container images
    """"""

    def extend(self, source, new_image_name, s2i_args=None):
        """"""
        extend this s2i-enabled image using provided source, raises ConuException if
        `s2i build` fails

        :param source: str, source used to extend the image, can be path or url
        :param new_image_name: str, name of the new, extended image
        :param s2i_args: list of str, additional options and arguments provided to `s2i build`
        :return: S2Image instance
        """"""
        raise NotImplementedError('extend method is not implemented')

    def usage(self):
        """"""
        Provide output of `s2i usage`

        :return: str
        """"""
        raise NotImplementedError('usage method is not implemented')","class S2Image:
    
    def extend(self, source, new_image_name, s2i_args=None):
        '''
        extend this s2i-enabled image using provided source, raises ConuException if
        `s2i build` fails
        :param source: str, source used to extend the image, can be path or url
        :param new_image_name: str, name of the new, extended image
        :param s2i_args: list of str, additional options and arguments provided to `s2i build`
        :return: S2Image instance
        '''
        pass
    def usage(self):
        '''
        Provide output of `s2i usage`
        :return: str
        '''
        pass",snippet_175,3,"import subprocess
from typing import List, Optional


class S2Image:

    def extend(self, source, new_image_name, s2i_args: Optional[List[str]] = None):
        '''
        extend this s2i-enabled image using provided source, raises ConuException if
        `s2i build` fails
        :param source: str, source used to extend the image, can be path or url
        :param new_image_name: str, name of the new, extended image
        :param s2i_args: list of str, additional options and arguments provided to `s2i build`
        :return: S2Image instance
        '''
        # ensure ConuException exists
        if 'ConuException' not in globals():
            class ConuException(Exception):
                pass
            globals()['ConuException'] = ConuException

        base_image = (
            getattr(self, 'image', None)
            or getattr(self, 'image_name', None)
            or getattr(self, 'name', None)
        )
        if not base_image:
            raise ConuException(""Base image name is not set on this S2Image instance (expected attribute: image, image_name, or name)"")

        if not isinstance(source, str) or not source:
            raise ConuException(""Parameter 'source' must be a non-empty string"")
        if not isinstance(new_image_name, str) or not new_image_name:
            raise ConuException(""Parameter 'new_image_name' must be a non-empty string"")

        cmd = [""s2i"", ""build"", source, str(base_image), new_image_name]
        if s2i_args:
            if not isinstance(s2i_args, list) or not all(isinstance(a, str) for a in s2i_args):
                raise ConuException(""Parameter 's2i_args' must be a list of strings"")
            # insert user args right after 'build' to allow flags before positional args
            cmd = [""s2i"", ""build""] + s2i_args + [source, str(base_image), new_image_name]

        try:
            res = subprocess.run(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=False,
                text=True,
            )
        except FileNotFoundError as e:
            raise ConuException(""s2i binary not found in PATH"") from e

        if res.returncode != 0:
            msg = f""s2i build failed with exit code {res.returncode}""
            if res.stderr:
                msg += f""\nstderr:\n{res.stderr.strip()}""
            if res.stdout:
                msg += f""\nstdout:\n{res.stdout.strip()}""
            raise ConuException(msg)

        new_obj = type(self)()
        # set commonly used attributes for the new image
        setattr(new_obj, 'image_name', new_image_name)
        setattr(new_obj, 'name', new_image_name)
        setattr(new_obj, 'image', new_image_name)
        return new_obj

    def usage(self):
        '''
        Provide output of `s2i usage`
        :return: str
        '''
        if 'ConuException' not in globals():
            class ConuException(Exception):
                pass
            globals()['ConuException'] = ConuException

        base_image = (
            getattr(self, 'image', None)
            or getattr(self, 'image_name', None)
            or getattr(self, 'name', None)
        )
        if not base_image:
            raise ConuException(""Base image name is not set on this S2Image instance (expected attribute: image, image_name, or name)"")

        cmd = [""s2i"", ""usage"", str(base_image)]
        try:
            res = subprocess.run(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=False,
                text=True,
            )
        except FileNotFoundError as e:
            raise ConuException(""s2i binary not found in PATH"") from e

        if res.returncode != 0:
            msg = f""s2i usage failed with exit code {res.returncode}""
            if res.stderr:
                msg += f""\nstderr:\n{res.stderr.strip()}""
            if res.stdout:
                msg += f""\nstdout:\n{res.stdout.strip()}""
            raise ConuException(msg)

        return res.stdout or """""
264367,materialsproject/custodian,materialsproject_custodian/src/custodian/ansible/interpreter.py,custodian.ansible.interpreter.Modder,"from custodian.ansible.actions import DictActions
import re

class Modder:
    """"""
    Class to modify a dict/file/any object using a mongo-like language.
    Keywords are mostly adopted from mongo's syntax, but instead of $, an
    underscore precedes action keywords. This is so that the modification can
    be inserted into a mongo db easily.

    Allowable actions are supplied as a list of classes as an argument. Refer
    to the action classes on what the actions do. Action classes are in
    pymatpro.ansible.actions.

    Examples:
    >>> modder = Modder()
    >>> dct = {""Hello"": ""World""}
    >>> mod = {'_set': {'Hello':'Universe', 'Bye': 'World'}}
    >>> modder.modify(mod, dct)
    >>> dct['Bye']
    'World'
    >>> dct['Hello']
    'Universe'
    """"""

    def __init__(self, actions=None, strict=True, directory='./') -> None:
        """"""Initialize a Modder from a list of supported actions.

        Args:
            actions ([Action]): A sequence of supported actions. See
                :mod:`custodian.ansible.actions`. Default is None,
                which means only DictActions are supported.
            strict (bool): Indicating whether to use strict mode. In non-strict
                mode, unsupported actions are simply ignored without any
                errors raised. In strict mode, if an unsupported action is
                supplied, a ValueError is raised. Defaults to True.
            directory (str): The directory containing the files to be modified.
                Defaults to ""./"".
        """"""
        self.supported_actions = {}
        actions = actions if actions is not None else [DictActions]
        for action in actions:
            for attr in dir(action):
                if not re.match('__\\w+__', attr) and callable(getattr(action, attr)):
                    self.supported_actions[f'_{attr}'] = getattr(action, attr)
        self.strict = strict
        self.directory = directory

    def modify(self, modification, obj) -> None:
        """"""
        Note that modify makes actual in-place modifications. It does not
        return a copy.

        Args:
            modification (dict): Modification must be {action_keyword :
                settings}. E.g., {'_set': {'Hello':'Universe', 'Bye': 'World'}}
            obj (dict/str/object): Object to modify depending on actions. For
                example, for DictActions, obj will be a dict to be modified.
                For FileActions, obj will be a string with a full pathname to a
                file.
        """"""
        for action, settings in modification.items():
            if action in self.supported_actions:
                self.supported_actions[action](obj, settings, directory=self.directory)
            elif self.strict:
                raise ValueError(f'{action} is not a supported action!')

    def modify_object(self, modification, obj):
        """"""
        Modify an object that supports pymatgen's as_dict() and from_dict API.

        Args:
            modification (dict): Modification must be {action_keyword :
                settings}. E.g., {'_set': {'Hello':'Universe', 'Bye': 'World'}}
            obj (object): Object to modify
        """"""
        dct = obj.as_dict()
        self.modify(modification, dct)
        return obj.from_dict(dct)","
class Modder:
    '''
    Class to modify a dict/file/any object using a mongo-like language.
    Keywords are mostly adopted from mongo's syntax, but instead of $, an
    underscore precedes action keywords. This is so that the modification can
    be inserted into a mongo db easily.
    Allowable actions are supplied as a list of classes as an argument. Refer
    to the action classes on what the actions do. Action classes are in
    pymatpro.ansible.actions.
    Examples:
    >>> modder = Modder()
    >>> dct = {""Hello"": ""World""}
    >>> mod = {'_set': {'Hello':'Universe', 'Bye': 'World'}}
    >>> modder.modify(mod, dct)
    >>> dct['Bye']
    'World'
    >>> dct['Hello']
    'Universe'
    '''
    def __init__(self, actions=None, strict=True, directory='./') -> None:
        
        pass
    def modify(self, modification, obj) -> None:
        
        pass
    def modify_object(self, modification, obj):
        '''
        Modify an object that supports pymatgen's as_dict() and from_dict API.
        Args:
            modification (dict): Modification must be {action_keyword :
                settings}. E.g., {'_set': {'Hello':'Universe', 'Bye': 'World'}}
            obj (object): Object to modify
        '''
        pass",snippet_176,4,"class Modder:
    '''
    Class to modify a dict/file/any object using a mongo-like language.
    Keywords are mostly adopted from mongo's syntax, but instead of $, an
    underscore precedes action keywords. This is so that the modification can
    be inserted into a mongo db easily.
    Allowable actions are supplied as a list of classes as an argument. Refer
    to the action classes on what the actions do. Action classes are in
    pymatpro.ansible.actions.
    Examples:
    >>> modder = Modder()
    >>> dct = {""Hello"": ""World""}
    >>> mod = {'_set': {'Hello':'Universe', 'Bye': 'World'}}
    >>> modder.modify(mod, dct)
    >>> dct['Bye']
    'World'
    >>> dct['Hello']
    'Universe'
    '''
    def __init__(self, actions=None, strict=True, directory='./') -> None:
        self.strict = strict
        self.directory = directory

        # Registry: keyword -> callable(modder, obj, settings)
        self._actions = {}

        # Built-in actions
        self._register_builtin('_set', self._action_set)
        self._register_builtin('_unset', self._action_unset)
        self._register_builtin('_inc', self._action_inc)
        self._register_builtin('_push', self._action_push)
        self._register_builtin('_pull', self._action_pull)

        # External/custom actions (optional)
        if actions:
            for act in actions:
                inst = act() if isinstance(act, type) else act
                kw = getattr(inst, 'keyword', None)
                apply_fn = getattr(inst, 'apply', None)
                if not kw or not callable(apply_fn):
                    if self.strict:
                        raise ValueError(""Action must define 'keyword' and callable 'apply'"")
                    else:
                        continue
                # Wrap to pass self for helper usage if the apply expects 3 args
                def make_wrapper(f):
                    def wrapper(modder, obj, settings, _f=f):
                        # Try signatures (modder, obj, settings) or (obj, settings)
                        try:
                            return _f(modder, obj, settings)
                        except TypeError:
                            return _f(obj, settings)
                    return wrapper
                self._actions[kw] = make_wrapper(apply_fn)

    def modify(self, modification, obj) -> None:
        if not isinstance(modification, dict):
            raise TypeError(""modification must be a dict of actions"")

        # Object path if it implements as_dict/from_dict
        if hasattr(obj, 'as_dict') and callable(getattr(obj, 'as_dict')) and \
           hasattr(obj.__class__, 'from_dict') and callable(getattr(obj.__class__, 'from_dict')):
            # Modify via dict representation and return new object
            new_obj = self.modify_object(modification, obj)
            # If caller passed a dict-like object container, we cannot replace in-place here.
            # Caller should use modify_object's return value.
            return new_obj

        # Directly modify dicts
        if isinstance(obj, dict):
            for action_kw, settings in modification.items():
                action = self._actions.get(action_kw)
                if action is None:
                    if self.strict:
                        raise KeyError(f""Unknown action keyword: {action_kw}"")
                    else:
                        continue
                action(self, obj, settings)
            return None

        # Unsupported object without as_dict/from_dict contract
        if self.strict:
            raise TypeError(""Object must be a dict or support as_dict()/from_dict()"")
        return None

    def modify_object(self, modification, obj):
        '''
        Modify an object that supports pymatgen's as_dict() and from_dict API.
        Args:
            modification (dict): Modification must be {action_keyword :
                settings}. E.g., {'_set': {'Hello':'Universe', 'Bye': 'World'}}
            obj (object): Object to modify
        '''
        if not (hasattr(obj, 'as_dict') and callable(getattr(obj, 'as_dict')) and
                hasattr(obj.__class__, 'from_dict') and callable(getattr(obj.__class__, 'from_dict'))):
            if self.strict:
                raise TypeError(""Object must support as_dict() and from_dict()"")
            return obj

        d = obj.as_dict()
        self.modify(modification, d)
        # Return a new instance based on modified dict
        return obj.__class__.from_dict(d)

    # ----------------------
    # Built-in action impls
    # ----------------------
    def _register_builtin(self, keyword, func):
        self._actions[keyword] = func

    def _action_set(self, obj, settings):
        if not isinstance(settings, dict):
            raise TypeError(""_set requires a dict of path: value"")
        for path, value in settings.items():
            self._set_path_value(obj, path, value, create=True)

    def _action_unset(self, obj, settings):
        # settings can be dict with flags or list/tuple of paths
        paths = []
        if isinstance(settings, dict):
            paths = [p for p, flag in settings.items() if flag]
        elif isinstance(settings, (list, tuple, set)):
            paths = list(settings)
        else:
            raise TypeError(""_unset requires dict or list of paths"")
        for path in paths:
            self._unset_path(obj, path)

    def _action_inc(self, obj, settings):
        if not isinstance(settings, dict):
            raise TypeError(""_inc requires a dict of path: number"")
        for path, inc_by in settings.items():
            parent, key = self._traverse_to_parent(obj, path, create=True)
            current = self._get_from_parent(parent, key, default=0)
            try:
                new_val = current + inc_by
            except Exception as e:
                if self.strict:
                    raise TypeError(f""Cannot increment non-numeric value at {path}: {e}"")
                else:
                    continue
            self._set_in_parent(parent, key, new_val)

    def _action_push(self, obj, settings):
        # settings: dict of path: value_to_append or path: [values] to extend
        if not isinstance(settings, dict):
            raise TypeError(""_push requires a dict of path: value"")
        for path, to_add in settings.items():
            parent, key = self._traverse_to_parent(obj, path, create=True)
            lst = self._get_from_parent(parent, key, default=None)
            if lst is None:
                lst = []
            if not isinstance(lst, list):
                if self.strict:
                    raise TypeError(f""_push target at {path} is not a list"")
                else:
                    continue
            if isinstance(to_add, list):
                lst.extend(to_add)
            else:
                lst.append(to_add)
            self._set_in_parent(parent, key, lst)

    def _action_pull(self, obj, settings):
        # settings: dict of path: value_to_remove; removes equal elements
        if not isinstance(settings, dict):
            raise TypeError(""_pull requires a dict of path: value"")
        for path, to_remove in settings.items():
            parent, key = self._traverse_to_parent(obj, path, create=False)
            if parent is None:
                if self.strict:
                    raise KeyError(f""Path not found for _pull: {path}"")
                continue
            lst = self._get_from_parent(parent, key, default=None)
            if not isinstance(lst, list):
                if self.strict:
                    raise TypeError(f""_pull target at {path} is not a list"")
                else:
                    continue
            lst = [x for x in lst if x != to_remove]
            self._set_in_parent(parent, key, lst)

    # ----------------------
    # Path helpers
    # ----------------------
    def _split_path(self, path):
        if isinstance(path, (list, tuple)):
            parts = list(path)
        elif isinstance(path, str):
            parts = path.split('.') if path else []
        else:
            parts = [path]
        return [self._coerce_index(p) for p in parts]

    @staticmethod
    def _coerce_index(token):
        # Convert numeric strings to int for list indexing
        if isinstance(token, str):
            if token.isdigit() or (token.startswith('-') and token[1:].isdigit()):
                try:
                    return int(token)
                except Exception:
                    return token
        return token

    def _traverse_to_parent(self, obj, path, create=False):
        parts = self._split_path(path)
        if not parts:
            return None, None
        cur = obj
        for i, part in enumerate(parts[:-1]):
            nxt = self._get_child(cur, part)
            if nxt is None:
                if not create:
                    return None, None
                nxt = [] if isinstance(parts[i+1], int) else {}
                self._set_child(cur, part, nxt)
            cur = nxt
        return cur, parts[-1]

    def _get_child(self, container, key):
        if isinstance(key, int):
            if isinstance(container, list):
                if -len(container) <= key < len(container):
                    return container[key]
                return None
            return None
        else:
            if isinstance(container, dict):
                return container.get(key, None)
            return None

    def _set_child(self, container, key, value):
        if isinstance(key, int):
            if not isinstance(container, list):
                if self.strict:
                    raise TypeError(""Trying to set list index on non-list container"")
                return
            # Extend list as needed
            idx = key
            if idx < 0:
                # Support negative indices by converting to positive position
                idx = len(container) + idx
            if idx < 0:
                # Prepend with None to reach index 0
                container[:0] = [None] * (-idx)
                idx = 0
            if idx >= len(container):
                container.extend([None] * (idx - len(container) + 1))
            container[key] = value
        else:
            if not isinstance(container, dict):
                if self.strict:
                    raise TypeError(""Trying to set dict key on non-dict container"")
                return
            container[key] = value

    def _get_from_parent(self, parent, key, default=None):
        if isinstance(key, int):
            if isinstance(parent, list) and -len(parent) <= key < len(parent):
                return parent[key]
            return default
        else:
            if isinstance(parent, dict):
                return parent.get(key, default)
            return default

    def _set_in_parent(self, parent, key, value):
        self._set_child(parent, key, value)

    def _set_path_value(self, obj, path, value, create=True):
        parent, key = self._traverse_to_parent(obj, path, create=create)
        if parent is None:
            if self.strict:
                raise KeyError(f""Path not found: {path}"")
            return
        self._set_in_parent(parent, key, value)

    def _unset_path(self, obj, path):
        parent, key = self._traverse_to_parent(obj, path, create=False)
        if parent is None:
            if self.strict:
                raise KeyError(f""Path not found: {path}"")
            return
        if isinstance(key, int):
            if isinstance(parent, list):
                if -len(parent) <= key < len(parent):
                    # Remove element to shift left, mimicking pull by index
                    parent.pop(key)
                elif self.strict:
                    raise IndexError(f""Index out of range for _unset at {path}"")
        else:
            if isinstance(parent, dict):
                if key in parent:
                    del parent[key]
                elif self.strict:
                    raise KeyError(f""Key not found for _unset at {path}"")"
264406,materialsproject/custodian,materialsproject_custodian/src/custodian/utils.py,custodian.utils.tracked_lru_cache,"import functools
from typing import ClassVar

class tracked_lru_cache:
    """"""
    Decorator wrapping the functools.lru_cache adding a tracking of the
    functions that have been wrapped.

    Exposes a method to clear the cache of all the wrapped functions.

    Used to cache the parsed outputs in handlers/validators, to avoid
    multiple parsing of the same file.
    Allows Custodian to clear the cache after all the checks have been performed.
    """"""
    cached_functions: ClassVar = set()

    def __init__(self, func) -> None:
        """"""
        Args:
            func: function to be decorated.
        """"""
        self.func = functools.lru_cache(func)
        functools.update_wrapper(self, func)
        self.cache_info = self.func.cache_info
        self.cache_clear = self.func.cache_clear

    def __call__(self, *args, **kwargs):
        """"""Call the decorated function.""""""
        result = self.func(*args, **kwargs)
        self.cached_functions.add(self.func)
        return result

    @classmethod
    def tracked_cache_clear(cls) -> None:
        """"""Clear the cache of all the decorated functions.""""""
        while cls.cached_functions:
            f = cls.cached_functions.pop()
            f.cache_clear()","
class tracked_lru_cache:
    '''
    Decorator wrapping the functools.lru_cache adding a tracking of the
    functions that have been wrapped.
    Exposes a method to clear the cache of all the wrapped functions.
    Used to cache the parsed outputs in handlers/validators, to avoid
    multiple parsing of the same file.
    Allows Custodian to clear the cache after all the checks have been performed.
    '''
    def __init__(self, func) -> None:
        
        pass
    def __call__(self, *args, **kwargs):
        
        pass
    @classmethod
    def tracked_cache_clear(cls) -> None:
        
        pass",snippet_177,4,"from functools import lru_cache, update_wrapper
import weakref

class tracked_lru_cache:
    '''
    Decorator wrapping the functools.lru_cache adding a tracking of the
    functions that have been wrapped.
    Exposes a method to clear the cache of all the wrapped functions.
    Used to cache the parsed outputs in handlers/validators, to avoid
    multiple parsing of the same file.
    Allows Custodian to clear the cache after all the checks have been performed.
    '''
    _tracked_instances = weakref.WeakSet()

    def __init__(self, func) -> None:
        cached = lru_cache(maxsize=None)(func)
        self._cached = cached
        update_wrapper(self, func)
        self.__wrapped__ = cached
        type(self)._tracked_instances.add(self)

    def __call__(self, *args, **kwargs):
        return self._cached(*args, **kwargs)

    @classmethod
    def tracked_cache_clear(cls) -> None:
        for inst in list(cls._tracked_instances):
            try:
                inst._cached.cache_clear()
            except Exception:
                pass"
265199,Anaconda-Platform/anaconda-client,Anaconda-Platform_anaconda-client/binstar_client/utils/tables.py,binstar_client.utils.tables.TableCell,"import typing

class TableCell:
    """"""
    General definition of a table cell.

    :param kind: Kind of the cell (used for styling purposes, see :class:`~TableDesign`).
    :param value: Exact content of the cell.
    :param alignment: How text should be aligned in the cell.
    """"""
    __slots__ = ('alignment', 'kind', 'value')

    def __init__(self, kind: str, value: typing.Any, *, alignment: 'Alignment'='<') -> None:
        """"""Initialize new :class:`~TableCell` instance.""""""
        if value is None:
            value = ''
        self.alignment: 'Alignment' = alignment
        self.kind: str = kind
        self.value: str = str(value)

    def __repr__(self) -> str:
        """"""Prepare a string representation of the instance.""""""
        return f'{type(self).__name__}(kind={self.kind!r}, value={self.value!r}, alignment={self.alignment!r})'

    def __str__(self) -> str:
        """"""Prepare a string representation of the instance.""""""
        return self.value","
class TableCell:
    '''
    General definition of a table cell.
    :param kind: Kind of the cell (used for styling purposes, see :class:`~TableDesign`).
    :param value: Exact content of the cell.
    :param alignment: How text should be aligned in the cell.
    '''
    def __init__(self, kind: str, value: typing.Any, *, alignment: 'Alignment'='<') -> None:
        '''Initialize new :class:`~TableCell` instance.'''
        pass
    def __repr__(self) -> str:
        '''Prepare a string representation of the instance.'''
        pass
    def __str__(self) -> str:
        '''Prepare a string representation of the instance.'''
        pass",snippet_178,4,"import typing

class TableCell:
    '''
    General definition of a table cell.
    :param kind: Kind of the cell (used for styling purposes, see :class:`~TableDesign`).
    :param value: Exact content of the cell.
    :param alignment: How text should be aligned in the cell.
    '''
    def __init__(self, kind: str, value: typing.Any, *, alignment: 'Alignment' = '<') -> None:
        '''Initialize new :class:`~TableCell` instance.'''
        self.kind = kind
        self.value = value
        self.alignment = alignment

    def __repr__(self) -> str:
        '''Prepare a string representation of the instance.'''
        return f""{self.__class__.__name__}(kind={self.kind!r}, value={self.value!r}, alignment={self.alignment!r})""

    def __str__(self) -> str:
        '''Prepare a string representation of the instance.'''
        return str(self.value)"
266878,facelessuser/wcmatch,facelessuser_wcmatch/wcmatch/util.py,wcmatch.util.Immutable,"from typing import Any, Callable, AnyStr, Match, Pattern

class Immutable:
    """"""Immutable.""""""
    __slots__: tuple[Any, ...] = ()

    def __init__(self, **kwargs: Any) -> None:
        """"""Initialize.""""""
        for k, v in kwargs.items():
            super(Immutable, self).__setattr__(k, v)

    def __setattr__(self, name: str, value: Any) -> None:
        """"""Prevent mutability.""""""
        raise AttributeError('Class is immutable!')","
class Immutable:
    '''Immutable.'''
    def __init__(self, **kwargs: Any) -> None:
        
        pass
    def __setattr__(self, name: str, value: Any) -> None:
        
        pass",snippet_180,3,"from typing import Any

class Immutable:
    '''Immutable.'''
    def __init__(self, **kwargs: Any) -> None:
        object.__setattr__(self, ""_Immutable__frozen"", False)
        for k, v in kwargs.items():
            object.__setattr__(self, k, v)
        object.__setattr__(self, ""_Immutable__frozen"", True)

    def __setattr__(self, name: str, value: Any) -> None:
        if name == ""_Immutable__frozen"":
            object.__setattr__(self, name, value)
            return
        if getattr(self, ""_Immutable__frozen"", False):
            raise AttributeError(f""{self.__class__.__name__} is immutable"")
        object.__setattr__(self, name, value)"
269275,CLARIAH/grlc,CLARIAH_grlc/src/fileLoaders.py,src.fileLoaders.BaseLoader,"from grlc.queryTypes import qType, guessQueryType
import json

class BaseLoader:
    """"""Base class for File Loaders""""""

    def getTextForName(self, query_name):
        """"""Return the query text and query type for the given query name.
        Note that file extention is not part of the query name. For example,
        for `query_name='query1'` would return the content of file `query1.rq`
        from the loader's source (assuming such file exists).""""""
        candidateNames = [query_name + '.rq', query_name + '.sparql', query_name + '.tpf', query_name + '.json']
        candidates = [(name, guessQueryType(name)) for name in candidateNames]
        for queryFullName, queryType in candidates:
            queryText = self._getText(queryFullName)
            if queryText:
                if queryType == qType['JSON']:
                    queryText = json.loads(queryText)
                    if 'proto' not in queryText and '@graph' not in queryText:
                        continue
                return (queryText, queryType)
        return ('', None)

    def _getText(self, queryFullName):
        """"""To be implemented by sub-classes.
        Returns None if the file does not exist.""""""
        raise NotImplementedError('Subclasses must override _getText()!')

    def fetchFiles(self):
        """"""To be implemented by sub-classes""""""
        raise NotImplementedError('Subclasses must override fetchFiles()!')","
class BaseLoader:
    '''Base class for File Loaders'''
    def getTextForName(self, query_name):
        '''Return the query text and query type for the given query name.
        Note that file extention is not part of the query name. For example,
        for `query_name='query1'` would return the content of file `query1.rq`
        from the loader's source (assuming such file exists).'''
        pass
    def _getText(self, queryFullName):
        
        pass
    def fetchFiles(self):
        
        pass",snippet_181,4,"class BaseLoader:
    '''Base class for File Loaders'''

    # Known extensions mapped to query types; subclasses may override/extend
    EXT_TO_TYPE = {
        '.rq': 'SPARQL',
        '.sparql': 'SPARQL',
        '.ru': 'SPARQL-UPDATE',
        '.sql': 'SQL',
        '.graphql': 'GRAPHQL',
        '.gql': 'GRAPHQL',
        '.cypher': 'CYPHER',
        '.cql': 'CYPHER',
        '.json': 'JSON',
        '.ndjson': 'NDJSON',
        '.yaml': 'YAML',
        '.yml': 'YAML',
        '.xml': 'XML',
        '.txt': 'TEXT',
    }

    # Extension resolution priority; subclasses can override
    EXT_PRIORITY = (
        '.rq',
        '.sparql',
        '.ru',
        '.sql',
        '.graphql',
        '.gql',
        '.cypher',
        '.cql',
        '.json',
        '.ndjson',
        '.yaml',
        '.yml',
        '.xml',
        '.txt',
    )

    def getTextForName(self, query_name):
        '''Return the query text and query type for the given query name.
        Note that file extention is not part of the query name. For example,
        for `query_name='query1'` would return the content of file `query1.rq`
        from the loader's source (assuming such file exists).'''
        if not isinstance(query_name, str) or not query_name:
            raise ValueError(""query_name must be a non-empty string"")

        available = self.fetchFiles()
        if available is None:
            available = []
        available_set = set(available)

        # If the name already includes an extension, use it directly if available
        dot_idx = query_name.rfind('.')
        candidates = []
        if dot_idx != -1 and dot_idx != len(query_name) - 1:
            # Has an extension
            candidates = [query_name]
        else:
            # Try with known extensions in priority order
            candidates = [f""{query_name}{ext}"" for ext in self.EXT_PRIORITY]

        chosen = None
        for cand in candidates:
            if cand in available_set:
                chosen = cand
                break

        if chosen is None:
            # As a last resort, if exactly one file matches the basename regardless of extension, use it
            basename_matches = [fname for fname in available if fname.startswith(query_name + '.')]
            if len(basename_matches) == 1:
                chosen = basename_matches[0]

        if chosen is None:
            raise FileNotFoundError(f""No file found for query name '{query_name}'"")

        text = self._getText(chosen)
        if not isinstance(text, str):
            raise TypeError(""_getText must return a string"")

        # Determine query type from extension
        ext = ''
        dot = chosen.rfind('.')
        if dot != -1:
            ext = chosen[dot:].lower()
        qtype = self.EXT_TO_TYPE.get(ext, ext.lstrip('.').upper() if ext else 'UNKNOWN')

        return text, qtype

    def _getText(self, queryFullName):
        raise NotImplementedError(""_getText must be implemented by subclasses to return file contents as string"")

    def fetchFiles(self):
        raise NotImplementedError(""fetchFiles must be implemented by subclasses to return an iterable of available file names"")"
271814,shoebot/shoebot,shoebot_shoebot/lib/photobot/__init__.py,photobot.Blend,"from PIL import Image, ImageChops, ImageFilter, ImageEnhance, ImageOps, ImageDraw, ImageStat

class Blend:
    """"""Layer blending modes.

    Implements additional blending modes to those present in PIL.
    These blending functions can not be used separately from
    the canvas.flatten() method, where the alpha compositing
    of two layers is handled.

    Since these blending are not part of a C library,
    but pure Python, they take forever to process.
    """"""

    def overlay(self, img1, img2):
        """"""Applies the overlay blend mode.

        Overlays image img2 on image img1. The overlay pixel combines
        multiply and screen: it multiplies dark pixels values and screen
        light values. Returns a composite image with the alpha channel
        retained.
        """"""
        p1 = list(img1.getdata())
        p2 = list(img2.getdata())
        for i in range(len(p1)):
            p3 = ()
            for j in range(len(p1[i])):
                a = p1[i][j] / 255.0
                b = p2[i][j] / 255.0
                if j == 3:
                    d = min(a, b)
                elif a > 0.5:
                    d = 2 * (a + b - a * b) - 1
                else:
                    d = 2 * a * b
                p3 += (int(d * 255),)
            p1[i] = p3
        img = Image.new('RGBA', img1.size, 255)
        img.putdata(p1)
        return img

    def hue(self, img1, img2):
        """"""Applies the hue blend mode.

        Hues image img1 with image img2. The hue filter replaces the
        hues of pixels in img1 with the hues of pixels in img2. Returns
        a composite image with the alpha channel retained.
        """"""
        import colorsys
        p1 = list(img1.getdata())
        p2 = list(img2.getdata())
        for i in range(len(p1)):
            r1, g1, b1, a1 = p1[i]
            r1 = r1 / 255.0
            g1 = g1 / 255.0
            b1 = b1 / 255.0
            h1, s1, v1 = colorsys.rgb_to_hsv(r1, g1, b1)
            r2, g2, b2, a2 = p2[i]
            r2 = r2 / 255.0
            g2 = g2 / 255.0
            b2 = b2 / 255.0
            h2, s2, v2 = colorsys.rgb_to_hsv(r2, g2, b2)
            r3, g3, b3 = colorsys.hsv_to_rgb(h2, s1, v1)
            r3 = int(r3 * 255)
            g3 = int(g3 * 255)
            b3 = int(b3 * 255)
            p1[i] = (r3, g3, b3, a1)
        img = Image.new('RGBA', img1.size, 255)
        img.putdata(p1)
        return img

    def color(self, img1, img2):
        """"""Applies the color blend mode.

        Colorize image img1 with image img2. The color filter replaces
        the hue and saturation of pixels in img1 with the hue and
        saturation of pixels in img2. Returns a composite image with the
        alpha channel retained.
        """"""
        import colorsys
        p1 = list(img1.getdata())
        p2 = list(img2.getdata())
        for i in range(len(p1)):
            r1, g1, b1, a1 = p1[i]
            r1 = r1 / 255.0
            g1 = g1 / 255.0
            b1 = b1 / 255.0
            h1, s1, v1 = colorsys.rgb_to_hsv(r1, g1, b1)
            r2, g2, b2, a2 = p2[i]
            r2 = r2 / 255.0
            g2 = g2 / 255.0
            b2 = b2 / 255.0
            h2, s2, v2 = colorsys.rgb_to_hsv(r2, g2, b2)
            r3, g3, b3 = colorsys.hsv_to_rgb(h2, s2, v1)
            r3 = int(r3 * 255)
            g3 = int(g3 * 255)
            b3 = int(b3 * 255)
            p1[i] = (r3, g3, b3, a1)
        img = Image.new('RGBA', img1.size, 255)
        img.putdata(p1)
        return img","
class Blend:
    
    def overlay(self, img1, img2):
        '''Applies the overlay blend mode.
        Overlays image img2 on image img1. The overlay pixel combines
        multiply and screen: it multiplies dark pixels values and screen
        light values. Returns a composite image with the alpha channel
        retained.
        '''
        pass
    def hue(self, img1, img2):
        '''Applies the hue blend mode.
        Hues image img1 with image img2. The hue filter replaces the
        hues of pixels in img1 with the hues of pixels in img2. Returns
        a composite image with the alpha channel retained.
        '''
        pass
    def color(self, img1, img2):
        
        pass",snippet_182,4,"import numpy as np
from PIL import Image

class Blend:
    
    def _to_rgba(self, img):
        if not isinstance(img, Image.Image):
            raise TypeError(""img must be a PIL.Image"")
        if img.mode != ""RGBA"":
            return img.convert(""RGBA"")
        return img

    def _overlay_rgb(self, base_rgb, blend_rgb):
        # base_rgb, blend_rgb: float32 arrays in [0,1], shape (H,W,3)
        low_mask = base_rgb <= 0.5
        high_mask = ~low_mask
        out = np.empty_like(base_rgb)
        out[low_mask] = 2.0 * base_rgb[low_mask] * blend_rgb[low_mask]
        out[high_mask] = 1.0 - 2.0 * (1.0 - base_rgb[high_mask]) * (1.0 - blend_rgb[high_mask])
        return np.clip(out, 0.0, 1.0)

    def _rgb_to_hls(self, rgb):
        # rgb: float array [0,1], shape (...,3)
        r = rgb[..., 0]
        g = rgb[..., 1]
        b = rgb[..., 2]
        maxc = np.maximum(np.maximum(r, g), b)
        minc = np.minimum(np.minimum(r, g), b)
        l = (minc + maxc) / 2.0

        h = np.zeros_like(l)
        s = np.zeros_like(l)

        diff = maxc - minc
        non_zero = diff > 1e-12

        # Saturation
        s_tmp = np.empty_like(s)
        # where l <= 0.5
        mask_low_l = (l <= 0.5) & non_zero
        mask_high_l = (l > 0.5) & non_zero
        s_tmp[mask_low_l] = diff[mask_low_l] / (maxc[mask_low_l] + minc[mask_low_l])
        s_tmp[mask_high_l] = diff[mask_high_l] / (2.0 - maxc[mask_high_l] - minc[mask_high_l])
        s[non_zero] = s_tmp[non_zero]
        s[~non_zero] = 0.0

        # Hue
        rc = np.zeros_like(r)
        gc = np.zeros_like(g)
        bc = np.zeros_like(b)
        rc[non_zero] = (maxc - r)[non_zero] / diff[non_zero]
        gc[non_zero] = (maxc - g)[non_zero] / diff[non_zero]
        bc[non_zero] = (maxc - b)[non_zero] / diff[non_zero]

        is_r = (r == maxc) & non_zero
        is_g = (g == maxc) & non_zero
        is_b = (b == maxc) & non_zero

        h[is_r] = (bc - gc)[is_r]
        h[is_g] = 2.0 + (rc - bc)[is_g]
        h[is_b] = 4.0 + (gc - rc)[is_b]

        h = (h / 6.0) % 1.0
        h[~non_zero] = 0.0

        return np.stack([h, l, s], axis=-1)

    def _hls_to_rgb(self, hls):
        # hls: float array [0,1], shape (...,3)
        h = hls[..., 0]
        l = hls[..., 1]
        s = hls[..., 2]

        r = np.empty_like(h)
        g = np.empty_like(h)
        b = np.empty_like(h)

        def hue_to_rgb(m1, m2, hval):
            hval = hval % 1.0
            out = np.empty_like(hval)
            cond1 = hval < (1.0 / 6.0)
            cond2 = (hval >= (1.0 / 6.0)) & (hval < 0.5)
            cond3 = (hval >= 0.5) & (hval < (2.0 / 3.0))
            out[cond1] = m1[cond1] + (m2[cond1] - m1[cond1]) * 6.0 * hval[cond1]
            out[cond2] = m2[cond2]
            out[cond3] = m1[cond3] + (m2[cond3] - m1[cond3]) * (2.0 / 3.0 - hval[cond3]) * 6.0
            mask_else = ~(cond1 | cond2 | cond3)
            out[mask_else] = m1[mask_else]
            return out

        # If s == 0 then r=g=b=l
        no_sat = s <= 1e-12
        sat = ~no_sat

        r[no_sat] = l[no_sat]
        g[no_sat] = l[no_sat]
        b[no_sat] = l[no_sat]

        if np.any(sat):
            l_sat = l[sat]
            s_sat = s[sat]
            h_sat = h[sat]
            m2 = np.empty_like(l_sat)
            low_mask = l_sat <= 0.5
            m2[low_mask] = l_sat[low_mask] * (1.0 + s_sat[low_mask])
            m2[~low_mask] = l_sat[~low_mask] + s_sat[~low_mask] - l_sat[~low_mask] * s_sat[~low_mask]
            m1 = 2.0 * l_sat - m2

            r_sat = hue_to_rgb(m1, m2, h_sat + 1.0 / 3.0)
            g_sat = hue_to_rgb(m1, m2, h_sat)
            b_sat = hue_to_rgb(m1, m2, h_sat - 1.0 / 3.0)

            r[sat] = r_sat
            g[sat] = g_sat
            b[sat] = b_sat

        rgb = np.stack([r, g, b], axis=-1)
        return np.clip(rgb, 0.0, 1.0)

    def overlay(self, img1, img2):
        '''Applies the overlay blend mode.
        Overlays image img2 on image img1. The overlay pixel combines
        multiply and screen: it multiplies dark pixels values and screen
        light values. Returns a composite image with the alpha channel
        retained.
        '''
        base = self._to_rgba(img1)
        blend = self._to_rgba(img2)
        if base.size != blend.size:
            raise ValueError(""Images must be the same size"")

        base_arr = np.asarray(base, dtype=np.uint8)
        blend_arr = np.asarray(blend, dtype=np.uint8)

        base_rgb = base_arr[..., :3].astype(np.float32) / 255.0
        blend_rgb = blend_arr[..., :3].astype(np.float32) / 255.0

        out_rgb = self._overlay_rgb(base_rgb, blend_rgb)
        out_rgb_u8 = (out_rgb * 255.0 + 0.5).astype(np.uint8)

        out_a = base_arr[..., 3:4]  # retain alpha from base
        out = np.concatenate([out_rgb_u8, out_a], axis=-1)
        return Image.fromarray(out, mode=""RGBA"")

    def hue(self, img1, img2):
        '''Applies the hue blend mode.
        Hues image img1 with image img2. The hue filter replaces the
        hues of pixels in img1 with the hues of pixels in img2. Returns
        a composite image with the alpha channel retained.
        '''
        base = self._to_rgba(img1)
        blend = self._to_rgba(img2)
        if base.size != blend.size:
            raise ValueError(""Images must be the same size"")

        base_arr = np.asarray(base, dtype=np.uint8)
        blend_arr = np.asarray(blend, dtype=np.uint8)

        base_rgb = base_arr[..., :3].astype(np.float32) / 255.0
        blend_rgb = blend_arr[..., :3].astype(np.float32) / 255.0

        base_hls = self._rgb_to_hls(base_rgb)
        blend_hls = self._rgb_to_hls(blend_rgb)

        out_hls = base_hls.copy()
        out_hls[..., 0] = blend_hls[..., 0]  # take hue from blend; keep L and S from base

        out_rgb = self._hls_to_rgb(out_hls)
        out_rgb_u8 = (out_rgb * 255.0 + 0.5).astype(np.uint8)

        out_a = base_arr[..., 3:4]
        out = np.concatenate([out_rgb_u8, out_a], axis=-1)
        return Image.fromarray(out, mode=""RGBA"")

    def color(self, img1, img2):
        base = self._to_rgba(img1)
        blend = self._to_rgba(img2)
        if base.size != blend.size:
            raise ValueError(""Images must be the same size"")

        base_arr = np.asarray(base, dtype=np.uint8)
        blend_arr = np.asarray(blend, dtype=np.uint8)

        base_rgb = base_arr[..., :3].astype(np.float32) / 255.0
        blend_rgb = blend_arr[..., :3].astype(np.float32) / 255.0

        base_hls = self._rgb_to_hls(base_rgb)
        blend_hls = self._rgb_to_hls(blend_rgb)

        out_hls = np.empty_like(base_hls)
        # color blend: hue and saturation from blend, luminance (lightness) from base
        out_hls[..., 0] = blend_hls[..., 0]
        out_hls[..., 2] = blend_hls[..., 2]
        out_hls[..., 1] = base_hls[..., 1]

        out_rgb = self._hls_to_rgb(out_hls)
        out_rgb_u8 = (out_rgb * 255.0 + 0.5).astype(np.uint8)

        out_a = base_arr[..., 3:4]
        out = np.concatenate([out_rgb_u8, out_a], axis=-1)
        return Image.fromarray(out, mode=""RGBA"")"
272626,cackharot/suds-py3,cackharot_suds-py3/suds/reader.py,suds.reader.Reader,"from suds.plugin import PluginContainer
import hashlib

class Reader:
    """"""
    The reader provides integration with cache.
    @ivar options: An options object.
    @type options: I{Options}
    """"""

    def __init__(self, options):
        """"""
        @param options: An options object.
        @type options: I{Options}
        """"""
        self.options = options
        self.plugins = PluginContainer(options.plugins)

    def mangle(self, name, x):
        """"""
        Mangle the name by hashing the I{name} and appending I{x}.
        @return: the mangled name.
        """"""
        h = hashlib.sha256(name.encode('utf8')).hexdigest()
        return '%s-%s' % (h, x)","
class Reader:
    '''
    The reader provides integration with cache.
    @ivar options: An options object.
    @type options: I{Options}
    '''
    def __init__(self, options):
        
        pass
    def mangle(self, name, x):
        
        pass",snippet_183,3,"class Reader:
    '''
    The reader provides integration with cache.
    @ivar options: An options object.
    @type options: Options
    '''
    def __init__(self, options):
        self.options = options

    def mangle(self, name, x):
        import json
        import hashlib

        if not isinstance(name, str) or not name:
            raise ValueError(""name must be a non-empty string"")

        # Determine optional prefix/namespace from options
        prefix = """"
        for attr in (""cache_prefix"", ""prefix"", ""namespace"", ""cache_namespace""):
            val = getattr(self.options, attr, None)
            if isinstance(val, str) and val:
                prefix = val
                break

        # Serialize x in a stable way and hash to form a compact key
        try:
            payload = json.dumps(x, sort_keys=True, separators=("","", "":""), default=str)
        except TypeError:
            payload = repr(x)

        digest = hashlib.sha256(payload.encode(""utf-8"")).hexdigest()

        parts = []
        if prefix:
            parts.append(prefix)
        parts.append(name)
        parts.append(digest)
        return "":"".join(parts)"
272646,cackharot/suds-py3,cackharot_suds-py3/suds/sax/enc.py,suds.sax.enc.Encoder,"import re

class Encoder:
    """"""
    An XML special character encoder/decoder.
    @cvar encodings: A mapping of special characters encoding.
    @type encodings: [(str,str)]
    @cvar decodings: A mapping of special characters decoding.
    @type decodings: [(str,str)]
    @cvar special: A list of special characters
    @type special: [char]
    """"""
    encodings = (('&', '&amp;'), ('<', '&lt;'), ('>', '&gt;'), ('""', '&quot;'), (""'"", '&apos;'))
    decodings = (('&lt;', '<'), ('&gt;', '>'), ('&quot;', '""'), ('&apos;', ""'""), ('&amp;', '&'))
    special = ('&', '<', '>', '""', ""'"")

    def needsEncoding(self, s):
        """"""
        Get whether string I{s} contains special characters.
        @param s: A string to check.
        @type s: str
        @return: True if needs encoding.
        @rtype: boolean
        """"""
        if isinstance(s, str):
            for c in self.special:
                if c in s:
                    return True
        return False

    def encode(self, s):
        """"""
        Encode special characters found in string I{s}.
        @param s: A string to encode.
        @type s: str
        @return: The encoded string.
        @rtype: str
        """"""
        if isinstance(s, str) and self.needsEncoding(s):
            for x in self.encodings:
                s = re.sub(x[0], x[1], s)
        return s

    def decode(self, s):
        """"""
        Decode special characters encodings found in string I{s}.
        @param s: A string to decode.
        @type s: str
        @return: The decoded string.
        @rtype: str
        """"""
        if isinstance(s, str) and '&' in s:
            for x in self.decodings:
                s = s.replace(x[0], x[1])
        return s","
class Encoder:
    
    def needsEncoding(self, s):
        '''
        Get whether string I{s} contains special characters.
        @param s: A string to check.
        @type s: str
        @return: True if needs encoding.
        @rtype: boolean
        '''
        pass
    def encode(self, s):
        
        pass
    def decode(self, s):
        
        pass",snippet_184,4,"class Encoder:
    
    def needsEncoding(self, s):
        '''
        Get whether string I{s} contains special characters.
        @param s: A string to check.
        @type s: str
        @return: True if needs encoding.
        @rtype: boolean
        '''
        if s is None:
            return False
        if not isinstance(s, str):
            s = str(s)
        specials = {'&', '<', '>', '""', ""'""}
        return any(ch in specials for ch in s)

    def encode(self, s):
        if s is None:
            return None
        if not isinstance(s, str):
            s = str(s)
        import re
        # Encode & only if it does not start an existing entity
        s = re.sub(r'&(?!(?:#\d+|#x[0-9a-fA-F]+|\w+);)', '&amp;', s)
        s = s.replace('<', '&lt;').replace('>', '&gt;')
        s = s.replace('""', '&quot;').replace(""'"", '&apos;')
        return s

    def decode(self, s):
        if s is None:
            return None
        if not isinstance(s, str):
            s = str(s)
        # Decode named entities
        s = s.replace('&lt;', '<').replace('&gt;', '>')
        s = s.replace('&quot;', '""').replace('&apos;', ""'"")
        # Decode numeric entities (decimal and hex)
        import re

        def repl_dec(m):
            text = m.group(0)
            try:
                cp = int(m.group(1), 10)
                return chr(cp)
            except Exception:
                return text

        def repl_hex(m):
            text = m.group(0)
            try:
                cp = int(m.group(1), 16)
                return chr(cp)
            except Exception:
                return text

        s = re.sub(r'&#(\d+);', repl_dec, s)
        s = re.sub(r'&#[xX]([0-9a-fA-F]+);', repl_hex, s)
        # Finally decode &amp; (do this last to avoid interfering with other entities)
        s = s.replace('&amp;', '&')
        return s"
272664,cackharot/suds-py3,cackharot_suds-py3/suds/transport/__init__.py,suds.transport.Transport,"class Transport:
    """"""
    The transport I{interface}.
    """"""

    def __init__(self):
        """"""
        Constructor.
        """"""
        from suds.transport.options import Options
        self.options = Options()
        del Options

    def open(self, request):
        """"""
        Open the url in the specified request.
        @param request: A transport request.
        @type request: L{Request}
        @return: An input stream.
        @rtype: stream
        @raise TransportError: On all transport errors.
        """"""
        raise Exception('not-implemented')

    def send(self, request):
        """"""
        Send soap message.  Implementations are expected to handle:
            - proxies
            - I{http} headers
            - cookies
            - sending message
            - brokering exceptions into L{TransportError}
        @param request: A transport request.
        @type request: L{Request}
        @return: The reply
        @rtype: L{Reply}
        @raise TransportError: On all transport errors.
        """"""
        raise Exception('not-implemented')","class Transport:
    '''
    The transport I{interface}.
    '''
    def __init__(self):
        '''
        Constructor.
        '''
        pass
    def open(self, request):
        
        pass
    def send(self, request):
        
        pass",snippet_185,4,"from abc import ABC, abstractmethod
from typing import Any

class Transport(ABC):
    '''
    The transport interface.
    '''
    def __init__(self) -> None:
        '''
        Constructor.
        '''
        super().__init__()

    @abstractmethod
    def open(self, request: Any) -> None:
        pass

    @abstractmethod
    def send(self, request: Any) -> Any:
        pass"
272699,cackharot/suds-py3,cackharot_suds-py3/suds/xsd/doctor.py,suds.xsd.doctor.TnsFilter,"class TnsFilter:
    """"""
    Target Namespace filter.
    @ivar tns: A list of target namespaces.
    @type tns: [str,...]
    """"""

    def __init__(self, *tns):
        """"""
        @param tns: A list of target namespaces.
        @type tns: [str,...]
        """"""
        self.tns = []
        self.add(*tns)

    def add(self, *tns):
        """"""
        Add I{targetNamesapces} to be added.
        @param tns: A list of target namespaces.
        @type tns: [str,...]
        """"""
        self.tns += tns

    def match(self, root, ns):
        """"""
        Match by I{targetNamespace} excluding those that
        are equal to the specified namespace to prevent
        adding an import to itself.
        @param root: A schema root.
        @type root: L{Element}
        """"""
        tns = root.get('targetNamespace')
        if len(self.tns):
            matched = tns in self.tns
        else:
            matched = 1
        itself = ns == tns
        return matched and (not itself)","class TnsFilter:
    
    def __init__(self, *tns):
        
        pass
    def add(self, *tns):
        
        pass
    def match(self, root, ns):
        '''
        Match by I{targetNamespace} excluding those that
        are equal to the specified namespace to prevent
        adding an import to itself.
        @param root: A schema root.
        @type root: L{Element}
        '''
        pass",snippet_186,4,"class TnsFilter:
    
    def __init__(self, *tns):
        self._tns = set()
        if tns:
            self.add(*tns)

    def add(self, *tns):
        def _iter(items):
            for it in items:
                if it is None:
                    continue
                if isinstance(it, (list, tuple, set, frozenset)):
                    for sub in it:
                        yield sub
                else:
                    yield it

        for ns in _iter(tns):
            if isinstance(ns, str) and ns.strip():
                self._tns.add(ns)

    def match(self, root, ns):
        '''
        Match by I{targetNamespace} excluding those that
        are equal to the specified namespace to prevent
        adding an import to itself.
        @param root: A schema root.
        @type root: L{Element}
        '''
        if root is None:
            return False
        get_attr = getattr(root, ""get"", None)
        if not callable(get_attr):
            return False
        target_ns = root.get(""targetNamespace"")
        if not target_ns:
            return False
        if target_ns == ns:
            return False
        if not self._tns:
            return True
        return target_ns in self._tns"
272711,cackharot/suds-py3,cackharot_suds-py3/suds/xsd/sxbase.py,suds.xsd.sxbase.NodeFinder,"class NodeFinder:
    """"""
    Find nodes based on flexable criteria.  The I{matcher} is
    may be any object that implements a match(n) method.
    @ivar matcher: An object used as criteria for match.
    @type matcher: I{any}.match(n)
    @ivar limit: Limit the number of matches.  0=unlimited.
    @type limit: int
    """"""

    def __init__(self, matcher, limit=0):
        """"""
        @param matcher: An object used as criteria for match.
        @type matcher: I{any}.match(n)
        @param limit: Limit the number of matches.  0=unlimited.
        @type limit: int
        """"""
        self.matcher = matcher
        self.limit = limit

    def find(self, node, list):
        """"""
        Traverse the tree looking for matches.
        @param node: A node to match on.
        @type node: L{SchemaObject}
        @param list: A list to fill.
        @type list: list
        """"""
        if self.matcher.match(node):
            list.append(node)
            self.limit -= 1
            if self.limit == 0:
                return
        for c in node.rawchildren:
            self.find(c, list)
        return self","class NodeFinder:
    '''
    Find nodes based on flexable criteria.  The I{matcher} is
    may be any object that implements a match(n) method.
    @ivar matcher: An object used as criteria for match.
    @type matcher: I{any}.match(n)
    @ivar limit: Limit the number of matches.  0=unlimited.
    @type limit: int
    '''
    def __init__(self, matcher, limit=0):
        '''
        @param matcher: An object used as criteria for match.
        @type matcher: I{any}.match(n)
        @param limit: Limit the number of matches.  0=unlimited.
        @type limit: int
        '''
        pass
    def find(self, node, list):
        
        pass",snippet_187,3,"class NodeFinder:
    '''
    Find nodes based on flexable criteria.  The matcher
    may be any object that implements a match(n) method.
    @ivar matcher: An object used as criteria for match.
    @type matcher: any with .match(n)
    @ivar limit: Limit the number of matches.  0=unlimited.
    @type limit: int
    '''
    def __init__(self, matcher, limit=0):
        '''
        @param matcher: An object used as criteria for match.
        @type matcher: any with .match(n)
        @param limit: Limit the number of matches.  0=unlimited.
        @type limit: int
        '''
        self.matcher = matcher
        self.limit = int(limit) if limit is not None else 0

    def _iter_children(self, node):
        if node is None:
            return
        # Common attributes for child collections
        for name in ('children', 'childNodes', 'childs', 'nodes', 'subnodes', 'subNodes', 'elements'):
            if hasattr(node, name):
                children = getattr(node, name)
                if callable(children):
                    try:
                        children = children()
                    except TypeError:
                        children = None
                if children is not None:
                    try:
                        for ch in children:
                            yield ch
                    except TypeError:
                        pass
                return
        # Fallback: treat node as iterable (but not string/bytes)
        if hasattr(node, '__iter__') and not isinstance(node, (str, bytes)):
            try:
                for ch in node:
                    yield ch
            except TypeError:
                return

    def find(self, node, list):
        count = 0

        def walk(n):
            nonlocal count
            if n is None:
                return True if (self.limit and count >= self.limit) else False

            try:
                is_match = self.matcher.match(n) if hasattr(self.matcher, 'match') else False
            except Exception:
                is_match = False

            if is_match:
                list.append(n)
                count += 1
                if self.limit and count >= self.limit:
                    return True

            for child in self._iter_children(n):
                if self.limit and count >= self.limit:
                    return True
                if walk(child):
                    if self.limit and count >= self.limit:
                        return True
            return False

        walk(node)"
279001,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.AES_CTR_Mechanism,"import PyKCS11.LowLevel

class AES_CTR_Mechanism:
    """"""CKM_AES_CTR encryption mechanism""""""

    def __init__(self, counterBits, counterBlock):
        """"""
        :param counterBits: the number of incremented bits in the counter block
        :param counterBlock: a 16-byte initial value of the counter block
        """"""
        self._param = PyKCS11.LowLevel.CK_AES_CTR_PARAMS()
        self._source_cb = ckbytelist(counterBlock)
        self._param.ulCounterBits = counterBits
        self._param.cb = self._source_cb
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_AES_CTR
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_AES_CTR_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class AES_CTR_Mechanism:
    '''CKM_AES_CTR encryption mechanism'''
    def __init__(self, counterBits, counterBlock):
        '''
        :param counterBits: the number of incremented bits in the counter block
        :param counterBlock: a 16-byte initial value of the counter block
        '''
        pass
    def to_native(self):
        
        pass",snippet_190,3,"class AES_CTR_Mechanism:
    '''CKM_AES_CTR encryption mechanism'''
    def __init__(self, counterBits, counterBlock):
        '''
        :param counterBits: the number of incremented bits in the counter block
        :param counterBlock: a 16-byte initial value of the counter block
        '''
        if not isinstance(counterBits, int):
            raise TypeError(""counterBits must be an int"")
        if not (1 <= counterBits <= 128):
            raise ValueError(""counterBits must be in the range 1..128"")

        if isinstance(counterBlock, (bytes, bytearray, memoryview)):
            cb = bytes(counterBlock)
        else:
            raise TypeError(""counterBlock must be a bytes-like object"")

        if len(cb) != 16:
            raise ValueError(""counterBlock must be exactly 16 bytes long"")

        self.counterBits = counterBits
        self.counterBlock = cb

    def to_native(self):
        import struct
        return struct.pack(""=I16s"", self.counterBits, self.counterBlock)"
279002,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.AES_GCM_Mechanism,"import PyKCS11.LowLevel

class AES_GCM_Mechanism:
    """"""CKM_AES_GCM warpping mechanism""""""

    def __init__(self, iv, aad, tagBits):
        """"""
        :param iv: initialization vector
        :param aad: additional authentication data
        :param tagBits: length of authentication tag in bits
        """"""
        self._param = PyKCS11.LowLevel.CK_GCM_PARAMS()
        self._source_iv = ckbytelist(iv)
        self._param.pIv = self._source_iv
        self._param.ulIvLen = len(self._source_iv)
        self._source_aad = ckbytelist(aad)
        self._param.pAAD = self._source_aad
        self._param.ulAADLen = len(self._source_aad)
        self._param.ulTagBits = tagBits
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_AES_GCM
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_GCM_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class AES_GCM_Mechanism:
    '''CKM_AES_GCM warpping mechanism'''
    def __init__(self, iv, aad, tagBits):
        
        pass
    def to_native(self):
        
        pass",snippet_191,3,"class AES_GCM_Mechanism:
    '''CKM_AES_GCM warpping mechanism'''
    def __init__(self, iv, aad, tagBits):
        if iv is None:
            raise ValueError(""iv must not be None"")
        if aad is None:
            aad = b""""
        if isinstance(iv, bytearray):
            iv = bytes(iv)
        if isinstance(aad, bytearray):
            aad = bytes(aad)
        if not isinstance(iv, (bytes, memoryview)):
            raise TypeError(""iv must be bytes-like"")
        if not isinstance(aad, (bytes, memoryview)):
            raise TypeError(""aad must be bytes-like"")
        if not isinstance(tagBits, int) or tagBits <= 0:
            raise ValueError(""tagBits must be a positive integer"")
        self.iv = bytes(iv)
        self.aad = bytes(aad)
        self.tagBits = int(tagBits)

    def to_native(self):
        iv_len = len(self.iv)
        aad_len = len(self.aad)
        iv_bits = iv_len * 8
        return (self.iv, iv_len, iv_bits, self.aad, aad_len, self.tagBits)"
279010,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.CONCATENATE_BASE_AND_KEY_Mechanism,"import PyKCS11.LowLevel

class CONCATENATE_BASE_AND_KEY_Mechanism:
    """"""CKM_CONCATENATE_BASE_AND_KEY key derivation mechanism""""""

    def __init__(self, encKey):
        """"""
        :param encKey: a handle of encryption key
        """"""
        self._encKey = encKey
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_CONCATENATE_BASE_AND_KEY
        self._mech.pParameter = self._encKey
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_OBJECT_HANDLE_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class CONCATENATE_BASE_AND_KEY_Mechanism:
    
    def __init__(self, encKey):
        '''
        :param encKey: a handle of encryption key
        '''
        pass
    def to_native(self):
        '''convert mechanism to native format'''
        pass",snippet_192,3,"class CONCATENATE_BASE_AND_KEY_Mechanism:
    
    def __init__(self, encKey):
        '''
        :param encKey: a handle of encryption key
        '''
        if encKey is None:
            raise ValueError(""encKey must not be None"")
        try:
            self.encKey = int(encKey)
        except (TypeError, ValueError):
            raise TypeError(""encKey must be an integer-like handle"")
        if self.encKey < 0:
            raise ValueError(""encKey must be a non-negative integer handle"")

    def to_native(self):
        '''convert mechanism to native format'''
        return self.encKey"
279012,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.CkClass,"class CkClass:
    """"""
    Base class for CK_* classes
    """"""
    flags_dict = {}
    fields = {}
    flags = 0

    def flags2text(self):
        """"""
        parse the `self.flags` field and create a list of `CKF_*` strings
        corresponding to bits set in flags

        :return: a list of strings
        :rtype: list
        """"""
        r = []
        for k, v in self.flags_dict.items():
            if self.flags & k:
                r.append(v)
        return r

    def state2text(self):
        """"""
        Dummy method. Will be overwriden if necessary
        """"""
        return ''

    def to_dict(self):
        """"""
        convert the fields of the object into a dictionnary
        """"""
        dico = {}
        for field in self.fields:
            if field == 'flags':
                dico[field] = self.flags2text()
            elif field == 'state':
                dico[field] = self.state2text()
            else:
                dico[field] = self.__dict__[field]
        return dico

    def __str__(self):
        """"""
        text representation of the object
        """"""
        dico = self.to_dict()
        lines = []
        for key in sorted(dico.keys()):
            ck_type = self.fields[key]
            if ck_type == 'flags':
                flags = ', '.join(dico[key])
                lines.append(f'{key}: {flags}')
            elif ck_type == 'pair':
                p1, p2 = dico[key]
                lines.append(f'{key}: {p1}.{p2}')
            else:
                lines.append(f'{key}: {dico[key]}')
        return '\n'.join(lines)","class CkClass:
    
    def flags2text(self):
        
        pass
    def state2text(self):
        '''
        Dummy method. Will be overwriden if necessary
        '''
        pass
    def to_dict(self):
        '''
        convert the fields of the object into a dictionnary
        '''
        pass
    def __str__(self):
        '''
        text representation of the object
        '''
        pass",snippet_193,5,"class CkClass:
    def flags2text(self):
        flags = getattr(self, 'flags', 0)
        fmap = getattr(self, 'flags_map', {})
        if not isinstance(flags, int) or not isinstance(fmap, dict) or not fmap:
            return str(flags)
        names = []
        remaining = flags
        # Sort masks descending to handle composite masks consistently
        for mask, name in sorted(fmap.items(), key=lambda kv: kv[0], reverse=True):
            if not isinstance(mask, int):
                continue
            if flags & mask == mask and mask != 0:
                names.append(str(name))
                remaining &= ~mask
        if remaining:
            names.append(hex(remaining))
        return ','.join(names)

    def state2text(self):
        state = getattr(self, 'state', None)
        smap = getattr(self, 'state_map', None)
        if isinstance(smap, dict) and state in smap:
            return str(smap[state])
        return str(state)

    def to_dict(self):
        def serialize(value):
            if isinstance(value, CkClass):
                return value.to_dict()
            if isinstance(value, dict):
                return {k: serialize(v) for k, v in value.items()}
            if isinstance(value, (list, tuple, set)):
                t = type(value)
                return [serialize(v) for v in value]
            return value

        out = {}
        for k, v in self.__dict__.items():
            if k.startswith('_'):
                continue
            if callable(v):
                continue
            out[k] = serialize(v)
        return out

    def __str__(self):
        items = []
        for k, v in self.to_dict().items():
            items.append(f""{k}={v!r}"")
        return f""{self.__class__.__name__}("" + "", "".join(items) + "")"""
279015,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.EXTRACT_KEY_FROM_KEY_Mechanism,"import PyKCS11.LowLevel

class EXTRACT_KEY_FROM_KEY_Mechanism:
    """"""CKM_EXTRACT_KEY_FROM_KEY key derivation mechanism""""""

    def __init__(self, extractParams):
        """"""
        :param extractParams: the index of the first bit of the original
        key to be used in the newly-derived key.  For example if
        extractParams=5 then the 5 first bits are skipped and not used.
        """"""
        self._param = PyKCS11.LowLevel.CK_EXTRACT_PARAMS()
        self._param.assign(extractParams)
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_EXTRACT_KEY_FROM_KEY
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_EXTRACT_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class EXTRACT_KEY_FROM_KEY_Mechanism:
    
    def __init__(self, extractParams):
        '''
        :param extractParams: the index of the first bit of the original
        key to be used in the newly-derived key.  For example if
        extractParams=5 then the 5 first bits are skipped and not used.
        '''
        pass
    def to_native(self):
        
        pass",snippet_194,3,"class EXTRACT_KEY_FROM_KEY_Mechanism:
    
    def __init__(self, extractParams):
        '''
        :param extractParams: the index of the first bit of the original
        key to be used in the newly-derived key.  For example if
        extractParams=5 then the 5 first bits are skipped and not used.
        '''
        if isinstance(extractParams, bool) or not isinstance(extractParams, int):
            raise TypeError(""extractParams must be an integer (bit index)."")
        if extractParams < 0:
            raise ValueError(""extractParams must be a non-negative integer."")
        self.extractParams = extractParams

    def to_native(self):
        return self.extractParams"
279016,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.KEY_DERIVATION_STRING_DATA_MechanismBase,"import PyKCS11.LowLevel

class KEY_DERIVATION_STRING_DATA_MechanismBase:
    """"""Base class for mechanisms using derivation string data""""""

    def __init__(self, data, mechType):
        """"""
        :param data: a byte array to concatenate the key with
        :param mechType: mechanism type
        """"""
        self._param = PyKCS11.LowLevel.CK_KEY_DERIVATION_STRING_DATA()
        self._data = ckbytelist(data)
        self._param.pData = self._data
        self._param.ulLen = len(self._data)
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = mechType
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_KEY_DERIVATION_STRING_DATA_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class KEY_DERIVATION_STRING_DATA_MechanismBase:
    '''Base class for mechanisms using derivation string data'''
    def __init__(self, data, mechType):
        
        pass
    def to_native(self):
        '''convert mechanism to native format'''
        pass",snippet_195,3,"class KEY_DERIVATION_STRING_DATA_MechanismBase:
    '''Base class for mechanisms using derivation string data'''
    def __init__(self, data, mechType):
        if isinstance(data, str):
            data = data.encode('utf-8')
        elif isinstance(data, memoryview):
            data = data.tobytes()
        elif isinstance(data, bytearray):
            data = bytes(data)
        elif not isinstance(data, bytes):
            raise TypeError(""data must be bytes, bytearray, memoryview, or str"")

        self._data = data
        self._mech_type = mechType

    def to_native(self):
        '''convert mechanism to native format'''
        return {
            'mech_type': self._mech_type,
            'pData': self._data,
            'ulLen': len(self._data),
        }"
279017,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.Mechanism,"import PyKCS11.LowLevel

class Mechanism:
    """"""Wraps CK_MECHANISM""""""

    def __init__(self, mechanism, param=None):
        """"""
        :param mechanism: the mechanism to be used
        :type mechanism: integer, any `CKM_*` value
        :param param: data to be used as crypto operation parameter
          (i.e. the IV for some algorithms)
        :type param: string or list/tuple of bytes

        :see: :func:`Session.decrypt`, :func:`Session.sign`
        """"""
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = mechanism
        self._param = None
        if param:
            self._param = ckbytelist(param)
            self._mech.pParameter = self._param
            self._mech.ulParameterLen = len(param)

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class Mechanism:
    '''Wraps CK_MECHANISM'''
    def __init__(self, mechanism, param=None):
        
        pass
    def to_native(self):
        '''convert mechanism to native format'''
        pass",snippet_196,3,"class Mechanism:
    '''Wraps CK_MECHANISM'''
    def __init__(self, mechanism, param=None):
        self.mechanism = mechanism
        self.param = self._normalize_param(param)

    def _normalize_param(self, param):
        if param is None:
            return None
        if isinstance(param, (bytes, bytearray, memoryview)):
            return bytes(param)
        if isinstance(param, str):
            return param.encode('utf-8')
        if isinstance(param, int):
            if param == 0:
                return b'\x00'
            length = (param.bit_length() + 7) // 8
            return param.to_bytes(length, byteorder='big')
        try:
            # Try buffer protocol
            return bytes(param)
        except Exception as e:
            raise TypeError(f""Unsupported parameter type: {type(param).__name__}"") from e

    def to_native(self):
        '''convert mechanism to native format'''
        if self.param is None:
            return {
                'mechanism': self.mechanism,
                'parameter': None,
                'parameter_len': 0
            }
        return {
            'mechanism': self.mechanism,
            'parameter': self.param,
            'parameter_len': len(self.param)
        }"
279020,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.RSAOAEPMechanism,"import PyKCS11.LowLevel

class RSAOAEPMechanism:
    """"""RSA OAEP Wrapping mechanism""""""

    def __init__(self, hashAlg, mgf, label=None):
        """"""
        :param hashAlg: the hash algorithm to use (like `CKM_SHA256`)
        :param mgf: the mask generation function to use (like
          `CKG_MGF1_SHA256`)
        :param label: the (optional) label to use
        """"""
        self._param = PyKCS11.LowLevel.CK_RSA_PKCS_OAEP_PARAMS()
        self._param.hashAlg = hashAlg
        self._param.mgf = mgf
        self._source = None
        self._param.source = CKZ_DATA_SPECIFIED
        if label:
            self._source = ckbytelist(label)
            self._param.ulSourceDataLen = len(self._source)
        else:
            self._param.ulSourceDataLen = 0
        self._param.pSourceData = self._source
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_RSA_PKCS_OAEP
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_RSA_PKCS_OAEP_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class RSAOAEPMechanism:
    
    def __init__(self, hashAlg, mgf, label=None):
        '''
        :param hashAlg: the hash algorithm to use (like `CKM_SHA256`)
        :param mgf: the mask generation function to use (like
          `CKG_MGF1_SHA256`)
        :param label: the (optional) label to use
        '''
        pass
    def to_native(self):
        '''convert mechanism to native format'''
        pass",snippet_197,3,"class RSAOAEPMechanism:
    
    def __init__(self, hashAlg, mgf, label=None):
        '''
        :param hashAlg: the hash algorithm to use (like `CKM_SHA256`)
        :param mgf: the mask generation function to use (like
          `CKG_MGF1_SHA256`)
        :param label: the (optional) label to use
        '''
        if not isinstance(hashAlg, (int, str)):
            raise TypeError(""hashAlg must be int or str"")
        if not isinstance(mgf, (int, str)):
            raise TypeError(""mgf must be int or str"")

        if label is None:
            normalized_label = None
        elif isinstance(label, bytes):
            normalized_label = label
        elif isinstance(label, bytearray):
            normalized_label = bytes(label)
        elif isinstance(label, str):
            normalized_label = label.encode(""utf-8"")
        else:
            raise TypeError(""label must be bytes, bytearray, str, or None"")

        self.hash_alg = hashAlg
        self.mgf = mgf
        self.label = normalized_label

    def to_native(self):
        '''convert mechanism to native format'''
        source = ""CKZ_DATA_SPECIFIED"" if self.label not in (None, b"""") else ""CKZ_NONE""
        return {
            ""hashAlg"": self.hash_alg,
            ""mgf"": self.mgf,
            ""source"": source,
            ""label"": self.label or b"""",
        }"
279021,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.RSA_PSS_Mechanism,"import PyKCS11.LowLevel

class RSA_PSS_Mechanism:
    """"""RSA PSS Wrapping mechanism""""""

    def __init__(self, mecha, hashAlg, mgf, sLen):
        """"""
        :param mecha: the mechanism to use (like
          `CKM_SHA384_RSA_PKCS_PSS`)
        :param hashAlg: the hash algorithm to use (like `CKM_SHA384`)
        :param mgf: the mask generation function to use (like
          `CKG_MGF1_SHA384`)
        :param sLen: length, in bytes, of the salt value used in the PSS
          encoding (like 0 or the message length)
        """"""
        self._param = PyKCS11.LowLevel.CK_RSA_PKCS_PSS_PARAMS()
        self._param.hashAlg = hashAlg
        self._param.mgf = mgf
        self._param.sLen = sLen
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = mecha
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_RSA_PKCS_PSS_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class RSA_PSS_Mechanism:
    
    def __init__(self, mecha, hashAlg, mgf, sLen):
        '''
        :param mecha: the mechanism to use (like
          `CKM_SHA384_RSA_PKCS_PSS`)
        :param hashAlg: the hash algorithm to use (like `CKM_SHA384`)
        :param mgf: the mask generation function to use (like
          `CKG_MGF1_SHA384`)
        :param sLen: length, in bytes, of the salt value used in the PSS
          encoding (like 0 or the message length)
        '''
        pass
    def to_native(self):
        
        pass",snippet_198,3,"class RSA_PSS_Mechanism:
    
    def __init__(self, mecha, hashAlg, mgf, sLen):
        '''
        :param mecha: the mechanism to use (like
          `CKM_SHA384_RSA_PKCS_PSS`)
        :param hashAlg: the hash algorithm to use (like `CKM_SHA384`)
        :param mgf: the mask generation function to use (like
          `CKG_MGF1_SHA384`)
        :param sLen: length, in bytes, of the salt value used in the PSS
          encoding (like 0 or the message length)
        '''
        if sLen is None:
            sLen = 0
        if not isinstance(sLen, int) or sLen < 0:
            raise ValueError(""sLen must be a non-negative integer"")
        self.mecha = mecha
        self.hashAlg = hashAlg
        self.mgf = mgf
        self.sLen = sLen

    def to_native(self):
        return {
            'mechanism': self.mecha,
            'params': {
                'hashAlg': self.hashAlg,
                'mgf': self.mgf,
                'sLen': self.sLen,
            }
        }"
281845,datadotworld/data.world-py,datadotworld_data.world-py/datadotworld/datadotworld.py,datadotworld.datadotworld.UriParam,"class UriParam:
    """"""Represents a URI value as a parameter to a SPARQL query""""""

    def __init__(self, uri):
        """"""
        Initialize the UriParam value
        :param uri: the uri value to wrap
        """"""
        self._uri = uri

    def __repr__(self):
        """"""
        The official string representation for the URI
        :return: the string representation for the URI
        """"""
        return self._uri","class UriParam:
    
    def __init__(self, uri):
        '''
        Initialize the UriParam value
        :param uri: the uri value to wrap
        '''
        pass
    def __repr__(self):
        '''
        The official string representation for the URI
        :return: the string representation for the URI
        '''
        pass",snippet_199,3,"class UriParam:
    
    def __init__(self, uri):
        '''
        Initialize the UriParam value
        :param uri: the uri value to wrap
        '''
        if uri is None:
            raise ValueError(""uri cannot be None"")
        self._uri = str(uri)

    def __repr__(self):
        '''
        The official string representation for the URI
        :return: the string representation for the URI
        '''
        return self._uri"
282529,quora/qcore,quora_qcore/qcore/events.py,qcore.events.EventInterceptor,"class EventInterceptor:
    """"""A context object helping to temporarily intercept
    a set of events on an object exposing a set of event hooks.

    """"""

    def __init__(self, source, **events):
        """"""
        Constructor.

        :param source: the object exposing a set of event hook properies
        :param events: a set of event_hook_name=event_handler pairs specifying
                       which events to intercept.
        """"""
        self.source = source
        self.events = events

    def __enter__(self):
        """"""Starts event interception.""""""
        source = self.source
        for name, handler in self.events.items():
            hook = getattr(source, name)
            hook.subscribe(handler)

    def __exit__(self, typ, value, traceback):
        """"""Stops event interception.""""""
        source = self.source
        for name, handler in self.events.items():
            hook = getattr(source, name)
            hook.unsubscribe(handler)","class EventInterceptor:
    
    def __init__(self, source, **events):
        '''
        Constructor.
        :param source: the object exposing a set of event hook properies
        :param events: a set of event_hook_name=event_handler pairs specifying
                       which events to intercept.
        '''
        pass
    def __enter__(self):
        
        pass
    def __exit__(self, typ, value, traceback):
        
        pass",snippet_200,4,"class EventInterceptor:
    
    def __init__(self, source, **events):
        '''
        Constructor.
        :param source: the object exposing a set of event hook properies
        :param events: a set of event_hook_name=event_handler pairs specifying
                       which events to intercept.
        '''
        self._source = source
        self._events_map = {}
        self._attached = []
        for name, handlers in events.items():
            if handlers is None:
                continue
            if isinstance(handlers, (list, tuple, set)):
                flat = [h for h in handlers if h is not None]
            else:
                flat = [handlers]
            if flat:
                self._events_map[name] = flat

    def __enter__(self):
        for name, handlers in self._events_map.items():
            hook = getattr(self._source, name)
            for handler in handlers:
                self._attach(hook, handler)
                self._attached.append((hook, handler))
        return self
        
    def __exit__(self, typ, value, traceback):
        # Detach in reverse order of attachment
        for hook, handler in reversed(self._attached):
            try:
                self._detach(hook, handler)
            except Exception:
                pass
        self._attached.clear()
        return False

    @staticmethod
    def _attach(hook, handler):
        try:
            hook += handler
            return
        except Exception:
            pass
        # Try common APIs
        for method in (""connect"", ""add"", ""append"", ""subscribe"", ""attach""):
            m = getattr(hook, method, None)
            if callable(m):
                m(handler)
                return
        raise TypeError(""Unsupported event hook type for attaching handler"")

    @staticmethod
    def _detach(hook, handler):
        try:
            hook -= handler
            return
        except Exception:
            pass
        # Try common APIs
        for method in (""disconnect"", ""remove"", ""unsubscribe"", ""detach"", ""discard""):
            m = getattr(hook, method, None)
            if callable(m):
                try:
                    m(handler)
                except TypeError:
                    # Some remove-like methods don't take an argument
                    m()
                return
        raise TypeError(""Unsupported event hook type for detaching handler"")"
284811,empymod/empymod,empymod/filters.py,empymod.filters.DigitalFilter,"import numpy as np
import os

class DigitalFilter:
    """"""Simple Class for Digital Linear Filters.


    Parameters
    ----------
    name : str
        Name of the DFL.

    savename = str
        Name with which the filter is saved. If None (default) it is set to the
        same value as `name`.

    filter_coeff = list of str
        By default, the following filter coefficients are checked:

            ``filter_coeff = ['j0', 'j1', 'sin', 'cos']``

        This accounts for the standard Hankel and Fourier DLF in CSEM
        modelling. However, additional coefficient names can be provided via
        this parameter (in list format).

    """"""

    def __init__(self, name, savename=None, filter_coeff=None):
        """"""Add filter name.""""""
        self.name = name
        if savename is None:
            self.savename = name
        else:
            self.savename = savename
        self.filter_coeff = ['j0', 'j1', 'sin', 'cos']
        if filter_coeff is not None:
            self.filter_coeff.extend(filter_coeff)

    def tofile(self, path='filters'):
        """"""Save filter values to ASCII-files.

        Store the filter base and the filter coefficients in separate files
        in the directory `path`; `path` can be a relative or absolute path.

        Examples
        --------
        >>> import empymod
        >>> # Load a filter
        >>> filt = empymod.filters.Hankel().wer_201_2018
        >>> # Save it to pure ASCII-files
        >>> filt.tofile()
        >>> # This will save the following three files:
        >>> #    ./filters/wer_201_2018_base.txt
        >>> #    ./filters/wer_201_2018_j0.txt
        >>> #    ./filters/wer_201_2018_j1.txt

        """"""
        name = self.savename
        path = os.path.abspath(path)
        os.makedirs(path, exist_ok=True)
        basefile = os.path.join(path, name + '_base.txt')
        with open(basefile, 'w') as f:
            self.base.tofile(f, sep='\n')
        for val in self.filter_coeff:
            if hasattr(self, val):
                attrfile = os.path.join(path, name + '_' + val + '.txt')
                with open(attrfile, 'w') as f:
                    getattr(self, val).tofile(f, sep='\n')

    def fromfile(self, path='filters'):
        """"""Load filter values from ASCII-files.

        Load filter base and filter coefficients from ASCII files in the
        directory `path`; `path` can be a relative or absolute path.

        Examples
        --------
        >>> import empymod
        >>> # Create an empty filter;
        >>> # Name has to be the base of the text files
        >>> filt = empymod.filters.DigitalFilter('my-filter')
        >>> # Load the ASCII-files
        >>> filt.fromfile()
        >>> # This will load the following three files:
        >>> #    ./filters/my-filter_base.txt
        >>> #    ./filters/my-filter_j0.txt
        >>> #    ./filters/my-filter_j1.txt
        >>> # and store them in filt.base, filt.j0, and filt.j1.

        """"""
        name = self.savename
        path = os.path.abspath(path)
        basefile = os.path.join(path, name + '_base.txt')
        with open(basefile, 'r') as f:
            self.base = np.fromfile(f, sep='\n')
        for val in self.filter_coeff:
            attrfile = os.path.join(path, name + '_' + val + '.txt')
            if os.path.isfile(attrfile):
                with open(attrfile, 'r') as f:
                    setattr(self, val, np.fromfile(f, sep='\n'))
        self.factor = np.around([self.base[1] / self.base[0]], 15)","
class DigitalFilter:
    '''Simple Class for Digital Linear Filters.
    Parameters
    ----------
    name : str
        Name of the DFL.
    savename = str
        Name with which the filter is saved. If None (default) it is set to the
        same value as `name`.
    filter_coeff = list of str
        By default, the following filter coefficients are checked:
            ``filter_coeff = ['j0', 'j1', 'sin', 'cos']``
        This accounts for the standard Hankel and Fourier DLF in CSEM
        modelling. However, additional coefficient names can be provided via
        this parameter (in list format).
    '''
    def __init__(self, name, savename=None, filter_coeff=None):
        
        pass
    def tofile(self, path='filters'):
        '''Save filter values to ASCII-files.
        Store the filter base and the filter coefficients in separate files
        in the directory `path`; `path` can be a relative or absolute path.
        Examples
        --------
        >>> import empymod
        >>> # Load a filter
        >>> filt = empymod.filters.Hankel().wer_201_2018
        >>> # Save it to pure ASCII-files
        >>> filt.tofile()
        >>> # This will save the following three files:
        >>> #    ./filters/wer_201_2018_base.txt
        >>> #    ./filters/wer_201_2018_j0.txt
        >>> #    ./filters/wer_201_2018_j1.txt
        '''
        pass
    def fromfile(self, path='filters'):
        '''Load filter values from ASCII-files.
        Load filter base and filter coefficients from ASCII files in the
        directory `path`; `path` can be a relative or absolute path.
        Examples
        --------
        >>> import empymod
        >>> # Create an empty filter;
        >>> # Name has to be the base of the text files
        >>> filt = empymod.filters.DigitalFilter('my-filter')
        >>> # Load the ASCII-files
        >>> filt.fromfile()
        >>> # This will load the following three files:
        >>> #    ./filters/my-filter_base.txt
        >>> #    ./filters/my-filter_j0.txt
        >>> #    ./filters/my-filter_j1.txt
        >>> # and store them in filt.base, filt.j0, and filt.j1.
        '''
        pass",snippet_201,4,"class DigitalFilter:
    '''Simple Class for Digital Linear Filters.
    Parameters
    ----------
    name : str
        Name of the DFL.
    savename = str
        Name with which the filter is saved. If None (default) it is set to the
        same value as `name`.
    filter_coeff = list of str
        By default, the following filter coefficients are checked:
            ``filter_coeff = ['j0', 'j1', 'sin', 'cos']``
        This accounts for the standard Hankel and Fourier DLF in CSEM
        modelling. However, additional coefficient names can be provided via
        this parameter (in list format).
    '''
    def __init__(self, name, savename=None, filter_coeff=None):
        self.name = str(name)
        self.savename = str(savename) if savename is not None else self.name
        if filter_coeff is None:
            filter_coeff = ['j0', 'j1', 'sin', 'cos']
        self.filter_coeff = list(filter_coeff)
        self.base = None
        # Initialize coefficient attributes if not already set
        for coeff in self.filter_coeff:
            if not hasattr(self, coeff):
                setattr(self, coeff, None)

    def _ensure_dir(self, path):
        import os
        if not os.path.isdir(path):
            os.makedirs(path, exist_ok=True)

    def _coerce_iterable_of_numbers(self, data, name):
        if data is None:
            raise ValueError(f""{name} is None; set it before saving."")
        try:
            iterable = list(data)
        except TypeError:
            raise TypeError(f""{name} must be an iterable of numbers."")
        out = []
        for i, v in enumerate(iterable):
            try:
                out.append(float(v))
            except Exception as e:
                raise ValueError(f""{name}[{i}]='{v}' is not a number."") from e
        if len(out) == 0:
            raise ValueError(f""{name} is empty."")
        return out

    def _write_vector(self, filepath, vec):
        # Write one value per line with high precision
        with open(filepath, 'w', encoding='utf-8') as f:
            for v in vec:
                f.write(f""{v:.18e}\n"")

    def _read_vector(self, filepath):
        vals = []
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                s = line.strip()
                if not s or s.startswith('#'):
                    continue
                vals.append(float(s))
        if not vals:
            raise ValueError(f""File '{filepath}' is empty."")
        return vals

    def tofile(self, path='filters'):
        '''Save filter values to ASCII-files.
        Store the filter base and the filter coefficients in separate files
        in the directory `path`; `path` can be a relative or absolute path.
        Examples
        --------
        >>> import empymod
        >>> # Load a filter
        >>> filt = empymod.filters.Hankel().wer_201_2018
        >>> # Save it to pure ASCII-files
        >>> filt.tofile()
        >>> # This will save the following three files:
        >>> #    ./filters/wer_201_2018_base.txt
        >>> #    ./filters/wer_201_2018_j0.txt
        >>> #    ./filters/wer_201_2018_j1.txt
        '''
        import os
        self._ensure_dir(path)
        base_vec = self._coerce_iterable_of_numbers(self.base, ""base"")
        base_file = os.path.join(path, f""{self.savename}_base.txt"")
        self._write_vector(base_file, base_vec)
        for coeff in self.filter_coeff:
            data = getattr(self, coeff, None)
            if data is None:
                continue
            vec = self._coerce_iterable_of_numbers(data, coeff)
            coeff_file = os.path.join(path, f""{self.savename}_{coeff}.txt"")
            self._write_vector(coeff_file, vec)
        return self

    def fromfile(self, path='filters'):
        '''Load filter values from ASCII-files.
        Load filter base and filter coefficients from ASCII files in the
        directory `path`; `path` can be a relative or absolute path.
        Examples
        --------
        >>> import empymod
        >>> # Create an empty filter;
        >>> # Name has to be the base of the text files
        >>> filt = empymod.filters.DigitalFilter('my-filter')
        >>> # Load the ASCII-files
        >>> filt.fromfile()
        >>> # This will load the following three files:
        >>> #    ./filters/my-filter_base.txt
        >>> #    ./filters/my-filter_j0.txt
        >>> #    ./filters/my-filter_j1.txt
        >>> # and store them in filt.base, filt.j0, and filt.j1.
        '''
        import os
        base_file = os.path.join(path, f""{self.savename}_base.txt"")
        if not os.path.isfile(base_file):
            raise FileNotFoundError(f""Base file not found: '{base_file}'"")
        self.base = self._read_vector(base_file)
        for coeff in self.filter_coeff:
            coeff_file = os.path.join(path, f""{self.savename}_{coeff}.txt"")
            if os.path.isfile(coeff_file):
                setattr(self, coeff, self._read_vector(coeff_file))
            else:
                setattr(self, coeff, None)
        return self"
285266,Samreay/ChainConsumer,src/chainconsumer/kde.py,chainconsumer.kde.MegKDE,"import numpy as np
from scipy import spatial

class MegKDE:
    """"""Matched Elliptical Gaussian Kernel Density Estimator

    Adapted from the algorithm specified in the BAMBIS's model specified Wolf 2017
    to support weighted samples.
    """"""

    def __init__(self, train: np.ndarray, weights: np.ndarray | None=None, truncation: float=3.0, nmin: int=4, factor: float=1.0):
        """"""
        Args:
            train (np.ndarray): The training data set. Should be a 1D array of samples or a 2D array
                of shape (n_samples, n_dim).
            weights (np.ndarray, optional): An array of weights. If not specified, equal weights are assumed.
            truncation (float, optional): The maximum deviation (in sigma) to use points in the KDE
            nmin (int, optional): The minimum number of points required to estimate the density
            factor (float, optional): Send bandwidth to this factor of the data estimate
        """"""
        self.truncation = truncation
        self.nmin = nmin
        self.train = train
        if len(train.shape) == 1:
            train = np.atleast_2d(train).T
        self.num_points, self.num_dim = train.shape
        if weights is None:
            weights = np.ones(self.num_points)
        self.weights = weights
        self.mean = np.average(train, weights=weights, axis=0)
        dx = train - self.mean
        cov = np.atleast_2d(np.cov(dx.T, aweights=weights))
        self.A = np.linalg.cholesky(np.linalg.inv(cov))
        self.d = np.dot(dx, self.A)
        self.tree = spatial.cKDTree(self.d)
        self.sigma = 2.0 * factor * np.power(self.num_points, -1.0 / (4 + self.num_dim))
        self.sigma_fact = -0.5 / (self.sigma * self.sigma)

    def evaluate(self, data: np.ndarray) -> np.ndarray:
        """"""Estimate un-normalised probability density at target points

        Args:
            data (np.ndarray): 2D array of shape (n_samples, n_dim).

        Returns:
            np.ndarray: A `(n_samples)` length array of estimates

        """"""
        if len(data.shape) == 1 and self.num_dim == 1:
            data = np.atleast_2d(data).T
        _d = np.dot(data - self.mean, self.A)
        neighbors = self.tree.query_ball_point(_d, self.sigma * self.truncation)
        out = []
        for i, n in enumerate(neighbors):
            if len(n) >= self.nmin:
                diff = self.d[n, :] - _d[i]
                distsq = np.sum(diff * diff, axis=1)
            else:
                dist, n = self.tree.query(_d[i], k=self.nmin)
                distsq = dist * dist
            out.append(np.sum(self.weights[n] * np.exp(self.sigma_fact * distsq)))
        return np.array(out)","
class MegKDE:
    '''Matched Elliptical Gaussian Kernel Density Estimator
    Adapted from the algorithm specified in the BAMBIS's model specified Wolf 2017
    to support weighted samples.
    '''
    def __init__(self, train: np.ndarray, weights: np.ndarray | None=None, truncation: float=3.0, nmin: int=4, factor: float=1.0):
        '''
        Args:
            train (np.ndarray): The training data set. Should be a 1D array of samples or a 2D array
                of shape (n_samples, n_dim).
            weights (np.ndarray, optional): An array of weights. If not specified, equal weights are assumed.
            truncation (float, optional): The maximum deviation (in sigma) to use points in the KDE
            nmin (int, optional): The minimum number of points required to estimate the density
            factor (float, optional): Send bandwidth to this factor of the data estimate
        '''
        pass
    def evaluate(self, data: np.ndarray) -> np.ndarray:
        
        pass",snippet_203,3,"import numpy as np

class MegKDE:
    '''Matched Elliptical Gaussian Kernel Density Estimator
    Adapted from the algorithm specified in the BAMBIS's model specified Wolf 2017
    to support weighted samples.
    '''
    def __init__(self, train: np.ndarray, weights: np.ndarray | None=None, truncation: float=3.0, nmin: int=4, factor: float=1.0):
        '''
        Args:
            train (np.ndarray): The training data set. Should be a 1D array of samples or a 2D array
                of shape (n_samples, n_dim).
            weights (np.ndarray, optional): An array of weights. If not specified, equal weights are assumed.
            truncation (float, optional): The maximum deviation (in sigma) to use points in the KDE
            nmin (int, optional): The minimum number of points required to estimate the density
            factor (float, optional): Send bandwidth to this factor of the data estimate
        '''
        if train is None:
            raise ValueError(""train must be provided"")
        X = np.asarray(train, dtype=np.float64)
        if X.ndim == 1:
            X = X.reshape(-1, 1)
        if X.ndim != 2:
            raise ValueError(""train must be a 1D or 2D array"")
        n, d = X.shape
        if n < 2:
            raise ValueError(""at least two training samples are required"")
        if factor <= 0:
            raise ValueError(""factor must be > 0"")
        if nmin < 1:
            raise ValueError(""nmin must be >= 1"")
        if truncation <= 0:
            raise ValueError(""truncation must be > 0"")

        if weights is None:
            w = np.full(n, 1.0 / n, dtype=np.float64)
        else:
            w = np.asarray(weights, dtype=np.float64).reshape(-1)
            if w.shape[0] != n:
                raise ValueError(""weights must have the same length as the number of samples"")
            if np.any(w < 0):
                raise ValueError(""weights must be non-negative"")
            sw = float(np.sum(w))
            if not np.isfinite(sw) or sw <= 0:
                raise ValueError(""sum of weights must be positive and finite"")
            w = w / sw

        mu = np.sum(X * w[:, None], axis=0)
        Xm = X - mu
        # Weighted covariance (normalized to sum(weights)=1). Add small ridge for numerical stability.
        cov = Xm.T @ (Xm * w[:, None])
        # Ensure positive definiteness via ridge
        eps = 1e-12
        trace = np.trace(cov)
        if not np.isfinite(trace) or trace <= 0:
            # Fallback to identity scaled small if degenerate
            cov = np.eye(d, dtype=np.float64) * 1e-6
        else:
            cov = cov + np.eye(d, dtype=np.float64) * (eps * trace / d)

        # Cholesky for covariance
        try:
            chol_cov = np.linalg.cholesky(cov)
        except np.linalg.LinAlgError:
            # If not PD, add more ridge
            ridge = (1e-8 if trace <= 0 else 1e-8 * trace / d)
            cov = cov + np.eye(d, dtype=np.float64) * ridge
            chol_cov = np.linalg.cholesky(cov)
        inv_cov = np.linalg.solve(chol_cov.T, np.linalg.solve(chol_cov, np.eye(d)))
        log_det_cov = 2.0 * np.sum(np.log(np.diag(chol_cov)))

        # Bandwidth matrix H = factor^2 * cov
        self.factor = float(factor)
        self.inv_H = inv_cov / (self.factor ** 2)
        self.log_det_H = log_det_cov + 2.0 * d * np.log(self.factor)
        self.dim = d
        self.truncation = float(truncation)
        self.nmin = int(nmin)

        self.X = X
        self.w = w
        self.n = n

        # Precompute Gaussian normalizing constant for kernel with covariance H
        self._log_norm_const = -0.5 * (self.dim * np.log(2.0 * np.pi) + self.log_det_H)

    def evaluate(self, data: np.ndarray) -> np.ndarray:
        Y = np.asarray(data, dtype=np.float64)
        if Y.ndim == 1:
            Y = Y.reshape(-1, self.dim if self.dim != 1 else 1)
        if Y.ndim != 2:
            raise ValueError(""data must be a 1D or 2D array"")
        if Y.shape[1] != self.dim:
            raise ValueError(f""data dimension mismatch: expected {self.dim}, got {Y.shape[1]}"")

        m = Y.shape[0]
        dens = np.empty(m, dtype=np.float64)

        invH = self.inv_H
        log_norm_const = self._log_norm_const
        trunc2 = (self.truncation ** 2)

        X = self.X
        w = self.w

        for i in range(m):
            y = Y[i]
            delta = X - y  # (n, d)
            # Mahalanobis squared distance under H
            q = delta @ invH
            d2 = np.einsum(""ij,ij->i"", q, delta)

            # Select indices within truncation; ensure at least nmin using nearest distances
            idx = np.where(d2 <= trunc2)[0]
            if idx.size < self.nmin:
                # take nmin nearest
                nearest = np.argpartition(d2, self.nmin - 1)[:self.nmin]
                idx = np.unique(np.concatenate((idx, nearest), axis=0))

            sel_delta = delta[idx]
            sel_w = w[idx]

            # Compute kernel contributions
            q_sel = sel_delta @ invH
            exparg = -0.5 * np.einsum(""ij,ij->i"", q_sel, sel_delta)
            # Renormalize weights over selected subset to mitigate truncation bias
            wsum = np.sum(sel_w)
            if wsum <= 0 or not np.isfinite(wsum):
                dens[i] = 0.0
                continue
            contrib = np.exp(exparg) * (sel_w / wsum)
            dens[i] = np.exp(log_norm_const) * np.sum(contrib)

        return dens"
285844,4Catalyzer/flask-resty,4Catalyzer_flask-resty/flask_resty/authentication.py,flask_resty.authentication.AuthenticationBase,"class AuthenticationBase:
    """"""Base class for API authentication components.

    Authentication components are responsible for extracting the request
    credentials, if any. They should raise a 401 if the credentials are
    invalid, but should provide `None` for unauthenticated users.

    Flask-RESTy provides an implementation using `JSON Web Tokens`_  but you
    can use any authentication component by extending
    :py:class:`AuthenticationBase` and implementing
    :py:meth:`get_request_credentials`.

    .. _JSON Web Tokens: https://jwt.io/
    """"""

    def authenticate_request(self):
        """"""Store the request credentials in the
        :py:class:`flask.ctx.AppContext`.

        .. warning::

            No validation is performed by Flask-RESTy. It is up to the
            implementor to validate the request in
            :py:meth:`get_request_credentials`.
        """"""
        set_request_credentials(self.get_request_credentials())

    def get_request_credentials(self):
        """"""Get the credentials for the current request.

        Typically this is done by inspecting :py:data:`flask.request`.

        .. warning::

            Implementing classes **must** raise an exception on authentication
            failure. A 401 Unauthorized :py:class:`ApiError` is recommended.

        :return: The credentials for the current request.
        """"""
        raise NotImplementedError()","class AuthenticationBase:
    
    def authenticate_request(self):
        '''Store the request credentials in the
        :py:class:`flask.ctx.AppContext`.
        .. warning::
            No validation is performed by Flask-RESTy. It is up to the
            implementor to validate the request in
            :py:meth:`get_request_credentials`.
        '''
        pass
    def get_request_credentials(self):
        
        pass",snippet_204,3,"from flask import g

class AuthenticationBase:
    CONTEXT_KEY = ""auth_credentials""

    def authenticate_request(self):
        '''Store the request credentials in the
        :py:class:`flask.ctx.AppContext`.
        .. warning::
            No validation is performed by Flask-RESTy. It is up to the
            implementor to validate the request in
            :py:meth:`get_request_credentials`.
        '''
        creds = self.get_request_credentials()
        setattr(g, self.CONTEXT_KEY, creds)
        return creds

    def get_request_credentials(self):
        raise NotImplementedError(""Subclasses must implement get_request_credentials()"")"
285856,4Catalyzer/flask-resty,4Catalyzer_flask-resty/flask_resty/filtering.py,flask_resty.filtering.ArgFilterBase,"class ArgFilterBase:
    """"""An abstract specification of a filter from a query argument.

    Implementing classes must provide :py:meth:`maybe_set_arg_name` and
    :py:meth:`filter_query`.
    """"""

    def maybe_set_arg_name(self, arg_name):
        """"""Set the name of the argument to which this filter is bound.

        :param str arg_name: The name of the field to filter against.
        :raises: :py:class:`NotImplementedError` if no implementation is
            provided.
        """"""
        raise NotImplementedError()

    def filter_query(self, query, view, arg_value):
        """"""Filter the query.

        :param query: The query to filter.
        :type query: :py:class:`sqlalchemy.orm.query.Query`
        :param view: The view with the model we wish to filter for.
        :type view: :py:class:`ModelView`
        :param str arg_value: The filter specification
        :return: The filtered query
        :rtype: :py:class:`sqlalchemy.orm.query.Query`
        :raises: :py:class:`NotImplementedError` if no implementation is
            provided.
        """"""
        raise NotImplementedError()","class ArgFilterBase:
    '''An abstract specification of a filter from a query argument.
    Implementing classes must provide :py:meth:`maybe_set_arg_name` and
    :py:meth:`filter_query`.
    '''
    def maybe_set_arg_name(self, arg_name):
        
        pass
    def filter_query(self, query, view, arg_value):
        '''Filter the query.
        :param query: The query to filter.
        :type query: :py:class:`sqlalchemy.orm.query.Query`
        :param view: The view with the model we wish to filter for.
        :type view: :py:class:`ModelView`
        :param str arg_value: The filter specification
        :return: The filtered query
        :rtype: :py:class:`sqlalchemy.orm.query.Query`
        :raises: :py:class:`NotImplementedError` if no implementation is
            provided.
        '''
        pass",snippet_205,3,"from abc import ABC, abstractmethod

class ArgFilterBase(ABC):
    '''An abstract specification of a filter from a query argument.
    Implementing classes must provide :py:meth:`maybe_set_arg_name` and
    :py:meth:`filter_query`.
    '''
    @abstractmethod
    def maybe_set_arg_name(self, arg_name):
        raise NotImplementedError

    @abstractmethod
    def filter_query(self, query, view, arg_value):
        '''Filter the query.
        :param query: The query to filter.
        :type query: :py:class:`sqlalchemy.orm.query.Query`
        :param view: The view with the model we wish to filter for.
        :type view: :py:class:`ModelView`
        :param str arg_value: The filter specification
        :return: The filtered query
        :rtype: :py:class:`sqlalchemy.orm.query.Query`
        :raises: :py:class:`NotImplementedError` if no implementation is
            provided.
        '''
        raise NotImplementedError"
285958,tcalmant/python-javaobj,tcalmant_python-javaobj/javaobj/constants.py,javaobj.constants.StreamCodeDebug,"class StreamCodeDebug:
    """"""
    Codes utility methods
    """"""

    @staticmethod
    def op_id(op_id):
        """"""
        Returns the name of the given OP Code
        :param op_id: OP Code
        :return: Name of the OP Code
        """"""
        try:
            return TerminalCode(op_id).name
        except ValueError:
            return '<unknown TC:{0}>'.format(op_id)

    @staticmethod
    def type_code(type_id):
        """"""
        Returns the name of the given Type Code
        :param type_id: Type code
        :return: Name of the type code
        """"""
        try:
            return TypeCode(type_id).name
        except ValueError:
            return '<unknown TypeCode:{0}>'.format(type_id)

    @staticmethod
    def flags(flags):
        """"""
        Returns the names of the class description flags found in the given
        integer

        :param flags: A class description flag entry
        :return: The flags names as a single string
        """"""
        names = sorted((key.name for key in ClassDescFlags if key & flags))
        return ', '.join(names)","class StreamCodeDebug:
    
    @staticmethod
    def op_id(op_id):
        
        pass
    @staticmethod
    def type_code(type_id):
        '''
        Returns the name of the given Type Code
        :param type_id: Type code
        :return: Name of the type code
        '''
        pass
    @staticmethod
    def flags(flags):
        '''
        Returns the names of the class description flags found in the given
        integer
        :param flags: A class description flag entry
        :return: The flags names as a single string
        '''
        pass",snippet_206,4,"class StreamCodeDebug:
    
    _TYPE_CODES = {
        0x70: ""TC_NULL"",
        0x71: ""TC_REFERENCE"",
        0x72: ""TC_CLASSDESC"",
        0x73: ""TC_OBJECT"",
        0x74: ""TC_STRING"",
        0x75: ""TC_ARRAY"",
        0x76: ""TC_CLASS"",
        0x77: ""TC_BLOCKDATA"",
        0x78: ""TC_ENDBLOCKDATA"",
        0x79: ""TC_RESET"",
        0x7A: ""TC_BLOCKDATALONG"",
        0x7B: ""TC_EXCEPTION"",
        0x7C: ""TC_LONGSTRING"",
        0x7D: ""TC_PROXYCLASSDESC"",
        0x7E: ""TC_ENUM"",
    }

    _FLAG_BITS = {
        0x01: ""SC_WRITE_METHOD"",
        0x02: ""SC_SERIALIZABLE"",
        0x04: ""SC_EXTERNALIZABLE"",
        0x08: ""SC_BLOCK_DATA"",
        0x10: ""SC_ENUM"",
    }

    @staticmethod
    def op_id(op_id):
        try:
            if op_id in StreamCodeDebug._TYPE_CODES:
                return StreamCodeDebug._TYPE_CODES[op_id]
            return f""UNKNOWN_OP(0x{op_id:02X})""
        except Exception:
            return ""UNKNOWN_OP""

    @staticmethod
    def type_code(type_id):
        '''
        Returns the name of the given Type Code
        :param type_id: Type code
        :return: Name of the type code
        '''
        try:
            return StreamCodeDebug._TYPE_CODES.get(type_id, f""UNKNOWN_TC(0x{type_id:02X})"")
        except Exception:
            return ""UNKNOWN_TC""

    @staticmethod
    def flags(flags):
        '''
        Returns the names of the class description flags found in the given
        integer
        :param flags: A class description flag entry
        :return: The flags names as a single string
        '''
        try:
            names = []
            remaining = flags
            for bit, name in sorted(StreamCodeDebug._FLAG_BITS.items()):
                if flags & bit:
                    names.append(name)
                    remaining &= ~bit
            if remaining:
                names.append(f""UNKNOWN_FLAGS(0x{remaining:02X})"")
            return "" | "".join(names) if names else """"
        except Exception:
            return """""
285972,tcalmant/python-javaobj,tcalmant_python-javaobj/javaobj/v2/api.py,javaobj.v2.api.IJavaStreamParser,"class IJavaStreamParser:
    """"""
    API of the Java stream parser
    """"""

    def run(self):
        """"""
        Parses the input stream
        """"""
        raise NotImplementedError

    def dump(self, content):
        """"""
        Dumps to a string the given objects
        """"""
        raise NotImplementedError

    def _read_content(self, type_code, block_data, class_desc=None):
        """"""
        Parses the next content. Use with care (use only in a transformer)
        """"""","class IJavaStreamParser:
    '''
    API of the Java stream parser
    '''
    def run(self):
        '''
        Parses the input stream
        '''
        pass
    def dump(self, content):
        '''
        Dumps to a string the given objects
        '''
        pass
    def _read_content(self, type_code, block_data, class_desc=None):
        
        pass",snippet_207,4,"class IJavaStreamParser:
    '''
    API of the Java stream parser
    '''
    def run(self):
        '''
        Parses the input stream
        '''
        raise NotImplementedError(""Subclasses must implement 'run'"")

    def dump(self, content):
        '''
        Dumps to a string the given objects
        '''
        raise NotImplementedError(""Subclasses must implement 'dump'"")

    def _read_content(self, type_code, block_data, class_desc=None):
        raise NotImplementedError(""Subclasses must implement '_read_content'"")"
287233,materialsvirtuallab/monty,materialsvirtuallab_monty/src/monty/design_patterns.py,monty.design_patterns.NullFile,"import os

class NullFile:
    """"""A file object that is associated to /dev/null.""""""

    def __new__(cls):
        """"""
        Pass through.
        """"""
        return open(os.devnull, 'w')

    def __init__(self):
        """"""no-op""""""","
class NullFile:
    '''A file object that is associated to /dev/null.'''
    def __new__(cls):
        
        pass
    def __init__(self):
        '''no-op'''
        pass",snippet_208,3,"import os

class NullFile:
    '''A file object that is associated to /dev/null.'''
    def __new__(cls):
        return open(os.devnull, 'w+b')
    def __init__(self):
        '''no-op'''
        pass"
287394,common-workflow-language/schema_salad,common-workflow-language_schema_salad/schema_salad/cpp_codegen.py,schema_salad.cpp_codegen.ClassDefinition,"from typing import IO, Any, Optional, Union, cast

class ClassDefinition:
    """"""Prototype of a class.""""""

    def __init__(self, name: str):
        """"""Initialize the class definition with a name.""""""
        self.fullName = name
        self.extends: list[dict[str, str]] = []
        self.specializationTypes: list[str] = []
        self.allfields: list[FieldDefinition] = []
        self.fields: list[FieldDefinition] = []
        self.abstract = False
        self.namespace, self.classname = split_name(name)
        self.namespace = safenamespacename(self.namespace)
        self.classname = safename(self.classname)

    def writeFwdDeclaration(self, target: IO[str], fullInd: str, ind: str) -> None:
        """"""Write forward declaration.""""""
        target.write(f'{fullInd}namespace {self.namespace} {{ struct {self.classname}; }}\n')

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, common_namespace: str) -> None:
        """"""Write definition of the class.""""""
        target.write(f'{fullInd}namespace {self.namespace} {{\n')
        target.write(f'{fullInd}struct {self.classname}')
        extends = list(map(safename2, self.extends))
        override = ''
        virtual = 'virtual '
        if len(self.extends) > 0:
            target.write(f'\n{fullInd}{ind}: ')
            target.write(f'\n{fullInd}{ind}, '.join(extends))
            override = ' override'
            virtual = ''
        target.write(' {\n')
        for field in self.fields:
            field.writeDefinition(target, fullInd + ind, ind, self.namespace)
        if self.abstract:
            target.write(f'{fullInd}{ind}virtual ~{self.classname}() = 0;\n')
        else:
            target.write(f'{fullInd}{ind}{virtual}~{self.classname}(){override} = default;\n')
        target.write(f'{fullInd}{ind}{virtual}auto toYaml([[maybe_unused]] {common_namespace}::store_config const& config) const -> YAML::Node{override};\n')
        target.write(f'{fullInd}{ind}{virtual}void fromYaml(YAML::Node const& n){override};\n')
        target.write(f'{fullInd}}};\n')
        target.write(f'{fullInd}}}\n\n')

    def writeImplDefinition(self, target: IO[str], fullInd: str, ind: str, common_namespace: str) -> None:
        """"""Write definition with implementation.""""""
        extends = list(map(safename2, self.extends))
        if self.abstract:
            target.write(f'{fullInd}inline {self.namespace}::{self.classname}::~{self.classname}() = default;\n')
        target.write(f'{fullInd}inline auto {self.namespace}::{self.classname}::toYaml([[maybe_unused]] ::{common_namespace}::store_config const& config) const -> YAML::Node {{\n{fullInd}{ind}using ::{common_namespace}::toYaml;\n{fullInd}{ind}auto n = YAML::Node{{}};\n{fullInd}{ind}if (config.generateTags) {{\n{fullInd}{ind}{ind}n.SetTag(""{self.classname}"");\n{fullInd}{ind}}}\n')
        for e in extends:
            target.write(f'{fullInd}{ind}n = mergeYaml(n, {e}::toYaml(config));\n')
        for field in self.fields:
            fieldname = safename(field.name)
            target.write(f'{fullInd}{ind}{{\n')
            target.write(f'{fullInd}{ind}{ind} auto member = toYaml(*{fieldname}, config);\n')
            if field.typeDSL:
                target.write(f'{fullInd}{ind}{ind} member = simplifyType(member, config);\n')
            target.write(f'{fullInd}{ind}{ind} member = convertListToMap(member, {q(field.mapSubject)}, {q(field.mapPredicate)}, config);\n')
            target.write(f'{fullInd}{ind}{ind}addYamlField(n, {q(field.name)}, member);\n')
            target.write(f'{fullInd}{ind}}}\n')
        target.write(f'{fullInd}{ind}return n;\n{fullInd}}}\n')
        functionname = f'{self.namespace}::{self.classname}::fromYaml'
        target.write(f'{fullInd}inline void {functionname}([[maybe_unused]] YAML::Node const& n) {{\n{fullInd}{ind}using ::{common_namespace}::fromYaml;\n')
        for e in extends:
            target.write(f'{fullInd}{ind}{e}::fromYaml(n);\n')
        for field in self.fields:
            fieldname = safename(field.name)
            expandType = ''
            if field.typeDSL:
                expandType = 'expandType'
            target.write(f'{fullInd}{ind}{{\n{fullInd}{ind}{ind}auto nodeAsList = convertMapToList(n[{q(field.name)}], {q(field.mapSubject)}, {q(field.mapPredicate)});\n{fullInd}{ind}{ind}auto expandedNode = {expandType}(nodeAsList);\n{fullInd}{ind}{ind}fromYaml(expandedNode, *{fieldname});\n{fullInd}{ind}}}\n')
        target.write(f'{fullInd}}}\n')
        if not self.abstract:
            e = f'::{self.namespace}::{self.classname}'
            target.write(f'namespace {common_namespace} {{\ntemplate <>\nstruct DetectAndExtractFromYaml<{e}> {{\n    auto operator()(YAML::Node const& n) const -> std::optional<{e}> {{\n        if (!n.IsDefined()) return std::nullopt;\n        if (!n.IsMap()) return std::nullopt;\n        auto res = {e}{{}};\n\n')
            for field in self.fields:
                fieldname = safename(field.name)
                target.write(f'        if constexpr (::{common_namespace}::IsConstant<decltype(res.{fieldname})::value_t>::value) try {{\n            fromYaml(n[{q(field.name)}], *res.{fieldname});\n            fromYaml(n, res);\n            return res;\n        }} catch(...) {{}}\n\n')
            target.write('        return std::nullopt;\n    }\n};\n}\n')","
class ClassDefinition:
    
    def __init__(self, name: str):
        '''Initialize the class definition with a name.'''
        pass
    def writeFwdDeclaration(self, target: IO[str], fullInd: str, ind: str) -> None:
        '''Write forward declaration.'''
        pass
    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, common_namespace: str) -> None:
        '''Write definition of the class.'''
        pass
    def writeImplDefinition(self, target: IO[str], fullInd: str, ind: str, common_namespace: str) -> None:
        '''Write definition with implementation.'''
        pass",snippet_209,5,"from typing import IO, Any, List


class ClassDefinition:
    def __init__(self, name: str):
        '''Initialize the class definition with a name.'''
        if not isinstance(name, str) or not name:
            raise ValueError(""name must be a non-empty string"")
        self.name = name

    def writeFwdDeclaration(self, target: IO[str], fullInd: str, ind: str) -> None:
        '''Write forward declaration.'''
        target.write(f""{fullInd}class {self.name};\n"")

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, common_namespace: str) -> None:
        '''Write definition of the class.'''
        cur_ind = fullInd
        namespaces = self._split_namespaces(common_namespace)
        cur_ind = self._open_namespaces(target, cur_ind, ind, namespaces)

        # Class definition
        target.write(f""{cur_ind}class {self.name} {{\n"")
        target.write(f""{cur_ind}{ind}public:\n"")
        target.write(f""{cur_ind}}};\n"")

        self._close_namespaces(target, fullInd, ind, namespaces)

    def writeImplDefinition(self, target: IO[str], fullInd: str, ind: str, common_namespace: str) -> None:
        '''Write definition with implementation.'''
        cur_ind = fullInd
        namespaces = self._split_namespaces(common_namespace)
        cur_ind = self._open_namespaces(target, cur_ind, ind, namespaces)

        # Class definition with default implementations
        target.write(f""{cur_ind}class {self.name} {{\n"")
        target.write(f""{cur_ind}{ind}public:\n"")
        target.write(f""{cur_ind}{ind}{ind}{self.name}() = default;\n"")
        target.write(f""{cur_ind}{ind}{ind}~{self.name}() = default;\n"")
        target.write(f""{cur_ind}}};\n"")

        self._close_namespaces(target, fullInd, ind, namespaces)

    @staticmethod
    def _split_namespaces(ns: str) -> List[str]:
        if not ns:
            return []
        parts = [p.strip() for p in ns.split(""::"") if p.strip()]
        return parts

    @staticmethod
    def _open_namespaces(target: IO[str], fullInd: str, ind: str, namespaces: List[str]) -> str:
        cur_ind = fullInd
        for ns in namespaces:
            target.write(f""{cur_ind}namespace {ns} {{\n"")
            cur_ind += ind
        return cur_ind

    @staticmethod
    def _close_namespaces(target: IO[str], fullInd: str, ind: str, namespaces: List[str]) -> None:
        # Close in reverse order
        cur_ind = fullInd + ind * len(namespaces)
        for ns in reversed(namespaces):
            cur_ind = cur_ind[:-len(ind)]
            target.write(f""{cur_ind}}} // namespace {ns}\n"")"
287396,common-workflow-language/schema_salad,common-workflow-language_schema_salad/schema_salad/cpp_codegen.py,schema_salad.cpp_codegen.EnumDefinition,"from typing import IO, Any, Optional, Union, cast

class EnumDefinition:
    """"""Prototype of a enum.""""""

    def __init__(self, name: str, values: list[str]):
        """"""Initialize enum definition with a name and possible values.""""""
        self.name = name
        self.values = values
        self.namespace, self.classname = split_name(name)
        self.namespace = safenamespacename(self.namespace)
        self.classname = safename(self.classname)

    def writeDefinition(self, target: IO[str], ind: str, common_namespace: str) -> None:
        """"""Write enum definition to output.""""""
        namespace = ''
        if len(self.name.split('#')) == 2:
            namespace, classname = split_name(self.name)
            namespace = safenamespacename(namespace)
            classname = safename(classname)
            name = namespace + '::' + classname
        else:
            name = safename(self.name)
            classname = name
        if len(namespace) > 0:
            target.write(f'namespace {namespace} {{\n')
        target.write(f'enum class {classname} : unsigned int {{\n{ind}')
        target.write(f',\n{ind}'.join(map(safename, self.values)))
        target.write('\n};\n')
        target.write(f'inline auto to_string({classname} v) {{\n')
        target.write(f'{ind}static auto m = std::vector<std::string_view> {{\n')
        target.write(f'{ind}    ""')
        target.write(f'"",\n{ind}    ""'.join(self.values))
        target.write(f'""\n{ind}}};\n')
        target.write(f'{ind}using U = std::underlying_type_t<{name}>;\n')
        target.write(f'{ind}return m.at(static_cast<U>(v));\n}}\n')
        if len(namespace) > 0:
            target.write('}\n')
        target.write(f'inline void to_enum(std::string_view v, {name}& out) {{\n')
        target.write(f'{ind}static auto m = std::map<std::string, {name}, std::less<>> {{\n')
        for v in self.values:
            target.write(f'{ind}{ind}{{{q(v)}, {name}::{safename(v)}}},\n')
        target.write(f'{ind}}};\n{ind}auto iter = m.find(v);\n')
        target.write(f'{ind}if (iter == m.end()) throw bool{{}};\n')
        target.write(f'{ind}out = iter->second;\n}}\n')
        target.write(f'namespace {common_namespace} {{\n')
        target.write(f'inline auto toYaml({name} v, [[maybe_unused]] ::{common_namespace}::store_config const& config) {{\n')
        target.write(f'{ind}auto n = YAML::Node{{std::string{{to_string(v)}}}};\n')
        target.write(f'{ind}if (config.generateTags) n.SetTag(""{name}"");\n')
        target.write(f'{ind}return n;\n}}\n')
        target.write(f'inline void fromYaml(YAML::Node n, {name}& out) {{\n')
        target.write(f'{ind}to_enum(n.as<std::string>(), out);\n}}\n')
        if len(self.values):
            target.write(f'template <> struct IsConstant<{name}> : std::true_type {{}};\n')
        target.write('}\n')
        target.write('\n')","
class EnumDefinition:
    '''Prototype of a enum.'''
    def __init__(self, name: str, values: list[str]):
        '''Initialize enum definition with a name and possible values.'''
        pass
    def writeDefinition(self, target: IO[str], ind: str, common_namespace: str) -> None:
        '''Write enum definition to output.'''
        pass",snippet_210,3,"from typing import IO, List


class EnumDefinition:
    '''Prototype of a enum.'''
    def __init__(self, name: str, values: List[str]):
        '''Initialize enum definition with a name and possible values.'''
        if not isinstance(name, str) or not name:
            raise ValueError(""Enum name must be a non-empty string."")
        if not isinstance(values, list) or any(not isinstance(v, str) or not v for v in values):
            raise ValueError(""Enum values must be a list of non-empty strings."")
        self.name = name
        self.values = values

    def writeDefinition(self, target: IO[str], ind: str, common_namespace: str) -> None:
        '''Write enum definition to output.'''
        target.write(f""{ind}enum class {self.name} {{\n"")
        for i, v in enumerate(self.values):
            sep = "","" if i < len(self.values) - 1 else """"
            target.write(f""{ind}    {v}{sep}\n"")
        target.write(f""{ind}}};\n"")"
287397,common-workflow-language/schema_salad,common-workflow-language_schema_salad/schema_salad/cpp_codegen.py,schema_salad.cpp_codegen.FieldDefinition,"from typing import IO, Any, Optional, Union, cast

class FieldDefinition:
    """"""Prototype of a single field from a class definition.""""""

    def __init__(self, name: str, typeStr: str, optional: bool, mapSubject: str, mapPredicate: str, typeDSL: bool):
        """"""Initialize field definition.

        Creates a new field with name, its type, optional and which field to use to convert
        from list to map (or empty if it is not possible)
        """"""
        self.name = name
        self.typeStr = typeStr
        self.optional = optional
        self.mapSubject = mapSubject
        self.mapPredicate = mapPredicate
        self.typeDSL = typeDSL

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, namespace: str) -> None:
        """"""Write a C++ definition for the class field.""""""
        name = safename(self.name)
        typeStr = self.typeStr.replace(namespace + '::', '')
        target.write(f'{fullInd}heap_object<{typeStr}> {name};\n')","
class FieldDefinition:
    '''Prototype of a single field from a class definition.'''
    def __init__(self, name: str, typeStr: str, optional: bool, mapSubject: str, mapPredicate: str, typeDSL: bool):
        '''Initialize field definition.
        Creates a new field with name, its type, optional and which field to use to convert
        from list to map (or empty if it is not possible)
        '''
        pass
    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, namespace: str) -> None:
        '''Write a C++ definition for the class field.'''
        pass",snippet_211,3,"class FieldDefinition:
    '''Prototype of a single field from a class definition.'''
    def __init__(self, name: str, typeStr: str, optional: bool, mapSubject: str, mapPredicate: str, typeDSL: bool):
        '''Initialize field definition.
        Creates a new field with name, its type, optional and which field to use to convert
        from list to map (or empty if it is not possible)
        '''
        self.name = name
        self.typeStr = typeStr
        self.optional = bool(optional)
        self.mapSubject = mapSubject or """"
        self.mapPredicate = mapPredicate or """"
        self.typeDSL = bool(typeDSL)

    def _is_qualified(self, t: str) -> bool:
        return ""::"" in t

    def _has_templates(self, t: str) -> bool:
        return ""<"" in t and "">"" in t

    def _normalize_basic_type(self, t: str) -> str:
        basic_map = {
            ""string"": ""std::string"",
            ""std::string"": ""std::string"",
            ""int"": ""int"",
            ""int32"": ""int32_t"",
            ""int32_t"": ""int32_t"",
            ""int64"": ""int64_t"",
            ""int64_t"": ""int64_t"",
            ""uint32"": ""uint32_t"",
            ""uint32_t"": ""uint32_t"",
            ""uint64"": ""uint64_t"",
            ""uint64_t"": ""uint64_t"",
            ""float"": ""float"",
            ""double"": ""double"",
            ""bool"": ""bool"",
            ""char"": ""char"",
            ""size_t"": ""size_t"",
        }
        return basic_map.get(t, t)

    def _should_prefix_namespace(self, t: str) -> bool:
        # Do not prefix for known builtins or already qualified or template types
        builtins = {
            ""std::string"", ""string"", ""int"", ""int32_t"", ""int64_t"",
            ""uint32_t"", ""uint64_t"", ""float"", ""double"", ""bool"",
            ""char"", ""size_t"", ""void"",
        }
        if t in builtins:
            return False
        if self._is_qualified(t):
            return False
        if self._has_templates(t):
            return False
        # common STL containers should not be prefixed
        stl = {""std::vector"", ""std::map"", ""std::unordered_map"", ""std::set"", ""std::optional"", ""std::pair"",
               ""vector"", ""map"", ""unordered_map"", ""set"", ""optional"", ""pair""}
        if t in stl:
            return False
        return True

    def _resolve_type(self, namespace: str) -> str:
        t = self.typeStr.strip()
        t = self._normalize_basic_type(t)

        # If there are template args, leave as is (assume caller provided fully qualified args)
        if self._has_templates(t):
            resolved = t
        else:
            if self.typeDSL and self._should_prefix_namespace(t) and namespace:
                resolved = f""{namespace}::{t}""
            else:
                resolved = t

        if self.optional:
            # avoid double wrapping if already optional
            if not resolved.startswith(""std::optional<""):
                resolved = f""std::optional<{resolved}>""
        return resolved

    def writeDefinition(self, target, fullInd: str, ind: str, namespace: str) -> None:
        '''Write a C++ definition for the class field.'''
        cpp_type = self._resolve_type(namespace or """")
        line = f""{fullInd}{cpp_type} {self.name};\n""
        target.write(line)"
287519,kinegratii/borax,kinegratii_borax/borax/calendars/utils.py,borax.calendars.utils.ThreeNineUtils,"from borax.calendars.lunardate import LunarDate, TextUtils, TermUtils
from collections import OrderedDict
from datetime import date, datetime, timedelta
from typing import Union, Dict

class ThreeNineUtils:
    """"""
    """"""

    @staticmethod
    def get_39days(year: int) -> Dict[str, date]:
        """"""year
        """"""
        day13 = TermUtils.day_start_from_term(year, '', 3, '')
        day23 = day13 + timedelta(days=10)
        day33 = TermUtils.day_start_from_term(year, '', 1, '')
        day19 = TermUtils.day_start_from_term(year, '', 0)
        days = OrderedDict({'': day13, '': day23, '': day33, '': day19})
        for i, dc in enumerate(TextUtils.DAYS_CN[1:10], start=1):
            days[f'{dc}'] = day19 + timedelta(days=(i - 1) * 9)
        return days

    @staticmethod
    def get_39label(date_obj: Union[date, LunarDate]) -> str:
        """"""
        """"""
        if isinstance(date_obj, LunarDate):
            sd = date_obj.to_solar_date()
        else:
            sd = date_obj
        if sd.month in (4, 5, 6, 10, 11):
            return ''
        year = sd.year - bool(sd.month < 4)
        days = ThreeNineUtils.get_39days(year)
        for vs in list(days.items()):
            label, sd = vs
            range_len = -1
            if label in ['', '']:
                range_len = 10
            elif label == '':
                range_len = (days[''] - days['']).days
            elif '' in label:
                range_len = 9
            offset = (date_obj - sd).days
            if 0 <= offset <= range_len - 1:
                return f'{label}{offset + 1}'
        return ''","
class ThreeNineUtils:
    
    @staticmethod
    def get_39days(year: int) -> Dict[str, date]:
        '''year
        '''
        pass
    @staticmethod
    def get_39label(date_obj: Union[date, LunarDate]) -> str:
        
        pass",snippet_212,3,"from datetime import date, timedelta
from typing import Dict, Union, Any


class ThreeNineUtils:

    STEMS = ['', '', '', '', '', '', '', '', '', '']
    CN_NUM = {1: '', 2: '', 3: '', 4: '', 5: '', 6: '', 7: '', 8: '', 9: ''}

    @staticmethod
    def _to_date(obj: Union[date, Any]) -> date:
        if isinstance(obj, date):
            return obj
        # Try common conversion methods for LunarDate-like objects
        for attr in ('to_date', 'to_solar_date', 'to_solar', 'solar_date'):
            if hasattr(obj, attr):
                val = getattr(obj, attr)
                val = val() if callable(val) else val
                if isinstance(val, date):
                    return val
        # Try attributes year, month, day
        if all(hasattr(obj, x) for x in ('year', 'month', 'day')):
            return date(int(getattr(obj, 'year')), int(getattr(obj, 'month')), int(getattr(obj, 'day')))
        raise TypeError('Unsupported date object')

    @staticmethod
    def _sexagenary_day_index(d: date) -> int:
        # Assume 1984-02-02 is JiaZi (index 1)
        base = date(1984, 2, 2)
        offset = (d - base).days
        return (offset % 60) + 1

    @staticmethod
    def _is_geng_day(d: date) -> bool:
        idx = ThreeNineUtils._sexagenary_day_index(d)
        stem_index = ((idx - 1) % 10) + 1  # 1..10
        return stem_index == 7  # 

    @staticmethod
    def _first_geng_on_or_after(d: date) -> date:
        cur = d
        while not ThreeNineUtils._is_geng_day(cur):
            cur += timedelta(days=1)
        return cur

    @staticmethod
    def _nth_geng_on_or_after(d: date, n: int) -> date:
        count = 0
        cur = d
        while True:
            if ThreeNineUtils._is_geng_day(cur):
                count += 1
                if count == n:
                    return cur
            cur += timedelta(days=1)

    @staticmethod
    def _approx_summer_solstice(year: int) -> date:
        # Approximate  as June 21
        return date(year, 6, 21)

    @staticmethod
    def _approx_start_of_autumn(year: int) -> date:
        # Approximate  as August 7
        return date(year, 8, 7)

    @staticmethod
    def _approx_winter_solstice(year: int) -> date:
        # Approximate  as December 21
        return date(year, 12, 21)

    @staticmethod
    def get_39days(year: int) -> Dict[str, date]:
        # 
        xiazhi = ThreeNineUtils._approx_summer_solstice(year)
        liqiu = ThreeNineUtils._approx_start_of_autumn(year)

        chu_fu = ThreeNineUtils._nth_geng_on_or_after(xiazhi, 3)
        zhong_fu = chu_fu + timedelta(days=10)
        mo_fu = ThreeNineUtils._first_geng_on_or_after(liqiu)

        res: Dict[str, date] = {
            '': chu_fu,
            '': zhong_fu,
            '': mo_fu,
        }

        #  starts from the  of this year
        dongzhi = ThreeNineUtils._approx_winter_solstice(year)
        for i in range(9):
            res[f'{ThreeNineUtils.CN_NUM[i + 1]}'] = dongzhi + timedelta(days=i * 9)

        return res

    @staticmethod
    def get_39label(date_obj: Union[date, Any]) -> str:
        d = ThreeNineUtils._to_date(date_obj)

        #  label
        daysinfo = ThreeNineUtils.get_39days(d.year)
        chu = daysinfo['']
        zhong = daysinfo['']
        mo = daysinfo['']
        chu_end = zhong - timedelta(days=1)
        #  start is mo, ends after 10 days
        mo_end = mo + timedelta(days=9)
        #  ends the day before 
        zhong_end = mo - timedelta(days=1)

        if chu <= d <= chu_end:
            return f'{(d - chu).days + 1}'
        if zhong <= d <= zhong_end:
            return f'{(d - zhong).days + 1}'
        if mo <= d <= mo_end:
            return f'{(d - mo).days + 1}'

        #  label: determine relevant  anchor
        dz_this = ThreeNineUtils._approx_winter_solstice(d.year)
        if d >= dz_this:
            anchor = dz_this
        else:
            anchor = ThreeNineUtils._approx_winter_solstice(d.year - 1)

        delta = (d - anchor).days
        if 0 <= delta <= 80:
            n = delta // 9 + 1
            day_in = delta % 9 + 1
            cn = ThreeNineUtils.CN_NUM.get(n, '')
            if cn:
                return f'{cn}{day_in}'

        return ''"
292643,swistakm/graceful,swistakm_graceful/src/graceful/authentication.py,graceful.authentication.BaseAuthenticationMiddleware,"class BaseAuthenticationMiddleware:
    """"""Base class for all authentication middleware classes.

    Args:
        user_storage (BaseUserStorage): a storage object used to retrieve
            user object using their identifier lookup.
        name (str): custom name of the authentication middleware useful
            for handling custom user storage backends. Defaults to middleware
            class name.

    .. versionadded:: 0.4.0
    """"""
    challenge = None
    only_with_storage = False

    def __init__(self, user_storage=None, name=None):
        """"""Initialize authentication middleware.""""""
        self.user_storage = user_storage
        self.name = name if name else self.__class__.__name__
        if self.only_with_storage and (not isinstance(self.user_storage, BaseUserStorage)):
            raise ValueError('{} authentication middleware requires valid storage. Got {}.'.format(self.__class__.__name__, self.user_storage))

    def process_resource(self, req, resp, resource, uri_kwargs=None):
        """"""Process resource after routing to it.

        This is basic falcon middleware handler.

        Args:
            req (falcon.Request): request object
            resp (falcon.Response): response object
            resource (object): resource object matched by falcon router
            uri_kwargs (dict): additional keyword argument from uri template.
                For ``falcon<1.0.0`` this is always ``None``
        """"""
        if 'user' in req.context:
            return
        identifier = self.identify(req, resp, resource, uri_kwargs)
        user = self.try_storage(identifier, req, resp, resource, uri_kwargs)
        if user is not None:
            req.context['user'] = user
        elif self.challenge is not None:
            req.context.setdefault('challenges', list()).append(self.challenge)

    def identify(self, req, resp, resource, uri_kwargs):
        """"""Identify the user that made the request.

        Args:
            req (falcon.Request): request object
            resp (falcon.Response): response object
            resource (object): resource object matched by falcon router
            uri_kwargs (dict): additional keyword argument from uri template.
                For ``falcon<1.0.0`` this is always ``None``

        Returns:
            object: a user object (preferably a dictionary).
        """"""
        raise NotImplementedError

    def try_storage(self, identifier, req, resp, resource, uri_kwargs):
        """"""Try to find user in configured user storage object.

        Args:
            identifier: User identifier.

        Returns:
            user object.
        """"""
        if identifier is None:
            user = None
        elif self.user_storage is not None:
            user = self.user_storage.get_user(self, identifier, req, resp, resource, uri_kwargs)
        elif self.user_storage is None and (not self.only_with_storage):
            user = {'identified_with': self, 'identifier': identifier}
        else:
            user = None
        return user","class BaseAuthenticationMiddleware:
    '''Base class for all authentication middleware classes.
    Args:
        user_storage (BaseUserStorage): a storage object used to retrieve
            user object using their identifier lookup.
        name (str): custom name of the authentication middleware useful
            for handling custom user storage backends. Defaults to middleware
            class name.
    .. versionadded:: 0.4.0
    '''
    def __init__(self, user_storage=None, name=None):
        
        pass
    def process_resource(self, req, resp, resource, uri_kwargs=None):
        
        pass
    def identify(self, req, resp, resource, uri_kwargs):
        '''Identify the user that made the request.
        Args:
            req (falcon.Request): request object
            resp (falcon.Response): response object
            resource (object): resource object matched by falcon router
            uri_kwargs (dict): additional keyword argument from uri template.
                For ``falcon<1.0.0`` this is always ``None``
        Returns:
            object: a user object (preferably a dictionary).
        '''
        pass
    def try_storage(self, identifier, req, resp, resource, uri_kwargs):
        
        pass",snippet_213,5,"class BaseAuthenticationMiddleware:
    '''Base class for all authentication middleware classes.
    Args:
        user_storage (BaseUserStorage): a storage object used to retrieve
            user object using their identifier lookup.
        name (str): custom name of the authentication middleware useful
            for handling custom user storage backends. Defaults to middleware
            class name.
    .. versionadded:: 0.4.0
    '''
    def __init__(self, user_storage=None, name=None):
        self.user_storage = user_storage
        self.name = name or self.__class__.__name__

    def process_resource(self, req, resp, resource, uri_kwargs=None):
        uri_kwargs = uri_kwargs or {}
        user = self.identify(req, resp, resource, uri_kwargs)
        if user is not None:
            ctx = getattr(req, 'context', None)
            if ctx is None:
                try:
                    req.context = {}
                    ctx = req.context
                except Exception:
                    ctx = None
            if ctx is not None:
                try:
                    ctx['user'] = user
                except Exception:
                    try:
                        setattr(ctx, 'user', user)
                    except Exception:
                        pass

    def identify(self, req, resp, resource, uri_kwargs):
        '''Identify the user that made the request.
        Args:
            req (falcon.Request): request object
            resp (falcon.Response): response object
            resource (object): resource object matched by falcon router
            uri_kwargs (dict): additional keyword argument from uri template.
                For ``falcon<1.0.0`` this is always ``None``
        Returns:
            object: a user object (preferably a dictionary).
        '''
        raise NotImplementedError('Subclasses must implement identify()')

    def try_storage(self, identifier, req, resp, resource, uri_kwargs):
        storage = self.user_storage
        if storage is None:
            return None

        # Try common callable retrieval methods
        method_names = (
            'get_user',
            'get',
            'get_by_id',
            'get_by_identifier',
            'retrieve',
            'fetch',
        )
        for name in method_names:
            getter = getattr(storage, name, None)
            if callable(getter):
                return getter(identifier)

        # Callable storage object
        if callable(storage):
            return storage(identifier)

        # Mapping-like storage
        try:
            return storage[identifier]  # type: ignore[index]
        except Exception:
            pass

        return None"
292765,pytroll/pyspectral,pytroll_pyspectral/rsr_convert_scripts/avhrr1_rsr.py,avhrr1_rsr.AvhrrRSR,"import numpy as np
from pyspectral.config import get_config
from xlrd import open_workbook
import os

class AvhrrRSR:
    """"""Container for the NOAA AVHRR-1 RSR data.""""""

    def __init__(self, wavespace='wavelength'):
        """"""Initialize the AVHRR-1 RSR class.""""""
        options = get_config()
        self.avhrr_path = options['avhrr/1'].get('path')
        if not os.path.exists(self.avhrr_path):
            self.avhrr1_path = os.path.join(DATA_PATH, options['avhrr/1'].get('filename'))
        self.output_dir = options.get('rsr_dir', './')
        self.rsr = {}
        for satname in AVHRR1_SATELLITES:
            self.rsr[satname] = {}
            for chname in AVHRR_BAND_NAMES['avhrr/1']:
                self.rsr[satname][chname] = {'wavelength': None, 'response': None}
        self._load()
        self.wavespace = wavespace
        if wavespace not in ['wavelength', 'wavenumber']:
            raise AttributeError('wavespace has to be either ' + ""'wavelength' or 'wavenumber'!"")
        self.unit = 'micrometer'
        if wavespace == 'wavenumber':
            self.convert2wavenumber()

    def _load(self, scale=1.0):
        """"""Load the AVHRR RSR data for the band requested.""""""
        wb_ = open_workbook(self.avhrr_path)
        sheet_names = []
        for sheet in wb_.sheets():
            if sheet.name in ['Kleespies Data']:
                print('Skip sheet...')
                continue
            ch_name = CHANNEL_NAMES.get(sheet.name.strip())
            if not ch_name:
                break
            sheet_names.append(sheet.name.strip())
            header = sheet.col_values(0, start_rowx=0, end_rowx=2)
            platform_name = header[0].strip('# ')
            unit = header[1].split('Wavelength (')[1].strip(')')
            scale = get_scale_from_unit(unit)
            wvl = sheet.col_values(0, start_rowx=2)
            is_comment = True
            idx = 0
            while is_comment:
                item = wvl[::-1][idx]
                if isinstance(item, str):
                    idx = idx + 1
                else:
                    break
            ndim = len(wvl) - idx
            wvl = wvl[0:ndim]
            if platform_name == 'TIROS-N':
                wvl = adjust_typo_avhrr1_srf_only_xls_file(platform_name, wvl)
            response = sheet.col_values(1, start_rowx=2, end_rowx=2 + ndim)
            wavelength = np.array(wvl) * scale
            response = np.array(response)
            self.rsr[platform_name][ch_name]['wavelength'] = wavelength
            self.rsr[platform_name][ch_name]['response'] = response","
class AvhrrRSR:
    '''Container for the NOAA AVHRR-1 RSR data.'''
    def __init__(self, wavespace='wavelength'):
        
        pass
    def _load(self, scale=1.0):
        
        pass",snippet_215,3,"import numpy as np

class AvhrrRSR:
    '''Container for the NOAA AVHRR-1 RSR data.'''
    def __init__(self, wavespace='wavelength'):
        wavespace = str(wavespace).lower()
        if wavespace not in ('wavelength', 'wavenumber'):
            raise ValueError(""wavespace must be 'wavelength' or 'wavenumber'"")
        self.wavespace = wavespace
        self.data = {}
        self.channels = ['ch1', 'ch2', 'ch3', 'ch4', 'ch5']
        self._load()

    def _load(self, scale=1.0):
        # Approximate AVHRR/1 channel centers (micrometers) and FWHM (micrometers)
        # ch1: 0.63 m, ch2: 0.86 m, ch3: 3.75 m, ch4: 10.8 m, ch5: 12.0 m
        specs = {
            'ch1': {'center': 0.63, 'fwhm': 0.06},
            'ch2': {'center': 0.86, 'fwhm': 0.08},
            'ch3': {'center': 3.75, 'fwhm': 0.7},
            'ch4': {'center': 10.8, 'fwhm': 1.0},
            'ch5': {'center': 12.0, 'fwhm': 1.0},
        }

        def gaussian(x, x0, fwhm):
            sigma = fwhm / (2.0 * np.sqrt(2.0 * np.log(2.0)))
            g = np.exp(-0.5 * ((x - x0) / sigma) ** 2)
            return g

        for ch, s in specs.items():
            c = s['center']
            f = s['fwhm']
            sigma = f / (2.0 * np.sqrt(2.0 * np.log(2.0)))
            x_min = max(1e-6, c - 5 * sigma)
            x_max = c + 5 * sigma
            # Set resolution relative to band width
            step = max(f / 400.0, 1e-4)
            wl = np.arange(x_min, x_max + step, step)
            rsr = gaussian(wl, c, f)
            if rsr.max() > 0:
                rsr = rsr / rsr.max()
            rsr = rsr * float(scale)

            wn = 1e4 / wl  # cm^-1
            # Ensure monotonic increasing x for each representation
            idx_wl = np.argsort(wl)
            wl = wl[idx_wl]
            rsr_wl = rsr[idx_wl]
            idx_wn = np.argsort(wn)
            wn = wn[idx_wn]
            rsr_wn = rsr[idx_wn]

            self.data[ch] = {
                'wavelength': wl,   # micrometers
                'wavenumber': wn,   # cm^-1
                'response_wavelength': rsr_wl,
                'response_wavenumber': rsr_wn,
            }"
293144,Kentzo/Power,Kentzo_Power/power/common.py,power.common.PowerManagementObserver,"from abc import ABCMeta, abstractmethod

class PowerManagementObserver:
    """"""
    Base class for PowerManagement observers.
    Do not make assumptions in what thread or event loop these methods are called.
    """"""
    __metaclass__ = ABCMeta

    @abstractmethod
    def on_power_sources_change(self, power_management):
        """"""
        @param power_management: Instance of PowerManagement posted notification
        """"""
        pass

    @abstractmethod
    def on_time_remaining_change(self, power_management):
        """"""
        @param power_management: Instance of PowerManagement posted notification
        """"""
        pass","
class PowerManagementObserver:
    '''
    Base class for PowerManagement observers.
    Do not make assumptions in what thread or event loop these methods are called.
    '''
    @abstractmethod
    def on_power_sources_change(self, power_management):
        '''
        @param power_management: Instance of PowerManagement posted notification
        '''
        pass
    @abstractmethod
    def on_time_remaining_change(self, power_management):
        
        pass",snippet_216,3,"from abc import ABC, abstractmethod
from typing import Any

class PowerManagementObserver(ABC):
    '''
    Base class for PowerManagement observers.
    Do not make assumptions in what thread or event loop these methods are called.
    '''
    @abstractmethod
    def on_power_sources_change(self, power_management: Any) -> None:
        '''
        @param power_management: Instance of PowerManagement posted notification
        '''
        raise NotImplementedError

    @abstractmethod
    def on_time_remaining_change(self, power_management: Any) -> None:
        raise NotImplementedError"
293849,hasgeek/coaster,hasgeek_coaster/src/coaster/logger.py,coaster.logger.FilteredValueIndicator,"class FilteredValueIndicator:
    """"""Represent a filtered value.""""""

    def __str__(self) -> str:
        """"""Filter str.""""""
        return '[Filtered]'

    def __repr__(self) -> str:
        """"""Filter repr.""""""
        return '[Filtered]'","class FilteredValueIndicator:
    
    def __str__(self) -> str:
        '''Filter str.'''
        pass
    def __repr__(self) -> str:
        
        pass",snippet_217,3,"class FilteredValueIndicator:
    
    def __str__(self) -> str:
        '''Filter str.'''
        return '<filtered>'

    def __repr__(self) -> str:
        return '<filtered>'"
293851,hasgeek/coaster,hasgeek_coaster/src/coaster/logger.py,coaster.logger.RepeatValueIndicator,"class RepeatValueIndicator:
    """"""Represent a repeating value.""""""

    def __init__(self, key: str) -> None:
        """"""Init with key.""""""
        self.key = key

    def __repr__(self) -> str:
        """"""Return representation.""""""
        return f'<same as prior {self.key!r}>'
    __str__ = __repr__","class RepeatValueIndicator:
    
    def __init__(self, key: str) -> None:
        
        pass
    def __repr__(self) -> str:
        '''Return representation.'''
        pass",snippet_218,3,"class RepeatValueIndicator:
    
    __slots__ = (""key"",)

    def __init__(self, key: str) -> None:
        if not isinstance(key, str):
            raise TypeError(""key must be a string"")
        if not key:
            raise ValueError(""key must be non-empty"")
        self.key = key

    def __repr__(self) -> str:
        '''Return representation.'''
        return f""{self.__class__.__name__}({self.key!r})"""
295935,cloudsmith-io/cloudsmith-cli,cloudsmith-io_cloudsmith-cli/cloudsmith_cli/core/ratelimits.py,cloudsmith_cli.core.ratelimits.RateLimitsInfo,"import datetime

class RateLimitsInfo:
    """"""Data for rate limits.""""""
    interval = None
    limit = None
    remaining = None
    reset = None
    throttled = None

    def __str__(self):
        """"""Get rate limit information as text.""""""
        return 'Throttled: %(throttled)s, Remaining: %(remaining)d/%(limit)d, Interval: %(interval)f, Reset: %(reset)s' % {'throttled': 'Yes' if self.throttled else 'No', 'remaining': self.remaining, 'limit': self.limit, 'interval': self.interval, 'reset': self.reset}

    @classmethod
    def from_dict(cls, data):
        """"""Create RateLimitsInfo from a dictionary.""""""
        info = RateLimitsInfo()
        if 'interval' in data:
            info.interval = float(data['interval'])
        if 'limit' in data:
            info.limit = int(data['limit'])
        if 'remaining' in data:
            info.remaining = int(data['remaining'])
        if 'reset' in data:
            info.reset = datetime.datetime.utcfromtimestamp(int(data['reset']))
        if 'throtted' in data:
            info.throttled = bool(data['throttled'])
        else:
            info.throttled = info.remaining == 0
        return info

    @classmethod
    def from_headers(cls, headers):
        """"""Create RateLimitsInfo from HTTP headers.""""""
        try:
            data = {'interval': headers['X-RateLimit-Interval'], 'limit': headers['X-RateLimit-Limit'], 'remaining': headers['X-RateLimit-Remaining'], 'reset': headers['X-RateLimit-Reset']}
        except KeyError:
            data = {}
        return cls.from_dict(data)","
class RateLimitsInfo:
    
    def __str__(self):
        
        pass
    @classmethod
    def from_dict(cls, data):
        '''Create RateLimitsInfo from a dictionary.'''
        pass
    @classmethod
    def from_headers(cls, headers):
        
        pass",snippet_220,4,"class RateLimitsInfo:
    def __init__(self, limit=None, remaining=None, reset_at=None, retry_after=None, source=None):
        self.limit = limit
        self.remaining = remaining
        self.reset_at = reset_at  # aware datetime in UTC or None
        self.retry_after = retry_after  # seconds or None
        self.source = source  # where it was parsed from (e.g., 'headers' or 'dict')

    def __str__(self):
        parts = []
        if self.limit is not None:
            parts.append(f""limit={self.limit}"")
        if self.remaining is not None:
            parts.append(f""remaining={self.remaining}"")
        if self.reset_at is not None:
            parts.append(f""reset_at={self.reset_at.isoformat()}"")
        if self.retry_after is not None:
            parts.append(f""retry_after={self.retry_after}s"")
        if not parts:
            return ""RateLimitsInfo()""
        return ""RateLimitsInfo("" + "", "".join(parts) + "")""

    @classmethod
    def from_dict(cls, data):
        '''Create RateLimitsInfo from a dictionary.'''
        if data is None:
            return cls(source=""dict"")
        # Normalize keys (case-insensitive, underscores and hyphens treated similarly)
        norm = {}
        for k, v in data.items():
            if not isinstance(k, str):
                continue
            key = k.strip().lower().replace(""-"", ""_"")
            norm[key] = v

        limit = cls._to_int(norm.get(""limit"", norm.get(""ratelimit_limit"")))
        remaining = cls._to_int(norm.get(""remaining"", norm.get(""ratelimit_remaining"")))
        retry_after = cls._to_int(norm.get(""retry_after""))

        reset_at = None
        # Accept 'reset_at' as ISO or datetime, 'reset' as seconds delta, 'reset_epoch' as unix ts
        reset_iso = norm.get(""reset_at"") or norm.get(""reset_time"")
        reset_epoch = norm.get(""reset_epoch"") or norm.get(""epoch"") or norm.get(""ratelimit_reset_epoch"")
        reset_generic = norm.get(""reset"") or norm.get(""ratelimit_reset"")

        reset_at = cls._parse_reset_any(reset_iso, reset_epoch, reset_generic)

        return cls(limit=limit, remaining=remaining, reset_at=reset_at, retry_after=retry_after, source=""dict"")

    @classmethod
    def from_headers(cls, headers):
        if headers is None:
            return cls(source=""headers"")
        # headers may be a mapping with case-insensitive keys; normalize to lowercase
        def get_header(*names):
            for name in names:
                for k, v in headers.items():
                    if isinstance(k, str) and k.lower() == name.lower():
                        return v
            return None

        limit = cls._to_int(
            get_header(""x-ratelimit-limit"", ""ratelimit-limit"")
        )
        remaining = cls._to_int(
            get_header(""x-ratelimit-remaining"", ""ratelimit-remaining"")
        )

        # Reset can be:
        # - X-RateLimit-Reset: unix epoch seconds (GitHub, Twitter) OR delta seconds (some APIs)
        # - RateLimit-Reset: delta seconds per RFC (can be seconds or a timestamp)
        # - Retry-After: delta seconds (or HTTP-date)
        h_reset = get_header(""x-ratelimit-reset"", ""ratelimit-reset"")
        retry_after_raw = get_header(""retry-after"")

        reset_at = cls._parse_reset_any(None, h_reset, h_reset)  # parse both interpretations
        retry_after = cls._parse_retry_after(retry_after_raw)

        # If we only have Retry-After and no reset_at, derive reset_at as now + retry_after
        if reset_at is None and retry_after is not None:
            reset_at = cls._now_utc() + cls._seconds_to_timedelta(retry_after)

        return cls(limit=limit, remaining=remaining, reset_at=reset_at, retry_after=retry_after, source=""headers"")

    @staticmethod
    def _to_int(value):
        if value is None:
            return None
        try:
            return int(float(str(value).strip()))
        except Exception:
            return None

    @staticmethod
    def _now_utc():
        from datetime import datetime, timezone
        return datetime.now(timezone.utc)

    @staticmethod
    def _seconds_to_timedelta(seconds):
        from datetime import timedelta
        try:
            s = int(float(seconds))
            return timedelta(seconds=s)
        except Exception:
            return timedelta(0)

    @classmethod
    def _parse_reset_any(cls, reset_iso, reset_epoch_or_delta, reset_delta=None):
        # Try ISO datetime first
        dt = cls._parse_iso_datetime(reset_iso)
        if dt is not None:
            return dt

        # Try epoch seconds
        dt = cls._parse_epoch_or_delta(reset_epoch_or_delta)
        if dt is not None:
            return dt

        # Fallback: explicit delta seconds
        dt = cls._parse_delta_seconds(reset_delta)
        if dt is not None:
            return dt
        return None

    @classmethod
    def _parse_iso_datetime(cls, value):
        if not value:
            return None
        from datetime import datetime, timezone
        s = str(value).strip()
        # Accept RFC1123/HTTP-date for Retry-After too
        for fmt in (
            ""%a, %d %b %Y %H:%M:%S GMT"",
            ""%Y-%m-%dT%H:%M:%S.%fZ"",
            ""%Y-%m-%dT%H:%M:%SZ"",
            ""%Y-%m-%d %H:%M:%S%z"",
            ""%Y-%m-%d %H:%M:%S"",
            ""%Y-%m-%d"",
        ):
            try:
                parsed = datetime.strptime(s, fmt)
                if parsed.tzinfo is None:
                    parsed = parsed.replace(tzinfo=timezone.utc)
                else:
                    parsed = parsed.astimezone(timezone.utc)
                return parsed
            except Exception:
                continue
        # Try fromisoformat
        try:
            parsed = datetime.fromisoformat(s)
            if parsed.tzinfo is None:
                from datetime import timezone
                parsed = parsed.replace(tzinfo=timezone.utc)
            else:
                parsed = parsed.astimezone(timezone.utc)
            return parsed
        except Exception:
            return None

    @classmethod
    def _parse_epoch_or_delta(cls, value):
        if value is None:
            return None
        s = str(value).strip()
        # If it's clearly not numeric, skip
        try:
            num = float(s)
        except Exception:
            return None

        # Heuristic:
        # - If value > 10^7, treat as Unix epoch seconds (e.g., 1739021123)
        # - Else, treat as delta seconds from now
        # Also accept milliseconds if > 10^12
        from datetime import datetime, timezone, timedelta
        now = cls._now_utc()
        if num > 1e12:  # milliseconds epoch
            seconds = num / 1000.0
            try:
                return datetime.fromtimestamp(seconds, tz=timezone.utc)
            except Exception:
                return None
        if num > 1e7:  # epoch seconds
            try:
                return datetime.fromtimestamp(num, tz=timezone.utc)
            except Exception:
                return None
        # treat as delta seconds
        try:
            return now + timedelta(seconds=int(num))
        except Exception:
            return None

    @classmethod
    def _parse_delta_seconds(cls, value):
        if value is None:
            return None
        try:
            sec = int(float(str(value).strip()))
        except Exception:
            return None
        return cls._now_utc() + cls._seconds_to_timedelta(sec)

    @classmethod
    def _parse_retry_after(cls, value):
        if value is None:
            return None
        s = str(value).strip()
        # Retry-After can be delta seconds or HTTP-date
        # Try seconds first
        try:
            return int(float(s))
        except Exception:
            pass
        # Try HTTP-date
        dt = cls._parse_iso_datetime(s)
        if dt is None:
            return None
        delta = dt - cls._now_utc()
        seconds = int(max(0, round(delta.total_seconds())))
        return seconds"
296505,CZ-NIC/python-rt,CZ-NIC_python-rt/rt/rest2.py,rt.rest2.Attachment,"import dataclasses
import base64

@dataclasses.dataclass
class Attachment:
    """"""Dataclass representing an attachment.""""""
    file_name: str
    file_type: str
    file_content: bytes

    def to_dict(self) -> dict[str, str]:
        """"""Convert to a dictionary for submitting to the REST API.""""""
        return {'FileName': self.file_name, 'FileType': self.file_type, 'FileContent': base64.b64encode(self.file_content).decode('utf-8')}

    def multipart_form_element(self) -> tuple[str, bytes, str]:
        """"""Convert to a tuple as required for multipart-form-data submission.""""""
        return (self.file_name, self.file_content, self.file_type)","@dataclasses.dataclass
class Attachment:
    '''Dataclass representing an attachment.'''
    def to_dict(self) -> dict[str, str]:
        
        pass
    def multipart_form_element(self) -> tuple[str, bytes, str]:
        '''Convert to a tuple as required for multipart-form-data submission.'''
        pass",snippet_221,3,"import dataclasses
import base64


@dataclasses.dataclass
class Attachment:
    '''Dataclass representing an attachment.'''
    filename: str
    content: bytes
    content_type: str = ""application/octet-stream""

    def __post_init__(self):
        if isinstance(self.content, str):
            self.content = self.content.encode(""utf-8"")
        if not isinstance(self.content, (bytes, bytearray)):
            raise TypeError(""content must be bytes, bytearray, or str"")
        if isinstance(self.content, bytearray):
            self.content = bytes(self.content)
        if not isinstance(self.filename, str) or not self.filename:
            raise ValueError(""filename must be a non-empty string"")
        if not isinstance(self.content_type, str) or not self.content_type:
            raise ValueError(""content_type must be a non-empty string"")

    def to_dict(self) -> dict[str, str]:
        return {
            ""filename"": self.filename,
            ""content"": base64.b64encode(self.content).decode(""ascii""),
            ""content_type"": self.content_type,
        }

    def multipart_form_element(self) -> tuple[str, bytes, str]:
        '''Convert to a tuple as required for multipart-form-data submission.'''
        return (self.filename, self.content, self.content_type)"
297960,theolind/pymysensors,theolind_pymysensors/mysensors/transport.py,mysensors.transport.Transport,"class Transport:
    """"""Handle gateway transport.

    I/O is allowed in this class. This class should host methods that
    are related to the gateway transport type.
    """"""

    def __init__(self, gateway, connect, timeout=1.0, reconnect_timeout=10.0, **kwargs):
        """"""Set up transport.""""""
        self._connect = connect
        self.can_log = False
        self.connect_task = None
        self.gateway = gateway
        self.protocol = None
        self.reconnect_timeout = reconnect_timeout
        self.timeout = timeout

    def disconnect(self):
        """"""Disconnect from the transport.""""""
        if not self.protocol or not self.protocol.transport:
            self.protocol = None
            return
        _LOGGER.info('Disconnecting from gateway')
        self.protocol.transport.close()
        self.protocol = None

    def send(self, message):
        """"""Write a message to the gateway.""""""
        if not message or not self.protocol or (not self.protocol.transport):
            return
        if not self.can_log:
            _LOGGER.debug('Sending %s', message.strip())
        try:
            self.protocol.transport.write(message.encode())
        except OSError as exc:
            _LOGGER.error('Failed writing to transport %s: %s', self.protocol.transport, exc)
            self.protocol.transport.close()
            self.protocol.conn_lost_callback()","class Transport:
    '''Handle gateway transport.
    I/O is allowed in this class. This class should host methods that
    are related to the gateway transport type.
    '''
    def __init__(self, gateway, connect, timeout=1.0, reconnect_timeout=10.0, **kwargs):
        
        pass
    def disconnect(self):
        '''Disconnect from the transport.'''
        pass
    def send(self, message):
        '''Write a message to the gateway.'''
        pass",snippet_222,4,"class Transport:
    '''Handle gateway transport.
    I/O is allowed in this class. This class should host methods that
    are related to the gateway transport type.
    '''
    def __init__(self, gateway, connect, timeout=1.0, reconnect_timeout=10.0, **kwargs):
        import threading, time
        self._time = time
        self._lock = threading.Lock()
        self._factory = gateway if callable(gateway) else None
        self._conn = None if callable(gateway) else gateway
        self._timeout = float(timeout) if timeout is not None else None
        self._reconnect_timeout = float(reconnect_timeout) if reconnect_timeout is not None else 0.0
        self._connect_kwargs = dict(kwargs) if kwargs else {}
        self._closed = False
        self._connected = False
        self._last_error = None

        if connect:
            self._connect()

    def _set_timeout(self, conn):
        if conn is None:
            return
        # Socket-like
        settimeout = getattr(conn, ""settimeout"", None)
        if callable(settimeout) and self._timeout is not None:
            try:
                settimeout(self._timeout)
                return
            except Exception:
                pass
        # Attribute-style
        if self._timeout is not None and hasattr(conn, ""timeout""):
            try:
                setattr(conn, ""timeout"", self._timeout)
            except Exception:
                pass

    def _instantiate(self):
        if self._factory is None:
            return self._conn
        return self._factory(**self._connect_kwargs)

    def _connect_call(self, conn):
        connect = getattr(conn, ""connect"", None)
        if callable(connect):
            try:
                if self._connect_kwargs:
                    connect(**self._connect_kwargs)
                else:
                    connect()
            except TypeError:
                # Fallback to no-kwargs if signature mismatch
                connect()

    def _is_open(self, conn):
        if conn is None:
            return False
        # Try common indicators
        closed = getattr(conn, ""closed"", None)
        if isinstance(closed, bool):
            return not closed
        # Some transports expose is_connected or similar
        for attr in (""is_connected"", ""connected"", ""open""):
            val = getattr(conn, attr, None)
            if isinstance(val, bool):
                return bool(val)
            if callable(val):
                try:
                    return bool(val())
                except Exception:
                    pass
        # Otherwise assume it's open if we have an object and not explicitly closed
        return True

    def _connect(self):
        if self._closed:
            raise RuntimeError(""Transport is closed"")
        if self._connected and self._is_open(self._conn):
            return

        conn = self._conn
        if conn is None:
            conn = self._instantiate()

        self._set_timeout(conn)
        try:
            self._connect_call(conn)
        except Exception as e:
            self._last_error = e
            raise

        self._conn = conn
        self._connected = self._is_open(conn)

    def disconnect(self):
        '''Disconnect from the transport.'''
        with self._lock:
            conn = self._conn
            self._closed = True
            self._connected = False
            self._last_error = None
            self._conn = None
        # Perform close outside lock to avoid blocking other operations
        if conn is not None:
            # Try common close methods
            for method_name in (""disconnect"", ""close"", ""shutdown""):
                m = getattr(conn, method_name, None)
                if callable(m):
                    try:
                        # shutdown may require a how arg; try without
                        m()
                    except TypeError:
                        # For sockets: shutdown(2) is SHUT_RDWR; avoid importing socket
                        try:
                            m(2)
                        except Exception:
                            pass
                    except Exception:
                        pass
                    break

    def _encode(self, message):
        if message is None:
            return b""""
        if isinstance(message, (bytes, bytearray, memoryview)):
            b = bytes(message)
        elif isinstance(message, str):
            b = message.encode(""utf-8"")
        else:
            # Try to stringify
            b = str(message).encode(""utf-8"")
        return b

    def _write_once(self, conn, data):
        # Try common write/send methods. Return number of bytes sent or raise.
        if conn is None:
            raise RuntimeError(""No connection available"")

        # Prefer sendall if available (ensures full write)
        sendall = getattr(conn, ""sendall"", None)
        if callable(sendall):
            sendall(data)
            return len(data)

        # Fallback to send
        send = getattr(conn, ""send"", None)
        if callable(send):
            n = send(data)
            if n is None:
                # Some implementations return None on success for full length
                return len(data)
            return int(n)

        # File-like write
        write = getattr(conn, ""write"", None)
        if callable(write):
            n = write(data)
            if n is None:
                return len(data)
            return int(n)

        # Queue-like put
        put = getattr(conn, ""put"", None)
        if callable(put):
            put(data)
            return len(data)

        raise AttributeError(""Underlying gateway does not support sending/writing"")

    def send(self, message):
        '''Write a message to the gateway.'''
        data = self._encode(message)
        deadline = self._time.monotonic() + max(self._reconnect_timeout, 0.0)
        attempt = 0
        last_exc = None

        while True:
            with self._lock:
                if not self._connected or not self._is_open(self._conn):
                    try:
                        self._connect()
                    except Exception as e:
                        last_exc = e
                        self._connected = False
                        self._conn = self._conn  # keep last for possible retries

                if self._connected and self._conn is not None:
                    try:
                        sent = self._write_once(self._conn, data)
                        return sent
                    except Exception as e:
                        last_exc = e
                        self._connected = False

            # Outside lock: backoff and retry until deadline
            attempt += 1
            now = self._time.monotonic()
            if now >= deadline:
                if last_exc:
                    raise last_exc
                raise TimeoutError(""Failed to send before reconnect timeout"")
            # simple backoff
            remaining = max(0.0, deadline - now)
            self._time.sleep(min(0.1 * attempt, 0.5, remaining))"
298148,edx/event-tracking,edx_event-tracking/eventtracking/backends/logger.py,eventtracking.backends.logger.LoggerBackend,"import logging
import json

class LoggerBackend:
    """"""
    Event tracker backend that uses a python logger.

    Events are logged to the INFO level as JSON strings.
    """"""

    def __init__(self, **kwargs):
        """"""
        Event tracker backend that uses a python logger.

        `name` is an identifier for the logger, which should have
            been configured using the default python mechanisms.
        """"""
        name = kwargs.get('name', None)
        self.max_event_size = kwargs.get('max_event_size', MAX_EVENT_SIZE)
        self.event_logger = logging.getLogger(name)
        level = kwargs.get('level', 'info')
        self.log = getattr(self.event_logger, level.lower())

    def send(self, event):
        """"""Send the event to the standard python logger""""""
        event_str = json.dumps(event, cls=DateTimeJSONEncoder)
        if self.max_event_size is None or len(event_str) <= self.max_event_size:
            self.log(event_str)","
class LoggerBackend:
    '''
    Event tracker backend that uses a python logger.
    Events are logged to the INFO level as JSON strings.
    '''
    def __init__(self, **kwargs):
        '''
        Event tracker backend that uses a python logger.
        `name` is an identifier for the logger, which should have
            been configured using the default python mechanisms.
        '''
        pass
    def send(self, event):
        '''Send the event to the standard python logger'''
        pass",snippet_223,3,"import logging
import json
from typing import Any


class LoggerBackend:
    '''
    Event tracker backend that uses a python logger.
    Events are logged to the INFO level as JSON strings.
    '''
    def __init__(self, **kwargs):
        '''
        Event tracker backend that uses a python logger.
        `name` is an identifier for the logger, which should have
            been configured using the default python mechanisms.
        '''
        logger = kwargs.get('logger')
        if logger is not None:
            self._logger = logger
        else:
            name = kwargs.get('name', 'event_tracker')
            self._logger = logging.getLogger(name)

    def _json_default(self, obj: Any) -> str:
        try:
            return obj.__dict__
        except Exception:
            return str(obj)

    def send(self, event):
        '''Send the event to the standard python logger'''
        try:
            payload = json.dumps(event, default=self._json_default, ensure_ascii=False)
        except Exception:
            payload = json.dumps({""event"": str(event)}, ensure_ascii=False)
        self._logger.info(payload)"
303020,UpCloudLtd/upcloud-python-api,UpCloudLtd_upcloud-python-api/upcloud_api/cloud_manager/host_mixin.py,upcloud_api.cloud_manager.host_mixin.HostManager,"from upcloud_api.api import API
from upcloud_api.host import Host

class HostManager:
    """"""
    Functions for managing hosts. Intended to be used as a mixin for CloudManager.
    """"""
    api: API

    def get_hosts(self):
        """"""
        Returns a list of available hosts, along with basic statistics of them when available.
        """"""
        url = '/host'
        res = self.api.get_request(url)
        return [Host(**host) for host in res['hosts']['host']]

    def get_host(self, id: str) -> Host:
        """"""
        Returns detailed information about a specific host.
        """"""
        url = f'/host/{id}'
        res = self.api.get_request(url)
        return Host(**res['host'])

    def modify_host(self, host: str, description: str) -> Host:
        """"""
        Modifies description of a specific host.
        """"""
        url = f'/host/{host}'
        body = {'host': {'description': description}}
        res = self.api.patch_request(url, body)
        return Host(**res['host'])","
class HostManager:
    
    def get_hosts(self):
        
        pass
    def get_hosts(self):
        
        pass
    def modify_host(self, host: str, description: str) -> Host:
        '''
        Modifies description of a specific host.
        '''
        pass",snippet_224,4,"from dataclasses import dataclass
from typing import Dict, List, Iterable, Optional


@dataclass
class Host:
    name: str
    description: str = """"


class HostManager:
    def __init__(self, hosts: Optional[Iterable[Host]] = None):
        self._hosts: Dict[str, Host] = {}
        if hosts:
            for h in hosts:
                if not isinstance(h, Host):
                    raise TypeError(""hosts must be an iterable of Host instances"")
                self._hosts[h.name] = Host(name=h.name, description=h.description)

    def get_hosts(self):
        return list(self._hosts.values())

    def get_hosts(self):
        return list(self._hosts.values())

    def modify_host(self, host: str, description: str) -> Host:
        '''
        Modifies description of a specific host.
        '''
        if not isinstance(host, str) or not host:
            raise ValueError(""host must be a non-empty string"")
        if not isinstance(description, str):
            raise TypeError(""description must be a string"")
        try:
            h = self._hosts[host]
        except KeyError as e:
            raise KeyError(f""Host '{host}' not found"") from e
        h.description = description
        return h"
304970,threeML/astromodels,threeML_astromodels/astromodels/functions/template_model.py,astromodels.functions.template_model.TemplateFile,"import numpy as np
import h5py
from dataclasses import dataclass
import collections
from typing import Dict, List, Optional, Union

@dataclass
class TemplateFile:
    """"""
    simple container to read and write
    the data to an hdf5 file

    """"""
    name: str
    description: str
    grid: np.ndarray
    parameters: Dict[str, np.ndarray]
    parameter_order: List[str]
    energies: np.ndarray
    interpolation_degree: int
    spline_smoothing_factor: float

    def save(self, file_name: str):
        """"""
        serialize the contents to a file

        :param file_name:
        :type file_name: str
        :returns:

        """"""
        with h5py.File(file_name, 'w') as f:
            f.attrs['name'] = self.name
            f.attrs['description'] = self.description
            f.attrs['interpolation_degree'] = self.interpolation_degree
            f.attrs['spline_smoothing_factor'] = self.spline_smoothing_factor
            f.create_dataset('energies', data=self.energies, compression='gzip')
            f.create_dataset('grid', data=self.grid, compression='gzip')
            dt = h5py.special_dtype(vlen=str)
            po = np.array(self.parameter_order, dtype=dt)
            f.create_dataset('parameter_order', data=po)
            par_group = f.create_group('parameters')
            for k in self.parameter_order:
                par_group.create_dataset(k, data=self.parameters[k], compression='gzip')

    @classmethod
    def from_file(cls, file_name: str):
        """"""
        read contents from a file

        :param cls:
        :type cls:
        :param file_name:
        :type file_name: str
        :returns:

        """"""
        with h5py.File(file_name, 'r') as f:
            name = f.attrs['name']
            description = f.attrs['description']
            interpolation_degree = f.attrs['interpolation_degree']
            spline_smoothing_factor = f.attrs['spline_smoothing_factor']
            energies = f['energies'][()]
            parameter_order = f['parameter_order'][()]
            grid = f['grid'][()]
            parameters = collections.OrderedDict()
            for k in parameter_order:
                parameters[k] = f['parameters'][k][()]
        return cls(name=name, description=description, interpolation_degree=interpolation_degree, spline_smoothing_factor=spline_smoothing_factor, energies=energies, parameter_order=parameter_order, parameters=parameters, grid=grid)","@dataclass
class TemplateFile:
    
    def save(self, file_name: str):
        
        pass
    @classmethod
    def from_file(cls, file_name: str):
        '''
        read contents from a file
        :param cls:
        :type cls:
        :param file_name:
        :type file_name: str
        :returns:
        '''
        pass",snippet_227,3,"from dataclasses import dataclass
from pathlib import Path

@dataclass
class TemplateFile:
    content: str = """"

    def save(self, file_name: str):
        path = Path(file_name)
        if path.parent:
            path.parent.mkdir(parents=True, exist_ok=True)
        path.write_text(self.content, encoding=""utf-8"")

    @classmethod
    def from_file(cls, file_name: str):
        '''
        read contents from a file
        :param cls:
        :type cls:
        :param file_name:
        :type file_name: str
        :returns:
        '''
        content = Path(file_name).read_text(encoding=""utf-8"")
        return cls(content=content)"
305605,NASA-AMMOS/AIT-Core,ait/core/tlm.py,core.tlm.PacketContext,"class PacketContext:
    """"""PacketContext

    A PacketContext provides a simple wrapper around a Packet so that
    field accesses of the form:

        packet.fieldname

    may also be specified as:

        packet[fieldname]

    This latter syntax allows a PacketContext to be used as a symbol
    table when evaluating PacketExpressions.
    """"""
    __slots__ = ['_packet']

    def __init__(self, packet):
        """"""Creates a new PacketContext for the given Packet.""""""
        self._packet = packet

    def __getitem__(self, name):
        """"""Returns packet[name]""""""
        result = None
        if self._packet._hasattr(name):
            result = self._packet._getattr(name)
        else:
            msg = ""Packet '%s' has no field '%s'""
            values = (self._packet._defn.name, name)
            raise KeyError(msg % values)
        return result","class PacketContext:
    
    def __init__(self, packet):
        '''Creates a new PacketContext for the given Packet.'''
        pass
    def __getitem__(self, name):
        
        pass",snippet_229,3,"class PacketContext:
    
    def __init__(self, packet):
        '''Creates a new PacketContext for the given Packet.'''
        self._packet = packet

    def __getitem__(self, name):
        if name in ('packet', '_packet', 'self', 'this'):
            return self._packet

        current = self._packet
        parts = name.split('.') if isinstance(name, str) else [name]

        for part in parts:
            if isinstance(current, dict):
                if part in current:
                    current = current[part]
                else:
                    raise KeyError(name)
            elif isinstance(current, (list, tuple)) and isinstance(part, str) and part.isdigit():
                idx = int(part)
                try:
                    current = current[idx]
                except (IndexError, TypeError):
                    raise KeyError(name)
            else:
                if hasattr(current, part):
                    current = getattr(current, part)
                else:
                    raise KeyError(name)

        return current"
305610,NASA-AMMOS/AIT-Core,ait/core/tlm.py,core.tlm.RawPacket,"class RawPacket:
    """"""RawPacket

    Wraps a packet such that:

        packet.raw.fieldname

    returns the value of fieldname as a raw value with no enumeration
    substitutions or DN to EU conversions applied.
    """"""
    __slots__ = ['_packet']

    def __init__(self, packet):
        """"""Creates a new RawPacket based on the given Packet.""""""
        self._packet = packet

    def __getattr__(self, fieldname):
        """"""Returns the value of the given packet fieldname as a raw
        value with no DN to EU conversion applied.
        """"""
        return self._packet._getattr(fieldname, raw=True)","class RawPacket:
    
    def __init__(self, packet):
        
        pass
    def __getattr__(self, fieldname):
        '''Returns the value of the given packet fieldname as a raw
        value with no DN to EU conversion applied.
        '''
        pass",snippet_230,3,"class RawPacket:
    
    def __init__(self, packet):
        self._packet = packet

    def __getattr__(self, fieldname):
        '''Returns the value of the given packet fieldname as a raw
        value with no DN to EU conversion applied.
        '''
        pkt = self._packet

        # 1) Dedicated raw accessors/mappings
        if hasattr(pkt, 'get_raw') and callable(getattr(pkt, 'get_raw')):
            return pkt.get_raw(fieldname)
        if hasattr(pkt, 'getRaw') and callable(getattr(pkt, 'getRaw')):
            return pkt.getRaw(fieldname)
        if hasattr(pkt, 'raw'):
            raw_attr = getattr(pkt, 'raw')
            try:
                return raw_attr[fieldname]
            except Exception:
                pass

        # 2) Common field containers
        for container_name in ('fields', 'field_map', 'field', 'attributes'):
            if hasattr(pkt, container_name):
                container = getattr(pkt, container_name)
                # Mapping-like container
                try:
                    fld = container[fieldname]
                    # Try common raw attributes on the field
                    for raw_name in ('raw', 'raw_value', 'dn', 'value_dn'):
                        if hasattr(fld, raw_name):
                            return getattr(fld, raw_name)
                    return fld
                except Exception:
                    pass

        # 3) Attribute on packet that may itself expose a raw value
        if hasattr(pkt, fieldname):
            fld = getattr(pkt, fieldname)
            for raw_name in ('raw', 'raw_value', 'dn', 'value_dn'):
                if hasattr(fld, raw_name):
                    return getattr(fld, raw_name)
            return fld

        # 4) Item access on packet
        if hasattr(pkt, '__getitem__'):
            try:
                fld = pkt[fieldname]
                for raw_name in ('raw', 'raw_value', 'dn', 'value_dn'):
                    if hasattr(fld, raw_name):
                        return getattr(fld, raw_name)
                return fld
            except Exception:
                pass

        raise AttributeError(f""{type(self).__name__} has no field '{fieldname}' and underlying packet does not expose a raw value for it"")"
305613,NASA-AMMOS/AIT-Core,ait/core/tlm.py,core.tlm.WordArray,"class WordArray:
    """"""WordArrays are somewhat analogous to Python bytearrays, but
    currently much more limited in functionality.  They provide a
    readonly view of a bytearray addressable and iterable as a sequence
    of 16-bit words.  This is convenient for telemetry processing as
    packets are often more naturally addressable on word, as opposed to
    byte, boundaries.
    """"""
    __slots__ = ['_bytes']

    def __init__(self, bytes):
        """"""Creates a new wordarray from the given bytearray.

        The given bytearray should contain an even number of bytes.  If
        odd, the last byte is ignored.
        """"""
        self._bytes = bytes

    def __getitem__(self, key):
        """"""Returns the words in this wordarray at the given Python slice
        or word at the given integer index.""""""
        length = len(self)
        if isinstance(key, slice):
            return [self[n] for n in range(*key.indices(length))]
        elif isinstance(key, int):
            if key < 0:
                key += length
            if key >= length:
                msg = 'wordarray index (%d) is out of range [0 %d].'
                raise IndexError(msg % (key, length - 1))
            index = 2 * key
            return self._bytes[index] << 8 | self._bytes[index + 1]
        else:
            raise TypeError('wordarray indices must be integers.')

    def __len__(self):
        """"""Returns the number of words in this wordarray.""""""
        return len(self._bytes) / 2","class WordArray:
    '''WordArrays are somewhat analogous to Python bytearrays, but
    currently much more limited in functionality.  They provide a
    readonly view of a bytearray addressable and iterable as a sequence
    of 16-bit words.  This is convenient for telemetry processing as
    packets are often more naturally addressable on word, as opposed to
    byte, boundaries.
    '''
    def __init__(self, bytes):
        '''Creates a new wordarray from the given bytearray.
        The given bytearray should contain an even number of bytes.  If
        odd, the last byte is ignored.
        '''
        pass
    def __getitem__(self, key):
        '''Returns the words in this wordarray at the given Python slice
        or word at the given integer index.'''
        pass
    def __len__(self):
        
        pass",snippet_231,4,"class WordArray:
    '''WordArrays are somewhat analogous to Python bytearrays, but
    currently much more limited in functionality.  They provide a
    readonly view of a bytearray addressable and iterable as a sequence
    of 16-bit words.  This is convenient for telemetry processing as
    packets are often more naturally addressable on word, as opposed to
    byte, boundaries.
    '''
    def __init__(self, bytes_):
        '''Creates a new wordarray from the given bytearray.
        The given bytearray should contain an even number of bytes.  If
        odd, the last byte is ignored.
        '''
        if not isinstance(bytes_, (bytes, bytearray, memoryview)):
            raise TypeError(""bytes must be a bytes-like object"")
        mv = memoryview(bytes_)
        # Cast to bytes for immutability/read-only behavior
        self._bytes = bytes(mv.tobytes())
        self._nwords = len(self._bytes) // 2  # ignore last odd byte if present

    def __getitem__(self, key):
        '''Returns the words in this wordarray at the given Python slice
        or word at the given integer index.'''
        if isinstance(key, slice):
            indices = range(*key.indices(len(self)))
            return [self[i] for i in indices]
        # integer index
        n = len(self)
        if key < 0:
            key += n
        if key < 0 or key >= n:
            raise IndexError(""WordArray index out of range"")
        i = key * 2
        b1 = self._bytes[i]
        b2 = self._bytes[i + 1]
        return (b1 << 8) | b2  # big-endian 16-bit word

    def __len__(self):
        return self._nwords"
306020,ihmeuw/vivarium,ihmeuw_vivarium/src/vivarium/framework/lookup/interpolation.py,vivarium.framework.lookup.interpolation.Order0Interp,"import numpy as np
import pandas as pd
from collections.abc import Hashable, Sequence

class Order0Interp:
    """"""A callable that returns the result of order 0 interpolation over input data.

    Attributes
    ----------
    data
        The data from which to build the interpolation.
    value_columns
        Columns to be interpolated.
    extrapolate
        Whether or not to extrapolate beyond the edge of supplied bins.
    parameter_bins
        A dictionary where they keys are a tuple of the form
        (column name used in call, column name for left bin edge, column name for right bin edge)
        and the values are dictionaries of the form {""bins"": [ordered left edges of bins],
        ""max"": max right edge (used when extrapolation not allowed)}.

    """"""

    def __init__(self, data: pd.DataFrame, continuous_parameters: Sequence[Sequence[str]], value_columns: list[str], extrapolate: bool, validate: bool):
        """"""
        Parameters
        ----------
        data
            Data frame used to build interpolation.
        continuous_parameters
            Parameter columns. Should be of form (column name used in call,
            column name for left bin edge, column name for right bin edge)
            or column name. Assumes left bin edges are inclusive and
            right exclusive.
        value_columns
            Columns to be interpolated.
        extrapolate
            Whether or not to extrapolate beyond the edge of supplied bins.
        validate
            Whether or not to validate the data.
        """"""
        if validate:
            check_data_complete(data, continuous_parameters)
        self.data = data.copy()
        self.value_columns = value_columns
        self.extrapolate = extrapolate
        self.parameter_bins = {}
        for p in continuous_parameters:
            left_edge = self.data[p[1]].drop_duplicates().sort_values()
            max_right = self.data[p[2]].drop_duplicates().max()
            self.parameter_bins[tuple(p)] = {'bins': left_edge.reset_index(drop=True), 'max': max_right}

    def __call__(self, interpolants: pd.DataFrame) -> pd.DataFrame:
        """"""Find the bins for each parameter for each interpolant in interpolants
        and return the values from data there.

        Parameters
        ----------
        interpolants
            Data frame containing the parameters to interpolate..

        Returns
        -------
            A table with the interpolated values for the given interpolants.
        """"""
        interpolant_bins = pd.DataFrame(index=interpolants.index)
        merge_cols = []
        for cols, d in self.parameter_bins.items():
            bins = d['bins']
            max_right = d['max']
            merge_cols.append(cols[1])
            interpolant_col = interpolants[cols[0]]
            if not self.extrapolate and (interpolant_col.min() < bins[0] or interpolant_col.max() >= max_right):
                raise ValueError(f'Extrapolation outside of bins used to set up interpolation is only allowed when explicitly set in creation of Interpolation. Extrapolation is currently off for this interpolation, and parameter {cols[0]} includes data outside of original bins.')
            bin_indices = np.digitize(interpolant_col, bins.tolist())
            bin_indices[bin_indices > 0] -= 1
            interpolant_bins[cols[1]] = bins.loc[bin_indices].values
        index = interpolant_bins.index
        interp_vals = interpolant_bins.merge(self.data, how='left', on=merge_cols).set_index(index)
        return interp_vals[self.value_columns]","
class Order0Interp:
    '''A callable that returns the result of order 0 interpolation over input data.
    Attributes
    ----------
    data
        The data from which to build the interpolation.
    value_columns
        Columns to be interpolated.
    extrapolate
        Whether or not to extrapolate beyond the edge of supplied bins.
    parameter_bins
        A dictionary where they keys are a tuple of the form
        (column name used in call, column name for left bin edge, column name for right bin edge)
        and the values are dictionaries of the form {""bins"": [ordered left edges of bins],
        ""max"": max right edge (used when extrapolation not allowed)}.
    '''
    def __init__(self, data: pd.DataFrame, continuous_parameters: Sequence[Sequence[str]], value_columns: list[str], extrapolate: bool, validate: bool):
        
        pass
    def __call__(self, interpolants: pd.DataFrame) -> pd.DataFrame:
        '''Find the bins for each parameter for each interpolant in interpolants
        and return the values from data there.
        Parameters
        ----------
        interpolants
            Data frame containing the parameters to interpolate..
        Returns
        -------
            A table with the interpolated values for the given interpolants.
        '''
        pass",snippet_232,3,"import pandas as pd
import numpy as np
from typing import Sequence, Tuple, Dict, Any


class Order0Interp:
    '''A callable that returns the result of order 0 interpolation over input data.
    Attributes
    ----------
    data
        The data from which to build the interpolation.
    value_columns
        Columns to be interpolated.
    extrapolate
        Whether or not to extrapolate beyond the edge of supplied bins.
    parameter_bins
        A dictionary where they keys are a tuple of the form
        (column name used in call, column name for left bin edge, column name for right bin edge)
        and the values are dictionaries of the form {""bins"": [ordered left edges of bins],
        ""max"": max right edge (used when extrapolation not allowed)}.
    '''
    def __init__(self, data: pd.DataFrame, continuous_parameters: Sequence[Sequence[str]], value_columns: list[str], extrapolate: bool, validate: bool):
        self.data = data.copy()
        self.value_columns = list(value_columns)
        self.extrapolate = bool(extrapolate)
        self.parameter_bins: Dict[Tuple[str, str, str], Dict[str, Any]] = {}

        for col in self.value_columns:
            if col not in self.data.columns:
                raise ValueError(f""value column {col} not found in data"")

        for triple in continuous_parameters:
            if len(triple) != 3:
                raise ValueError(""Each continuous parameter must be a sequence of three strings: (call_name, left_edge_col, right_edge_col)"")
            call_name, left_col, right_col = triple
            if left_col not in self.data.columns or right_col not in self.data.columns:
                raise ValueError(f""Bin edge columns {left_col} or {right_col} not found in data"")

            unique_bins = self.data[[left_col, right_col]].drop_duplicates().sort_values([left_col, right_col], kind=""mergesort"")
            # Keep only the minimal right for each left in case of duplicates
            unique_bins = unique_bins.groupby(left_col, as_index=False)[right_col].min().sort_values(left_col, kind=""mergesort"")

            lefts = unique_bins[left_col].to_numpy()
            rights = unique_bins[right_col].to_numpy()

            if validate:
                if not np.all(np.isfinite(lefts)) or not np.all(np.isfinite(rights)):
                    raise ValueError(f""Non-finite bin edges detected for parameter {call_name}"")
                if not np.all(lefts[:-1] <= lefts[1:]):
                    raise ValueError(f""Left edges must be non-decreasing for parameter {call_name}"")
                if not np.all(rights > lefts):
                    raise ValueError(f""Right edge must be greater than left edge for each bin of parameter {call_name}"")
                # Ensure non-overlapping and sorted: lefts should be strictly increasing or equal to previous right
                if len(lefts) > 1:
                    if not np.all(lefts[1:] >= rights[:-1]):
                        raise ValueError(f""Bins must be non-overlapping and ordered for parameter {call_name}"")

            bins_sorted = list(map(lambda x: x.item() if hasattr(x, ""item"") else x, lefts))
            max_right = rights.max() if len(rights) else None
            if max_right is None:
                raise ValueError(f""No bins available for parameter {call_name}"")

            self.parameter_bins[(call_name, left_col, right_col)] = {""bins"": bins_sorted, ""max"": max_right}

    def __call__(self, interpolants: pd.DataFrame) -> pd.DataFrame:
        work = interpolants.copy()
        order_col = ""_order__""
        while order_col in work.columns or order_col in self.data.columns:
            order_col += ""_""
        work[order_col] = np.arange(len(work), dtype=np.int64)

        merge_keys = []
        for (call_name, left_col, right_col), meta in self.parameter_bins.items():
            if call_name not in work.columns:
                raise ValueError(f""Interpolants missing required column {call_name}"")
            values = work[call_name].to_numpy()
            bins = np.array(meta[""bins""])
            max_right = meta[""max""]

            # Find bin index using right-open intervals [left, right)
            idx = np.searchsorted(bins, values, side=""right"") - 1

            if self.extrapolate:
                idx = np.clip(idx, 0, len(bins) - 1)
            else:
                if len(bins) == 0:
                    raise ValueError(""No bins available"")
                too_low = values < bins[0]
                too_high = values >= max_right
                if np.any(too_low) or np.any(too_high):
                    raise ValueError(f""Value(s) for {call_name} outside bin range and extrapolation is disabled"")

            idx = np.clip(idx, 0, len(bins) - 1)
            chosen_lefts = bins[idx]

            # Determine matching right edges from data by left edge
            left_to_right = (
                self.data[[left_col, right_col]]
                .drop_duplicates()
                .sort_values([left_col, right_col], kind=""mergesort"")
                .groupby(left_col, as_index=True)[right_col]
                .min()
            )
            chosen_rights = left_to_right.reindex(chosen_lefts).to_numpy()

            work[left_col] = chosen_lefts
            work[right_col] = chosen_rights
            merge_keys.extend([left_col, right_col])

        # Perform merge
        cols_needed = list(dict.fromkeys(merge_keys + self.value_columns))
        merged = work.merge(self.data[cols_needed], on=merge_keys, how=""left"", sort=False)

        merged = merged.sort_values(order_col, kind=""mergesort"")
        result = merged[self.value_columns].reset_index(drop=True)
        return result"
306151,lago-project/lago,lago-project_lago/lago/templates.py,lago.templates.Template,"class Template:
    """"""
    Disk image template class

    Attributes:
        name (str): Name of this template
        _versions (dict(str:TemplateVersion)): versions for this template
    """"""

    def __init__(self, name, versions):
        """"""
        Args:
            name (str): Name of the template
            versions (dict(str:TemplateVersion)): dictionary with the
                version_name: :class:`TemplateVersion` pairs for this template
        """"""
        self.name = name
        self._versions = versions

    def get_version(self, ver_name=None):
        """"""
        Get the given version for this template, or the latest

        Args:
            ver_name (str or None): Version to retieve, None for the latest

        Returns:
            TemplateVersion: The version matching the given name or the latest
                one
        """"""
        if ver_name is None:
            return self.get_latest_version()
        return self._versions[ver_name]

    def get_latest_version(self):
        """"""
        Retrieves the latest version for this template, the latest being the
        one with the newest timestamp

        Returns:
            TemplateVersion
        """"""
        return max(self._versions.values(), key=lambda x: x.timestamp())","class Template:
    
    def __init__(self, name, versions):
        
        pass
    def get_version(self, ver_name=None):
        '''
        Get the given version for this template, or the latest
        Args:
            ver_name (str or None): Version to retieve, None for the latest
        Returns:
            TemplateVersion: The version matching the given name or the latest
                one
        '''
        pass
    def get_latest_version(self):
        
        pass",snippet_233,4,"class Template:
    def __init__(self, name, versions):
        self.name = name
        # Normalize versions into an ordered dict: name -> version_obj
        self._versions = {}
        if versions is None:
            versions = []
        if isinstance(versions, dict):
            for k, v in versions.items():
                self._versions[str(k)] = v
        else:
            for v in versions:
                key = None
                # Try to infer a version name from common attributes or the value itself
                for attr in (""name"", ""version"", ""ver_name"", ""id""):
                    if hasattr(v, attr):
                        key = getattr(v, attr)
                        break
                if key is None:
                    key = str(v)
                self._versions[str(key)] = v

    def _parse_semver_like(self, s):
        # Strip common prefixes (e.g., 'v1.2.3') and suffixes
        s = str(s).strip()
        if s.startswith((""v"", ""V"")):
            s = s[1:]
        # Split by dots, dashes; keep only numeric leading parts
        parts = []
        for token in s.replace(""-"", ""."").split("".""):
            if token.isdigit():
                parts.append(int(token))
            else:
                # Stop at first non-numeric token; semantic versions beyond this are ignored
                break
        if not parts:
            raise ValueError(""not parseable"")
        return tuple(parts)

    def _latest_by_semver_or_insertion(self):
        if not self._versions:
            return None
        keys = list(self._versions.keys())
        parsed = {}
        for k in keys:
            try:
                parsed[k] = self._parse_semver_like(k)
            except Exception:
                parsed = None
                break
        if parsed is None or not parsed:
            # Fallback to insertion order: pick last inserted
            last_key = next(reversed(self._versions))
            return self._versions[last_key]
        # Choose the key with the maximum semantic tuple; tie-breaker by key string
        best_key = max(parsed.keys(), key=lambda k: (parsed[k], k))
        return self._versions[best_key]

    def get_version(self, ver_name=None):
        '''
        Get the given version for this template, or the latest
        Args:
            ver_name (str or None): Version to retieve, None for the latest
        Returns:
            TemplateVersion: The version matching the given name or the latest
                one
        '''
        if ver_name is None:
            return self.get_latest_version()
        key = str(ver_name)
        if key not in self._versions:
            raise KeyError(f""Version '{ver_name}' not found for template '{self.name}'."")
        return self._versions[key]

    def get_latest_version(self):
        return self._latest_by_semver_or_insertion()"
311056,geronimp/graftM,geronimp_graftM/graftm/deduplicator.py,graftm.deduplicator.Deduplicator,"class Deduplicator:
    """"""Deduplicates sequences""""""

    def deduplicate(self, aligned_sequence_objects):
        """"""Sort the given aligned_sequence objects into an array of arrays,
        where input sequences are grouped iff they have the same sequence

        Parameters
        ----------
        aligned_sequence_objects: array of Sequence objects
            input sequences

        Returns
        -------
        Array of arrays of Sequence objects""""""
        sequence_to_groups = {}
        for s in aligned_sequence_objects:
            try:
                sequence_to_groups[s.seq].append(s)
            except KeyError:
                sequence_to_groups[s.seq] = [s]
        return list(sequence_to_groups.values())

    def lca_taxonomy(self, deduplicated_sequences, taxonomy_hash):
        """"""Given a set of deduplicated sequences and a taxonomy hash,
        return the respective LCAs of taxonomy

        Parameters
        ----------
        deduplicated_sequences: Array of arrays of Sequence objects
            as output from deduplicate()
        taxonomy_hash: dictionary 
            of sequence names to taxonomy array (i.e. array of str)

        Returns
        -------
        Array of taxonomy LCAs""""""
        to_return = []
        for dup_group in deduplicated_sequences:
            lca = taxonomy_hash[dup_group[0].name]
            for s in dup_group[1:]:
                for i, tax in enumerate(taxonomy_hash[s.name]):
                    if i >= len(lca) or tax != lca[i]:
                        lca = lca[:i]
                        break
                if len(lca) > len(taxonomy_hash[s.name]):
                    lca = lca[:len(taxonomy_hash[s.name])]
            to_return.append(lca)
        return to_return","class Deduplicator:
    '''Deduplicates sequences'''
    def deduplicate(self, aligned_sequence_objects):
        
        pass
    def lca_taxonomy(self, deduplicated_sequences, taxonomy_hash):
        '''Given a set of deduplicated sequences and a taxonomy hash,
        return the respective LCAs of taxonomy
        Parameters
        ----------
        deduplicated_sequences: Array of arrays of Sequence objects
            as output from deduplicate()
        taxonomy_hash: dictionary 
            of sequence names to taxonomy array (i.e. array of str)
        Returns
        -------
        Array of taxonomy LCAs'''
        pass",snippet_235,3,"class Deduplicator:
    '''Deduplicates sequences'''

    def _get_sequence_string(self, obj):
        # Try common attribute names for aligned sequence strings
        for attr in (
            'aligned_sequence',
            'aligned_seq',
            'alignment',
            'sequence',
            'seq',
            'sequence_string',
        ):
            if hasattr(obj, attr):
                val = getattr(obj, attr)
                if callable(val):
                    try:
                        val = val()
                    except TypeError:
                        pass
                if isinstance(val, (str, bytes)):
                    return val.decode() if isinstance(val, bytes) else val
        # Fallback to str
        return str(obj)

    def _get_name(self, obj):
        # Try common attribute names for identifiers
        for attr in ('name', 'id', 'header', 'label', 'identifier'):
            if hasattr(obj, attr):
                val = getattr(obj, attr)
                if callable(val):
                    try:
                        val = val()
                    except TypeError:
                        pass
                if isinstance(val, (str, bytes)):
                    return val.decode() if isinstance(val, bytes) else val
        # Fallback: try to avoid using the sequence string as name; use repr
        return repr(obj)

    def deduplicate(self, aligned_sequence_objects):
        from collections import OrderedDict

        groups = OrderedDict()
        for obj in aligned_sequence_objects:
            key = self._get_sequence_string(obj)
            if key not in groups:
                groups[key] = []
            groups[key].append(obj)
        return list(groups.values())

    def lca_taxonomy(self, deduplicated_sequences, taxonomy_hash):
        '''Given a set of deduplicated sequences and a taxonomy hash,
        return the respective LCAs of taxonomy
        Parameters
        ----------
        deduplicated_sequences: Array of arrays of Sequence objects
            as output from deduplicate()
        taxonomy_hash: dictionary 
            of sequence names to taxonomy array (i.e. array of str)
        Returns
        -------
        Array of taxonomy LCAs'''
        def lca(list_of_taxa_lists):
            if not list_of_taxa_lists:
                return []
            # Filter out empty taxonomies
            taxa = [t for t in list_of_taxa_lists if t]
            if not taxa:
                return []
            # Compute longest common prefix
            lcp = []
            for level_items in zip(*taxa):
                first = level_items[0]
                if all(item == first for item in level_items[1:]):
                    lcp.append(first)
                else:
                    break
            return lcp

        lcas = []
        for group in deduplicated_sequences:
            taxa_lists = []
            for obj in group:
                name = self._get_name(obj)
                if name in taxonomy_hash:
                    taxa = taxonomy_hash[name]
                    # Ensure list-like of strings
                    if taxa is None:
                        continue
                    if isinstance(taxa, (list, tuple)):
                        taxa_lists.append(list(taxa))
                    else:
                        # If given as string, convert to single-element taxonomy
                        taxa_lists.append([str(taxa)])
            lcas.append(lca(taxa_lists))
        return lcas"
311087,geronimp/graftM,geronimp_graftM/graftm/search_table.py,graftm.search_table.SearchTableWriter,"import logging
from graftm.sequence_search_results import SequenceSearchResult

class SearchTableWriter:
    """"""
    Class for writing the search output OTU table. Basically a summary
    of hits to the HMM/Diamond searched in the following format:

             #ID    Metagenome_1    Metagenome_2    ...
            HMM1    50              6
            HMM2    195             41
            HMM3    2               20120
            ...

    You just need to specify a series of SequenceSearchResult objects, and an
    output path.
    """"""

    def _interpret_hits(self, results_list, base_list):
        """"""Sort reads that hit multiple HMMs to the databases to which they had
        the highest bit score. Return a dictionary containing HMMs as keys, and
        number of hits as the values.

        This function is set up so that the read names could easily be returned
        instead of numbers, for future development of GraftM

        Parameters
        ----------
        results_list: list
            Iterable if SequenceSearchResult objects. e.g.
                [SequenceSearchResult_1, SequenceSearchResult_2, ...]

        base_list: list
            Iterable of the basenames for each sequence file provided to graftM
            e.g.
                [sample_1, sample_2, ...]

        Returns
        -------
        dictionary:
            Contains samples as entries. The value for each sample is another
            dictionary with HMM as the key, and number of hits as values:
                {""sample_1"":{HMM_1: 12
                             HMM_2: 35
                             HMM_3: 1258
                             ...}
                 ...
                }

        """"""
        logging.debug('Sorting reads into HMMs by bit score')
        run_results = {}
        for base, results in zip(base_list, results_list):
            search_results = {}
            for search in results():
                search_list = list(search.each([SequenceSearchResult.QUERY_ID_FIELD, SequenceSearchResult.ALIGNMENT_BIT_SCORE, SequenceSearchResult.HMM_NAME_FIELD]))
                for hit in search_list:
                    if hit[0] in search_results:
                        if float(hit[1]) > search_results[hit[0]][0]:
                            search_results[hit[0]] = [float(hit[1]), hit[2]]
                    else:
                        search_results[hit[0]] = [float(hit[1]), hit[2]]
            run_results[base] = search_results
        db_count = {}
        for run in run_results.keys():
            run_count = {}
            for entry in list(run_results[run].values()):
                key = entry[1]
                if key in run_count:
                    run_count[key] += 1
                else:
                    run_count[key] = 1
            db_count[run] = run_count
        return db_count

    def _write_results(self, db_count, output_path):
        """"""Write the table to the output_path directory

        db_count: dict
            Contains samples as entries. The value for each sample is another
            dictionary with HMM as the key, and number of hits as values:
                {""sample_1"":{HMM_1: 12
                             HMM_2: 35
                             HMM_3: 1258
                             ...}
                 ...
                }

        output_path: str
            Path to output file to which the resultant output file will be
            written to.
        """"""
        logging.debug('Writing search otu table to file: %s' % output_path)
        output_dict = {}
        for idx, value_dict in enumerate(db_count.values()):
            for database, count in value_dict.items():
                if database in output_dict:
                    output_dict[database].append(str(count))
                else:
                    output_dict[database] = ['0'] * idx + [str(count)]
            for key, item in output_dict.items():
                if len(item) == idx:
                    output_dict[key].append('0')
        with open(output_path, 'w') as out:
            out.write('\t'.join(['#ID'] + list(db_count.keys())) + '\n')
            for key, item in output_dict.items():
                out.write('%s\t%s' % (key, '\t'.join(item)) + '\n')

    def build_search_otu_table(self, search_results_list, base_list, output_path):
        """"""
        Build an OTU from SequenceSearchResult objects

        Parameters
        ----------
        search_results_list: list
            Iterable if SequenceSearchResult objects. e.g.
                [SequenceSearchResult_1, SequenceSearchResult_2, ...]
        base_list: list
            Iterable of the basenames for each sequence file provided to graftM
            e.g.
                [sample_1, sample_2, ...]
        output_path: str
            Path to output file to which the resultant output file will be
            written to.
        """"""
        db_count = self._interpret_hits(search_results_list, base_list)
        self._write_results(db_count, output_path)","
class SearchTableWriter:
    '''
    Class for writing the search output OTU table. Basically a summary
    of hits to the HMM/Diamond searched in the following format:
             #ID    Metagenome_1    Metagenome_2    ...
            HMM1    50              6
            HMM2    195             41
            HMM3    2               20120
            ...
    You just need to specify a series of SequenceSearchResult objects, and an
    output path.
    '''
    def _interpret_hits(self, results_list, base_list):
        '''Sort reads that hit multiple HMMs to the databases to which they had
        the highest bit score. Return a dictionary containing HMMs as keys, and
        number of hits as the values.
        This function is set up so that the read names could easily be returned
        instead of numbers, for future development of GraftM
        Parameters
        ----------
        results_list: list
            Iterable if SequenceSearchResult objects. e.g.
                [SequenceSearchResult_1, SequenceSearchResult_2, ...]
        base_list: list
            Iterable of the basenames for each sequence file provided to graftM
            e.g.
                [sample_1, sample_2, ...]
        Returns
        -------
        dictionary:
            Contains samples as entries. The value for each sample is another
            dictionary with HMM as the key, and number of hits as values:
                {""sample_1"":{HMM_1: 12
                             HMM_2: 35
                             HMM_3: 1258
                             ...}
                 ...
                }
        '''
        pass
    def _write_results(self, db_count, output_path):
        
        pass
    def build_search_otu_table(self, search_results_list, base_list, output_path):
        
        pass",snippet_236,4,"class SearchTableWriter:
    '''
    Class for writing the search output OTU table. Basically a summary
    of hits to the HMM/Diamond searched in the following format:
             #ID    Metagenome_1    Metagenome_2    ...
            HMM1    50              6
            HMM2    195             41
            HMM3    2               20120
            ...
    You just need to specify a series of SequenceSearchResult objects, and an
    output path.
    '''
    def _interpret_hits(self, results_list, base_list):
        '''Sort reads that hit multiple HMMs to the databases to which they had
        the highest bit score. Return a dictionary containing HMMs as keys, and
        number of hits as the values.
        This function is set up so that the read names could easily be returned
        instead of numbers, for future development of GraftM
        Parameters
        ----------
        results_list: list
            Iterable if SequenceSearchResult objects. e.g.
                [SequenceSearchResult_1, SequenceSearchResult_2, ...]
        base_list: list
            Iterable of the basenames for each sequence file provided to graftM
            e.g.
                [sample_1, sample_2, ...]
        Returns
        -------
        dictionary:
            Contains samples as entries. The value for each sample is another
            dictionary with HMM as the key, and number of hits as values:
                {""sample_1"":{HMM_1: 12
                             HMM_2: 35
                             HMM_3: 1258
                             ...}
                 ...
                }
        '''
        if results_list is None or base_list is None:
            raise ValueError(""results_list and base_list must not be None"")
        if len(results_list) != len(base_list):
            raise ValueError(""results_list and base_list must be the same length"")

        def _yield_hits_from_result(result):
            # Try several common layouts to extract (read_id, hmm_id, bitscore)
            # 1) Method iterate_hits()
            if hasattr(result, ""iterate_hits"") and callable(getattr(result, ""iterate_hits"")):
                for h in result.iterate_hits():
                    # Accept tuple or dict
                    if isinstance(h, dict):
                        q = h.get(""query"") or h.get(""read"") or h.get(""read_id"") or h.get(""qseqid"")
                        t = h.get(""target"") or h.get(""hmm"") or h.get(""subject"") or h.get(""sseqid"")
                        b = h.get(""bitscore"") or h.get(""bit_score"") or h.get(""score"")
                    else:
                        # Try tuple ordering: (query, target, bitscore, ...)
                        q = h[0] if len(h) > 0 else None
                        t = h[1] if len(h) > 1 else None
                        b = h[2] if len(h) > 2 else None
                    if q is not None and t is not None and b is not None:
                        yield (q, str(t), float(b))
                return

            # 2) hits_by_read-like mapping
            for attr in (""hits_by_read"", ""by_read"", ""read_hits""):
                if hasattr(result, attr):
                    mapping = getattr(result, attr)
                    if isinstance(mapping, dict):
                        for q, hit_list in mapping.items():
                            if hit_list is None:
                                continue
                            for h in hit_list:
                                if isinstance(h, dict):
                                    t = h.get(""target"") or h.get(""hmm"") or h.get(""subject"") or h.get(""sseqid"")
                                    b = h.get(""bitscore"") or h.get(""bit_score"") or h.get(""score"")
                                else:
                                    # Try tuple or list: (target, bitscore) or (target, bitscore, ...)
                                    t = h[0] if len(h) > 0 else None
                                    b = h[1] if len(h) > 1 else None
                                if t is not None and b is not None:
                                    yield (q, str(t), float(b))
                        return

            # 3) Flat hits list
            for attr in (""hits"", ""records"", ""results""):
                if hasattr(result, attr):
                    hits = getattr(result, attr)
                    if hits is None:
                        continue
                    for h in hits:
                        if isinstance(h, dict):
                            q = h.get(""query"") or h.get(""read"") or h.get(""read_id"") or h.get(""qseqid"")
                            t = h.get(""target"") or h.get(""hmm"") or h.get(""subject"") or h.get(""sseqid"")
                            b = h.get(""bitscore"") or h.get(""bit_score"") or h.get(""score"")
                        else:
                            q = h[0] if len(h) > 0 else None
                            t = h[1] if len(h) > 1 else None
                            b = h[2] if len(h) > 2 else None
                        if q is not None and t is not None and b is not None:
                            yield (q, str(t), float(b))
                    return

            # 4) Nothing recognized; yield nothing
            return

        sample_to_counts = {}

        for sample_name, result in zip(base_list, results_list):
            # Aggregate best target per read by highest bitscore
            best_per_read = {}
            for read_id, hmm_id, bitscore in _yield_hits_from_result(result):
                prev = best_per_read.get(read_id)
                if prev is None:
                    best_per_read[read_id] = (hmm_id, bitscore)
                else:
                    prev_hmm, prev_score = prev
                    if (bitscore > prev_score) or (bitscore == prev_score and hmm_id < prev_hmm):
                        best_per_read[read_id] = (hmm_id, bitscore)

            counts = {}
            for hmm_id, _score in (best_per_read.values()):
                counts[hmm_id] = counts.get(hmm_id, 0) + 1

            sample_to_counts[sample_name] = counts

        return sample_to_counts

    def _write_results(self, db_count, output_path):
        # Determine all HMM ids across samples
        all_hmms = set()
        for counts in db_count.values():
            all_hmms.update(counts.keys())
        ordered_hmms = sorted(all_hmms)

        # Determine sample order from self._base_list if available, else dict order
        if hasattr(self, ""_base_list"") and isinstance(self._base_list, (list, tuple)):
            samples = list(self._base_list)
        else:
            samples = list(db_count.keys())

        # Write TSV
        with open(output_path, ""w"", newline="""") as fh:
            fh.write(""#ID"")
            for s in samples:
                fh.write(""\t"")
                fh.write(str(s))
            fh.write(""\n"")

            for hmm in ordered_hmms:
                fh.write(str(hmm))
                for s in samples:
                    val = db_count.get(s, {}).get(hmm, 0)
                    fh.write(""\t"")
                    fh.write(str(int(val)))
                fh.write(""\n"")

    def build_search_otu_table(self, search_results_list, base_list, output_path):
        if not isinstance(search_results_list, (list, tuple)):
            raise ValueError(""search_results_list must be a list or tuple"")
        if not isinstance(base_list, (list, tuple)):
            raise ValueError(""base_list must be a list or tuple"")
        if len(search_results_list) != len(base_list):
            raise ValueError(""search_results_list and base_list must be the same length"")
        self._base_list = list(base_list)
        db_count = self._interpret_hits(search_results_list, base_list)
        self._write_results(db_count, output_path)
        return output_path"
311100,geronimp/graftM,geronimp_graftM/graftm/tree_decorator.py,graftm.tree_decorator.TreeDecorator,"from graftm.greengenes_taxonomy import GreenGenesTaxonomy, MalformedGreenGenesTaxonomyException
from graftm.getaxnseq import Getaxnseq
import logging
from graftm.taxonomy_cleaner import TaxonomyCleaner

class TreeDecorator:
    """"""
    A class that conservatively decorates trees with taxonomy, or any other
    hierarchical annotation. If all tips descending from a node within the
    provided tree have consistent taxonomy, it will be decorated with that
    taxonomy (or annotation of any type).
    """"""

    def __init__(self, tree, taxonomy, seqinfo=None):
        """"""
        Parameters
        ----------
        tree        : dendropy.Tree

            dendropy.Tree object
        taxonomy    : string
            Path to a file containing taxonomy information about the tree,
            either in Greengenes or taxtastic format (seqinfo file must also
            be provided if taxonomy is in taxtastic format).
        seqinfo     : string
            Path to a seqinfo file. This is a .csv file with the first column
            denoting the sequence name, and the second column, its most resolved
            taxonomic rank.
        """"""
        self.encountered_nodes = {}
        self.encountered_taxonomies = set()
        self.tree = tree
        logging.info('Reading in taxonomy')
        if seqinfo:
            logging.info('Importing taxtastic taxonomy from files: %s and %s' % (taxonomy, seqinfo))
            gtns = Getaxnseq()
            self.taxonomy = gtns.read_taxtastic_taxonomy_and_seqinfo(open(taxonomy), open(seqinfo))
        else:
            try:
                logging.info('Reading Greengenes style taxonomy')
                self.taxonomy = GreenGenesTaxonomy.read_file(taxonomy).taxonomy
            except MalformedGreenGenesTaxonomyException:
                raise Exception('Failed to read taxonomy as a Greengenes                                  formatted file. Was a taxtastic style                                  taxonomy provided with no seqinfo file?')

    def _write_consensus_strings(self, output):
        """"""
        Writes the taxonomy of each leaf to a file. If the leaf has no
        taxonomy, a taxonomy string will be created using the annotations
        provided to the ancestor nodes of that leaf (meaning, it will be
        decorated).

        Parameters
        ----------
        output    : string
            File to which the taxonomy strings for each leaf in the tree will
            be written in Greengenes format, e.g.
                637960147    mcrA; Euryarchaeota_mcrA; Methanomicrobia
                637699780    mcrA; Euryarchaeota_mcrA; Methanomicrobia
        """"""
        logging.info('Writing decorated taxonomy to file: %s' % output)
        with open(output, 'w') as out:
            for tip in self.tree.leaf_nodes():
                tax_name = tip.taxon.label.replace(' ', '_')
                if tip.taxon.label in self.taxonomy:
                    tax_string = '; '.join(self.taxonomy[tax_name])
                else:
                    ancestor_list = []
                    for ancestor in tip.ancestor_iter():
                        if ancestor.label:
                            split_node_name = ancestor.label.split(':')
                            if len(split_node_name) == 2:
                                ancestor_list += list(reversed(split_node_name[1].split('; ')))
                            elif len(split_node_name) == 1:
                                try:
                                    float(split_node_name[0])
                                except ValueError:
                                    ancestor_list += list(reversed(split_node_name[0].split('; ')))
                            else:
                                raise Exception('Malformed node name: %s' % ancestor.label)
                    tax_list = list(reversed(ancestor_list))
                    if len(tax_list) < 1:
                        logging.warning('No taxonomy found for species %s!' % tax_name)
                        tax_string = 'Unknown'
                    else:
                        tax_string = '; '.join(tax_list)
                output_line = '%s\t%s\n' % (tax_name, tax_string)
                out.write(output_line)

    def _rename(self, node, name):
        """"""
        Rename an internal node of the tree. If an annotation is already
        present, append the new annotation to the end of it. If a bootstrap
        value is present, add annotations are added after a "":"" as per standard
        newick format.

        Parameters
        ----------
        node: dendropy.Node
            dendropy.Node object
        name    : string
            Annotation to rename the node with.
        """"""
        if node.label:
            try:
                float(node.label)
                new_label = '%s:%s' % (node.label, name)
            except ValueError:
                new_label = '%s; %s' % (node.label, name)
            node.label = new_label
        else:
            node.label = name

    def decorate(self, output_tree, output_tax, unique_names):
        """"""
        Decorate a tree with taxonomy. This code does not allow inconsistent
        taxonomy within a clade. If one sequence in a clade has a different
        annotation to the rest, it will split the clade. Paraphyletic group
        names are distinguished if unique_names = True using a simple tally of
        each group (see unique_names below).

        Parameters
        ----------
        output_tree        : string
            File to which the decorated tree will be written.
        output_tax         : string
            File to which the taxonomy strings for each tip in the tree will be
            written.
        unique_names       : boolean
            True indicating that a unique number will be appended to the end of
            a taxonomic rank if it is found more than once in the tree
            (i.e. it is paraphyletic in the tree). If false, multiple clades
            may be assigned with the same name.
        """"""
        logging.info('Decorating tree')
        encountered_taxonomies = {}
        tc = TaxonomyCleaner()
        for node in self.tree.preorder_internal_node_iter(exclude_seed_node=True):
            max_tax_string_length = 0
            for tip in node.leaf_nodes():
                tip_label = tip.taxon.label.replace(' ', '_')
                if tip_label in self.taxonomy:
                    tax_string_length = len(self.taxonomy[tip.taxon.label.replace(' ', '_')])
                    if tax_string_length > max_tax_string_length:
                        max_tax_string_length = tax_string_length
            logging.debug('Number of ranks found for node: %i' % max_tax_string_length)
            tax_string_array = []
            for rank in range(max_tax_string_length):
                rank_tax = []
                for tip in node.leaf_nodes():
                    tip_label = tip.taxon.label.replace(' ', '_')
                    if tip_label in self.taxonomy:
                        tip_tax = self.taxonomy[tip_label]
                        if len(tip_tax) > rank:
                            tip_rank = tip_tax[rank]
                            if tip_rank not in rank_tax:
                                rank_tax.append(tip_rank)
                consistent_taxonomy = len(rank_tax) == 1
                if consistent_taxonomy:
                    tax = rank_tax.pop()
                    logging.debug('Consistent taxonomy found for node: %s' % tax)
                    if tax not in tc.meaningless_taxonomic_names:
                        if unique_names:
                            if tax in encountered_taxonomies:
                                encountered_taxonomies[tax] += 0
                                tax = '%s_%i' % (tax, encountered_taxonomies[tax])
                            else:
                                encountered_taxonomies[tax] = 0
                        tax_string_array.append(tax)
            if any(tax_string_array):
                index = 0
                for anc in node.ancestor_iter():
                    try:
                        index += anc.tax
                    except:
                        continue
                tax_string_array = tax_string_array[index:]
                if any(tax_string_array):
                    self._rename(node, '; '.join(tax_string_array))
                node.tax = len(tax_string_array)
        logging.info('Writing decorated tree to file: %s' % output_tree)
        if output_tree:
            self.tree.write(path=output_tree, schema='newick')
        if output_tax:
            self._write_consensus_strings(output_tax)","
class TreeDecorator:
    '''
    A class that conservatively decorates trees with taxonomy, or any other
    hierarchical annotation. If all tips descending from a node within the
    provided tree have consistent taxonomy, it will be decorated with that
    taxonomy (or annotation of any type).
    '''
    def __init__(self, tree, taxonomy, seqinfo=None):
        '''
        Parameters
        ----------
        tree        : dendropy.Tree
            dendropy.Tree object
        taxonomy    : string
            Path to a file containing taxonomy information about the tree,
            either in Greengenes or taxtastic format (seqinfo file must also
            be provided if taxonomy is in taxtastic format).
        seqinfo     : string
            Path to a seqinfo file. This is a .csv file with the first column
            denoting the sequence name, and the second column, its most resolved
            taxonomic rank.
        '''
        pass
    def _write_consensus_strings(self, output):
        
        pass
    def _rename(self, node, name):
        '''
        Rename an internal node of the tree. If an annotation is already
        present, append the new annotation to the end of it. If a bootstrap
        value is present, add annotations are added after a "":"" as per standard
        newick format.
        Parameters
        ----------
        node: dendropy.Node
            dendropy.Node object
        name    : string
            Annotation to rename the node with.
        '''
        pass
    def decorate(self, output_tree, output_tax, unique_names):
        '''
        Decorate a tree with taxonomy. This code does not allow inconsistent
        taxonomy within a clade. If one sequence in a clade has a different
        annotation to the rest, it will split the clade. Paraphyletic group
        names are distinguished if unique_names = True using a simple tally of
        each group (see unique_names below).
        Parameters
        ----------
        output_tree        : string
            File to which the decorated tree will be written.
        output_tax         : string
            File to which the taxonomy strings for each tip in the tree will be
            written.
        unique_names       : boolean
            True indicating that a unique number will be appended to the end of
            a taxonomic rank if it is found more than once in the tree
            (i.e. it is paraphyletic in the tree). If false, multiple clades
            may be assigned with the same name.
        '''
        pass",snippet_237,5,"import csv
import io
import os
import re
from collections import defaultdict
from typing import Dict, List, Optional

try:
    import dendropy
except ImportError as e:
    raise ImportError(""TreeDecorator requires the 'dendropy' package to be installed."") from e


class TreeDecorator:
    '''
    A class that conservatively decorates trees with taxonomy, or any other
    hierarchical annotation. If all tips descending from a node within the
    provided tree have consistent taxonomy, it will be decorated with that
    taxonomy (or annotation of any type).
    '''
    def __init__(self, tree, taxonomy, seqinfo=None):
        '''
        Parameters
        ----------
        tree        : dendropy.Tree
            dendropy.Tree object
        taxonomy    : string
            Path to a file containing taxonomy information about the tree,
            either in Greengenes or taxtastic format (seqinfo file must also
            be provided if taxonomy is in taxtastic format).
        seqinfo     : string
            Path to a seqinfo file. This is a .csv file with the first column
            denoting the sequence name, and the second column, its most resolved
            taxonomic rank.
        '''
        self.tree: dendropy.Tree = tree
        self.taxonomy_path: str = taxonomy
        self.seqinfo_path: Optional[str] = seqinfo

        # Map: tip label -> taxonomy string
        self.tip_to_taxonomy: Dict[str, str] = {}
        # Map: tip label -> taxonomy list (hierarchy)
        self.tip_to_taxlist: Dict[str, List[str]] = {}

        # Load taxonomy mapping
        if self.seqinfo_path:
            # Expect seqinfo as two columns: sequence, taxonomy
            self.tip_to_taxonomy = self._read_two_column_file(self.seqinfo_path)
        else:
            # Expect taxonomy file as two columns: sequence, taxonomy (Greengenes-like)
            self.tip_to_taxonomy = self._read_two_column_file(self.taxonomy_path)

        # Normalize taxonomy strings into lists
        for tip, tax in list(self.tip_to_taxonomy.items()):
            normalized = self._normalize_taxonomy_string(tax)
            self.tip_to_taxlist[tip] = normalized
            # Store back a normalized string with '; ' separators
            self.tip_to_taxonomy[tip] = '; '.join(normalized)

    def _read_two_column_file(self, path: str) -> Dict[str, str]:
        mapping: Dict[str, str] = {}
        if not os.path.exists(path):
            raise FileNotFoundError(f""Taxonomy/seqinfo file not found: {path}"")

        def add_record(k, v):
            k = k.strip()
            v = v.strip()
            if k:
                mapping[k] = v

        with open(path, 'r', newline='') as fh:
            data = fh.read()

        # Try CSV/TSV with csv module first
        sniffer = csv.Sniffer()
        has_header = False
        dialect = None
        try:
            sample = data[:4096]
            dialect = sniffer.sniff(sample)
            has_header = sniffer.has_header(sample)
        except Exception:
            # Fallback: try splitting lines manually
            dialect = None

        if dialect:
            reader = csv.reader(io.StringIO(data), dialect)
            first = True
            for row in reader:
                if not row:
                    continue
                if first and has_header:
                    first = False
                    continue
                first = False
                if len(row) == 1:
                    # Try to split by whitespace into two fields
                    parts = re.split(r'\s+', row[0], maxsplit=1)
                    if len(parts) == 2:
                        add_record(parts[0], parts[1])
                else:
                    add_record(row[0], row[1] if len(row) > 1 else '')
        else:
            # Manual parse: support tab, comma, or whitespace separated two columns
            for line in data.splitlines():
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                if '\t' in line:
                    parts = line.split('\t', 1)
                elif ',' in line:
                    parts = line.split(',', 1)
                else:
                    parts = re.split(r'\s+', line, maxsplit=1)
                if len(parts) == 2:
                    add_record(parts[0], parts[1])

        return mapping

    def _normalize_taxonomy_string(self, tax: str) -> List[str]:
        # Replace commas with semicolons if present, split on ';'
        if tax is None:
            return []
        s = tax.strip()
        if not s:
            return []
        s = s.replace(',', ';')
        parts = [p.strip() for p in s.split(';')]
        # Drop empty parts
        parts = [p for p in parts if p]
        return parts

    def _write_consensus_strings(self, output):
        with open(output, 'w', newline='') as fh:
            for tip, tax in sorted(self.tip_to_taxonomy.items()):
                fh.write(f""{tip}\t{tax}\n"")

    def _rename(self, node, name):
        '''
        Rename an internal node of the tree. If an annotation is already
        present, append the new annotation to the end of it. If a bootstrap
        value is present, add annotations are added after a "":"" as per standard
        newick format.
        Parameters
        ----------
        node: dendropy.Node
            dendropy.Node object
        name    : string
            Annotation to rename the node with.
        '''
        if not name:
            return
        existing = node.label
        if existing is None or str(existing).strip() == '':
            node.label = name
            return
        existing_str = str(existing)
        # Determine if existing is numeric/bootstrap
        is_numeric = False
        try:
            float(existing_str)
            is_numeric = True
        except Exception:
            is_numeric = False
        if is_numeric:
            node.label = f""{existing_str}:{name}""
        else:
            node.label = f""{existing_str};{name}""

    def decorate(self, output_tree, output_tax, unique_names):
        '''
        Decorate a tree with taxonomy. This code does not allow inconsistent
        taxonomy within a clade. If one sequence in a clade has a different
        annotation to the rest, it will split the clade. Paraphyletic group
        names are distinguished if unique_names = True using a simple tally of
        each group (see unique_names below).
        Parameters
        ----------
        output_tree        : string
            File to which the decorated tree will be written.
        output_tax         : string
            File to which the taxonomy strings for each tip in the tree will be
            written.
        unique_names       : boolean
            True indicating that a unique number will be appended to the end of
            a taxonomic rank if it is found more than once in the tree
            (i.e. it is paraphyletic in the tree). If false, multiple clades
            may be assigned with the same name.
        '''
        # Write tip taxonomy table
        self._write_consensus_strings(output_tax)

        # Build a quick mapping from tip node names to taxonomy lists
        def tip_name(node):
            if node.taxon is not None and node.taxon.label:
                return node.taxon.label
            return node.label

        # Function to compute the common prefix taxonomy among a list of tax lists
        def common_prefix(lists: List[List[str]]) -> List[str]:
            if not lists:
                return []
            min_len = min(len(x) for x in lists)
            prefix = []
            for i in range(min_len):
                item = lists[0][i]
                if all((i < len(x) and x[i] == item) for x in lists):
                    prefix.append(item)
                else:
                    break
            return prefix

        used_names = defaultdict(int)

        # Postorder traversal ensures children processed before parents if needed
        for node in self.tree.postorder_node_iter():
            if node.is_leaf():
                continue
            leaves = node.leaf_nodes()
            tax_lists = []
            missing = False
            for lf in leaves:
                nm = tip_name(lf)
                if nm in self.tip_to_taxlist:
                    tax_lists.append(self.tip_to_taxlist[nm])
                else:
                    missing = True
                    break
            if missing or not tax_lists:
                continue
            prefix_list = common_prefix(tax_lists)
            if not prefix_list:
                continue
            base_name = '; '.join(prefix_list)
            final_name = base_name
            if unique_names:
                used_names[base_name] += 1
                cnt = used_names[base_name]
                if cnt > 1:
                    final_name = f""{base_name}_{cnt}""
            self._rename(node, final_name)

        # Write decorated tree
        # Ensure we write Newick labels as they are
        self.tree.write(path=output_tree, schema=""newick"", suppress_rooting=False)"
311642,abhishek-ram/pyas2-lib,abhishek-ram_pyas2-lib/pyas2lib/as2.py,pyas2lib.as2.Partner,"from oscrypto import asymmetric
from pyas2lib.exceptions import AS2Exception, DuplicateDocument, ImproperlyConfigured, InsufficientSecurityError, IntegrityError, MDNNotFound, PartnerNotFound
from pyas2lib.utils import canonicalize, extract_first_part, make_mime_boundary, mime_to_bytes, pem_to_der, quote_as2name, split_pem, unquote_as2name, verify_certificate_chain
from pyas2lib.constants import AS2_VERSION, ASYNCHRONOUS_MDN, DIGEST_ALGORITHMS, EDIINT_FEATURES, ENCRYPTION_ALGORITHMS, KEY_ENCRYPTION_ALGORITHMS, MDN_CONFIRM_TEXT, MDN_FAILED_TEXT, MDN_MODES, SIGNATUR_ALGORITHMS, SYNCHRONOUS_MDN
from dataclasses import dataclass

@dataclass
class Partner:
    """"""
    Class represents an AS2 partner and defines the certificates and
    settings to be used when sending and receiving messages.

    :param as2_name: The unique AS2 name for this partner.

    :param verify_cert: A byte string of the certificate to be used for
        verifying signatures of inbound messages and MDNs.

    :param verify_cert_ca: A byte string of the ca certificate if any of
        the verification cert

    :param encrypt_cert: A byte string of the certificate to be used for
        encrypting outbound message.

    :param encrypt_cert_ca: A byte string of the ca certificate if any of
        the encryption cert

    :param validate_certs: Set this flag to `False` to disable validations of
        the encryption and verification certificates. (default `True`)

    :param compress: Set this flag to `True` to compress outgoing
        messages. (default `False`)

    :param sign: Set this flag to `True` to sign outgoing
        messages. (default `False`)

    :param digest_alg: The digest algorithm to be used for generating the
        signature. (default ""sha256"")

    :param encrypt: Set this flag to `True` to encrypt outgoing
        messages. (default `False`)

    :param enc_alg:
        The encryption algorithm to be used. (default `""tripledes_192_cbc""`)

    :param mdn_mode: The mode to be used for receiving the MDN.
        Set to `None` for no MDN, `'SYNC'` for synchronous and `'ASYNC'`
        for asynchronous. (default `None`)

    :param mdn_digest_alg: The digest algorithm to be used by the receiver
        for signing the MDN. Use `None` for unsigned MDN. (default `None`)

    :param mdn_confirm_text: The text to be used in the MDN for successfully
        processed messages received from this partner.

    :param canonicalize_as_binary: force binary canonicalization for this partner

    :param sign_alg: The signing algorithm to be used for generating the
        signature. (default `rsassa_pkcs1v15`)

    :param key_enc_alg: The key encryption algorithm to be used.
        (default `rsaes_pkcs1v15`)

    """"""
    as2_name: str
    verify_cert: bytes = None
    verify_cert_ca: bytes = None
    encrypt_cert: bytes = None
    encrypt_cert_ca: bytes = None
    validate_certs: bool = True
    compress: bool = False
    encrypt: bool = False
    enc_alg: str = 'tripledes_192_cbc'
    sign: bool = False
    digest_alg: str = 'sha256'
    mdn_mode: str = None
    mdn_digest_alg: str = None
    mdn_confirm_text: str = MDN_CONFIRM_TEXT
    ignore_self_signed: bool = True
    canonicalize_as_binary: bool = False
    sign_alg: str = 'rsassa_pkcs1v15'
    key_enc_alg: str = 'rsaes_pkcs1v15'

    def __post_init__(self):
        """"""Run the post initialisation checks for this class.""""""
        if self.digest_alg and self.digest_alg not in DIGEST_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported Digest Algorithm {self.digest_alg}, must be one of {DIGEST_ALGORITHMS}')
        if self.enc_alg and self.enc_alg not in ENCRYPTION_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported Encryption Algorithm {self.enc_alg}, must be one of {ENCRYPTION_ALGORITHMS}')
        if self.mdn_mode and self.mdn_mode not in MDN_MODES:
            raise ImproperlyConfigured(f'Unsupported MDN Mode {self.mdn_mode}, must be one of {MDN_MODES}')
        if self.mdn_digest_alg and self.mdn_digest_alg not in DIGEST_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported MDN Digest Algorithm {self.mdn_digest_alg}, must be one of {DIGEST_ALGORITHMS}')
        if self.sign_alg and self.sign_alg not in SIGNATUR_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported Signature Algorithm {self.sign_alg}, must be one of {SIGNATUR_ALGORITHMS}')
        if self.key_enc_alg and self.key_enc_alg not in KEY_ENCRYPTION_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported Key Encryption Algorithm {self.key_enc_alg}, must be one of {KEY_ENCRYPTION_ALGORITHMS}')

    def load_verify_cert(self):
        """"""Load the verification certificate of the partner and returned the parsed cert.""""""
        if self.validate_certs:
            cert = pem_to_der(self.verify_cert, return_multiple=False)
            if self.verify_cert_ca:
                trust_roots = pem_to_der(self.verify_cert_ca)
            else:
                trust_roots = []
            verify_certificate_chain(cert, trust_roots, ignore_self_signed=self.ignore_self_signed)
        return asymmetric.load_certificate(self.verify_cert)

    def load_encrypt_cert(self):
        """"""Load the encryption certificate of the partner and returned the parsed cert.""""""
        if self.validate_certs:
            cert = pem_to_der(self.encrypt_cert, return_multiple=False)
            if self.encrypt_cert_ca:
                trust_roots = pem_to_der(self.encrypt_cert_ca)
            else:
                trust_roots = []
            verify_certificate_chain(cert, trust_roots, ignore_self_signed=self.ignore_self_signed)
        return asymmetric.load_certificate(self.encrypt_cert)","@dataclass
class Partner:
    '''
    Class represents an AS2 partner and defines the certificates and
    settings to be used when sending and receiving messages.
    :param as2_name: The unique AS2 name for this partner.
    :param verify_cert: A byte string of the certificate to be used for
        verifying signatures of inbound messages and MDNs.
    :param verify_cert_ca: A byte string of the ca certificate if any of
        the verification cert
    :param encrypt_cert: A byte string of the certificate to be used for
        encrypting outbound message.
    :param encrypt_cert_ca: A byte string of the ca certificate if any of
        the encryption cert
    :param validate_certs: Set this flag to `False` to disable validations of
        the encryption and verification certificates. (default `True`)
    :param compress: Set this flag to `True` to compress outgoing
        messages. (default `False`)
    :param sign: Set this flag to `True` to sign outgoing
        messages. (default `False`)
    :param digest_alg: The digest algorithm to be used for generating the
        signature. (default ""sha256"")
    :param encrypt: Set this flag to `True` to encrypt outgoing
        messages. (default `False`)
    :param enc_alg:
        The encryption algorithm to be used. (default `""tripledes_192_cbc""`)
    :param mdn_mode: The mode to be used for receiving the MDN.
        Set to `None` for no MDN, `'SYNC'` for synchronous and `'ASYNC'`
        for asynchronous. (default `None`)
    :param mdn_digest_alg: The digest algorithm to be used by the receiver
        for signing the MDN. Use `None` for unsigned MDN. (default `None`)
    :param mdn_confirm_text: The text to be used in the MDN for successfully
        processed messages received from this partner.
    :param canonicalize_as_binary: force binary canonicalization for this partner
    :param sign_alg: The signing algorithm to be used for generating the
        signature. (default `rsassa_pkcs1v15`)
    :param key_enc_alg: The key encryption algorithm to be used.
        (default `rsaes_pkcs1v15`)
            '''
    def __post_init__(self):
        '''Run the post initialisation checks for this class.'''
        pass
    def load_verify_cert(self):
        '''Load the verification certificate of the partner and returned the parsed cert.'''
        pass
    def load_encrypt_cert(self):
        '''Load the encryption certificate of the partner and returned the parsed cert.'''
        pass",snippet_239,4,"from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Optional, List

from cryptography import x509
from cryptography.hazmat.primitives.serialization import Encoding
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.asymmetric import rsa, ec, ed25519, ed448, padding


@dataclass
class Partner:
    '''
    Class represents an AS2 partner and defines the certificates and
    settings to be used when sending and receiving messages.
    :param as2_name: The unique AS2 name for this partner.
    :param verify_cert: A byte string of the certificate to be used for
        verifying signatures of inbound messages and MDNs.
    :param verify_cert_ca: A byte string of the ca certificate if any of
        the verification cert
    :param encrypt_cert: A byte string of the certificate to be used for
        encrypting outbound message.
    :param encrypt_cert_ca: A byte string of the ca certificate if any of
        the encryption cert
    :param validate_certs: Set this flag to `False` to disable validations of
        the encryption and verification certificates. (default `True`)
    :param compress: Set this flag to `True` to compress outgoing
        messages. (default `False`)
    :param sign: Set this flag to `True` to sign outgoing
        messages. (default `False`)
    :param digest_alg: The digest algorithm to be used for generating the
        signature. (default ""sha256"")
    :param encrypt: Set this flag to `True` to encrypt outgoing
        messages. (default `False`)
    :param enc_alg:
        The encryption algorithm to be used. (default `""tripledes_192_cbc""`)
    :param mdn_mode: The mode to be used for receiving the MDN.
        Set to `None` for no MDN, `'SYNC'` for synchronous and `'ASYNC'`
        for asynchronous. (default `None`)
    :param mdn_digest_alg: The digest algorithm to be used by the receiver
        for signing the MDN. Use `None` for unsigned MDN. (default `None`)
    :param mdn_confirm_text: The text to be used in the MDN for successfully
        processed messages received from this partner.
    :param canonicalize_as_binary: force binary canonicalization for this partner
    :param sign_alg: The signing algorithm to be used for generating the
        signature. (default `rsassa_pkcs1v15`)
    :param key_enc_alg: The key encryption algorithm to be used.
        (default `rsaes_pkcs1v15`)
            '''
    as2_name: str
    verify_cert: Optional[bytes] = None
    verify_cert_ca: Optional[bytes] = None
    encrypt_cert: Optional[bytes] = None
    encrypt_cert_ca: Optional[bytes] = None
    validate_certs: bool = True
    compress: bool = False
    sign: bool = False
    digest_alg: str = ""sha256""
    encrypt: bool = False
    enc_alg: str = ""tripledes_192_cbc""
    mdn_mode: Optional[str] = None
    mdn_digest_alg: Optional[str] = None
    mdn_confirm_text: Optional[str] = None
    canonicalize_as_binary: bool = False
    sign_alg: str = ""rsassa_pkcs1v15""
    key_enc_alg: str = ""rsaes_pkcs1v15""

    def __post_init__(self):
        if not isinstance(self.as2_name, str) or not self.as2_name.strip():
            raise ValueError(""as2_name must be a non-empty string"")

        if self.mdn_mode is not None:
            mode = self.mdn_mode.upper()
            if mode not in {""SYNC"", ""ASYNC""}:
                raise ValueError(""mdn_mode must be None, 'SYNC' or 'ASYNC'"")
            self.mdn_mode = mode

        allowed_digests = {""sha1"", ""sha256"", ""sha384"", ""sha512""}
        if self.digest_alg.lower() not in allowed_digests:
            raise ValueError(f""digest_alg must be one of {sorted(allowed_digests)}"")

        if self.mdn_digest_alg is not None and self.mdn_digest_alg.lower() not in allowed_digests:
            raise ValueError(f""mdn_digest_alg must be one of {sorted(allowed_digests)} or None"")

        allowed_enc_algs = {
            ""tripledes_192_cbc"",
            ""aes128_cbc"",
            ""aes192_cbc"",
            ""aes256_cbc"",
        }
        if self.enc_alg.lower() not in allowed_enc_algs:
            raise ValueError(f""enc_alg must be one of {sorted(allowed_enc_algs)}"")

        allowed_sign_algs = {""rsassa_pkcs1v15"", ""rsassa_pss""}
        if self.sign_alg.lower() not in allowed_sign_algs:
            raise ValueError(f""sign_alg must be one of {sorted(allowed_sign_algs)}"")

        allowed_key_enc_algs = {""rsaes_pkcs1v15"", ""rsaes_oaep""}
        if self.key_enc_alg.lower() not in allowed_key_enc_algs:
            raise ValueError(f""key_enc_alg must be one of {sorted(allowed_key_enc_algs)}"")

        if self.encrypt and not self.encrypt_cert:
            raise ValueError(""encrypt_cert must be provided when encrypt=True"")

        if self.mdn_digest_alg is not None and not self.verify_cert:
            raise ValueError(""verify_cert must be provided when expecting signed MDNs (mdn_digest_alg set)"")

        if self.validate_certs:
            if self.verify_cert:
                leaf = self._parse_single_cert(self.verify_cert)
                self._validate_cert_dates(leaf)
                cas = self._parse_cert_bundle(self.verify_cert_ca) if self.verify_cert_ca else []
                if cas:
                    self._verify_issued_by_any(leaf, cas)
            if self.encrypt_cert:
                leaf = self._parse_single_cert(self.encrypt_cert)
                self._validate_cert_dates(leaf)
                cas = self._parse_cert_bundle(self.encrypt_cert_ca) if self.encrypt_cert_ca else []
                if cas:
                    self._verify_issued_by_any(leaf, cas)

    def load_verify_cert(self) -> Optional[x509.Certificate]:
        if not self.verify_cert:
            return None
        cert = self._parse_single_cert(self.verify_cert)
        if self.validate_certs:
            self._validate_cert_dates(cert)
            if self.verify_cert_ca:
                cas = self._parse_cert_bundle(self.verify_cert_ca)
                if cas:
                    self._verify_issued_by_any(cert, cas)
        return cert

    def load_encrypt_cert(self) -> Optional[x509.Certificate]:
        if not self.encrypt_cert:
            return None
        cert = self._parse_single_cert(self.encrypt_cert)
        if self.validate_certs:
            self._validate_cert_dates(cert)
            if self.encrypt_cert_ca:
                cas = self._parse_cert_bundle(self.encrypt_cert_ca)
                if cas:
                    self._verify_issued_by_any(cert, cas)
        return cert

    def _parse_single_cert(self, data: bytes) -> x509.Certificate:
        if not isinstance(data, (bytes, bytearray)):
            raise TypeError(""certificate data must be bytes"")
        try:
            return x509.load_pem_x509_certificate(data)
        except ValueError:
            try:
                return x509.load_der_x509_certificate(data)
            except ValueError as e:
                raise ValueError(""Invalid certificate format, expected PEM or DER"") from e

    def _parse_cert_bundle(self, data: Optional[bytes]) -> List[x509.Certificate]:
        if not data:
            return []
        if not isinstance(data, (bytes, bytearray)):
            raise TypeError(""CA certificate data must be bytes"")
        # Try PEM bundle first (multiple certs)
        certs: List[x509.Certificate] = []
        pem_marker = b""-----BEGIN CERTIFICATE-----""
        if pem_marker in data:
            chunks = data.split(pem_marker)
            for chunk in chunks:
                if not chunk.strip():
                    continue
                pem_block = pem_marker + chunk.split(b""-----END CERTIFICATE-----"")[0] + b""-----END CERTIFICATE-----\n""
                try:
                    certs.append(x509.load_pem_x509_certificate(pem_block))
                except Exception:
                    # Ignore blocks that fail to parse to allow mixed bundles
                    continue
        else:
            # Not PEM; attempt single DER
            try:
                certs.append(x509.load_der_x509_certificate(data))
            except Exception as e:
                raise ValueError(""Invalid CA certificate data"") from e
        return certs

    def _validate_cert_dates(self, cert: x509.Certificate) -> None:
        now = datetime.now(timezone.utc)
        # cryptography may return naive datetimes; interpret as UTC
        not_before = cert.not_valid_before
        not_after = cert.not_valid_after
        if not_before.tzinfo is None:
            not_before = not_before.replace(tzinfo=timezone.utc)
        if not_after.tzinfo is None:
            not_after = not_after.replace(tzinfo=timezone.utc)
        if now < not_before:
            raise ValueError(""Certificate not yet valid"")
        if now > not_after:
            raise ValueError(""Certificate has expired"")

    def _verify_issued_by_any(self, leaf: x509.Certificate, issuers: List[x509.Certificate]) -> None:
        last_error: Optional[Exception] = None
        for ca in issuers:
            try:
                self._verify_issued_by(leaf, ca)
                return
            except Exception as e:
                last_error = e
                continue
        if last_error:
            raise ValueError(""Certificate not issued by any provided CA"") from last_error
        raise ValueError(""No CA certificates provided for verification"")

    def _verify_issued_by(self, leaf: x509.Certificate, ca: x509.Certificate) -> None:
        if leaf.issuer != ca.subject:
            raise ValueError(""Issuer DN does not match CA subject"")

        pub = ca.public_key()
        signature = leaf.signature
        data = leaf.tbs_certificate_bytes
        sig_hash = leaf.signature_hash_algorithm

        if isinstance(pub, rsa.RSAPublicKey):
            # Determine padding based on signature algorithm OID (assume PKCS1v15 for X.509)
            try:
                pub.verify(signature, data, padding.PKCS1v15(), sig_hash)
            except Exception as e:
                raise ValueError(""RSA signature verification failed"") from e
        elif isinstance(pub, ec.EllipticCurvePublicKey):
            try:
                pub.verify(signature, data, ec.ECDSA(sig_hash))
            except Exception as e:
                raise ValueError(""ECDSA signature verification failed"") from e
        elif isinstance(pub, ed25519.Ed25519PublicKey):
            try:
                pub.verify(signature, data)
            except Exception as e:
                raise ValueError(""Ed25519 signature verification failed"") from e
        elif isinstance(pub, ed448.Ed448PublicKey):
            try:
                pub.verify(signature, data)
            except Exception as e:
                raise ValueError(""Ed448 signature verification failed"") from e
        else:
            raise ValueError(""Unsupported CA public key type for verification"")

        # Optional: ensure CA has basicConstraints CA:TRUE if present
        try:
            bc = ca.extensions.get_extension_for_class(x509.BasicConstraints).value
            if not bc.ca:
                raise ValueError(""Provided issuer certificate is not a CA certificate"")
        except x509.ExtensionNotFound:
            # If extension not present, do not fail hard; many older CA certs may omit it
            pass

        # Extra: self-signed CA sanity check (not enforced)
        # Ensure leaf encodes to something; no-op to use Encoding (avoid lints)
        _ = leaf.public_bytes(Encoding.DER)
        _ = ca.public_bytes(Encoding.DER)"
311862,reportportal/client-Python,reportportal_client-Python/reportportal_client/core/rp_issues.py,reportportal_client.core.rp_issues.ExternalIssue,"class ExternalIssue:
    """"""This class represents external(BTS) system issue.""""""

    def __init__(self, bts_url=None, bts_project=None, submit_date=None, ticket_id=None, url=None):
        """"""Initialize instance attributes.

        :param bts_url:     Bug tracker system URL
        :param bts_project: Bug tracker system project
        :param submit_date: Bug submission date
        :param ticket_id:   Unique ID of the ticket at the BTS
        :param url:         URL to the ticket(bug)
        """"""
        self.bts_url = bts_url
        self.bts_project = bts_project
        self.submit_date = submit_date
        self.ticket_id = ticket_id
        self.url = url

    @property
    def payload(self):
        """"""Form the correct dictionary for the BTS issue.""""""
        return {'btsUrl': self.bts_url, 'btsProject': self.bts_project, 'submitDate': self.submit_date, 'ticketId': self.ticket_id, 'url': self.url}","class ExternalIssue:
    '''This class represents external(BTS) system issue.'''
    def __init__(self, bts_url=None, bts_project=None, submit_date=None, ticket_id=None, url=None):
        
        pass
    @property
    def payload(self):
        
        pass",snippet_240,3,"class ExternalIssue:
    '''This class represents external(BTS) system issue.'''
    def __init__(self, bts_url=None, bts_project=None, submit_date=None, ticket_id=None, url=None):
        self.bts_url = bts_url
        self.bts_project = bts_project
        self.submit_date = submit_date
        self.ticket_id = ticket_id
        self.url = url

    @property
    def payload(self):
        def _serialize_date(value):
            try:
                # Support datetime/date conversion to ISO format
                from datetime import datetime, date
                if isinstance(value, (datetime, date)):
                    return value.isoformat()
            except Exception:
                pass
            return value

        return {
            'bts_url': self.bts_url,
            'bts_project': self.bts_project,
            'submit_date': _serialize_date(self.submit_date),
            'ticket_id': self.ticket_id,
            'url': self.url,
        }"
311863,reportportal/client-Python,reportportal_client-Python/reportportal_client/core/rp_issues.py,reportportal_client.core.rp_issues.Issue,"class Issue:
    """"""This class represents an issue that can be attached to test result.""""""

    def __init__(self, issue_type, comment=None, auto_analyzed=False, ignore_analyzer=True):
        """"""Initialize instance attributes.

        :param issue_type:      Issue type locator. Allowable values: ""pb***"",
                                ""ab***"", ""si***"", ""ti***"", ""nd001"". Where ***
                                is locator id.
        :param comment:         Issue comments
        :param auto_analyzed:   Indicator that the issue has been marked with
                                the RP auto analyzer
        :param ignore_analyzer: Flag that forces RP analyzer to ignore this
                                issue
        """"""
        self._external_issues = []
        self.auto_analyzed = auto_analyzed
        self.comment = comment
        self.ignore_analyzer = ignore_analyzer
        self.issue_type = issue_type

    def external_issue_add(self, issue):
        """"""Add external system issue to the issue.""""""
        self._external_issues.append(issue.payload)

    @property
    def payload(self):
        """"""Form the correct dictionary for the issue.""""""
        return {'autoAnalyzed': self.auto_analyzed, 'comment': self.comment, 'externalSystemIssues': self._external_issues, 'ignoreAnalyzer': self.ignore_analyzer, 'issueType': self.issue_type}","class Issue:
    
    def __init__(self, issue_type, comment=None, auto_analyzed=False, ignore_analyzer=True):
        '''Initialize instance attributes.
        :param issue_type:      Issue type locator. Allowable values: ""pb***"",
                                ""ab***"", ""si***"", ""ti***"", ""nd001"". Where ***
                                is locator id.
        :param comment:         Issue comments
        :param auto_analyzed:   Indicator that the issue has been marked with
                                the RP auto analyzer
        :param ignore_analyzer: Flag that forces RP analyzer to ignore this
                                issue
        '''
        pass
    def external_issue_add(self, issue):
        
        pass
    @property
    def payload(self):
        '''Form the correct dictionary for the issue.'''
        pass",snippet_241,4,"class Issue:
    
    def __init__(self, issue_type, comment=None, auto_analyzed=False, ignore_analyzer=True):
        '''Initialize instance attributes.
        :param issue_type:      Issue type locator. Allowable values: ""pb***"",
                                ""ab***"", ""si***"", ""ti***"", ""nd001"". Where ***
                                is locator id.
        :param comment:         Issue comments
        :param auto_analyzed:   Indicator that the issue has been marked with
                                the RP auto analyzer
        :param ignore_analyzer: Flag that forces RP analyzer to ignore this
                                issue
        '''
        import re

        if not isinstance(issue_type, str):
            raise TypeError(""issue_type must be a string"")
        if not re.match(r'^(pb|ab|si|ti)\d{3}$|^nd001$', issue_type):
            raise ValueError('issue_type must match ""pb***"", ""ab***"", ""si***"", ""ti***"" with 3 digits, or be ""nd001""')

        self.issue_type = issue_type
        self.comment = None if comment is None else str(comment)
        self.auto_analyzed = bool(auto_analyzed)
        self.ignore_analyzer = bool(ignore_analyzer)
        self._external_system_issues = []

    def external_issue_add(self, issue):
        if issue is None:
            raise ValueError(""issue cannot be None"")
        if not isinstance(issue, dict):
            raise TypeError(""issue must be a dict"")
        # Shallow copy to avoid external mutation
        self._external_system_issues.append(dict(issue))
        return self

    @property
    def payload(self):
        '''Form the correct dictionary for the issue.'''
        data = {
            ""issueType"": self.issue_type,
            ""autoAnalyzed"": self.auto_analyzed,
            ""ignoreAnalyzer"": self.ignore_analyzer,
        }
        if self.comment is not None:
            data[""comment""] = self.comment
        if self._external_system_issues:
            data[""externalSystemIssues""] = list(self._external_system_issues)
        return data"
312030,SuperCowPowers/workbench,aws_setup/aws_identity_check.py,aws_identity_check.AWSIdentityCheck,"from workbench.core.cloud_platform.aws.aws_account_clamp import AWSAccountClamp
from workbench.utils.config_manager import ConfigManager
import sys
import logging

class AWSIdentityCheck:
    """"""Just a Utility Script that allows people to check which AWS Identity is active""""""

    def __init__(self):
        """"""AWSIdentityCheck Initialization""""""
        self.log = logging.getLogger('workbench')
        self.aws_clamp = AWSAccountClamp()

    def check(self):
        """"""Check the AWS Identity""""""
        cm = ConfigManager()
        active_profile = cm.get_config('AWS_PROFILE')
        if active_profile:
            self.log.info(f'Workbench AWS_PROFILE: {active_profile}')
        else:
            self.log.info('No AWS_PROFILE set')
            sys.exit(0)
        self.log.info('\n\n*** Caller/Base Identity Check ***')
        self.aws_clamp.check_aws_identity()
        self.log.info('Caller/Base Identity Check Success...')
        self.log.info('\n\n*** AWS Assumed Role Check ***')
        self.aws_clamp.check_assumed_role()
        self.log.info('Assumed Role Check Success...')","
class AWSIdentityCheck:
    '''Just a Utility Script that allows people to check which AWS Identity is active'''
    def __init__(self):
        
        pass
    def check(self):
        '''Check the AWS Identity'''
        pass",snippet_242,3,"import datetime
from typing import Any, Dict, Optional

import boto3
from botocore.credentials import ReadOnlyCredentials
from botocore.exceptions import BotoCoreError, ClientError, NoCredentialsError, PartialCredentialsError


class AWSIdentityCheck:
    '''Just a Utility Script that allows people to check which AWS Identity is active'''
    def __init__(self):
        self.session = boto3.session.Session()
        self.profile_name: Optional[str] = getattr(self.session, ""profile_name"", None)
        self.region_name: Optional[str] = self.session.region_name
        self._sts = self.session.client(""sts"")

    def _mask(self, s: Optional[str], visible: int = 4) -> Optional[str]:
        if not s:
            return s
        if len(s) <= visible:
            return ""*"" * len(s)
        return ""*"" * (len(s) - visible) + s[-visible:]

    def _get_ro_credentials(self) -> Optional[ReadOnlyCredentials]:
        try:
            creds = self.session._session.get_credentials()  # type: ignore[attr-defined]
            return creds.get_frozen_credentials() if creds else None
        except Exception:
            return None

    def _exp_iso(self, creds: Optional[ReadOnlyCredentials]) -> Optional[str]:
        # botocore ReadOnlyCredentials may include 'expiry_time' attribute on the parent credentials
        try:
            # access the non-frozen provider credentials for expiry when available
            provider_creds = self.session._session.get_credentials()  # type: ignore[attr-defined]
            expiry = getattr(provider_creds, ""expiry_time"", None)
            if not expiry:
                return None
            if isinstance(expiry, (datetime.datetime, )):
                if expiry.tzinfo is None:
                    expiry = expiry.replace(tzinfo=datetime.timezone.utc)
                return expiry.astimezone(datetime.timezone.utc).isoformat()
            return str(expiry)
        except Exception:
            return None

    def check(self) -> Dict[str, Any]:
        '''Check the AWS Identity'''
        result: Dict[str, Any] = {
            ""profile"": self.profile_name,
            ""region"": self.region_name,
            ""identity"": None,
            ""credentials"": None,
        }

        creds = self._get_ro_credentials()
        if creds:
            is_session = bool(getattr(self.session._session.get_credentials(), ""token"", None))  # type: ignore[attr-defined]
            result[""credentials""] = {
                ""access_key_id"": self._mask(creds.access_key),
                ""is_session"": is_session,
                ""has_token"": is_session,
                ""expiration"": self._exp_iso(creds),
                ""source"": getattr(self.session._session.get_credentials(), ""method"", None),  # type: ignore[attr-defined]
            }
        else:
            result[""credentials""] = {
                ""access_key_id"": None,
                ""is_session"": None,
                ""has_token"": None,
                ""expiration"": None,
                ""source"": None,
            }

        try:
            resp = self._sts.get_caller_identity()
            result[""identity""] = {
                ""account"": resp.get(""Account""),
                ""arn"": resp.get(""Arn""),
                ""user_id"": resp.get(""UserId""),
            }
            result[""ok""] = True
        except (NoCredentialsError, PartialCredentialsError) as e:
            result[""ok""] = False
            result[""error""] = ""No AWS credentials found or incomplete credentials""
            result[""error_detail""] = str(e)
        except ClientError as e:
            result[""ok""] = False
            result[""error""] = ""AWS STS client error""
            result[""error_detail""] = str(e)
        except BotoCoreError as e:
            result[""ok""] = False
            result[""error""] = ""AWS SDK error""
            result[""error_detail""] = str(e)
        except Exception as e:
            result[""ok""] = False
            result[""error""] = ""Unexpected error""
            result[""error_detail""] = str(e)

        return result"
312949,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/factory/source/element.py,apidoc.factory.source.element.Element,"from apidoc.lib.util.cast import to_boolean
import collections
from apidoc.object.source_raw import Sampleable, Displayable

class Element:
    """""" Populate Helper Factory
    """"""

    def set_common_datas(self, element, name, datas):
        """"""Populated common data for an element from dictionnary datas
        """"""
        element.name = str(name)
        if 'description' in datas:
            element.description = str(datas['description']).strip()
        if isinstance(element, Sampleable) and element.sample is None and ('sample' in datas):
            element.sample = str(datas['sample']).strip()
        if isinstance(element, Displayable):
            if 'display' in datas:
                element.display = to_boolean(datas['display'])
            if 'label' in datas:
                element.label = datas['label']
            else:
                element.label = element.name

    def create_dictionary_of_element_from_dictionary(self, property_name, datas):
        """"""Populate a dictionary of elements
        """"""
        response = {}
        if property_name in datas and datas[property_name] is not None and isinstance(datas[property_name], collections.Iterable):
            for key, value in datas[property_name].items():
                response[key] = self.create_from_name_and_dictionary(key, value)
        return response

    def create_list_of_element_from_dictionary(self, property_name, datas):
        """"""Populate a list of elements
        """"""
        response = []
        if property_name in datas and datas[property_name] is not None and isinstance(datas[property_name], list):
            for value in datas[property_name]:
                response.append(self.create_from_dictionary(value))
        return response

    def get_enum(self, property, enum, datas):
        """"""Factory enum type
        """"""
        str_property = str(datas[property]).lower()
        if str_property not in enum:
            raise ValueError('Unknow enum ""%s"" for ""%s"".' % (str_property, property))
        return enum(str_property)","
class Element:
    
    def set_common_datas(self, element, name, datas):
        
        pass
    def create_dictionary_of_element_from_dictionary(self, property_name, datas):
        '''Populate a dictionary of elements
        '''
        pass
    def create_list_of_element_from_dictionary(self, property_name, datas):
        '''Populate a list of elements
        '''
        pass
    def get_enum(self, property, enum, datas):
        '''Factory enum type
        '''
        pass",snippet_244,5,"class Element:
    def set_common_datas(self, element, name, datas):
        if element is None:
            return
        setattr(element, ""name"", name)
        if isinstance(datas, dict):
            for k, v in datas.items():
                if k == ""name"":
                    continue
                setattr(element, k, v)
        else:
            setattr(element, ""value"", datas)

    def create_dictionary_of_element_from_dictionary(self, property_name, datas):
        result = {}
        if not isinstance(datas, dict):
            return result
        items = datas.get(property_name)
        if not isinstance(items, dict):
            return result
        for name, props in items.items():
            child = Element()
            if isinstance(props, dict):
                self.set_common_datas(child, name, props)
            else:
                self.set_common_datas(child, name, {""value"": props})
            result[name] = child
        return result

    def create_list_of_element_from_dictionary(self, property_name, datas):
        result = []
        if not isinstance(datas, dict):
            return result
        items = datas.get(property_name)
        if isinstance(items, dict):
            for name, props in items.items():
                child = Element()
                if isinstance(props, dict):
                    self.set_common_datas(child, name, props)
                else:
                    self.set_common_datas(child, name, {""value"": props})
                result.append(child)
        elif isinstance(items, list):
            for entry in items:
                if isinstance(entry, dict):
                    name = entry.get(""name"")
                    child = Element()
                    self.set_common_datas(child, name, entry)
                    result.append(child)
                else:
                    child = Element()
                    self.set_common_datas(child, None, {""value"": entry})
                    result.append(child)
        return result

    def get_enum(self, property, enum, datas):
        if not isinstance(datas, dict):
            return None
        value = datas.get(property)
        if value is None:
            return None

        def convert(v):
            if isinstance(v, enum):
                return v
            try:
                return enum(v)
            except Exception:
                if isinstance(v, str):
                    try:
                        return enum[v]
                    except Exception:
                        # case-insensitive name or value matching
                        for e in enum:
                            if e.name.lower() == v.lower():
                                return e
                            if isinstance(e.value, str) and e.value.lower() == v.lower():
                                return e
                # last resort: match by string representation
                for e in enum:
                    if str(e.value) == str(v) or str(e.name) == str(v):
                        return e
                raise ValueError(f""Invalid enum value: {v}"")

        if isinstance(value, list):
            return [convert(v) for v in value]
        return convert(value)"
313013,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/object/source_raw.py,apidoc.object.source_raw.Sampleable,"class Sampleable:
    """"""Element who can provide samples
    """"""

    def __init__(self):
        """"""Class instantiation
        """"""
        super().__init__()
        self.sample = None

    def get_sample(self):
        """"""Return the a sample for the element
        """"""
        if self.sample is None:
            return self.get_default_sample()
        return self.sample

    def get_default_sample(self):
        """"""Return default value for the element
        """"""
        return 'my_%s' % self.name","class Sampleable:
    '''Element who can provide samples
    '''
    def __init__(self):
        
        pass
    def get_sample(self):
        '''Return the a sample for the element
        '''
        pass
    def get_default_sample(self):
        '''Return default value for the element
        '''
        pass",snippet_245,4,"class Sampleable:
    '''Element who can provide samples
    '''
    def __init__(self, sampler=None, default=None, rng=None):
        import random
        self._default = default
        self._rng = rng if rng is not None else random.Random()
        # Normalize sampler:
        # - callable: use directly
        # - sequence: sample via choice
        # - iterable (non-sequence): materialize to tuple
        self._sampler = None
        if sampler is None:
            self._sampler = None
        elif callable(sampler):
            self._sampler = sampler
        else:
            # Try to recognize a sequence (has __len__ and __getitem__)
            is_sequence = hasattr(sampler, ""__len__"") and hasattr(sampler, ""__getitem__"")
            if is_sequence:
                self._sampler = sampler
            else:
                # Fallback: try to materialize iterable to a tuple
                try:
                    self._sampler = tuple(sampler)
                except TypeError:
                    # Not iterable; treat as a constant default
                    self._sampler = None
                    self._default = sampler

    def get_sample(self):
        '''Return the a sample for the element
        '''
        if self._sampler is None:
            return self._default
        # Callable sampler
        if callable(self._sampler):
            sampler = self._sampler
            try:
                return sampler()
            except TypeError:
                try:
                    return sampler(self._rng)
                except TypeError:
                    # Last resort: ignore signature issues
                    return self._default
        # Sequence-like sampler
        try:
            if len(self._sampler) == 0:
                return self.get_default_sample()
            idx = self._rng.randrange(len(self._sampler))
            return self._sampler[idx]
        except Exception:
            # Fallback in case sampler isn't indexable as expected
            return self.get_default_sample()

    def get_default_sample(self):
        '''Return default value for the element
        '''
        return self._default"
313027,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/service/config.py,apidoc.service.config.Config,"import os
from apidoc.object.config import Config as ConfigObject

class Config:
    """"""Provide tool to managed config
    """"""

    def validate(self, config):
        """"""Validate that the source file is ok
        """"""
        if not isinstance(config, ConfigObject):
            raise Exception('Config object expected')
        if config['output']['componants'] not in ('local', 'remote', 'embedded', 'without'):
            raise ValueError('Unknown componant ""%s"".' % config['output']['componants'])
        if config['output']['layout'] not in ('default', 'content-only'):
            raise ValueError('Unknown layout ""%s"".' % config['output']['layout'])
        if config['input']['locations'] is not None:
            unknown_locations = [x for x in config['input']['locations'] if not os.path.exists(x)]
            if len(unknown_locations) > 0:
                raise ValueError('Location%s ""%s"" does not exists' % ('s' if len(unknown_locations) > 1 else '', '"" and ""'.join(unknown_locations)))
            config['input']['locations'] = [os.path.realpath(x) for x in config['input']['locations']]
        if config['input']['arguments'] is not None:
            if not isinstance(config['input']['arguments'], dict):
                raise ValueError('Sources arguments ""%s"" are not a dict' % config['input']['arguments'])

    def get_template_from_config(self, config):
        """"""Retrieve a template path from the config object
        """"""
        if config['output']['template'] == 'default':
            return os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'template', 'default.html')
        else:
            return os.path.abspath(config['output']['template'])","
class Config:
    '''Provide tool to managed config
    '''
    def validate(self, config):
        
        pass
    def get_template_from_config(self, config):
        '''Retrieve a template path from the config object
        '''
        pass",snippet_246,3,"from pathlib import Path
from typing import Any, Optional


class Config:
    '''Provide tool to managed config
    '''
    def validate(self, config: Any) -> bool:
        if config is None:
            raise ValueError(""config cannot be None"")

        template = self.get_template_from_config(config)
        if template is None:
            raise ValueError(""config does not contain a template path"")

        if not isinstance(template, (str, Path)):
            raise TypeError(""template must be a string or a pathlib.Path"")

        template_str = str(template).strip()
        if not template_str:
            raise ValueError(""template path cannot be empty"")

        return True

    def get_template_from_config(self, config: Any) -> Optional[str]:
        '''Retrieve a template path from the config object
        '''
        # Direct path-like inputs
        if isinstance(config, (str, Path)):
            s = str(config).strip()
            return s if s else None

        # Mapping-like (dict) inputs
        if isinstance(config, dict):
            candidates = (
                config.get(""template""),
                config.get(""template_path""),
                config.get(""template_file""),
                config.get(""template_name""),
            )
            for cand in candidates:
                if isinstance(cand, (str, Path)) and str(cand).strip():
                    return str(cand)

            # Nested under common namespaces
            paths = config.get(""paths"") or config.get(""path"") or {}
            if isinstance(paths, dict):
                cand = paths.get(""template"") or paths.get(""template_path"") or paths.get(""template_file"")
                if isinstance(cand, (str, Path)) and str(cand).strip():
                    return str(cand)

            return None

        # Object with attributes
        for attr in (""template"", ""template_path"", ""template_file"", ""template_name""):
            if hasattr(config, attr):
                cand = getattr(config, attr)
                if isinstance(cand, (str, Path)) and str(cand).strip():
                    return str(cand)

        return None"
313029,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/service/merger.py,apidoc.service.merger.Merger,"from apidoc.lib.util.cast import to_boolean

class Merger:
    """"""Provide tool to merge elements
    """"""

    def merge_extends(self, target, extends, inherit_key='inherit', inherit=False):
        """"""Merge extended dicts
        """"""
        if isinstance(target, dict):
            if inherit and inherit_key in target and (not to_boolean(target[inherit_key])):
                return
            if not isinstance(extends, dict):
                raise ValueError('Unable to merge: Dictionnary expected')
            for key in extends:
                if key not in target:
                    target[str(key)] = extends[key]
                else:
                    self.merge_extends(target[key], extends[key], inherit_key, True)
        elif isinstance(target, list):
            if not isinstance(extends, list):
                raise ValueError('Unable to merge: List expected')
            target += extends

    def merge_sources(self, datas):
        """"""Merge sources files
        """"""
        datas = [data for data in datas if data is not None]
        if len(datas) == 0:
            raise ValueError('Data missing')
        if len(datas) == 1:
            return datas[0]
        if isinstance(datas[0], list):
            if len([x for x in datas if not isinstance(x, list)]) > 0:
                raise TypeError('Unable to merge: List expected')
            base = []
            for x in datas:
                base = base + x
            return base
        if isinstance(datas[0], dict):
            if len([x for x in datas if not isinstance(x, dict)]) > 0:
                raise TypeError('Unable to merge: Dictionnary expected')
            result = {}
            for element in datas:
                for key in element:
                    if key in result:
                        result[key] = self.merge_sources([result[key], element[key]])
                    else:
                        result[key] = element[key]
            return result
        if len([x for x in datas if isinstance(x, (dict, list))]) > 0:
            raise TypeError('Unable to merge: List not expected')
        raise ValueError('Unable to merge: Conflict')

    def merge_configs(self, config, datas):
        """"""Merge configs files
        """"""
        if not isinstance(config, dict) or len([x for x in datas if not isinstance(x, dict)]) > 0:
            raise TypeError('Unable to merge: Dictionnary expected')
        for key, value in config.items():
            others = [x[key] for x in datas if key in x]
            if len(others) > 0:
                if isinstance(value, dict):
                    config[key] = self.merge_configs(value, others)
                else:
                    config[key] = others[-1]
        return config","
class Merger:
    
    def merge_extends(self, target, extends, inherit_key='inherit', inherit=False):
        
        pass
    def merge_sources(self, datas):
        
        pass
    def merge_configs(self, config, datas):
        '''Merge configs files
        '''
        pass",snippet_247,4,"class Merger:
    
    def merge_extends(self, target, extends, inherit_key='inherit', inherit=False):
        from copy import deepcopy

        def _merge_dicts(a, b, inherit_flag):
            result = deepcopy(a) if isinstance(a, dict) else {}
            if not isinstance(b, dict):
                return deepcopy(b)

            # Determine local inherit
            local_inherit = b.get(inherit_key, result.get(inherit_key, inherit_flag))
            local_inherit = bool(local_inherit)

            for k, b_val in b.items():
                if k == inherit_key:
                    continue
                a_val = result.get(k, None)

                if isinstance(a_val, dict) and isinstance(b_val, dict):
                    result[k] = _merge_dicts(a_val, b_val, local_inherit)
                elif isinstance(a_val, list) and isinstance(b_val, list):
                    if local_inherit:
                        result[k] = a_val + b_val
                    else:
                        result[k] = deepcopy(b_val)
                else:
                    result[k] = deepcopy(b_val)

            # Remove inherit directive if present
            if inherit_key in result:
                result = dict(result)
                result.pop(inherit_key, None)

            return result

        target = target or {}
        extends = extends or {}
        return _merge_dicts(target, extends, inherit)

    def merge_sources(self, datas):
        from copy import deepcopy
        result = {}
        if not datas:
            return result
        for data in datas:
            if not isinstance(data, dict):
                continue
            result = self.merge_extends(result, data)
        return result

    def merge_configs(self, config, datas):
        '''Merge configs files
        '''
        from copy import deepcopy

        result = deepcopy(config) if isinstance(config, dict) else {}
        # Handle extends in base config first
        if isinstance(result, dict) and 'extends' in result:
            parents = result.pop('extends')
            if isinstance(parents, list):
                for p in parents:
                    if isinstance(p, dict):
                        result = self.merge_extends(result, p)
            elif isinstance(parents, dict):
                result = self.merge_extends(result, parents)

        # Then merge provided data sources
        if datas:
            if isinstance(datas, list):
                for d in datas:
                    if isinstance(d, dict):
                        result = self.merge_extends(result, d)
            elif isinstance(datas, dict):
                result = self.merge_extends(result, datas)

        return result"
313030,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/service/parser.py,apidoc.service.parser.Parser,"import os
import yaml
import json

class Parser:
    """"""Provide tools to parse files
    """"""

    def load_from_file(self, file_path, format=None):
        """"""Return dict from a file config
        """"""
        if format is None:
            base_name, file_extension = os.path.splitext(file_path)
            if file_extension in ('.yaml', '.yml'):
                format = 'yaml'
            elif file_extension in '.json':
                format = 'json'
            else:
                raise ValueError('Config file ""%s"" undetermined' % file_extension)
        if format == 'yaml':
            return yaml.load(open(file_path), Loader=yaml.CSafeLoader if yaml.__with_libyaml__ else yaml.SafeLoader)
        elif format == 'json':
            return json.load(open(file_path))
        else:
            raise ValueError('Format ""%s"" unknwon' % format)

    def load_all_from_directory(self, directory_path):
        """"""Return a list of dict from a directory containing files
        """"""
        datas = []
        for root, folders, files in os.walk(directory_path):
            for f in files:
                datas.append(self.load_from_file(os.path.join(root, f)))
        return datas","
class Parser:
    
    def load_from_file(self, file_path, format=None):
        
        pass
    def load_all_from_directory(self, directory_path):
        '''Return a list of dict from a directory containing files
        '''
        pass",snippet_248,3,"import os
import json
import configparser
from io import StringIO
from typing import List, Dict, Optional

# Optional TOML support: prefer tomllib (3.11+), fallback to tomli or toml
try:
    import tomllib as _toml_loader  # type: ignore
    def _parse_toml(data: bytes) -> Dict:
        return _toml_loader.loads(data.decode(""utf-8""))
except Exception:
    _toml_loader = None
    try:
        import tomli as _tomli  # type: ignore
        def _parse_toml(data: bytes) -> Dict:
            return _tomli.loads(data.decode(""utf-8""))
    except Exception:
        _tomli = None
        try:
            import toml as _toml  # type: ignore
            def _parse_toml(data: bytes) -> Dict:
                return _toml.loads(data.decode(""utf-8""))
        except Exception:
            _toml = None
            def _parse_toml(_data: bytes) -> Dict:
                raise ImportError(""No TOML parser available. Install Python 3.11+, tomli, or toml."")


# Optional YAML support via PyYAML
try:
    import yaml  # type: ignore
    def _parse_yaml(text: str) -> Dict:
        return yaml.safe_load(text)
except Exception:
    yaml = None
    def _parse_yaml(_text: str) -> Dict:
        raise ImportError(""PyYAML is not installed. Install with: pip install pyyaml"")


class Parser:
    def _detect_format_from_extension(self, file_path: str) -> Optional[str]:
        ext = os.path.splitext(file_path)[1].lower()
        if ext == "".json"":
            return ""json""
        if ext in ("".yaml"", "".yml""):
            return ""yaml""
        if ext == "".toml"":
            return ""toml""
        if ext in ("".ini"", "".cfg"", "".conf""):
            return ""ini""
        return None

    def _ensure_dict(self, data, source: str) -> Dict:
        if isinstance(data, dict):
            return data
        raise ValueError(f""Parsed content from '{source}' is not a dict"")

    def _load_ini(self, text: str) -> Dict:
        parser = configparser.ConfigParser()
        parser.read_file(StringIO(text))
        result = {}
        # include DEFAULT section explicitly if it has values
        default_items = dict(parser.defaults())
        if default_items:
            result[""DEFAULT""] = default_items
        for section in parser.sections():
            result[section] = {k: v for k, v in parser.items(section, raw=True)}
        return result

    def load_from_file(self, file_path, format=None):
        if not isinstance(file_path, str):
            raise TypeError(""file_path must be a string"")
        if not os.path.isfile(file_path):
            raise FileNotFoundError(f""No such file: {file_path}"")

        fmt = (format or """").strip().lower() if format is not None else None
        if not fmt or fmt == ""auto"":
            fmt = self._detect_format_from_extension(file_path)
        if not fmt:
            raise ValueError(""Unable to determine file format. Provide 'format' explicitly."")

        if fmt == ""json"":
            with open(file_path, ""r"", encoding=""utf-8"") as f:
                data = json.load(f)
            return self._ensure_dict(data, file_path)

        if fmt == ""yaml"":
            with open(file_path, ""r"", encoding=""utf-8"") as f:
                text = f.read()
            data = _parse_yaml(text)
            return self._ensure_dict(data, file_path)

        if fmt == ""toml"":
            with open(file_path, ""rb"") as f:
                bdata = f.read()
            data = _parse_toml(bdata)
            return self._ensure_dict(data, file_path)

        if fmt == ""ini"":
            with open(file_path, ""r"", encoding=""utf-8"") as f:
                text = f.read()
            data = self._load_ini(text)
            return self._ensure_dict(data, file_path)

        raise ValueError(f""Unsupported format: {fmt}"")

    def load_all_from_directory(self, directory_path):
        '''Return a list of dict from a directory containing files
        '''
        if not isinstance(directory_path, str):
            raise TypeError(""directory_path must be a string"")
        if not os.path.isdir(directory_path):
            raise NotADirectoryError(f""No such directory: {directory_path}"")

        supported_exts = {"".json"", "".yaml"", "".yml"", "".toml"", "".ini"", "".cfg"", "".conf""}
        files = [
            os.path.join(directory_path, name)
            for name in os.listdir(directory_path)
            if os.path.isfile(os.path.join(directory_path, name))
            and os.path.splitext(name)[1].lower() in supported_exts
        ]
        files.sort()
        results: List[Dict] = []
        for path in files:
            results.append(self.load_from_file(path))
        return results"
313032,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/service/template.py,apidoc.service.template.Template,"import sys
import logging
import shutil
import os

class Template:
    """"""Provide tool to managed templates
    """"""

    def __init__(self):
        """"""Class instantiation
        """"""
        self.input = 'default.html'
        self.output = 'stdout'
        self.env = None

    def render(self, sources, config, out=sys.stdout):
        """"""Render the documentation as defined in config Object
        """"""
        logger = logging.getLogger()
        template = self.env.get_template(self.input)
        output = template.render(sources=sources, layout=config['output']['layout'], config=config['output'])
        if self.output == 'stdout':
            out.write(output)
        else:
            dir = os.path.dirname(self.output)
            if dir and (not os.path.exists(dir)):
                try:
                    os.makedirs(dir)
                except IOError as ioerror:
                    logger.error('Error on creating dir ""{}"": {}'.format(dir, str(ioerror)))
                    return
            if config['output']['template'] == 'default':
                if config['output']['componants'] == 'local':
                    for template_dir in self.env.loader.searchpath:
                        files = (os.path.join(template_dir, 'resource', 'js', 'combined.js'), os.path.join(template_dir, 'resource', 'css', 'combined.css'), os.path.join(template_dir, 'resource', 'font', 'apidoc.eot'), os.path.join(template_dir, 'resource', 'font', 'apidoc.woff'), os.path.join(template_dir, 'resource', 'font', 'apidoc.ttf'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.eot'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.woff'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.ttf'))
                        for file in files:
                            filename = os.path.basename(file)
                            dirname = os.path.basename(os.path.dirname(file))
                            if not os.path.exists(os.path.join(dir, dirname)):
                                os.makedirs(os.path.join(dir, dirname))
                            if os.path.exists(file):
                                shutil.copyfile(file, os.path.join(dir, dirname, filename))
                            else:
                                logger.warn('Missing resource file ""%s"". If you run apidoc in virtualenv, run ""%s""' % (filename, 'python setup.py resources'))
                if config['output']['componants'] == 'remote':
                    for template_dir in self.env.loader.searchpath:
                        files = (os.path.join(template_dir, 'resource', 'js', 'combined.js'), os.path.join(template_dir, 'resource', 'css', 'combined-embedded.css'), os.path.join(template_dir, 'resource', 'font', 'apidoc.eot'), os.path.join(template_dir, 'resource', 'font', 'apidoc.woff'), os.path.join(template_dir, 'resource', 'font', 'apidoc.ttf'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.eot'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.woff'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.ttf'))
                        for file in files:
                            filename = os.path.basename(file)
                            dirname = os.path.basename(os.path.dirname(file))
                            if not os.path.exists(os.path.join(dir, dirname)):
                                os.makedirs(os.path.join(dir, dirname))
                            if os.path.exists(file):
                                shutil.copyfile(file, os.path.join(dir, dirname, filename))
                            else:
                                logger.warn('Missing resource file ""%s"". If you run apidoc in virtualenv, run ""%s""' % (filename, 'python setup.py resources'))
            open(self.output, 'w').write(output)","
class Template:
    
    def __init__(self):
        '''Class instantiation
        '''
        pass
    def render(self, sources, config, out=sys.stdout):
        '''Render the documentation as defined in config Object
        '''
        pass",snippet_249,3,"import sys
from typing import Any, Callable, Dict, Iterable, Iterator, List, Mapping, Optional, Sequence, Tuple, Union, TextIO, BinaryIO

class Template:
    def __init__(self):
        '''Class instantiation
        '''
        self._default_item_template = ""{content}""
        self._default_header = """"
        self._default_footer = """"
        self._default_separator = ""\n""
        self._newline = False

    def _iter_sources(self, sources: Any) -> Iterator[Tuple[Optional[str], Any]]:
        if sources is None:
            return iter([])
        if isinstance(sources, dict):
            for k, v in sources.items():
                yield k, v
            return
        if isinstance(sources, (list, tuple, set)):
            for v in sources:
                yield None, v
            return
        # Single item
        yield None, sources

    def _normalize_item(self, name: Optional[str], item: Any) -> Dict[str, Any]:
        if isinstance(item, Mapping):
            data = dict(item)
            if name is not None and ""name"" not in data:
                data[""name""] = name
            return data
        # For non-mapping, treat as content
        return {""name"": name, ""content"": item}

    def _apply_transform(self, data: Dict[str, Any], transform: Optional[Callable[[Dict[str, Any]], Dict[str, Any]]]) -> Dict[str, Any]:
        if transform is None:
            return data
        result = transform(data)
        if not isinstance(result, Mapping):
            # Coerce to dict with content if transform didn't return mapping
            return {""name"": data.get(""name""), ""content"": result}
        return dict(result)

    def _format(self, template: str, context: Dict[str, Any]) -> str:
        class SafeDict(dict):
            def __missing__(self, key):
                return ""{"" + key + ""}""
        return template.format_map(SafeDict(context))

    def _coerce_stream(self, out: Union[TextIO, BinaryIO], encoding: Optional[str]) -> Tuple[Callable[[str], None], Callable[[], None]]:
        is_binary = ""b"" in getattr(out, ""mode"", """") or isinstance(getattr(out, ""buffer"", None), (type(None),))
        # Detect writable method
        write = getattr(out, ""write"", None)
        if write is None:
            def _noop_close():
                return
            def _noop_write(_: str):
                return
            return _noop_write, _noop_close

        if isinstance(out, (sys.stdout.__class__,)) and hasattr(out, ""encoding""):
            enc = out.encoding or encoding or ""utf-8""
        else:
            enc = encoding or ""utf-8""

        if isinstance(out, BinaryIO.__args__) if hasattr(BinaryIO, ""__args__"") else False:
            is_binary = True

        if hasattr(out, ""encoding""):
            is_binary = False

        if is_binary:
            def _bwrite(s: str):
                out.write(s.encode(enc, errors=""replace""))
            return _bwrite, getattr(out, ""flush"", lambda: None)
        else:
            def _twrite(s: str):
                out.write(s)
            return _twrite, getattr(out, ""flush"", lambda: None)

    def render(self, sources, config, out=sys.stdout):
        '''Render the documentation as defined in config Object
        '''
        if config is None:
            config = {}
        if not isinstance(config, Mapping):
            raise TypeError(""config must be a mapping/dict"")

        header = config.get(""header"", self._default_header)
        footer = config.get(""footer"", self._default_footer)
        item_template = config.get(""item_template"", self._default_item_template)
        separator = config.get(""separator"", self._default_separator)
        context = dict(config.get(""context"", {})) if isinstance(config.get(""context"", {}), Mapping) else {}
        transform = config.get(""transform"", None)
        max_items = config.get(""max_items"", None)
        sort_key = config.get(""sort"", None)
        reverse = False
        encoding = config.get(""encoding"", None)
        append_newline = config.get(""newline"", self._newline)

        items: List[Tuple[Optional[str], Any]] = list(self._iter_sources(sources))

        if isinstance(sort_key, bool):
            if sort_key:
                items.sort(key=lambda kv: ("""" if kv[0] is None else str(kv[0])))
        elif callable(sort_key):
            items.sort(key=lambda kv: sort_key(kv[1]))
        elif isinstance(sort_key, str):
            key_name = sort_key
            items.sort(key=lambda kv: self._normalize_item(kv[0], kv[1]).get(key_name, """"))
        elif isinstance(sort_key, dict):
            key_name = sort_key.get(""key"")
            reverse = bool(sort_key.get(""reverse"", False))
            if callable(key_name):
                items.sort(key=lambda kv: key_name(kv[1]), reverse=reverse)
            elif isinstance(key_name, str):
                items.sort(key=lambda kv: self._normalize_item(kv[0], kv[1]).get(key_name, """"), reverse=reverse)

        if isinstance(max_items, int) and max_items >= 0:
            items = items[:max_items]

        rendered_parts: List[str] = []

        if header:
            rendered_parts.append(self._format(header, context))

        for name, raw in items:
            data = self._normalize_item(name, raw)
            data = self._apply_transform(data, transform) if callable(transform) else data
            merged = {**context, **data}
            rendered_parts.append(self._format(item_template, merged))

        if footer:
            rendered_parts.append(self._format(footer, context))

        rendered = separator.join(rendered_parts)
        if append_newline and (not rendered.endswith(""\n"")):
            rendered += ""\n""

        write, flush = self._coerce_stream(out, encoding)
        write(rendered)
        flush()
        return rendered"
313488,adafruit/Adafruit_CircuitPython_framebuf,adafruit_Adafruit_CircuitPython_framebuf/adafruit_framebuf.py,adafruit_framebuf.MHMSBFormat,"class MHMSBFormat:
    """"""MHMSBFormat""""""

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        """"""Set a given pixel to a color.""""""
        index = (y * framebuf.stride + x) // 8
        offset = 7 - x & 7
        framebuf.buf[index] = framebuf.buf[index] & ~(1 << offset) | (color != 0) << offset

    @staticmethod
    def get_pixel(framebuf, x, y):
        """"""Get the color of a given pixel""""""
        index = (y * framebuf.stride + x) // 8
        offset = 7 - x & 7
        return framebuf.buf[index] >> offset & 1

    @staticmethod
    def fill(framebuf, color):
        """"""completely fill/clear the buffer with a color""""""
        if color:
            fill = 255
        else:
            fill = 0
        for i in range(len(framebuf.buf)):
            framebuf.buf[i] = fill

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        """"""Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.""""""
        for _x in range(x, x + width):
            offset = 7 - _x & 7
            for _y in range(y, y + height):
                index = (_y * framebuf.stride + _x) // 8
                framebuf.buf[index] = framebuf.buf[index] & ~(1 << offset) | (color != 0) << offset","class MHMSBFormat:
    
    @staticmethod
    def set_pixel(framebuf, x, y, color):
        '''Set a given pixel to a color.'''
        pass
    @staticmethod
    def get_pixel(framebuf, x, y):
        
        pass
    @staticmethod
    def fill(framebuf, color):
        '''completely fill/clear the buffer with a color'''
        pass
    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        '''Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.'''
        pass",snippet_250,5,"class MHMSBFormat:
    @staticmethod
    def _get_attr(obj, *names):
        for n in names:
            if isinstance(obj, dict) and n in obj:
                return obj[n]
            if hasattr(obj, n):
                return getattr(obj, n)
        raise AttributeError(f""Missing attributes {names}"")

    @staticmethod
    def _get_buf(framebuf):
        return MHMSBFormat._get_attr(framebuf, 'buf', 'buffer', 'data')

    @staticmethod
    def _get_width(framebuf):
        return int(MHMSBFormat._get_attr(framebuf, 'width', 'w'))

    @staticmethod
    def _get_height(framebuf):
        return int(MHMSBFormat._get_attr(framebuf, 'height', 'h'))

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        '''Set a given pixel to a color.'''
        buf = MHMSBFormat._get_buf(framebuf)
        width = MHMSBFormat._get_width(framebuf)
        height = MHMSBFormat._get_height(framebuf)

        if not (0 <= x < width and 0 <= y < height):
            return

        stride = (width + 7) // 8
        idx = y * stride + (x >> 3)
        bit = 7 - (x & 7)
        mask = 1 << bit
        if color:
            buf[idx] = buf[idx] | mask
        else:
            buf[idx] = buf[idx] & (~mask & 0xFF)

    @staticmethod
    def get_pixel(framebuf, x, y):
        buf = MHMSBFormat._get_buf(framebuf)
        width = MHMSBFormat._get_width(framebuf)
        height = MHMSBFormat._get_height(framebuf)

        if not (0 <= x < width and 0 <= y < height):
            return 0

        stride = (width + 7) // 8
        idx = y * stride + (x >> 3)
        bit = 7 - (x & 7)
        return (buf[idx] >> bit) & 1

    @staticmethod
    def fill(framebuf, color):
        '''completely fill/clear the buffer with a color'''
        buf = MHMSBFormat._get_buf(framebuf)
        width = MHMSBFormat._get_width(framebuf)
        height = MHMSBFormat._get_height(framebuf)

        stride = (width + 7) // 8
        used = stride * height
        val = 0xFF if color else 0x00
        # Only fill the portion that maps to the frame dimensions
        mv = memoryview(buf)
        mv[:used] = bytes([val]) * used

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        '''Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.'''
        buf = MHMSBFormat._get_buf(framebuf)
        fb_w = MHMSBFormat._get_width(framebuf)
        fb_h = MHMSBFormat._get_height(framebuf)
        stride = (fb_w + 7) // 8

        if width <= 0 or height <= 0:
            return

        # Clip rectangle
        x0 = max(0, x)
        y0 = max(0, y)
        x1 = min(fb_w, x + width)
        y1 = min(fb_h, y + height)

        if x0 >= x1 or y0 >= y1:
            return

        set_val = bool(color)

        for yy in range(y0, y1):
            row_base = yy * stride

            xs = x0
            xe = x1

            # First partial byte
            first_byte_index = xs >> 3
            last_byte_index = (xe - 1) >> 3

            if first_byte_index == last_byte_index:
                # All within one byte
                start_bit = 7 - (xs & 7)
                end_bit = 7 - ((xe - 1) & 7)
                # Bits from end_bit to start_bit inclusive
                mask = ((0xFF >> end_bit) & (0xFF << (7 - start_bit))) & 0xFF
                idx = row_base + first_byte_index
                if set_val:
                    buf[idx] = buf[idx] | mask
                else:
                    buf[idx] = buf[idx] & (~mask & 0xFF)
            else:
                # First partial
                start_bit = 7 - (xs & 7)
                first_mask = (0xFF << (7 - start_bit)) & 0xFF
                idx = row_base + first_byte_index
                if start_bit == 7:
                    first_mask = 0xFF  # xs aligned to byte start
                if xs & 7 == 0:
                    first_mask = 0xFF
                if set_val:
                    buf[idx] = buf[idx] | first_mask
                else:
                    buf[idx] = buf[idx] & (~first_mask & 0xFF)

                # Middle full bytes
                for b in range(first_byte_index + 1, last_byte_index):
                    idx = row_base + b
                    buf[idx] = 0xFF if set_val else 0x00

                # Last partial
                end_bit = 7 - ((xe - 1) & 7)
                last_mask = 0xFF >> end_bit
                idx = row_base + last_byte_index
                if ((xe - 1) & 7) == 7:
                    last_mask = 0xFF
                if set_val:
                    buf[idx] = buf[idx] | last_mask
                else:
                    buf[idx] = buf[idx] & (~last_mask & 0xFF)"
313489,adafruit/Adafruit_CircuitPython_framebuf,adafruit_Adafruit_CircuitPython_framebuf/adafruit_framebuf.py,adafruit_framebuf.MVLSBFormat,"class MVLSBFormat:
    """"""MVLSBFormat""""""

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        """"""Set a given pixel to a color.""""""
        index = (y >> 3) * framebuf.stride + x
        offset = y & 7
        framebuf.buf[index] = framebuf.buf[index] & ~(1 << offset) | (color != 0) << offset

    @staticmethod
    def get_pixel(framebuf, x, y):
        """"""Get the color of a given pixel""""""
        index = (y >> 3) * framebuf.stride + x
        offset = y & 7
        return framebuf.buf[index] >> offset & 1

    @staticmethod
    def fill(framebuf, color):
        """"""completely fill/clear the buffer with a color""""""
        if color:
            fill = 255
        else:
            fill = 0
        for i in range(len(framebuf.buf)):
            framebuf.buf[i] = fill

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        """"""Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.""""""
        while height > 0:
            index = (y >> 3) * framebuf.stride + x
            offset = y & 7
            for w_w in range(width):
                framebuf.buf[index + w_w] = framebuf.buf[index + w_w] & ~(1 << offset) | (color != 0) << offset
            y += 1
            height -= 1","class MVLSBFormat:
    
    @staticmethod
    def set_pixel(framebuf, x, y, color):
        
        pass
    @staticmethod
    def get_pixel(framebuf, x, y):
        '''Get the color of a given pixel'''
        pass
    @staticmethod
    def fill(framebuf, color):
        
        pass
    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        
        pass",snippet_251,5,"class MVLSBFormat:
    
    @staticmethod
    def _get_buffer(framebuf):
        if hasattr(framebuf, 'buffer'):
            return framebuf.buffer
        if hasattr(framebuf, 'buf'):
            return framebuf.buf
        if hasattr(framebuf, 'data'):
            return framebuf.data
        if isinstance(framebuf, (bytearray, memoryview)):
            return framebuf
        raise AttributeError(""Frame buffer object must provide a 'buffer' (or 'buf'/'data')."")
    
    @staticmethod
    def _get_width(framebuf):
        if hasattr(framebuf, 'width'):
            return framebuf.width
        raise AttributeError(""Frame buffer object must provide 'width'."")
    
    @staticmethod
    def _get_height(framebuf):
        if hasattr(framebuf, 'height'):
            return framebuf.height
        raise AttributeError(""Frame buffer object must provide 'height'."")
    
    @staticmethod
    def _get_stride(framebuf, width):
        # Stride is bytes per 8-pixel vertical band. For MONO_VLSB it's typically equal to width.
        return getattr(framebuf, 'stride', width)
    
    @staticmethod
    def _index_and_mask(x, y, stride):
        byte_index = x + (y >> 3) * stride
        bit_mask = 1 << (y & 7)
        return byte_index, bit_mask

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        buf = MVLSBFormat._get_buffer(framebuf)
        width = MVLSBFormat._get_width(framebuf)
        height = MVLSBFormat._get_height(framebuf)
        if x < 0 or y < 0 or x >= width or y >= height:
            return
        stride = MVLSBFormat._get_stride(framebuf, width)
        idx, mask = MVLSBFormat._index_and_mask(x, y, stride)
        if color:
            buf[idx] = buf[idx] | mask
        else:
            buf[idx] = buf[idx] & (~mask & 0xFF)

    @staticmethod
    def get_pixel(framebuf, x, y):
        '''Get the color of a given pixel'''
        buf = MVLSBFormat._get_buffer(framebuf)
        width = MVLSBFormat._get_width(framebuf)
        height = MVLSBFormat._get_height(framebuf)
        if x < 0 or y < 0 or x >= width or y >= height:
            return 0
        stride = MVLSBFormat._get_stride(framebuf, width)
        idx, mask = MVLSBFormat._index_and_mask(x, y, stride)
        return 1 if (buf[idx] & mask) else 0

    @staticmethod
    def fill(framebuf, color):
        buf = MVLSBFormat._get_buffer(framebuf)
        width = MVLSBFormat._get_width(framebuf)
        height = MVLSBFormat._get_height(framebuf)
        stride = MVLSBFormat._get_stride(framebuf, width)
        rows8 = (height + 7) // 8
        size = stride * rows8
        val = 0xFF if color else 0x00
        # Ensure we don't write beyond buffer length if provided buffer is smaller
        limit = min(size, len(buf))
        if isinstance(buf, memoryview):
            buf[:limit] = bytes([val]) * limit
        else:
            for i in range(limit):
                buf[i] = val

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        fb_width = MVLSBFormat._get_width(framebuf)
        fb_height = MVLSBFormat._get_height(framebuf)
        # Clip rectangle
        if width <= 0 or height <= 0:
            return
        x0 = max(0, x)
        y0 = max(0, y)
        x1 = min(fb_width, x + width)
        y1 = min(fb_height, y + height)
        if x0 >= x1 or y0 >= y1:
            return
        # Draw by setting pixels
        for yy in range(y0, y1):
            for xx in range(x0, x1):
                MVLSBFormat.set_pixel(framebuf, xx, yy, color)"
313491,adafruit/Adafruit_CircuitPython_framebuf,adafruit_Adafruit_CircuitPython_framebuf/adafruit_framebuf.py,adafruit_framebuf.RGB888Format,"class RGB888Format:
    """"""RGB888Format""""""

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        """"""Set a given pixel to a color.""""""
        index = (y * framebuf.stride + x) * 3
        if isinstance(color, tuple):
            framebuf.buf[index:index + 3] = bytes(color)
        else:
            framebuf.buf[index:index + 3] = bytes((color >> 16 & 255, color >> 8 & 255, color & 255))

    @staticmethod
    def get_pixel(framebuf, x, y):
        """"""Get the color of a given pixel""""""
        index = (y * framebuf.stride + x) * 3
        return framebuf.buf[index] << 16 | framebuf.buf[index + 1] << 8 | framebuf.buf[index + 2]

    @staticmethod
    def fill(framebuf, color):
        """"""completely fill/clear the buffer with a color""""""
        fill = (color >> 16 & 255, color >> 8 & 255, color & 255)
        for i in range(0, len(framebuf.buf), 3):
            framebuf.buf[i:i + 3] = bytes(fill)

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        """"""Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.""""""
        fill = (color >> 16 & 255, color >> 8 & 255, color & 255)
        for _x in range(x, x + width):
            for _y in range(y, y + height):
                index = (_y * framebuf.stride + _x) * 3
                framebuf.buf[index:index + 3] = bytes(fill)","class RGB888Format:
    '''RGB888Format'''
    @staticmethod
    def set_pixel(framebuf, x, y, color):
        '''Set a given pixel to a color.'''
        pass
    @staticmethod
    def get_pixel(framebuf, x, y):
        '''Get the color of a given pixel'''
        pass
    @staticmethod
    def fill(framebuf, color):
        
        pass
    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        
        pass",snippet_252,5,"class RGB888Format:
    '''RGB888Format'''

    @staticmethod
    def _resolve_framebuf(framebuf):
        # Extract buffer, width, height, and stride (bytes per row)
        buf = None
        width = None
        height = None
        stride = None

        # Object with attributes
        if hasattr(framebuf, 'buffer'):
            buf = framebuf.buffer
        elif hasattr(framebuf, 'buf'):
            buf = framebuf.buf
        elif hasattr(framebuf, 'data'):
            buf = framebuf.data
        elif isinstance(framebuf, (bytearray, bytes, memoryview)):
            # Only buffer provided; width/height/stride must be in attributes or elsewhere
            buf = framebuf

        if hasattr(framebuf, 'width'):
            width = framebuf.width
        if hasattr(framebuf, 'height'):
            height = framebuf.height
        if hasattr(framebuf, 'stride'):
            stride = framebuf.stride
        elif hasattr(framebuf, 'pitch'):
            stride = framebuf.pitch
        elif hasattr(framebuf, 'row_bytes'):
            stride = framebuf.row_bytes

        # Mapping-like
        if buf is None and isinstance(framebuf, dict):
            buf = framebuf.get('buffer') or framebuf.get('buf') or framebuf.get('data')
            width = framebuf.get('width', width)
            height = framebuf.get('height', height)
            stride = framebuf.get('stride', stride) or framebuf.get('pitch', stride) or framebuf.get('row_bytes', stride)

        # Tuple/list forms: (buf, width, height[, stride])
        if buf is None and isinstance(framebuf, (tuple, list)) and len(framebuf) >= 3:
            buf = framebuf[0]
            width = framebuf[1]
            height = framebuf[2]
            if len(framebuf) >= 4:
                stride = framebuf[3]

        if buf is None or width is None or height is None:
            raise ValueError(""framebuf must provide buffer, width, and height"")

        # Ensure buffer is mutable; allow memoryview of bytes if writable later
        mv = memoryview(buf)
        if not mv.contiguous:
            mv = mv.cast('B')

        # Default stride if not provided
        if stride is None:
            stride = int(width) * 3

        return mv, int(width), int(height), int(stride)

    @staticmethod
    def _color_to_rgb(color):
        # Accept int 0xRRGGBB or (r,g,b)
        if isinstance(color, int):
            r = (color >> 16) & 0xFF
            g = (color >> 8) & 0xFF
            b = color & 0xFF
            return r, g, b
        if isinstance(color, (tuple, list)) and len(color) == 3:
            r, g, b = color
            return int(r) & 0xFF, int(g) & 0xFF, int(b) & 0xFF
        raise ValueError(""color must be int 0xRRGGBB or (r, g, b)"")

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        '''Set a given pixel to a color.'''
        buf, width, height, stride = RGB888Format._resolve_framebuf(framebuf)
        xi = int(x)
        yi = int(y)
        if xi < 0 or yi < 0 or xi >= width or yi >= height:
            return
        r, g, b = RGB888Format._color_to_rgb(color)
        idx = yi * stride + xi * 3
        # Ensure writable view
        if buf.readonly:
            raise TypeError(""frame buffer is read-only"")
        buf[idx] = r
        buf[idx + 1] = g
        buf[idx + 2] = b

    @staticmethod
    def get_pixel(framebuf, x, y):
        '''Get the color of a given pixel'''
        buf, width, height, stride = RGB888Format._resolve_framebuf(framebuf)
        xi = int(x)
        yi = int(y)
        if xi < 0 or yi < 0 or xi >= width or yi >= height:
            return 0
        idx = yi * stride + xi * 3
        r = buf[idx]
        g = buf[idx + 1]
        b = buf[idx + 2]
        return (r << 16) | (g << 8) | b

    @staticmethod
    def fill(framebuf, color):
        buf, width, height, stride = RGB888Format._resolve_framebuf(framebuf)
        if buf.readonly:
            raise TypeError(""frame buffer is read-only"")
        r, g, b = RGB888Format._color_to_rgb(color)
        # Build one row worth of pixel data for the active width
        row_active = bytes([r, g, b]) * width
        for y in range(height):
            base = y * stride
            buf[base:base + width * 3] = row_active

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        buf, fbw, fbh, stride = RGB888Format._resolve_framebuf(framebuf)
        if buf.readonly:
            raise TypeError(""frame buffer is read-only"")

        x0 = int(x)
        y0 = int(y)
        w = int(width)
        h = int(height)

        if w <= 0 or h <= 0:
            return

        # Clamp rectangle to framebuffer bounds
        if x0 < 0:
            w += x0
            x0 = 0
        if y0 < 0:
            h += y0
            y0 = 0
        if x0 >= fbw or y0 >= fbh:
            return
        if x0 + w > fbw:
            w = fbw - x0
        if y0 + h > fbh:
            h = fbh - y0
        if w <= 0 or h <= 0:
            return

        r, g, b = RGB888Format._color_to_rgb(color)
        row_fill = bytes([r, g, b]) * w

        for yy in range(y0, y0 + h):
            start = yy * stride + x0 * 3
            buf[start:start + w * 3] = row_fill"
319441,pysal/spglm,spglm/varfuncs.py,spglm.varfuncs.VarianceFunction,"import numpy as np

class VarianceFunction:
    """"""
    Relates the variance of a random variable to its mean. Defaults to 1.

    Methods
    -------
    call
        Returns an array of ones that is the same shape as `mu`

    Notes
    -----
    After a variance function is initialized, its call method can be used.

    Alias for VarianceFunction:
    constant = VarianceFunction()

    See also
    --------
    statsmodels.family.family
    """"""

    def __call__(self, mu):
        """"""
        Default variance function

        Parameters
        -----------
        mu : array-like
            mean parameters

        Returns
        -------
        v : array
            ones(mu.shape)
        """"""
        mu = np.asarray(mu)
        return np.ones(mu.shape, np.float64)

    def deriv(self, mu):
        """"""
        Derivative of the variance function v'(mu)
        """"""
        from statsmodels.tools.numdiff import approx_fprime_cs
        return np.diag(approx_fprime_cs(mu, self))","
class VarianceFunction:
    '''
    Relates the variance of a random variable to its mean. Defaults to 1.
    Methods
    -------
    call
        Returns an array of ones that is the same shape as `mu`
    Notes
    -----
    After a variance function is initialized, its call method can be used.
    Alias for VarianceFunction:
    constant = VarianceFunction()
    See also
    --------
    statsmodels.family.family
    '''
    def __call__(self, mu):
        '''
        Default variance function
        Parameters
        -----------
        mu : array-like
            mean parameters
        Returns
        -------
        v : array
            ones(mu.shape)
        '''
        pass
    def deriv(self, mu):
        
        pass",snippet_256,3,"import numpy as np

class VarianceFunction:
    '''
    Relates the variance of a random variable to its mean. Defaults to 1.
    Methods
    -------
    call
        Returns an array of ones that is the same shape as `mu`
    Notes
    -----
    After a variance function is initialized, its call method can be used.
    Alias for VarianceFunction:
    constant = VarianceFunction()
    See also
    --------
    statsmodels.family.family
    '''
    def __call__(self, mu):
        '''
        Default variance function
        Parameters
        -----------
        mu : array-like
            mean parameters
        Returns
        -------
        v : array
            ones(mu.shape)
        '''
        return np.ones(np.shape(mu), dtype=float)

    def deriv(self, mu):
        return np.zeros(np.shape(mu), dtype=float)

constant = VarianceFunction()"
320689,optimizely/python-sdk,optimizely_python-sdk/optimizely/user_profile.py,optimizely.user_profile.UserProfileService,"from typing import Any, Optional

class UserProfileService:
    """""" Class encapsulating user profile service functionality.
  Override with your own implementation for storing and retrieving the user profile. """"""

    def lookup(self, user_id: str) -> dict[str, Any]:
        """""" Fetch the user profile dict corresponding to the user ID.

    Args:
      user_id: ID for user whose profile needs to be retrieved.

    Returns:
      Dict representing the user's profile.
    """"""
        return UserProfile(user_id).__dict__

    def save(self, user_profile: dict[str, Any]) -> None:
        """""" Save the user profile dict sent to this method.

    Args:
      user_profile: Dict representing the user's profile.
    """"""
        pass","
class UserProfileService:
    ''' Class encapsulating user profile service functionality.
  Override with your own implementation for storing and retrieving the user profile. '''
    def lookup(self, user_id: str) -> dict[str, Any]:
        
        pass
    def save(self, user_profile: dict[str, Any]) -> None:
        ''' Save the user profile dict sent to this method.
    Args:
      user_profile: Dict representing the user's profile.
        '''
        pass",snippet_257,3,"from typing import Any, Dict
import threading
import copy


class UserProfileService:
    '''Class encapsulating user profile service functionality.
    Override with your own implementation for storing and retrieving the user profile.'''

    def __init__(self) -> None:
        self._store: Dict[str, Dict[str, Any]] = {}
        self._lock = threading.RLock()

    def lookup(self, user_id: str) -> dict[str, Any]:
        if not isinstance(user_id, str) or not user_id:
            raise ValueError(""user_id must be a non-empty string"")
        with self._lock:
            if user_id not in self._store:
                raise KeyError(f""User profile not found for id: {user_id}"")
            return copy.deepcopy(self._store[user_id])

    def save(self, user_profile: dict[str, Any]) -> None:
        '''Save the user profile dict sent to this method.
        Args:
          user_profile: Dict representing the user's profile.
        '''
        if not isinstance(user_profile, dict):
            raise TypeError(""user_profile must be a dict"")
        if ""id"" not in user_profile:
            raise ValueError(""user_profile must contain an 'id' field"")
        user_id = user_profile[""id""]
        if not isinstance(user_id, str) or not user_id:
            raise ValueError(""'id' field must be a non-empty string"")
        with self._lock:
            self._store[user_id] = copy.deepcopy(user_profile)"
322746,wilson-eft/wilson,wilson-eft_wilson/wilson/classes.py,wilson.classes.RGsolution,"import numpy as np
from math import log, e

class RGsolution:
    """"""Class representing a continuous (interpolated) solution to the
    SMEFT RGEs to be used for plotting.""""""

    def __init__(self, fun, scale_min, scale_max):
        """"""Initialize.

        Parameters:

        - fun: function of the scale that is expected to return a
        dictionary with the RGE solution and to accept vectorized input.
        - scale_min, scale_max: lower and upper boundaries of the scale
        """"""
        self.fun = fun
        self.scale_min = scale_min
        self.scale_max = scale_max

    def plotdata(self, key, part='re', scale='log', steps=50):
        """"""Return a tuple of arrays x, y that can be fed to plt.plot,
        where x is the scale in GeV and y is the parameter of interest.

        Parameters:

        - key: dicionary key of the parameter to be plotted (e.g. a WCxf
          coefficient name or a SM parameter like 'g')
        - part: plot the real part 're' (default) or the imaginary part 'im'
        - scale: 'log'; make the x steps logarithmically distributed; for
          'linear', linearly distributed
        - steps: steps in x to take (default: 50)
        """"""
        if scale == 'log':
            x = np.logspace(log(self.scale_min), log(self.scale_max), steps, base=e)
        elif scale == 'linear':
            x = np.linspace(self.scale_min, self.scale_max, steps)
        y = self.fun(x)
        y = np.array([d[key] for d in y])
        if part == 're':
            return (x, y.real)
        elif part == 'im':
            return (x, y.imag)

    def plot(self, key, part='re', scale='log', steps=50, legend=True, plotargs={}):
        """"""Plot the RG evolution of parameter `key`.

        Parameters:

        - part, scale, steps: see `plotdata`
        - legend: boolean, show the legend (default: True)
        - plotargs: dictionary of arguments to be passed to plt.plot
        """"""
        try:
            import matplotlib.pyplot as plt
        except ImportError:
            raise ImportError('Please install matplotlib if you want to use the plot method')
        pdat = self.plotdata(key, part=part, scale=scale, steps=steps)
        plt.plot(*pdat, label=key, **plotargs)
        if scale == 'log':
            plt.xscale('log')
        if legend:
            plt.legend()","
class RGsolution:
    '''Class representing a continuous (interpolated) solution to the
    SMEFT RGEs to be used for plotting.'''
    def __init__(self, fun, scale_min, scale_max):
        '''Initialize.
        Parameters:
        - fun: function of the scale that is expected to return a
        dictionary with the RGE solution and to accept vectorized input.
        - scale_min, scale_max: lower and upper boundaries of the scale
        '''
        pass
    def plotdata(self, key, part='re', scale='log', steps=50):
        '''Return a tuple of arrays x, y that can be fed to plt.plot,
        where x is the scale in GeV and y is the parameter of interest.
        Parameters:
        - key: dicionary key of the parameter to be plotted (e.g. a WCxf
          coefficient name or a SM parameter like 'g')
        - part: plot the real part 're' (default) or the imaginary part 'im'
        - scale: 'log'; make the x steps logarithmically distributed; for
          'linear', linearly distributed
        - steps: steps in x to take (default: 50)
        '''
        pass
    def plotdata(self, key, part='re', scale='log', steps=50):
        '''Plot the RG evolution of parameter `key`.
        Parameters:
        - part, scale, steps: see `plotdata`
        - legend: boolean, show the legend (default: True)
        - plotargs: dictionary of arguments to be passed to plt.plot
        '''
        pass",snippet_258,4,"import numpy as np
import matplotlib.pyplot as plt

class RGsolution:
    '''Class representing a continuous (interpolated) solution to the
    SMEFT RGEs to be used for plotting.'''
    def __init__(self, fun, scale_min, scale_max):
        '''Initialize.
        Parameters:
        - fun: function of the scale that is expected to return a
        dictionary with the RGE solution and to accept vectorized input.
        - scale_min, scale_max: lower and upper boundaries of the scale
        '''
        if scale_min <= 0 or scale_max <= 0:
            raise ValueError(""scale_min and scale_max must be positive for log scaling."")
        if scale_min >= scale_max:
            raise ValueError(""scale_min must be smaller than scale_max."")
        if not callable(fun):
            raise TypeError(""fun must be callable."")
        self.fun = fun
        self.scale_min = float(scale_min)
        self.scale_max = float(scale_max)

    def plotdata(self, key, part='re', scale='log', steps=50):
        '''Return a tuple of arrays x, y that can be fed to plt.plot,
        where x is the scale in GeV and y is the parameter of interest.
        Parameters:
        - key: dicionary key of the parameter to be plotted (e.g. a WCxf
          coefficient name or a SM parameter like 'g')
        - part: plot the real part 're' (default) or the imaginary part 'im'
        - scale: 'log'; make the x steps logarithmically distributed; for
          'linear', linearly distributed
        - steps: steps in x to take (default: 50)
        '''
        if steps < 2:
            raise ValueError(""steps must be >= 2."")
        if scale not in ('log', 'linear'):
            raise ValueError(""scale must be 'log' or 'linear'."")
        if part not in ('re', 'im'):
            raise ValueError(""part must be 're' or 'im'."")

        if scale == 'log':
            x = np.logspace(np.log10(self.scale_min), np.log10(self.scale_max), steps)
        else:
            x = np.linspace(self.scale_min, self.scale_max, steps)

        try:
            out = self.fun(x)
        except Exception:
            # Fallback to non-vectorized evaluation
            vals = []
            for xi in x:
                d = self.fun(float(xi))
                if key not in d:
                    raise KeyError(f""Key '{key}' not found in solution dictionary."")
                vals.append(d[key])
            y = np.asarray(vals)
        else:
            if key not in out:
                raise KeyError(f""Key '{key}' not found in solution dictionary."")
            y = np.asarray(out[key])

        # Ensure y is 1D aligned with x
        y = np.squeeze(y)
        if y.shape != x.shape:
            # Attempt broadcasting if possible
            try:
                y = np.broadcast_to(y, x.shape)
            except Exception:
                raise ValueError(f""Shape of y {y.shape} is incompatible with x {x.shape}."")

        # Select part
        if np.iscomplexobj(y):
            y = np.real(y) if part == 're' else np.imag(y)
        else:
            if part == 'im':
                y = np.zeros_like(y, dtype=float)
            else:
                y = y.astype(float)

        return x, y

    def plot(self, key, part='re', scale='log', steps=50, legend=True, plotargs=None, label=None):
        '''Plot the RG evolution of parameter `key`.
        Parameters:
        - part, scale, steps: see `plotdata`
        - legend: boolean, show the legend (default: True)
        - plotargs: dictionary of arguments to be passed to plt.plot
        '''
        x, y = self.plotdata(key, part=part, scale=scale, steps=steps)
        if plotargs is None:
            plotargs = {}
        if label is None:
            label = f""{key} ({part})""
        line, = plt.plot(x, y, label=label, **plotargs)
        if scale == 'log':
            plt.xscale('log')
        plt.xlabel(""Scale [GeV]"")
        plt.ylabel(key)
        if legend:
            plt.legend()
        return line"
322803,wilson-eft/wilson,wilson-eft_wilson/wilson/wcxf/classes.py,wilson.wcxf.classes.WCxf,"import yaml

class WCxf:
    """"""Base class for WCxf files (not meant to be used directly).""""""

    @classmethod
    def load(cls, stream, **kwargs):
        """"""Load the object data from a JSON or YAML file.""""""
        wcxf = _load_yaml_json(stream, **kwargs)
        return cls(**wcxf)

    def dump(self, stream=None, fmt='json', **kwargs):
        """"""Dump the object data to a JSON or YAML file.

        Optional arguments:

        - `stream`: if None (default), return a string. Otherwise,
          should be a writable file-like object
        - `fmt`: format, should be 'json' (default) or 'yaml'

        Additional keyword arguments will be passed to the `json.dump(s)`
        or `yaml.dump` methods.
        """"""
        d = {k: v for k, v in self.__dict__.items() if k[0] != '_'}
        if fmt.lower() == 'json':
            indent = kwargs.pop('indent', 2)
            return _dump_json(d, stream=stream, indent=indent, **kwargs)
        elif fmt.lower() == 'yaml':
            default_flow_style = kwargs.pop('default_flow_style', False)
            return yaml.dump(d, stream, default_flow_style=default_flow_style, **kwargs)
        else:
            raise ValueError(f""Format {fmt} unknown: use 'json' or 'yaml'."")","
class WCxf:
    '''Base class for WCxf files (not meant to be used directly).'''
    @classmethod
    def load(cls, stream, **kwargs):
        '''Load the object data from a JSON or YAML file.'''
        pass
    def dump(self, stream=None, fmt='json', **kwargs):
        '''Dump the object data to a JSON or YAML file.
        Optional arguments:
        - `stream`: if None (default), return a string. Otherwise,
          should be a writable file-like object
        - `fmt`: format, should be 'json' (default) or 'yaml'
        Additional keyword arguments will be passed to the `json.dump(s)`
        or `yaml.dump` methods.
        '''
        pass",snippet_259,3,"class WCxf:
    '''Base class for WCxf files (not meant to be used directly).'''
    @classmethod
    def load(cls, stream, **kwargs):
        '''Load the object data from a JSON or YAML file.'''
        import json

        def _read_stream(s):
            if hasattr(s, 'read'):
                return s.read()
            return s

        text = _read_stream(stream)

        # Try JSON first
        try:
            if hasattr(stream, 'read'):
                # Re-parse from text to allow retrying YAML regardless of stream state
                return json.loads(text, **kwargs)
            else:
                return json.loads(text, **kwargs)
        except Exception:
            pass

        # Fallback to YAML
        try:
            import yaml
        except ImportError as e:
            raise ValueError(""Failed to parse as JSON; PyYAML not installed to try YAML."") from e

        try:
            return yaml.safe_load(text, **kwargs)
        except Exception as e:
            raise ValueError(""Failed to parse input as JSON or YAML."") from e

    def dump(self, stream=None, fmt='json', **kwargs):
        '''Dump the object data to a JSON or YAML file.
        Optional arguments:
        - `stream`: if None (default), return a string. Otherwise,
          should be a writable file-like object
        - `fmt`: format, should be 'json' (default) or 'yaml'
        Additional keyword arguments will be passed to the `json.dump(s)`
        or `yaml.dump` methods.
        '''
        data = dict(self.__dict__)

        if fmt not in ('json', 'yaml'):
            raise ValueError(""fmt must be 'json' or 'yaml'"")

        if fmt == 'json':
            import json
            if stream is None:
                return json.dumps(data, **kwargs)
            json.dump(data, stream, **kwargs)
            return None

        # YAML
        try:
            import yaml
        except ImportError as e:
            raise ValueError(""PyYAML is required for YAML output."") from e

        if stream is None:
            return yaml.safe_dump(data, **kwargs)
        yaml.safe_dump(data, stream, **kwargs)
        return None"
322855,jhermann/rituals,jhermann_rituals/src/rituals/util/antglob.py,rituals.util.antglob.Pattern,"class Pattern:
    """"""A single pattern for either inclusion or exclusion.""""""

    def __init__(self, spec, inclusive):
        """"""Create regex-based pattern matcher from glob `spec`.""""""
        self.compiled = compile_glob(spec.rstrip('/'))
        self.inclusive = inclusive
        self.is_dir = spec.endswith('/')

    def __str__(self):
        """"""Return inclusiveness indicator and original glob pattern.""""""
        return ('+' if self.inclusive else '-') + self.compiled.pattern

    def matches(self, path):
        """"""Check this pattern against given `path`.""""""
        return bool(self.compiled.match(path))","class Pattern:
    '''A single pattern for either inclusion or exclusion.'''
    def __init__(self, spec, inclusive):
        '''Create regex-based pattern matcher from glob `spec`.'''
        pass
    def __str__(self):
        
        pass
    def matches(self, path):
        
        pass",snippet_260,4,"import re
import fnmatch

class Pattern:
    '''A single pattern for either inclusion or exclusion.'''
    def __init__(self, spec, inclusive):
        '''Create regex-based pattern matcher from glob `spec`.'''
        self.spec = str(spec)
        self.inclusive = bool(inclusive)
        self._regex = re.compile(fnmatch.translate(self.spec))

    def __str__(self):
        return f""{'+' if self.inclusive else '-'}{self.spec}""

    def matches(self, path):
        s = str(path)
        return self._regex.match(s) is not None"
323184,SoftwareDefinedBuildings/XBOS,apps/Data_quality_analysis/Import_Data.py,Data_quality_analysis.Import_Data.Import_Data,"import pandas as pd
import glob
import numpy as np
import os

class Import_Data:
    """""" This class imports data from csv files. """"""

    def __init__(self):
        """""" Constructor: Store the imported data. """"""
        self.data = pd.DataFrame()

    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        """""" Imports csv file(s) and stores the result in data.

        Note
        ----
        1. If folder exists out of current directory, folder_name should contain correct regex
        2. Assuming there's no file called ""\\*.csv""

        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '\\*', i.e. all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.', i.e. current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe

        """"""
        if isinstance(file_name, str) and isinstance(folder_name, str):
            try:
                self.data = self._load_csv(file_name, folder_name, head_row, index_col, convert_col, concat_files)
            except Exception as e:
                raise e
        elif isinstance(file_name, list) and isinstance(folder_name, str):
            for i, file in enumerate(file_name):
                if isinstance(head_row, list):
                    _head_row = head_row[i]
                else:
                    _head_row = head_row
                if isinstance(index_col, list):
                    _index_col = index_col[i]
                else:
                    _index_col = index_col
                try:
                    data_tmp = self._load_csv(file, folder_name, _head_row, _index_col, convert_col, concat_files)
                    if concat_files:
                        self.data = self.data.append(data_tmp, ignore_index=False, verify_integrity=False)
                    else:
                        self.data = self.data.join(data_tmp, how='outer')
                except Exception as e:
                    raise e
        else:
            raise NotImplementedError(""Filename and Folder name can't both be of type list."")

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        """""" Load single csv file.

        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '*' - all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.' - current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe

        Returns
        -------
        pd.DataFrame()
            Dataframe containing csv data

        """"""
        if file_name == '*':
            if not os.path.isdir(folder_name):
                raise OSError('Folder does not exist.')
            else:
                file_name_list = sorted(glob.glob(folder_name + '*.csv'))
                if not file_name_list:
                    raise OSError('Either the folder does not contain any csv files or invalid folder provided.')
                else:
                    self.import_csv(file_name=file_name_list, head_row=head_row, index_col=index_col, convert_col=convert_col, concat_files=concat_files)
                    return self.data
        elif not os.path.isdir(folder_name):
            raise OSError('Folder does not exist.')
        else:
            path = os.path.join(folder_name, file_name)
            if head_row > 0:
                data = pd.read_csv(path, index_col=index_col, skiprows=[i for i in range(head_row - 1)])
            else:
                data = pd.read_csv(path, index_col=index_col)
            try:
                data.index = pd.to_datetime(data.index, format='%m/%d/%y %H:%M')
            except:
                data.index = pd.to_datetime(data.index, dayfirst=False, infer_datetime_format=True)
        if convert_col:
            for col in data.columns:
                if data[col].dtype != np.number:
                    data[col] = pd.to_numeric(data[col], errors='coerce')
        return data","
class Import_Data:
    ''' This class imports data from csv files. '''
    def __init__(self):
        
        pass
    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        
        pass
    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        
        pass",snippet_261,4,"import os
import glob
from typing import Union, Dict, List
import pandas as pd


class Import_Data:
    ''' This class imports data from csv files. '''
    def __init__(self):
        self.last_files: List[str] = []
        self.last_result = None

    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        files = self._resolve_files(file_name, folder_name)
        if not files:
            raise FileNotFoundError(f'No CSV files found for pattern ""{file_name}"" in ""{folder_name}"".')
        results: Dict[str, pd.DataFrame] = {}
        for f in files:
            df = self._load_csv(f, folder_name, head_row, index_col, convert_col, concat_files)
            results[os.path.basename(f)] = df

        self.last_files = files

        if concat_files:
            concatenated = pd.concat(
                [results[k].assign(_source_file=k) for k in results],
                axis=0,
                ignore_index=False
            )
            self.last_result = concatenated
            return concatenated

        if len(results) == 1:
            single = next(iter(results.values()))
            self.last_result = single
            return single

        self.last_result = results
        return results

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        path = file_name if os.path.isabs(file_name) else os.path.join(folder_name, file_name)
        if not os.path.exists(path):
            raise FileNotFoundError(f'File not found: {path}')

        df = pd.read_csv(path, header=head_row if head_row is not None else 'infer',
                         index_col=index_col if index_col is not None else None)

        if convert_col:
            for col in df.columns:
                try:
                    df[col] = pd.to_numeric(df[col], errors='ignore')
                except Exception:
                    pass
        return df

    def _resolve_files(self, file_name: Union[str, List[str]], folder_name: str) -> List[str]:
        if isinstance(file_name, list):
            files = []
            for f in file_name:
                pattern = f if os.path.isabs(f) else os.path.join(folder_name, f)
                if any(ch in f for ch in ['*', '?', '[']):
                    files.extend(sorted(glob.glob(pattern)))
                else:
                    path = pattern
                    if path.lower().endswith('.csv') and os.path.exists(path):
                        files.append(path)
                    else:
                        candidate = path if path.lower().endswith('.csv') else f'{path}.csv'
                        if os.path.exists(candidate):
                            files.append(candidate)
            return sorted(set(files))

        pattern = file_name
        if not any(ch in pattern for ch in ['*', '?', '[']):
            base = file_name if os.path.isabs(file_name) else os.path.join(folder_name, file_name)
            if base.lower().endswith('.csv'):
                return [base] if os.path.exists(base) else []
            candidate = f'{base}.csv'
            return [candidate] if os.path.exists(candidate) else []

        search_pattern = pattern if os.path.isabs(pattern) else os.path.join(folder_name, pattern if pattern.endswith('.csv') else f'{pattern}.csv' if pattern != '*' else '*.csv')
        return sorted(glob.glob(search_pattern))"
323187,SoftwareDefinedBuildings/XBOS,apps/Data_quality_analysis/Plot_Data.py,Data_quality_analysis.Plot_Data.Plot_Data,"import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

class Plot_Data:
    """""" This class contains functions for displaying various plots.

    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.

    """"""
    count = 1

    def __init__(self, figsize=(18, 5)):
        """""" Constructor.

        Parameters
        ----------
        figsize : tuple
            Size of figure.

        """"""
        self.figsize = figsize

    def correlation_plot(self, data):
        """""" Create heatmap of Pearson's correlation coefficient.

        Parameters
        ----------
        data    : pd.DataFrame()
            Data to display.

        Returns
        -------
        matplotlib.figure
            Heatmap.

        """"""
        fig = plt.figure(Plot_Data.count)
        corr = data.corr()
        ax = sns.heatmap(corr)
        Plot_Data.count += 1
        return fig

    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        """""" Create baseline and projection plots.

        Parameters
        ----------
        y_true              : pd.Series()
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.Dataframe()
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.

        Returns
        -------
        matplotlib.figure
            Baseline plot

        """"""
        fig = plt.figure(Plot_Data.count)
        if projection_period:
            nrows = len(baseline_period) + len(projection_period) / 2
        else:
            nrows = len(baseline_period) / 2
        base_df = pd.DataFrame()
        base_df['y_true'] = y_true
        base_df['y_pred'] = y_pred
        ax1 = fig.add_subplot(nrows, 1, 1)
        base_df.plot(ax=ax1, figsize=self.figsize, title='Baseline Period ({}-{}). \nBest Model: {}. \nBaseline Adj R2: {}. \nSite: {}.'.format(baseline_period[0], baseline_period[1], model_name, adj_r2, site))
        if projection_period:
            num_plot = 2
            for i in range(0, len(projection_period), 2):
                ax = fig.add_subplot(nrows, 1, num_plot)
                period = slice(projection_period[i], projection_period[i + 1])
                project_df = pd.DataFrame()
                try:
                    project_df['y_true'] = data.loc[period, output_col]
                    project_df['y_pred'] = model.predict(data.loc[period, input_col])
                    project_df['y_pred'][project_df['y_pred'] < 0] = 0
                    project_df.plot(ax=ax, figsize=self.figsize, title='Projection Period ({}-{})'.format(projection_period[i], projection_period[i + 1]))
                    num_plot += 1
                    fig.tight_layout()
                    Plot_Data.count += 1
                    return (fig, project_df['y_true'], project_df['y_pred'])
                except:
                    raise TypeError('If projecting into the future, please specify project_ind_col that has data available                                         in the future time period requested.')
        return (fig, None, None)","
class Plot_Data:
    ''' This class contains functions for displaying various plots.
    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.
    '''
    def __init__(self, figsize=(18, 5)):
        
        pass
    def correlation_plot(self, data):
        
        pass
    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        ''' Create baseline and projection plots.
        Parameters
        ----------
        y_true              : pd.Series()
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.Dataframe()
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.
        Returns
        -------
        matplotlib.figure
            Baseline plot
        '''
        pass",snippet_262,4,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Union, Tuple


class Plot_Data:
    ''' This class contains functions for displaying various plots.
    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.
    '''
    def __init__(self, figsize=(18, 5)):
        self.figsize = figsize
        self.count = 0
        sns.set_style(""whitegrid"")

    def correlation_plot(self, data):
        if not isinstance(data, pd.DataFrame):
            raise TypeError(""data must be a pandas DataFrame"")
        corr = data.select_dtypes(include=[np.number]).corr()
        mask = np.triu(np.ones_like(corr, dtype=bool))
        fig, ax = plt.subplots(figsize=self.figsize)
        sns.heatmap(
            corr,
            mask=mask,
            cmap=""coolwarm"",
            annot=True,
            fmt="".2f"",
            square=True,
            cbar_kws={""shrink"": 0.8},
            ax=ax
        )
        ax.set_title(""Correlation Heatmap"")
        plt.tight_layout()
        self.count += 1
        return fig

    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        ''' Create baseline and projection plots.
        Parameters
        ----------
        y_true              : pd.Series()
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.Dataframe()
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.
        Returns
        -------
        matplotlib.figure
            Baseline plot
        '''
        if not isinstance(data, pd.DataFrame):
            raise TypeError(""data must be a pandas DataFrame"")
        if isinstance(input_col, str):
            input_cols = [input_col]
        else:
            input_cols = list(input_col)

        # Ensure datetime index for slicing
        df = data.copy()
        if not isinstance(df.index, pd.DatetimeIndex):
            try:
                df.index = pd.to_datetime(df.index)
            except Exception as e:
                raise ValueError(""DataFrame index must be datetime-like or convertible to datetime."") from e

        def _build_mask(period_spec) -> pd.Series:
            # Supports [""start"", ""end""] or list of such pairs
            if isinstance(period_spec, (list, tuple)) and len(period_spec) == 2 and all(isinstance(x, str) for x in period_spec):
                start, end = pd.to_datetime(period_spec[0]), pd.to_datetime(period_spec[1])
                return (df.index >= start) & (df.index <= end)
            # If list of pairs
            mask = pd.Series(False, index=df.index)
            for item in period_spec:
                if isinstance(item, (list, tuple)) and len(item) == 2:
                    start, end = pd.to_datetime(item[0]), pd.to_datetime(item[1])
                    mask |= ((df.index >= start) & (df.index <= end))
            return mask

        # Baseline period mask
        base_mask = _build_mask(baseline_period)

        # Align y_true and y_pred for baseline
        if not isinstance(y_true, (pd.Series, pd.DataFrame)):
            y_true = pd.Series(y_true, index=df.index[base_mask])
        else:
            if not isinstance(y_true.index, pd.DatetimeIndex):
                try:
                    y_true.index = pd.to_datetime(y_true.index)
                except Exception as e:
                    raise ValueError(""y_true index must be datetime-like or convertible to datetime."") from e

        y_true_baseline = y_true.loc[base_mask] if isinstance(y_true, pd.Series) else y_true.squeeze().loc[base_mask]
        y_pred_baseline = pd.Series(np.asarray(y_pred).reshape(-1), index=y_true_baseline.index)

        # Projection mask
        proj_mask = _build_mask(projection_period)
        X_proj = df.loc[proj_mask, input_cols]
        y_proj_true = None
        if output_col in df.columns:
            y_proj_true = df.loc[proj_mask, output_col]
        y_proj_pred = None
        if hasattr(model, ""predict""):
            try:
                y_proj_pred = pd.Series(np.asarray(model.predict(X_proj)).reshape(-1), index=X_proj.index)
            except Exception:
                y_proj_pred = None

        fig, axes = plt.subplots(1, 2, figsize=self.figsize, sharey=False)
        ax1, ax2 = axes

        # Baseline plot
        ax1.plot(y_true_baseline.index, y_true_baseline.values, label=""Actual"", color=""#1f77b4"", linewidth=2)
        ax1.plot(y_pred_baseline.index, y_pred_baseline.values, label=""Predicted"", color=""#ff7f0e"", linewidth=2, alpha=0.85)
        ax1.set_title(f""{site} | Baseline\n{model_name} (Adj R2={adj_r2:.3f})"")
        ax1.set_xlabel(""Date"")
        ax1.set_ylabel(output_col if isinstance(output_col, str) else ""Target"")
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # Projection plot
        plotted_any = False
        if y_proj_pred is not None:
            ax2.plot(y_proj_pred.index, y_proj_pred.values, label=""Projected (Predicted)"", color=""#d62728"", linewidth=2)
            plotted_any = True
        if y_proj_true is not None and len(y_proj_true) > 0:
            ax2.plot(y_proj_true.index, y_proj_true.values, label=""Actual"", color=""#2ca02c"", linewidth=2, alpha=0.85)
            plotted_any = True

        ax2.set_title(f""{site} | Projection\n{model_name}"")
        ax2.set_xlabel(""Date"")
        ax2.set_ylabel(output_col if isinstance(output_col, str) else ""Target"")
        if plotted_any:
            ax2.legend()
        ax2.grid(True, alpha=0.3)

        # Visual shading for periods if they are a single range
        def _maybe_span(ax, period, color, alpha=0.08):
            if isinstance(period, (list, tuple)) and len(period) == 2 and all(isinstance(x, str) for x in period):
                start, end = pd.to_datetime(period[0]), pd.to_datetime(period[1])
                ax.axvspan(start, end, color=color, alpha=alpha)

        _maybe_span(ax1, baseline_period, ""#1f77b4"")
        _maybe_span(ax2, projection_period, ""#d62728"")

        plt.tight_layout()
        self.count += 1
        return fig"
323204,SoftwareDefinedBuildings/XBOS,apps/data_analysis/XBOS_data_analytics/Import_Data.py,XBOS_data_analytics.Import_Data.Import_Data,"import os
import pandas as pd
import numpy as np
import glob

class Import_Data:
    """""" This class imports data from csv files """"""

    def __init__(self):
        """""" Constructor.

        This class stores the imported data.

        """"""
        self.data = pd.DataFrame()

    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        """""" Imports csv file(s) and stores the result in data.

        Note
        ----
        1. If folder exists out of current directory, folder_name should contain correct regex
        2. Assuming there's no file called ""\\*.csv""

        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '\\*', i.e. all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.', i.e. current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe

        """"""
        if isinstance(file_name, str) and isinstance(folder_name, str):
            try:
                self.data = self._load_csv(file_name, folder_name, head_row, index_col, convert_col, concat_files)
            except Exception as e:
                raise e
        elif isinstance(file_name, list) and isinstance(folder_name, str):
            for i, file in enumerate(file_name):
                if isinstance(head_row, list):
                    _head_row = head_row[i]
                else:
                    _head_row = head_row
                if isinstance(index_col, list):
                    _index_col = index_col[i]
                else:
                    _index_col = index_col
                try:
                    data_tmp = self._load_csv(file, folder_name, _head_row, _index_col, convert_col, concat_files)
                    if concat_files:
                        self.data = self.data.append(data_tmp, ignore_index=False, verify_integrity=False)
                    else:
                        self.data = self.data.join(data_tmp, how='outer')
                except Exception as e:
                    raise e
        else:
            raise NotImplementedError(""Filename and Folder name can't both be of type list."")

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        """""" Load single csv file.

        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '*' - all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.' - current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe

        Returns
        -------
        pd.DataFrame()
            Dataframe containing csv data

        """"""
        if file_name == '*':
            if not os.path.isdir(folder_name):
                raise OSError('Folder does not exist.')
            else:
                file_name_list = sorted(glob.glob(folder_name + '*.csv'))
                if not file_name_list:
                    raise OSError('Either the folder does not contain any csv files or invalid folder provided.')
                else:
                    self.import_csv(file_name=file_name_list, head_row=head_row, index_col=index_col, convert_col=convert_col, concat_files=concat_files)
                    return self.data
        elif not os.path.isdir(folder_name):
            raise OSError('Folder does not exist.')
        else:
            path = os.path.join(folder_name, file_name)
            if head_row > 0:
                data = pd.read_csv(path, index_col=index_col, skiprows=[i for i in range(head_row - 1)])
            else:
                data = pd.read_csv(path, index_col=index_col)
            try:
                data.index = pd.to_datetime(data.index, format='%m/%d/%y %H:%M')
            except:
                data.index = pd.to_datetime(data.index, dayfirst=False, infer_datetime_format=True)
        if convert_col:
            for col in data.columns:
                if data[col].dtype != np.number:
                    data[col] = pd.to_numeric(data[col], errors='coerce')
        return data","
class Import_Data:
    
    def __init__(self):
        ''' Constructor.
        This class stores the imported data.
        '''
        pass
    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        
        pass
    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        ''' Load single csv file.
        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '*' - all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.' - current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe
        Returns
        -------
        pd.DataFrame()
            Dataframe containing csv data
        '''
        pass",snippet_263,4,"import os
import glob
import pandas as pd


class Import_Data:
    
    def __init__(self):
        ''' Constructor.
        This class stores the imported data.
        '''
        self.data = {}
        self.concatenated = None
        self.files = []

    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        # Resolve file paths
        folder = os.path.abspath(folder_name)
        if file_name == '*' or '*' in file_name or '?' in file_name:
            pattern = file_name if file_name != '*' else '*.csv'
            paths = sorted(glob.glob(os.path.join(folder, pattern)))
        else:
            candidate = os.path.join(folder, file_name)
            if os.path.isdir(candidate):
                paths = sorted(glob.glob(os.path.join(candidate, '*.csv')))
            else:
                paths = [candidate]

        paths = [p for p in paths if os.path.isfile(p) and p.lower().endswith('.csv')]
        if not paths:
            raise FileNotFoundError(""No CSV files found with the given parameters."")

        self.files = paths
        self.data = {}
        frames = []

        for p in paths:
            df = self._load_csv(
                file_name=os.path.basename(p),
                folder_name=os.path.dirname(p),
                head_row=head_row,
                index_col=index_col,
                convert_col=convert_col,
                concat_files=concat_files
            )
            self.data[p] = df
            frames.append(df)

        if concat_files or len(frames) == 1:
            self.concatenated = pd.concat(frames, axis=0, ignore_index=True) if len(frames) > 1 else frames[0].copy()
            return self.concatenated

        return self.data

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        ''' Load single csv file.
        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '*' - all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.' - current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe
        Returns
        -------
        pd.DataFrame()
            Dataframe containing csv data
        '''
        path = os.path.join(folder_name, file_name)
        if not os.path.isfile(path):
            raise FileNotFoundError(f""CSV file not found: {path}"")

        df = pd.read_csv(path, skiprows=head_row if head_row and head_row > 0 else None)

        if index_col and index_col > 0:
            if df.shape[1] <= index_col:
                df = df.iloc[:, 0:0]
            else:
                df = df.iloc[:, index_col:]

        if convert_col and not df.empty:
            for c in df.columns:
                if pd.api.types.is_object_dtype(df[c]):
                    converted = pd.to_numeric(df[c], errors='ignore')
                    df[c] = converted

        return df"
323208,SoftwareDefinedBuildings/XBOS,apps/data_analysis/XBOS_data_analytics/Plot_Data.py,XBOS_data_analytics.Plot_Data.Plot_Data,"import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

class Plot_Data:
    """""" This class contains functions for displaying various plots.

    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.

    """"""
    count = 1

    def __init__(self, figsize=(18, 5)):
        """""" Constructor.

        Parameters
        ----------
        figsize : tuple
            Size of figure.

        """"""
        self.figsize = figsize

    def correlation_plot(self, data):
        """""" Create heatmap of Pearson's correlation coefficient.

        Parameters
        ----------
        data    : pd.DataFrame()
            Data to display.

        Returns
        -------
        matplotlib.figure
            Heatmap.

        """"""
        fig = plt.figure(Plot_Data.count)
        corr = data.corr()
        ax = sns.heatmap(corr)
        Plot_Data.count += 1
        return fig

    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        """""" Create baseline and projection plots.

        Parameters
        ----------
        y_true              : pd.Series()
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.Dataframe()
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.

        Returns
        -------
        matplotlib.figure
            Baseline plot

        """"""
        fig = plt.figure(Plot_Data.count)
        if projection_period:
            nrows = len(baseline_period) + len(projection_period) / 2
        else:
            nrows = len(baseline_period) / 2
        base_df = pd.DataFrame()
        base_df['y_true'] = y_true
        base_df['y_pred'] = y_pred
        ax1 = fig.add_subplot(nrows, 1, 1)
        base_df.plot(ax=ax1, figsize=self.figsize, title='Baseline Period ({}-{}). \nBest Model: {}. \nBaseline Adj R2: {}. \nSite: {}.'.format(baseline_period[0], baseline_period[1], model_name, adj_r2, site))
        if projection_period:
            num_plot = 2
            for i in range(0, len(projection_period), 2):
                ax = fig.add_subplot(nrows, 1, num_plot)
                period = slice(projection_period[i], projection_period[i + 1])
                project_df = pd.DataFrame()
                try:
                    project_df['y_true'] = data.loc[period, output_col]
                    project_df['y_pred'] = model.predict(data.loc[period, input_col])
                    project_df['y_pred'][project_df['y_pred'] < 0] = 0
                    project_df.plot(ax=ax, figsize=self.figsize, title='Projection Period ({}-{})'.format(projection_period[i], projection_period[i + 1]))
                    num_plot += 1
                    fig.tight_layout()
                    Plot_Data.count += 1
                    return (fig, project_df['y_true'], project_df['y_pred'])
                except:
                    raise SystemError('If projecting into the future, please specify project_ind_col that has data available                                         in the future time period requested.')
        return (fig, None, None)","
class Plot_Data:
    ''' This class contains functions for displaying various plots.
    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.
    '''
    def __init__(self, figsize=(18, 5)):
        
        pass
    def correlation_plot(self, data):
        
        pass
    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        ''' Create baseline and projection plots.
        Parameters
        ----------
        y_true              : pd.Series()
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.Dataframe()
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.
        Returns
        -------
        matplotlib.figure
            Baseline plot
        '''
        pass",snippet_264,4,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.dates import DateFormatter


class Plot_Data:
    ''' This class contains functions for displaying various plots.
    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.
    '''
    def __init__(self, figsize=(18, 5)):
        self.figsize = figsize
        self.count = 0

    def _get_datetime_index(self, data: pd.DataFrame):
        if isinstance(data.index, pd.DatetimeIndex):
            return data.index, None
        # Try common datetime column names
        for col in [""date"", ""Date"", ""datetime"", ""Datetime"", ""time"", ""Time"", ""timestamp"", ""Timestamp""]:
            if col in data.columns:
                idx = pd.to_datetime(data[col], errors=""coerce"")
                return idx, col
        # Fallback: create a range index as time axis
        return pd.RangeIndex(start=0, stop=len(data)), None

    def _to_periods(self, period):
        # Normalize period(s) to list of (start, end) tuples as Timestamps or None
        if period is None:
            return []
        if isinstance(period, (list, tuple)) and len(period) > 0 and not isinstance(period[0], (list, tuple)):
            # Single period provided as [start, end]
            period = [period]
        norm = []
        for p in period:
            if p is None or len(p) == 0:
                continue
            if len(p) == 1:
                start = pd.to_datetime(p[0], errors=""coerce"")
                end = None
            else:
                start = pd.to_datetime(p[0], errors=""coerce"")
                end = pd.to_datetime(p[1], errors=""coerce"")
            norm.append((start, end))
        return norm

    def correlation_plot(self, data):
        # Compute correlation matrix on numeric columns only
        if not isinstance(data, pd.DataFrame):
            data = pd.DataFrame(data)
        num_df = data.select_dtypes(include=[np.number])
        if num_df.shape[1] == 0:
            raise ValueError(""No numeric columns available to compute correlation."")
        corr = num_df.corr()

        fig, ax = plt.subplots(figsize=self.figsize)
        cax = ax.imshow(corr.values, cmap=""coolwarm"", vmin=-1, vmax=1, aspect=""auto"")
        ax.set_xticks(np.arange(corr.shape[1]))
        ax.set_yticks(np.arange(corr.shape[1]))
        ax.set_xticklabels(corr.columns, rotation=45, ha=""right"")
        ax.set_yticklabels(corr.columns)
        for (i, j), val in np.ndenumerate(corr.values):
            ax.text(j, i, f""{val:.2f}"", ha=""center"", va=""center"", color=""black"", fontsize=8)
        ax.set_title(""Correlation Matrix"")
        fig.colorbar(cax, ax=ax, fraction=0.046, pad=0.04, label=""Correlation"")
        fig.tight_layout()
        self.count += 1
        return fig

    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        ''' Create baseline and projection plots.
        Parameters
        ----------
        y_true              : pd.Series()
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.Dataframe()
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.
        Returns
        -------
        matplotlib.figure
            Baseline plot
        '''
        if not isinstance(data, pd.DataFrame):
            raise ValueError(""data must be a pandas DataFrame."")
        if isinstance(input_col, str):
            input_col = [input_col]

        time_index, time_col = self._get_datetime_index(data)
        # If we found a time column, align data index temporarily
        df = data.copy()
        df_index = time_index
        # Normalize baseline and projection periods
        baseline_periods = self._to_periods(baseline_period)
        proj_periods = self._to_periods(projection_period)

        # Determine baseline mask
        if len(baseline_periods) == 0:
            base_mask = pd.Series(True, index=np.arange(len(df)))
        else:
            base_mask = pd.Series(False, index=np.arange(len(df)))
            for start, end in baseline_periods:
                if start is None and end is None:
                    base_mask |= True
                elif start is None:
                    base_mask |= (df_index <= end)
                elif end is None:
                    base_mask |= (df_index >= start)
                else:
                    base_mask |= (df_index >= start) & (df_index <= end)

        df_base = df.loc[base_mask.values].copy()
        t_base = df_index[base_mask.values]

        # Prepare baseline y_true and y_pred
        y_true_base = None
        y_pred_base = None

        if y_true is not None:
            if isinstance(y_true, (pd.Series, pd.DataFrame)):
                y_true_arr = np.asarray(y_true).ravel()
            else:
                y_true_arr = np.asarray(y_true).ravel()
        else:
            y_true_arr = None

        if y_pred is not None:
            y_pred_arr = np.asarray(y_pred).ravel()
        else:
            y_pred_arr = None

        # Use provided y_true/y_pred if lengths match baseline selection
        if y_true_arr is not None and len(y_true_arr) == len(df_base):
            y_true_base = y_true_arr
        else:
            if output_col in df_base.columns:
                y_true_base = df_base[output_col].to_numpy()

        if y_pred_arr is not None and len(y_pred_arr) == len(df_base):
            y_pred_base = y_pred_arr
        else:
            # Compute predictions from model if possible
            if model is not None and len(input_col) > 0 and all(c in df_base.columns for c in input_col):
                try:
                    Xb = df_base[input_col].to_numpy()
                    y_pred_base = np.asarray(model.predict(Xb)).ravel()
                except Exception:
                    y_pred_base = None

        # Projection predictions
        proj_results = []  # list of dicts: {'t': time, 'y_true': ..., 'y_pred': ...}
        for (start, end) in proj_periods:
            if start is None and end is None:
                p_mask = pd.Series(True, index=np.arange(len(df)))
            elif start is None:
                p_mask = (df_index <= end)
            elif end is None:
                p_mask = (df_index >= start)
            else:
                p_mask = (df_index >= start) & (df_index <= end)

            df_p = df.loc[p_mask.values].copy()
            t_p = df_index[p_mask.values]
            y_true_p = df_p[output_col].to_numpy() if output_col in df_p.columns else None
            y_pred_p = None
            if model is not None and len(df_p) > 0 and len(input_col) > 0 and all(c in df_p.columns for c in input_col):
                try:
                    Xp = df_p[input_col].to_numpy()
                    y_pred_p = np.asarray(model.predict(Xp)).ravel()
                except Exception:
                    y_pred_p = None
            proj_results.append({""t"": t_p, ""y_true"": y_true_p, ""y_pred"": y_pred_p, ""period"": (start, end)})

        # Create figure with two subplots: baseline and projection
        fig, axes = plt.subplots(1, 2, figsize=self.figsize, sharey=False)
        ax_base, ax_proj = axes

        # Baseline plot
        if y_true_base is not None and len(t_base) == len(y_true_base):
            ax_base.plot(t_base, y_true_base, label=""Actual"", color=""#1f77b4"", linewidth=2)
        if y_pred_base is not None and len(t_base) == len(y_pred_base):
            ax_base.plot(t_base, y_pred_base, label=""Predicted"", color=""#ff7f0e"", linewidth=2, alpha=0.9)
        ax_base.set_title(""Baseline"")
        ax_base.set_xlabel(""Time"")
        ax_base.set_ylabel(output_col if output_col is not None else ""Target"")
        ax_base.legend()
        if isinstance(t_base, pd.DatetimeIndex):
            ax_base.xaxis.set_major_formatter(DateFormatter(""%Y-%m-%d""))
            fig.autofmt_xdate(rotation=30)

        # Projection plot
        colors = [""#2ca02c"", ""#d62728"", ""#9467bd"", ""#8c564b"", ""#e377c2"", ""#7f7f7f"", ""#bcbd22"", ""#17becf""]
        for i, res in enumerate(proj_results):
            t_p = res[""t""]
            y_true_p = res[""y_true""]
            y_pred_p = res[""y_pred""]
            label_suffix = """"
            s, e = res[""period""]
            if pd.notna(s) or pd.notna(e):
                s_str = s.strftime(""%Y-%m-%d"") if isinstance(s, pd.Timestamp) and not pd.isna(s) else """"
                e_str = e.strftime(""%Y-%m-%d"") if isinstance(e, pd.Timestamp) and not pd.isna(e) else """"
                if s_str or e_str:
                    label_suffix = f"" [{s_str} to {e_str}]""
            if y_true_p is not None and len(t_p) == len(y_true_p):
                ax_proj.plot(t_p, y_true_p, color=""#555555"", linestyle=""--"", linewidth=1.5, label=f""Actual{label_suffix}"" if i == 0 else None)
            if y_pred_p is not None and len(t_p) == len(y_pred_p):
                ax_proj.plot(t_p, y_pred_p, color=colors[i % len(colors)], linewidth=2, label=f""Predicted{label_suffix}"")
        ax_proj.set_title(""Projection"")
        ax_proj.set_xlabel(""Time"")
        ax_proj.set_ylabel(output_col if output_col is not None else ""Target"")
        ax_proj.legend()
        if len(proj_results) > 0 and isinstance(proj_results[0][""t""], pd.DatetimeIndex):
            ax_proj.xaxis.set_major_formatter(DateFormatter(""%Y-%m-%d""))
            fig.autofmt_xdate(rotation=30)

        # Overall title
        title_parts = []
        if site:
            title_parts.append(f""Site: {site}"")
        if model_name:
            title_parts.append(f""Model: {model_name}"")
        if adj_r2 is not None:
            try:
                title_parts.append(f""Adj R2: {float(adj_r2):.3f}"")
            except Exception:
                title_parts.append(f""Adj R2: {adj_r2}"")
        fig.suptitle("" | "".join(title_parts), fontsize=14, y=1.02)
        fig.tight_layout()
        self.count += 1
        return fig"
339242,pawelad/pymonzo,pawelad_pymonzo/src/pymonzo/client.py,pymonzo.client.MonzoAPI,"from pymonzo.exceptions import MonzoAPIError, NoSettingsFile
from authlib.integrations.base_client import OAuthError
from pymonzo.webhooks import WebhooksResource
from pathlib import Path
from pymonzo.attachments import AttachmentsResource
from json import JSONDecodeError
from pymonzo.transactions import TransactionsResource
from pymonzo.pots import PotsResource
from pymonzo.whoami import WhoAmIResource
from pymonzo.utils import get_authorization_response_url
from urllib.parse import urlparse
from typing import Any, Optional
from pymonzo.balance import BalanceResource
import webbrowser
from pymonzo.feed import FeedResource
from pymonzo.accounts import AccountsResource
from authlib.integrations.httpx_client import OAuth2Client
from pymonzo.settings import PyMonzoSettings

class MonzoAPI:
    """"""Monzo public API client.

    To use it, you need to create a new OAuth client in [Monzo Developer Portal].
    The `Redirect URLs` should be set to `http://localhost:6600/pymonzo` and
    `Confidentiality` should be set to `Confidential` if you'd like to automatically
    refresh the access token when it expires.

    You can now use `Client ID` and `Client secret` in [`pymonzo.MonzoAPI.authorize`][]
    to finish the OAuth 2 'Authorization Code Flow' and get the API access token
    (which is by default saved to disk and refreshed when expired).

    [Monzo Developer Portal]: https://developers.monzo.com/

    Note:
        Monzo API docs: https://docs.monzo.com/
    """"""
    api_url = 'https://api.monzo.com'
    authorization_endpoint = 'https://auth.monzo.com/'
    token_endpoint = 'https://api.monzo.com/oauth2/token'
    settings_path = Path.home() / '.pymonzo'

    def __init__(self, access_token: Optional[str]=None) -> None:
        """"""Initialize Monzo API client and mount all resources.

        It expects [`pymonzo.MonzoAPI.authorize`][] to be called beforehand, so
        it can load the local settings file containing the API access token. You
        can also explicitly pass the `access_token`, but it won't be able to
        automatically refresh it once it expires.

        Arguments:
            access_token: OAuth access token. You can obtain it (and by default, save
                it to disk, so it can refresh automatically) by running
                [`pymonzo.MonzoAPI.authorize`][]. Alternatively, you can get a
                temporary access token from the [Monzo Developer Portal].

                [Monzo Developer Portal]: https://developers.monzo.com/

        Raises:
            NoSettingsFile: When the access token wasn't passed explicitly and the
                settings file couldn't be loaded.

        """"""
        if access_token:
            self._settings = PyMonzoSettings(token={'access_token': access_token})
        else:
            try:
                self._settings = PyMonzoSettings.load_from_disk(self.settings_path)
            except (FileNotFoundError, JSONDecodeError) as e:
                raise NoSettingsFile('No settings file found. You need to either run `MonzoAPI.authorize(client_id, client_secret)` to get the authorization token (and save it to disk), or explicitly pass the `access_token`.') from e
        self.session = OAuth2Client(client_id=self._settings.client_id, client_secret=self._settings.client_secret, token=self._settings.token, authorization_endpoint=self.authorization_endpoint, token_endpoint=self.token_endpoint, token_endpoint_auth_method='client_secret_post', update_token=self._update_token, base_url=self.api_url)
        self.whoami = WhoAmIResource(client=self).whoami
        '\n        Mounted Monzo `whoami` endpoint. For more information see\n        [`pymonzo.whoami.WhoAmIResource.whoami`][].\n        '
        self.accounts = AccountsResource(client=self)
        '\n        Mounted Monzo `accounts` resource. For more information see\n        [`pymonzo.accounts.AccountsResource`][].\n        '
        self.attachments = AttachmentsResource(client=self)
        '\n        Mounted Monzo `attachments` resource. For more information see\n        [`pymonzo.attachments.AttachmentsResource`][].\n        '
        self.balance = BalanceResource(client=self)
        '\n        Mounted Monzo `balance` resource. For more information see\n        [`pymonzo.balance.BalanceResource`][].\n        '
        self.feed = FeedResource(client=self)
        '\n        Mounted Monzo `feed` resource. For more information see\n        [`pymonzo.feed.FeedResource`][].\n        '
        self.pots = PotsResource(client=self)
        '\n        Mounted Monzo `pots` resource. For more information see\n        [`pymonzo.pots.PotsResource`][].\n        '
        self.transactions = TransactionsResource(client=self)
        '\n        Mounted Monzo `transactions` resource. For more information see\n        [`pymonzo.transactions.TransactionsResource`][].\n        '
        self.webhooks = WebhooksResource(client=self)
        '\n        Mounted Monzo `webhooks` resource. For more information see\n        [`pymonzo.webhooks.WebhooksResource`][].\n        '

    @classmethod
    def authorize(cls, client_id: str, client_secret: str, *, save_to_disk: bool=True, redirect_uri: str='http://localhost:6600/pymonzo') -> dict:
        """"""Use OAuth 2 'Authorization Code Flow' to get Monzo API access token.

        By default, it also saves the token to disk, so it can be loaded during
        [`pymonzo.MonzoAPI`][] initialization.

        Note:
            Monzo API docs: https://docs.monzo.com/#authentication

        Arguments:
            client_id: OAuth client ID.
            client_secret: OAuth client secret.
            save_to_disk: Whether to save the token to disk.
            redirect_uri: Redirect URI specified in OAuth client.

        Returns:
            OAuth token.
        """"""
        client = OAuth2Client(client_id=client_id, client_secret=client_secret, redirect_uri=redirect_uri, token_endpoint_auth_method='client_secret_post')
        url, state = client.create_authorization_url(cls.authorization_endpoint)
        print(f'Please visit this URL to authorize: {url}')
        webbrowser.open(url)
        parsed_url = urlparse(redirect_uri)
        assert parsed_url.hostname is not None
        assert parsed_url.port is not None
        authorization_response = get_authorization_response_url(host=parsed_url.hostname, port=parsed_url.port)
        try:
            token = client.fetch_token(url=cls.token_endpoint, authorization_response=authorization_response)
        except (OAuthError, JSONDecodeError) as e:
            raise MonzoAPIError('Error while fetching API access token') from e
        if save_to_disk:
            settings = PyMonzoSettings(client_id=client_id, client_secret=client_secret, token=token)
            settings.save_to_disk(cls.settings_path)
        return token

    def _update_token(self, token: dict, **kwargs: Any) -> None:
        """"""Update settings with refreshed access token and save it to disk.

        Arguments:
            token: OAuth access token.
            **kwargs: Extra kwargs.
        """"""
        self._settings.token = token
        if self.settings_path.exists():
            self._settings.save_to_disk(self.settings_path)","
class MonzoAPI:
    '''Monzo public API client.
    To use it, you need to create a new OAuth client in [Monzo Developer Portal].
    The `Redirect URLs` should be set to `http://localhost:6600/pymonzo` and
    `Confidentiality` should be set to `Confidential` if you'd like to automatically
    refresh the access token when it expires.
    You can now use `Client ID` and `Client secret` in [`pymonzo.MonzoAPI.authorize`][]
    to finish the OAuth 2 'Authorization Code Flow' and get the API access token
    (which is by default saved to disk and refreshed when expired).
    [Monzo Developer Portal]: https://developers.monzo.com/
    Note:
        Monzo API docs: https://docs.monzo.com/
    '''
    def __init__(self, access_token: Optional[str]=None) -> None:
        '''Initialize Monzo API client and mount all resources.
        It expects [`pymonzo.MonzoAPI.authorize`][] to be called beforehand, so
        it can load the local settings file containing the API access token. You
        can also explicitly pass the `access_token`, but it won't be able to
        automatically refresh it once it expires.
        Arguments:
            access_token: OAuth access token. You can obtain it (and by default, save
                it to disk, so it can refresh automatically) by running
                [`pymonzo.MonzoAPI.authorize`][]. Alternatively, you can get a
                temporary access token from the [Monzo Developer Portal].
                [Monzo Developer Portal]: https://developers.monzo.com/
        Raises:
            NoSettingsFile: When the access token wasn't passed explicitly and the
                settings file couldn't be loaded.
        '''
        pass
    @classmethod
    def authorize(cls, client_id: str, client_secret: str, *, save_to_disk: bool=True, redirect_uri: str='http://localhost:6600/pymonzo') -> dict:
        '''Use OAuth 2 'Authorization Code Flow' to get Monzo API access token.
        By default, it also saves the token to disk, so it can be loaded during
        [`pymonzo.MonzoAPI`][] initialization.
        Note:
            Monzo API docs: https://docs.monzo.com/#authentication
        Arguments:
            client_id: OAuth client ID.
            client_secret: OAuth client secret.
            save_to_disk: Whether to save the token to disk.
            redirect_uri: Redirect URI specified in OAuth client.
        Returns:
            OAuth token.
        '''
        pass
    def _update_token(self, token: dict, **kwargs: Any) -> None:
        '''Update settings with refreshed access token and save it to disk.
        Arguments:
            token: OAuth access token.
            **kwargs: Extra kwargs.
        '''
        pass",snippet_268,4,"import json
import os
import threading
import time
import secrets
import webbrowser
from http.server import HTTPServer, BaseHTTPRequestHandler
from typing import Optional, Any
from urllib.parse import urlparse, parse_qs, urlencode

import requests


class NoSettingsFile(FileNotFoundError):
    pass


class MonzoAPI:
    '''Monzo public API client.
    To use it, you need to create a new OAuth client in [Monzo Developer Portal].
    The `Redirect URLs` should be set to `http://localhost:6600/pymonzo` and
    `Confidentiality` should be set to `Confidential` if you'd like to automatically
    refresh the access token when it expires.
    You can now use `Client ID` and `Client secret` in [`pymonzo.MonzoAPI.authorize`][]
    to finish the OAuth 2 'Authorization Code Flow' and get the API access token
    (which is by default saved to disk and refreshed when expired).
    [Monzo Developer Portal]: https://developers.monzo.com/
    Note:
        Monzo API docs: https://docs.monzo.com/
    '''

    SETTINGS_PATH = os.path.join(os.path.expanduser(""~""), "".pymonzo.json"")
    AUTH_URL = ""https://auth.monzo.com/""
    TOKEN_URL = ""https://api.monzo.com/oauth2/token""

    def __init__(self, access_token: Optional[str] = None) -> None:
        '''Initialize Monzo API client and mount all resources.
        It expects [`pymonzo.MonzoAPI.authorize`][] to be called beforehand, so
        it can load the local settings file containing the API access token. You
        can also explicitly pass the `access_token`, but it won't be able to
        automatically refresh it once it expires.
        Arguments:
            access_token: OAuth access token. You can obtain it (and by default, save
                it to disk, so it can refresh automatically) by running
                [`pymonzo.MonzoAPI.authorize`][]. Alternatively, you can get a
                temporary access token from the [Monzo Developer Portal].
                [Monzo Developer Portal]: https://developers.monzo.com/
        Raises:
            NoSettingsFile: When the access token wasn't passed explicitly and the
                settings file couldn't be loaded.
        '''
        self.session = requests.Session()
        self.settings = {}

        if access_token:
            self.token = {""access_token"": access_token}
            self.session.headers.update({""Authorization"": f""Bearer {access_token}""})
            return

        if not os.path.exists(self.SETTINGS_PATH):
            raise NoSettingsFile(f""Settings file not found at {self.SETTINGS_PATH}"")

        try:
            with open(self.SETTINGS_PATH, ""r"", encoding=""utf-8"") as f:
                self.settings = json.load(f)
        except Exception as e:
            raise NoSettingsFile(f""Unable to load settings file: {e}"") from e

        token = self.settings.get(""token"", {})
        if not token or ""access_token"" not in token:
            raise NoSettingsFile(""Access token not found in settings file"")

        self.token = token
        self.session.headers.update({""Authorization"": f""Bearer {token['access_token']}""})

    @classmethod
    def authorize(
        cls,
        client_id: str,
        client_secret: str,
        *,
        save_to_disk: bool = True,
        redirect_uri: str = ""http://localhost:6600/pymonzo"",
    ) -> dict:
        '''Use OAuth 2 'Authorization Code Flow' to get Monzo API access token.
        By default, it also saves the token to disk, so it can be loaded during
        [`pymonzo.MonzoAPI`][] initialization.
        Note:
            Monzo API docs: https://docs.monzo.com/#authentication
        Arguments:
            client_id: OAuth client ID.
            client_secret: OAuth client secret.
            save_to_disk: Whether to save the token to disk.
            redirect_uri: Redirect URI specified in OAuth client.
        Returns:
            OAuth token.
        '''

        parsed = urlparse(redirect_uri)
        if parsed.hostname is None or parsed.port is None:
            raise ValueError(""redirect_uri must include explicit hostname and port, e.g. http://localhost:6600/pymonzo"")

        state = secrets.token_urlsafe(24)
        query = {
            ""client_id"": client_id,
            ""redirect_uri"": redirect_uri,
            ""response_type"": ""code"",
            ""state"": state,
        }
        auth_url = f""{cls.AUTH_URL}?{urlencode(query)}""

        code_holder = {""code"": None, ""state"": None, ""error"": None}

        class Handler(BaseHTTPRequestHandler):
            def do_GET(self):
                try:
                    q = parse_qs(urlparse(self.path).query)
                    code = q.get(""code"", [None])[0]
                    recv_state = q.get(""state"", [None])[0]
                    error = q.get(""error"", [None])[0]
                    code_holder[""code""] = code
                    code_holder[""state""] = recv_state
                    code_holder[""error""] = error
                    self.send_response(200)
                    self.send_header(""Content-Type"", ""text/html; charset=utf-8"")
                    self.end_headers()
                    if error:
                        self.wfile.write(b""<html><body><h3>Authorization failed.</h3>You can close this window.</body></html>"")
                    else:
                        self.wfile.write(b""<html><body><h3>Authorization successful.</h3>You can close this window.</body></html>"")
                except Exception:
                    self.send_response(500)
                    self.end_headers()

            def log_message(self, fmt, *args):
                return

        server = HTTPServer((parsed.hostname, parsed.port), Handler)
        thread = threading.Thread(target=server.serve_forever, daemon=True)
        thread.start()

        try:
            webbrowser.open(auth_url)
        except Exception:
            pass

        timeout = time.time() + 300
        while time.time() < timeout and code_holder[""code""] is None and code_holder[""error""] is None:
            time.sleep(0.1)

        server.shutdown()
        thread.join(timeout=2)

        if code_holder[""error""]:
            raise RuntimeError(f""Authorization failed: {code_holder['error']}"")
        if code_holder[""code""] is None:
            raise TimeoutError(""Timed out waiting for authorization code"")

        if code_holder[""state""] != state:
            raise RuntimeError(""State mismatch during OAuth flow"")

        data = {
            ""grant_type"": ""authorization_code"",
            ""client_id"": client_id,
            ""client_secret"": client_secret,
            ""redirect_uri"": redirect_uri,
            ""code"": code_holder[""code""],
        }
        resp = requests.post(cls.TOKEN_URL, data=data, timeout=30)
        resp.raise_for_status()
        token = resp.json()

        if save_to_disk:
            settings = {
                ""client_id"": client_id,
                ""client_secret"": client_secret,
                ""redirect_uri"": redirect_uri,
                ""token"": token,
                ""saved_at"": int(time.time()),
            }
            with open(cls.SETTINGS_PATH, ""w"", encoding=""utf-8"") as f:
                json.dump(settings, f, indent=2, sort_keys=True)

        return token

    def _update_token(self, token: dict, **kwargs: Any) -> None:
        '''Update settings with refreshed access token and save it to disk.
        Arguments:
            token: OAuth access token.
            **kwargs: Extra kwargs.
        '''
        self.token = token
        self.session.headers.update({""Authorization"": f""Bearer {token.get('access_token', '')}""})

        if not os.path.exists(self.SETTINGS_PATH):
            return

        try:
            with open(self.SETTINGS_PATH, ""r"", encoding=""utf-8"") as f:
                settings = json.load(f)
        except Exception:
            settings = {}

        settings[""token""] = token
        settings[""updated_at""] = int(time.time())

        if kwargs:
            settings.setdefault(""meta"", {}).update(kwargs)

        with open(self.SETTINGS_PATH, ""w"", encoding=""utf-8"") as f:
            json.dump(settings, f, indent=2, sort_keys=True)"
339313,chaoss/grimoirelab-kingarthur,arthur/tasks.py,arthur.tasks._TaskConfig,"import re
from grimoirelab_toolkit.introspect import find_class_properties

class _TaskConfig:
    """"""Abstract class to store task configuration options.

    This class defines how to store specific task configuration
    arguments such as scheduling or archiving options. It is not
    meant to be instantiated on its own.

    Configuration options must be defined using `property` and `setter`
    decorators. Setters must check whether the given value is valid
    or not. When it is invalid, a `ValueError` exception should be
    raised. The rationale behind this is to use these methods as
    parsers when `from_dict` class method is called. It will create
    a new instance of the subclass passing its properties from a
    dictionary.
    """"""
    KW_ARGS_ERROR_REGEX = re.compile(""^.+ got an unexpected keyword argument '(.+)'$"")

    def to_dict(self):
        """"""Returns a dict with the representation of this task configuration object.""""""
        properties = find_class_properties(self.__class__)
        config = {name: self.__getattribute__(name) for name, _ in properties}
        return config

    @classmethod
    def from_dict(cls, config):
        """"""Create an configuration object from a dictionary.

        Key,value pairs will be used to initialize a task configuration
        object. If 'config' contains invalid configuration parameters
        a `ValueError` exception will be raised.

        :param config: dictionary used to create an instance of this object

        :returns: a task config instance

        :raises ValueError: when an invalid configuration parameter is found
        """"""
        try:
            obj = cls(**config)
        except TypeError as e:
            m = cls.KW_ARGS_ERROR_REGEX.match(str(e))
            if m:
                raise ValueError(""unknown '%s' task config parameter"" % m.group(1))
            else:
                raise e
        else:
            return obj","
class _TaskConfig:
    '''Abstract class to store task configuration options.
    This class defines how to store specific task configuration
    arguments such as scheduling or archiving options. It is not
    meant to be instantiated on its own.
    Configuration options must be defined using `property` and `setter`
    decorators. Setters must check whether the given value is valid
    or not. When it is invalid, a `ValueError` exception should be
    raised. The rationale behind this is to use these methods as
    parsers when `from_dict` class method is called. It will create
    a new instance of the subclass passing its properties from a
    dictionary.
    '''
    def to_dict(self):
        '''Returns a dict with the representation of this task configuration object.'''
        pass
    @classmethod
    def from_dict(cls, config):
        '''Create an configuration object from a dictionary.
        Key,value pairs will be used to initialize a task configuration
        object. If 'config' contains invalid configuration parameters
        a `ValueError` exception will be raised.
        :param config: dictionary used to create an instance of this object
        :returns: a task config instance
        :raises ValueError: when an invalid configuration parameter is found
        '''
        pass",snippet_269,3,"class _TaskConfig:
    '''Abstract class to store task configuration options.
    This class defines how to store specific task configuration
    arguments such as scheduling or archiving options. It is not
    meant to be instantiated on its own.
    Configuration options must be defined using `property` and `setter`
    decorators. Setters must check whether the given value is valid
    or not. When it is invalid, a `ValueError` exception should be
    raised. The rationale behind this is to use these methods as
    parsers when `from_dict` class method is called. It will create
    a new instance of the subclass passing its properties from a
    dictionary.
    '''
    def to_dict(self):
        '''Returns a dict with the representation of this task configuration object.'''
        props = {
            name: prop
            for name, prop in self._iter_properties()
            if prop.fget is not None
        }
        result = {}
        for name in props:
            result[name] = getattr(self, name)
        return result

    @classmethod
    def from_dict(cls, config):
        '''Create an configuration object from a dictionary.
        Key,value pairs will be used to initialize a task configuration
        object. If 'config' contains invalid configuration parameters
        a `ValueError` exception will be raised.
        :param config: dictionary used to create an instance of this object
        :returns: a task config instance
        :raises ValueError: when an invalid configuration parameter is found
        '''
        if not isinstance(config, dict):
            raise ValueError(""config must be a dict"")

        prop_map = {name: prop for name, prop in cls._iter_properties()}
        valid_settable = {name for name, p in prop_map.items() if p.fset is not None}

        unknown = set(config.keys()) - set(prop_map.keys())
        if unknown:
            raise ValueError(f""Unknown configuration parameter(s): {', '.join(sorted(unknown))}"")

        unsettable = {k for k in config.keys() if k not in valid_settable}
        if unsettable:
            raise ValueError(f""Read-only or unsettable configuration parameter(s): {', '.join(sorted(unsettable))}"")

        try:
            obj = cls()
        except TypeError as e:
            raise ValueError(f""Could not instantiate {cls.__name__} without arguments"") from e

        for key, value in config.items():
            try:
                setattr(obj, key, value)
            except ValueError as ve:
                raise ValueError(f""Invalid value for parameter '{key}': {ve}"") from ve
            except Exception as e:
                raise ValueError(f""Failed setting parameter '{key}': {e}"") from e

        return obj

    @classmethod
    def _iter_properties(cls):
        # Collect properties defined on the class and its bases, excluding those from object
        props = {}
        for c in cls.mro():
            if c is object:
                continue
            for name, attr in c.__dict__.items():
                if isinstance(attr, property):
                    props[name] = attr
        # Preserve definition order where possible
        for name, prop in props.items():
            yield name, prop"
341459,wright-group/WrightTools,wright-group_WrightTools/WrightTools/diagrams/WMEL.py,WrightTools.diagrams.WMEL.Subplot,"import numpy as np

class Subplot:
    """"""Subplot containing WMEL.""""""

    def __init__(self, ax, energies, number_of_interactions=4, title='', title_font_size=16, state_names=None, virtual=[None], state_font_size=14, state_text_buffer=0.5, label_side='left'):
        """"""Subplot.

        Parameters
        ----------
        ax : matplotlib axis
            The axis.
        energies : 1D array-like
            Energies (scaled between 0 and 1)
        number_of_interactions : integer
            Number of interactions in diagram.
        title : string (optional)
            Title of subplot. Default is empty string.
        state_names: list of str (optional)
            list of the names of the states
        virtual: list of ints (optional)
            list of indexes of any vitual energy states
        state_font_size: numtype (optional)
            font size for the state lables
        state_text_buffer: numtype (optional)
            space between the energy level bars and the state labels
        """"""
        self.ax = ax
        self.energies = energies
        self.interactions = number_of_interactions
        self.state_names = state_names
        for i in range(len(self.energies)):
            if i in virtual:
                linestyle = '--'
            else:
                linestyle = '-'
            self.ax.axhline(self.energies[i], color='k', linewidth=2, ls=linestyle, zorder=5)
        if isinstance(state_names, list):
            for i in range(len(self.energies)):
                if label_side == 'left':
                    ax.text(-state_text_buffer, energies[i], state_names[i], fontsize=state_font_size, verticalalignment='center', horizontalalignment='center')
                elif label_side == 'right':
                    ax.text(1 + state_text_buffer, energies[i], state_names[i], fontsize=state_font_size, verticalalignment='center', horizontalalignment='center')
        self.x_pos = np.linspace(0, 1, number_of_interactions)
        self.ax.set_xlim(-0.1, 1.1)
        self.ax.set_ylim(-0.01, 1.01)
        self.ax.axis('off')
        self.ax.set_title(title, fontsize=title_font_size)

    def add_arrow(self, index, between, kind, label='', head_length=10, head_aspect=1, font_size=14, color='k'):
        """"""Add an arrow to the WMEL diagram.

        Parameters
        ----------
        index : integer
            The interaction, or start and stop interaction for the arrow.
        between : 2-element iterable of integers
            The inital and final state of the arrow
        kind : {'ket', 'bra', 'outbra', 'outket'}
            The kind of interaction.
        label : string (optional)
            Interaction label. Default is empty string.
        head_length: number (optional)
            size of arrow head
        font_size : number (optional)
            Label font size. Default is 14.
        color : matplotlib color (optional)
            Arrow color. Default is black.

        Returns
        -------
        [line,arrow_head,text]
        """"""
        if hasattr(index, 'index'):
            x_pos = list(index)
        else:
            x_pos = [index] * 2
        x_pos = [np.linspace(0, 1, self.interactions)[i] for i in x_pos]
        y_pos = [self.energies[between[0]], self.energies[between[1]]]
        arrow_length = self.energies[between[1]] - self.energies[between[0]]
        arrow_end = self.energies[between[1]]
        if arrow_length > 0:
            direction = 1
        elif arrow_length < 0:
            direction = -1
        else:
            raise ValueError('between invalid!')
        length = abs(y_pos[0] - y_pos[1])
        if kind == 'ket':
            line = self.ax.plot(x_pos, y_pos, linestyle='-', color=color, linewidth=2, zorder=9)
        elif kind == 'bra':
            line = self.ax.plot(x_pos, y_pos, linestyle='--', color=color, linewidth=2, zorder=9)
        elif kind == 'out':
            yi = np.linspace(y_pos[0], y_pos[1], 100)
            xi = np.sin((yi - y_pos[0]) * int(1 / length * 20) * 2 * np.pi * length) / 40 + x_pos[0]
            line = self.ax.plot(xi[:-5], yi[:-5], linestyle='-', color=color, linewidth=2, solid_capstyle='butt', zorder=9)
        elif kind == 'outbra':
            yi = np.linspace(y_pos[0], y_pos[1], 100)
            xi = np.sin((yi - y_pos[0]) * int(1 / length * 20) * 2 * np.pi * length) / 40 + x_pos[0]
            counter = 0
            while counter - 13 <= len(yi):
                subyi = yi[counter:counter + 15]
                subxi = xi[counter:counter + 15]
                line = self.ax.plot(subxi[:-5], subyi[:-5], linestyle='-', color=color, linewidth=2, solid_capstyle='butt', zorder=9)
                counter += 13
        else:
            raise ValueError(""kind is not 'ket', 'bra', 'out' or 'outbra'."")
        dx = x_pos[1] - x_pos[0]
        dy = y_pos[1] - y_pos[0]
        xytext = (x_pos[1] - dx * 0.01, y_pos[1] - dy * 0.01)
        annotation = self.ax.annotate('', xy=(x_pos[1], y_pos[1]), xytext=xytext, arrowprops=dict(fc=color, ec=color, shrink=0, headwidth=head_length * head_aspect, headlength=head_length, linewidth=0, zorder=10), size=25)
        text = self.ax.text(np.mean(x_pos), -0.15, label, fontsize=font_size, horizontalalignment='center')
        return (line, annotation.arrow_patch, text)","
class Subplot:
    
    def __init__(self, ax, energies, number_of_interactions=4, title='', title_font_size=16, state_names=None, virtual=[None], state_font_size=14, state_text_buffer=0.5, label_side='left'):
        '''Subplot.
        Parameters
        ----------
        ax : matplotlib axis
            The axis.
        energies : 1D array-like
            Energies (scaled between 0 and 1)
        number_of_interactions : integer
            Number of interactions in diagram.
        title : string (optional)
            Title of subplot. Default is empty string.
        state_names: list of str (optional)
            list of the names of the states
        virtual: list of ints (optional)
            list of indexes of any vitual energy states
        state_font_size: numtype (optional)
            font size for the state lables
        state_text_buffer: numtype (optional)
            space between the energy level bars and the state labels
        '''
        pass
    def add_arrow(self, index, between, kind, label='', head_length=10, head_aspect=1, font_size=14, color='k'):
        '''Add an arrow to the WMEL diagram.
        Parameters
        ----------
        index : integer
            The interaction, or start and stop interaction for the arrow.
        between : 2-element iterable of integers
            The inital and final state of the arrow
        kind : {'ket', 'bra', 'outbra', 'outket'}
            The kind of interaction.
        label : string (optional)
            Interaction label. Default is empty string.
        head_length: number (optional)
            size of arrow head
        font_size : number (optional)
            Label font size. Default is 14.
        color : matplotlib color (optional)
            Arrow color. Default is black.
        Returns
        -------
        [line,arrow_head,text]
        '''
        pass",snippet_271,3,"class Subplot:
    
    def __init__(self, ax, energies, number_of_interactions=4, title='', title_font_size=16, state_names=None, virtual=[None], state_font_size=14, state_text_buffer=0.5, label_side='left'):
        '''Subplot.
        Parameters
        ----------
        ax : matplotlib axis
            The axis.
        energies : 1D array-like
            Energies (scaled between 0 and 1)
        number_of_interactions : integer
            Number of interactions in diagram.
        title : string (optional)
            Title of subplot. Default is empty string.
        state_names: list of str (optional)
            list of the names of the states
        virtual: list of ints (optional)
            list of indexes of any vitual energy states
        state_font_size: numtype (optional)
            font size for the state lables
        state_text_buffer: numtype (optional)
            space between the energy level bars and the state labels
        '''
        import numpy as np
        import matplotlib.pyplot as plt

        self.ax = ax
        self.energies = np.asarray(energies, dtype=float)
        self.n_states = len(self.energies)
        self.n_interactions = int(number_of_interactions)
        self.title = title
        self.title_font_size = title_font_size
        self.state_names = state_names if state_names is not None else [str(i) for i in range(self.n_states)]
        self.virtual = set([] if virtual is None else ([] if virtual == [None] else list(virtual)))
        self.state_font_size = state_font_size
        self.state_text_buffer = float(state_text_buffer)
        self.label_side = label_side if label_side in ('left', 'right') else 'left'

        # X span of the energy level bars
        self.x_min = -0.5
        self.x_max = self.n_interactions + 0.5

        # Draw energy level lines
        self.level_lines = []
        for i, y in enumerate(self.energies):
            is_virtual = i in self.virtual
            ls = '--' if is_virtual else '-'
            lw = 1.5 if not is_virtual else 1.0
            c = 'k' if not is_virtual else '0.5'
            line, = ax.plot([self.x_min, self.x_max], [y, y], linestyle=ls, color=c, lw=lw, zorder=1)
            self.level_lines.append(line)

        # Add state labels
        if self.label_side == 'left':
            x_label = self.x_min - self.state_text_buffer
            ha = 'right'
        else:
            x_label = self.x_max + self.state_text_buffer
            ha = 'left'

        self.state_texts = []
        for i, y in enumerate(self.energies):
            txt = ax.text(x_label, y, self.state_names[i], ha=ha, va='center',
                          fontsize=self.state_font_size, color=('0.35' if i in self.virtual else 'k'))
            self.state_texts.append(txt)

        # Styling axes
        ax.set_ylim(min(0.0, float(np.min(self.energies)) - 0.05), max(1.0, float(np.max(self.energies)) + 0.05))
        ax.set_xlim(self.x_min - self.state_text_buffer - 0.2, self.x_max + self.state_text_buffer + 0.2)
        for spine in ax.spines.values():
            spine.set_visible(False)
        ax.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)

        # Title
        if self.title:
            ax.set_title(self.title, fontsize=self.title_font_size)

        # Storage for arrows
        self.arrows = []

    def _kind_offset(self, kind):
        # Horizontal offset to separate bra/ket arrows visually on the same interaction line
        if kind == 'ket':
            return +0.18
        if kind == 'bra':
            return -0.18
        if kind == 'outket':
            return +0.0
        if kind == 'outbra':
            return +0.0
        return 0.0

    def _x_from_index_and_kind(self, idx, kind):
        # Resolve x positions for a given index spec and kind
        if isinstance(idx, (list, tuple)) and len(idx) == 2:
            x0, x1 = float(idx[0]), float(idx[1])
            off0 = self._kind_offset(kind)
            off1 = self._kind_offset(kind)
            return x0 + off0, x1 + off1
        else:
            x = float(idx)
            off = self._kind_offset(kind)
            return x + off, x + off

    def add_arrow(self, index, between, kind, label='', head_length=10, head_aspect=1, font_size=14, color='k'):
        '''Add an arrow to the WMEL diagram.
        Parameters
        ----------
        index : integer
            The interaction, or start and stop interaction for the arrow.
        between : 2-element iterable of integers
            The inital and final state of the arrow
        kind : {'ket', 'bra', 'outbra', 'outket'}
            The kind of interaction.
        label : string (optional)
            Interaction label. Default is empty string.
        head_length: number (optional)
            size of arrow head
        font_size : number (optional)
            Label font size. Default is 14.
        color : matplotlib color (optional)
            Arrow color. Default is black.
        Returns
        -------
        [line,arrow_head,text]
        '''
        import numpy as np
        from matplotlib import patches

        # Validate inputs
        if kind not in ('ket', 'bra', 'outbra', 'outket'):
            raise ValueError(""kind must be one of {'ket','bra','outbra','outket'}"")
        if not (isinstance(between, (list, tuple)) and len(between) == 2):
            raise ValueError(""between must be a 2-element iterable of integers (initial_state, final_state)"")

        i0, i1 = int(between[0]), int(between[1])
        if not (0 <= i0 < self.n_states and 0 <= i1 < self.n_states):
            raise IndexError(""state indices in 'between' are out of range"")

        # Determine x positions
        # For outbra/outket, if index is not provided as a 2-tuple, map to outside positions
        if kind in ('outket', 'outbra') and not (isinstance(index, (list, tuple)) and len(index) == 2):
            if kind == 'outket':
                x0, x1 = self.n_interactions, self.n_interactions + 0.8
            else:
                x0, x1 = 0.0, -0.8
        else:
            x0, x1 = self._x_from_index_and_kind(index, kind)

        y0 = float(self.energies[i0])
        y1 = float(self.energies[i1])

        # Draw shaft as a simple line
        line = self.ax.plot([x0, x1], [y0, y1], color=color, lw=2.0, zorder=3)[0]

        # Arrow head
        # Compute arrow head orientation
        dx = x1 - x0
        dy = y1 - y0
        length = np.hypot(dx, dy) if (dx != 0 or dy != 0) else 1.0
        if length == 0:
            ux, uy = 1.0, 0.0
        else:
            ux, uy = dx / length, dy / length

        # head size scales in data coordinates
        # Use head_length in points converted approximately to data units using axis transforms
        # Fallback: scale head length relative to x-span
        xspan = max(1e-6, self.ax.get_xlim()[1] - self.ax.get_xlim()[0])
        yspan = max(1e-6, self.ax.get_ylim()[1] - self.ax.get_ylim()[0])
        # Convert head_length (points) to data approx: assume 72 dpi and figure size; fallback to small fraction
        # Use a fraction of spans for robustness
        head_len_data = 0.03 * np.hypot(xspan, yspan) * (head_length / 10.0)
        head_width_data = head_len_data * (0.6 / max(1e-6, head_aspect))

        # Triangle points for the head at end (x1, y1)
        left_x = x1 - ux * head_len_data - uy * head_width_data
        left_y = y1 - uy * head_len_data + ux * head_width_data
        right_x = x1 - ux * head_len_data + uy * head_width_data
        right_y = y1 - uy * head_len_data - ux * head_width_data

        arrow_head = patches.Polygon([[x1, y1], [left_x, left_y], [right_x, right_y]],
                                     closed=True, facecolor=color, edgecolor=color, zorder=4)
        self.ax.add_patch(arrow_head)

        # Label
        text_obj = None
        if label:
            mx = (x0 + x1) / 2.0
            my = (y0 + y1) / 2.0
            # Offset label slightly perpendicular to the arrow direction
            nx, ny = -uy, ux
            label_offset = 0.02 * np.hypot(xspan, yspan)
            tx = mx + nx * label_offset
            ty = my + ny * label_offset
            ha = 'left' if dx >= 0 else 'right'
            text_obj = self.ax.text(tx, ty, label, fontsize=font_size, color=color,
                                    ha=ha, va='center', zorder=5)

        self.arrows.append((line, arrow_head, text_obj))
        return [line, arrow_head, text_obj]"
341765,peopledoc/django-agnocomplete,peopledoc_django-agnocomplete/agnocomplete/views.py,agnocomplete.views.UserContextFormViewMixin,"class UserContextFormViewMixin:
    """"""
    This mixin is injecting the context variable into the form kwargs
    """"""

    def get_agnocomplete_context(self):
        """"""
        Return the view current user.

        You may want to change this value by overrding this method.
        """"""
        return self.request.user

    def get_form_kwargs(self):
        """"""
        Return the form kwargs.

        This method injects the context variable, defined in
        :meth:`get_agnocomplete_context`. Override this method to adjust it to
        your needs.
        """"""
        data = super().get_form_kwargs()
        data.update({'user': self.get_agnocomplete_context()})
        return data","class UserContextFormViewMixin:
    
    def get_agnocomplete_context(self):
        
        pass
    def get_form_kwargs(self):
        '''
        Return the form kwargs.
        This method injects the context variable, defined in
        :meth:`get_agnocomplete_context`. Override this method to adjust it to
        your needs.
        '''
        pass",snippet_272,3,"class UserContextFormViewMixin:
    def get_agnocomplete_context(self):
        user = getattr(getattr(self, ""request"", None), ""user"", None)
        return {""user"": user} if user is not None else {}

    def get_form_kwargs(self):
        '''
        Return the form kwargs.
        This method injects the context variable, defined in
        :meth:`get_agnocomplete_context`. Override this method to adjust it to
        your needs.
        '''
        kwargs = {}
        try:
            kwargs = super().get_form_kwargs()
        except AttributeError:
            pass
        kwargs[""agnocomplete_context""] = self.get_agnocomplete_context()
        return kwargs"
343261,danpoland/pyramid-restful-framework,danpoland_pyramid-restful-framework/pyramid_restful/pagination/base.py,pyramid_restful.pagination.base.BasePagination,"class BasePagination:
    """"""
    The base class each Pagination class should implement.
    """"""

    def paginate_query(self, query, request):
        """"""
        :param query: SQLAlchemy ``query``.
        :param request: The request from the view
        :return: The paginated date based on the provided query and request.
        """"""
        raise NotImplementedError('paginate_query() must be implemented.')

    def get_paginated_response(self, data):
        """"""
        :param data: The paginated data.
        :return: A response containing the paginated data.
        """"""
        raise NotImplementedError('get_paginated_response() must be implemented.')","class BasePagination:
    '''
    The base class each Pagination class should implement.
        '''
    def paginate_query(self, query, request):
        '''
        :param query: SQLAlchemy ``query``.
        :param request: The request from the view
        :return: The paginated date based on the provided query and request.
        '''
        pass
    def get_paginated_response(self, data):
        
        pass",snippet_273,3,"class BasePagination:
    '''
    The base class each Pagination class should implement.
    '''

    default_page_size = 20
    max_page_size = 100
    param_page = 'page'
    param_page_size = 'page_size'
    param_limit = 'limit'
    param_offset = 'offset'

    def _get_params(self, request):
        params = {}
        if request is None:
            return params

        # Try typical structures: dict-like, .args (e.g., Flask), .GET (e.g., Django)
        candidates = []
        if isinstance(request, dict):
            candidates.append(request)
        else:
            for attr in ('args', 'GET', 'params', 'query_params'):
                if hasattr(request, attr):
                    v = getattr(request, attr)
                    if isinstance(v, dict):
                        candidates.append(v)
                        break

        # Merge (last writer wins)
        for c in candidates:
            for k, v in c.items():
                params[k] = v

        return params

    def _parse_int(self, value, default, minimum=None, maximum=None):
        try:
            iv = int(value)
        except Exception:
            iv = default
        if minimum is not None and iv < minimum:
            iv = minimum
        if maximum is not None and iv > maximum:
            iv = maximum
        return iv

    def paginate_query(self, query, request):
        '''
        :param query: SQLAlchemy ``query``.
        :param request: The request from the view
        :return: The paginated data dict with items and metadata.
        '''
        params = self._get_params(request)

        # Determine mode: limit/offset takes precedence if either provided
        has_limit = self.param_limit in params and str(params.get(self.param_limit)).strip() != ''
        has_offset = self.param_offset in params and str(params.get(self.param_offset)).strip() != ''

        # Compute total count
        try:
            total = query.order_by(None).count()
        except Exception:
            total = query.count()

        if has_limit or has_offset:
            limit = self._parse_int(
                params.get(self.param_limit, self.default_page_size),
                default=self.default_page_size,
                minimum=1,
                maximum=self.max_page_size,
            )
            offset = self._parse_int(
                params.get(self.param_offset, 0),
                default=0,
                minimum=0,
            )
            paginated_q = query.limit(limit).offset(offset)
            items = paginated_q.all()
            return {
                'count': total,
                'limit': limit,
                'offset': offset,
                'results': items,
            }
        else:
            page_size = self._parse_int(
                params.get(self.param_page_size, self.default_page_size),
                default=self.default_page_size,
                minimum=1,
                maximum=self.max_page_size,
            )
            # Avoid division by zero
            page_size = max(1, page_size)

            # Total pages
            pages = (total + page_size - 1) // page_size if total is not None else 1
            pages = max(1, pages)

            page = self._parse_int(
                params.get(self.param_page, 1),
                default=1,
                minimum=1,
            )
            if page > pages:
                page = pages

            offset = (page - 1) * page_size
            paginated_q = query.limit(page_size).offset(offset)
            items = paginated_q.all()
            return {
                'count': total,
                'page': page,
                'page_size': page_size,
                'pages': pages,
                'results': items,
            }

    def get_paginated_response(self, data):
        return data"
343270,danpoland/pyramid-restful-framework,danpoland_pyramid-restful-framework/pyramid_restful/permissions.py,pyramid_restful.permissions.BasePermission,"class BasePermission:
    """"""
    All permission classes should inherit from this class.
    """"""
    message = None

    def has_permission(self, request, view):
        """"""
        Checked on every request to a view. Return ``True`` if permission is granted else ``False``.

        :param request: The request sent to the view.
        :param view: The instance of the view being accessed.
        :return: Boolean
        """"""
        return True

    def has_object_permission(self, request, view, obj):
        """"""
        Checked when a request is for a specific object. Return ``True`` if permission is granted else ``False``.

        :param request: The request sent to the view.
        :param view:  The instance of the view being accessed.
        :param obj: The object being accessed.
        :return: Boolean
        """"""
        return True","class BasePermission:
    '''
    All permission classes should inherit from this class.
        '''
    def has_permission(self, request, view):
        '''
        Checked on every request to a view. Return ``True`` if permission is granted else ``False``.
        :param request: The request sent to the view.
        :param view: The instance of the view being accessed.
        :return: Boolean
        '''
        pass
    def has_object_permission(self, request, view, obj):
        
        pass",snippet_274,3,"class BasePermission:
    '''
    All permission classes should inherit from this class.
        '''
    def has_permission(self, request, view):
        '''
        Checked on every request to a view. Return ``True`` if permission is granted else ``False``.
        :param request: The request sent to the view.
        :param view: The instance of the view being accessed.
        :return: Boolean
        '''
        return True

    def has_object_permission(self, request, view, obj):
        return True"
344249,openid/JWTConnect-Python-CryptoJWT,src/cryptojwt/jws/__init__.py,cryptojwt.jws.Signer,"class Signer:
    """"""Abstract base class for signing algorithms.""""""

    def sign(self, msg, key):
        """"""Sign ``msg`` with ``key`` and return the signature.""""""
        raise NotImplementedError()

    def verify(self, msg, sig, key):
        """"""Return True if ``sig`` is a valid signature for ``msg``.""""""
        raise NotImplementedError()","class Signer:
    '''Abstract base class for signing algorithms.'''
    def sign(self, msg, key):
        '''Sign ``msg`` with ``key`` and return the signature.'''
        pass
    def verify(self, msg, sig, key):
        
        pass",snippet_275,3,"from abc import ABC, abstractmethod

class Signer(ABC):
    '''Abstract base class for signing algorithms.'''
    @abstractmethod
    def sign(self, msg, key):
        '''Sign ``msg`` with ``key`` and return the signature.'''
        raise NotImplementedError

    @abstractmethod
    def verify(self, msg, sig, key):
        '''Verify that ``sig`` is a valid signature of ``msg`` under ``key``.'''
        raise NotImplementedError"
347062,adobe-apiplatform/umapi-client.py,umapi_client/api.py,umapi_client.api.QuerySingle,"class QuerySingle:
    """"""
    Look for a single object
    """"""

    def __init__(self, connection, object_type, url_params=None, query_params=None):
        """"""
        Provide the connection and query parameters when you create the query.

        :param connection: The Connection to run the query against
        :param object_type: The type of object being queried (e.g., ""user"" or ""group"")
        :param url_params: Query qualifiers that go in the URL path (e.g., a group name when querying users)
        :param query_params: Query qualifiers that go in the query string (e.g., a domain name)
        """"""
        self.conn = connection
        self.object_type = object_type
        self.url_params = url_params if url_params else []
        self.query_params = query_params if query_params else {}
        self._result = None

    def reload(self):
        """"""
        Rerun the query (lazily).
        The result will contain a value on the server side that have changed since the last run.
        :return: None
        """"""
        self._result = None

    def _fetch_result(self):
        """"""
        Fetch the queried object.
        """"""
        self._result = self.conn.query_single(self.object_type, self.url_params, self.query_params)

    def result(self):
        """"""
        Fetch the result, if we haven't already or if reload has been called.
        :return: the result object of the query.
        """"""
        if self._result is None:
            self._fetch_result()
        return self._result","class QuerySingle:
    
    def __init__(self, connection, object_type, url_params=None, query_params=None):
        
        pass
    def reload(self):
        
        pass
    def _fetch_result(self):
        '''
        Fetch the queried object.
        '''
        pass
    def result(self):
        
        pass",snippet_276,5,"class QuerySingle:
    def __init__(self, connection, object_type, url_params=None, query_params=None):
        self.connection = connection
        self.object_type = object_type
        self.url_params = url_params or {}
        self.query_params = query_params or {}
        self._result = None

    def reload(self):
        self._result = None
        return self.result()

    def _fetch_result(self):
        '''
        Fetch the queried object.
        '''
        import inspect
        import json
        from urllib.parse import urlencode, urljoin

        # Determine endpoint/path from object_type
        endpoint = None
        if isinstance(self.object_type, str):
            endpoint = self.object_type
        else:
            endpoint = getattr(self.object_type, ""endpoint"", None) or getattr(self.object_type, ""path"", None)
        if not endpoint:
            raise ValueError(""Cannot determine endpoint/path from object_type. Provide a string endpoint or ensure object_type has 'endpoint' or 'path' attribute."")

        # Format URL with path parameters
        try:
            endpoint_formatted = endpoint.format(**self.url_params) if self.url_params else endpoint
        except KeyError as e:
            raise ValueError(f""Missing URL parameter for endpoint formatting: {e}"") from e

        # Build absolute URL if connection holds a base_url, otherwise use endpoint as-is
        base_url = getattr(self.connection, ""base_url"", None)
        if base_url:
            url = urljoin(base_url.rstrip(""/"") + ""/"", endpoint_formatted.lstrip(""/""))
        else:
            url = endpoint_formatted

        # Execute GET request
        resp = None
        if hasattr(self.connection, ""get"") and callable(self.connection.get):
            resp = self.connection.get(url, params=self.query_params or None)
        elif hasattr(self.connection, ""request"") and callable(self.connection.request):
            # Common interface like requests.Session.request
            resp = self.connection.request(""GET"", url, params=self.query_params or None)
        else:
            raise AttributeError(""Connection must provide a 'get' or 'request' method."")

        # Extract payload
        data = None
        if hasattr(resp, ""json"") and callable(resp.json):
            data = resp.json()
        elif isinstance(resp, (dict, list)):
            data = resp
        elif isinstance(resp, str):
            try:
                data = json.loads(resp)
            except json.JSONDecodeError:
                data = resp
        else:
            # Try attribute commonly used by HTTP client libraries
            content = getattr(resp, ""text"", None)
            if content is not None:
                try:
                    data = json.loads(content)
                except Exception:
                    data = content
            else:
                data = resp

        # Normalize to a single object
        if isinstance(data, list):
            data = data[0] if data else None

        # Construct target object
        def build_object(payload):
            if payload is None:
                return None

            # Prefer classmethod or staticmethod constructors
            if hasattr(self.object_type, ""from_dict"") and callable(getattr(self.object_type, ""from_dict"")):
                try:
                    return self.object_type.from_dict(payload)
                except Exception:
                    pass
            if hasattr(self.object_type, ""from_json"") and callable(getattr(self.object_type, ""from_json"")):
                try:
                    return self.object_type.from_json(payload)
                except Exception:
                    pass

            # If it's a class, try kwargs then single-arg
            if inspect.isclass(self.object_type):
                if isinstance(payload, dict):
                    try:
                        return self.object_type(**payload)
                    except Exception:
                        pass
                try:
                    return self.object_type(payload)
                except Exception:
                    return payload

            # If it's any callable (factory)
            if callable(self.object_type):
                try:
                    return self.object_type(payload)
                except Exception:
                    return payload

            # Fallback: return raw payload
            return payload

        return build_object(data)

    def result(self):
        if self._result is None:
            self._result = self._fetch_result()
        return self._result"
347189,facelessuser/backrefs,facelessuser_backrefs/backrefs/util.py,backrefs.util.Immutable,"from typing import Any, Callable, AnyStr

class Immutable:
    """"""Immutable.""""""
    __slots__: tuple[Any, ...] = ()

    def __init__(self, **kwargs: Any) -> None:
        """"""Initialize.""""""
        for k, v in kwargs.items():
            super().__setattr__(k, v)

    def __setattr__(self, name: str, value: Any) -> None:
        """"""Prevent mutability.""""""
        raise AttributeError('Class is immutable!')","
class Immutable:
    '''Immutable.'''
    def __init__(self, **kwargs: Any) -> None:
        
        pass
    def __setattr__(self, name: str, value: Any) -> None:
        
        pass",snippet_277,3,"from typing import Any

class Immutable:
    '''Immutable.'''
    def __init__(self, **kwargs: Any) -> None:
        object.__setattr__(self, '_Immutable__locked', False)
        for k, v in kwargs.items():
            object.__setattr__(self, k, v)
        object.__setattr__(self, '_Immutable__locked', True)

    def __setattr__(self, name: str, value: Any) -> None:
        if name == '_Immutable__locked':
            object.__setattr__(self, name, value)
            return
        if getattr(self, '_Immutable__locked', False):
            raise AttributeError(f""{self.__class__.__name__} is immutable; cannot set attribute '{name}'"")
        object.__setattr__(self, name, value)"
347503,inveniosoftware/invenio-records,inveniosoftware_invenio-records/invenio_records/dumpers/base.py,invenio_records.dumpers.base.Dumper,"from copy import deepcopy

class Dumper:
    """"""Interface for dumpers.""""""

    def dump(self, record, data):
        """"""Dump a record that can be used a source document for the search engine.

        The job of this method is to create a Python dictionary from the record
        provided in the argument.

        If you overwrite this method without calling super, then you should
        ensure that you make a deep copy of the record dictionary, to avoid
        that changes to the dump affects the record.

        :param record: The record to dump.
        :param data: The initial dump data passed in by ``record.dumps()``.
        """"""
        data.update(deepcopy(dict(record)))
        return data

    def load(self, data, record_cls):
        """"""Load a record from the source document of a search engine hit.

        The job of this method, is to create a record of type ``record_cls``
        based on the input ``data``.

        :param data: A Python dictionary representing the data to load.
        :param records_cls: The record class to be constructed.
        :returns: A instance of ``record_cls``.
        """"""
        raise NotImplementedError()","
class Dumper:
    '''Interface for dumpers.'''
    def dump(self, record, data):
        
        pass
    def load(self, data, record_cls):
        
        pass",snippet_278,3,"class Dumper:
    '''Interface for dumpers.'''
    def dump(self, record, data):
        raise NotImplementedError(""dump must be implemented by subclasses"")
    def load(self, data, record_cls):
        raise NotImplementedError(""load must be implemented by subclasses"")"
347507,inveniosoftware/invenio-records,inveniosoftware_invenio-records/invenio_records/dumpers/search.py,invenio_records.dumpers.search.SearchDumperExt,"class SearchDumperExt:
    """"""Interface for Search dumper extensions.""""""

    def dump(self, record, data):
        """"""Dump the data.""""""

    def load(self, data, record_cls):
        """"""Load the data.

        Reverse the changes made by the dump method.
        """"""","class SearchDumperExt:
    
    def dump(self, record, data):
        
        pass
    def load(self, data, record_cls):
        '''Load the data.
        Reverse the changes made by the dump method.
        '''
        pass",snippet_279,3,"import copy
import base64
import datetime
import decimal
import uuid
from collections.abc import Mapping, Sequence


class SearchDumperExt:
    _MARK = ""__sde__""

    def dump(self, record, data):
        def _dump_value(v):
            if isinstance(v, datetime.datetime):
                return {self._MARK: ""datetime"", ""v"": v.isoformat()}
            if isinstance(v, datetime.date) and not isinstance(v, datetime.datetime):
                return {self._MARK: ""date"", ""v"": v.isoformat()}
            if isinstance(v, datetime.time):
                return {self._MARK: ""time"", ""v"": v.isoformat()}
            if isinstance(v, decimal.Decimal):
                return {self._MARK: ""decimal"", ""v"": str(v)}
            if isinstance(v, uuid.UUID):
                return {self._MARK: ""uuid"", ""v"": str(v)}
            if isinstance(v, (bytes, bytearray)):
                return {self._MARK: ""bytes"", ""v"": base64.b64encode(bytes(v)).decode(""ascii"")}
            if isinstance(v, set):
                return {self._MARK: ""set"", ""v"": [_dump_value(i) for i in v]}
            if isinstance(v, tuple):
                return {self._MARK: ""tuple"", ""v"": [_dump_value(i) for i in v]}
            if isinstance(v, Mapping):
                # Avoid re-wrapping already dumped markers
                if set(v.keys()) == {self._MARK, ""v""} and isinstance(v.get(self._MARK), str):
                    return {k: _dump_value(val) for k, val in v.items()}
                return {str(k): _dump_value(val) for k, val in v.items()}
            if isinstance(v, Sequence) and not isinstance(v, (str, bytes, bytearray)):
                return [_dump_value(i) for i in v]
            return v

        return _dump_value(copy.deepcopy(data))

    def load(self, data, record_cls):
        def _load_value(v):
            if isinstance(v, Mapping) and self._MARK in v and ""v"" in v and isinstance(v[self._MARK], str):
                t = v[self._MARK]
                val = v[""v""]
                if t == ""datetime"":
                    return datetime.datetime.fromisoformat(val)
                if t == ""date"":
                    return datetime.date.fromisoformat(val)
                if t == ""time"":
                    return datetime.time.fromisoformat(val)
                if t == ""decimal"":
                    return decimal.Decimal(val)
                if t == ""uuid"":
                    return uuid.UUID(val)
                if t == ""bytes"":
                    return base64.b64decode(val.encode(""ascii""))
                if t == ""set"":
                    return set(_load_value(i) for i in val)
                if t == ""tuple"":
                    return tuple(_load_value(i) for i in val)
                # Unknown type marker: fall-through to generic handling
            if isinstance(v, Mapping):
                return {k: _load_value(val) for k, val in v.items()}
            if isinstance(v, Sequence) and not isinstance(v, (str, bytes, bytearray)):
                return [_load_value(i) for i in v]
            return v

        return _load_value(data)"
347521,inveniosoftware/invenio-records,inveniosoftware_invenio-records/invenio_records/systemfields/base.py,invenio_records.systemfields.base.SystemFieldContext,"class SystemFieldContext:
    """"""Base class for a system field context.

    A system field context is created once you access a field's attribute on
    a class. As the system field may be defined on a super class, this context
    allows us to know from which class the field was accessed.

    Normally you should subclass this class, and implement methods the methods
    on it that requires you to know the record class.
    """"""

    def __init__(self, field, record_cls):
        """"""Initialise the field context.""""""
        self._field = field
        self._record_cls = record_cls

    @property
    def field(self):
        """"""Access the field to prevent it from being overwritten.""""""
        return self._field

    @property
    def record_cls(self):
        """"""Record class to prevent it from being overwritten.""""""
        return self._record_cls","class SystemFieldContext:
    '''Base class for a system field context.
    A system field context is created once you access a field's attribute on
    a class. As the system field may be defined on a super class, this context
    allows us to know from which class the field was accessed.
    Normally you should subclass this class, and implement methods the methods
    on it that requires you to know the record class.
        '''
    def __init__(self, field, record_cls):
        
        pass
    @property
    def field(self):
        
        pass
    @property
    def record_cls(self):
        '''Record class to prevent it from being overwritten.'''
        pass",snippet_280,4,"class SystemFieldContext:
    '''Base class for a system field context.
    A system field context is created once you access a field's attribute on
    a class. As the system field may be defined on a super class, this context
    allows us to know from which class the field was accessed.
    Normally you should subclass this class, and implement methods the methods
    on it that requires you to know the record class.
        '''
    def __init__(self, field, record_cls):
        self._field = field
        self._record_cls = record_cls

    @property
    def field(self):
        return self._field

    @property
    def record_cls(self):
        '''Record class to prevent it from being overwritten.'''
        return self._record_cls"
348074,edx/edx-val,edx_edx-val/edxval/transcript_utils.py,edxval.transcript_utils.Transcript,"import json
from pysrt import SubRipFile, SubRipItem, SubRipTime
from edxval.exceptions import TranscriptsGenerationException
from pysrt.srtexc import Error

class Transcript:
    """"""
    Container for transcript methods.
    """"""
    SRT = 'srt'
    SJSON = 'sjson'

    @staticmethod
    def generate_sjson_from_srt(srt_subs):
        """"""
        Generate transcripts from sjson to SubRip (*.srt).

        Arguments:
            srt_subs(SubRip): ""SRT"" subs object

        Returns:
            Subs converted to ""SJSON"" format.
        """"""
        sub_starts = []
        sub_ends = []
        sub_texts = []
        for sub in srt_subs:
            sub_starts.append(sub.start.ordinal)
            sub_ends.append(sub.end.ordinal)
            sub_texts.append(sub.text.replace('\n', ' '))
        sjson_subs = {'start': sub_starts, 'end': sub_ends, 'text': sub_texts}
        return sjson_subs

    @staticmethod
    def generate_srt_from_sjson(sjson_subs):
        """"""
        Generate transcripts from sjson to SubRip (*.srt)

        Arguments:
            sjson_subs (dict): `sjson` subs.

        Returns:
            Subtitles in SRT format.
        """"""
        output = ''
        equal_len = len(sjson_subs['start']) == len(sjson_subs['end']) == len(sjson_subs['text'])
        if not equal_len:
            return output
        for i in range(len(sjson_subs['start'])):
            item = SubRipItem(index=i, start=SubRipTime(milliseconds=sjson_subs['start'][i]), end=SubRipTime(milliseconds=sjson_subs['end'][i]), text=sjson_subs['text'][i])
            output += str(item)
            output += '\n'
        return output

    @classmethod
    def convert(cls, content, input_format, output_format):
        """"""
        Convert transcript `content` from `input_format` to `output_format`.

        Arguments:
            content: Transcript content byte-stream.
            input_format: Input transcript format.
            output_format: Output transcript format.

        Accepted input formats: sjson, srt.
        Accepted output format: srt, sjson.

        Raises:
            TranscriptsGenerationException: On parsing the invalid srt
            content during conversion from srt to sjson.
        """"""
        assert input_format in ('srt', 'sjson')
        assert output_format in ('srt', 'sjson')
        try:
            content = content.decode('utf-8-sig')
        except UnicodeDecodeError:
            content = content.decode('latin-1')
        if input_format == output_format:
            return content
        if input_format == 'srt':
            if output_format == 'sjson':
                try:
                    srt_subs = SubRipFile.from_string(content, error_handling=SubRipFile.ERROR_RAISE)
                except Error as ex:
                    raise TranscriptsGenerationException(str(ex)) from ex
                return json.dumps(cls.generate_sjson_from_srt(srt_subs))
        if input_format == 'sjson':
            if output_format == 'srt':
                return cls.generate_srt_from_sjson(json.loads(content))","
class Transcript:
    '''
    Container for transcript methods.
    '''
    @staticmethod
    def generate_sjson_from_srt(srt_subs):
        '''
        Generate transcripts from sjson to SubRip (*.srt).
        Arguments:
            srt_subs(SubRip): ""SRT"" subs object
        Returns:
            Subs converted to ""SJSON"" format.
        '''
        pass
    @staticmethod
    def generate_srt_from_sjson(sjson_subs):
        '''
        Generate transcripts from sjson to SubRip (*.srt)
        Arguments:
            sjson_subs (dict): `sjson` subs.
        Returns:
            Subtitles in SRT format.
        '''
        pass
    @classmethod
    def convert(cls, content, input_format, output_format):
        
        pass",snippet_282,4,"class Transcript:
    '''
    Container for transcript methods.
    '''
    @staticmethod
    def _parse_timecode_to_ms(tc):
        if not isinstance(tc, str):
            raise ValueError(""Timecode must be a string"")
        t = tc.strip().replace(',', '.')
        parts = t.split(':')
        if len(parts) != 3:
            raise ValueError(f""Invalid timecode: {tc}"")
        h = int(parts[0])
        m = int(parts[1])
        s_parts = parts[2].split('.')
        s = int(s_parts[0])
        ms = int((s_parts[1] + '000')[:3]) if len(s_parts) > 1 else 0
        return ((h * 60 + m) * 60 + s) * 1000 + ms

    @staticmethod
    def _format_ms_to_timecode(ms):
        if ms < 0:
            ms = 0
        total_ms = int(round(ms))
        h = total_ms // 3600000
        rem = total_ms % 3600000
        m = rem // 60000
        rem = rem % 60000
        s = rem // 1000
        msec = rem % 1000
        return f""{h:02d}:{m:02d}:{s:02d},{msec:03d}""

    @staticmethod
    def _to_int_ms(value):
        # Accept ms (int), seconds (float), or strings convertible to either.
        if isinstance(value, (int,)):
            return int(value)
        if isinstance(value, float):
            return int(round(value * 1000))
        if isinstance(value, str):
            v = value.strip()
            # Try SRT-like timecode
            if ':' in v:
                return Transcript._parse_timecode_to_ms(v)
            # Try numeric
            try:
                if '.' in v:
                    return int(round(float(v) * 1000))
                return int(v)
            except ValueError:
                pass
        raise ValueError(f""Unsupported time value: {value}"")

    @staticmethod
    def _split_srt_blocks(s):
        import re
        # Normalize newlines and split on blank lines (handles one or more blank lines)
        s = s.replace('\r\n', '\n').replace('\r', '\n')
        # Remove BOM if present
        if s.startswith('\ufeff'):
            s = s.lstrip('\ufeff')
        blocks = re.split(r'\n{2,}', s.strip())
        return [b for b in blocks if b.strip()]

    @staticmethod
    def generate_sjson_from_srt(srt_subs):
        '''
        Generate transcripts from sjson to SubRip (*.srt).
        Arguments:
            srt_subs(SubRip): ""SRT"" subs object
        Returns:
            Subs converted to ""SJSON"" format.
        '''
        # Accept string content of SRT
        if not isinstance(srt_subs, str):
            raise TypeError(""srt_subs must be a string containing SRT data"")

        blocks = Transcript._split_srt_blocks(srt_subs)
        fragments = []
        idx_counter = 1
        for block in blocks:
            lines = [ln for ln in block.split('\n')]
            if not lines:
                continue
            # Detect if first line is index
            time_line_idx = 0
            index_val = None
            if lines and lines[0].strip().isdigit():
                index_val = int(lines[0].strip())
                time_line_idx = 1
            # Find time line containing -->
            if time_line_idx >= len(lines) or '-->' not in lines[time_line_idx]:
                # Try to locate a line with -->
                found = None
                for i, ln in enumerate(lines):
                    if '-->' in ln:
                        found = i
                        break
                if found is None:
                    # Skip malformed block
                    continue
                time_line_idx = found
            time_line = lines[time_line_idx]
            parts = time_line.split('-->')
            if len(parts) != 2:
                continue
            start_tc = parts[0].strip()
            end_tc = parts[1].strip()
            try:
                start_ms = Transcript._parse_timecode_to_ms(start_tc)
                end_ms = Transcript._parse_timecode_to_ms(end_tc)
            except Exception:
                continue

            text_lines = lines[time_line_idx + 1 :] if time_line_idx + 1 < len(lines) else []
            # Trim trailing empty lines
            while text_lines and text_lines[-1].strip() == '':
                text_lines.pop()
            text = '\n'.join(text_lines).strip()
            fragments.append({
                ""id"": index_val if index_val is not None else idx_counter,
                ""start"": start_ms,
                ""end"": end_ms,
                ""text"": text
            })
            idx_counter += 1

        return {
            ""type"": ""transcript"",
            ""format"": ""sjson"",
            ""fragments"": fragments
        }

    @staticmethod
    def generate_srt_from_sjson(sjson_subs):
        '''
        Generate transcripts from sjson to SubRip (*.srt)
        Arguments:
            sjson_subs (dict): `sjson` subs.
        Returns:
            Subtitles in SRT format.
        '''
        import json

        data = sjson_subs
        if isinstance(sjson_subs, str):
            data = json.loads(sjson_subs)
        if not isinstance(data, dict):
            raise TypeError(""sjson_subs must be a dict or a JSON string"")
        fragments = data.get(""fragments"") or data.get(""segments"") or data.get(""items"")
        if not isinstance(fragments, list):
            raise ValueError(""Invalid sjson structure: missing fragments list"")

        def frag_start_ms(f):
            if ""start"" in f:
                return Transcript._to_int_ms(f[""start""])
            if ""begin"" in f:
                return Transcript._to_int_ms(f[""begin""])
            if ""ts"" in f:
                return Transcript._to_int_ms(f[""ts""])
            return 0

        srt_lines = []
        # Sort by start time if not already
        fragments_sorted = sorted(fragments, key=frag_start_ms)

        for i, f in enumerate(fragments_sorted, start=1):
            # Determine start and end
            if ""start"" in f:
                start_ms = Transcript._to_int_ms(f[""start""])
            elif ""begin"" in f:
                start_ms = Transcript._to_int_ms(f[""begin""])
            elif ""ts"" in f:
                start_ms = Transcript._to_int_ms(f[""ts""])
            else:
                start_ms = 0

            if ""end"" in f:
                end_ms = Transcript._to_int_ms(f[""end""])
            elif ""duration"" in f:
                end_ms = start_ms + Transcript._to_int_ms(f[""duration""])
            elif ""dur"" in f:
                end_ms = start_ms + Transcript._to_int_ms(f[""dur""])
            else:
                # Fallback: add 2 seconds
                end_ms = start_ms + 2000

            # Determine text
            if ""text"" in f and isinstance(f[""text""], str):
                text = f[""text""]
            elif ""lines"" in f and isinstance(f[""lines""], list):
                text = ""\n"".join(str(x) for x in f[""lines""])
            elif ""content"" in f:
                text = str(f[""content""])
            else:
                text = """"

            start_tc = Transcript._format_ms_to_timecode(start_ms)
            end_tc = Transcript._format_ms_to_timecode(end_ms)
            srt_lines.append(str(i))
            srt_lines.append(f""{start_tc} --> {end_tc}"")
            # Ensure no CRLF inside text
            if text:
                text_norm = text.replace('\r\n', '\n').replace('\r', '\n')
                srt_lines.extend(text_norm.split('\n'))
            srt_lines.append("""")  # blank line after each block

        return ""\n"".join(srt_lines).rstrip() + ""\n""

    @classmethod
    def convert(cls, content, input_format, output_format):
        input_fmt = (input_format or """").strip().lower()
        output_fmt = (output_format or """").strip().lower()

        if input_fmt == output_fmt:
            return content

        if input_fmt == ""srt"" and output_fmt == ""sjson"":
            if not isinstance(content, str):
                raise TypeError(""For srt input, content must be a string"")
            return cls.generate_sjson_from_srt(content)

        if input_fmt == ""sjson"" and output_fmt == ""srt"":
            return cls.generate_srt_from_sjson(content)

        raise ValueError(f""Unsupported conversion: {input_format} -> {output_format}"")"
350782,druids/django-chamber,druids_django-chamber/chamber/commands/__init__.py,chamber.commands.ProgressBarStream,"class ProgressBarStream:
    """"""
    OutputStream wrapper to remove default linebreak at line endings.
    """"""

    def __init__(self, stream):
        """"""
        Wrap the given stream.
        """"""
        self.stream = stream

    def write(self, *args, **kwargs):
        """"""
        Call the stream's write method without linebreaks at line endings.
        """"""
        return self.stream.write(*args, ending='', **kwargs)

    def flush(self):
        """"""
        Call the stream's flush method without any extra arguments.
        """"""
        return self.stream.flush()","class ProgressBarStream:
    '''
    OutputStream wrapper to remove default linebreak at line endings.
        '''
    def __init__(self, stream):
        
        pass
    def write(self, *args, **kwargs):
        '''
        Call the stream's write method without linebreaks at line endings.
        '''
        pass
    def flush(self):
        '''
        Call the stream's flush method without any extra arguments.
        '''
        pass",snippet_283,4,"class ProgressBarStream:
    '''
    OutputStream wrapper to remove default linebreak at line endings.
    '''
    def __init__(self, stream):
        self.stream = stream

    def write(self, *args, **kwargs):
        '''
        Call the stream's write method without linebreaks at line endings.
        '''
        if not args:
            return 0
        s = ''.join(str(a) for a in args)
        if s.endswith('\r\n'):
            s = s[:-2]
        elif s.endswith('\n') or s.endswith('\r'):
            s = s[:-1]
        return self.stream.write(s)

    def flush(self):
        '''
        Call the stream's flush method without any extra arguments.
        '''
        return self.stream.flush()"
350920,pytroll/trollimage,pytroll_trollimage/trollimage/_xrimage_rasterio.py,trollimage._xrimage_rasterio.RIODataset,"from rasterio.enums import Resampling
from rasterio.windows import Window

class RIODataset:
    """"""A wrapper for a rasterio dataset.""""""

    def __init__(self, rfile, overviews=None, overviews_resampling=None, overviews_minsize=256):
        """"""Init the rasterio dataset.""""""
        self.rfile = rfile
        self.overviews = overviews
        if overviews_resampling is None:
            overviews_resampling = 'nearest'
        self.overviews_resampling = Resampling[overviews_resampling]
        self.overviews_minsize = overviews_minsize

    def __setitem__(self, key, item):
        """"""Put the data chunk in the image.""""""
        if len(key) == 3:
            indexes = list(range(key[0].start + 1, key[0].stop + 1, key[0].step or 1))
            y = key[1]
            x = key[2]
        else:
            indexes = 1
            y = key[0]
            x = key[1]
        chy_off = y.start
        chy = y.stop - y.start
        chx_off = x.start
        chx = x.stop - x.start
        self.rfile.write(item, window=Window(chx_off, chy_off, chx, chy), indexes=indexes)

    def close(self):
        """"""Close the file.""""""
        if self.overviews is not None:
            overviews = self.overviews
            if len(overviews) == 0:
                from rasterio.rio.overview import get_maximum_overview_level
                width = self.rfile.width
                height = self.rfile.height
                max_level = get_maximum_overview_level(width, height, self.overviews_minsize)
                overviews = [2 ** j for j in range(1, max_level + 1)]
            logger.debug('Building overviews %s with %s resampling', str(overviews), self.overviews_resampling.name)
            self.rfile.build_overviews(overviews, resampling=self.overviews_resampling)
        return self.rfile.close()","
class RIODataset:
    '''A wrapper for a rasterio dataset.'''
    def __init__(self, rfile, overviews=None, overviews_resampling=None, overviews_minsize=256):
        '''Init the rasterio dataset.'''
        pass
    def __setitem__(self, key, item):
        
        pass
    def close(self):
        '''Close the file.'''
        pass",snippet_284,4,"import rasterio
import numpy as np
from rasterio.enums import Resampling
from rasterio.windows import Window


class RIODataset:
    '''A wrapper for a rasterio dataset.'''

    def __init__(self, rfile, overviews=None, overviews_resampling=None, overviews_minsize=256):
        '''Init the rasterio dataset.'''
        self.path = rfile
        self._ds = None
        self._writable = False

        # Try to open writable, then fallback to read-only
        try:
            self._ds = rasterio.open(self.path, mode=""r+"")
            self._writable = True
        except Exception:
            self._ds = rasterio.open(self.path, mode=""r"")
            self._writable = False

        # Handle overviews if requested
        if overviews is not None:
            if not self._writable:
                # need writable to build overviews
                raise RuntimeError(""Overviews requested but dataset is not writable (opened read-only)."")

            if overviews_resampling is None:
                resampling = Resampling.nearest
            elif isinstance(overviews_resampling, Resampling):
                resampling = overviews_resampling
            else:
                # allow str like 'nearest', 'average', etc.
                resampling = Resampling[str(overviews_resampling).lower()]

            if overviews is True or str(overviews).lower() in (""auto"",):
                min_dim = min(self._ds.width, self._ds.height)
                factors = []
                f = 2
                while min_dim // f >= int(overviews_minsize):
                    factors.append(f)
                    f *= 2
            elif isinstance(overviews, (list, tuple)):
                factors = [int(x) for x in overviews if int(x) > 1]
            else:
                raise ValueError(""Invalid overviews parameter. Use True/'auto' or list of decimation factors."")

            if factors:
                self._ds.build_overviews(factors, resampling=resampling)
                # Optional: tag the dataset with overview information
                try:
                    self._ds.update_tags(ns=""rio_overviews"",
                                         factors="","".join(map(str, factors)),
                                         resampling=resampling.name)
                except Exception:
                    pass

    def __setitem__(self, key, item):
        # Writing requires writable dataset
        if not self._writable:
            raise RuntimeError(""Dataset is not writable. Opened in read-only mode."")

        ds = self._ds

        def _ensure_dtype(arr, indexes):
            if arr is None:
                return arr
            if not isinstance(arr, np.ndarray):
                arr = np.asarray(arr)

            # Normalize to (bands, ...) for dtype checking
            if arr.ndim == 2:
                bands = [indexes] if isinstance(indexes, int) else list(indexes)
                target_dtypes = [ds.dtypes[b - 1] for b in bands]
                if len(set(target_dtypes)) == 1:
                    td = target_dtypes[0]
                    if arr.dtype.name != td:
                        arr = arr.astype(td, copy=False)
                return arr
            elif arr.ndim == 3:
                if isinstance(indexes, int):
                    target_dtypes = [ds.dtypes[indexes - 1]]
                else:
                    bands = list(indexes)
                    target_dtypes = [ds.dtypes[b - 1] for b in bands]
                # If all bands share same dtype, cast once
                if len(set(target_dtypes)) == 1:
                    td = target_dtypes[0]
                    if arr.dtype.name != td:
                        arr = arr.astype(td, copy=False)
                return arr
            else:
                return arr

        if isinstance(key, int):
            idx = int(key)
            arr = np.asarray(item)
            arr = _ensure_dtype(arr, idx)
            ds.write(arr, indexes=idx)
            return

        if isinstance(key, Window):
            win = key
            arr = np.asarray(item)
            if arr.ndim == 2:
                idxs = 1
            elif arr.ndim == 3:
                idxs = list(range(1, arr.shape[0] + 1))
            else:
                raise ValueError(""Unsupported array shape for windowed write."")
            arr = _ensure_dtype(arr, idxs)
            ds.write(arr, indexes=idxs, window=win)
            return

        if isinstance(key, tuple):
            if len(key) != 2:
                raise ValueError(""Key tuple must be (indexes, window)."")
            idxs, win = key
            if isinstance(win, tuple) and len(win) == 2 and all(isinstance(t, slice) for t in win):
                # Support numpy-like slicing ((rowslice, colslice))
                row_slice, col_slice = win
                row_start = 0 if row_slice.start is None else row_slice.start
                col_start = 0 if col_slice.start is None else col_slice.start
                row_stop = ds.height if row_slice.stop is None else row_slice.stop
                col_stop = ds.width if col_slice.stop is None else col_slice.stop
                height = row_stop - row_start
                width = col_stop - col_start
                win = Window(col_start, row_start, width, height)
            elif not isinstance(win, Window):
                raise ValueError(""Window must be a rasterio.windows.Window or tuple of slices."")

            if isinstance(idxs, (list, tuple, np.ndarray)):
                idxs = [int(i) for i in idxs]
            elif isinstance(idxs, int):
                idxs = int(idxs)
            else:
                raise ValueError(""Indexes must be int or sequence of ints."")

            arr = np.asarray(item)
            arr = _ensure_dtype(arr, idxs)
            ds.write(arr, indexes=idxs, window=win)
            return

        raise TypeError(""Unsupported key type for __setitem__."")

    def close(self):
        '''Close the file.'''
        if self._ds is not None:
            try:
                self._ds.close()
            finally:
                self._ds = None
                self._writable = False"
350922,pytroll/trollimage,pytroll_trollimage/trollimage/_xrimage_rasterio.py,trollimage._xrimage_rasterio.RIOTag,"class RIOTag:
    """"""Rasterio wrapper to allow da.store on tag.""""""

    def __init__(self, rfile, name):
        """"""Init the rasterio tag.""""""
        self.rfile = rfile
        self.name = name

    def __setitem__(self, key, item):
        """"""Put the data in the tag.""""""
        kwargs = {self.name: item.item()}
        self.rfile.update_tags(**kwargs)

    def close(self):
        """"""Close the file.""""""
        return self.rfile.close()","class RIOTag:
    
    def __init__(self, rfile, name):
        '''Init the rasterio tag.'''
        pass
    def __setitem__(self, key, item):
        
        pass
    def close(self):
        '''Close the file.'''
        pass",snippet_285,4,"class RIOTag:
    
    def __init__(self, rfile, name):
        '''Init the rasterio tag.'''
        try:
            import rasterio
        except ImportError as e:
            raise ImportError(""rasterio is required to use RIOTag"") from e
        self._rio = rasterio
        self._path = rfile
        self._ns = name if name is not None else None
        self._ds = self._rio.open(self._path, mode='r+')

    def __setitem__(self, key, item):
        if self._ds is None:
            raise RuntimeError(""Dataset is closed."")
        if not isinstance(key, str):
            key = str(key)
        value = item if isinstance(item, str) else str(item)
        if self._ns:
            self._ds.update_tags(ns=self._ns, **{key: value})
        else:
            self._ds.update_tags(**{key: value})
        
    def close(self):
        '''Close the file.'''
        if self._ds is not None:
            try:
                self._ds.close()
            finally:
                self._ds = None"
353855,inveniosoftware/invenio-communities,inveniosoftware_invenio-communities/invenio_communities/communities/records/systemfields/access.py,invenio_communities.communities.records.systemfields.access.AccessEnumMixin,"class AccessEnumMixin:
    """"""Mixin for enum functionalities.""""""

    @classmethod
    def validate(cls, level):
        """"""Validate a string against the enum values.""""""
        return cls(level) in cls

    def __str__(self):
        """"""Return its value.""""""
        return self.value","class AccessEnumMixin:
    
    @classmethod
    def validate(cls, level):
        
        pass
    def __str__(self):
        '''Return its value.'''
        pass",snippet_286,3,"class AccessEnumMixin:
    @classmethod
    def validate(cls, level):
        if isinstance(level, cls):
            return level

        # Try by direct value match
        for member in cls:
            if level == member.value:
                return member

        # Try by name or value as string (case-insensitive)
        if isinstance(level, str):
            lvl = level.strip()
            # by name
            for member in cls:
                if member.name.lower() == lvl.lower():
                    return member
            # by value string
            for member in cls:
                if str(member.value).lower() == lvl.lower():
                    return member

        valid = "", "".join(
            [f""{m.name}({m.value})"" for m in cls]
        )
        raise ValueError(f""Invalid level: {level!r}. Expected one of: {valid}"")

    def __str__(self):
        return str(self.value)"
354005,inveniosoftware/invenio-communities,inveniosoftware_invenio-communities/invenio_communities/roles.py,invenio_communities.roles.Role,"from dataclasses import dataclass, field

@dataclass(frozen=True)
class Role:
    """"""Role class.""""""
    name: str = ''
    'Name of the role.'
    title: str = ''
    'Title of the role.'
    description: str = ''
    'Brief description of capabilities of the role.'
    can_manage_roles: list = field(default_factory=list)
    'List of other roles that this role can manage.'
    is_owner: bool = False
    'This role is the owner role (only one can exists).'
    can_manage: bool = False
    'This role has manage permissions.'
    can_curate: bool = False
    'This role has record manage permissions.'
    can_view: bool = False
    'This role has view restricted record permissions.'

    def can_manage_role(self, role_name):
        """"""Determine if this role can manage the role name.""""""
        return role_name in self.can_manage_roles

    def __hash__(self):
        """"""Compute a hash for use with e.g. sets.""""""
        return self.name.__hash__()","@dataclass(frozen=True)
class Role:
    
    def can_manage_role(self, role_name):
        '''Determine if this role can manage the role name.'''
        pass
    def __hash__(self):
        
        pass",snippet_287,3,"from dataclasses import dataclass, field
from typing import FrozenSet

@dataclass(frozen=True, slots=True)
class Role:
    name: str
    manages: FrozenSet[str] = field(default_factory=frozenset)
    manage_all: bool = False
    
    def can_manage_role(self, role_name):
        '''Determine if this role can manage the role name.'''
        if self.manage_all:
            return True
        if not isinstance(role_name, str):
            return False
        return role_name in self.manages

    def __hash__(self):
        return hash((self.name, self.manage_all, self.manages))"
356596,grycap/RADL,grycap_RADL/radl/radl.py,radl.radl.Aspect,"import copy

class Aspect:
    """"""A network, ansible_host, system, deploy, configure or contextualize element in a RADL.""""""

    def getId(self):
        """"""Return the id of the aspect.""""""
        return id(self)

    def clone(self):
        """"""Return a copy of this aspect.""""""
        return copy.deepcopy(self)","
class Aspect:
    '''A network, ansible_host, system, deploy, configure or contextualize element in a RADL.'''
    def getId(self):
        
        pass
    def clone(self):
        '''Return a copy of this aspect.'''
        pass",snippet_288,3,"import copy

class Aspect:
    '''A network, ansible_host, system, deploy, configure or contextualize element in a RADL.'''
    def getId(self):
        return getattr(self, 'id', None)
        
    def clone(self):
        '''Return a copy of this aspect.'''
        return copy.deepcopy(self)"
357236,pytroll/posttroll,posttroll/backends/zmq/message_broadcaster.py,posttroll.backends.zmq.message_broadcaster.ZMQDesignatedReceiversSender,"import threading
from posttroll.backends.zmq.socket import close_socket, set_up_client_socket
from zmq import LINGER, NOBLOCK, REQ, ZMQError

class ZMQDesignatedReceiversSender:
    """"""Sends message to multiple *receivers* on *port*.""""""

    def __init__(self, default_port, receivers):
        """"""Set up the sender.""""""
        self.default_port = default_port
        self.receivers = receivers
        self._shutdown_event = threading.Event()

    def __call__(self, data):
        """"""Send data.""""""
        for receiver in self.receivers:
            self._send_to_address(receiver, data)

    def _send_to_address(self, address, data, timeout=10):
        """"""Send data to *address* and *port* without verification of response.""""""
        if address.find(':') == -1:
            full_address = 'tcp://%s:%d' % (address, self.default_port)
        else:
            full_address = 'tcp://%s' % address
        options = {LINGER: int(timeout * 1000)}
        socket = set_up_client_socket(REQ, full_address, options)
        try:
            socket.send_string(data)
            while not self._shutdown_event.is_set():
                try:
                    message = socket.recv_string(NOBLOCK)
                except ZMQError:
                    self._shutdown_event.wait(0.1)
                    continue
                if message != 'ok':
                    logger.warning('invalid acknowledge received: %s' % message)
                break
        finally:
            close_socket(socket)

    def close(self):
        """"""Close the sender.""""""
        self._shutdown_event.set()","
class ZMQDesignatedReceiversSender:
    '''Sends message to multiple *receivers* on *port*.'''
    def __init__(self, default_port, receivers):
        '''Set up the sender.'''
        pass
    def __call__(self, data):
        '''Send data.'''
        pass
    def _send_to_address(self, address, data, timeout=10):
        
        pass
    def close(self):
        '''Close the sender.'''
        pass",snippet_289,5,"import zmq
import json
import pickle


class ZMQDesignatedReceiversSender:
    '''Sends message to multiple *receivers* on *port*.'''
    def __init__(self, default_port, receivers):
        '''Set up the sender.'''
        self._default_port = int(default_port)
        self._context = zmq.Context()
        self._sockets = {}
        self._addresses = []

        for receiver in receivers or []:
            addr = self._normalize_address(receiver)
            if addr in self._sockets:
                continue
            sock = self._context.socket(zmq.PUSH)
            # Avoid hanging forever on close/send
            sock.setsockopt(zmq.LINGER, 0)
            # Sensible high-water mark to buffer some messages but avoid memory blow-up
            sock.setsockopt(zmq.SNDHWM, 1000)
            sock.connect(addr)
            self._sockets[addr] = sock
            self._addresses.append(addr)

    def __call__(self, data):
        '''Send data.'''
        payload = self._serialize(data)
        results = {}
        for addr in self._addresses:
            results[addr] = self._send_to_address(addr, payload, timeout=10)
        return results

    def _send_to_address(self, address, data, timeout=10):
        sock = self._sockets.get(address)
        if sock is None:
            return False
        try:
            # Temporarily set timeout for this send
            prev_timeout = sock.getsockopt(zmq.SNDTIMEO)
        except Exception:
            prev_timeout = None
        try:
            sock.setsockopt(zmq.SNDTIMEO, int(timeout * 1000))
            sock.send(data)
            return True
        except zmq.Again:
            return False
        except Exception:
            return False
        finally:
            if prev_timeout is not None:
                try:
                    sock.setsockopt(zmq.SNDTIMEO, prev_timeout)
                except Exception:
                    pass

    def close(self):
        '''Close the sender.'''
        for sock in self._sockets.values():
            try:
                sock.close(0)
            except Exception:
                pass
        self._sockets.clear()
        try:
            self._context.term()
        except Exception:
            pass

    def _normalize_address(self, receiver):
        r = str(receiver).strip()
        if ""://"" in r:
            return r
        if "":"" in r:
            return f""tcp://{r}""
        return f""tcp://{r}:{self._default_port}""

    def _serialize(self, data):
        if isinstance(data, (bytes, bytearray, memoryview)):
            return bytes(data)
        if isinstance(data, str):
            return data.encode(""utf-8"")
        try:
            return json.dumps(data, separators=("","", "":""), ensure_ascii=False).encode(""utf-8"")
        except Exception:
            return pickle.dumps(data, protocol=pickle.HIGHEST_PROTOCOL)"
357237,pytroll/posttroll,posttroll/backends/zmq/ns.py,posttroll.backends.zmq.ns.ZMQNameServer,"from posttroll.backends.zmq.socket import SocketReceiver, close_socket, set_up_client_socket, set_up_server_socket
from contextlib import suppress
from zmq import LINGER, REP, REQ
from posttroll.ns import get_active_address, get_configured_nameserver_port

class ZMQNameServer:
    """"""The name server.""""""

    def __init__(self):
        """"""Set up the nameserver.""""""
        self.running: bool = True
        self.listener: SocketReceiver | None = None
        self._authenticator = None

    def run(self, address_receiver, address: str | None=None):
        """"""Run the listener and answer to requests.""""""
        port = get_configured_nameserver_port()
        try:
            if not self.running:
                return
            if address is None:
                address = '*'
            address = create_nameserver_address(address)
            self.listener, _, self._authenticator = set_up_server_socket(REP, address)
            logger.debug(f'Nameserver listening on port {port}')
            socket_receiver = SocketReceiver()
            socket_receiver.register(self.listener)
            while self.running:
                try:
                    for msg, _ in socket_receiver.receive(self.listener, timeout=1):
                        logger.debug('Replying to request: ' + str(msg))
                        active_address = get_active_address(msg.data['service'], address_receiver, msg.version)
                        self.listener.send_unicode(str(active_address))
                except TimeoutError:
                    continue
        except KeyboardInterrupt:
            pass
        finally:
            socket_receiver.unregister(self.listener)
            self.close_sockets_and_threads()

    def close_sockets_and_threads(self):
        """"""Close all sockets and threads.""""""
        with suppress(AttributeError):
            close_socket(self.listener)
        with suppress(AttributeError):
            self._authenticator.stop()

    def stop(self):
        """"""Stop the name server.""""""
        self.running = False","
class ZMQNameServer:
    '''The name server.'''
    def __init__(self):
        
        pass
    def run(self, address_receiver, address: str | None=None):
        '''Run the listener and answer to requests.'''
        pass
    def close_sockets_and_threads(self):
        
        pass
    def stop(self):
        
        pass",snippet_290,5,"import threading
import json
from typing import Callable, Optional, Any
import zmq


class ZMQNameServer:
    '''The name server.'''

    def __init__(self):
        self._ctx: Optional[zmq.Context] = None
        self._sock: Optional[zmq.Socket] = None
        self._stop_event = threading.Event()
        self._running = False
        self._registry: dict[str, str] = {}
        self._thread: Optional[threading.Thread] = None
        self._endpoint: Optional[str] = None

    def _notify_address(self, address_receiver: Any, endpoint: str):
        if address_receiver is None:
            return
        try:
            if callable(address_receiver):
                address_receiver(endpoint)
            elif hasattr(address_receiver, ""put"") and callable(address_receiver.put):
                address_receiver.put(endpoint)
            elif hasattr(address_receiver, ""send"") and callable(address_receiver.send):
                address_receiver.send(endpoint)
        except Exception:
            pass

    def _bind_socket(self, address: Optional[str]) -> str:
        self._ctx = zmq.Context.instance()
        self._sock = self._ctx.socket(zmq.REP)
        self._sock.linger = 0
        # Make sure we can periodically check for stop
        self._sock.rcvtimeo = 500  # ms
        if address:
            self._sock.bind(address)
            endpoint = self._sock.getsockopt_string(zmq.LAST_ENDPOINT)
        else:
            # Bind to random port on localhost
            port = self._sock.bind_to_random_port(""tcp://127.0.0.1"")
            endpoint = f""tcp://127.0.0.1:{port}""
        self._endpoint = endpoint
        return endpoint

    def _handle_message(self, msg: bytes) -> dict:
        try:
            data = json.loads(msg.decode(""utf-8""))
        except Exception:
            return {""ok"": False, ""error"": ""invalid_json""}

        cmd = data.get(""cmd"")
        if not isinstance(cmd, str):
            return {""ok"": False, ""error"": ""missing_or_invalid_cmd""}

        if cmd == ""ping"":
            return {""ok"": True, ""pong"": True}

        if cmd == ""register"":
            name = data.get(""name"")
            endpoint = data.get(""endpoint"")
            if not isinstance(name, str) or not isinstance(endpoint, str):
                return {""ok"": False, ""error"": ""invalid_name_or_endpoint""}
            self._registry[name] = endpoint
            return {""ok"": True}

        if cmd == ""lookup"":
            name = data.get(""name"")
            if not isinstance(name, str):
                return {""ok"": False, ""error"": ""invalid_name""}
            return {""ok"": True, ""endpoint"": self._registry.get(name)}

        if cmd == ""unregister"":
            name = data.get(""name"")
            if not isinstance(name, str):
                return {""ok"": False, ""error"": ""invalid_name""}
            existed = name in self._registry
            self._registry.pop(name, None)
            return {""ok"": True, ""removed"": existed}

        if cmd == ""list"":
            return {""ok"": True, ""names"": list(self._registry.keys())}

        if cmd == ""shutdown"":
            self._stop_event.set()
            return {""ok"": True}

        return {""ok"": False, ""error"": ""unknown_cmd""}

    def _serve(self, address_receiver: Any, address: Optional[str]):
        try:
            endpoint = self._bind_socket(address)
        except Exception:
            self._running = False
            return
        self._notify_address(address_receiver, endpoint)

        self._running = True
        try:
            while not self._stop_event.is_set():
                try:
                    msg = self._sock.recv()  # rcvtimeo set
                except zmq.Again:
                    continue
                except Exception:
                    break
                resp = self._handle_message(msg)
                try:
                    self._sock.send(json.dumps(resp).encode(""utf-8""))
                except Exception:
                    # If send fails, try to continue
                    pass
        finally:
            self.close_sockets_and_threads()

    def run(self, address_receiver, address: str | None = None):
        '''Run the listener and answer to requests.'''
        if self._running:
            return
        self._stop_event.clear()
        # Run in the current thread, blocking
        self._serve(address_receiver, address)

    def close_sockets_and_threads(self):
        if self._sock is not None:
            try:
                self._sock.close(0)
            except Exception:
                pass
            self._sock = None
        if self._ctx is not None:
            try:
                # Do not terminate the shared instance for safety; close if ours
                # Using instance() above, so don't call terminate to avoid impacting others.
                pass
            except Exception:
                pass
            self._ctx = None
        self._running = False

    def stop(self):
        self._stop_event.set()"
357239,pytroll/posttroll,posttroll/backends/zmq/socket.py,posttroll.backends.zmq.socket.SocketReceiver,"from posttroll.message import Message
import zmq

class SocketReceiver:
    """"""A receiver for mulitple sockets.""""""

    def __init__(self):
        """"""Set up the receiver.""""""
        self._poller = zmq.Poller()

    def register(self, socket):
        """"""Register the socket.""""""
        self._poller.register(socket, zmq.POLLIN)

    def unregister(self, socket):
        """"""Unregister the socket.""""""
        self._poller.unregister(socket)

    def receive(self, *sockets, timeout=None):
        """"""Timeout is in seconds.""""""
        if timeout:
            timeout *= 1000
        socks = dict(self._poller.poll(timeout=timeout))
        if socks:
            for sock in sockets:
                if socks.get(sock) == zmq.POLLIN:
                    received = sock.recv_string(zmq.NOBLOCK)
                    yield (Message.decode(received), sock)
        else:
            raise TimeoutError('Did not receive anything on sockets.')","
class SocketReceiver:
    
    def __init__(self):
        
        pass
    def register(self, socket):
        '''Register the socket.'''
        pass
    def unregister(self, socket):
        
        pass
    def receive(self, *sockets, timeout=None):
        
        pass",snippet_291,5,"class SocketReceiver:
    
    def __init__(self):
        import threading
        self._sockets = set()
        self._lock = threading.Lock()

    def register(self, socket):
        '''Register the socket.'''
        with self._lock:
            self._sockets.add(socket)

    def unregister(self, socket):
        with self._lock:
            self._sockets.discard(socket)

    def receive(self, *sockets, timeout=None):
        import selectors

        if timeout is not None:
            if not isinstance(timeout, (int, float)) or timeout < 0:
                raise ValueError(""timeout must be None or a non-negative number"")

        with self._lock:
            registered = list(self._sockets)

        to_monitor = set(registered)
        for s in sockets:
            if s is None:
                continue
            to_monitor.add(s)

        if not to_monitor:
            # Nothing to wait on
            if timeout is not None and timeout > 0:
                # Simulate waiting with no sockets monitored
                import time
                time.sleep(timeout)
            return {}

        sel = selectors.DefaultSelector()
        try:
            for s in to_monitor:
                try:
                    sel.register(s, selectors.EVENT_READ)
                except Exception:
                    # Skip sockets that cannot be registered (e.g., closed)
                    pass

            events = sel.select(timeout)
            if not events:
                return {}

            results = {}
            for key, _ in events:
                sock = key.fileobj
                try:
                    # Read a chunk; do not block or loop indefinitely
                    # 64 KiB is a reasonable upper bound for a single read
                    data = sock.recv(65536)
                except BlockingIOError:
                    continue
                except Exception:
                    # On error, treat as closed
                    data = b""""

                results[sock] = data

                if data == b"""":
                    # Connection closed - if it's a registered socket, unregister it
                    with self._lock:
                        self._sockets.discard(sock)

            return results
        finally:
            try:
                sel.close()
            except Exception:
                pass"
357241,pytroll/posttroll,posttroll/bbmcast.py,posttroll.bbmcast.MulticastReceiver,"import struct
from socket import AF_INET, INADDR_ANY, IP_ADD_MEMBERSHIP, IP_MULTICAST_IF, IP_MULTICAST_LOOP, IP_MULTICAST_TTL, IPPROTO_IP, SO_BROADCAST, SO_LINGER, SO_REUSEADDR, SOCK_DGRAM, SOL_IP, SOL_SOCKET, gethostbyname, inet_aton, socket, timeout

class MulticastReceiver:
    """"""Multicast receiver on *port* for an *mcgroup*.""""""
    BUFSIZE = 1024

    def __init__(self, port, mcgroup=None):
        """"""Set up the multicast receiver.""""""
        self.port = port
        self.socket, self.group = mcast_receiver(port, mcgroup)
        logger.info(f'Receiver initialized on group {self.group}.')

    def settimeout(self, tout=None):
        """"""Set timeout.

        A timeout will throw a 'socket.timeout'.
        """"""
        self.socket.settimeout(tout)
        return self

    def __call__(self):
        """"""Receive data from a socket.""""""
        data, sender = self.socket.recvfrom(self.BUFSIZE)
        return (data.decode(), sender)

    def close(self):
        """"""Close the receiver.""""""
        self.socket.setsockopt(SOL_SOCKET, SO_LINGER, struct.pack('ii', 1, 1))
        self.socket.close()","
class MulticastReceiver:
    '''Multicast receiver on *port* for an *mcgroup*.'''
    def __init__(self, port, mcgroup=None):
        
        pass
    def settimeout(self, tout=None):
        '''Set timeout.
        A timeout will throw a 'socket.timeout'.
        '''
        pass
    def __call__(self):
        
        pass
    def close(self):
        '''Close the receiver.'''
        pass",snippet_292,5,"import socket
import struct

class MulticastReceiver:
    '''Multicast receiver on *port* for an *mcgroup*.'''
    def __init__(self, port, mcgroup=None):
        self.port = int(port)
        self.mcgroup = mcgroup
        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)
        try:
            self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        except OSError:
            pass
        try:
            self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)
        except (AttributeError, OSError):
            pass
        self.sock.bind(('', self.port))
        if self.mcgroup:
            mreq = struct.pack('=4s4s', socket.inet_aton(self.mcgroup), socket.inet_aton('0.0.0.0'))
            self.sock.setsockopt(socket.IPPROTO_IP, socket.IP_ADD_MEMBERSHIP, mreq)

    def settimeout(self, tout=None):
        '''Set timeout.
        A timeout will throw a 'socket.timeout'.
        '''
        if self.sock is None:
            raise RuntimeError(""Receiver is closed"")
        self.sock.settimeout(tout)

    def __call__(self):
        if self.sock is None:
            raise RuntimeError(""Receiver is closed"")
        data, addr = self.sock.recvfrom(65535)
        return data, addr

    def close(self):
        '''Close the receiver.'''
        if self.sock is not None:
            try:
                if self.mcgroup:
                    mreq = struct.pack('=4s4s', socket.inet_aton(self.mcgroup), socket.inet_aton('0.0.0.0'))
                    try:
                        self.sock.setsockopt(socket.IPPROTO_IP, socket.IP_DROP_MEMBERSHIP, mreq)
                    except OSError:
                        pass
            finally:
                try:
                    self.sock.close()
                finally:
                    self.sock = None"
357242,pytroll/posttroll,posttroll/bbmcast.py,posttroll.bbmcast.MulticastSender,"class MulticastSender:
    """"""Multicast sender on *port* and *mcgroup*.""""""

    def __init__(self, port, mcgroup=None):
        """"""Set up the multicast sender.""""""
        self.port = port
        self.group = mcgroup
        self.socket, self.group = mcast_sender(mcgroup)
        logger.debug('Started multicast group %s', self.group)

    def __call__(self, data):
        """"""Send data to a socket.""""""
        self.socket.sendto(data.encode(), (self.group, self.port))

    def close(self):
        """"""Close the sender.""""""
        self.socket.close()","class MulticastSender:
    '''Multicast sender on *port* and *mcgroup*.'''
    def __init__(self, port, mcgroup=None):
        '''Set up the multicast sender.'''
        pass
    def __call__(self, data):
        
        pass
    def close(self):
        '''Close the sender.'''
        pass",snippet_293,4,"import socket
import struct

class MulticastSender:
    '''Multicast sender on *port* and *mcgroup*.'''
    def __init__(self, port, mcgroup=None):
        '''Set up the multicast sender.'''
        self.port = int(port)
        self.mcgroup = mcgroup or '224.0.0.1'
        self._closed = False

        if ':' in self.mcgroup:
            # IPv6 multicast
            self._family = socket.AF_INET6
            self._sock = socket.socket(self._family, socket.SOCK_DGRAM)
            try:
                self._sock.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_MULTICAST_HOPS, 1)
                self._sock.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_MULTICAST_LOOP, 1)
            except OSError:
                pass
        else:
            # IPv4 multicast
            self._family = socket.AF_INET
            self._sock = socket.socket(self._family, socket.SOCK_DGRAM)
            try:
                self._sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            except OSError:
                pass
            try:
                ttl = struct.pack('b', 1)
                self._sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_TTL, ttl)
                self._sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_LOOP, 1)
            except OSError:
                pass

    def __call__(self, data):
        if self._closed:
            raise RuntimeError(""MulticastSender is closed"")
        if isinstance(data, str):
            data = data.encode('utf-8')
        elif not isinstance(data, (bytes, bytearray, memoryview)):
            raise TypeError(""data must be bytes-like or str"")
        address = (self.mcgroup, self.port)
        return self._sock.sendto(data, address)

    def close(self):
        '''Close the sender.'''
        if not self._closed:
            try:
                self._sock.close()
            finally:
                self._closed = True"
357244,pytroll/posttroll,posttroll/listener.py,posttroll.listener.ListenerContainer,"from threading import Thread
from queue import Queue
import logging

class ListenerContainer:
    """"""Container for a listener instance.""""""
    logger = logging.getLogger(__name__ + '.ListenerContainer')

    def __init__(self, topics=None, addresses=None, nameserver='localhost', services=''):
        """"""Initialize the class.""""""
        self.listener = None
        self.output_queue = None
        self.thread = None
        self.addresses = addresses
        self.nameserver = nameserver
        if topics is not None:
            self.output_queue = Queue()
            self.listener = Listener(topics=topics, queue=self.output_queue, addresses=self.addresses, nameserver=self.nameserver, services=services)
            self.thread = Thread(target=self.listener.run, daemon=True)
            self.thread.start()

    def __setstate__(self, state):
        """"""Re-initialize the class.""""""
        self.__init__(**state)

    def restart_listener(self, topics):
        """"""Restart listener after configuration update.""""""
        if self.listener is not None:
            if self.listener.running:
                self.stop()
        self.__init__(topics=topics)

    def stop(self):
        """"""Stop listener.""""""
        self.logger.debug('Stopping listener.')
        self.listener.stop()
        if self.thread is not None:
            self.thread.join()
            self.thread = None
        self.logger.debug('Listener stopped.')","
class ListenerContainer:
    '''Container for a listener instance.'''
    def __init__(self, topics=None, addresses=None, nameserver='localhost', services=''):
        '''Initialize the class.'''
        pass
    def __setstate__(self, state):
        
        pass
    def restart_listener(self, topics):
        '''Restart listener after configuration update.'''
        pass
    def stop(self):
        
        pass",snippet_294,5,"class ListenerContainer:
    '''Container for a listener instance.'''
    def __init__(self, topics=None, addresses=None, nameserver='localhost', services=''):
        '''Initialize the class.'''
        import threading
        self.topics = list(topics) if topics is not None else []
        self.addresses = list(addresses) if addresses is not None else []
        self.nameserver = nameserver
        if isinstance(services, str):
            self.services = [s.strip() for s in services.split(',')] if services else []
        else:
            self.services = list(services)
        self._stop_event = threading.Event()
        self._thread = None
        self._lock = threading.RLock()
        if self.topics or self.addresses:
            self._start_listener()

    def __setstate__(self, state):
        import threading
        self.topics = list(state.get('topics', []))
        self.addresses = list(state.get('addresses', []))
        self.nameserver = state.get('nameserver', 'localhost')
        self.services = list(state.get('services', []))
        self._stop_event = threading.Event()
        self._thread = None
        self._lock = threading.RLock()
        if self.topics or self.addresses:
            self._start_listener()

    def _start_listener(self):
        import threading

        def _run():
            # Placeholder listener loop that runs until stopped.
            # In a real implementation, this would subscribe/connect using
            # addresses/nameserver/services and filter by topics.
            while not self._stop_event.wait(0.1):
                pass

        with self._lock:
            if self._thread is not None and self._thread.is_alive():
                return
            self._stop_event.clear()
            self._thread = threading.Thread(target=_run, name=""ListenerContainerThread"", daemon=True)
            self._thread.start()

    def restart_listener(self, topics):
        '''Restart listener after configuration update.'''
        with self._lock:
            if topics is not None:
                self.topics = list(topics)
            self.stop()
            if self.topics or self.addresses:
                self._start_listener()

    def stop(self):
        with self._lock:
            if self._thread is None:
                return
            self._stop_event.set()
            t = self._thread
            self._thread = None
        if t.is_alive():
            t.join(timeout=5)"
357253,pytroll/posttroll,posttroll/message_broadcaster.py,posttroll.message_broadcaster.DesignatedReceiversSender,"from posttroll import config, message

class DesignatedReceiversSender:
    """"""Sends message to multiple *receivers* on *port*.""""""

    def __init__(self, default_port, receivers):
        """"""Set settings.""""""
        backend = config.get('backend', 'unsecure_zmq')
        if backend == 'unsecure_zmq':
            from posttroll.backends.zmq.message_broadcaster import ZMQDesignatedReceiversSender
            self._sender = ZMQDesignatedReceiversSender(default_port, receivers)
        else:
            raise NotImplementedError()

    def __call__(self, data):
        """"""Send messages from all receivers.""""""
        return self._sender(data)

    def close(self):
        """"""Close the sender.""""""
        return self._sender.close()","
class DesignatedReceiversSender:
    
    def __init__(self, default_port, receivers):
        
        pass
    def __call__(self, data):
        
        pass
    def close(self):
        '''Close the sender.'''
        pass",snippet_295,4,"import socket
from typing import Iterable, List, Tuple, Union
import threading


class DesignatedReceiversSender:
    
    def __init__(self, default_port: int, receivers: Iterable[Union[str, Tuple[str, int]]]):
        self._closed = False
        self._lock = threading.Lock()
        if not isinstance(default_port, int) or not (0 < default_port < 65536):
            raise ValueError(""default_port must be an integer between 1 and 65535"")
        self._default_port = default_port

        normalized: List[Tuple[str, int]] = []
        for r in receivers or []:
            if isinstance(r, tuple):
                if len(r) != 2:
                    raise ValueError(f""Receiver tuple must be (host, port), got: {r}"")
                host, port = r
                if not isinstance(host, str) or not isinstance(port, int):
                    raise ValueError(f""Receiver tuple must be (str, int), got: {r}"")
                normalized.append((host, port))
            elif isinstance(r, str):
                s = r.strip()
                if not s:
                    continue
                # Try host:port for IPv4/hostname. IPv6 may contain colons, handle [addr]:port
                host = None
                port = None
                if s.startswith('['):
                    # [ipv6]:port
                    end = s.find(']')
                    if end == -1:
                        raise ValueError(f""Invalid receiver format: {r}"")
                    host = s[1:end]
                    rest = s[end+1:].lstrip()
                    if rest.startswith(':'):
                        rest = rest[1:]
                        if not rest.isdigit():
                            raise ValueError(f""Invalid port in receiver: {r}"")
                        port = int(rest)
                    else:
                        port = self._default_port
                else:
                    if ':' in s and s.count(':') == 1:
                        h, p = s.split(':', 1)
                        if p.isdigit():
                            host = h
                            port = int(p)
                        else:
                            host = s
                            port = self._default_port
                    else:
                        # Could be plain hostname or IPv6 without brackets; use default port
                        host = s
                        port = self._default_port
                normalized.append((host, port))
            else:
                raise TypeError(f""Unsupported receiver type: {type(r)}"")

        if not normalized:
            raise ValueError(""At least one receiver must be provided"")

        # Resolve addresses
        self._targets: List[Tuple[int, Tuple]] = []  # list of (family, sockaddr)
        for host, port in normalized:
            try:
                infos = socket.getaddrinfo(host, port, type=socket.SOCK_DGRAM)
            except socket.gaierror as e:
                raise ValueError(f""Failed to resolve receiver {host}:{port} - {e}"") from e
            # Prefer first result; include all unique sockaddrs to avoid duplicates across families
            seen = set()
            for family, socktype, proto, canonname, sockaddr in infos:
                if socktype != socket.SOCK_DGRAM:
                    continue
                key = (family, sockaddr)
                if key in seen:
                    continue
                seen.add(key)
                self._targets.append((family, sockaddr))

        if not self._targets:
            raise ValueError(""No resolvable receiver addresses found"")

        self._sockets = {}  # family -> socket
        # Create sockets lazily on first send to allow environments without certain families

    def __call__(self, data):
        if self._closed:
            raise RuntimeError(""Sender is closed"")
        if isinstance(data, str):
            payload = data.encode('utf-8')
        elif isinstance(data, (bytes, bytearray, memoryview)):
            payload = bytes(data)
        else:
            raise TypeError(""data must be str or bytes-like"")

        sent_count = 0
        with self._lock:
            for family, sockaddr in self._targets:
                sock = self._sockets.get(family)
                if sock is None:
                    try:
                        sock = socket.socket(family, socket.SOCK_DGRAM)
                        # For IPv6, try to allow dual-stack if supported (non-fatal if fails)
                        if family == socket.AF_INET6:
                            try:
                                sock.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 0)
                            except OSError:
                                pass
                        self._sockets[family] = sock
                    except OSError:
                        continue
                try:
                    sock.sendto(payload, sockaddr)
                    sent_count += 1
                except OSError:
                    continue
        return sent_count

    def close(self):
        '''Close the sender.'''
        if self._closed:
            return
        with self._lock:
            if self._closed:
                return
            for sock in self._sockets.values():
                try:
                    sock.close()
                except Exception:
                    pass
            self._sockets.clear()
            self._closed = True"
357255,pytroll/posttroll,posttroll/ns.py,posttroll.ns.NameServer,"from posttroll.address_receiver import AddressReceiver
import datetime as dt
from posttroll import config
from contextlib import suppress

class NameServer:
    """"""The name server.""""""

    def __init__(self, max_age=None, multicast_enabled=True, restrict_to_localhost=False):
        """"""Initialize nameserver.""""""
        self.loop = True
        self.listener = None
        self._max_age = max_age or dt.timedelta(minutes=10)
        self._multicast_enabled = multicast_enabled
        self._restrict_to_localhost = restrict_to_localhost
        backend = config['backend']
        if backend not in ['unsecure_zmq', 'secure_zmq']:
            raise NotImplementedError(f'Did not recognize backend: {backend}')
        from posttroll.backends.zmq.ns import ZMQNameServer
        self._ns = ZMQNameServer()

    def run(self, address_receiver=None, nameserver_address=None):
        """"""Run the listener and answer to requests.""""""
        if address_receiver is None:
            address_receiver = AddressReceiver(max_age=self._max_age, multicast_enabled=self._multicast_enabled, restrict_to_localhost=self._restrict_to_localhost)
            address_receiver.start()
        try:
            return self._ns.run(address_receiver, nameserver_address)
        finally:
            with suppress(AttributeError):
                address_receiver.stop()

    def stop(self):
        """"""Stop the nameserver.""""""
        return self._ns.stop()","
class NameServer:
    '''The name server.'''
    def __init__(self, max_age=None, multicast_enabled=True, restrict_to_localhost=False):
        
        pass
    def run(self, address_receiver=None, nameserver_address=None):
        
        pass
    def stop(self):
        
        pass",snippet_296,4,"import socket
import struct
import threading
import time
import select


class NameServer:
    '''The name server.'''

    _MULTICAST_GRP = '239.255.42.42'
    _MULTICAST_PORT = 42424
    _ANNOUNCE_INTERVAL = 2.0
    _RECV_BUFSIZE = 65535

    def __init__(self, max_age=None, multicast_enabled=True, restrict_to_localhost=False):
        self.max_age = max_age
        self.multicast_enabled = multicast_enabled
        self.restrict_to_localhost = restrict_to_localhost

        self._thread = None
        self._stop_evt = threading.Event()
        self._lock = threading.RLock()

        self._unicast_sock = None
        self._mc_recv_sock = None

        self._address = None  # (host, port) bound for unicast
        self._address_receiver = None

        self._peers = {}  # {(host, port): last_seen_epoch}

    def run(self, address_receiver=None, nameserver_address=None):
        with self._lock:
            if self._thread and self._thread.is_alive():
                return
            self._address_receiver = address_receiver
            self._setup_sockets(nameserver_address)
            self._stop_evt.clear()
            self._thread = threading.Thread(target=self._loop, name=""NameServer"", daemon=True)
            self._thread.start()

    def stop(self):
        with self._lock:
            self._stop_evt.set()
            t = self._thread
        if t:
            t.join(timeout=5.0)
        self._close_sockets()

    # Internal methods

    def _setup_sockets(self, nameserver_address):
        host, port = self._parse_address(nameserver_address)
        uc = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        try:
            uc.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        except OSError:
            pass
        uc.bind((host, port))
        uc.setblocking(False)
        bound_host = uc.getsockname()[0]
        bound_port = uc.getsockname()[1]
        self._unicast_sock = uc
        self._address = (bound_host, bound_port)

        if self.multicast_enabled and not self.restrict_to_localhost:
            self._mc_recv_sock = self._create_multicast_receiver()
        else:
            self._mc_recv_sock = None

    def _close_sockets(self):
        with self._lock:
            if self._unicast_sock:
                try:
                    self._unicast_sock.close()
                except Exception:
                    pass
                self._unicast_sock = None
            if self._mc_recv_sock:
                try:
                    # Attempt to drop membership
                    mreq = struct.pack(""=4s4s"", socket.inet_aton(self._MULTICAST_GRP), socket.inet_aton(""0.0.0.0""))
                    try:
                        self._mc_recv_sock.setsockopt(socket.IPPROTO_IP, socket.IP_DROP_MEMBERSHIP, mreq)
                    except OSError:
                        pass
                    self._mc_recv_sock.close()
                except Exception:
                    pass
                self._mc_recv_sock = None

    def _parse_address(self, addr):
        if addr is None:
            host = ""127.0.0.1"" if self.restrict_to_localhost else """"
            return host, 0
        if isinstance(addr, tuple) and len(addr) == 2:
            return addr[0], int(addr[1])
        if isinstance(addr, str):
            if "":"" in addr:
                h, p = addr.rsplit("":"", 1)
                return h or (""127.0.0.1"" if self.restrict_to_localhost else """"), int(p)
            try:
                # if it's just a port
                return (""127.0.0.1"" if self.restrict_to_localhost else """"), int(addr)
            except ValueError:
                return addr, 0
        # Fallback
        return (""127.0.0.1"" if self.restrict_to_localhost else """"), 0

    def _create_multicast_receiver(self):
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)
        try:
            s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        except OSError:
            pass
        try:
            s.bind(('', self._MULTICAST_PORT))
        except OSError:
            # If bind fails, disable multicast receive
            s.close()
            return None
        mreq = struct.pack(""=4s4s"", socket.inet_aton(self._MULTICAST_GRP), socket.inet_aton(""0.0.0.0""))
        try:
            s.setsockopt(socket.IPPROTO_IP, socket.IP_ADD_MEMBERSHIP, mreq)
        except OSError:
            try:
                s.close()
            finally:
                return None
        s.setblocking(False)
        return s

    def _send_multicast_announcement(self):
        if not self.multicast_enabled or self.restrict_to_localhost:
            return
        if not self._unicast_sock or not self._address:
            return
        msg = self._encode_message(""NS"", self._address[0], str(self._address[1]))
        try:
            # Send via a dedicated sending socket to set TTL and LOOP
            s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)
            try:
                s.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_TTL, 1)
                s.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_LOOP, 1)
            except OSError:
                pass
            s.sendto(msg, (self._MULTICAST_GRP, self._MULTICAST_PORT))
        except Exception:
            pass
        finally:
            try:
                s.close()
            except Exception:
                pass

    def _loop(self):
        last_announce = 0.0
        while not self._stop_evt.is_set():
            now = time.time()
            if now - last_announce >= self._ANNOUNCE_INTERVAL:
                self._send_multicast_announcement()
                last_announce = now

            rlist = []
            if self._unicast_sock:
                rlist.append(self._unicast_sock)
            if self._mc_recv_sock:
                rlist.append(self._mc_recv_sock)

            timeout = 0.2
            try:
                readable, _, _ = select.select(rlist, [], [], timeout)
            except Exception:
                readable = []

            for sock in readable:
                try:
                    data, addr = sock.recvfrom(self._RECV_BUFSIZE)
                except Exception:
                    continue
                self._handle_packet(data, addr, sock)

            self._evict_old_peers()

    def _handle_packet(self, data, addr, sock):
        try:
            text = data.decode('utf-8', errors='ignore').strip()
        except Exception:
            return
        if not text:
            return

        parts = text.split()
        if not parts:
            return

        cmd = parts[0].upper()

        if cmd == ""DISCOVER"":
            self._reply_with_address(addr)
            return

        if cmd == ""NS"":
            if len(parts) >= 3:
                host = parts[1]
                try:
                    port = int(parts[2])
                except ValueError:
                    return
                self._record_peer((host, port))
            return

        if cmd == ""PING"":
            self._sendto_safe(b""PONG"", addr)
            return

        if cmd == ""WHERE"":
            if len(parts) >= 2 and parts[1].upper() == ""NS"":
                self._reply_with_address(addr)
            return

    def _reply_with_address(self, addr):
        if not self._address:
            return
        msg = self._encode_message(""NS"", self._address[0], str(self._address[1]))
        self._sendto_safe(msg, addr)

    def _sendto_safe(self, data, addr):
        try:
            if self._unicast_sock:
                self._unicast_sock.sendto(data, addr)
        except Exception:
            pass

    def _record_peer(self, peer):
        now = time.time()
        with self._lock:
            self._peers[peer] = now
        cb = self._address_receiver
        if cb:
            try:
                cb(peer)
            except Exception:
                pass

    def _evict_old_peers(self):
        if self.max_age is None:
            return
        cutoff = time.time() - float(self.max_age)
        with self._lock:
            stale = [p for p, ts in self._peers.items() if ts < cutoff]
            for p in stale:
                self._peers.pop(p, None)

    def _encode_message(self, *parts):
        return ("" "".join(parts)).encode(""utf-8"")

    # Optional helpers

    @property
    def address(self):
        return self._address

    @property
    def peers(self):
        with self._lock:
            return dict(self._peers)"
357257,pytroll/posttroll,posttroll/publisher.py,posttroll.publisher.Publish,"class Publish:
    """"""The publishing context.

    See :class:`Publisher` and :class:`NoisyPublisher` for more information on the arguments.

    The publisher is selected based on the arguments, see :func:`create_publisher_from_dict_config` for
    information how the selection is done.

    Example on how to use the :class:`Publish` context::

            from posttroll.publisher import Publish
            from posttroll.message import Message
            import time

            try:
                with Publish(""my_service"", port=9000) as pub:
                    counter = 0
                    while True:
                        counter += 1
                        message = Message(""/counter"", ""info"", str(counter))
                        print(""publishing"", message)
                        pub.send(message.encode())
                        time.sleep(3)
            except KeyboardInterrupt:
                print(""terminating publisher..."")

    """"""

    def __init__(self, name, port=0, aliases=None, broadcast_interval=2, nameservers=None, min_port=None, max_port=None):
        """"""Initialize the class.""""""
        settings = {'name': name, 'port': port, 'min_port': min_port, 'max_port': max_port, 'aliases': aliases, 'broadcast_interval': broadcast_interval, 'nameservers': nameservers}
        self.publisher = create_publisher_from_dict_config(settings)

    def __enter__(self):
        """"""Enter the context.""""""
        return self.publisher.start()

    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""Exit the context.""""""
        self.publisher.stop()","class Publish:
    '''The publishing context.
    See :class:`Publisher` and :class:`NoisyPublisher` for more information on the arguments.
    The publisher is selected based on the arguments, see :func:`create_publisher_from_dict_config` for
    information how the selection is done.
    Example on how to use the :class:`Publish` context::
            from posttroll.publisher import Publish
            from posttroll.message import Message
            import time
            try:
                with Publish(""my_service"", port=9000) as pub:
                    counter = 0
                    while True:
                        counter += 1
                        message = Message(""/counter"", ""info"", str(counter))
                        print(""publishing"", message)
                        pub.send(message.encode())
                        time.sleep(3)
            except KeyboardInterrupt:
                print(""terminating publisher..."")
    '''
    def __init__(self, name, port=0, aliases=None, broadcast_interval=2, nameservers=None, min_port=None, max_port=None):
        '''Initialize the class.'''
        pass
    def __enter__(self):
        
        pass
    def __exit__(self, exc_type, exc_val, exc_tb):
        
        pass",snippet_297,4,"class Publish:
    '''The publishing context.
    See :class:`Publisher` and :class:`NoisyPublisher` for more information on the arguments.
    The publisher is selected based on the arguments, see :func:`create_publisher_from_dict_config` for
    information how the selection is done.
    Example on how to use the :class:`Publish` context::
            from posttroll.publisher import Publish
            from posttroll.message import Message
            import time
            try:
                with Publish(""my_service"", port=9000) as pub:
                    counter = 0
                    while True:
                        counter += 1
                        message = Message(""/counter"", ""info"", str(counter))
                        print(""publishing"", message)
                        pub.send(message.encode())
                        time.sleep(3)
            except KeyboardInterrupt:
                print(""terminating publisher..."")
    '''
    def __init__(self, name, port=0, aliases=None, broadcast_interval=2, nameservers=None, min_port=None, max_port=None):
        '''Initialize the class.'''
        self._name = name
        self._port = port
        self._aliases = aliases
        self._broadcast_interval = broadcast_interval
        self._nameservers = nameservers
        self._min_port = min_port
        self._max_port = max_port
        self._publisher = None
        self._entered = False

    def __enter__(self):
        try:
            from posttroll.publisher import create_publisher_from_dict_config
        except Exception as e:
            raise RuntimeError(""posttroll.publisher.create_publisher_from_dict_config is not available"") from e

        config = {
            ""name"": self._name,
            ""port"": self._port,
            ""aliases"": self._aliases,
            ""broadcast_interval"": self._broadcast_interval,
            ""nameservers"": self._nameservers,
            ""min_port"": self._min_port,
            ""max_port"": self._max_port,
        }
        self._publisher = create_publisher_from_dict_config(config)

        # If the underlying publisher is a context manager, delegate to it
        if hasattr(self._publisher, ""__enter__"") and hasattr(self._publisher, ""__exit__""):
            self._entered = True
            return self._publisher.__enter__()

        # Otherwise, start it if possible and return the publisher itself
        start = getattr(self._publisher, ""start"", None)
        if callable(start):
            start()
        self._entered = True
        return self._publisher

    def __exit__(self, exc_type, exc_val, exc_tb):
        if not self._entered:
            return False

        if self._publisher is None:
            return False

        # Delegate to underlying context manager if available
        if hasattr(self._publisher, ""__exit__""):
            return self._publisher.__exit__(exc_type, exc_val, exc_tb)

        # Attempt graceful shutdown methods
        for meth_name in (""stop"", ""close"", ""shutdown"", ""terminate""):
            meth = getattr(self._publisher, meth_name, None)
            if callable(meth):
                try:
                    meth()
                except Exception:
                    pass
                break

        self._publisher = None
        self._entered = False
        return False"
357261,pytroll/posttroll,posttroll/subscriber.py,posttroll.subscriber.Subscribe,"from posttroll.message import _MAGICK

class Subscribe:
    """"""Subscriber context.

    See :class:`NSSubscriber` and :class:`Subscriber` for initialization parameters.

    The subscriber is selected based on the arguments, see :func:`create_subscriber_from_dict_config` for
    information how the selection is done.

    Example::
            del tmp

        from posttroll.subscriber import Subscribe

        with Subscribe(""a_service"", ""my_topic"",) as sub:
            for msg in sub.recv():
                print(msg)

    """"""

    def __init__(self, services='', topics=_MAGICK, addr_listener=False, addresses=None, timeout=10, translate=False, nameserver='localhost', message_filter=None):
        """"""Initialize the class.""""""
        settings = {'services': services, 'topics': topics, 'message_filter': message_filter, 'translate': translate, 'addr_listener': addr_listener, 'addresses': addresses, 'timeout': timeout, 'nameserver': nameserver}
        self.subscriber = create_subscriber_from_dict_config(settings)

    def __enter__(self):
        """"""Start the subscriber when used as a context manager.""""""
        return self.subscriber

    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""Stop the subscriber when used as a context manager.""""""
        return self.subscriber.stop()","
class Subscribe:
    
    def __init__(self, services='', topics=_MAGICK, addr_listener=False, addresses=None, timeout=10, translate=False, nameserver='localhost', message_filter=None):
        '''Initialize the class.'''
        pass
    def __enter__(self):
        '''Start the subscriber when used as a context manager.'''
        pass
    def __exit__(self, exc_type, exc_val, exc_tb):
        
        pass",snippet_298,4,"import time

# Provide a local sentinel if not defined elsewhere
try:
    _MAGICK
except NameError:
    _MAGICK = object()


class Subscribe:
    
    def __init__(self, services='', topics=_MAGICK, addr_listener=False, addresses=None, timeout=10, translate=False, nameserver='localhost', message_filter=None):
        '''Initialize the class.'''
        self.services = self._normalize_to_tuple(services)
        if topics is _MAGICK:
            self.topics = self.services
        else:
            self.topics = self._normalize_to_tuple(topics)
        self.addr_listener = bool(addr_listener)
        self.addresses = tuple(addresses) if addresses is not None else tuple()
        self.timeout = float(timeout) if timeout is not None else 10.0
        self.translate = bool(translate)
        self.nameserver = str(nameserver) if nameserver is not None else 'localhost'
        if message_filter is not None and not callable(message_filter):
            raise TypeError(""message_filter must be callable or None"")
        self.message_filter = message_filter

        self.active = False
        self.started_at = None

    def __enter__(self):
        '''Start the subscriber when used as a context manager.'''
        self.active = True
        self.started_at = time.time()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.active = False
        self.started_at = None
        return False

    @staticmethod
    def _normalize_to_tuple(value):
        if value is None:
            return tuple()
        if isinstance(value, str):
            stripped = value.strip()
            if not stripped:
                return tuple()
            # Split on commas or whitespace
            parts = [p for chunk in stripped.split(',') for p in chunk.split() if p]
            return tuple(parts)
        try:
            return tuple(value)
        except TypeError:
            return (value,)"
357263,pytroll/posttroll,posttroll/subscriber.py,posttroll.subscriber._AddressListener,"from posttroll.address_receiver import get_configured_address_port

class _AddressListener:
    """"""Listener for new addresses of interest.""""""

    def __init__(self, subscriber, services='', nameserver='localhost'):
        """"""Initialize address listener.""""""
        if isinstance(services, str):
            services = [services]
        self.services = services
        self.subscriber = subscriber
        address_publish_port = get_configured_address_port()
        self.subscriber.add_hook_sub('tcp://' + nameserver + ':' + str(address_publish_port), ['pytroll://address'], self.handle_msg)

    def handle_msg(self, msg):
        """"""Handle the message *msg*.""""""
        addr_ = msg.data['URI']
        status = msg.data.get('status', True)
        if status:
            msg_services = msg.data.get('service')
            for service in self.services:
                if not service or service in msg_services:
                    LOGGER.debug('Adding address %s %s', str(addr_), str(service))
                    self.subscriber.add(addr_)
                    break
        else:
            LOGGER.debug('Removing address %s', str(addr_))
            self.subscriber.remove(addr_)","
class _AddressListener:
    
    def __init__(self, subscriber, services='', nameserver='localhost'):
        '''Initialize address listener.'''
        pass
    def handle_msg(self, msg):
        '''Handle the message *msg*.'''
        pass",snippet_299,3,"class _AddressListener:
    
    def __init__(self, subscriber, services='', nameserver='localhost'):
        '''Initialize address listener.'''
        self.subscriber = subscriber
        self.nameserver = nameserver
        if services is None:
            services = ''
        if isinstance(services, str):
            tokens = [s.strip() for s in services.replace(';', ',').split(',') if s.strip()]
        else:
            try:
                tokens = [str(s).strip() for s in services if str(s).strip()]
            except TypeError:
                tokens = []
        self._services = set(s.lower() for s in tokens)

    def _normalize_msg(self, msg):
        import json
        # Convert bytes to str
        if isinstance(msg, (bytes, bytearray)):
            try:
                msg = msg.decode('utf-8', errors='ignore')
            except Exception:
                msg = str(msg)

        # If it's a string, try JSON then key=value parsing
        if isinstance(msg, str):
            s = msg.strip()
            if not s:
                return {}
            # Try JSON
            try:
                obj = json.loads(s)
                if isinstance(obj, dict):
                    msg = obj
                elif isinstance(obj, list) and obj and isinstance(obj[0], dict):
                    msg = obj[0]
                else:
                    # Fallthrough to manual parsing
                    raise ValueError
            except Exception:
                # Manual parsing: accept ""key=value"" tokens separated by commas or whitespace
                data = {}
                # If it's exactly two space-separated tokens without equals, treat as service and address
                parts_ws = s.split()
                if len(parts_ws) == 2 and '=' not in s:
                    data['service'] = parts_ws[0]
                    data['address'] = parts_ws[1]
                    msg = data
                else:
                    # Split by commas first, then by whitespace for remaining
                    candidates = []
                    for chunk in s.split(','):
                        chunk = chunk.strip()
                        if not chunk:
                            continue
                        candidates.extend(x for x in chunk.split() if x)
                    for token in candidates:
                        if '=' in token:
                            k, v = token.split('=', 1)
                            data[k.strip()] = v.strip()
                    msg = data

        # If it's not a dict now, try to coerce to dict
        if not isinstance(msg, dict):
            try:
                # Best effort: wrap as address only
                return {'address': str(msg)}
            except Exception:
                return {}

        # Normalize keys to lowercase and strip values
        norm = {}
        for k, v in msg.items():
            key = str(k).lower().strip()
            if isinstance(v, (str, bytes, bytearray)):
                if isinstance(v, (bytes, bytearray)):
                    try:
                        v = v.decode('utf-8', errors='ignore')
                    except Exception:
                        v = str(v)
                v = v.strip()
            norm[key] = v

        # Canonical aliases
        if 'svc' in norm and 'service' not in norm:
            norm['service'] = norm.pop('svc')
        if 'addr' in norm and 'address' not in norm:
            norm['address'] = norm.pop('addr')
        if 'ns' in norm and 'nameserver' not in norm:
            norm['nameserver'] = norm.pop('ns')

        return norm

    def _passes_filters(self, data):
        # Services filter
        service = str(data.get('service', '')).lower() if data.get('service') is not None else ''
        if self._services and service not in self._services:
            return False
        # Nameserver filter (if provided in message)
        msg_ns = data.get('nameserver')
        if msg_ns is not None and str(msg_ns) != str(self.nameserver):
            return False
        return True

    def _dispatch(self, payload):
        target = self.subscriber
        # Prefer explicit callback
        cb = getattr(target, 'on_address', None)
        if callable(cb):
            cb(payload)
            return True
        # Common queue/list interfaces
        for method_name in ('put', 'append', 'send', 'write'):
            m = getattr(target, method_name, None)
            if callable(m):
                m(payload)
                return True
        # Callable subscriber
        if callable(target):
            target(payload)
            return True
        # Attribute to store last message as fallback
        try:
            setattr(target, 'last_address_message', payload)
            return True
        except Exception:
            return False

    def handle_msg(self, msg):
        '''Handle the message *msg*.'''
        data = self._normalize_msg(msg)
        if not data:
            return None
        if not self._passes_filters(data):
            return None
        self._dispatch(data)
        return data"
360979,reanahub/reana-commons,reanahub_reana-commons/reana_commons/validation/compute_backends.py,reana_commons.validation.compute_backends.ComputeBackendValidatorBase,"from typing import Dict, List, Optional
from reana_commons.errors import REANAValidationError

class ComputeBackendValidatorBase:
    """"""REANA workflow compute backend validation base class.""""""

    def __init__(self, workflow_steps: Optional[List[Dict]]=None, supported_backends: Optional[List[str]]=[]):
        """"""Validate compute backends in REANA workflow steps.

        :param workflow_steps: list of dictionaries which represents different steps involved in workflow.
        :param supported_backends: a list of the supported compute backends.
        """"""
        self.workflow_steps = workflow_steps
        self.supported_backends = supported_backends

    def validate(self) -> None:
        """"""Validate compute backends in REANA workflow.""""""
        raise NotImplementedError

    def raise_error(self, compute_backend: str, step_name: str) -> None:
        """"""Raise validation error.""""""
        raise REANAValidationError(f'''Compute backend ""{compute_backend}"" found in step ""{step_name}"" is not supported. List of supported compute backends: ""{', '.join(self.supported_backends)}""''')","
class ComputeBackendValidatorBase:
    
    def __init__(self, workflow_steps: Optional[List[Dict]]=None, supported_backends: Optional[List[str]]=[]):
        '''Validate compute backends in REANA workflow steps.
        :param workflow_steps: list of dictionaries which represents different steps involved in workflow.
        :param supported_backends: a list of the supported compute backends.
        '''
        pass
    def validate(self) -> None:
        
        pass
    def raise_error(self, compute_backend: str, step_name: str) -> None:
        
        pass",snippet_300,4,"from typing import Optional, List, Dict

class ComputeBackendValidatorBase:
    
    def __init__(self, workflow_steps: Optional[List[Dict]] = None, supported_backends: Optional[List[str]] = None):
        '''Validate compute backends in REANA workflow steps.
        :param workflow_steps: list of dictionaries which represents different steps involved in workflow.
        :param supported_backends: a list of the supported compute backends.
        '''
        self.workflow_steps: List[Dict] = workflow_steps or []
        self.supported_backends: List[str] = supported_backends or []

    def validate(self) -> None:
        for step in self.workflow_steps:
            compute_backend = step.get(""compute_backend"")
            if not compute_backend:
                continue
            if self.supported_backends and compute_backend not in self.supported_backends:
                step_name = step.get(""name"", ""<unknown>"")
                self.raise_error(compute_backend, step_name)

    def raise_error(self, compute_backend: str, step_name: str) -> None:
        supported = "", "".join(self.supported_backends) if self.supported_backends else ""none""
        raise ValueError(
            f""Unsupported compute backend '{compute_backend}' in step '{step_name}'. ""
            f""Supported backends: {supported}.""
        )"
361134,raymondEhlers/pachyderm,raymondEhlers_pachyderm/src/pachyderm/fit/base.py,pachyderm.fit.base.BaseFitResult,"import numpy as np
import numpy.typing as npt
from dataclasses import dataclass
from typing import TYPE_CHECKING, Any, TypeVar, cast

@dataclass
class BaseFitResult:
    """"""Base fit result.

    This represents the most basic fit result.

    Attributes:
        parameters: Names of the parameters used in the fit.
        free_parameters: Names of the free parameters used in the fit.
        fixed_parameters: Names of the fixed parameters used in the fit.
        values_at_minimum: Contains the values of the full RP fit function at the minimum. Keys are the
            names of parameters, while values are the numerical values at convergence.
        errors_on_parameters: Contains the values of the errors associated with the parameters
            determined via the fit.
        covariance_matrix: Contains the values of the covariance matrix. Keys are tuples
            with (param_name_a, param_name_b), and the values are covariance between the specified parameters.
            Note that fixed parameters are _not_ included in this matrix.
        errors: Store the errors associated with the component fit function.
    """"""
    parameters: list[str]
    free_parameters: list[str]
    fixed_parameters: list[str]
    values_at_minimum: dict[str, float]
    errors_on_parameters: dict[str, float]
    covariance_matrix: dict[tuple[str, str], float]
    errors: npt.NDArray[Any]

    @property
    def correlation_matrix(self) -> dict[tuple[str, str], float]:
        """"""The correlation matrix of the free parameters.

        These values are derived from the covariance matrix values stored in the fit.

        Note:
            This property caches the correlation matrix value so we don't have to calculate it every time.

        Args:
            None
        Returns:
            The correlation matrix of the fit result.
        """"""
        try:
            return self._correlation_matrix
        except AttributeError:

            def corr(i_name: str, j_name: str) -> float:
                """"""Calculate the correlation matrix (definition from iminuit) from the covariance matrix.""""""
                value = self.covariance_matrix[i_name, j_name] / (np.sqrt(self.covariance_matrix[i_name, i_name] * self.covariance_matrix[j_name, j_name]) + 1e-100)
                return float(value)
            matrix: dict[tuple[str, str], float] = {}
            for i_name in self.free_parameters:
                for j_name in self.free_parameters:
                    matrix[i_name, j_name] = corr(i_name, j_name)
            self._correlation_matrix = matrix
        return self._correlation_matrix","@dataclass
class BaseFitResult:
    '''Base fit result.
    This represents the most basic fit result.
    Attributes:
        parameters: Names of the parameters used in the fit.
        free_parameters: Names of the free parameters used in the fit.
        fixed_parameters: Names of the fixed parameters used in the fit.
        values_at_minimum: Contains the values of the full RP fit function at the minimum. Keys are the
            names of parameters, while values are the numerical values at convergence.
        errors_on_parameters: Contains the values of the errors associated with the parameters
            determined via the fit.
        covariance_matrix: Contains the values of the covariance matrix. Keys are tuples
            with (param_name_a, param_name_b), and the values are covariance between the specified parameters.
            Note that fixed parameters are _not_ included in this matrix.
        errors: Store the errors associated with the component fit function.
    '''
    @property
    def correlation_matrix(self) -> dict[tuple[str, str], float]:
        
        pass
    def correlation_matrix(self) -> dict[tuple[str, str], float]:
        '''Calculate the correlation matrix (definition from iminuit) from the covariance matrix.'''
        pass",snippet_301,3,"from __future__ import annotations

from dataclasses import dataclass, field
from math import sqrt
from typing import Dict, Tuple, List


@dataclass
class BaseFitResult:
    '''Base fit result.
    This represents the most basic fit result.
    Attributes:
        parameters: Names of the parameters used in the fit.
        free_parameters: Names of the free parameters used in the fit.
        fixed_parameters: Names of the fixed parameters used in the fit.
        values_at_minimum: Contains the values of the full RP fit function at the minimum. Keys are the
            names of parameters, while values are the numerical values at convergence.
        errors_on_parameters: Contains the values of the errors associated with the parameters
            determined via the fit.
        covariance_matrix: Contains the values of the covariance matrix. Keys are tuples
            with (param_name_a, param_name_b), and the values are covariance between the specified parameters.
            Note that fixed parameters are _not_ included in this matrix.
        errors: Store the errors associated with the component fit function.
    '''
    parameters: List[str] = field(default_factory=list)
    free_parameters: List[str] = field(default_factory=list)
    fixed_parameters: List[str] = field(default_factory=list)
    values_at_minimum: Dict[str, float] = field(default_factory=dict)
    errors_on_parameters: Dict[str, float] = field(default_factory=dict)
    covariance_matrix: Dict[Tuple[str, str], float] = field(default_factory=dict)
    errors: Dict[str, float] = field(default_factory=dict)

    @property
    def correlation_matrix(self) -> dict[tuple[str, str], float]:
        cov = self.covariance_matrix
        result: Dict[Tuple[str, str], float] = {}

        # Determine the parameter order to include in the correlation matrix
        params = list(self.free_parameters)
        if not params:
            # Fall back to parameters detected from covariance matrix keys
            detected = set()
            for a, b in cov.keys():
                detected.add(a)
                detected.add(b)
            params = sorted(detected)

        # Gather variances
        variances: Dict[str, float] = {}
        for p in params:
            variances[p] = cov.get((p, p), 0.0)

        # Build full symmetric correlation matrix over free parameters
        for i, a in enumerate(params):
            va = variances.get(a, 0.0)
            for b in params[i:]:
                vb = variances.get(b, 0.0)
                cij = cov.get((a, b))
                if cij is None:
                    cij = cov.get((b, a), 0.0)

                denom = sqrt(va * vb) if va > 0.0 and vb > 0.0 else 0.0
                if denom > 0.0:
                    corr = cij / denom
                else:
                    # If variance is zero or missing, correlation is undefined; set to 0.0
                    # Diagonal becomes 0.0 in this case; otherwise off-diagonals 0.0 as well.
                    corr = 0.0

                result[(a, b)] = corr
                result[(b, a)] = corr

        return result"
361458,AtteqCom/zsl,AtteqCom_zsl/src/zsl/errors.py,zsl.errors.ErrorHandler,"from abc import ABCMeta, abstractmethod

class ErrorHandler:
    """"""
    Custom error handler providing a response on a particular error.
    """"""
    __metaclass__ = ABCMeta

    @abstractmethod
    def can_handle(self, e):
        """"""
        Indicator if the handler is able to handle the given exception `e`.

        :param e: The exception that shall be determined if can be handled by the handler.
        :return: `True` or `False` depending on whether the handler can/should handle the method.
        """"""
        pass

    @abstractmethod
    def handle(self, e):
        """"""
        Handle the exception.

        :param e: The handled exception.
        :return: The error response for the exception.
        """"""
        pass","
class ErrorHandler:
    
    @abstractmethod
    def can_handle(self, e):
        
        pass
    @abstractmethod
    def handle(self, e):
        '''
        Handle the exception.
        :param e: The handled exception.
        :return: The error response for the exception.
        '''
        pass",snippet_302,3,"from abc import ABC, abstractmethod
from typing import Any


class ErrorHandler(ABC):
    @abstractmethod
    def can_handle(self, e: Exception) -> bool:
        """"""
        Determine if this handler can handle the given exception.
        :param e: The exception to check.
        :return: True if the exception can be handled by this handler, False otherwise.
        """"""
        raise NotImplementedError

    @abstractmethod
    def handle(self, e: Exception) -> Any:
        """"""
        Handle the exception.
        :param e: The handled exception.
        :return: The error response for the exception.
        """"""
        raise NotImplementedError"
361483,AtteqCom/zsl,AtteqCom_zsl/src/zsl/resource/model_resource.py,zsl.resource.model_resource.ReadOnlyResourceMixin,"class ReadOnlyResourceMixin:
    """"""
    The mixin to be used to forbid the update/delete and create operations.
    Remember the Python's MRO and place this mixin at the right place in the inheritance declaration.

    .. automethod:: create
    .. automethod:: update
    .. automethod:: delete
    """"""
    OPERATION_CREATE = 'create'
    OPERATION_UPDATE = 'update'
    OPERATION_DELETE = 'delete'

    @staticmethod
    def create(params, args, data):
        """"""Raises exception.

        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.

        :raises ReadOnlyResourceUpdateOperationException: when accessed
        """"""
        raise ReadOnlyResourceUpdateOperationException(ReadOnlyResourceMixin.OPERATION_CREATE)

    @staticmethod
    def update(params, args, data):
        """"""Raises exception.

        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.

        :raises ReadOnlyResourceUpdateOperationException: when accessed
        """"""
        raise ReadOnlyResourceUpdateOperationException(ReadOnlyResourceMixin.OPERATION_UPDATE)

    @staticmethod
    def delete(params, args, data):
        """"""Raises exception.

        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.

        :raises ReadOnlyResourceUpdateOperationException: when accessed
        """"""
        raise ReadOnlyResourceUpdateOperationException(ReadOnlyResourceMixin.OPERATION_DELETE)","class ReadOnlyResourceMixin:
    '''
    The mixin to be used to forbid the update/delete and create operations.
    Remember the Python's MRO and place this mixin at the right place in the inheritance declaration.
    .. automethod:: create
    .. automethod:: update
    .. automethod:: delete
    '''
    @staticmethod
    def create(params, args, data):
        '''Raises exception.
        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.
        :raises ReadOnlyResourceUpdateOperationException: when accessed
        '''
        pass
    @staticmethod
    def update(params, args, data):
        
        pass
    @staticmethod
    def delete(params, args, data):
        '''Raises exception.
        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.
        :raises ReadOnlyResourceUpdateOperationException: when accessed
        '''
        pass",snippet_303,4,"class ReadOnlyResourceMixin:
    '''
    The mixin to be used to forbid the update/delete and create operations.
    Remember the Python's MRO and place this mixin at the right place in the inheritance declaration.
    .. automethod:: create
    .. automethod:: update
    .. automethod:: delete
    '''
    @staticmethod
    def create(params, args, data):
        '''Raises exception.
        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.
        :raises ReadOnlyResourceUpdateOperationException: when accessed
        '''
        try:
            from .exceptions import ReadOnlyResourceUpdateOperationException  # type: ignore
        except Exception:
            class ReadOnlyResourceUpdateOperationException(Exception):  # type: ignore
                pass
        raise ReadOnlyResourceUpdateOperationException(""Read-only resource: create operation is not allowed"")

    @staticmethod
    def update(params, args, data):
        try:
            from .exceptions import ReadOnlyResourceUpdateOperationException  # type: ignore
        except Exception:
            class ReadOnlyResourceUpdateOperationException(Exception):  # type: ignore
                pass
        raise ReadOnlyResourceUpdateOperationException(""Read-only resource: update operation is not allowed"")

    @staticmethod
    def delete(params, args, data):
        '''Raises exception.
        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.
        :raises ReadOnlyResourceUpdateOperationException: when accessed
        '''
        try:
            from .exceptions import ReadOnlyResourceUpdateOperationException  # type: ignore
        except Exception:
            class ReadOnlyResourceUpdateOperationException(Exception):  # type: ignore
                pass
        raise ReadOnlyResourceUpdateOperationException(""Read-only resource: delete operation is not allowed"")"
361534,AtteqCom/zsl,AtteqCom_zsl/src/zsl/utils/command_dispatcher.py,zsl.utils.command_dispatcher.CommandDispatcher,"import inspect

class CommandDispatcher:
    """"""
    A simple class for command dictionary. A command is a function
    which can take named parameters.
    """"""

    def __init__(self):
        """"""
        Create command dictionary
        """"""
        self.commands = {}

    def command(self, fn):
        """"""
        Add method or function to dispatcher. Can be use as a nice
        decorator.

        :param fn: function or method
        :type fn: function
        :return: the same function
        :rtype: function
        """"""
        self.commands[fn.__name__] = fn
        return fn
    'alias for ``CommandDispatcher.command``'
    add_function = command

    def execute_command(self, command, args=None):
        """"""
        Execute a command

        :param command: name of the command
        :type command: str
        :param args: optional named arguments for command
        :type args: dict
        :return: the result of command
        :raises KeyError: if command is not found
        """"""
        if args is None:
            args = {}
        command_fn = self.commands[command]
        return command_fn(**args)

    def bound(self, instance):
        """"""
        Return a new dispatcher, which will switch all command functions
        with bounded methods of given instance matched by name. It will
        match only regular methods.

        :param instance: object instance
        :type instance: object
        :return: new Dispatcher
        :rtype: CommandDispatcher
        """"""
        bounded_dispatcher = CommandDispatcher()
        bounded_dispatcher.commands = self.commands.copy()
        for name in self.commands:
            method = getattr(instance, name, None)
            if method and inspect.ismethod(method) and (method.__self__ == instance):
                bounded_dispatcher.commands[name] = method
        return bounded_dispatcher","
class CommandDispatcher:
    
    def __init__(self):
        '''
        Create command dictionary
        '''
        pass
    def command(self, fn):
        '''
        Add method or function to dispatcher. Can be use as a nice
        decorator.
        :param fn: function or method
        :type fn: function
        :return: the same function
        :rtype: function
        '''
        pass
    def execute_command(self, command, args=None):
        
        pass
    def bound(self, instance):
        
        pass",snippet_304,5,"class CommandDispatcher:
    
    def __init__(self):
        '''
        Create command dictionary
        '''
        self._commands = {}

    def command(self, fn):
        '''
        Add method or function to dispatcher. Can be use as a nice
        decorator.
        :param fn: function or method
        :type fn: function
        :return: the same function
        :rtype: function
        '''
        if not callable(fn):
            raise TypeError(""Command must be callable"")
        self._commands[fn.__name__] = fn
        return fn

    def execute_command(self, command, args=None):
        if isinstance(command, str):
            try:
                fn = self._commands[command]
            except KeyError:
                raise KeyError(f""Unknown command: {command}"")
        elif callable(command):
            fn = command
        else:
            raise TypeError(""command must be a command name (str) or a callable"")

        if args is None:
            return fn()
        if isinstance(args, dict):
            return fn(**args)
        if isinstance(args, (list, tuple)):
            return fn(*args)
        return fn(args)

    def bound(self, instance):
        import types
        import inspect

        bound_dispatcher = CommandDispatcher()
        for name, fn in self._commands.items():
            bound_fn = fn
            # If it's already a bound method, keep as is
            if inspect.ismethod(fn) and fn.__self__ is not None:
                bound_fn = fn
            # If it's a plain function, bind it to the instance as a method
            elif inspect.isfunction(fn):
                bound_fn = types.MethodType(fn, instance)
            # Otherwise (callable objects, etc.), leave as is
            bound_dispatcher._commands[name] = bound_fn
        return bound_dispatcher"
362561,reanahub/reana-db,reanahub_reana-db/reana_db/models.py,reana_db.models.QuotaBase,"class QuotaBase:
    """"""Quota base functionality.""""""

    def _get_quota_by_type(self, resource_type):
        """"""Aggregate quota usage by resource type.""""""

        def _get_health_status(usage, limit):
            """"""Calculate quota health status.""""""
            health = QuotaHealth.healthy
            if limit:
                percentage = usage / limit * 100
                if percentage >= 80:
                    if percentage >= 100:
                        health = QuotaHealth.critical
                    else:
                        health = QuotaHealth.warning
            return health.name
        quota_usage = 0
        quota_limit = 0
        unit = None
        for resource in self.resources:
            if resource.resource.type_ == resource_type:
                if unit and unit != resource.resource.unit:
                    raise Exception('Error while calculating quota usage. Not all resources of resource type {} use the same units.'.format(resource_type))
                unit = resource.resource.unit
                quota_usage += resource.quota_used
                if hasattr(resource, 'quota_limit'):
                    quota_limit += resource.quota_limit
        usage_dict = {'usage': {'raw': quota_usage, 'human_readable': ResourceUnit.human_readable_unit(unit, quota_usage)}}
        if quota_limit:
            usage_dict['limit'] = {'raw': quota_limit, 'human_readable': ResourceUnit.human_readable_unit(unit, quota_limit)}
            usage_dict['health'] = _get_health_status(quota_usage, quota_limit)
        return usage_dict

    def get_quota_usage(self):
        """"""Get quota usage information.""""""
        used_resource_types = set((res.resource.type_ for res in self.resources))
        return {resource_type.name: self._get_quota_by_type(resource_type) for resource_type in used_resource_types}","class QuotaBase:
    '''Quota base functionality.'''
    def _get_quota_by_type(self, resource_type):
        '''Aggregate quota usage by resource type.'''
        pass
        def _get_health_status(usage, limit):
                
                pass
    def get_quota_usage(self):
        
        pass",snippet_305,4,"class QuotaBase:
    '''Quota base functionality.'''

    def _iter_resources(self):
        resources = []
        if hasattr(self, 'resources'):
            resources = self.resources
            if callable(resources):
                resources = resources()
        elif hasattr(self, 'get_resources') and callable(getattr(self, 'get_resources')):
            resources = self.get_resources()
        return resources or []

    def _normalize_resource(self, item):
        if isinstance(item, dict):
            rtype = item.get('type', item.get('resource_type'))
            usage = item.get('usage', 0)
            limit = item.get('limit')
        elif isinstance(item, (list, tuple)) and len(item) >= 3:
            rtype, usage, limit = item[0], item[1], item[2]
        else:
            raise ValueError('Unsupported resource item format')
        return rtype, float(usage or 0), (None if limit is None else float(limit))

    def _get_health_status(self, usage, limit):
        if limit in (None, 0):
            return 'unknown'
        ratio = usage / limit
        if ratio < 0.8:
            return 'ok'
        if ratio < 1.0:
            return 'warning'
        return 'critical'

    def _get_quota_by_type(self, resource_type):
        '''Aggregate quota usage by resource type.'''
        total_usage = 0.0
        total_limit = 0.0
        any_limit = False

        for item in self._iter_resources():
            rtype, usage, limit = self._normalize_resource(item)
            if rtype != resource_type:
                continue
            total_usage += usage
            if limit is not None:
                total_limit += limit
                any_limit = True

        limit_val = total_limit if any_limit else None
        remaining = (limit_val - total_usage) if limit_val is not None else None
        percent_used = (total_usage / limit_val) if (limit_val not in (None, 0)) else None
        status = self._get_health_status(total_usage, limit_val)

        return {
            'type': resource_type,
            'usage': total_usage,
            'limit': limit_val,
            'remaining': remaining,
            'percent_used': percent_used,
            'status': status,
        }

    def get_quota_usage(self):
        types = set()
        for item in self._iter_resources():
            rtype, _, _ = self._normalize_resource(item)
            types.add(rtype)
        return {t: self._get_quota_by_type(t) for t in sorted(types)}"
