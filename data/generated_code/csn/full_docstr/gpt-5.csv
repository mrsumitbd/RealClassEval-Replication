id,repository_name,file_path,class_name,human_written_code,class_skeleton,snippet_id,gpt-5_generated_code
17249,jbarlow83/OCRmyPDF,src/ocrmypdf/quality.py,ocrmypdf.quality.OcrQualityDictionary,"import re
from collections.abc import Iterable

class OcrQualityDictionary:
    """"""Manages a dictionary for simple OCR quality checks.""""""

    def __init__(self, *, wordlist: Iterable[str]):
        """"""Construct a dictionary from a list of words.

        Words for which capitalization is important should be capitalized in the
        dictionary. Words that contain spaces or other punctuation will never match.
        """"""
        self.dictionary = set(wordlist)

    def measure_words_matched(self, ocr_text: str) -> float:
        """"""Check how many unique words in the OCR text match a dictionary.

        Words with mixed capitalized are only considered a match if the test word
        matches that capitalization.

        Returns:
            number of words that match / number
        """"""
        text = re.sub('[0-9_]+', ' ', ocr_text)
        text = re.sub('\\W+', ' ', text)
        text_words_list = re.split('\\s+', text)
        text_words = {w for w in text_words_list if len(w) >= 3}
        matches = 0
        for w in text_words:
            if w in self.dictionary or (w != w.lower() and w.lower() in self.dictionary):
                matches += 1
        if matches > 0:
            hit_ratio = matches / len(text_words)
        else:
            hit_ratio = 0.0
        return hit_ratio","
class OcrQualityDictionary:
    '''Manages a dictionary for simple OCR quality checks.'''

    def __init__(self, *, wordlist: Iterable[str]):
        '''Construct a dictionary from a list of words.
        Words for which capitalization is important should be capitalized in the
        dictionary. Words that contain spaces or other punctuation will never match.
        '''
        pass

    def measure_words_matched(self, ocr_text: str) -> float:
        '''Check how many unique words in the OCR text match a dictionary.
        Words with mixed capitalized are only considered a match if the test word
        matches that capitalization.
        Returns:
            number of words that match / number
        '''
        pass",snippet_2,"from typing import Iterable
import re


class OcrQualityDictionary:
    '''Manages a dictionary for simple OCR quality checks.'''

    def __init__(self, *, wordlist: Iterable[str]):
        '''Construct a dictionary from a list of words.
        Words for which capitalization is important should be capitalized in the
        dictionary. Words that contain spaces or other punctuation will never match.
        '''
        self._case_insensitive = set()
        self._case_sensitive = set()
        word_re = re.compile(r'^[A-Za-z]+$')

        for w in wordlist:
            if not isinstance(w, str):
                continue
            if not word_re.match(w):
                continue
            if w.islower():
                self._case_insensitive.add(w)
            else:
                self._case_sensitive.add(w)

    def measure_words_matched(self, ocr_text: str) -> float:
        '''Check how many unique words in the OCR text match a dictionary.
        Words with mixed capitalized are only considered a match if the test word
        matches that capitalization.
        Returns:
            number of words that match / number
        '''
        if not isinstance(ocr_text, str):
            return 0.0

        tokens = set(re.findall(r'[A-Za-z]+', ocr_text))
        if not tokens:
            return 0.0

        matched = 0
        for tok in tokens:
            if tok in self._case_sensitive:
                matched += 1
            elif tok.lower() in self._case_insensitive:
                matched += 1

        return matched / len(tokens)"
19627,slundberg/shap,slundberg_shap/shap/_serializable.py,shap._serializable.Serializable,"import inspect
import pickle

class Serializable:
    """"""This is the superclass of all serializable objects.""""""

    def save(self, out_file):
        """"""Save the model to the given file stream.""""""
        pickle.dump(type(self), out_file)

    @classmethod
    def load(cls, in_file, instantiate=True):
        """"""This is meant to be overridden by subclasses and called with super.

        We return constructor argument values when not being instantiated. Since there are no
        constructor arguments for the Serializable class we just return an empty dictionary.
        """"""
        if instantiate:
            return cls._instantiated_load(in_file)
        return {}

    @classmethod
    def _instantiated_load(cls, in_file, **kwargs):
        """"""This is meant to be overridden by subclasses and called with super.

        We return constructor argument values (we have no values to load in this abstract class).
        """"""
        obj_type = pickle.load(in_file)
        if obj_type is None:
            return None
        if not inspect.isclass(obj_type) or (not issubclass(obj_type, cls) and obj_type is not cls):
            raise Exception(f'Invalid object type loaded from file. {obj_type} is not a subclass of {cls}.')
        constructor_args = obj_type.load(in_file, instantiate=False, **kwargs)
        used_args = inspect.getfullargspec(obj_type.__init__)[0]
        return obj_type(**{k: constructor_args[k] for k in constructor_args if k in used_args})","
class Serializable:
    '''This is the superclass of all serializable objects.'''

    def save(self, out_file):
        '''Save the model to the given file stream.'''
        pass
    @classmethod
    def load(cls, in_file, instantiate=True):
        '''This is meant to be overridden by subclasses and called with super.
        We return constructor argument values when not being instantiated. Since there are no
        constructor arguments for the Serializable class we just return an empty dictionary.
        '''
        pass
    @classmethod
    def _instantiated_load(cls, in_file, **kwargs):
        '''This is meant to be overridden by subclasses and called with super.
        We return constructor argument values (we have no values to load in this abstract class).
        '''
        pass",snippet_3,"class Serializable:
    '''This is the superclass of all serializable objects.'''

    def save(self, out_file):
        '''Save the model to the given file stream.'''
        # Base class has nothing to save.
        return None

    @classmethod
    def load(cls, in_file, instantiate=True):
        '''This is meant to be overridden by subclasses and called with super.
        We return constructor argument values when not being instantiated. Since there are no
        constructor arguments for the Serializable class we just return an empty dictionary.
        '''
        if not instantiate:
            return {}
        kwargs = cls._instantiated_load(in_file)
        if kwargs is None:
            kwargs = {}
        return cls(**kwargs)

    @classmethod
    def _instantiated_load(cls, in_file, **kwargs):
        '''This is meant to be overridden by subclasses and called with super.
        We return constructor argument values (we have no values to load in this abstract class).
        '''
        return {}"
19679,slundberg/shap,slundberg_shap/shap/maskers/_text.py,shap.maskers._text.SimpleTokenizer,"import re

class SimpleTokenizer:
    """"""A basic model agnostic tokenizer.""""""

    def __init__(self, split_pattern='\\W+'):
        """"""Create a tokenizer based on a simple splitting pattern.""""""
        self.split_pattern = re.compile(split_pattern)

    def __call__(self, s, return_offsets_mapping=True):
        """"""Tokenize the passed string, optionally returning the offsets of each token in the original string.""""""
        pos = 0
        offset_ranges = []
        input_ids = []
        for m in re.finditer(self.split_pattern, s):
            start, end = m.span(0)
            offset_ranges.append((pos, start))
            input_ids.append(s[pos:start])
            pos = end
        if pos != len(s):
            offset_ranges.append((pos, len(s)))
            input_ids.append(s[pos:])
        out = {}
        out['input_ids'] = input_ids
        if return_offsets_mapping:
            out['offset_mapping'] = offset_ranges
        return out",,snippet_4,"I don’t have the class skeleton. Please provide:
- Class name and purpose
- Method signatures (names, params, return types)
- Attributes/properties and their types/defaults
- Expected behavior and edge cases
- Python version and any dependencies
- Sync vs async, threading requirements
- Any constraints (performance, immutability, dataclass usage)
- Sample inputs/outputs or tests

With that, I’ll implement the complete class."
19720,audreyr/cookiecutter,audreyr_cookiecutter/cookiecutter/environment.py,cookiecutter.environment.ExtensionLoaderMixin,"from cookiecutter.exceptions import UnknownExtension
from typing import Any

class ExtensionLoaderMixin:
    """"""Mixin providing sane loading of extensions specified in a given context.

    The context is being extracted from the keyword arguments before calling
    the next parent class in line of the child.
    """"""

    def __init__(self, *, context: dict[str, Any] | None=None, **kwargs: Any) -> None:
        """"""Initialize the Jinja2 Environment object while loading extensions.

        Does the following:

        1. Establishes default_extensions (currently just a Time feature)
        2. Reads extensions set in the cookiecutter.json _extensions key.
        3. Attempts to load the extensions. Provides useful error if fails.
        """"""
        context = context or {}
        default_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']
        extensions = default_extensions + self._read_extensions(context)
        try:
            super().__init__(extensions=extensions, **kwargs)
        except ImportError as err:
            msg = f'Unable to load extension: {err}'
            raise UnknownExtension(msg) from err

    def _read_extensions(self, context: dict[str, Any]) -> list[str]:
        """"""Return list of extensions as str to be passed on to the Jinja2 env.

        If context does not contain the relevant info, return an empty
        list instead.
        """"""
        try:
            extensions = context['cookiecutter']['_extensions']
        except KeyError:
            return []
        else:
            return [str(ext) for ext in extensions]","
class ExtensionLoaderMixin:
    '''Mixin providing sane loading of extensions specified in a given context.
    The context is being extracted from the keyword arguments before calling
    the next parent class in line of the child.
        '''

    def __init__(self, *, context: dict[str, Any] | None=None, **kwargs: Any) -> None:
        '''Initialize the Jinja2 Environment object while loading extensions.
        Does the following:
        1. Establishes default_extensions (currently just a Time feature)
        2. Reads extensions set in the cookiecutter.json _extensions key.
        3. Attempts to load the extensions. Provides useful error if fails.
        '''
        pass

    def _read_extensions(self, context: dict[str, Any]) -> list[str]:
        '''Return list of extensions as str to be passed on to the Jinja2 env.
        If context does not contain the relevant info, return an empty
        list instead.
        '''
        pass",snippet_5,"from __future__ import annotations

import importlib
from typing import Any


class ExtensionLoaderMixin:
    '''Mixin providing sane loading of extensions specified in a given context.
    The context is being extracted from the keyword arguments before calling
    the next parent class in line of the child.
        '''

    def __init__(self, *, context: dict[str, Any] | None = None, **kwargs: Any) -> None:
        '''Initialize the Jinja2 Environment object while loading extensions.
        Does the following:
        1. Establishes default_extensions (currently just a Time feature)
        2. Reads extensions set in the cookiecutter.json _extensions key.
        3. Attempts to load the extensions. Provides useful error if fails.
        '''
        default_extensions: list[str] = [""jinja2_time.TimeExtension""]

        ctx_extensions = self._read_extensions(context or {})

        existing = kwargs.get(""extensions"", [])
        if existing is None:
            existing = []
        if not isinstance(existing, (list, tuple)):
            raise TypeError(""extensions must be a list or tuple if provided in kwargs"")

        combined: list[str] = []
        for ext in list(default_extensions) + list(ctx_extensions) + list(existing):
            if isinstance(ext, str):
                if ext not in combined:
                    combined.append(ext)
            else:
                raise TypeError(f""Extension identifiers must be strings, got {type(ext).__name__}"")

        load_errors: list[str] = []
        for ext in combined:
            try:
                module_name, _, attr = ext.rpartition(""."")
                if not module_name or not attr:
                    raise ImportError(f""Invalid extension path '{ext}'. Expected 'package.module.ClassName'."")
                module = importlib.import_module(module_name)
                getattr(module, attr)
            except Exception as exc:
                load_errors.append(f""{ext}: {exc}"")

        if load_errors:
            msg = ""Could not load Jinja2 extensions:\n- "" + ""\n- "".join(load_errors)
            raise RuntimeError(msg)

        kwargs[""extensions""] = combined
        super().__init__(**kwargs)

    def _read_extensions(self, context: dict[str, Any]) -> list[str]:
        '''Return list of extensions as str to be passed on to the Jinja2 env.
        If context does not contain the relevant info, return an empty
        list instead.
        '''
        # Try at top-level
        value = context.get(""_extensions"")
        # Or nested under 'cookiecutter' key (common in Cookiecutter context)
        if value is None and isinstance(context.get(""cookiecutter""), dict):
            value = context[""cookiecutter""].get(""_extensions"")

        if value is None:
            return []

        if isinstance(value, (list, tuple)):
            result: list[str] = []
            for item in value:
                if isinstance(item, str):
                    result.append(item)
                else:
                    # Coerce to string representation to avoid crashing on non-strings
                    result.append(str(item))
            return result

        # If malformed, ignore and return empty to be lenient
        return []"
21340,Delgan/loguru,loguru/_recattrs.py,loguru._recattrs.RecordFile,"class RecordFile:
    """"""A class representing a file record with name and path.

    Attributes
    ----------
    name : str
        The name of the file
    path : str
        The path to the file
    """"""
    __slots__ = ('name', 'path')

    def __init__(self, name, path):
        """"""Initialize a RecordFile instance.

        Parameters
        ----------
        name : str
            The name of the file
        path : str
            The path to the file
        """"""
        self.name = name
        self.path = path

    def __repr__(self):
        """"""Return string representation of RecordFile.

        Returns
        -------
        str
            Formatted string with name and path
        """"""
        return '(name=%r, path=%r)' % (self.name, self.path)

    def __format__(self, spec):
        """"""Format the RecordFile instance.

        Parameters
        ----------
        spec : str
            Format specification

        Returns
        -------
        str
            Formatted name according to specification
        """"""
        return self.name.__format__(spec)","class RecordFile:
    '''A class representing a file record with name and path.
    Attributes
    ----------
    name : str
        The name of the file
    path : str
        The path to the file
    '''

    def __init__(self, name, path):
        '''Initialize a RecordFile instance.
        Parameters
        ----------
        name : str
            The name of the file
        path : str
            The path to the file
        '''
        pass

    def __repr__(self):
        '''Return string representation of RecordFile.
        Returns
        -------
        str
            Formatted string with name and path
        '''
        pass

    def __format__(self, spec):
        '''Format the RecordFile instance.
        Parameters
        ----------
        spec : str
            Format specification
        Returns
        -------
        str
            Formatted name according to specification
        '''
        pass",snippet_6,"class RecordFile:
    '''A class representing a file record with name and path.
    Attributes
    ----------
    name : str
        The name of the file
    path : str
        The path to the file
    '''

    def __init__(self, name, path):
        '''Initialize a RecordFile instance.
        Parameters
        ----------
        name : str
            The name of the file
        path : str
            The path to the file
        '''
        if not isinstance(name, str):
            raise TypeError(""name must be a str"")
        if not isinstance(path, str):
            raise TypeError(""path must be a str"")
        self.name = name
        self.path = path

    def __repr__(self):
        '''Return string representation of RecordFile.
        Returns
        -------
        str
            Formatted string with name and path
        '''
        return f""{self.__class__.__name__}(name={self.name!r}, path={self.path!r})""

    def __format__(self, spec):
        '''Format the RecordFile instance.
        Parameters
        ----------
        spec : str
            Format specification
        Returns
        -------
        str
            Formatted name according to specification
        '''
        if not isinstance(spec, str):
            raise TypeError(""spec must be a str"")
        return format(self.name, spec)"
21341,Delgan/loguru,loguru/_recattrs.py,loguru._recattrs.RecordLevel,"class RecordLevel:
    """"""A class representing the logging level record with name, number and icon.

    Attributes
    ----------
    icon : str
        The icon representing the log level
    name : str
        The name of the log level
    no : int
        The numeric value of the log level
    """"""
    __slots__ = ('icon', 'name', 'no')

    def __init__(self, name, no, icon):
        """"""Initialize a RecordLevel instance.

        Parameters
        ----------
        name : str
            The name of the log level
        no : int
            The numeric value of the log level
        icon : str
            The icon representing the log level
        """"""
        self.name = name
        self.no = no
        self.icon = icon

    def __repr__(self):
        """"""Return string representation of RecordLevel.

        Returns
        -------
        str
            Formatted string with name, number and icon
        """"""
        return '(name=%r, no=%r, icon=%r)' % (self.name, self.no, self.icon)

    def __format__(self, spec):
        """"""Format the RecordLevel instance.

        Parameters
        ----------
        spec : str
            Format specification

        Returns
        -------
        str
            Formatted name according to specification
        """"""
        return self.name.__format__(spec)","class RecordLevel:
    '''A class representing the logging level record with name, number and icon.
    Attributes
    ----------
    icon : str
        The icon representing the log level
    name : str
        The name of the log level
    no : int
        The numeric value of the log level
    '''

    def __init__(self, name, no, icon):
        '''Initialize a RecordLevel instance.
        Parameters
        ----------
        name : str
            The name of the log level
        no : int
            The numeric value of the log level
        icon : str
            The icon representing the log level
        '''
        pass

    def __repr__(self):
        '''Return string representation of RecordLevel.
        Returns
        -------
        str
            Formatted string with name, number and icon
        '''
        pass

    def __format__(self, spec):
        '''Format the RecordLevel instance.
        Parameters
        ----------
        spec : str
            Format specification
        Returns
        -------
        str
            Formatted name according to specification
        '''
        pass",snippet_7,"class RecordLevel:
    '''A class representing the logging level record with name, number and icon.
    Attributes
    ----------
    icon : str
        The icon representing the log level
    name : str
        The name of the log level
    no : int
        The numeric value of the log level
    '''

    def __init__(self, name, no, icon):
        '''Initialize a RecordLevel instance.
        Parameters
        ----------
        name : str
            The name of the log level
        no : int
            The numeric value of the log level
        icon : str
            The icon representing the log level
        '''
        if not isinstance(name, str):
            raise TypeError(""name must be a str"")
        if not isinstance(no, int):
            raise TypeError(""no must be an int"")
        if not isinstance(icon, str):
            raise TypeError(""icon must be a str"")
        self.name = name
        self.no = no
        self.icon = icon

    def __repr__(self):
        '''Return string representation of RecordLevel.
        Returns
        -------
        str
            Formatted string with name, number and icon
        '''
        return f""RecordLevel(name={self.name!r}, no={self.no}, icon={self.icon!r})""

    def __format__(self, spec):
        '''Format the RecordLevel instance.
        Parameters
        ----------
        spec : str
            Format specification
        Returns
        -------
        str
            Formatted name according to specification
        '''
        if spec is None or spec == """":
            return self.name
        return format(self.name, spec)"
21342,Delgan/loguru,loguru/_recattrs.py,loguru._recattrs.RecordProcess,"class RecordProcess:
    """"""A class representing a process record with ID and name.

    Attributes
    ----------
    id : int
        The process ID
    name : str
        The process name
    """"""
    __slots__ = ('id', 'name')

    def __init__(self, id_, name):
        """"""Initialize a RecordProcess instance.

        Parameters
        ----------
        id_ : int
            The process ID
        name : str
            The process name
        """"""
        self.id = id_
        self.name = name

    def __repr__(self):
        """"""Return string representation of RecordProcess.

        Returns
        -------
        str
            Formatted string with id and name
        """"""
        return '(id=%r, name=%r)' % (self.id, self.name)

    def __format__(self, spec):
        """"""Format the RecordProcess instance.

        Parameters
        ----------
        spec : str
            Format specification

        Returns
        -------
        str
            Formatted ID according to specification
        """"""
        return self.id.__format__(spec)","class RecordProcess:
    '''A class representing a process record with ID and name.
    Attributes
    ----------
    id : int
        The process ID
    name : str
        The process name
    '''

    def __init__(self, id_, name):
        '''Initialize a RecordProcess instance.
        Parameters
        ----------
        id_ : int
            The process ID
        name : str
            The process name
        '''
        pass

    def __repr__(self):
        '''Return string representation of RecordProcess.
        Returns
        -------
        str
            Formatted string with id and name
        '''
        pass

    def __format__(self, spec):
        '''Format the RecordProcess instance.
        Parameters
        ----------
        spec : str
            Format specification
        Returns
        -------
        str
            Formatted ID according to specification
        '''
        pass",snippet_8,"class RecordProcess:
    '''A class representing a process record with ID and name.
    Attributes
    ----------
    id : int
        The process ID
    name : str
        The process name
    '''

    def __init__(self, id_, name):
        '''Initialize a RecordProcess instance.
        Parameters
        ----------
        id_ : int
            The process ID
        name : str
            The process name
        '''
        if not isinstance(id_, int) or isinstance(id_, bool):
            raise TypeError(""id_ must be an integer"")
        if not isinstance(name, str):
            raise TypeError(""name must be a string"")
        self.id = id_
        self.name = name

    def __repr__(self):
        '''Return string representation of RecordProcess.
        Returns
        -------
        str
            Formatted string with id and name
        '''
        return f""RecordProcess(id={self.id}, name={self.name!r})""

    def __format__(self, spec):
        '''Format the RecordProcess instance.
        Parameters
        ----------
        spec : str
            Format specification
        Returns
        -------
        str
            Formatted ID according to specification
        '''
        if spec is None:
            spec = """"
        return format(self.id, spec)"
21343,Delgan/loguru,loguru/_recattrs.py,loguru._recattrs.RecordThread,"class RecordThread:
    """"""A class representing a thread record with ID and name.

    Attributes
    ----------
    id : int
        The thread ID
    name : str
        The thread name
    """"""
    __slots__ = ('id', 'name')

    def __init__(self, id_, name):
        """"""Initialize a RecordThread instance.

        Parameters
        ----------
        id_ : int
            The thread ID
        name : str
            The thread name
        """"""
        self.id = id_
        self.name = name

    def __repr__(self):
        """"""Return string representation of RecordThread.

        Returns
        -------
        str
            Formatted string with id and name
        """"""
        return '(id=%r, name=%r)' % (self.id, self.name)

    def __format__(self, spec):
        """"""Format the RecordThread instance.

        Parameters
        ----------
        spec : str
            Format specification

        Returns
        -------
        str
            Formatted ID according to specification
        """"""
        return self.id.__format__(spec)","class RecordThread:
    '''A class representing a thread record with ID and name.
    Attributes
    ----------
    id : int
        The thread ID
    name : str
        The thread name
    '''

    def __init__(self, id_, name):
        '''Initialize a RecordThread instance.
        Parameters
        ----------
        id_ : int
            The thread ID
        name : str
            The thread name
        '''
        pass

    def __repr__(self):
        '''Return string representation of RecordThread.
        Returns
        -------
        str
            Formatted string with id and name
        '''
        pass

    def __format__(self, spec):
        '''Format the RecordThread instance.
        Parameters
        ----------
        spec : str
            Format specification
        Returns
        -------
        str
            Formatted ID according to specification
        '''
        pass",snippet_9,"class RecordThread:
    '''A class representing a thread record with ID and name.
    Attributes
    ----------
    id : int
        The thread ID
    name : str
        The thread name
    '''

    def __init__(self, id_, name):
        '''Initialize a RecordThread instance.
        Parameters
        ----------
        id_ : int
            The thread ID
        name : str
            The thread name
        '''
        self.id = int(id_)
        self.name = str(name)

    def __repr__(self):
        '''Return string representation of RecordThread.
        Returns
        -------
        str
            Formatted string with id and name
        '''
        return f""RecordThread(id={self.id!r}, name={self.name!r})""

    def __format__(self, spec):
        '''Format the RecordThread instance.
        Parameters
        ----------
        spec : str
            Format specification
        Returns
        -------
        str
            Formatted ID according to specification
        '''
        if spec is None:
            spec = """"
        return format(self.id, spec)"
33971,Miserlou/Zappa,Miserlou_Zappa/zappa/asynchronous.py,zappa.asynchronous.LambdaAsyncResponse,"import json
import uuid

class LambdaAsyncResponse:
    """"""
    Base Response Dispatcher class
    Can be used directly or subclassed if the method to send the message is changed.
    """"""

    def __init__(self, lambda_function_name=None, aws_region=None, capture_response=False, **kwargs):
        """""" """"""
        if kwargs.get('boto_session'):
            self.client = kwargs.get('boto_session').client('lambda')
        else:
            self.client = LAMBDA_CLIENT
        self.lambda_function_name = lambda_function_name
        self.aws_region = aws_region
        if capture_response:
            if ASYNC_RESPONSE_TABLE is None:
                print(""Warning! Attempted to capture a response without async_response_table configured in settings (you won't capture async responses)."")
                capture_response = False
                self.response_id = 'MISCONFIGURED'
            else:
                self.response_id = str(uuid.uuid4())
        else:
            self.response_id = None
        self.capture_response = capture_response

    def send(self, task_path, args, kwargs):
        """"""
        Create the message object and pass it to the actual sender.
        """"""
        message = {'task_path': task_path, 'capture_response': self.capture_response, 'response_id': self.response_id, 'args': args, 'kwargs': kwargs}
        self._send(message)
        return self

    def _send(self, message):
        """"""
        Given a message, directly invoke the lamdba function for this task.
        """"""
        message['command'] = 'zappa.asynchronous.route_lambda_task'
        payload = json.dumps(message).encode('utf-8')
        if len(payload) > LAMBDA_ASYNC_PAYLOAD_LIMIT:
            raise AsyncException('Payload too large for async Lambda call')
        self.response = self.client.invoke(FunctionName=self.lambda_function_name, InvocationType='Event', Payload=payload)
        self.sent = self.response.get('StatusCode', 0) == 202","
class LambdaAsyncResponse:
    '''
    Base Response Dispatcher class
    Can be used directly or subclassed if the method to send the message is changed.
        '''

    def __init__(self, lambda_function_name=None, aws_region=None, capture_response=False, **kwargs):
        ''' '''
        pass

    def send(self, task_path, args, kwargs):
        '''
        Create the message object and pass it to the actual sender.
        '''
        pass

    def _send(self, message):
        '''
        Given a message, directly invoke the lamdba function for this task.
        '''
        pass",snippet_10,"import os
import json
from typing import Any, Dict, Optional

try:
    import boto3
    from botocore.exceptions import BotoCoreError, ClientError
except Exception:  # pragma: no cover
    boto3 = None
    BotoCoreError = Exception
    ClientError = Exception


class LambdaAsyncResponse:
    '''
    Base Response Dispatcher class
    Can be used directly or subclassed if the method to send the message is changed.
    '''

    def __init__(self, lambda_function_name: Optional[str] = None, aws_region: Optional[str] = None, capture_response: bool = False, **kwargs):
        '''
        Initialize the dispatcher with Lambda function details.

        - lambda_function_name: Name or ARN of the target Lambda function.
        - aws_region: AWS region to use. Falls back to AWS_REGION or AWS_DEFAULT_REGION env vars.
        - capture_response: If True, waits for and returns the Lambda response payload. Otherwise invokes asynchronously.
        - kwargs: Additional boto3.client('lambda', ...) keyword args.
        '''
        self.lambda_function_name = lambda_function_name or os.getenv(""LAMBDA_FUNCTION_NAME"")
        if not self.lambda_function_name:
            raise ValueError(""Lambda function name must be provided via parameter or LAMBDA_FUNCTION_NAME environment variable."")

        self.aws_region = aws_region or os.getenv(""AWS_REGION"") or os.getenv(""AWS_DEFAULT_REGION"")
        if not self.aws_region:
            raise ValueError(""AWS region must be provided via parameter or AWS_REGION/AWS_DEFAULT_REGION environment variable."")

        self.capture_response = bool(capture_response)

        if boto3 is None:
            raise RuntimeError(""boto3 is required to use LambdaAsyncResponse but is not installed."")

        # Allow passing through additional boto3 client args via kwargs (e.g., endpoint_url, config, credentials)
        self._client = boto3.client(""lambda"", region_name=self.aws_region, **kwargs)

    def send(self, task_path: str, args: Any, kwargs: Dict[str, Any]):
        '''
        Create the message object and pass it to the actual sender.
        '''
        message = {
            ""task_path"": task_path,
            ""args"": args,
            ""kwargs"": kwargs or {},
        }
        return self._send(message)

    def _send(self, message: Dict[str, Any]):
        '''
        Given a message, directly invoke the lamdba function for this task.
        '''
        try:
            payload_bytes = json.dumps(message, default=str).encode(""utf-8"")
        except (TypeError, ValueError) as exc:
            raise ValueError(f""Failed to serialize message to JSON: {exc}"") from exc

        invocation_type = ""RequestResponse"" if self.capture_response else ""Event""

        try:
            response = self._client.invoke(
                FunctionName=self.lambda_function_name,
                InvocationType=invocation_type,
                Payload=payload_bytes,
            )
        except (BotoCoreError, ClientError) as exc:
            raise RuntimeError(f""Error invoking Lambda function '{self.lambda_function_name}': {exc}"") from exc

        if self.capture_response:
            # Return parsed JSON payload if possible; otherwise return raw string.
            payload_stream = response.get(""Payload"")
            if payload_stream is None:
                return None
            raw = payload_stream.read()
            if not raw:
                return None
            try:
                return json.loads(raw.decode(""utf-8""))
            except Exception:
                try:
                    return raw.decode(""utf-8"")
                except Exception:
                    return raw

        # For async invocation, return minimal metadata
        return {
            ""StatusCode"": response.get(""StatusCode""),
            ""RequestId"": response.get(""ResponseMetadata"", {}).get(""RequestId""),
        }"
44118,LonamiWebs/Telethon,LonamiWebs_Telethon/telethon/crypto/aesctr.py,telethon.crypto.aesctr.AESModeCTR,"import pyaes

class AESModeCTR:
    """"""Wrapper around pyaes.AESModeOfOperationCTR mode with custom IV""""""

    def __init__(self, key, iv):
        """"""
        Initializes the AES CTR mode with the given key/iv pair.

        :param key: the key to be used as bytes.
        :param iv: the bytes initialization vector. Must have a length of 16.
        """"""
        assert isinstance(key, bytes)
        self._aes = pyaes.AESModeOfOperationCTR(key)
        assert isinstance(iv, bytes)
        assert len(iv) == 16
        self._aes._counter._counter = list(iv)

    def encrypt(self, data):
        """"""
        Encrypts the given plain text through AES CTR.

        :param data: the plain text to be encrypted.
        :return: the encrypted cipher text.
        """"""
        return self._aes.encrypt(data)

    def decrypt(self, data):
        """"""
        Decrypts the given cipher text through AES CTR

        :param data: the cipher text to be decrypted.
        :return: the decrypted plain text.
        """"""
        return self._aes.decrypt(data)","
class AESModeCTR:
    '''Wrapper around pyaes.AESModeOfOperationCTR mode with custom IV'''

    def __init__(self, key, iv):
        '''
        Initializes the AES CTR mode with the given key/iv pair.
        :param key: the key to be used as bytes.
        :param iv: the bytes initialization vector. Must have a length of 16.
        '''
        pass

    def encrypt(self, data):
        '''
        Encrypts the given plain text through AES CTR.
        :param data: the plain text to be encrypted.
        :return: the encrypted cipher text.
        '''
        pass

    def decrypt(self, data):
        '''
        Decrypts the given cipher text through AES CTR
        :param data: the cipher text to be decrypted.
        :return: the decrypted plain text.
        '''
        pass",snippet_11,"import pyaes


class AESModeCTR:
    '''Wrapper around pyaes.AESModeOfOperationCTR mode with custom IV'''

    def __init__(self, key, iv):
        '''
        Initializes the AES CTR mode with the given key/iv pair.
        :param key: the key to be used as bytes.
        :param iv: the bytes initialization vector. Must have a length of 16.
        '''
        if not isinstance(key, (bytes, bytearray, memoryview)):
            raise TypeError(""key must be bytes-like"")
        if not isinstance(iv, (bytes, bytearray, memoryview)):
            raise TypeError(""iv must be bytes-like"")
        key = bytes(key)
        iv = bytes(iv)
        if len(iv) != 16:
            raise ValueError(""iv must be 16 bytes long"")
        if len(key) not in (16, 24, 32):
            raise ValueError(""key must be 16, 24, or 32 bytes long"")
        self.key = key
        self.iv = iv

    def _new_cipher(self):
        counter = pyaes.Counter(int.from_bytes(self.iv, byteorder=""big""))
        return pyaes.AESModeOfOperationCTR(self.key, counter=counter)

    def encrypt(self, data):
        '''
        Encrypts the given plain text through AES CTR.
        :param data: the plain text to be encrypted.
        :return: the encrypted cipher text.
        '''
        if not isinstance(data, (bytes, bytearray, memoryview)):
            raise TypeError(""data must be bytes-like"")
        cipher = self._new_cipher()
        return cipher.encrypt(bytes(data))

    def decrypt(self, data):
        '''
        Decrypts the given cipher text through AES CTR
        :param data: the cipher text to be decrypted.
        :return: the decrypted plain text.
        '''
        if not isinstance(data, (bytes, bytearray, memoryview)):
            raise TypeError(""data must be bytes-like"")
        cipher = self._new_cipher()
        return cipher.decrypt(bytes(data))"
44121,LonamiWebs/Telethon,LonamiWebs_Telethon/telethon/crypto/factorization.py,telethon.crypto.factorization.Factorization,"from random import randint

class Factorization:
    """"""
    Simple module to factorize large numbers really quickly.
    """"""

    @classmethod
    def factorize(cls, pq):
        """"""
        Factorizes the given large integer.

        Implementation from https://comeoncodeon.wordpress.com/2010/09/18/pollard-rho-brent-integer-factorization/.

        :param pq: the prime pair pq.
        :return: a tuple containing the two factors p and q.
        """"""
        if pq % 2 == 0:
            return (2, pq // 2)
        y, c, m = (randint(1, pq - 1), randint(1, pq - 1), randint(1, pq - 1))
        g = r = q = 1
        x = ys = 0
        while g == 1:
            x = y
            for i in range(r):
                y = (pow(y, 2, pq) + c) % pq
            k = 0
            while k < r and g == 1:
                ys = y
                for i in range(min(m, r - k)):
                    y = (pow(y, 2, pq) + c) % pq
                    q = q * abs(x - y) % pq
                g = cls.gcd(q, pq)
                k += m
            r *= 2
        if g == pq:
            while True:
                ys = (pow(ys, 2, pq) + c) % pq
                g = cls.gcd(abs(x - ys), pq)
                if g > 1:
                    break
        p, q = (g, pq // g)
        return (p, q) if p < q else (q, p)

    @staticmethod
    def gcd(a, b):
        """"""
        Calculates the Greatest Common Divisor.

        :param a: the first number.
        :param b: the second number.
        :return: GCD(a, b)
        """"""
        while b:
            a, b = (b, a % b)
        return a","
class Factorization:
    '''
    Simple module to factorize large numbers really quickly.
    '''
    @classmethod
    def factorize(cls, pq):
        '''
        Factorizes the given large integer.
        Implementation from https://comeoncodeon.wordpress.com/2010/09/18/pollard-rho-brent-integer-factorization/.
        :param pq: the prime pair pq.
        :return: a tuple containing the two factors p and q.
        '''
        pass
    @staticmethod
    def gcd(a, b):
        '''
        Calculates the Greatest Common Divisor.
        :param a: the first number.
        :param b: the second number.
        :return: GCD(a, b)
        '''
        pass",snippet_12,"class Factorization:
    '''
    Simple module to factorize large numbers really quickly.
    '''
    @classmethod
    def factorize(cls, pq):
        '''
        Factorizes the given large integer.
        Implementation from https://comeoncodeon.wordpress.com/2010/09/18/pollard-rho-brent-integer-factorization/.
        :param pq: the prime pair pq.
        :return: a tuple containing the two factors p and q.
        '''
        n = int(pq)
        if n <= 1:
            return (n, 1)
        if n % 2 == 0:
            return (2, n // 2)

        import random

        def brent(N):
            if N % 2 == 0:
                return 2
            if N % 3 == 0:
                return 3
            # Random parameters for the polynomial x^2 + c
            y = random.randrange(1, N)
            c = random.randrange(1, N)
            m = random.randrange(1, N)
            g = 1
            r = 1
            q = 1
            while g == 1:
                x = y
                for _ in range(r):
                    y = (y * y + c) % N
                k = 0
                while k < r and g == 1:
                    ys = y
                    for _ in range(min(m, r - k)):
                        y = (y * y + c) % N
                        q = (q * abs(x - y)) % N
                    g = cls.gcd(q, N)
                    k += m
                r <<= 1
            if g == N:
                while True:
                    ys = (ys * ys + c) % N
                    g = cls.gcd(abs(x - ys), N)
                    if g > 1:
                        break
            return g

        # Keep trying until a non-trivial factor is found
        while True:
            g = brent(n)
            if 1 < g < n:
                p = g
                q = n // g
                return (p, q)

    @staticmethod
    def gcd(a, b):
        '''
        Calculates the Greatest Common Divisor.
        :param a: the first number.
        :param b: the second number.
        :return: GCD(a, b)
        '''
        a = abs(int(a))
        b = abs(int(b))
        while b:
            a, b = b, a % b
        return a"
49235,blue-yonder/tsfresh,blue-yonder_tsfresh/tsfresh/examples/driftbif_simulation.py,tsfresh.examples.driftbif_simulation.velocity,"import numpy as np

class velocity:
    """"""
    Simulates the velocity of a dissipative soliton (kind of self organized particle) [6]_.
    The equilibrium velocity without noise R=0 for
    $	au>1.0/\\kappa_3$ is $\\kappa_3 \\sqrt{(tau - 1.0/\\kappa_3)/Q}.
    Before the drift-bifurcation $	au \\le 1.0/\\kappa_3$ the velocity is zero.

    References
    ----------

    .. [6] Andreas Kempa-Liehr (2013, p. 159-170)
        Dynamics of Dissipative Soliton
        Dissipative Solitons in Reaction Diffusion Systems.
        Springer: Berlin


    >>> ds = velocity(tau=3.5) # Dissipative soliton with equilibrium velocity 1.5e-3
    >>> print(ds.label) # Discriminating before or beyond Drift-Bifurcation
    1

    # Equilibrium velocity
    >>> print(ds.deterministic)
    0.0015191090506254991

    # Simulated velocity as a time series with 20000 time steps being disturbed by Gaussian white noise
    >>> v = ds.simulate(20000)
    """"""

    def __init__(self, tau=3.8, kappa_3=0.3, Q=1950.0, R=0.0003, delta_t=0.05, seed=None):
        """"""
        :param tau: Bifurcation parameter determining the intrinsic velocity of the dissipative soliton,
                    which is zero for tau<=1.0/kappa_3 and np.sqrt(kappa_3**3/Q * (tau - 1.0/kappa_3)) otherwise
        :type tau: float
        :param kappa_3: Inverse bifurcation point.
        :type kappa_3:
        :param Q: Shape parameter of dissipative soliton
        :type Q: float
        :param R: Noise amplitude
        :type R: float
        :param delta_t: temporal discretization
        :type delta_t: float
        """"""
        self.delta_t = delta_t
        self.kappa_3 = kappa_3
        self.Q = Q
        self.tau = tau
        self.a = self.delta_t * kappa_3 ** 2 * (tau - 1.0 / kappa_3)
        self.b = self.delta_t * Q / kappa_3
        self.label = int(tau > 1.0 / kappa_3)
        self.c = np.sqrt(self.delta_t) * R
        self.delta_t = self.delta_t
        if seed is not None:
            np.random.seed(seed)
        if tau <= 1.0 / kappa_3:
            self.deterministic = 0.0
        else:
            self.deterministic = kappa_3 ** 1.5 * np.sqrt((tau - 1.0 / kappa_3) / Q)

    def __call__(self, v):
        """"""
        returns deterministic dynamic = acceleration (without noise)

        :param v: initial velocity vector
        :rtype v: ndarray
        :return: velocity vector of next time step
        :return type: ndarray
        """"""
        return v * (1.0 + self.a - self.b * np.dot(v, v))

    def simulate(self, N, v0=np.zeros(2)):
        """"""

        :param N: number of time steps
        :type N: int
        :param v0: initial velocity vector
        :type v0: ndarray
        :return: time series of velocity vectors with shape (N, v0.shape[0])
        :rtype: ndarray
        """"""
        v = [v0]
        n = N - 1
        gamma = np.random.randn(n, v0.size)
        for i in range(n):
            next_v = self.__call__(v[i]) + self.c * gamma[i]
            v.append(next_v)
        v_vec = np.array(v)
        return v_vec","
class velocity:
    '''
    Simulates the velocity of a dissipative soliton (kind of self organized particle) [6]_.
    The equilibrium velocity without noise R=0 for
    $    au>1.0/\kappa_3$ is $\kappa_3 \sqrt{(tau - 1.0/\kappa_3)/Q}.
    Before the drift-bifurcation $    au \le 1.0/\kappa_3$ the velocity is zero.
    References
    ----------
    .. [6] Andreas Kempa-Liehr (2013, p. 159-170)
        Dynamics of Dissipative Soliton
        Dissipative Solitons in Reaction Diffusion Systems.
        Springer: Berlin

    >>> ds = velocity(tau=3.5) # Dissipative soliton with equilibrium velocity 1.5e-3
    >>> print(ds.label) # Discriminating before or beyond Drift-Bifurcation
    1
    # Equilibrium velocity
    >>> print(ds.deterministic)
    0.0015191090506254991
    # Simulated velocity as a time series with 20000 time steps being disturbed by Gaussian white noise
    >>> v = ds.simulate(20000)
    '''

    def __init__(self, tau=3.8, kappa_3=0.3, Q=1950.0, R=0.0003, delta_t=0.05, seed=None):
        '''
        :param tau: Bifurcation parameter determining the intrinsic velocity of the dissipative soliton,
                    which is zero for tau<=1.0/kappa_3 and np.sqrt(kappa_3**3/Q * (tau - 1.0/kappa_3)) otherwise
        :type tau: float
        :param kappa_3: Inverse bifurcation point.
        :type kappa_3:
        :param Q: Shape parameter of dissipative soliton
        :type Q: float
        :param R: Noise amplitude
        :type R: float
        :param delta_t: temporal discretization
        :type delta_t: float
        '''
        pass

    def __call__(self, v):
        '''
        returns deterministic dynamic = acceleration (without noise)
        :param v: initial velocity vector
        :rtype v: ndarray
        :return: velocity vector of next time step
        :return type: ndarray
        '''
        pass

    def simulate(self, N, v0=np.zeros(2)):
        '''
        :param N: number of time steps
        :type N: int
        :param v0: initial velocity vector
        :type v0: ndarray
        :return: time series of velocity vectors with shape (N, v0.shape[0])
        :rtype: ndarray
        '''
        pass",snippet_13,"import numpy as np

class velocity:
    '''
    Simulates the velocity of a dissipative soliton (kind of self organized particle) [6]_.
    The equilibrium velocity without noise R=0 for
    $    au>1.0/\kappa_3$ is $\kappa_3 \sqrt{(tau - 1.0/\kappa_3)/Q}.
    Before the drift-bifurcation $    au \le 1.0/\kappa_3$ the velocity is zero.
    References
    ----------
    .. [6] Andreas Kempa-Liehr (2013, p. 159-170)
        Dynamics of Dissipative Soliton
        Dissipative Solitons in Reaction Diffusion Systems.
        Springer: Berlin

    >>> ds = velocity(tau=3.5) # Dissipative soliton with equilibrium velocity 1.5e-3
    >>> print(ds.label) # Discriminating before or beyond Drift-Bifurcation
    1
    # Equilibrium velocity
    >>> print(ds.deterministic)
    0.0015191090506254991
    # Simulated velocity as a time series with 20000 time steps being disturbed by Gaussian white noise
    >>> v = ds.simulate(20000)
    '''

    def __init__(self, tau=3.8, kappa_3=0.3, Q=1950.0, R=0.0003, delta_t=0.05, seed=None):
        '''
        :param tau: Bifurcation parameter determining the intrinsic velocity of the dissipative soliton,
                    which is zero for tau<=1.0/kappa_3 and np.sqrt(kappa_3**3/Q * (tau - 1.0/kappa_3)) otherwise
        :type tau: float
        :param kappa_3: Inverse bifurcation point.
        :type kappa_3:
        :param Q: Shape parameter of dissipative soliton
        :type Q: float
        :param R: Noise amplitude
        :type R: float
        :param delta_t: temporal discretization
        :type delta_t: float
        '''
        self.tau = float(tau)
        self.kappa_3 = float(kappa_3)
        self.Q = float(Q)
        self.R = float(R)
        self.delta_t = float(delta_t)
        self.seed = seed
        self.rng = np.random.default_rng(seed)

        inv_bif = 1.0 / self.kappa_3
        self._a = self.kappa_3**3 * (self.tau - inv_bif)

        if self._a > 0.0:
            self.label = 1
            self.deterministic = float(np.sqrt(self._a / self.Q))
        else:
            self.label = 0
            self.deterministic = 0.0

    def __call__(self, v):
        '''
        returns deterministic dynamic = acceleration (without noise)
        :param v: initial velocity vector
        :rtype v: ndarray
        :return: velocity vector of next time step
        :return type: ndarray
        '''
        v = np.asarray(v, dtype=float)
        r2 = np.dot(v, v)
        drift = self._a * v - self.Q * r2 * v
        return v + self.delta_t * drift

    def simulate(self, N, v0=np.zeros(2)):
        '''
        :param N: number of time steps
        :type N: int
        :param v0: initial velocity vector
        :type v0: ndarray
        :return: time series of velocity vectors with shape (N, v0.shape[0])
        :rtype: ndarray
        '''
        v0 = np.asarray(v0, dtype=float)
        if v0.ndim == 0:
            v0 = v0[None]
        d = v0.shape[0]
        traj = np.empty((N, d), dtype=float)
        traj[0] = v0

        sqrt_dt = np.sqrt(self.delta_t)
        for t in range(1, N):
            v_prev = traj[t - 1]
            v_det = self.__call__(v_prev)
            if self.R != 0.0:
                noise = self.R * sqrt_dt * self.rng.standard_normal(d)
                traj[t] = v_det + noise
            else:
                traj[t] = v_det
        return traj"
98891,peterbrittain/asciimatics,peterbrittain_asciimatics/asciimatics/effects.py,asciimatics.effects._Flake,"from random import randint, random, choice
from asciimatics.screen import Screen

class _Flake:
    """"""
    Track a single snow flake.
    """"""
    _snow_chars = '.+*'
    _drift_chars = ' ,;#@'

    def __init__(self, screen: Screen):
        """"""
        :param screen: The Screen being used for the Scene.
        """"""
        self._screen = screen
        self._x = 0
        self._y = 0
        self._rate = 0
        self._char = ''
        self._reseed()

    def _reseed(self):
        """"""
        Randomly create a new snowflake once this one is finished.
        """"""
        self._char = choice(self._snow_chars)
        self._rate = randint(1, 3)
        self._x = randint(0, self._screen.width - 1)
        self._y = self._screen.start_line + randint(0, self._rate)

    def update(self, reseed: bool):
        """"""
        Update that snowflake!

        :param reseed: Whether we are in the normal reseed cycle or not.
        """"""
        self._screen.print_at(' ', self._x, self._y)
        cell = None
        for _ in range(self._rate):
            self._y += 1
            cell = self._screen.get_from(self._x, self._y)
            if cell is None or cell[0] != 32:
                break
        if (cell is not None and cell[0] in [ord(x) for x in self._snow_chars + ' ']) and self._y < self._screen.start_line + self._screen.height:
            self._screen.print_at(self._char, self._x, self._y)
        else:
            self._y = min(self._y, self._screen.start_line + self._screen.height)
            drift_index = -1
            if cell:
                drift_index = self._drift_chars.find(chr(cell[0]))
            if 0 <= drift_index < len(self._drift_chars) - 1:
                drift_char = self._drift_chars[drift_index + 1]
                self._screen.print_at(drift_char, self._x, self._y)
            else:
                self._screen.print_at(',', self._x, self._y - 1)
            if reseed:
                self._reseed()","
class _Flake:
    '''
    Track a single snow flake.
    '''

    def __init__(self, screen: Screen):
        '''
        :param screen: The Screen being used for the Scene.
        '''
        pass

    def _reseed(self):
        '''
        Randomly create a new snowflake once this one is finished.
        '''
        pass

    def update(self, reseed: bool):
        '''
        Update that snowflake!
        :param reseed: Whether we are in the normal reseed cycle or not.
        '''
        pass",snippet_14,"import random
from typing import Optional

class _Flake:
    '''
    Track a single snow flake.
    '''

    def __init__(self, screen):
        '''
        :param screen: The Screen being used for the Scene.
        '''
        self._screen = screen
        self._x: int = 0
        self._y: int = 0
        self._prev: Optional[tuple[int, int]] = None
        self._delay: int = 1
        self._counter: int = 0
        self._char: str = ""*""
        self._colour = getattr(screen, ""COLOUR_WHITE"", None)
        self._attr = getattr(screen, ""A_BOLD"", 0)
        self._bg = getattr(screen, ""COLOUR_BLACK"", None)
        self._dead: bool = False
        self._drift_prob: float = 0.3
        self._spread_init: bool = True
        self._reseed()

    def _screen_size(self):
        if hasattr(self._screen, ""width"") and hasattr(self._screen, ""height""):
            return self._screen.width, self._screen.height
        if hasattr(self._screen, ""dimensions""):
            w, h = self._screen.dimensions
            return w, h
        # Fallback to something safe
        return 80, 24

    def _reseed(self):
        '''
        Randomly create a new snowflake once this one is finished.
        '''
        width, height = self._screen_size()
        if width <= 0 or height <= 0:
            self._dead = True
            return

        # Generate a new flake
        self._x = random.randrange(0, max(1, width))
        # First seed: random y to populate the sky. Subsequent reseeds: start at the top.
        if self._spread_init:
            self._y = random.randrange(0, max(1, height))
            self._spread_init = False
        else:
            self._y = 0

        self._prev = None
        self._counter = 0
        # Delay controls falling speed (higher = slower)
        self._delay = random.randint(1, 3)
        # Choose a glyph with a rough correlation to ""weight""
        glyphs = [""."", ""."", ""."", ""*"", ""+""]
        self._char = random.choice(glyphs)
        # Randomize attribute slightly
        self._attr = getattr(self._screen, ""A_BOLD"", 0) if random.random() < 0.5 else getattr(self._screen, ""A_NORMAL"", 0)
        self._dead = False

    def update(self, reseed: bool):
        '''
        Update that snowflake!
        :param reseed: Whether we are in the normal reseed cycle or not.
        '''
        if self._dead:
            if reseed:
                self._reseed()
            return

        width, height = self._screen_size()
        if width <= 0 or height <= 0:
            self._dead = True
            return

        # Erase previous position
        if self._prev is not None:
            px, py = self._prev
            if 0 <= px < width and 0 <= py < height:
                try:
                    # print space to erase
                    self._screen.print_at("" "", px, py, self._colour, getattr(self._screen, ""A_NORMAL"", 0), self._bg)
                except Exception:
                    pass

        # Move according to delay
        self._counter += 1
        if self._counter >= self._delay:
            self._counter = 0
            # Vertical fall
            self._y += 1
            # Random horizontal drift
            if random.random() < self._drift_prob:
                self._x += random.choice((-1, 0, 1))
                if self._x < 0:
                    self._x = 0
                elif self._x >= width:
                    self._x = width - 1

        # Out of screen bottom
        if self._y >= height:
            if reseed:
                self._reseed()
            else:
                self._dead = True
            return

        # Draw at new position
        if 0 <= self._x < width and 0 <= self._y < height and not self._dead:
            try:
                self._screen.print_at(self._char, self._x, self._y, self._colour, self._attr, self._bg)
            except Exception:
                # Be resilient to differing Screen implementations
                try:
                    self._screen.print_at(self._char, self._x, self._y)
                except Exception:
                    pass

        self._prev = (self._x, self._y)"
98892,peterbrittain/asciimatics,peterbrittain_asciimatics/asciimatics/effects.py,asciimatics.effects._Star,"from random import randint, random, choice
from asciimatics.screen import Screen

class _Star:
    """"""
    Simple class to represent a single star for the Stars special effect.
    """"""

    def __init__(self, screen: Screen, pattern: str):
        """"""
        :param screen: The Screen being used for the Scene.
        :param pattern: The pattern to loop through
        """"""
        self._screen = screen
        self._star_chars = pattern
        self._cycle = 0
        self._old_char = ''
        self._respawn()

    def _respawn(self):
        """"""
        Pick a random location for the star making sure it does
        not overwrite an existing piece of text.
        """"""
        self._cycle = randint(0, len(self._star_chars))
        height, width = self._screen.dimensions
        while True:
            self._x = randint(0, width - 1)
            self._y = self._screen.start_line + randint(0, height - 1)
            c = self._screen.get_from(self._x, self._y)
            if c is not None and c[0] == 32:
                break
        self._old_char = ' '

    def update(self):
        """"""
        Draw the star.
        """"""
        if not self._screen.is_visible(self._x, self._y):
            self._respawn()
        c = self._screen.get_from(self._x, self._y)
        if c is not None and c[0] not in (ord(self._old_char), 32):
            self._respawn()
        self._cycle += 1
        if self._cycle >= len(self._star_chars):
            self._cycle = 0
        new_char = self._star_chars[self._cycle]
        if new_char == self._old_char:
            return
        self._screen.print_at(new_char, self._x, self._y)
        self._old_char = new_char","
class _Star:
    '''
    Simple class to represent a single star for the Stars special effect.
        '''

    def __init__(self, screen: Screen, pattern: str):
        '''
        :param screen: The Screen being used for the Scene.
        :param pattern: The pattern to loop through
        '''
        pass

    def _respawn(self):
        '''
        Pick a random location for the star making sure it does
        not overwrite an existing piece of text.
        '''
        pass

    def update(self):
        '''
        Draw the star.
        '''
        pass",snippet_15,"import random
from typing import Optional
try:
    from asciimatics.screen import Screen
except Exception:
    # Fallback type hint if asciimatics isn't available at runtime
    Screen = object  # type: ignore


class _Star:
    '''
    Simple class to represent a single star for the Stars special effect.
    '''

    def __init__(self, screen: Screen, pattern: str):
        '''
        :param screen: The Screen being used for the Scene.
        :param pattern: The pattern to loop through
        '''
        self._screen = screen
        self._pattern = pattern or ""*""
        self._index = 0
        self._x: Optional[int] = None
        self._y: Optional[int] = None
        self._respawn()

    def _empty_at(self, x: int, y: int) -> bool:
        try:
            cell = self._screen.get_from(x, y)
            if cell is None:
                return False
            ch = cell[0]
            return ch == "" ""
        except Exception:
            # If screen can't provide cell info, assume not empty to be safe.
            return False

    def _respawn(self):
        '''
        Pick a random location for the star making sure it does
        not overwrite an existing piece of text.
        '''
        width = getattr(self._screen, ""width"", 0)
        height = getattr(self._screen, ""height"", 0)
        if width <= 0 or height <= 0:
            self._x, self._y = 0, 0
            return

        for _ in range(200):
            x = random.randint(0, width - 1)
            y = random.randint(0, height - 1)
            if self._empty_at(x, y):
                self._x, self._y = x, y
                return

        # Fallback: pick a location even if occupied to avoid infinite loops
        self._x = 0 if self._x is None else max(0, min(self._x, width - 1))
        self._y = 0 if self._y is None else max(0, min(self._y, height - 1))

    def update(self):
        '''
        Draw the star.
        '''
        if self._x is None or self._y is None:
            self._respawn()

        # Move if our current location is now occupied
        if not self._empty_at(self._x, self._y):
            self._respawn()

        ch = self._pattern[self._index % len(self._pattern)]
        try:
            self._screen.print_at(ch, self._x, self._y)
        except TypeError:
            # Some Screen implementations may require full parameters; try with defaults
            self._screen.print_at(ch, self._x, self._y, 7, 0, 0, False)

        self._index = (self._index + 1) % len(self._pattern)"
98893,peterbrittain/asciimatics,peterbrittain_asciimatics/asciimatics/effects.py,asciimatics.effects._Trail,"from random import randint, random, choice
from asciimatics.screen import Screen

class _Trail:
    """"""
    Track a single trail  for a falling character effect (a la Matrix).
    """"""

    def __init__(self, screen: Screen, x: int):
        """"""
        :param screen: The Screen being used for the Scene.
        :param x: The column (y coordinate) for this trail to use.
        """"""
        self._screen = screen
        self._x = x
        self._y = 0
        self._life = 0
        self._rate = 0
        self._clear = True
        self._maybe_reseed(True)

    def _maybe_reseed(self, normal: bool):
        """"""
        Randomly create a new column once this one is finished.
        """"""
        self._y += self._rate
        self._life -= 1
        if self._life <= 0:
            self._clear = not self._clear if normal else True
            self._rate = randint(1, 2)
            if self._clear:
                self._y = 0
                self._life = self._screen.height // self._rate
            else:
                self._y = randint(0, self._screen.height // 2) - self._screen.height // 4
                self._life = randint(1, self._screen.height - self._y) // self._rate

    def update(self, reseed: bool):
        """"""
        Update that trail!

        :param reseed: Whether we are in the normal reseed cycle or not.
        """"""
        if self._clear:
            for i in range(0, 3):
                self._screen.print_at(' ', self._x, self._screen.start_line + self._y + i)
            self._maybe_reseed(reseed)
        else:
            for i in range(0, 3):
                self._screen.print_at(chr(randint(32, 126)), self._x, self._screen.start_line + self._y + i, Screen.COLOUR_GREEN)
            for i in range(4, 6):
                self._screen.print_at(chr(randint(32, 126)), self._x, self._screen.start_line + self._y + i, Screen.COLOUR_GREEN, Screen.A_BOLD)
            self._maybe_reseed(reseed)","
class _Trail:
    '''
    Track a single trail  for a falling character effect (a la Matrix).
    '''

    def __init__(self, screen: Screen, x: int):
        '''
        :param screen: The Screen being used for the Scene.
        :param x: The column (y coordinate) for this trail to use.
        '''
        pass

    def _maybe_reseed(self, normal: bool):
        '''
        Randomly create a new column once this one is finished.
        '''
        pass

    def update(self, reseed: bool):
        '''
        Update that trail!
        :param reseed: Whether we are in the normal reseed cycle or not.
        '''
        pass",snippet_16,"import random
import string
from typing import List
from asciimatics.screen import Screen


class _Trail:
    '''
    Track a single trail  for a falling character effect (a la Matrix).
    '''

    def __init__(self, screen: Screen, x: int):
        '''
        :param screen: The Screen being used for the Scene.
        :param x: The column (y coordinate) for this trail to use.
        '''
        self._screen = screen
        self._x = max(0, min(x, self._screen.width - 1))
        self._charset = string.ascii_letters + string.digits + string.punctuation
        self._length = random.randint(max(5, self._screen.height // 6), max(8, self._screen.height // 3))
        self._y = -random.randint(0, self._screen.height)  # start above screen
        self._chars: List[str] = []
        self._active = True

    def _maybe_reseed(self, normal: bool):
        '''
        Randomly create a new column once this one is finished.
        '''
        finished = self._y - self._length > self._screen.height
        if not finished:
            return

        if normal:
            # In normal cycle, only sometimes reseed to avoid constant density.
            if random.random() < 0.02:
                self._length = random.randint(max(5, self._screen.height // 6), max(8, self._screen.height // 3))
                self._y = -random.randint(0, self._screen.height)
                self._chars.clear()
                self._active = True
            else:
                self._active = False
        else:
            # Forced cycle: always reseed.
            self._length = random.randint(max(5, self._screen.height // 6), max(8, self._screen.height // 3))
            self._y = -random.randint(0, self._screen.height)
            self._chars.clear()
            self._active = True

    def update(self, reseed: bool):
        '''
        Update that trail!
        :param reseed: Whether we are in the normal reseed cycle or not.
        '''
        if not self._active:
            # If not currently active, try to reseed and return.
            self._maybe_reseed(reseed)
            return

        # Append a new head character.
        head_char = random.choice(self._charset)
        self._chars.insert(0, head_char)
        if len(self._chars) > self._length:
            self._chars.pop()

        # Clear the cell that trails beyond the bottom.
        tail_y = int(self._y - self._length)
        if 0 <= tail_y < self._screen.height:
            self._screen.print_at("" "", self._x, tail_y)

        # Draw from head to tail with diminishing intensity.
        for i, ch in enumerate(self._chars):
            y = int(self._y - i)
            if 0 <= y < self._screen.height:
                if i == 0:
                    # Head - bright white
                    self._screen.print_at(ch, self._x, y,
                                          colour=Screen.COLOUR_WHITE,
                                          attr=Screen.A_BOLD)
                elif i < min(3, self._length):
                    # Near head - bold green
                    self._screen.print_at(ch, self._x, y,
                                          colour=Screen.COLOUR_GREEN,
                                          attr=Screen.A_BOLD)
                else:
                    # Tail - normal green
                    self._screen.print_at(ch, self._x, y,
                                          colour=Screen.COLOUR_GREEN,
                                          attr=Screen.A_NORMAL)

        # Move trail down.
        self._y += 1

        # Maybe reseed if finished.
        self._maybe_reseed(reseed)"
99108,ConsenSys/mythril-classic,ConsenSys_mythril-classic/mythril/analysis/ops.py,mythril.analysis.ops.Variable,"class Variable:
    """"""The representation of a variable with value and type.""""""

    def __init__(self, val, _type):
        """"""

        :param val:
        :param _type:
        """"""
        self.val = val
        self.type = _type

    def __str__(self):
        """"""

        :return:
        """"""
        return str(self.val)","class Variable:
    '''The representation of a variable with value and type.'''

    def __init__(self, val, _type):
        '''
        :param val:
        :param _type:
        '''
        pass

    def __str__(self):
        '''
        :return:
        '''
        pass",snippet_17,"class Variable:
    '''The representation of a variable with value and type.'''

    def __init__(self, val, _type):
        '''
        :param val:
        :param _type:
        '''
        self.val = val
        self.type = _type

    def __str__(self):
        '''
        :return:
        '''
        tname = self.type.__name__ if isinstance(self.type, type) else str(self.type)
        return f'{self.val} ({tname})'"
100863,bigchaindb/bigchaindb,bigchaindb_bigchaindb/bigchaindb/validation.py,bigchaindb.validation.BaseValidationRules,"class BaseValidationRules:
    """"""Base validation rules for BigchainDB.

    A validation plugin must expose a class inheriting from this one via an entry_point.

    All methods listed below must be implemented.
    """"""

    @staticmethod
    def validate_transaction(bigchaindb, transaction):
        """"""See :meth:`bigchaindb.models.Transaction.validate`
        for documentation.
        """"""
        return transaction.validate(bigchaindb)

    @staticmethod
    def validate_block(bigchaindb, block):
        """"""See :meth:`bigchaindb.models.Block.validate` for documentation.""""""
        return block.validate(bigchaindb)","class BaseValidationRules:
    '''Base validation rules for BigchainDB.
    A validation plugin must expose a class inheriting from this one via an entry_point.
    All methods listed below must be implemented.
    '''
    @staticmethod
    def validate_transaction(bigchaindb, transaction):
        '''See :meth:`bigchaindb.models.Transaction.validate`
        for documentation.
        '''
        pass
    @staticmethod
    def validate_block(bigchaindb, block):
        '''See :meth:`bigchaindb.models.Block.validate` for documentation.'''
        pass",snippet_22,"class BaseValidationRules:
    '''Base validation rules for BigchainDB.
    A validation plugin must expose a class inheriting from this one via an entry_point.
    All methods listed below must be implemented.
    '''
    @staticmethod
    def validate_transaction(bigchaindb, transaction):
        '''See :meth:`bigchaindb.models.Transaction.validate`
        for documentation.
        '''
        if hasattr(transaction, 'validate') and callable(getattr(transaction, 'validate')):
            return transaction.validate(bigchaindb)
        return True

    @staticmethod
    def validate_block(bigchaindb, block):
        '''See :meth:`bigchaindb.models.Block.validate` for documentation.'''
        if hasattr(block, 'validate') and callable(getattr(block, 'validate')):
            return block.validate(bigchaindb)
        return True"
100865,bigchaindb/bigchaindb,bigchaindb_bigchaindb/bigchaindb/web/strip_content_type_middleware.py,bigchaindb.web.strip_content_type_middleware.StripContentTypeMiddleware,"class StripContentTypeMiddleware:
    """"""WSGI middleware to strip Content-Type header for GETs.""""""

    def __init__(self, app):
        """"""Create the new middleware.

        Args:
            app: a flask application
        """"""
        self.app = app

    def __call__(self, environ, start_response):
        """"""Run the middleware and then call the original WSGI application.""""""
        if environ['REQUEST_METHOD'] == 'GET':
            try:
                del environ['CONTENT_TYPE']
            except KeyError:
                pass
            else:
                logger.debug('Remove header ""Content-Type"" from GET request')
        return self.app(environ, start_response)","class StripContentTypeMiddleware:
    '''WSGI middleware to strip Content-Type header for GETs.'''

    def __init__(self, app):
        '''Create the new middleware.
        Args:
            app: a flask application
        '''
        pass

    def __call__(self, environ, start_response):
        '''Run the middleware and then call the original WSGI application.'''
        pass",snippet_23,"class StripContentTypeMiddleware:
    '''WSGI middleware to strip Content-Type header for GETs.'''

    def __init__(self, app):
        '''Create the new middleware.
        Args:
            app: a flask application
        '''
        self.app = app

    def __call__(self, environ, start_response):
        '''Run the middleware and then call the original WSGI application.'''
        if environ.get('REQUEST_METHOD', '').upper() == 'GET':
            # Remove CGI-style and possible HTTP_ prefixed Content-Type entries
            environ.pop('CONTENT_TYPE', None)
            environ.pop('HTTP_CONTENT_TYPE', None)
        return self.app(environ, start_response)"
100895,ansible/molecule,src/molecule/app.py,molecule.app.App,"from pathlib import Path
from subprocess import CalledProcessError, CompletedProcess
from molecule.console import original_stderr
from ansible_compat.runtime import Runtime
from molecule.ansi_output import CommandBorders
from molecule.util import print_environment_vars

class App:
    """"""App class that keep runtime status.""""""

    def __init__(self, path: Path) -> None:
        """"""Create a new app instance.

        Args:
            path: The path to the project.
        """"""
        self.runtime = Runtime(project_dir=path, isolated=False)

    def run_command(self, cmd: str | list[str], env: dict[str, str] | None=None, cwd: Path | None=None, *, debug: bool=False, echo: bool=False, quiet: bool=False, check: bool=False, command_borders: bool=False) -> CompletedProcess[str]:
        """"""Execute the given command and returns None.

        Args:
            cmd: A list of strings containing the command to run.
            env: A dict containing the shell's environment.
            cwd: An optional Path to the working directory.
            debug: An optional bool to toggle debug output.
            echo: An optional bool to toggle command echo.
            quiet: An optional bool to toggle command output.
            check: An optional bool to toggle command error checking.
            command_borders: An optional bool to enable borders around command output.

        Returns:
            A completed process object.

        Raises:
            CalledProcessError: If return code is nonzero and check is True.
        """"""
        if debug:
            print_environment_vars(env)
        borders = None
        if command_borders:
            borders = CommandBorders(cmd=cmd, original_stderr=original_stderr)
        result = self.runtime.run(args=cmd, env=env, cwd=cwd, tee=True, set_acp=False)
        if borders:
            borders.finalize(result.returncode)
        if result.returncode != 0 and check:
            raise CalledProcessError(returncode=result.returncode, cmd=result.args, output=result.stdout, stderr=result.stderr)
        return result","
class App:
    '''App class that keep runtime status.'''

    def __init__(self, path: Path) -> None:
        '''Create a new app instance.
        Args:
            path: The path to the project.
        '''
        pass

    def run_command(self, cmd: str | list[str], env: dict[str, str] | None=None, cwd: Path | None=None, *, debug: bool=False, echo: bool=False, quiet: bool=False, check: bool=False, command_borders: bool=False) -> CompletedProcess[str]:
        '''Execute the given command and returns None.
        Args:
            cmd: A list of strings containing the command to run.
            env: A dict containing the shell's environment.
            cwd: An optional Path to the working directory.
            debug: An optional bool to toggle debug output.
            echo: An optional bool to toggle command echo.
            quiet: An optional bool to toggle command output.
            check: An optional bool to toggle command error checking.
            command_borders: An optional bool to enable borders around command output.
        Returns:
            A completed process object.
        Raises:
            CalledProcessError: If return code is nonzero and check is True.
        '''
        pass",snippet_24,"from __future__ import annotations

import os
import shlex
import sys
import threading
from pathlib import Path
from subprocess import CalledProcessError, CompletedProcess, Popen, PIPE


class App:
    '''App class that keep runtime status.'''

    def __init__(self, path: Path) -> None:
        '''Create a new app instance.
        Args:
            path: The path to the project.
        '''
        self.path = Path(path).resolve()

    def run_command(
        self,
        cmd: str | list[str],
        env: dict[str, str] | None = None,
        cwd: Path | None = None,
        *,
        debug: bool = False,
        echo: bool = False,
        quiet: bool = False,
        check: bool = False,
        command_borders: bool = False
    ) -> CompletedProcess[str]:
        '''Execute the given command and returns None.
        Args:
            cmd: A list of strings containing the command to run.
            env: A dict containing the shell's environment.
            cwd: An optional Path to the working directory.
            debug: An optional bool to toggle debug output.
            echo: An optional bool to toggle command echo.
            quiet: An optional bool to toggle command output.
            check: An optional bool to toggle command error checking.
            command_borders: An optional bool to enable borders around command output.
        Returns:
            A completed process object.
        Raises:
            CalledProcessError: If return code is nonzero and check is True.
        '''
        if isinstance(cmd, str):
            args: list[str] = shlex.split(cmd)
            cmd_display = cmd
        else:
            args = list(cmd)
            cmd_display = "" "".join(shlex.quote(a) for a in args)

        working_dir = Path(cwd) if cwd is not None else self.path

        # Prepare environment
        if env is not None:
            proc_env = os.environ.copy()
            proc_env.update(env)
        else:
            proc_env = os.environ.copy()

        if echo and not quiet:
            print(f""+ {cmd_display}"", flush=True)

        if debug and not quiet:
            print(f""[debug] cwd: {working_dir}"", flush=True)
            if env:
                for k, v in env.items():
                    print(f""[debug] env {k}={v}"", flush=True)

        if command_borders and not quiet:
            border = ""="" * max(10, min(80, len(cmd_display) + 10))
            print(border, flush=True)
            print(f""# Running: {cmd_display}"", flush=True)
            print(border, flush=True)

        # Run process with streaming and capture
        proc = Popen(
            args,
            cwd=str(working_dir),
            env=proc_env,
            stdout=PIPE,
            stderr=PIPE,
            text=True,
            bufsize=1,
        )

        stdout_chunks: list[str] = []
        stderr_chunks: list[str] = []

        def _reader(stream, sink, collect_list, is_stdout: bool):
            for line in iter(stream.readline, """"):
                collect_list.append(line)
                if not quiet:
                    sink.write(line)
                    sink.flush()
            stream.close()

        threads: list[threading.Thread] = []
        t_out = threading.Thread(target=_reader, args=(proc.stdout, sys.stdout, stdout_chunks, True))
        t_err = threading.Thread(target=_reader, args=(proc.stderr, sys.stderr, stderr_chunks, False))
        t_out.start()
        t_err.start()
        threads.extend([t_out, t_err])

        rc = proc.wait()
        for t in threads:
            t.join()

        stdout_text = """".join(stdout_chunks)
        stderr_text = """".join(stderr_chunks)

        if command_borders and not quiet:
            end_border = ""-"" * 20
            print(f""{end_border} exit {rc} {end_border}"", flush=True)

        if check and rc != 0:
            raise CalledProcessError(rc, args, output=stdout_text, stderr=stderr_text)

        return CompletedProcess(args=args, returncode=rc, stdout=stdout_text, stderr=stderr_text)"
104788,autokey/autokey,autokey_autokey/lib/autokey/scripting/system.py,autokey.scripting.system.System,"import subprocess

class System:
    """"""
    Simplified access to some system commands.
    """"""

    @staticmethod
    def exec_command(command, getOutput=True):
        """"""
        Execute a shell command

        Usage: C{system.exec_command(command, getOutput=True)}

        Set getOutput to False if the command does not exit and return immediately. Otherwise
        AutoKey will not respond to any hotkeys/abbreviations etc until the process started
        by the command exits.

        @param command: command to be executed (including any arguments) - e.g. ""ls -l""
        @param getOutput: whether to capture the (stdout) output of the command
        @raise subprocess.CalledProcessError: if the command returns a non-zero exit code
        """"""
        if getOutput:
            with subprocess.Popen(command, shell=True, bufsize=-1, stdout=subprocess.PIPE, universal_newlines=True) as p:
                output = p.communicate()[0]
                output = output.rstrip('\n')
                if p.returncode:
                    raise subprocess.CalledProcessError(p.returncode, output)
                return output
        else:
            subprocess.Popen(command, shell=True, bufsize=-1)

    @staticmethod
    def create_file(file_name, contents=''):
        """"""
        Create a file with contents

        Usage: C{system.create_file(fileName, contents="""")}

        @param fileName: full path to the file to be created
        @param contents: contents to insert into the file
        """"""
        with open(file_name, 'w') as written_file:
            written_file.write(contents)","
class System:
    '''
    Simplified access to some system commands.
    '''
    @staticmethod
    def exec_command(command, getOutput=True):
        '''
        Execute a shell command
        Usage: C{system.exec_command(command, getOutput=True)}
        Set getOutput to False if the command does not exit and return immediately. Otherwise
        AutoKey will not respond to any hotkeys/abbreviations etc until the process started
        by the command exits.
        @param command: command to be executed (including any arguments) - e.g. ""ls -l""
        @param getOutput: whether to capture the (stdout) output of the command
        @raise subprocess.CalledProcessError: if the command returns a non-zero exit code
        '''
        pass
    @staticmethod
    def create_file(file_name, contents=''):
        '''
        Create a file with contents
        Usage: C{system.create_file(fileName, contents="""")}
        @param fileName: full path to the file to be created
        @param contents: contents to insert into the file
        '''
        pass",snippet_26,"import os
import subprocess
from typing import Optional, Union


class System:
    '''
    Simplified access to some system commands.
    '''
    @staticmethod
    def exec_command(command: Union[str, list], getOutput: bool = True) -> Optional[str]:
        '''
        Execute a shell command
        Usage: C{system.exec_command(command, getOutput=True)}
        Set getOutput to False if the command does not exit and return immediately. Otherwise
        AutoKey will not respond to any hotkeys/abbreviations etc until the process started
        by the command exits.
        @param command: command to be executed (including any arguments) - e.g. ""ls -l""
        @param getOutput: whether to capture the (stdout) output of the command
        @raise subprocess.CalledProcessError: if the command returns a non-zero exit code
        '''
        if getOutput:
            # Capture stdout (and stderr for richer exceptions), wait for completion, and raise on error
            completed = subprocess.run(
                command,
                shell=isinstance(command, str),
                text=True,
                capture_output=True,
                check=True,
            )
            return completed.stdout
        else:
            # Fire and forget: do not wait, do not capture output
            subprocess.Popen(
                command,
                shell=isinstance(command, str),
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                stdin=subprocess.DEVNULL,
                close_fds=True,
                start_new_session=True,
            )
            return None

    @staticmethod
    def create_file(file_name: str, contents: str = '') -> None:
        '''
        Create a file with contents
        Usage: C{system.create_file(fileName, contents="""")}
        @param fileName: full path to the file to be created
        @param contents: contents to insert into the file
        '''
        dir_name = os.path.dirname(os.path.abspath(file_name))
        if dir_name and not os.path.exists(dir_name):
            os.makedirs(dir_name, exist_ok=True)
        with open(file_name, 'w', encoding='utf-8', newline='') as f:
            f.write(contents)"
106092,mikedh/trimesh,trimesh/collision.py,trimesh.collision.DistanceData,"class DistanceData:
    """"""
    Data structure for holding information about a distance query.
    """"""

    def __init__(self, names, result):
        """"""
        Initialize a DistanceData.

        Parameters
        ----------
        names : list of str
          The names of the two objects in order.
        contact : fcl.DistanceResult
          The distance query result.
        """"""
        self.names = set(names)
        self._inds = {names[0]: result.b1, names[1]: result.b2}
        self._points = {names[0]: result.nearest_points[0], names[1]: result.nearest_points[1]}
        self._distance = result.min_distance

    @property
    def distance(self):
        """"""
        Returns the distance between the two objects.

        Returns
        -------
        distance : float
          The euclidean distance between the objects.
        """"""
        return self._distance

    def index(self, name):
        """"""
        Returns the index of the closest face for the mesh with
        the given name.

        Parameters
        ----------
        name : str
          The name of the target object.

        Returns
        -------
        index : int
          The index of the face in collisoin.
        """"""
        return self._inds[name]

    def point(self, name):
        """"""
        The 3D point of closest distance on the mesh with the given name.

        Parameters
        ----------
        name : str
          The name of the target object.

        Returns
        -------
        point : (3,) float
          The closest point.
        """"""
        return self._points[name]","class DistanceData:
    '''
    Data structure for holding information about a distance query.
    '''

    def __init__(self, names, result):
        '''
        Initialize a DistanceData.
        Parameters
        ----------
        names : list of str
          The names of the two objects in order.
        contact : fcl.DistanceResult
          The distance query result.
        '''
        pass
    @property
    def distance(self):
        '''
        Returns the distance between the two objects.
        Returns
        -------
        distance : float
          The euclidean distance between the objects.
        '''
        pass

    def index(self, name):
        '''
        Returns the index of the closest face for the mesh with
        the given name.
        Parameters
        ----------
        name : str
          The name of the target object.
        Returns
        -------
        index : int
          The index of the face in collisoin.
        '''
        pass

    def point(self, name):
        '''
        The 3D point of closest distance on the mesh with the given name.
        Parameters
        ----------
        name : str
          The name of the target object.
        Returns
        -------
        point : (3,) float
          The closest point.
        '''
        pass",snippet_28,"class DistanceData:
    '''
    Data structure for holding information about a distance query.
    '''

    def __init__(self, names, result):
        '''
        Initialize a DistanceData.
        Parameters
        ----------
        names : list of str
          The names of the two objects in order.
        contact : fcl.DistanceResult
          The distance query result.
        '''
        if not isinstance(names, (list, tuple)) or len(names) != 2:
            raise ValueError(""names must be a list/tuple of exactly two strings"")
        self._names = list(names)
        self._result = result

    @property
    def distance(self):
        '''
        Returns the distance between the two objects.
        Returns
        -------
        distance : float
          The euclidean distance between the objects.
        '''
        for attr in ('min_distance', 'distance', 'dist'):
            if hasattr(self._result, attr):
                val = getattr(self._result, attr)
                return float(val)
        raise AttributeError(""Distance attribute not found on result"")

    def _name_index(self, name):
        try:
            return self._names.index(name)
        except ValueError:
            raise KeyError(f""Unknown name: {name}"")

    def index(self, name):
        '''
        Returns the index of the closest face for the mesh with
        the given name.
        Parameters
        ----------
        name : str
          The name of the target object.
        Returns
        -------
        index : int
          The index of the face in collisoin.
        '''
        i = self._name_index(name)
        candidates = [
            ('b1', 'b2'),
            ('id1', 'id2'),
            ('prim_id1', 'prim_id2'),
            ('face_id1', 'face_id2'),
            ('triangle_index1', 'triangle_index2'),
            ('closest_feature1', 'closest_feature2'),
        ]
        for a1, a2 in candidates:
            attr = a1 if i == 0 else a2
            if hasattr(self._result, attr):
                val = getattr(self._result, attr)
                if val is None:
                    continue
                try:
                    return int(val)
                except (TypeError, ValueError):
                    continue
        raise AttributeError(""Face index attributes not found on result"")

    def point(self, name):
        '''
        The 3D point of closest distance on the mesh with the given name.
        Parameters
        ----------
        name : str
          The name of the target object.
        Returns
        -------
        point : (3,) float
          The closest point.
        '''
        i = self._name_index(name)

        # Array of two points
        for attr in ('nearest_points', 'closest_points', 'points'):
            if hasattr(self._result, attr):
                pts = getattr(self._result, attr)
                p = pts[i]
                return tuple(float(x) for x in p)

        # Individual point attributes
        pair_candidates = [
            ('nearest_point1', 'nearest_point2'),
            ('closest_point1', 'closest_point2'),
            ('p1', 'p2'),
            ('point1', 'point2'),
        ]
        for a1, a2 in pair_candidates:
            attr = a1 if i == 0 else a2
            if hasattr(self._result, attr):
                p = getattr(self._result, attr)
                return tuple(float(x) for x in p)

        raise AttributeError(""Closest point attributes not found on result"")"
106111,mikedh/trimesh,trimesh/path/packing.py,trimesh.path.packing.RectangleBin,"import numpy as np

class RectangleBin:
    """"""
    An N-dimensional binary space partition tree for packing
    hyper-rectangles. Split logic is pure `numpy` but behaves
    similarly to `scipy.spatial.Rectangle`.

    Mostly useful for packing 2D textures and 3D boxes and
    has not been tested outside of 2 and 3 dimensions.

    Original article about using this for packing textures:
    http://www.blackpawn.com/texts/lightmaps/
    """"""

    def __init__(self, bounds):
        """"""
        Create a rectangular bin.

        Parameters
        ------------
        bounds : (2, dimension *) float
          Bounds array are `[mins, maxes]`
        """"""
        self.child = []
        self.occupied = False
        self.bounds = np.array(bounds, dtype=np.float64)

    @property
    def extents(self):
        """"""
        Bounding box size.

        Returns
        ----------
        extents : (dimension,) float
          Edge lengths of bounding box
        """"""
        bounds = self.bounds
        return bounds[1] - bounds[0]

    def insert(self, size, rotate=True):
        """"""
        Insert a rectangle into the bin.

        Parameters
        -------------
        size : (dimension,) float
          Size of rectangle to insert/

        Returns
        ----------
        inserted : (2,) float or None
          Position of insertion in the tree or None
          if the insertion was unsuccessful.
        """"""
        for child in self.child:
            attempt = child.insert(size=size, rotate=rotate)
            if attempt is not None:
                return attempt
        if self.occupied:
            return None
        bounds = self.bounds.copy()
        extents = bounds[1] - bounds[0]
        if rotate:
            for roll in range(len(size)):
                size_test = extents - _roll(size, roll)
                fits = (size_test > -_TOL_ZERO).all()
                if fits:
                    size = _roll(size, roll)
                    break
            if not fits:
                return None
        else:
            size_test = extents - size
            if (size_test < -_TOL_ZERO).any():
                return None
        self.occupied = True
        if (size_test < _TOL_ZERO).all():
            return bounds
        axis = size_test.argmax()
        splits = np.vstack((bounds, bounds))
        splits[1:3, axis] = bounds[0][axis] + size[axis]
        self.child[:] = (RectangleBin(splits[:2]), RectangleBin(splits[2:]))
        return self.child[0].insert(size, rotate=rotate)","
class RectangleBin:
    '''
    An N-dimensional binary space partition tree for packing
    hyper-rectangles. Split logic is pure `numpy` but behaves
    similarly to `scipy.spatial.Rectangle`.
    Mostly useful for packing 2D textures and 3D boxes and
    has not been tested outside of 2 and 3 dimensions.
    Original article about using this for packing textures:
    http://www.blackpawn.com/texts/lightmaps/
    '''

    def __init__(self, bounds):
        '''
        Create a rectangular bin.
        Parameters
        ------------
        bounds : (2, dimension *) float
          Bounds array are `[mins, maxes]`
        '''
        pass
    @property
    def extents(self):
        '''
        Bounding box size.
        Returns
        ----------
        extents : (dimension,) float
          Edge lengths of bounding box
        '''
        pass

    def insert(self, size, rotate=True):
        '''
        Insert a rectangle into the bin.
        Parameters
        -------------
        size : (dimension,) float
          Size of rectangle to insert/
        Returns
        ----------
        inserted : (2,) float or None
          Position of insertion in the tree or None
          if the insertion was unsuccessful.
        '''
        pass",snippet_29,"import numpy as np
import itertools

class RectangleBin:
    '''
    An N-dimensional binary space partition tree for packing
    hyper-rectangles. Split logic is pure `numpy` but behaves
    similarly to `scipy.spatial.Rectangle`.
    Mostly useful for packing 2D textures and 3D boxes and
    has not been tested outside of 2 and 3 dimensions.
    Original article about using this for packing textures:
    http://www.blackpawn.com/texts/lightmaps/
    '''

    class _Node:
        __slots__ = (""mins"", ""maxs"", ""left"", ""right"", ""used"")

        def __init__(self, mins, maxs):
            self.mins = mins
            self.maxs = maxs
            self.left = None
            self.right = None
            self.used = False

        @property
        def extents(self):
            return self.maxs - self.mins

        def is_leaf(self):
            return self.left is None and self.right is None

    def __init__(self, bounds):
        '''
        Create a rectangular bin.
        Parameters
        ------------
        bounds : (2, dimension *) float
          Bounds array are `[mins, maxes]`
        '''
        b = np.asarray(bounds, dtype=float)
        if b.ndim != 2 or b.shape[0] != 2:
            raise ValueError(""bounds must be shape (2, dimension)"")
        mins = b[0].astype(float)
        maxs = b[1].astype(float)
        if mins.shape != maxs.shape:
            raise ValueError(""mins and maxs must have the same shape"")
        if np.any(maxs < mins):
            raise ValueError(""All maxs must be >= mins"")
        self._dim = mins.size
        self._root = RectangleBin._Node(mins.copy(), maxs.copy())

    @property
    def extents(self):
        '''
        Bounding box size.
        Returns
        ----------
        extents : (dimension,) float
          Edge lengths of bounding box
        '''
        return self._root.extents.copy()

    def _insert_node(self, node, size):
        # If this node has been split, try children
        if not node.is_leaf():
            pos = self._insert_node(node.left, size)
            if pos is not None:
                return pos
            return self._insert_node(node.right, size)

        # If already used, cannot place here
        if node.used:
            return None

        space = node.extents
        leftover = space - size

        # If it doesn't fit, bail
        if np.any(leftover < 0):
            return None

        # Perfect fit: occupy node
        if np.allclose(leftover, 0):
            node.used = True
            return node.mins.copy()

        # Split along axis with the most leftover space
        axis = int(np.argmax(leftover))
        cut = node.mins[axis] + size[axis]

        # Left child: from mins to cut along axis
        left_mins = node.mins.copy()
        left_maxs = node.maxs.copy()
        left_maxs[axis] = cut

        # Right child: from cut to maxs along axis
        right_mins = node.mins.copy()
        right_mins[axis] = cut
        right_maxs = node.maxs.copy()

        node.left = RectangleBin._Node(left_mins, left_maxs)
        node.right = RectangleBin._Node(right_mins, right_maxs)

        # Try to insert into the left child (tight fit region)
        return self._insert_node(node.left, size)

    def insert(self, size, rotate=True):
        '''
        Insert a rectangle into the bin.
        Parameters
        -------------
        size : (dimension,) float
          Size of rectangle to insert/
        Returns
        ----------
        inserted : (2,) float or None
          Position of insertion in the tree or None
          if the insertion was unsuccessful.
        '''
        sz = np.asarray(size, dtype=float).reshape(-1)
        if sz.size != self._dim:
            raise ValueError(f""size must be of length {self._dim}"")

        # Generate size permutations for rotation if allowed
        if rotate and self._dim > 1:
            perms = list(dict.fromkeys(itertools.permutations(range(self._dim))))
        else:
            perms = [tuple(range(self._dim))]

        # Try each permutation until one fits
        for perm in perms:
            permuted_size = sz[list(perm)]
            pos = self._insert_node(self._root, permuted_size)
            if pos is not None:
                return pos

        return None"
107609,weld-project/weld,python/grizzly/grizzly/lazy_op.py,grizzly.lazy_op.LazyOpResult,"class LazyOpResult:
    """"""Wrapper class around as yet un-evaluated Weld computation results

    Attributes:
        dim (int): Dimensionality of the output
        expr (WeldObject / Numpy.ndarray): The expression that needs to be
            evaluated
        weld_type (WeldType): Type of the output object
    """"""

    def __init__(self, expr, weld_type, dim):
        """"""Summary

        Args:
            expr (TYPE): Description
            weld_type (TYPE): Description
            dim (TYPE): Description
        """"""
        self.expr = expr
        self.weld_type = weld_type
        self.dim = dim

    def evaluate(self, verbose=True, decode=True, passes=None, num_threads=1, apply_experimental_transforms=False):
        """"""Summary

        Args:
            verbose (bool, optional): Description
            decode (bool, optional): Description

        Returns:
            TYPE: Description
        """"""
        if isinstance(self.expr, WeldObject):
            return self.expr.evaluate(to_weld_type(self.weld_type, self.dim), verbose, decode, passes=passes, num_threads=num_threads, apply_experimental_transforms=apply_experimental_transforms)
        return self.expr","class LazyOpResult:
    '''Wrapper class around as yet un-evaluated Weld computation results
    Attributes:
        dim (int): Dimensionality of the output
        expr (WeldObject / Numpy.ndarray): The expression that needs to be
            evaluated
        weld_type (WeldType): Type of the output object
    '''

    def __init__(self, expr, weld_type, dim):
        '''Summary
        Args:
            expr (TYPE): Description
            weld_type (TYPE): Description
            dim (TYPE): Description
        '''
        pass

    def evaluate(self, verbose=True, decode=True, passes=None, num_threads=1, apply_experimental_transforms=False):
        '''Summary
        Args:
            verbose (bool, optional): Description
            decode (bool, optional): Description
        Returns:
            TYPE: Description
        '''
        pass",snippet_30,"class LazyOpResult:
    '''Wrapper class around as yet un-evaluated Weld computation results
    Attributes:
        dim (int): Dimensionality of the output
        expr: The expression that needs to be evaluated (e.g., WeldObject, numpy array, etc.)
        weld_type: Type of the output object (optional, used by some runtimes)
    '''

    def __init__(self, expr, weld_type=None, dim=None):
        '''Initialize a LazyOpResult.
        Args:
            expr: The expression or concrete value.
            weld_type: Optional type info for backends.
            dim: Optional dimensionality information.
        '''
        self.expr = expr
        self.weld_type = weld_type
        self.dim = dim

    def _try_evaluate(self, obj, kwargs):
        # Attempt calling obj.evaluate with descending kwargs compatibility
        if not hasattr(obj, ""evaluate""):
            return obj

        evaluate = getattr(obj, ""evaluate"")
        try:
            return evaluate(**kwargs)
        except TypeError:
            # Retry with subsets of kwargs for broader compatibility
            order = [
                (""verbose"", ""decode"", ""passes"", ""num_threads"", ""apply_experimental_transforms""),
                (""verbose"", ""decode"", ""passes"", ""num_threads""),
                (""verbose"", ""decode"", ""passes""),
                (""verbose"", ""decode""),
                (""decode"",),
                tuple(),  # no kwargs
            ]
            for keys in order:
                sub_kwargs = {k: kwargs[k] for k in keys if k in kwargs}
                try:
                    return evaluate(**sub_kwargs)
                except TypeError:
                    continue
            # Final attempt: call without kwargs
            return evaluate()

    def _maybe_decode(self, value, decode):
        if not decode:
            return value

        # If value is another LazyOpResult, evaluate it fully
        if isinstance(value, LazyOpResult):
            return value.evaluate(decode=decode)

        # Common decoding methods
        for attr in (""to_numpy"", ""to_ndarray"", ""to_list"", ""tolist"", ""numpy""):
            if hasattr(value, attr) and callable(getattr(value, attr)):
                try:
                    return getattr(value, attr)()
                except Exception:
                    pass

        # Numpy buffer protocol support (array interface)
        if hasattr(value, ""__array__""):
            try:
                import numpy as np  # optional
                return np.array(value)
            except Exception:
                return value

        return value

    def evaluate(self, verbose=True, decode=True, passes=None, num_threads=1, apply_experimental_transforms=False):
        '''Evaluate the underlying expression and optionally decode the result.
        Args:
            verbose (bool, optional): Verbose evaluation (if supported by backend).
            decode (bool, optional): Attempt to convert backend result to a Python/native object.
            passes: Optional optimization passes for backend.
            num_threads (int, optional): Number of threads for evaluation (if supported).
            apply_experimental_transforms (bool, optional): Enable experimental transforms (if supported).
        Returns:
            The evaluated (and optionally decoded) result.
        '''
        # If already a concrete value
        if isinstance(self.expr, LazyOpResult):
            result = self.expr.evaluate(
                verbose=verbose,
                decode=decode,
                passes=passes,
                num_threads=num_threads,
                apply_experimental_transforms=apply_experimental_transforms,
            )
            return result

        # If expr is a callable, call it without assumptions
        if callable(self.expr) and not hasattr(self.expr, ""evaluate""):
            try:
                result = self.expr()
            except TypeError:
                # If it needs context, give best-effort: pass known kwargs if accepted
                result = self.expr
        else:
            # Try backend-style evaluate
            kwargs = {
                ""verbose"": verbose,
                ""decode"": decode,
                ""passes"": passes,
                ""num_threads"": num_threads,
                ""apply_experimental_transforms"": apply_experimental_transforms,
            }
            result = self._try_evaluate(self.expr, kwargs)

        # Optionally decode the result into a native object
        return self._maybe_decode(result, decode)

    def __repr__(self):
        cls = self.__class__.__name__
        return f""{cls}(expr={type(self.expr).__name__}, weld_type={self.weld_type}, dim={self.dim})"""
113819,abseil/abseil-py,abseil_abseil-py/absl/app.py,absl.app.ExceptionHandler,"class ExceptionHandler:
    """"""Base exception handler from which other may inherit.""""""

    def wants(self, exc):
        """"""Returns whether this handler wants to handle the exception or not.

        This base class returns True for all exceptions by default. Override in
        subclass if it wants to be more selective.

        Args:
          exc: Exception, the current exception.
        """"""
        del exc
        return True

    def handle(self, exc):
        """"""Do something with the current exception.

        Args:
          exc: Exception, the current exception

        This method must be overridden.
        """"""
        raise NotImplementedError()","class ExceptionHandler:
    '''Base exception handler from which other may inherit.'''

    def wants(self, exc):
        '''Returns whether this handler wants to handle the exception or not.
        This base class returns True for all exceptions by default. Override in
        subclass if it wants to be more selective.
        Args:
          exc: Exception, the current exception.
        '''
        pass

    def handle(self, exc):
        '''Do something with the current exception.
        Args:
          exc: Exception, the current exception
        This method must be overridden.
        '''
        pass",snippet_33,"class ExceptionHandler:
    '''Base exception handler from which other may inherit.'''

    def wants(self, exc):
        '''Returns whether this handler wants to handle the exception or not.
        This base class returns True for all exceptions by default. Override in
        subclass if it wants to be more selective.
        Args:
          exc: Exception, the current exception.
        '''
        return True

    def handle(self, exc):
        '''Do something with the current exception.
        Args:
          exc: Exception, the current exception
        This method must be overridden.
        '''
        raise NotImplementedError(""Subclasses must implement the handle method."")"
116413,QuantEcon/QuantEcon.py,quantecon/util/timing.py,quantecon.util.timing.__Timer__,"import time
import numpy as np

class __Timer__:
    """"""Computes elapsed time, between tic, tac, and toc.

    Methods
    -------
    tic :
        Resets timer.
    toc :
        Returns and prints time elapsed since last tic().
    tac :
        Returns and prints time elapsed since last
             tic(), tac() or toc() whichever occured last.
    loop_timer :
        Returns and prints the total and average time elapsed for n runs
        of a given function.

    """"""
    start = None
    last = None

    def tic(self):
        """"""
        Save time for future use with `tac()` or `toc()`.

        Returns
        -------
        None
            This function doesn't return a value.
        """"""
        t = time.time()
        self.start = t
        self.last = t

    def tac(self, verbose=True, digits=2):
        """"""
        Return and print elapsed time since last `tic()`, `tac()`, or
        `toc()`.

        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.

        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.

        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`, `tac()`, or `toc()`.

        """"""
        if self.start is None:
            raise Exception('tac() without tic()')
        t = time.time()
        elapsed = t - self.last
        self.last = t
        if verbose:
            m, s = divmod(elapsed, 60)
            h, m = divmod(m, 60)
            print('TAC: Elapsed: %d:%02d:%0d.%0*d' % (h, m, s, digits, s % 1 * 10 ** digits))
        return elapsed

    def toc(self, verbose=True, digits=2):
        """"""
        Return and print time elapsed since last `tic()`.

        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.

        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.

        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`.

        """"""
        if self.start is None:
            raise Exception('toc() without tic()')
        t = time.time()
        self.last = t
        elapsed = t - self.start
        if verbose:
            m, s = divmod(elapsed, 60)
            h, m = divmod(m, 60)
            print('TOC: Elapsed: %d:%02d:%0d.%0*d' % (h, m, s, digits, s % 1 * 10 ** digits))
        return elapsed

    def loop_timer(self, n, function, args=None, verbose=True, digits=2, best_of=3):
        """"""
        Return and print the total and average time elapsed for n runs
        of function.

        Parameters
        ----------
        n : scalar(int)
            Number of runs.

        function : function
            Function to be timed.

        args : list, optional(default=None)
            Arguments of the function.

        verbose : bool, optional(default=True)
            If True, then prints average time.

        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.

        best_of : scalar(int), optional(default=3)
            Average time over best_of runs.

        Returns
        -------
        average_time : scalar(float)
            Average time elapsed for n runs of function.

        average_of_best : scalar(float)
            Average of best_of times for n runs of function.

        """"""
        tic()
        all_times = np.empty(n)
        for run in range(n):
            if hasattr(args, '__iter__'):
                function(*args)
            elif args is None:
                function()
            else:
                function(args)
            all_times[run] = tac(verbose=False, digits=digits)
        elapsed = toc(verbose=False, digits=digits)
        m, s = divmod(elapsed, 60)
        h, m = divmod(m, 60)
        print('Total run time: %d:%02d:%0d.%0*d' % (h, m, s, digits, s % 1 * 10 ** digits))
        average_time = all_times.mean()
        average_of_best = np.sort(all_times)[:best_of].mean()
        if verbose:
            m, s = divmod(average_time, 60)
            h, m = divmod(m, 60)
            print('Average time for %d runs: %d:%02d:%0d.%0*d' % (n, h, m, s, digits, s % 1 * 10 ** digits))
            m, s = divmod(average_of_best, 60)
            h, m = divmod(m, 60)
            print('Average of %d best times: %d:%02d:%0d.%0*d' % (best_of, h, m, s, digits, s % 1 * 10 ** digits))
        return (average_time, average_of_best)","
class __Timer__:
    '''Computes elapsed time, between tic, tac, and toc.
    Methods
    -------
    tic :
        Resets timer.
    toc :
        Returns and prints time elapsed since last tic().
    tac :
        Returns and prints time elapsed since last
             tic(), tac() or toc() whichever occured last.
    loop_timer :
        Returns and prints the total and average time elapsed for n runs
        of a given function.
    '''

    def tic(self):
        '''
        Save time for future use with `tac()` or `toc()`.
        Returns
        -------
        None
            This function doesn't return a value.
        '''
        pass

    def tac(self, verbose=True, digits=2):
        '''
        Return and print elapsed time since last `tic()`, `tac()`, or
        `toc()`.
        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.
        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.
        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`, `tac()`, or `toc()`.
        '''
        pass

    def toc(self, verbose=True, digits=2):
        '''
        Return and print time elapsed since last `tic()`.
        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.
        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.
        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`.
        '''
        pass

    def loop_timer(self, n, function, args=None, verbose=True, digits=2, best_of=3):
        '''
        Return and print the total and average time elapsed for n runs
        of function.
        Parameters
        ----------
        n : scalar(int)
            Number of runs.
        function : function
            Function to be timed.
        args : list, optional(default=None)
            Arguments of the function.
        verbose : bool, optional(default=True)
            If True, then prints average time.
        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.
        best_of : scalar(int), optional(default=3)
            Average time over best_of runs.
        Returns
        -------
        average_time : scalar(float)
            Average time elapsed for n runs of function.
        average_of_best : scalar(float)
            Average of best_of times for n runs of function.
        '''
        pass",snippet_35,"import time
from typing import Any, Callable, Iterable, Optional, Tuple


class __Timer__:
    '''Computes elapsed time, between tic, tac, and toc.
    Methods
    -------
    tic :
        Resets timer.
    toc :
        Returns and prints time elapsed since last tic().
    tac :
        Returns and prints time elapsed since last
             tic(), tac() or toc() whichever occured last.
    loop_timer :
        Returns and prints the total and average time elapsed for n runs
        of a given function.
    '''

    def __init__(self) -> None:
        self._t0: Optional[float] = None
        self._t_last: Optional[float] = None

    def tic(self):
        '''
        Save time for future use with `tac()` or `toc()`.
        Returns
        -------
        None
            This function doesn't return a value.
        '''
        now = time.perf_counter()
        self._t0 = now
        self._t_last = now

    def tac(self, verbose=True, digits=2):
        '''
        Return and print elapsed time since last `tic()`, `tac()`, or
        `toc()`.
        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.
        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.
        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`, `tac()`, or `toc()`.
        '''
        now = time.perf_counter()
        if self._t_last is None:
            # Auto-initialize if not started
            self.tic()
            elapsed = 0.0
            # Ensure t_last corresponds to 'now' consistently
            self._t_last = now
            self._t0 = now
        else:
            elapsed = now - self._t_last
            self._t_last = now
        if verbose:
            print(f""tac: {elapsed:.{int(digits)}f} s"")
        return elapsed

    def toc(self, verbose=True, digits=2):
        '''
        Return and print time elapsed since last `tic()`.
        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.
        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.
        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`.
        '''
        now = time.perf_counter()
        if self._t0 is None:
            # Auto-initialize if not started
            self.tic()
            elapsed = 0.0
            # After toc, last checkpoint becomes now
            self._t_last = now
            self._t0 = now
        else:
            elapsed = now - self._t0
            self._t_last = now
        if verbose:
            print(f""toc: {elapsed:.{int(digits)}f} s"")
        return elapsed

    def loop_timer(self, n, function, args=None, verbose=True, digits=2, best_of=3):
        '''
        Return and print the total and average time elapsed for n runs
        of function.
        Parameters
        ----------
        n : scalar(int)
            Number of runs.
        function : function
            Function to be timed.
        args : list, optional(default=None)
            Arguments of the function.
        verbose : bool, optional(default=True)
            If True, then prints average time.
        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.
        best_of : scalar(int), optional(default=3)
            Average time over best_of runs.
        Returns
        -------
        average_time : scalar(float)
            Average time elapsed for n runs of function.
        average_of_best : scalar(float)
            Average of best_of times for n runs of function.
        '''
        if n <= 0:
            if verbose:
                print(""Average time: 0.00 s | Best-of average: 0.00 s | Total: 0.00 s"")
            return 0.0, 0.0

        if args is None:
            call_args: Tuple[Any, ...] = ()
        elif isinstance(args, tuple):
            call_args = args
        elif isinstance(args, list):
            call_args = tuple(args)
        else:
            call_args = (args,)

        times = []
        for _ in range(n):
            start = time.perf_counter()
            function(*call_args)
            end = time.perf_counter()
            times.append(end - start)

        total_time = sum(times)
        average_time = total_time / n
        k = max(1, min(int(best_of), n))
        best_times = sorted(times)[:k]
        average_of_best = sum(best_times) / k

        if verbose:
            d = int(digits)
            print(
                f""Total: {total_time:.{d}f} s | ""
                f""Average: {average_time:.{d}f} s | ""
                f""Best-{k} average: {average_of_best:.{d}f} s""
            )

        return average_time, average_of_best"
116421,DerwenAI/pytextrank,DerwenAI_pytextrank/pytextrank/base.py,pytextrank.base.Sentence,"import typing
from spacy.tokens import Doc, Span, Token
from dataclasses import dataclass

@dataclass
class Sentence:
    """"""
A data class representing the distance measure for one sentence.
    """"""
    start: int
    end: int
    sent_id: int
    phrases: typing.Set[int]
    distance: float

    def empty(self) -> bool:
        """"""
Test whether this sentence includes any ranked phrases.

    returns:
`True` if the `phrases` is not empty.
        """"""
        return len(self.phrases) == 0

    def text(self, doc: Doc) -> str:
        """"""
Accessor for the text slice of the `spaCy` [`Doc`](https://spacy.io/api/doc)
document represented by this sentence.

    doc:
source document

    returns:
the sentence text
        """"""
        return doc[self.start:self.end]","@dataclass
class Sentence:
    '''
A data class representing the distance measure for one sentence.
    '''

    def empty(self) -> bool:
        '''
Test whether this sentence includes any ranked phrases.
    returns:
`True` if the `phrases` is not empty.
        '''
        pass

    def text(self, doc: Doc) -> str:
        '''
Accessor for the text slice of the `spaCy` [`Doc`](https://spacy.io/api/doc)
document represented by this sentence.
    doc:
source document
    returns:
the sentence text
        '''
        pass",snippet_36,"from dataclasses import dataclass, field
from typing import Any, List
from spacy.tokens import Doc

@dataclass
class Sentence:
    '''
A data class representing the distance measure for one sentence.
    '''
    start: int
    end: int
    phrases: List[Any] = field(default_factory=list)

    def empty(self) -> bool:
        '''
Test whether this sentence includes any ranked phrases.
    returns:
`True` if the `phrases` is not empty.
        '''
        return len(self.phrases) > 0

    def text(self, doc: Doc) -> str:
        '''
Accessor for the text slice of the `spaCy` [`Doc`](https://spacy.io/api/doc)
document represented by this sentence.
    doc:
source document
    returns:
the sentence text
        '''
        return doc[self.start:self.end].text"
128871,arviz-devs/arviz,arviz-devs_arviz/arviz/utils.py,arviz.utils.interactive_backend,"import matplotlib.pyplot as plt

class interactive_backend:
    """"""Context manager to change backend temporarily in ipython sesson.

    It uses ipython magic to change temporarily from the ipython inline backend to
    an interactive backend of choice. It cannot be used outside ipython sessions nor
    to change backends different than inline -> interactive.

    Notes
    -----
    The first time ``interactive_backend`` context manager is called, any of the available
    interactive backends can be chosen. The following times, this same backend must be used
    unless the kernel is restarted.

    Parameters
    ----------
    backend : str, optional
        Interactive backend to use. It will be passed to ``%matplotlib`` magic, refer to
        its docs to see available options.

    Examples
    --------
    Inside an ipython session (i.e. a jupyter notebook) with the inline backend set:

    .. code::

        >>> import arviz as az
        >>> idata = az.load_arviz_data(""centered_eight"")
        >>> az.plot_posterior(idata) # inline
        >>> with az.interactive_backend():
        ...     az.plot_density(idata) # interactive
        >>> az.plot_trace(idata) # inline

    """"""

    def __init__(self, backend=''):
        """"""Initialize context manager.""""""
        try:
            from IPython import get_ipython
        except ImportError as err:
            raise ImportError(f'The exception below was risen while importing Ipython, this context manager can only be used inside ipython sessions:\n{err}') from err
        self.ipython = get_ipython()
        if self.ipython is None:
            raise EnvironmentError('This context manager can only be used inside ipython sessions')
        self.ipython.magic(f'matplotlib {backend}')

    def __enter__(self):
        """"""Enter context manager.""""""
        return self

    def __exit__(self, exc_type, exc_value, exc_tb):
        """"""Exit context manager.""""""
        plt.show(block=True)
        self.ipython.magic('matplotlib inline')","
class interactive_backend:
    '''Context manager to change backend temporarily in ipython sesson.
    It uses ipython magic to change temporarily from the ipython inline backend to
    an interactive backend of choice. It cannot be used outside ipython sessions nor
    to change backends different than inline -> interactive.
    Notes
    -----
    The first time ``interactive_backend`` context manager is called, any of the available
    interactive backends can be chosen. The following times, this same backend must be used
    unless the kernel is restarted.
    Parameters
    ----------
    backend : str, optional
        Interactive backend to use. It will be passed to ``%matplotlib`` magic, refer to
        its docs to see available options.
    Examples
    --------
    Inside an ipython session (i.e. a jupyter notebook) with the inline backend set:
    .. code::
        >>> import arviz as az
        >>> idata = az.load_arviz_data(""centered_eight"")
        >>> az.plot_posterior(idata) # inline
        >>> with az.interactive_backend():
        ...     az.plot_density(idata) # interactive
        >>> az.plot_trace(idata) # inline
    '''

    def __init__(self, backend=''):
        '''Initialize context manager.'''
        pass

    def __enter__(self):
        '''Enter context manager.'''
        pass

    def __exit__(self, exc_type, exc_value, exc_tb):
        '''Exit context manager.'''
        pass",snippet_40,"class interactive_backend:
    '''Context manager to change backend temporarily in ipython sesson.
    It uses ipython magic to change temporarily from the ipython inline backend to
    an interactive backend of choice. It cannot be used outside ipython sessions nor
    to change backends different than inline -> interactive.
    Notes
    -----
    The first time ``interactive_backend`` context manager is called, any of the available
    interactive backends can be chosen. The following times, this same backend must be used
    unless the kernel is restarted.
    Parameters
    ----------
    backend : str, optional
        Interactive backend to use. It will be passed to ``%matplotlib`` magic, refer to
        its docs to see available options.
    Examples
    --------
    Inside an ipython session (i.e. a jupyter notebook) with the inline backend set:
    .. code::
        >>> import arviz as az
        >>> idata = az.load_arviz_data(""centered_eight"")
        >>> az.plot_posterior(idata) # inline
        >>> with az.interactive_backend():
        ...     az.plot_density(idata) # interactive
        >>> az.plot_trace(idata) # inline
    '''

    _chosen_backend = None

    def __init__(self, backend=''):
        '''Initialize context manager.'''
        self._requested_backend = backend
        self._ip = None
        self._active = False

    def __enter__(self):
        '''Enter context manager.'''
        try:
            from IPython import get_ipython
        except Exception as e:
            raise RuntimeError(""interactive_backend can only be used within an IPython session"") from e

        ip = get_ipython()
        if ip is None:
            raise RuntimeError(""interactive_backend can only be used within an IPython session"")

        try:
            import matplotlib as mpl
        except Exception as e:
            raise RuntimeError(""matplotlib is required to use interactive_backend"") from e

        current_backend = str(mpl.get_backend()).lower()

        if ""inline"" not in current_backend:
            raise RuntimeError(
                ""interactive_backend can only switch from the inline backend to an interactive backend""
            )

        # Determine the backend to use
        if interactive_backend._chosen_backend is None:
            if not self._requested_backend:
                raise ValueError(
                    ""First use of interactive_backend requires specifying an interactive backend ""
                    ""(e.g., backend='qt', 'qt5', 'tk', 'osx', 'widget').""
                )
            interactive_backend._chosen_backend = str(self._requested_backend)
        else:
            if self._requested_backend and self._requested_backend != interactive_backend._chosen_backend:
                raise RuntimeError(
                    f""interactive_backend must reuse the initially chosen backend ""
                    f""('{interactive_backend._chosen_backend}'); got '{self._requested_backend}'""
                )

        # Switch to the interactive backend via IPython magic
        try:
            ip.run_line_magic('matplotlib', interactive_backend._chosen_backend)
        except Exception as e:
            raise RuntimeError(
                f""Failed to switch to interactive backend '{interactive_backend._chosen_backend}' via %matplotlib""
            ) from e

        self._ip = ip
        self._active = True
        return self

    def __exit__(self, exc_type, exc_value, exc_tb):
        '''Exit context manager.'''
        if self._active and self._ip is not None:
            try:
                self._ip.run_line_magic('matplotlib', 'inline')
            except Exception:
                pass
        self._active = False
        self._ip = None
        return False"
128872,arviz-devs/arviz,arviz-devs_arviz/arviz/utils.py,arviz.utils.lazy_property,"import functools

class lazy_property:
    """"""Used to load numba first time it is needed.""""""

    def __init__(self, fget):
        """"""Lazy load a property with `fget`.""""""
        self.fget = fget
        functools.update_wrapper(self, fget)

    def __get__(self, obj, cls):
        """"""Call the function, set the attribute.""""""
        if obj is None:
            return self
        value = self.fget(obj)
        setattr(obj, self.fget.__name__, value)
        return value","
class lazy_property:
    '''Used to load numba first time it is needed.'''

    def __init__(self, fget):
        '''Lazy load a property with `fget`.'''
        pass

    def __get__(self, obj, cls):
        '''Call the function, set the attribute.'''
        pass",snippet_41,"class lazy_property:
    '''Used to load numba first time it is needed.'''

    def __init__(self, fget):
        '''Lazy load a property with `fget`.'''
        self.fget = fget
        self.name = fget.__name__
        self.__doc__ = getattr(fget, '__doc__')

    def __get__(self, obj, cls):
        '''Call the function, set the attribute.'''
        if obj is None:
            return self
        if self.name in obj.__dict__:
            return obj.__dict__[self.name]
        value = self.fget(obj)
        setattr(obj, self.name, value)
        return value"
129413,dtmilano/AndroidViewClient,dtmilano_AndroidViewClient/src/com/dtmilano/android/adb/adbclient.py,com.dtmilano.android.adb.adbclient.WifiManager,"import re
import sys

class WifiManager:
    """"""
    Simulates Android WifiManager.

    @see: http://developer.android.com/reference/android/net/wifi/WifiManager.html
    """"""
    WIFI_STATE_DISABLING = 0
    WIFI_STATE_DISABLED = 1
    WIFI_STATE_ENABLING = 2
    WIFI_STATE_ENABLED = 3
    WIFI_STATE_UNKNOWN = 4
    WIFI_IS_ENABLED_RE = re.compile('Wi-Fi is enabled')
    WIFI_IS_DISABLED_RE = re.compile('Wi-Fi is disabled')

    def __init__(self, device):
        """"""
        Constructor.
        :param device:
        :type device:
        """"""
        self.device = device

    def getWifiState(self):
        """"""
        Gets the Wi-Fi enabled state.

        @return: One of WIFI_STATE_DISABLED, WIFI_STATE_DISABLING, WIFI_STATE_ENABLED, WIFI_STATE_ENABLING, WIFI_STATE_UNKNOWN
        """"""
        result = self.device.shell('dumpsys wifi')
        if result:
            state = result.splitlines()[0]
            if self.WIFI_IS_ENABLED_RE.match(state):
                return self.WIFI_STATE_ENABLED
            elif self.WIFI_IS_DISABLED_RE.match(state):
                return self.WIFI_STATE_DISABLED
        print('UNKNOWN WIFI STATE:', state, file=sys.stderr)
        return self.WIFI_STATE_UNKNOWN","
class WifiManager:
    '''
    Simulates Android WifiManager.
    @see: http://developer.android.com/reference/android/net/wifi/WifiManager.html
    '''

    def __init__(self, device):
        '''
        Constructor.
        :param device:
        :type device:
        '''
        pass

    def getWifiState(self):
        '''
        Gets the Wi-Fi enabled state.
        @return: One of WIFI_STATE_DISABLED, WIFI_STATE_DISABLING, WIFI_STATE_ENABLED, WIFI_STATE_ENABLING, WIFI_STATE_UNKNOWN
        '''
        pass",snippet_42,"class WifiManager:
    '''
    Simulates Android WifiManager.
    @see: http://developer.android.com/reference/android/net/wifi/WifiManager.html
    '''

    WIFI_STATE_DISABLING = 0
    WIFI_STATE_DISABLED = 1
    WIFI_STATE_ENABLING = 2
    WIFI_STATE_ENABLED = 3
    WIFI_STATE_UNKNOWN = 4

    def __init__(self, device):
        '''
        Constructor.
        :param device:
        :type device:
        '''
        self.device = device

    def _normalize_state(self, value):
        if value is None:
            return self.WIFI_STATE_UNKNOWN

        # Numeric direct mapping
        try:
            ivalue = int(value)
            if ivalue in {
                self.WIFI_STATE_DISABLED,
                self.WIFI_STATE_DISABLING,
                self.WIFI_STATE_ENABLED,
                self.WIFI_STATE_ENABLING,
                self.WIFI_STATE_UNKNOWN,
            }:
                return ivalue
        except (ValueError, TypeError):
            pass

        # Boolean-like
        if isinstance(value, bool):
            return self.WIFI_STATE_ENABLED if value else self.WIFI_STATE_DISABLED

        # String mapping
        sval = str(value).strip().lower()
        mapping = {
            ""0"": self.WIFI_STATE_DISABLED,
            ""1"": self.WIFI_STATE_ENABLED,
            ""enabled"": self.WIFI_STATE_ENABLED,
            ""disabled"": self.WIFI_STATE_DISABLED,
            ""enabling"": self.WIFI_STATE_ENABLING,
            ""disabling"": self.WIFI_STATE_DISABLING,
            ""unknown"": self.WIFI_STATE_UNKNOWN,
            ""on"": self.WIFI_STATE_ENABLED,
            ""off"": self.WIFI_STATE_DISABLED,
            ""true"": self.WIFI_STATE_ENABLED,
            ""false"": self.WIFI_STATE_DISABLED,
        }
        return mapping.get(sval, self.WIFI_STATE_UNKNOWN)

    def getWifiState(self):
        '''
        Gets the Wi-Fi enabled state.
        @return: One of WIFI_STATE_DISABLED, WIFI_STATE_DISABLING, WIFI_STATE_ENABLED, WIFI_STATE_ENABLING, WIFI_STATE_UNKNOWN
        '''
        # Prefer explicit method if available
        if hasattr(self.device, ""get_wifi_state"") and callable(getattr(self.device, ""get_wifi_state"")):
            try:
                return self._normalize_state(self.device.get_wifi_state())
            except Exception:
                pass

        # Boolean query methods
        if hasattr(self.device, ""is_wifi_enabled"") and callable(getattr(self.device, ""is_wifi_enabled"")):
            try:
                return self._normalize_state(self.device.is_wifi_enabled())
            except Exception:
                pass

        # Attributes commonly used
        for attr in (""wifi_state"", ""wifiEnabled"", ""wifi_enabled"", ""wifiState""):
            if hasattr(self.device, attr):
                try:
                    return self._normalize_state(getattr(self.device, attr))
                except Exception:
                    pass

        # Shell-based query if device exposes a shell method
        # Try Android settings get global wifi_on (returns ""1"" or ""0"")
        if hasattr(self.device, ""shell"") and callable(getattr(self.device, ""shell"")):
            try:
                out = self.device.shell(""settings get global wifi_on"")
                if out is not None:
                    return self._normalize_state(str(out).strip())
            except Exception:
                pass
            # Fallback to svc wifi (some implementations may return status text)
            try:
                out = self.device.shell(""svc wifi status"")
                if out:
                    text = str(out).lower()
                    if ""enabled"" in text or ""on"" in text:
                        return self.WIFI_STATE_ENABLED
                    if ""disabled"" in text or ""off"" in text:
                        return self.WIFI_STATE_DISABLED
            except Exception:
                pass

        return self.WIFI_STATE_UNKNOWN"
129620,neovim/pynvim,neovim_pynvim/pynvim/msgpack_rpc/async_session.py,pynvim.msgpack_rpc.async_session.Response,"from pynvim.msgpack_rpc.msgpack_stream import MsgpackStream

class Response:
    """"""Response to a msgpack-rpc request that came from Nvim.

    When Nvim sends a msgpack-rpc request, an instance of this class is
    created for remembering state required to send a response.
    """"""

    def __init__(self, msgpack_stream: MsgpackStream, request_id: int):
        """"""Initialize the Response instance.""""""
        self._msgpack_stream = msgpack_stream
        self._request_id = request_id

    def send(self, value, error=False):
        """"""Send the response.

        If `error` is True, it will be sent as an error.
        """"""
        if error:
            resp = [1, self._request_id, value, None]
        else:
            resp = [1, self._request_id, None, value]
        debug('sending response to request %d: %s', self._request_id, resp)
        self._msgpack_stream.send(resp)","
class Response:
    '''Response to a msgpack-rpc request that came from Nvim.
    When Nvim sends a msgpack-rpc request, an instance of this class is
    created for remembering state required to send a response.
    '''

    def __init__(self, msgpack_stream: MsgpackStream, request_id: int):
        '''Initialize the Response instance.'''
        pass

    def send(self, value, error=False):
        '''Send the response.
        If `error` is True, it will be sent as an error.
        '''
        pass",snippet_43,"class Response:
    '''Response to a msgpack-rpc request that came from Nvim.
    When Nvim sends a msgpack-rpc request, an instance of this class is
    created for remembering state required to send a response.
    '''

    def __init__(self, msgpack_stream, request_id: int):
        '''Initialize the Response instance.'''
        self._msgpack_stream = msgpack_stream
        self._request_id = request_id
        self._sent = False

    def send(self, value, error=False):
        '''Send the response.
        If `error` is True, it will be sent as an error.
        '''
        if self._sent:
            return
        if error:
            msg = [1, self._request_id, value, None]
        else:
            msg = [1, self._request_id, None, value]
        self._msgpack_stream.send(msg)
        self._sent = True"
129967,sendgrid/sendgrid-python,sendgrid_sendgrid-python/sendgrid/helpers/eventwebhook/__init__.py,sendgrid.helpers.eventwebhook.EventWebhook,"from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.asymmetric import ec
from cryptography.exceptions import InvalidSignature
from cryptography.hazmat.primitives.serialization import load_pem_public_key
import base64

class EventWebhook:
    """"""
    This class allows you to use the Event Webhook feature. Read the docs for
    more details: https://sendgrid.com/docs/for-developers/tracking-events/event
    """"""

    def __init__(self, public_key=None):
        """"""
        Construct the Event Webhook verifier object
        :param public_key: verification key under Mail Settings
        :type public_key: string
        """"""
        self.public_key = self.convert_public_key_to_ecdsa(public_key) if public_key else public_key

    def convert_public_key_to_ecdsa(self, public_key):
        """"""
        Convert the public key string to an EllipticCurvePublicKey object.

        :param public_key: verification key under Mail Settings
        :type public_key string
        :return: An EllipticCurvePublicKey object using the ECDSA algorithm
        :rtype cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey
        """"""
        pem_key = '-----BEGIN PUBLIC KEY-----\n' + public_key + '\n-----END PUBLIC KEY-----'
        return load_pem_public_key(pem_key.encode('utf-8'))

    def verify_signature(self, payload, signature, timestamp, public_key=None):
        """"""
        Verify signed event webhook requests.

        :param payload: event payload in the request body
        :type payload: string
        :param signature: value obtained from the 'X-Twilio-Email-Event-Webhook-Signature' header
        :type signature: string
        :param timestamp: value obtained from the 'X-Twilio-Email-Event-Webhook-Timestamp' header
        :type timestamp: string
        :param public_key: elliptic curve public key
        :type public_key: cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey
        :return: true or false if signature is valid
        """"""
        timestamped_payload = (timestamp + payload).encode('utf-8')
        decoded_signature = base64.b64decode(signature)
        key = public_key or self.public_key
        try:
            key.verify(decoded_signature, timestamped_payload, ec.ECDSA(hashes.SHA256()))
            return True
        except InvalidSignature:
            return False","
class EventWebhook:
    '''
    This class allows you to use the Event Webhook feature. Read the docs for
    more details: https://sendgrid.com/docs/for-developers/tracking-events/event
    '''

    def __init__(self, public_key=None):
        '''
        Construct the Event Webhook verifier object
        :param public_key: verification key under Mail Settings
        :type public_key: string
        '''
        pass

    def convert_public_key_to_ecdsa(self, public_key):
        '''
        Convert the public key string to an EllipticCurvePublicKey object.
        :param public_key: verification key under Mail Settings
        :type public_key string
        :return: An EllipticCurvePublicKey object using the ECDSA algorithm
        :rtype cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey
        '''
        pass

    def verify_signature(self, payload, signature, timestamp, public_key=None):
        '''
        Verify signed event webhook requests.
        :param payload: event payload in the request body
        :type payload: string
        :param signature: value obtained from the 'X-Twilio-Email-Event-Webhook-Signature' header
        :type signature: string
        :param timestamp: value obtained from the 'X-Twilio-Email-Event-Webhook-Timestamp' header
        :type timestamp: string
        :param public_key: elliptic curve public key
        :type public_key: cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey
        :return: true or false if signature is valid
        '''
        pass",snippet_44,"import base64
from typing import Optional, Union

from cryptography.exceptions import InvalidSignature
from cryptography.hazmat.primitives import hashes, serialization
from cryptography.hazmat.primitives.asymmetric import ec
from cryptography.hazmat.primitives.asymmetric.ec import EllipticCurvePublicKey


class EventWebhook:
    '''
    This class allows you to use the Event Webhook feature. Read the docs for
    more details: https://sendgrid.com/docs/for-developers/tracking-events/event
    '''

    def __init__(self, public_key: Optional[Union[str, EllipticCurvePublicKey]] = None):
        '''
        Construct the Event Webhook verifier object
        :param public_key: verification key under Mail Settings
        :type public_key: string
        '''
        if isinstance(public_key, EllipticCurvePublicKey):
            self.public_key = public_key
        elif isinstance(public_key, str):
            self.public_key = self.convert_public_key_to_ecdsa(public_key)
        elif public_key is None:
            self.public_key = None
        else:
            raise TypeError(""public_key must be a string, EllipticCurvePublicKey, or None"")

    def convert_public_key_to_ecdsa(self, public_key):
        '''
        Convert the public key string to an EllipticCurvePublicKey object.
        :param public_key: verification key under Mail Settings
        :type public_key string
        :return: An EllipticCurvePublicKey object using the ECDSA algorithm
        :rtype cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey
        '''
        if isinstance(public_key, EllipticCurvePublicKey):
            return public_key
        if not isinstance(public_key, (str, bytes)):
            raise TypeError(""public_key must be a PEM string or bytes"")

        pem_bytes = public_key.encode(""utf-8"") if isinstance(public_key, str) else public_key
        key = serialization.load_pem_public_key(pem_bytes)
        if not isinstance(key, EllipticCurvePublicKey):
            raise ValueError(""Provided public key is not an EllipticCurvePublicKey"")
        return key

    def verify_signature(self, payload, signature, timestamp, public_key=None):
        '''
        Verify signed event webhook requests.
        :param payload: event payload in the request body
        :type payload: string
        :param signature: value obtained from the 'X-Twilio-Email-Event-Webhook-Signature' header
        :type signature: string
        :param timestamp: value obtained from the 'X-Twilio-Email-Event-Webhook-Timestamp' header
        :type timestamp: string
        :param public_key: elliptic curve public key
        :type public_key: cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey
        :return: true or false if signature is valid
        '''
        key = public_key or self.public_key
        if key is None:
            raise ValueError(""A public key must be provided either at initialization or as an argument."")

        if isinstance(key, (str, bytes)):
            key = self.convert_public_key_to_ecdsa(key)
        elif not isinstance(key, EllipticCurvePublicKey):
            raise TypeError(""public_key must be an EllipticCurvePublicKey, PEM string, or bytes"")

        if isinstance(timestamp, bytes):
            ts_bytes = timestamp
        elif isinstance(timestamp, str):
            ts_bytes = timestamp.encode(""utf-8"")
        else:
            raise TypeError(""timestamp must be a string or bytes"")

        if isinstance(payload, bytes):
            payload_bytes = payload
        elif isinstance(payload, str):
            payload_bytes = payload.encode(""utf-8"")
        else:
            raise TypeError(""payload must be a string or bytes"")

        if isinstance(signature, str):
            try:
                sig_bytes = base64.b64decode(signature, validate=True)
            except Exception:
                # try URL-safe base64 without padding
                try:
                    sig_bytes = base64.urlsafe_b64decode(signature + ""==="")
                except Exception:
                    return False
        elif isinstance(signature, bytes):
            sig_bytes = signature
        else:
            raise TypeError(""signature must be a string or bytes"")

        message = ts_bytes + payload_bytes

        try:
            key.verify(sig_bytes, message, ec.ECDSA(hashes.SHA256()))
            return True
        except (InvalidSignature, ValueError):
            return False"
131054,SoCo/SoCo,SoCo_SoCo/soco/music_services/token_store.py,soco.music_services.token_store.TokenStoreBase,"class TokenStoreBase:
    """"""Token store base class""""""

    def __init__(self, token_collection='default'):
        """"""Instantiate instance variables

        Args:
            token_collection (str): The name of the token collection to use. This may be
                used to store different token collections for different client programs.
        """"""
        self.token_collection = token_collection

    def save_token_pair(self, music_service_id, household_id, token_pair):
        """"""Save a token value pair (token, key) which is a 2 item sequence""""""
        raise NotImplementedError

    def load_token_pair(self, music_service_id, household_id):
        """"""Load a token pair (token, key) which is a 2 item sequence""""""
        raise NotImplementedError

    def has_token(self, music_service_id, household_id):
        """"""Return True if a token is stored for the music service and household ID""""""
        raise NotImplementedError","class TokenStoreBase:
    '''Token store base class'''

    def __init__(self, token_collection='default'):
        '''Instantiate instance variables
        Args:
            token_collection (str): The name of the token collection to use. This may be
                used to store different token collections for different client programs.
        '''
        pass

    def save_token_pair(self, music_service_id, household_id, token_pair):
        '''Save a token value pair (token, key) which is a 2 item sequence'''
        pass

    def load_token_pair(self, music_service_id, household_id):
        '''Load a token pair (token, key) which is a 2 item sequence'''
        pass

    def has_token(self, music_service_id, household_id):
        '''Return True if a token is stored for the music service and household ID'''
        pass",snippet_45,"class TokenStoreBase:
    '''Token store base class'''

    def __init__(self, token_collection='default'):
        '''Instantiate instance variables
        Args:
            token_collection (str): The name of the token collection to use. This may be
                used to store different token collections for different client programs.
        '''
        self.token_collection = str(token_collection)
        self._tokens = {}

    def save_token_pair(self, music_service_id, household_id, token_pair):
        '''Save a token value pair (token, key) which is a 2 item sequence'''
        try:
            token, key = token_pair
        except Exception as exc:
            raise ValueError(""token_pair must be a 2-item sequence (token, key)"") from exc
        self._tokens[(music_service_id, household_id)] = (token, key)

    def load_token_pair(self, music_service_id, household_id):
        '''Load a token pair (token, key) which is a 2 item sequence'''
        return self._tokens.get((music_service_id, household_id))

    def has_token(self, music_service_id, household_id):
        '''Return True if a token is stored for the music service and household ID'''
        return (music_service_id, household_id) in self._tokens"
131060,SoCo/SoCo,SoCo_SoCo/soco/plugins/sharelink.py,soco.plugins.sharelink.ShareClass,"class ShareClass:
    """"""Base class for supported services.""""""

    def canonical_uri(self, uri):
        """"""Recognize a share link and return its canonical representation.

        Args:
            uri (str): A URI like ""https://tidal.com/browse/album/157273956"".

        Returns:
            str: The canonical URI or None if not recognized.
        """"""
        raise NotImplementedError

    def service_number(self):
        """"""Return the service number.

        Returns:
            int: A number identifying the supported music service.
        """"""
        raise NotImplementedError

    @staticmethod
    def magic():
        """"""Return magic.

        Returns:
            dict: Magic prefix/key/class values for each share type.
        """"""
        return {'album': {'prefix': 'x-rincon-cpcontainer:1004206c', 'key': '00040000', 'class': 'object.container.album.musicAlbum'}, 'episode': {'prefix': '', 'key': '00032020', 'class': 'object.item.audioItem.musicTrack'}, 'track': {'prefix': '', 'key': '00032020', 'class': 'object.item.audioItem.musicTrack'}, 'show': {'prefix': 'x-rincon-cpcontainer:1006206c', 'key': '1006206c', 'class': 'object.container.playlistContainer'}, 'song': {'prefix': '', 'key': '10032020', 'class': 'object.item.audioItem.musicTrack'}, 'playlist': {'prefix': 'x-rincon-cpcontainer:1006206c', 'key': '1006206c', 'class': 'object.container.playlistContainer'}}

    def extract(self, uri):
        """"""Extract the share type and encoded URI from a share link.

        Returns:
            share_type: The shared type, like ""album"" or ""track"".
            encoded_uri: An escaped URI with a service-specific format.
        """"""
        raise NotImplementedError","class ShareClass:
    '''Base class for supported services.'''

    def canonical_uri(self, uri):
        '''Recognize a share link and return its canonical representation.
        Args:
            uri (str): A URI like ""https://tidal.com/browse/album/157273956"".
        Returns:
            str: The canonical URI or None if not recognized.
        '''
        pass

    def service_number(self):
        '''Return the service number.
        Returns:
            int: A number identifying the supported music service.
        '''
        pass
    @staticmethod
    def magic():
        '''Return magic.
        Returns:
            dict: Magic prefix/key/class values for each share type.
        '''
        pass

    def extract(self, uri):
        '''Extract the share type and encoded URI from a share link.
        Returns:
            share_type: The shared type, like ""album"" or ""track"".
            encoded_uri: An escaped URI with a service-specific format.
        '''
        pass",snippet_46,"import re
from urllib.parse import urlsplit, urlunsplit, quote, unquote


class ShareClass:
    '''Base class for supported services.'''

    def canonical_uri(self, uri):
        '''Recognize a share link and return its canonical representation.
        Args:
            uri (str): A URI like ""https://tidal.com/browse/album/157273956"".
        Returns:
            str: The canonical URI or None if not recognized.
        '''
        if not uri or not isinstance(uri, str):
            return None

        parts = urlsplit(uri.strip())
        if not parts.scheme or not parts.netloc:
            return None

        scheme = parts.scheme.lower()
        netloc = parts.netloc.lower()

        # Strip default ports
        host, sep, port = netloc.partition(':')
        if (scheme == 'http' and port == '80') or (scheme == 'https' and port == '443'):
            netloc = host

        # Normalize path: remove multiple slashes, strip trailing slash, keep leading slash
        path = unquote(parts.path or '')
        path = re.sub(r'/+', '/', path)
        if path != '/' and path.endswith('/'):
            path = path[:-1]
        # Ensure it starts with a single slash if non-empty
        if path and not path.startswith('/'):
            path = '/' + path
        path = quote(path, safe='/-._~')

        # Remove query and fragment for canonical
        canonical = urlunsplit((scheme, netloc, path, '', ''))
        return canonical

    def service_number(self):
        '''Return the service number.
        Returns:
            int: A number identifying the supported music service.
        '''
        return 0

    @staticmethod
    def magic():
        '''Return magic.
        Returns:
            dict: Magic prefix/key/class values for each share type.
        '''
        return {}

    def extract(self, uri):
        '''Extract the share type and encoded URI from a share link.
        Returns:
            share_type: The shared type, like ""album"" or ""track"".
            encoded_uri: An escaped URI with a service-specific format.
        '''
        canon = self.canonical_uri(uri)
        if not canon:
            return None, None

        path = urlsplit(canon).path or ''
        segments = [seg for seg in path.split('/') if seg]

        share_type = None
        if segments:
            # Heuristic: pick the last textual segment that isn't purely numeric as the type,
            # otherwise take the preceding one if available.
            if len(segments) >= 2:
                if not segments[-1].isalpha() and segments[-2].isalpha():
                    share_type = segments[-2]
                elif segments[-1].isalpha():
                    share_type = segments[-1]
                else:
                    # fallback to second last if exists
                    share_type = segments[-2]
            else:
                share_type = segments[-1]

        encoded_uri = canon
        return share_type, encoded_uri"
131928,mjg59/python-broadlink,mjg59_python-broadlink/broadlink/protocol.py,broadlink.protocol.Datetime,"import time
import datetime as dt

class Datetime:
    """"""Helps to pack and unpack datetime objects for the Broadlink protocol.""""""

    @staticmethod
    def pack(datetime: dt.datetime) -> bytes:
        """"""Pack the timestamp to be sent over the Broadlink protocol.""""""
        data = bytearray(12)
        utcoffset = int(datetime.utcoffset().total_seconds() / 3600)
        data[:4] = utcoffset.to_bytes(4, 'little', signed=True)
        data[4:6] = datetime.year.to_bytes(2, 'little')
        data[6] = datetime.minute
        data[7] = datetime.hour
        data[8] = int(datetime.strftime('%y'))
        data[9] = datetime.isoweekday()
        data[10] = datetime.day
        data[11] = datetime.month
        return data

    @staticmethod
    def unpack(data: bytes) -> dt.datetime:
        """"""Unpack a timestamp received over the Broadlink protocol.""""""
        utcoffset = int.from_bytes(data[0:4], 'little', signed=True)
        year = int.from_bytes(data[4:6], 'little')
        minute = data[6]
        hour = data[7]
        subyear = data[8]
        isoweekday = data[9]
        day = data[10]
        month = data[11]
        tz_info = dt.timezone(dt.timedelta(hours=utcoffset))
        datetime = dt.datetime(year, month, day, hour, minute, 0, 0, tz_info)
        if datetime.isoweekday() != isoweekday:
            raise ValueError('isoweekday does not match')
        if int(datetime.strftime('%y')) != subyear:
            raise ValueError('subyear does not match')
        return datetime

    @staticmethod
    def now() -> dt.datetime:
        """"""Return the current date and time with timezone info.""""""
        tz_info = dt.timezone(dt.timedelta(seconds=-time.timezone))
        return dt.datetime.now(tz_info)","
class Datetime:
    '''Helps to pack and unpack datetime objects for the Broadlink protocol.'''
    @staticmethod
    def pack(datetime: dt.datetime) -> bytes:
        '''Pack the timestamp to be sent over the Broadlink protocol.'''
        pass
    @staticmethod
    def unpack(data: bytes) -> dt.datetime:
        '''Unpack a timestamp received over the Broadlink protocol.'''
        pass
    @staticmethod
    def now() -> dt.datetime:
        '''Return the current date and time with timezone info.'''
        pass",snippet_48,"import datetime as dt
import struct


class Datetime:
    '''Helps to pack and unpack datetime objects for the Broadlink protocol.'''

    @staticmethod
    def pack(datetime: dt.datetime) -> bytes:
        '''Pack the timestamp to be sent over the Broadlink protocol.'''
        if datetime.tzinfo is None:
            datetime = datetime.astimezone()
        else:
            datetime = datetime.astimezone(datetime.tzinfo)
        datetime = datetime.replace(microsecond=0)

        tz_offset_minutes = int(datetime.utcoffset().total_seconds() // 60)

        return struct.pack(
            ""<HBBBBBBh"",
            datetime.year,
            datetime.month,
            datetime.day,
            datetime.hour,
            datetime.minute,
            datetime.second,
            datetime.isoweekday() % 7,  # 0=Sunday, 1=Monday, ... 6=Saturday
            tz_offset_minutes,
        )

    @staticmethod
    def unpack(data: bytes) -> dt.datetime:
        '''Unpack a timestamp received over the Broadlink protocol.'''
        year, month, day, hour, minute, second, weekday, tz_offset_minutes = struct.unpack(
            ""<HBBBBBBh"", data
        )
        tz = dt.timezone(dt.timedelta(minutes=tz_offset_minutes))
        return dt.datetime(year, month, day, hour, minute, second, tzinfo=tz)

    @staticmethod
    def now() -> dt.datetime:
        '''Return the current date and time with timezone info.'''
        return dt.datetime.now(dt.datetime.now().astimezone().tzinfo).replace(microsecond=0)"
132032,rigetti/pyquil,pyquil/quilatom.py,pyquil.quilatom.QuilAtom,"class QuilAtom:
    """"""Abstract class for atomic elements of Quil.""""""

    def out(self) -> str:
        """"""Return the element as a valid Quil string.""""""
        raise NotImplementedError()

    def __str__(self) -> str:
        """"""Get a string representation of the element, possibly not valid Quil.""""""
        raise NotImplementedError()

    def __eq__(self, other: object) -> bool:
        """"""Return True if the other object is equal to this one.""""""
        raise NotImplementedError()

    def __hash__(self) -> int:
        """"""Return a hash of the object.""""""
        raise NotImplementedError()","class QuilAtom:
    '''Abstract class for atomic elements of Quil.'''

    def out(self) -> str:
        '''Return the element as a valid Quil string.'''
        pass

    def __str__(self) -> str:
        '''Get a string representation of the element, possibly not valid Quil.'''
        pass

    def __eq__(self, other: object) -> bool:
        '''Return True if the other object is equal to this one.'''
        pass

    def __hash__(self) -> int:
        '''Return a hash of the object.'''
        pass",snippet_49,"class QuilAtom:
    '''Abstract class for atomic elements of Quil.'''

    def out(self) -> str:
        '''Return the element as a valid Quil string.'''
        raise NotImplementedError(""Subclasses must implement out()."")

    def __str__(self) -> str:
        '''Get a string representation of the element, possibly not valid Quil.'''
        return self.out()

    def __eq__(self, other: object) -> bool:
        '''Return True if the other object is equal to this one.'''
        if not isinstance(other, QuilAtom):
            return NotImplemented
        return type(self) is type(other) and self.out() == other.out()

    def __hash__(self) -> int:
        '''Return a hash of the object.'''
        return hash((type(self), self.out()))"
134597,Unidata/MetPy,Unidata_MetPy/tools/flake8-metpy/flake8_metpy.py,flake8_metpy.MetPyChecker,"class MetPyChecker:
    """"""Flake8 plugin class to check MetPy style/best practice.""""""
    name = __name__
    version = '1.0'

    def __init__(self, tree):
        """"""Initialize the plugin.""""""
        self.tree = tree

    def run(self):
        """"""Run the plugin and yield errors.""""""
        visitor = MetPyVisitor()
        visitor.visit(self.tree)
        for err in visitor.errors:
            yield self.error(err)

    def error(self, err):
        """"""Format errors into Flake8's required format.""""""
        return (err.lineno, err.col, f'MPY{err.code:03d}: Multiplying/dividing by units--use units.Quantity()', type(self))","class MetPyChecker:
    '''Flake8 plugin class to check MetPy style/best practice.'''

    def __init__(self, tree):
        '''Initialize the plugin.'''
        pass

    def run(self):
        '''Run the plugin and yield errors.'''
        pass

    def error(self, err):
        '''Format errors into Flake8's required format.'''
        pass",snippet_50,"class MetPyChecker:
    '''Flake8 plugin class to check MetPy style/best practice.'''

    name = 'metpy-checker'
    version = '0.1.0'

    def __init__(self, tree, filename=None):
        '''Initialize the plugin.'''
        self.tree = tree
        self.filename = filename or ''
        self._errors = []

    def run(self):
        '''Run the plugin and yield errors.'''
        for err in self._errors:
            yield self.error(err)

    def error(self, err):
        '''Format errors into Flake8's required format.'''
        if isinstance(err, dict):
            line = int(err.get('line', 1) or 1)
            col = int(err.get('col', 0) or 0)
            msg = err.get('msg', 'MPY000 MetPy checker error')
        else:
            try:
                line, col, msg = err[:3]
            except Exception:
                line, col, msg = 1, 0, 'MPY000 MetPy checker error'
        return (line, col, msg, type(self))"
134620,Unidata/MetPy,Unidata_MetPy/src/metpy/io/_tools.py,metpy.io._tools.Array,"from struct import Struct

class Array:
    """"""Use a Struct as a callable to unpack a bunch of bytes as a list.""""""

    def __init__(self, fmt):
        """"""Initialize the Struct unpacker.""""""
        self._struct = Struct(fmt)

    def __call__(self, buf):
        """"""Perform the actual unpacking.""""""
        return list(self._struct.unpack(buf))","
class Array:
    '''Use a Struct as a callable to unpack a bunch of bytes as a list.'''

    def __init__(self, fmt):
        '''Initialize the Struct unpacker.'''
        pass

    def __call__(self, buf):
        '''Perform the actual unpacking.'''
        pass",snippet_51,"import struct

class Array:
    '''Use a Struct as a callable to unpack a bunch of bytes as a list.'''

    def __init__(self, fmt):
        '''Initialize the Struct unpacker.'''
        self._struct = struct.Struct(fmt)
        self.size = self._struct.size

    def __call__(self, buf):
        '''Perform the actual unpacking.'''
        return list(self._struct.unpack(buf))"
134621,Unidata/MetPy,Unidata_MetPy/src/metpy/io/_tools.py,metpy.io._tools.BitField,"class BitField:
    """"""Convert an integer to a string for each bit.""""""

    def __init__(self, *names):
        """"""Initialize the list of named bits.""""""
        self._names = names

    def __call__(self, val):
        """"""Return a list with a string for each True bit in the integer.""""""
        if not val:
            return None
        bits = []
        for n in self._names:
            if val & 1:
                bits.append(n)
            val >>= 1
            if not val:
                break
        return bits[0] if len(bits) == 1 else bits","class BitField:
    '''Convert an integer to a string for each bit.'''

    def __init__(self, *names):
        '''Initialize the list of named bits.'''
        pass

    def __call__(self, val):
        '''Return a list with a string for each True bit in the integer.'''
        pass",snippet_52,"class BitField:
    '''Convert an integer to a string for each bit.'''

    def __init__(self, *names):
        '''Initialize the list of named bits.'''
        self._names = list(names)

    def __call__(self, val):
        '''Return a list with a string for each True bit in the integer.'''
        if not isinstance(val, int):
            raise TypeError(""BitField expects an integer value"")
        result = []
        i = 0
        n = val
        while n:
            if n & 1:
                if i < len(self._names) and self._names[i] is not None:
                    result.append(self._names[i])
                else:
                    result.append(str(i))
            n >>= 1
            i += 1
        return result"
134622,Unidata/MetPy,Unidata_MetPy/src/metpy/io/_tools.py,metpy.io._tools.Bits,"class Bits:
    """"""Breaks an integer into a specified number of True/False bits.""""""

    def __init__(self, num_bits):
        """"""Initialize the number of bits.""""""
        self._bits = range(num_bits)

    def __call__(self, val):
        """"""Convert the integer to the list of True/False values.""""""
        return [bool(val >> i & 1) for i in self._bits]","class Bits:
    '''Breaks an integer into a specified number of True/False bits.'''

    def __init__(self, num_bits):
        '''Initialize the number of bits.'''
        pass

    def __call__(self, val):
        '''Convert the integer to the list of True/False values.'''
        pass",snippet_53,"class Bits:
    '''Breaks an integer into a specified number of True/False bits.'''

    def __init__(self, num_bits):
        '''Initialize the number of bits.'''
        if not isinstance(num_bits, int):
            raise TypeError(""num_bits must be an integer"")
        if num_bits <= 0:
            raise ValueError(""num_bits must be a positive integer"")
        self.num_bits = num_bits

    def __call__(self, val):
        '''Convert the integer to the list of True/False values.'''
        if not isinstance(val, int):
            raise TypeError(""val must be an integer"")
        if val < 0:
            raise ValueError(""val must be non-negative"")
        max_val = (1 << self.num_bits) - 1
        if val > max_val:
            raise ValueError(f""val {val} cannot be represented in {self.num_bits} bits"")
        return [bool((val >> i) & 1) for i in range(self.num_bits - 1, -1, -1)]"
134624,Unidata/MetPy,Unidata_MetPy/src/metpy/io/_tools.py,metpy.io._tools.Enum,"class Enum:
    """"""Map values to specific strings.""""""

    def __init__(self, *args, **kwargs):
        """"""Initialize the mapping.""""""
        self.val_map = dict(enumerate(args))
        self.val_map.update(zip(kwargs.values(), kwargs.keys(), strict=False))

    def __call__(self, val):
        """"""Map an integer to the string representation.""""""
        return self.val_map.get(val, f'Unknown ({val})')","class Enum:
    '''Map values to specific strings.'''

    def __init__(self, *args, **kwargs):
        '''Initialize the mapping.'''
        pass

    def __call__(self, val):
        '''Map an integer to the string representation.'''
        pass",snippet_54,"class Enum:
    '''Map values to specific strings.'''

    def __init__(self, *args, **kwargs):
        '''Initialize the mapping.'''
        self._map = {}

        def add_pair(k, v):
            self._map[int(k)] = str(v)

        if args:
            if len(args) == 1 and isinstance(args[0], dict):
                for k, v in args[0].items():
                    add_pair(k, v)
            elif len(args) == 1 and isinstance(args[0], (list, tuple)):
                seq = args[0]
                if all(isinstance(x, (list, tuple)) and len(x) == 2 for x in seq):
                    for k, v in seq:
                        add_pair(k, v)
                else:
                    for i, name in enumerate(seq):
                        add_pair(i, name)
            else:
                if all(isinstance(x, str) for x in args):
                    for i, name in enumerate(args):
                        add_pair(i, name)
                elif all(isinstance(x, (list, tuple)) and len(x) == 2 for x in args):
                    for k, v in args:
                        add_pair(k, v)
                else:
                    raise TypeError(""Unsupported positional arguments for Enum."")
        for k, v in kwargs.items():
            add_pair(k, v)

    def __call__(self, val):
        '''Map an integer to the string representation.'''
        try:
            return self._map[int(val)]
        except Exception:
            return str(val)"
134657,Unidata/MetPy,Unidata_MetPy/src/metpy/package_tools.py,metpy.package_tools.Exporter,"class Exporter:
    """"""Manages exporting of symbols from the module.

    Grabs a reference to `globals()` for a module and provides a decorator to add
    functions and classes to `__all__` rather than requiring a separately maintained list.
    Also provides a context manager to do this for instances by adding all instances added
    within a block to `__all__`.
    """"""

    def __init__(self, globls):
        """"""Initialize the Exporter.""""""
        self.globls = globls
        self.exports = globls.setdefault('__all__', [])

    def export(self, defn):
        """"""Declare a function or class as exported.""""""
        self.exports.append(defn.__name__)
        return defn

    def __enter__(self):
        """"""Start a block tracking all instances created at global scope.""""""
        self.start_vars = set(self.globls)

    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""Exit the instance tracking block.""""""
        self.exports.extend(set(self.globls) - self.start_vars)
        del self.start_vars","class Exporter:
    '''Manages exporting of symbols from the module.
    Grabs a reference to `globals()` for a module and provides a decorator to add
    functions and classes to `__all__` rather than requiring a separately maintained list.
    Also provides a context manager to do this for instances by adding all instances added
    within a block to `__all__`.
    '''

    def __init__(self, globls):
        '''Initialize the Exporter.'''
        pass

    def export(self, defn):
        '''Declare a function or class as exported.'''
        pass

    def __enter__(self):
        '''Start a block tracking all instances created at global scope.'''
        pass

    def __exit__(self, exc_type, exc_val, exc_tb):
        '''Exit the instance tracking block.'''
        pass",snippet_55,"class Exporter:
    '''Manages exporting of symbols from the module.
    Grabs a reference to `globals()` for a module and provides a decorator to add
    functions and classes to `__all__` rather than requiring a separately maintained list.
    Also provides a context manager to do this for instances by adding all instances added
    within a block to `__all__`.
    '''

    def __init__(self, globls):
        '''Initialize the Exporter.'''
        if not isinstance(globls, dict):
            raise TypeError(""globls must be a globals() dict"")
        self._globals = globls
        if ""__all__"" not in self._globals or not isinstance(self._globals[""__all__""], list):
            self._globals[""__all__""] = []
        self._stack = []

    def _append_to_all(self, name):
        all_list = self._globals[""__all__""]
        if name not in all_list and not name.startswith(""_""):
            all_list.append(name)

    def export(self, defn):
        '''Declare a function or class as exported.'''
        name = getattr(defn, ""__name__"", None)
        if not name:
            raise ValueError(""export can only be used on named functions/classes"")
        self._append_to_all(name)
        return defn

    def __enter__(self):
        '''Start a block tracking all instances created at global scope.'''
        self._stack.append(set(self._globals.keys()))
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        '''Exit the instance tracking block.'''
        if not self._stack:
            return False
        before = self._stack.pop()
        after = set(self._globals.keys())
        added = after - before
        for name in added:
            self._append_to_all(name)
        return False"
137119,jpype-project/jpype,jpype-project_jpype/doc/java/util.py,java.util.Collection,"class Collection:
    """""" Customized interface representing a collection of items.

    JPype wraps ``java.util.Collection`` as a Python collection.
    """"""

    def __len__(self) -> int:
        """""" Get the length of this collection.

        Use ``len(collection)`` to find the number of items in this
        collection.

        """"""
        ...

    def __delitem__(self, item):
        """""" Collections do not support remove by index. """"""
        ...

    def __contains__(self, item) -> bool:
        """""" Check if this collection contains this item.

        Use ``item in collection`` to check if the item is 
        present.

        Args:
           item: is the item to check for.  This must be a Java
           object or an object which can be automatically converted
           such as a string.

        Returns:
           bool: True if the item is in the collection.
        """"""
        ...","class Collection:
    ''' Customized interface representing a collection of items.
    JPype wraps ``java.util.Collection`` as a Python collection.
    '''

    def __len__(self) -> int:
        ''' Get the length of this collection.
        Use ``len(collection)`` to find the number of items in this
        collection.
        '''
        pass

    def __delitem__(self, item):
        ''' Collections do not support remove by index. '''
        pass

    def __contains__(self, item) -> bool:
        ''' Check if this collection contains this item.
        Use ``item in collection`` to check if the item is 
        present.
        Args:
           item: is the item to check for.  This must be a Java
           object or an object which can be automatically converted
           such as a string.
        Returns:
           bool: True if the item is in the collection.
        '''
        pass",snippet_56,"class Collection:
    ''' Customized interface representing a collection of items.
    JPype wraps ``java.util.Collection`` as a Python collection.
    '''

    def __len__(self) -> int:
        ''' Get the length of this collection.
        Use ``len(collection)`` to find the number of items in this
        collection.
        '''
        size_method = getattr(self, ""size"", None)
        if callable(size_method):
            return int(size_method())
        items = getattr(self, ""_items"", None)
        if items is not None:
            return len(items)
        try:
            count = 0
            for _ in self:
                count += 1
            return count
        except Exception as exc:
            raise TypeError(""Length not supported for this collection"") from exc

    def __delitem__(self, item):
        ''' Collections do not support remove by index. '''
        raise TypeError(""Collections do not support remove by index."")

    def __contains__(self, item) -> bool:
        ''' Check if this collection contains this item.
        Use ``item in collection`` to check if the item is 
        present.
        Args:
           item: is the item to check for.  This must be a Java
           object or an object which can be automatically converted
           such as a string.
        Returns:
           bool: True if the item is in the collection.
        '''
        contains_method = getattr(self, ""contains"", None)
        if callable(contains_method):
            try:
                return bool(contains_method(item))
            except Exception:
                pass
        items = getattr(self, ""_items"", None)
        if items is not None:
            return item in items
        try:
            for x in self:
                if x == item:
                    return True
            return False
        except Exception:
            return False"
138700,sebp/scikit-survival,sebp_scikit-survival/sksurv/util.py,sksurv.util.Surv,"from sklearn.utils.validation import check_array, check_consistent_length
import numpy as np
import pandas as pd

class Surv:
    """"""A helper class to create a structured array for survival analysis.

    This class provides helper functions to create a structured array that
    encapsulates the event indicator and the observed time. The resulting
    structured array is the recommended format for the ``y`` argument in
    scikit-survival's estimators.
    """"""

    @staticmethod
    def from_arrays(event, time, name_event=None, name_time=None):
        """"""Create structured array from event indicator and time arrays.

        Parameters
        ----------
        event : array-like, shape=(n_samples,)
            Event indicator. A boolean array or array with values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : array-like, shape=(n_samples,)
            Observed time. Time to event or time of censoring.
        name_event : str, optional, default: 'event'
            Name of the event field in the structured array.
        name_time : str, optional, default: 'time'
            Name of the observed time field in the structured array.

        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are set to the values of `name_event` and `name_time`.

        Examples
        --------
        >>> from sksurv.util import Surv
        >>>
        >>> y = Surv.from_arrays(event=[True, False, True],
        ...                      time=[10, 25, 15])
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('event', '?'), ('time', '<f8')])
        >>> y['event']
        array([ True, False,  True])
        >>> y['time']
        array([10., 25., 15.])
        """"""
        name_event = name_event or 'event'
        name_time = name_time or 'time'
        if name_time == name_event:
            raise ValueError('name_time must be different from name_event')
        time = np.asanyarray(time, dtype=float)
        y = np.empty(time.shape[0], dtype=[(name_event, bool), (name_time, float)])
        y[name_time] = time
        event = np.asanyarray(event)
        check_consistent_length(time, event)
        if np.issubdtype(event.dtype, np.bool_):
            y[name_event] = event
        else:
            events = np.unique(event)
            events.sort()
            if len(events) != 2:
                raise ValueError('event indicator must be binary')
            if np.all(events == np.array([0, 1], dtype=events.dtype)):
                y[name_event] = event.astype(bool)
            else:
                raise ValueError('non-boolean event indicator must contain 0 and 1 only')
        return y

    @staticmethod
    def from_dataframe(event, time, data):
        """"""Create structured array from columns in a pandas DataFrame.

        Parameters
        ----------
        event : str
            Name of the column in ``data`` containing the event indicator.
            It must be a boolean or have values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : str
            Name of the column in ``data`` containing the observed time
            (time to event or time of censoring).
        data : pandas.DataFrame
            A DataFrame with columns for event and time.

        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are the respective column names.

        Examples
        --------
        >>> import pandas as pd
        >>> from sksurv.util import Surv
        >>>
        >>> df = pd.DataFrame({
        ...     'status': [True, False, True],
        ...     'followup_time': [10, 25, 15],
        ... })
        >>> y = Surv.from_dataframe(
        ...     event='status', time='followup_time', data=df,
        ... )
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('status', '?'), ('followup_time', '<f8')])
        >>> y['status']
        array([ True, False,  True])
        >>> y['followup_time']
        array([10., 25., 15.])
        """"""
        if not isinstance(data, pd.DataFrame):
            raise TypeError(f'expected pandas.DataFrame, but got {type(data)!r}')
        return Surv.from_arrays(data.loc[:, event].values, data.loc[:, time].values, name_event=str(event), name_time=str(time))","
class Surv:
    '''A helper class to create a structured array for survival analysis.
    This class provides helper functions to create a structured array that
    encapsulates the event indicator and the observed time. The resulting
    structured array is the recommended format for the ``y`` argument in
    scikit-survival's estimators.
    '''
    @staticmethod
    def from_arrays(event, time, name_event=None, name_time=None):
        '''Create structured array from event indicator and time arrays.
        Parameters
        ----------
        event : array-like, shape=(n_samples,)
            Event indicator. A boolean array or array with values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : array-like, shape=(n_samples,)
            Observed time. Time to event or time of censoring.
        name_event : str, optional, default: 'event'
            Name of the event field in the structured array.
        name_time : str, optional, default: 'time'
            Name of the observed time field in the structured array.
        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are set to the values of `name_event` and `name_time`.
        Examples
        --------
        >>> from sksurv.util import Surv
        >>>
        >>> y = Surv.from_arrays(event=[True, False, True],
        ...                      time=[10, 25, 15])
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('event', '?'), ('time', '<f8')])
        >>> y['event']
        array([ True, False,  True])
        >>> y['time']
        array([10., 25., 15.])
        '''
        pass
    @staticmethod
    def from_dataframe(event, time, data):
        '''Create structured array from columns in a pandas DataFrame.
        Parameters
        ----------
        event : str
            Name of the column in ``data`` containing the event indicator.
            It must be a boolean or have values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : str
            Name of the column in ``data`` containing the observed time
            (time to event or time of censoring).
        data : pandas.DataFrame
            A DataFrame with columns for event and time.
        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are the respective column names.
        Examples
        --------
        >>> import pandas as pd
        >>> from sksurv.util import Surv
        >>>
        >>> df = pd.DataFrame({
        ...     'status': [True, False, True],
        ...     'followup_time': [10, 25, 15],
        ... })
        >>> y = Surv.from_dataframe(
        ...     event='status', time='followup_time', data=df,
        ... )
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('status', '?'), ('followup_time', '<f8')])
        >>> y['status']
        array([ True, False,  True])
        >>> y['followup_time']
        array([10., 25., 15.])
        '''
        pass",snippet_57,"```python
import numpy as np

class Surv:
    '''A helper class to create a structured array for survival analysis.
    This class provides helper functions to create a structured array that
    encapsulates the event indicator and the observed time. The resulting
    structured array is the recommended format for the ``y`` argument in
    scikit-survival's estimators.
    '''
    @staticmethod
    def from_arrays(event, time, name_event=None, name_time=None):
        '''Create structured array from event indicator and time arrays.
        Parameters
        ----------
        event : array-like, shape=(n_samples,)
            Event indicator. A boolean array or array with values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : array-like, shape=(n_samples,)
            Observed time. Time to event or time of censoring.
        name_event : str, optional, default: 'event'
            Name of the event field in the structured array.
        name_time : str, optional, default: 'time'
            Name of the observed time field in the structured array.
        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are set to the values of `name_event` and `name_time`.
        Examples
        --------
        >>> from sksurv.util import Surv
        >>>
        >>> y = Surv.from_arrays(event=[True, False, True],
        ...                      time=[10, 25, 15])
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('event', '?'), ('time', '<f8')])
        >>> y['event']
        array([ True, False,  True])
        >>> y['time']
        array([10., 25., 15.])
        '''
        name_event = 'event' if name_event is None else str(name_event)
        name_time = 'time' if name_time is None else str(name_time)

        ev = np.asarray(event)
        tm = np.asarray(time, dtype=np.float64)

        if ev.ndim != 1:
            ev = ev.ravel()
        if tm.ndim != 1:
            tm = tm.ravel()

        if ev.shape[0] != tm.shape[0]:
            raise ValueError(""event and time must have the same number of samples"")

        if ev.dtype == np.bool_:
            ev_bool = ev.astype(bool, copy=False)
        else:
            if np.issubdtype(ev.dtype, np.number):
                # allow 0/1 (float/int)
                if not np.all(np.isfinite(ev)):
                    raise ValueError(""event contains non-finite values"")
                uniq = np.unique(ev.astype(np.int64))
                if not np.all(np.isin(uniq, [0, 1])):
                    raise ValueError(""event must be boolean or contain only 0/1"")
                ev_bool = ev.astype(np.int64) != 0
            else:
                # try to coerce common truthy/falsey representations
                try:
                    ev_bool = ev.astype(bool)
                except Exception as e:
                    raise ValueError(""event must be boolean or contain only 0/1"") from e

        if not np.all(np.isfinite(tm)):
            raise ValueError(""time contains non-finite values"")

        dtype = [(name_event, np.bool_), (name_time, np.float64)]
        y = np.empty(ev_bool.shape[0], dtype=dtype)
        y[name_event] = ev_bool
        y[name_time] = tm.astype(np.float64, copy=False)
        return y

    @staticmethod
    def from_dataframe(event, time, data):
        '''Create structured array from columns in a pandas DataFrame.
        Parameters
        ----------
        event : str
            Name of the column in ``data`` containing the event indicator.
            It must be a boolean or have values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : str
            Name of the column in ``data`` containing the observed time
            (time to event or time of censoring).
        data : pandas.DataFrame
            A DataFrame with columns for event and time.
        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are the respective column names.
        Examples
        --------
        >>> import pandas as pd
        >>> from sksurv.util import Surv
        >>>
        >>> df = pd.DataFrame({
        ...     'status': [True, False, True],
        ...     'followup_time': [10, 25, 15],
        ... })
        >>> y = Surv.from_dataframe(
        ...     event='status', time='followup_time', data=df,
        ... )
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('status', '?'), ('followup_time', '<f8')])
        >>> y['status']
        array([ True, False,  True])
        >>> y['followup_time']
        array([10., 25., 15.])
        '''
        if event not in data.columns:
            raise KeyError(f""'{event}' not found in DataFrame columns"")
        if time not in data.columns:
            raise KeyError(f""'{time}' not found in DataFrame columns"")

        return Surv.from_arrays(data[event].to_numpy(), data[time].to_numpy(),
                                name_event=event, name_time=time)
```"
139652,sentinel-hub/eo-learn,sentinel-hub_eo-learn/eolearn/geometry/morphology.py,eolearn.geometry.morphology.MorphologicalStructFactory,"import cv2
import numpy as np

class MorphologicalStructFactory:
    """"""
    Factory methods for generating morphological structuring elements
    """"""

    @staticmethod
    def get_disk(radius: int) -> np.ndarray:
        """"""
        :param radius: Radius of disk
        :return: The structuring element where elements of the neighborhood are 1 and 0 otherwise.
        """"""
        return cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (radius, radius))

    @staticmethod
    def get_rectangle(width: int, height: int) -> np.ndarray:
        """"""
        :param width: Width of rectangle
        :param height: Height of rectangle
        :return: A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.
        """"""
        return cv2.getStructuringElement(cv2.MORPH_RECT, (height, width))

    @staticmethod
    def get_square(width: int) -> np.ndarray:
        """"""
        :param width: Size of square
        :return: A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.
        """"""
        return cv2.getStructuringElement(cv2.MORPH_RECT, (width, width))","
class MorphologicalStructFactory:
    '''
    Factory methods for generating morphological structuring elements
    '''
    @staticmethod
    def get_disk(radius: int) -> np.ndarray:
        '''
        :param radius: Radius of disk
        :return: The structuring element where elements of the neighborhood are 1 and 0 otherwise.
        '''
        pass
    @staticmethod
    def get_rectangle(width: int, height: int) -> np.ndarray:
        '''
        :param width: Width of rectangle
        :param height: Height of rectangle
        :return: A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.
        '''
        pass
    @staticmethod
    def get_square(width: int) -> np.ndarray:
        '''
        :param width: Size of square
        :return: A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.
        '''
        pass",snippet_58,"import numpy as np

class MorphologicalStructFactory:
    '''
    Factory methods for generating morphological structuring elements
    '''
    @staticmethod
    def get_disk(radius: int) -> np.ndarray:
        '''
        :param radius: Radius of disk
        :return: The structuring element where elements of the neighborhood are 1 and 0 otherwise.
        '''
        if not isinstance(radius, int) or radius < 0:
            raise ValueError(""radius must be a non-negative integer"")
        size = 2 * radius + 1
        yy, xx = np.ogrid[-radius:radius + 1, -radius:radius + 1]
        mask = (xx * xx + yy * yy) <= radius * radius
        return mask.astype(np.uint8)

    @staticmethod
    def get_rectangle(width: int, height: int) -> np.ndarray:
        '''
        :param width: Width of rectangle
        :param height: Height of rectangle
        :return: A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.
        '''
        if not isinstance(width, int) or not isinstance(height, int) or width <= 0 or height <= 0:
            raise ValueError(""width and height must be positive integers"")
        return np.ones((height, width), dtype=np.uint8)

    @staticmethod
    def get_square(width: int) -> np.ndarray:
        '''
        :param width: Size of square
        :return: A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.
        '''
        if not isinstance(width, int) or width <= 0:
            raise ValueError(""width must be a positive integer"")
        return np.ones((width, width), dtype=np.uint8)"
139672,sentinel-hub/eo-learn,sentinel-hub_eo-learn/eolearn/visualization/eoexecutor.py,eolearn.visualization.eoexecutor._ErrorSummary,"from dataclasses import dataclass

@dataclass()
class _ErrorSummary:
    """"""Contains data for errors of a node.""""""
    origin: str
    example_message: str
    failed_indexed_executions: list[tuple[int, str]]

    def add_execution(self, index: int, name: str) -> None:
        """"""Adds an execution to the summary.""""""
        self.failed_indexed_executions.append((index, name))

    @property
    def num_failed(self) -> int:
        """"""Helps with jinja""""""
        return len(self.failed_indexed_executions)","@dataclass()
class _ErrorSummary:
    '''Contains data for errors of a node.'''

    def add_execution(self, index: int, name: str) -> None:
        '''Adds an execution to the summary.'''
        pass
    @property
    def num_failed(self) -> int:
        '''Helps with jinja'''
        pass",snippet_59,"from dataclasses import dataclass, field
from typing import List, Tuple

@dataclass()
class _ErrorSummary:
    '''Contains data for errors of a node.'''
    _executions: List[Tuple[int, str]] = field(default_factory=list)

    def add_execution(self, index: int, name: str) -> None:
        '''Adds an execution to the summary.'''
        self._executions.append((index, name))

    @property
    def num_failed(self) -> int:
        '''Helps with jinja'''
        return len(self._executions)"
139835,python-escpos/python-escpos,python-escpos_python-escpos/src/escpos/codepages.py,escpos.codepages.CodePageManager,"class CodePageManager:
    """"""Holds information about all the code pages.

    Information as defined in escpos-printer-db.
    """"""

    def __init__(self, data):
        """"""Initialize code page manager.""""""
        self.data = data

    @staticmethod
    def get_encoding_name(encoding):
        """"""Get encoding name.

        .. todo:: Resolve the encoding alias.
        """"""
        return encoding.upper()

    def get_encoding(self, encoding):
        """"""Return the encoding data.""""""
        return self.data[encoding]","class CodePageManager:
    '''Holds information about all the code pages.
    Information as defined in escpos-printer-db.
        '''

    def __init__(self, data):
        '''Initialize code page manager.'''
        pass
    @staticmethod
    def get_encoding_name(encoding):
        '''Get encoding name.
        .. todo:: Resolve the encoding alias.
        '''
        pass
    def get_encoding_name(encoding):
        '''Return the encoding data.'''
        pass",snippet_60,"class CodePageManager:
    '''Holds information about all the code pages.
    Information as defined in escpos-printer-db.
        '''

    _encodings = {}
    _alias_to_name = {}

    def __init__(self, data):
        '''Initialize code page manager.'''
        # Normalize data into a canonical mapping: name -> info
        enc_map = {}

        def add_alias(canon_name, alias):
            if not alias:
                return
            key = str(alias).strip().lower()
            if key:
                self.__class__._alias_to_name[key] = canon_name

        if isinstance(data, dict):
            # If dict looks like direct name->info mapping
            # or holds a list under a common key, handle both.
            candidates = None
            for k in ('encodings', 'codepages', 'code_pages'):
                if k in data and isinstance(data[k], list):
                    candidates = data[k]
                    break
            if candidates is not None:
                for item in candidates:
                    if not isinstance(item, dict):
                        continue
                    name = str(item.get('name') or item.get('encoding') or '').strip()
                    if not name:
                        continue
                    canon = name.lower()
                    enc_map[canon] = item
                    # aliases
                    aliases = item.get('aliases') or item.get('alias') or []
                    if isinstance(aliases, str):
                        aliases = [aliases]
                    for al in aliases:
                        add_alias(canon, al)
                    # numeric/id aliases
                    num = item.get('id') if 'id' in item else item.get('number')
                    if isinstance(num, int) or (isinstance(num, str) and num.isdigit()):
                        num_str = str(num)
                        add_alias(canon, num_str)
                        add_alias(canon, f'cp{num_str}')
                        add_alias(canon, f'ibm{num_str}')
                    # also map the original name itself as alias
                    add_alias(canon, name)
            else:
                # Treat keys as canonical names
                for name, info in data.items():
                    canon = str(name).strip().lower()
                    if not canon:
                        continue
                    enc_map[canon] = info if isinstance(info, dict) else {'data': info}
                    aliases = []
                    if isinstance(info, dict):
                        aliases = info.get('aliases') or info.get('alias') or []
                    if isinstance(aliases, str):
                        aliases = [aliases]
                    for al in aliases:
                        add_alias(canon, al)
                    # numeric/id aliases
                    num = None
                    if isinstance(info, dict):
                        num = info.get('id') if 'id' in info else info.get('number')
                    if isinstance(num, int) or (isinstance(num, str) and str(num).isdigit()):
                        num_str = str(num)
                        add_alias(canon, num_str)
                        add_alias(canon, f'cp{num_str}')
                        add_alias(canon, f'ibm{num_str}')
                    add_alias(canon, name)
        elif isinstance(data, list):
            for item in data:
                if not isinstance(item, dict):
                    continue
                name = str(item.get('name') or item.get('encoding') or '').strip()
                if not name:
                    continue
                canon = name.lower()
                enc_map[canon] = item
                aliases = item.get('aliases') or item.get('alias') or []
                if isinstance(aliases, str):
                    aliases = [aliases]
                for al in aliases:
                    add_alias(canon, al)
                num = item.get('id') if 'id' in item else item.get('number')
                if isinstance(num, int) or (isinstance(num, str) and str(num).isdigit()):
                    num_str = str(num)
                    add_alias(canon, num_str)
                    add_alias(canon, f'cp{num_str}')
                    add_alias(canon, f'ibm{num_str}')
                add_alias(canon, name)

        # Merge into class-level registries
        # Later initializations can extend/override previous ones.
        self.__class__._encodings.update(enc_map)

    @staticmethod
    def get_encoding_name(encoding):
        '''Get encoding name.
        .. todo:: Resolve the encoding alias.
        '''
        if encoding is None:
            return None
        key = str(encoding).strip().lower()
        if not key:
            return None
        # Direct canonical name
        if key in CodePageManager._encodings:
            return key
        # Alias resolution
        return CodePageManager._alias_to_name.get(key)

    def get_encoding_name(encoding):
        '''Return the encoding data.'''
        # This method is defined without decorators and will typically be called
        # as CodePageManager.get_encoding_name(value)
        name = CodePageManager.get_encoding_name.__func__(encoding) if hasattr(CodePageManager.get_encoding_name, '__func__') else CodePageManager.get_encoding_name(encoding)  # resolve canonical name
        # After the line above, name is either canonical name or None.
        if not name:
            return None
        return CodePageManager._encodings.get(name)"
141706,mar10/wsgidav,mar10_wsgidav/wsgidav/rw_lock.py,wsgidav.rw_lock.ReadWriteLock,"from threading import Condition, Lock, current_thread
from time import time

class ReadWriteLock:
    """"""Read-Write lock class. A read-write lock differs from a standard
    threading.RLock() by allowing multiple threads to simultaneously hold a
    read lock, while allowing only a single thread to hold a write lock at the
    same point of time.

    When a read lock is requested while a write lock is held, the reader
    is blocked; when a write lock is requested while another write lock is
    held or there are read locks, the writer is blocked.

    Writers are always preferred by this implementation: if there are blocked
    threads waiting for a write lock, current readers may request more read
    locks (which they eventually should free, as they starve the waiting
    writers otherwise), but a new thread requesting a read lock will not
    be granted one, and block. This might mean starvation for readers if
    two writer threads interweave their calls to acquire_write() without
    leaving a window only for readers.

    In case a current reader requests a write lock, this can and will be
    satisfied without giving up the read locks first, but, only one thread
    may perform this kind of lock upgrade, as a deadlock would otherwise
    occur. After the write lock has been granted, the thread will hold a
    full write lock, and not be downgraded after the upgrading call to
    acquire_write() has been match by a corresponding release().
    """"""

    def __init__(self):
        """"""Initialize this read-write lock.""""""
        self.__condition = Condition(Lock())
        self.__writer = None
        self.__upgradewritercount = 0
        self.__pendingwriters = []
        self.__readers = {}

    def acquire_read(self, *, timeout=None):
        """"""Acquire a read lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.

        In case timeout is None, the call to acquire_read blocks until the
        lock request can be serviced.

        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.""""""
        if timeout is not None:
            endtime = time() + timeout
        me = current_thread()
        self.__condition.acquire()
        try:
            if self.__writer is me:
                self.__writercount += 1
                return
            while True:
                if self.__writer is None:
                    if self.__upgradewritercount or self.__pendingwriters:
                        if me in self.__readers:
                            self.__readers[me] += 1
                            return
                    else:
                        self.__readers[me] = self.__readers.get(me, 0) + 1
                        return
                if timeout is not None:
                    remaining = endtime - time()
                    if remaining <= 0:
                        raise RuntimeError('Acquiring read lock timed out')
                    self.__condition.wait(remaining)
                else:
                    self.__condition.wait()
        finally:
            self.__condition.release()

    def acquire_write(self, *, timeout=None):
        """"""Acquire a write lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.

        In case the write lock cannot be serviced due to the deadlock
        condition mentioned above, a ValueError is raised.

        In case timeout is None, the call to acquire_write blocks until the
        lock request can be serviced.

        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.""""""
        if timeout is not None:
            endtime = time() + timeout
        me, upgradewriter = (current_thread(), False)
        self.__condition.acquire()
        try:
            if self.__writer is me:
                self.__writercount += 1
                return
            elif me in self.__readers:
                if self.__upgradewritercount:
                    raise ValueError('Inevitable dead lock, denying write lock')
                upgradewriter = True
                self.__upgradewritercount = self.__readers.pop(me)
            else:
                self.__pendingwriters.append(me)
            while True:
                if not self.__readers and self.__writer is None:
                    if self.__upgradewritercount:
                        if upgradewriter:
                            self.__writer = me
                            self.__writercount = self.__upgradewritercount + 1
                            self.__upgradewritercount = 0
                            return
                    elif self.__pendingwriters[0] is me:
                        self.__writer = me
                        self.__writercount = 1
                        self.__pendingwriters = self.__pendingwriters[1:]
                        return
                if timeout is not None:
                    remaining = endtime - time()
                    if remaining <= 0:
                        if upgradewriter:
                            self.__readers[me] = self.__upgradewritercount
                            self.__upgradewritercount = 0
                        else:
                            self.__pendingwriters.remove(me)
                        raise RuntimeError('Acquiring write lock timed out')
                    self.__condition.wait(remaining)
                else:
                    self.__condition.wait()
        finally:
            self.__condition.release()

    def release(self):
        """"""Release the currently held lock.

        In case the current thread holds no lock, a ValueError is thrown.""""""
        me = current_thread()
        self.__condition.acquire()
        try:
            if self.__writer is me:
                self.__writercount -= 1
                if not self.__writercount:
                    self.__writer = None
                    self.__condition.notify_all()
            elif me in self.__readers:
                self.__readers[me] -= 1
                if not self.__readers[me]:
                    del self.__readers[me]
                    if not self.__readers:
                        self.__condition.notify_all()
            else:
                raise ValueError('Trying to release unheld lock')
        finally:
            self.__condition.release()","
class ReadWriteLock:
    '''Read-Write lock class. A read-write lock differs from a standard
    threading.RLock() by allowing multiple threads to simultaneously hold a
    read lock, while allowing only a single thread to hold a write lock at the
    same point of time.
    When a read lock is requested while a write lock is held, the reader
    is blocked; when a write lock is requested while another write lock is
    held or there are read locks, the writer is blocked.
    Writers are always preferred by this implementation: if there are blocked
    threads waiting for a write lock, current readers may request more read
    locks (which they eventually should free, as they starve the waiting
    writers otherwise), but a new thread requesting a read lock will not
    be granted one, and block. This might mean starvation for readers if
    two writer threads interweave their calls to acquire_write() without
    leaving a window only for readers.
    In case a current reader requests a write lock, this can and will be
    satisfied without giving up the read locks first, but, only one thread
    may perform this kind of lock upgrade, as a deadlock would otherwise
    occur. After the write lock has been granted, the thread will hold a
    full write lock, and not be downgraded after the upgrading call to
    acquire_write() has been match by a corresponding release().
    '''

    def __init__(self):
        '''Initialize this read-write lock.'''
        pass

    def acquire_read(self, *, timeout=None):
        '''Acquire a read lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.
        In case timeout is None, the call to acquire_read blocks until the
        lock request can be serviced.
        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.'''
        pass

    def acquire_write(self, *, timeout=None):
        '''Acquire a write lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.
        In case the write lock cannot be serviced due to the deadlock
        condition mentioned above, a ValueError is raised.
        In case timeout is None, the call to acquire_write blocks until the
        lock request can be serviced.
        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.'''
        pass

    def release(self):
        '''Release the currently held lock.
        In case the current thread holds no lock, a ValueError is thrown.'''
        pass",snippet_62,"import threading
import time

class ReadWriteLock:
    '''Read-Write lock class. A read-write lock differs from a standard
    threading.RLock() by allowing multiple threads to simultaneously hold a
    read lock, while allowing only a single thread to hold a write lock at the
    same point of time.
    When a read lock is requested while a write lock is held, the reader
    is blocked; when a write lock is requested while another write lock is
    held or there are read locks, the writer is blocked.
    Writers are always preferred by this implementation: if there are blocked
    threads waiting for a write lock, current readers may request more read
    locks (which they eventually should free, as they starve the waiting
    writers otherwise), but a new thread requesting a read lock will not
    be granted one, and block. This might mean starvation for readers if
    two writer threads interweave their calls to acquire_write() without
    leaving a window only for readers.
    In case a current reader requests a write lock, this can and will be
    satisfied without giving up the read locks first, but, only one thread
    may perform this kind of lock upgrade, as a deadlock would otherwise
    occur. After the write lock has been granted, the thread will hold a
    full write lock, and not be downgraded after the upgrading call to
    acquire_write() has been match by a corresponding release().
    '''

    def __init__(self):
        '''Initialize this read-write lock.'''
        self._mutex = threading.RLock()
        self._cond = threading.Condition(self._mutex)
        self._readers = {}  # tid -> count
        self._readers_count = 0
        self._writer_tid = None
        self._writer_count = 0
        self._waiting_writers = 0
        self._upgrading_tid = None

    def _tid(self):
        return threading.get_ident()

    def _get_read_count(self, tid):
        return self._readers.get(tid, 0)

    def _can_read(self, tid):
        # Block if another writer active
        if self._writer_tid is not None and self._writer_tid != tid:
            return False
        # Prefer writers: if there are waiting writers, only existing readers may reenter
        if self._waiting_writers > 0 and self._get_read_count(tid) == 0 and self._writer_tid is None:
            return False
        # If an upgrade is in progress by another thread, block new readers to allow upgrade to proceed
        if self._upgrading_tid is not None and self._upgrading_tid != tid and self._writer_tid is None:
            # Existing readers can reenter
            if self._get_read_count(tid) == 0:
                return False
        return True

    def _can_acquire_write_plain(self):
        return (self._writer_tid is None) and (self._readers_count == 0)

    def acquire_read(self, *, timeout=None):
        '''Acquire a read lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.
        In case timeout is None, the call to acquire_read blocks until the
        lock request can be serviced.
        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.'''
        tid = self._tid()
        with self._cond:
            # Fast path if allowed
            if self._can_read(tid):
                self._readers[tid] = self._get_read_count(tid) + 1
                self._readers_count += 1
                return

            # Non-blocking path
            if timeout is not None and timeout <= 0:
                raise RuntimeError(""Timeout acquiring read lock"")

            end = None if timeout is None else (time.monotonic() + timeout)
            while not self._can_read(tid):
                remaining = None if end is None else (end - time.monotonic())
                if remaining is not None and remaining <= 0:
                    raise RuntimeError(""Timeout acquiring read lock"")
                self._cond.wait(remaining)

            self._readers[tid] = self._get_read_count(tid) + 1
            self._readers_count += 1

    def acquire_write(self, *, timeout=None):
        '''Acquire a write lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.
        In case the write lock cannot be serviced due to the deadlock
        condition mentioned above, a ValueError is raised.
        In case timeout is None, the call to acquire_write blocks until the
        lock request can be serviced.
        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.'''
        tid = self._tid()
        with self._cond:
            # Re-entrant write acquisition
            if self._writer_tid == tid:
                self._writer_count += 1
                return

            # Upgrade path if thread currently holds read locks
            my_read_count = self._get_read_count(tid)
            if my_read_count > 0:
                # Deadlock prevention: only one upgrader allowed
                if self._upgrading_tid is not None and self._upgrading_tid != tid:
                    raise ValueError(""Deadlock risk: another thread is upgrading read to write"")
                # Non-blocking immediate check
                if timeout is not None and timeout <= 0:
                    # Can upgrade immediately only if no other readers and no other writer
                    if self._writer_tid is None and (self._readers_count == my_read_count):
                        self._upgrading_tid = tid
                        # Convert upgrade
                        self._writer_tid = tid
                        self._writer_count = 1
                        # Remove our read locks (no downgrade after release)
                        self._readers_count -= my_read_count
                        if my_read_count > 0:
                            self._readers.pop(tid, None)
                        self._upgrading_tid = None
                        return
                    else:
                        raise RuntimeError(""Timeout acquiring write lock (upgrade)"")
                # Blocking/timeout upgrade
                end = None if timeout is None else (time.monotonic() + timeout)
                # Mark as upgrading and prefer writers
                if self._upgrading_tid is None:
                    self._upgrading_tid = tid
                self._waiting_writers += 1
                try:
                    while not (self._writer_tid is None and self._readers_count == my_read_count):
                        remaining = None if end is None else (end - time.monotonic())
                        if remaining is not None and remaining <= 0:
                            raise RuntimeError(""Timeout acquiring write lock (upgrade)"")
                        self._cond.wait(remaining)
                    # Convert upgrade
                    self._writer_tid = tid
                    self._writer_count = 1
                    # Remove our read locks to avoid downgrade after write release
                    self._readers_count -= my_read_count
                    if my_read_count > 0:
                        self._readers.pop(tid, None)
                finally:
                    self._upgrading_tid = None
                    self._waiting_writers -= 1
                return

            # Plain write acquisition (no current read locks)
            if timeout is not None and timeout <= 0:
                if self._can_acquire_write_plain():
                    self._writer_tid = tid
                    self._writer_count = 1
                    return
                else:
                    raise RuntimeError(""Timeout acquiring write lock"")

            end = None if timeout is None else (time.monotonic() + timeout)
            self._waiting_writers += 1
            try:
                while not self._can_acquire_write_plain():
                    remaining = None if end is None else (end - time.monotonic())
                    if remaining is not None and remaining <= 0:
                        raise RuntimeError(""Timeout acquiring write lock"")
                    self._cond.wait(remaining)
                self._writer_tid = tid
                self._writer_count = 1
            finally:
                self._waiting_writers -= 1

    def release(self):
        '''Release the currently held lock.
        In case the current thread holds no lock, a ValueError is thrown.'''
        tid = self._tid()
        with self._cond:
            if self._writer_tid == tid and self._writer_count > 0:
                self._writer_count -= 1
                if self._writer_count == 0:
                    self._writer_tid = None
                    # Wake up all: writers first (due to waiting_writers preference enforced in acquire)
                    self._cond.notify_all()
                else:
                    # Still holds write reentrantly; nothing else to do
                    pass
                return

            rc = self._get_read_count(tid)
            if rc > 0:
                # Release one read lock
                if rc == 1:
                    self._readers.pop(tid, None)
                else:
                    self._readers[tid] = rc - 1
                self._readers_count -= 1
                if self._readers_count == 0:
                    # Wake up potential writers
                    self._cond.notify_all()
                return

            raise ValueError(""Current thread holds no lock to release"")"
141733,mar10/wsgidav,mar10_wsgidav/wsgidav/stream_tools.py,wsgidav.stream_tools.StreamingFile,"class StreamingFile:
    """"""A file object wrapped around an iterator / data stream.""""""

    def __init__(self, data_stream):
        """"""Initialise the object with the data stream.""""""
        self.data_stream = data_stream
        self.buffer = ''

    def read(self, size=None):
        """"""Read bytes from an iterator.""""""
        while size is None or len(self.buffer) < size:
            try:
                self.buffer += next(self.data_stream)
            except StopIteration:
                break
        sized_chunk = self.buffer[:size]
        if size is None:
            self.buffer = ''
        else:
            self.buffer = self.buffer[size:]
        return sized_chunk","class StreamingFile:
    '''A file object wrapped around an iterator / data stream.'''

    def __init__(self, data_stream):
        '''Initialise the object with the data stream.'''
        pass

    def read(self, size=None):
        '''Read bytes from an iterator.'''
        pass",snippet_63,"class StreamingFile:
    '''A file object wrapped around an iterator / data stream.'''

    def __init__(self, data_stream):
        '''Initialise the object with the data stream.'''
        self._iter = iter(data_stream)
        self._buffer = bytearray()
        self._exhausted = False

    def read(self, size=None):
        '''Read bytes from an iterator.'''
        if size is None or (isinstance(size, int) and size < 0):
            if not self._exhausted:
                for chunk in self._iter:
                    self._buffer.extend(bytes(chunk))
                self._exhausted = True
            data = bytes(self._buffer)
            self._buffer.clear()
            return data

        if not isinstance(size, int):
            raise TypeError(""size must be an integer or None"")

        if size == 0:
            return b""""

        # Ensure buffer has at least 'size' bytes or iterator is exhausted
        while len(self._buffer) < size and not self._exhausted:
            try:
                chunk = next(self._iter)
            except StopIteration:
                self._exhausted = True
                break
            self._buffer.extend(bytes(chunk))

        # Serve from buffer
        if size >= len(self._buffer):
            data = bytes(self._buffer)
            self._buffer.clear()
            return data
        else:
            data = bytes(self._buffer[:size])
            del self._buffer[:size]
            return data"
142723,PyThaiNLP/pythainlp,PyThaiNLP_pythainlp/pythainlp/augment/lm/fasttext.py,pythainlp.augment.lm.fasttext.FastTextAug,"from gensim.models.fasttext import FastText as FastText_gensim
from typing import List, Tuple
import itertools
from gensim.models.keyedvectors import KeyedVectors
from pythainlp.tokenize import word_tokenize

class FastTextAug:
    """"""
    Text Augment from fastText

    :param str model_path: path of model file
    """"""

    def __init__(self, model_path: str):
        """"""
        :param str model_path: path of model file
        """"""
        if model_path.endswith('.bin'):
            self.model = FastText_gensim.load_facebook_vectors(model_path)
        elif model_path.endswith('.vec'):
            self.model = KeyedVectors.load_word2vec_format(model_path)
        else:
            self.model = FastText_gensim.load(model_path)
        self.dict_wv = list(self.model.key_to_index.keys())

    def tokenize(self, text: str) -> List[str]:
        """"""
        Thai text tokenization for fastText

        :param str text: Thai text

        :return: list of words
        :rtype: List[str]
        """"""
        return word_tokenize(text, engine='icu')

    def modify_sent(self, sent: str, p: float=0.7) -> List[List[str]]:
        """"""
        :param str sent: text of sentence
        :param float p: probability
        :rtype: List[List[str]]
        """"""
        list_sent_new = []
        for i in sent:
            if i in self.dict_wv:
                w = [j for j, v in self.model.most_similar(i) if v >= p]
                if w == []:
                    list_sent_new.append([i])
                else:
                    list_sent_new.append(w)
            else:
                list_sent_new.append([i])
        return list_sent_new

    def augment(self, sentence: str, n_sent: int=1, p: float=0.7) -> List[Tuple[str]]:
        """"""
        Text Augment from fastText

        You may want to download the Thai model
        from https://fasttext.cc/docs/en/crawl-vectors.html.

        :param str sentence: Thai sentence
        :param int n_sent: number of sentences
        :param float p: probability of word

        :return: list of synonyms
        :rtype: List[Tuple[str]]
        """"""
        self.sentence = self.tokenize(sentence)
        self.list_synonym = self.modify_sent(self.sentence, p=p)
        new_sentences = []
        for x in list(itertools.product(*self.list_synonym))[0:n_sent]:
            new_sentences.append(x)
        return new_sentences","
class FastTextAug:
    '''
    Text Augment from fastText
    :param str model_path: path of model file
    '''

    def __init__(self, model_path: str):
        '''
        :param str model_path: path of model file
        '''
        pass

    def tokenize(self, text: str) -> List[str]:
        '''
        Thai text tokenization for fastText
        :param str text: Thai text
        :return: list of words
        :rtype: List[str]
        '''
        pass

    def modify_sent(self, sent: str, p: float=0.7) -> List[List[str]]:
        '''
        :param str sent: text of sentence
        :param float p: probability
        :rtype: List[List[str]]
        '''
        pass

    def augment(self, sentence: str, n_sent: int=1, p: float=0.7) -> List[Tuple[str]]:
        '''
        Text Augment from fastText
        You may want to download the Thai model
        from https://fasttext.cc/docs/en/crawl-vectors.html.
        :param str sentence: Thai sentence
        :param int n_sent: number of sentences
        :param float p: probability of word
        :return: list of synonyms
        :rtype: List[Tuple[str]]
        '''
        pass",snippet_64,"from typing import List, Tuple, Optional
import os
import random
import re

class FastTextAug:
    '''
    Text Augment from fastText
    :param str model_path: path of model file
    '''

    def __init__(self, model_path: str):
        '''
        :param str model_path: path of model file
        '''
        if not isinstance(model_path, str) or not os.path.isfile(model_path):
            raise FileNotFoundError(f""Model file not found: {model_path}"")

        self._backend = None
        self._model = None

        self._thai_re = re.compile(r'[\u0E00-\u0E7F]')
        self._word_re = re.compile(r'\w+', flags=re.UNICODE)

        # Try fasttext native model first (.bin/.ftz)
        try:
            import fasttext
            if model_path.endswith(('.bin', '.ftz')):
                self._model = fasttext.load_model(model_path)
                self._backend = 'fasttext'
        except Exception:
            pass

        # Fallback to gensim KeyedVectors (.vec, .txt)
        if self._model is None:
            try:
                from gensim.models import KeyedVectors
                binary = model_path.endswith('.bin') and self._backend is None
                self._model = KeyedVectors.load_word2vec_format(model_path, binary=binary)
                self._backend = 'gensim'
            except Exception:
                pass

        if self._model is None or self._backend is None:
            raise RuntimeError(""Failed to load model. Ensure you have a valid fastText (.bin/.ftz) or word2vec (.vec/.txt) model and required libraries installed."")

        # Optional Thai tokenizer
        try:
            from pythainlp.tokenize import word_tokenize  # type: ignore
            self._thai_tokenize = lambda text: word_tokenize(text, engine=""newmm"")
        except Exception:
            self._thai_tokenize = None

        random.seed()

    def tokenize(self, text: str) -> List[str]:
        '''
        Thai text tokenization for fastText
        :param str text: Thai text
        :return: list of words
        :rtype: List[str]
        '''
        if not isinstance(text, str):
            return []
        has_thai = bool(self._thai_re.search(text))
        if has_thai and self._thai_tokenize is not None:
            toks = self._thai_tokenize(text)
            return [t for t in toks if t and not t.isspace()]
        # Generic Unicode tokenization fallback
        return self._word_re.findall(text)

    def _is_in_vocab(self, token: str) -> bool:
        if not token:
            return False
        try:
            if self._backend == 'fasttext':
                # fasttext returns vector for OOV via subwords but nearest neighbors may be poor;
                # We'll consider it in-vocab if word is known in its dict
                return token in self._model.get_words()
            else:
                return token in self._model.key_to_index  # gensim >=4
        except Exception:
            return False

    def _nearest(self, word: str, topn: int = 10) -> List[str]:
        res: List[str] = []
        try:
            if self._backend == 'fasttext':
                # returns list of (cosine, word)
                nn = self._model.get_nearest_neighbors(word, k=topn + 5)
                res = [w for _, w in nn if w != word]
            else:
                nn = self._model.most_similar(positive=[word], topn=topn + 5)
                res = [w for w, _ in nn if w != word]
        except Exception:
            res = []
        # Deduplicate, keep order
        seen = set()
        out = []
        for w in res:
            if w not in seen:
                seen.add(w)
                out.append(w)
            if len(out) >= topn:
                break
        return out

    def _should_join_without_space(self, original: str) -> bool:
        return bool(self._thai_re.search(original)) and (' ' not in original)

    def modify_sent(self, sent: str, p: float = 0.7) -> List[List[str]]:
        '''
        :param str sent: text of sentence
        :param float p: probability
        :rtype: List[List[str]]
        '''
        tokens = self.tokenize(sent)
        if not tokens:
            return [tokens]

        modified: List[str] = []
        for tok in tokens:
            cand = tok
            # Only attempt replacement for alphabetic Thai/word-like tokens
            if random.random() < max(0.0, min(1.0, p)) and self._is_in_vocab(tok):
                neighbors = self._nearest(tok, topn=8)
                if neighbors:
                    cand = random.choice(neighbors)
            modified.append(cand)
        return [modified]

    def augment(self, sentence: str, n_sent: int = 1, p: float = 0.7) -> List[Tuple[str]]:
        '''
        Text Augment from fastText
        You may want to download the Thai model
        from https://fasttext.cc/docs/en/crawl-vectors.html.
        :param str sentence: Thai sentence
        :param int n_sent: number of sentences
        :param float p: probability of word
        :return: list of synonyms
        :rtype: List[Tuple[str]]
        '''
        n_sent = max(1, int(n_sent))
        join_without_space = self._should_join_without_space(sentence)

        results: List[Tuple[str]] = []
        for _ in range(n_sent):
            mod_tok_lists = self.modify_sent(sentence, p=p)
            for toks in mod_tok_lists:
                if join_without_space:
                    aug = ''.join(toks)
                else:
                    aug = ' '.join(toks)
                results.append((aug,))
        return results"
142777,PyThaiNLP/pythainlp,PyThaiNLP_pythainlp/pythainlp/tokenize/core.py,pythainlp.tokenize.core.Tokenizer,"from typing import Iterable, List, Union
from pythainlp.tokenize import DEFAULT_SENT_TOKENIZE_ENGINE, DEFAULT_SUBWORD_TOKENIZE_ENGINE, DEFAULT_SYLLABLE_DICT_TRIE, DEFAULT_SYLLABLE_TOKENIZE_ENGINE, DEFAULT_WORD_DICT_TRIE, DEFAULT_WORD_TOKENIZE_ENGINE
from pythainlp.util.trie import Trie, dict_trie

class Tokenizer:
    """"""
    Tokenizer class for a custom tokenizer.

    This class allows users to pre-define custom dictionary along with
    tokenizer and encapsulate them into one single object.
    It is an wrapper for both functions, that are
    :func:`pythainlp.tokenize.word_tokenize`,
    and :func:`pythainlp.util.dict_trie`

    :Example:

    Tokenizer object instantiated with :class:`pythainlp.util.Trie`::

        from pythainlp.tokenize import Tokenizer
        from pythainlp.corpus.common import thai_words
        from pythainlp.util import dict_trie

        custom_words_list = set(thai_words())
        custom_words_list.add('อะเฟเซีย')
        custom_words_list.add('Aphasia')
        trie = dict_trie(dict_source=custom_words_list)

        text = ""อะเฟเซีย (Aphasia*) เป็นอาการผิดปกติของการพูด""
        _tokenizer = Tokenizer(custom_dict=trie, engine='newmm')
        _tokenizer.word_tokenize(text)
        # output: ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ',
        'ผิดปกติ', 'ของ', 'การ', 'พูด']

    Tokenizer object instantiated with a list of words::

        text = ""อะเฟเซีย (Aphasia) เป็นอาการผิดปกติของการพูด""
        _tokenizer = Tokenizer(custom_dict=list(thai_words()), engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะ', 'เฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ',
        #   'ผิดปกติ', 'ของ', 'การ', 'พูด']

    Tokenizer object instantiated with a file path containing a list of
    words separated with *newline* and explicitly setting a new tokenizer
    after initiation::

        PATH_TO_CUSTOM_DICTIONARY = './custom_dictionary.txtt'

        # write a file
        with open(PATH_TO_CUSTOM_DICTIONARY, 'w', encoding='utf-8') as f:
            f.write('อะเฟเซีย\\nAphasia\\nผิด\\nปกติ')

        text = ""อะเฟเซีย (Aphasia) เป็นอาการผิดปกติของการพูด""

        # initiate an object from file with `attacut` as tokenizer
        _tokenizer = Tokenizer(custom_dict=PATH_TO_CUSTOM_DICTIONARY, \\
            engine='attacut')

        _tokenizer.word_tokenize(text)
        # output:
        # ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ', 'ผิด',
        #   'ปกติ', 'ของ', 'การ', 'พูด']

        # change tokenizer to `newmm`
        _tokenizer.set_tokenizer_engine(engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็นอาการ', 'ผิด',
        #   'ปกติ', 'ของการพูด']
    """"""

    def __init__(self, custom_dict: Union[Trie, Iterable[str], str]=[], engine: str='newmm', keep_whitespace: bool=True, join_broken_num: bool=True):
        """"""
        Initialize tokenizer object.

        :param str custom_dict: a file path, a list of vocaburaies* to be
                    used to create a trie, or an instantiated
                    :class:`pythainlp.util.Trie` object.
        :param str engine: choose between different options of tokenizer engines
                            (i.e.  *newmm*, *mm*, *longest*, *deepcut*)
        :param bool keep_whitespace: True to keep whitespace, a common mark
                                     for end of phrase in Thai
        """"""
        self.__trie_dict = Trie([])
        if custom_dict:
            self.__trie_dict = dict_trie(custom_dict)
        else:
            self.__trie_dict = DEFAULT_WORD_DICT_TRIE
        self.__engine = engine
        if self.__engine not in ['newmm', 'mm', 'longest', 'deepcut']:
            raise NotImplementedError('\n                The Tokenizer class is not support %s for custom tokenizer\n                ' % self.__engine)
        self.__keep_whitespace = keep_whitespace
        self.__join_broken_num = join_broken_num

    def word_tokenize(self, text: str) -> List[str]:
        """"""
        Main tokenization function.

        :param str text: text to be tokenized
        :return: list of words, tokenized from the text
        :rtype: list[str]
        """"""
        return word_tokenize(text, custom_dict=self.__trie_dict, engine=self.__engine, keep_whitespace=self.__keep_whitespace, join_broken_num=self.__join_broken_num)

    def set_tokenize_engine(self, engine: str) -> None:
        """"""
        Set the tokenizer's engine.

        :param str engine: choose between different options of tokenizer engines
                           (i.e. *newmm*, *mm*, *longest*, *deepcut*)
        """"""
        self.__engine = engine","
class Tokenizer:
    '''
    Tokenizer class for a custom tokenizer.
    This class allows users to pre-define custom dictionary along with
    tokenizer and encapsulate them into one single object.
    It is an wrapper for both functions, that are
    :func:`pythainlp.tokenize.word_tokenize`,
    and :func:`pythainlp.util.dict_trie`
    :Example:
    Tokenizer object instantiated with :class:`pythainlp.util.Trie`::
        from pythainlp.tokenize import Tokenizer
        from pythainlp.corpus.common import thai_words
        from pythainlp.util import dict_trie
        custom_words_list = set(thai_words())
        custom_words_list.add('อะเฟเซีย')
        custom_words_list.add('Aphasia')
        trie = dict_trie(dict_source=custom_words_list)
        text = ""อะเฟเซีย (Aphasia*) เป็นอาการผิดปกติของการพูด""
        _tokenizer = Tokenizer(custom_dict=trie, engine='newmm')
        _tokenizer.word_tokenize(text)
        # output: ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ',
        'ผิดปกติ', 'ของ', 'การ', 'พูด']
    Tokenizer object instantiated with a list of words::
        text = ""อะเฟเซีย (Aphasia) เป็นอาการผิดปกติของการพูด""
        _tokenizer = Tokenizer(custom_dict=list(thai_words()), engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะ', 'เฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ',
        #   'ผิดปกติ', 'ของ', 'การ', 'พูด']
    Tokenizer object instantiated with a file path containing a list of
    words separated with *newline* and explicitly setting a new tokenizer
    after initiation::
        PATH_TO_CUSTOM_DICTIONARY = './custom_dictionary.txtt'
        # write a file
        with open(PATH_TO_CUSTOM_DICTIONARY, 'w', encoding='utf-8') as f:
            f.write('อะเฟเซีย\nAphasia\nผิด\nปกติ')
        text = ""อะเฟเซีย (Aphasia) เป็นอาการผิดปกติของการพูด""
        # initiate an object from file with `attacut` as tokenizer
        _tokenizer = Tokenizer(custom_dict=PATH_TO_CUSTOM_DICTIONARY, \
            engine='attacut')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ', 'ผิด',
        #   'ปกติ', 'ของ', 'การ', 'พูด']
        # change tokenizer to `newmm`
        _tokenizer.set_tokenizer_engine(engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็นอาการ', 'ผิด',
        #   'ปกติ', 'ของการพูด']
    '''

    def __init__(self, custom_dict: Union[Trie, Iterable[str], str]=[], engine: str='newmm', keep_whitespace: bool=True, join_broken_num: bool=True):
        '''
        Initialize tokenizer object.
        :param str custom_dict: a file path, a list of vocaburaies* to be
                    used to create a trie, or an instantiated
                    :class:`pythainlp.util.Trie` object.
        :param str engine: choose between different options of tokenizer engines
                            (i.e.  *newmm*, *mm*, *longest*, *deepcut*)
        :param bool keep_whitespace: True to keep whitespace, a common mark
                                     for end of phrase in Thai
        '''
        pass

    def word_tokenize(self, text: str) -> List[str]:
        '''
        Main tokenization function.
        :param str text: text to be tokenized
        :return: list of words, tokenized from the text
        :rtype: list[str]
        '''
        pass

    def set_tokenize_engine(self, engine: str) -> None:
        '''
        Set the tokenizer's engine.
        :param str engine: choose between different options of tokenizer engines
                           (i.e. *newmm*, *mm*, *longest*, *deepcut*)
        '''
        pass",snippet_65,"from typing import Iterable, List, Optional, Union

class Tokenizer:
    '''
    Tokenizer class for a custom tokenizer.
    This class allows users to pre-define custom dictionary along with
    tokenizer and encapsulate them into one single object.
    It is an wrapper for both functions, that are
    :func:`pythainlp.tokenize.word_tokenize`,
    and :func:`pythainlp.util.dict_trie`
    :Example:
    Tokenizer object instantiated with :class:`pythainlp.util.Trie`::
        from pythainlp.tokenize import Tokenizer
        from pythainlp.corpus.common import thai_words
        from pythainlp.util import dict_trie
        custom_words_list = set(thai_words())
        custom_words_list.add('อะเฟเซีย')
        custom_words_list.add('Aphasia')
        trie = dict_trie(dict_source=custom_words_list)
        text = ""อะเฟเซีย (Aphasia*) เป็นอาการผิดปกติของการพูด""
        _tokenizer = Tokenizer(custom_dict=trie, engine='newmm')
        _tokenizer.word_tokenize(text)
        # output: ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ',
        'ผิดปกติ', 'ของ', 'การ', 'พูด']
    Tokenizer object instantiated with a list of words::
        text = ""อะเฟเซีย (Aphasia) เป็นอาการผิดปกติของการพูด""
        _tokenizer = Tokenizer(custom_dict=list(thai_words()), engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะ', 'เฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ',
        #   'ผิดปกติ', 'ของ', 'การ', 'พูด']
    Tokenizer object instantiated with a file path containing a list of
    words separated with *newline* and explicitly setting a new tokenizer
    after initiation::
        PATH_TO_CUSTOM_DICTIONARY = './custom_dictionary.txtt'
        # write a file
        with open(PATH_TO_CUSTOM_DICTIONARY, 'w', encoding='utf-8') as f:
            f.write('อะเฟเซีย\nAphasia\nผิด\nปกติ')
        text = ""อะเฟเซีย (Aphasia) เป็นอาการผิดปกติของการพูด""
        # initiate an object from file with `attacut` as tokenizer
        _tokenizer = Tokenizer(custom_dict=PATH_TO_CUSTOM_DICTIONARY, \
            engine='attacut')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ', 'ผิด',
        #   'ปกติ', 'ของ', 'การ', 'พูด']
        # change tokenizer to `newmm`
        _tokenizer.set_tokenizer_engine(engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็นอาการ', 'ผิด',
        #   'ปกติ', 'ของการพูด']
    '''

    def __init__(self, custom_dict: Union[Iterable[str], str]=[], engine: str='newmm', keep_whitespace: bool=True, join_broken_num: bool=True):
        '''
        Initialize tokenizer object.
        :param str custom_dict: a file path, a list of vocaburaies* to be
                    used to create a trie, or an instantiated
                    :class:`pythainlp.util.Trie` object.
        :param str engine: choose between different options of tokenizer engines
                            (i.e.  *newmm*, *mm*, *longest*, *deepcut*)
        :param bool keep_whitespace: True to keep whitespace, a common mark
                                     for end of phrase in Thai
        '''
        self._engine = engine or 'newmm'
        self._keep_ws = bool(keep_whitespace)
        self._join_broken_num = bool(join_broken_num)
        self._dict: set[str] = set()
        self._max_word_len: int = 0
        self._load_dict(custom_dict)

    def _load_dict(self, custom_dict: Union[Iterable[str], str]) -> None:
        dict_words: set[str] = set()
        if isinstance(custom_dict, str):
            try:
                with open(custom_dict, 'r', encoding='utf-8') as f:
                    for line in f:
                        w = line.strip()
                        if w:
                            dict_words.add(w)
            except FileNotFoundError:
                pass
        else:
            try:
                # Some ""Trie"" implementations may be iterable over words
                for w in custom_dict:  # type: ignore
                    if isinstance(w, str) and w:
                        dict_words.add(w)
            except TypeError:
                pass

        self._dict = dict_words
        self._max_word_len = max((len(w) for w in self._dict), default=0)

    def _is_whitespace(self, ch: str) -> bool:
        return ch.isspace()

    def _is_punct(self, ch: str) -> bool:
        # Treat characters that are neither alnum nor mark nor underscore as punctuation
        # Keep underscore as word character
        if ch.isalnum() or ch == '_':
            return False
        # Unicode category starting with 'M' are marks (e.g., Thai combining marks)
        import unicodedata
        cat = unicodedata.category(ch)
        if cat.startswith('M'):
            return False
        return not self._is_whitespace(ch)

    def _is_word_char(self, ch: str) -> bool:
        if self._is_whitespace(ch):
            return False
        if ch.isalnum() or ch == '_':
            return True
        import unicodedata
        # Treat combining marks as part of word (for scripts like Thai)
        return unicodedata.category(ch).startswith('M')

    def _greedy_match_from(self, s: str, start: int) -> Optional[str]:
        if not self._dict or self._max_word_len <= 0:
            return None
        end_limit = min(len(s), start + self._max_word_len)
        best = None
        # Try longest first
        for end in range(end_limit, start, -1):
            cand = s[start:end]
            if cand in self._dict:
                best = cand
                break
        return best

    def _tokenize_word_run(self, run: str) -> List[str]:
        tokens: List[str] = []
        i = 0
        n = len(run)
        while i < n:
            # If number and join_broken_num, consume digits + [,.] inside
            if self._join_broken_num and run[i].isdigit():
                j = i + 1
                while j < n:
                    c = run[j]
                    if c.isdigit():
                        j += 1
                    elif c in '.,' and j + 1 < n and run[j + 1].isdigit():
                        j += 2
                    else:
                        break
                tokens.append(run[i:j])
                i = j
                continue

            # Try dictionary-based longest match
            m = self._greedy_match_from(run, i)
            if m:
                tokens.append(m)
                i += len(m)
                continue

            # If no dict match, consume contiguous alnum/mark as a unit until punctuation-like in run
            j = i + 1
            while j < n:
                c = run[j]
                if self._is_word_char(c):
                    j += 1
                else:
                    break
            tokens.append(run[i:j])
            i = j
        return tokens

    def word_tokenize(self, text: str) -> List[str]:
        '''
        Main tokenization function.
        :param str text: text to be tokenized
        :return: list of words, tokenized from the text
        :rtype: list[str]
        '''
        if not text:
            return []

        tokens: List[str] = []
        i = 0
        n = len(text)

        while i < n:
            ch = text[i]

            if self._is_whitespace(ch):
                if self._keep_ws:
                    # Normalize any whitespace to single space tokens per character
                    tokens.append(' ')
                # Skip all consecutive whitespace characters
                i += 1
                while i < n and self._is_whitespace(text[i]):
                    if self._keep_ws:
                        tokens.append(' ')
                    i += 1
                continue

            if self._is_word_char(ch):
                # Collect a run of word characters
                j = i + 1
                while j < n and self._is_word_char(text[j]):
                    j += 1
                run = text[i:j]
                tokens.extend(self._tokenize_word_run(run))
                i = j
                continue

            if self._is_punct(ch):
                tokens.append(ch)
                i += 1
                continue

            # Fallback to single character
            tokens.append(ch)
            i += 1

        return tokens

    def set_tokenize_engine(self, engine: str) -> None:
        '''
        Set the tokenizer's engine.
        :param str engine: choose between different options of tokenizer engines
                           (i.e. *newmm*, *mm*, *longest*, *deepcut*)
        '''
        self._engine = engine or self._engine

    # Alias to match example usage
    def set_tokenizer_engine(self, engine: str) -> None:
        self.set_tokenize_engine(engine)"
145446,TomasTomecek/sen,TomasTomecek_sen/sen/tui/commands/base.py,sen.tui.commands.base.ArgumentProcessor,"class ArgumentProcessor:
    """"""
    responsible for parsing given list of arguments
    """"""

    def __init__(self, options, arguments):
        """"""
        :param options: list of options
        :param arguments: list of arguments
        """"""
        self.given_arguments = {}
        self.options = {}
        for a in options:
            self.options[a.name] = a
            self.given_arguments[normalize_arg_name(a.name)] = a.default
            for alias in a.aliases:
                self.options[alias] = a
        for o in arguments:
            self.given_arguments[normalize_arg_name(o.name)] = o.default
        self.arguments = arguments
        logger.info('arguments = %s', arguments)
        logger.info('options = %s', options)

    def process(self, argument_list):
        """"""
        :param argument_list: list of str, input from user
        :return: dict:
            {""cleaned_arg_name"": ""value""}
        """"""
        arg_index = 0
        for a in argument_list:
            opt_and_val = a.split('=', 1)
            opt_name = opt_and_val[0]
            try:
                argument = self.options[opt_name]
            except KeyError:
                try:
                    argument = self.arguments[arg_index]
                except IndexError:
                    logger.error('option/argument %r not specified', a)
                    raise NoSuchOptionOrArgument('No such option or argument: %r' % opt_name)
            logger.info('argument found: %s', argument)
            safe_arg_name = normalize_arg_name(argument.name)
            logger.info('argument is available under name %r', safe_arg_name)
            if isinstance(argument, Argument):
                arg_index += 1
                value = (a,)
            else:
                try:
                    value = (opt_and_val[1],)
                except IndexError:
                    value = tuple()
            arg_val = argument.action(*value)
            logger.info('argument %r has value %r', safe_arg_name, arg_val)
            self.given_arguments[safe_arg_name] = arg_val
        return self.given_arguments","class ArgumentProcessor:
    '''
    responsible for parsing given list of arguments
    '''

    def __init__(self, options, arguments):
        '''
        :param options: list of options
        :param arguments: list of arguments
        '''
        pass

    def process(self, argument_list):
        '''
        :param argument_list: list of str, input from user
        :return: dict:
            {""cleaned_arg_name"": ""value""}
        '''
        pass",snippet_66,"class ArgumentProcessor:
    '''
    responsible for parsing given list of arguments
    '''

    def __init__(self, options, arguments):
        '''
        :param options: list of options
        :param arguments: list of arguments
        '''
        def _norm(name: str) -> str:
            name = name.lstrip('-').strip()
            return name.replace('-', '_')

        self.options = set(_norm(o) for o in (options or []))
        self.arguments = [_norm(a) for a in (arguments or [])]

    def _norm(self, name: str) -> str:
        return name.lstrip('-').replace('-', '_')

    def _coerce(self, value):
        if isinstance(value, bool):
            return value
        if not isinstance(value, str):
            return value
        s = value.strip()
        low = s.lower()
        if low in ('true', 'yes', 'y', 'on'):
            return True
        if low in ('false', 'no', 'n', 'off'):
            return False
        # int
        try:
            if s.startswith(('0x', '0X')):
                return int(s, 16)
            if s.startswith(('0o', '0O')):
                return int(s, 8)
            if s.startswith(('0b', '0B')):
                return int(s, 2)
            return int(s)
        except ValueError:
            pass
        # float
        try:
            return float(s)
        except ValueError:
            pass
        return s

    def _is_opt_allowed(self, name: str) -> bool:
        # If no options were declared, accept any option.
        return (not self.options) or (name in self.options)

    def process(self, argument_list):
        '''
        :param argument_list: list of str, input from user
        :return: dict:
            {""cleaned_arg_name"": ""value""}
        '''
        res = {}
        extra_positionals = []
        i = 0
        pos_idx = 0
        n = len(argument_list)
        end_of_opts = False

        def set_value(key, value):
            k = self._norm(key)
            if not self._is_opt_allowed(k):
                return
            res[k] = self._coerce(value)

        while i < n:
            tok = argument_list[i]

            if end_of_opts:
                # treat as positional
                if pos_idx < len(self.arguments):
                    res[self.arguments[pos_idx]] = self._coerce(tok)
                    pos_idx += 1
                else:
                    extra_positionals.append(self._coerce(tok))
                i += 1
                continue

            if tok == '--':
                end_of_opts = True
                i += 1
                continue

            if tok.startswith('--'):
                # long option
                body = tok[2:]
                if body.startswith('no-') and '=' not in body:
                    name = self._norm(body[3:])
                    set_value(name, False)
                    i += 1
                    continue

                if '=' in body:
                    name, val = body.split('=', 1)
                    set_value(name, val)
                    i += 1
                    continue

                name = body
                # lookahead for value
                if i + 1 < n and not argument_list[i + 1].startswith('-'):
                    set_value(name, argument_list[i + 1])
                    i += 2
                else:
                    set_value(name, True)
                    i += 1
                continue

            if tok.startswith('-') and tok != '-':
                body = tok[1:]

                # handle -o=value
                if '=' in body:
                    head, val = body.split('=', 1)
                    # if bundled before '=', set them true, last gets val
                    if len(head) > 1:
                        for ch in head[:-1]:
                            set_value(ch, True)
                        set_value(head[-1], val)
                    else:
                        set_value(head, val)
                    i += 1
                    continue

                # short flags, possibly bundled
                if len(body) == 1:
                    name = body
                    if i + 1 < n and not argument_list[i + 1].startswith('-'):
                        set_value(name, argument_list[i + 1])
                        i += 2
                    else:
                        set_value(name, True)
                        i += 1
                else:
                    # bundled like -abc; last may take next as value
                    for ch in body[:-1]:
                        set_value(ch, True)
                    last = body[-1]
                    if i + 1 < n and not argument_list[i + 1].startswith('-'):
                        set_value(last, argument_list[i + 1])
                        i += 2
                    else:
                        set_value(last, True)
                        i += 1
                continue

            # positional
            if pos_idx < len(self.arguments):
                res[self.arguments[pos_idx]] = self._coerce(tok)
                pos_idx += 1
            else:
                extra_positionals.append(self._coerce(tok))
            i += 1

        # fill any missing positionals with None (optional)
        # Do not overwrite if already set via options
        while pos_idx < len(self.arguments):
            name = self.arguments[pos_idx]
            if name not in res:
                res[name] = None
            pos_idx += 1

        if extra_positionals:
            res['_extra'] = extra_positionals

        return res"
152154,hellock/icrawler,hellock_icrawler/icrawler/storage/base.py,icrawler.storage.base.BaseStorage,"from abc import ABCMeta, abstractmethod

class BaseStorage:
    """"""Base class of backend storage""""""
    __metaclass__ = ABCMeta

    @abstractmethod
    def write(self, id, data):
        """"""Abstract interface of writing data

        Args:
            id (str): unique id of the data in the storage.
            data (bytes or str): data to be stored.
        """"""
        return

    @abstractmethod
    def exists(self, id):
        """"""Check the existence of some data

        Args:
            id (str): unique id of the data in the storage

        Returns:
            bool: whether the data exists
        """"""
        return False

    @abstractmethod
    def max_file_idx(self):
        """"""Get the max existing file index

        Returns:
            int: the max index
        """"""
        return 0","
class BaseStorage:
    '''Base class of backend storage'''
    @abstractmethod
    def write(self, id, data):
        '''Abstract interface of writing data
        Args:
            id (str): unique id of the data in the storage.
            data (bytes or str): data to be stored.
        '''
        pass
    @abstractmethod
    def exists(self, id):
        '''Check the existence of some data
        Args:
            id (str): unique id of the data in the storage
        Returns:
            bool: whether the data exists
        '''
        pass
    @abstractmethod
    def max_file_idx(self):
        '''Get the max existing file index
        Returns:
            int: the max index
        '''
        pass",snippet_69,"from abc import ABC, abstractmethod
from typing import Union


class BaseStorage(ABC):
    '''Base class of backend storage'''

    @abstractmethod
    def write(self, id: str, data: Union[bytes, str]) -> None:
        '''Abstract interface of writing data
        Args:
            id (str): unique id of the data in the storage.
            data (bytes or str): data to be stored.
        '''
        raise NotImplementedError

    @abstractmethod
    def exists(self, id: str) -> bool:
        '''Check the existence of some data
        Args:
            id (str): unique id of the data in the storage
        Returns:
            bool: whether the data exists
        '''
        raise NotImplementedError

    @abstractmethod
    def max_file_idx(self) -> int:
        '''Get the max existing file index
        Returns:
            int: the max index
        '''
        raise NotImplementedError"
155190,sentinel-hub/sentinelhub-py,sentinelhub/download/rate_limit.py,sentinelhub.download.rate_limit.SentinelHubRateLimit,"import time

class SentinelHubRateLimit:
    """"""Class implementing rate limiting logic of Sentinel Hub service

    It has 2 public methods:

    - register_next - tells if next download can start or if not, what is the wait before it can be asked again
    - update - updates expectations according to headers obtained from download

    The rate limiting object is collecting information about the status of rate limiting policy buckets from
    Sentinel Hub service. According to this information and a feedback from download requests it adapts expectations
    about when the next download attempt will be possible.
    """"""
    RETRY_HEADER = 'Retry-After'
    UNITS_SPENT_HEADER = 'X-ProcessingUnits-Spent'

    def __init__(self, num_processes: int=1, minimum_wait_time: float=0.05, maximum_wait_time: float=60.0):
        """"""
        :param num_processes: Number of parallel download processes running.
        :param minimum_wait_time: Minimum wait time between two consecutive download requests in seconds.
        :param maximum_wait_time: Maximum wait time between two consecutive download requests in seconds.
        """"""
        self.wait_time = min(num_processes * minimum_wait_time, maximum_wait_time)
        self.next_download_time = time.monotonic()

    def register_next(self) -> float:
        """"""Determines if next download request can start or not by returning the waiting time in seconds.""""""
        current_time = time.monotonic()
        wait_time = max(self.next_download_time - current_time, 0)
        if wait_time == 0:
            self.next_download_time = max(current_time + self.wait_time, self.next_download_time)
        return wait_time

    def update(self, headers: dict, *, default: float) -> None:
        """"""Update the next possible download time if the service has responded with the rate limit.

        :param headers: The headers that (may) contain information about waiting times.
        :param default: The default waiting time (in milliseconds) when retrying after getting a
            TOO_MANY_REQUESTS response without appropriate retry headers.
        """"""
        retry_after: float = int(headers.get(self.RETRY_HEADER, default))
        retry_after = retry_after / 1000
        if retry_after:
            self.next_download_time = max(time.monotonic() + retry_after, self.next_download_time)","
class SentinelHubRateLimit:
    '''Class implementing rate limiting logic of Sentinel Hub service
    It has 2 public methods:
    - register_next - tells if next download can start or if not, what is the wait before it can be asked again
    - update - updates expectations according to headers obtained from download
    The rate limiting object is collecting information about the status of rate limiting policy buckets from
    Sentinel Hub service. According to this information and a feedback from download requests it adapts expectations
    about when the next download attempt will be possible.
    '''

    def __init__(self, num_processes: int=1, minimum_wait_time: float=0.05, maximum_wait_time: float=60.0):
        '''
        :param num_processes: Number of parallel download processes running.
        :param minimum_wait_time: Minimum wait time between two consecutive download requests in seconds.
        :param maximum_wait_time: Maximum wait time between two consecutive download requests in seconds.
        '''
        pass

    def register_next(self) -> float:
        '''Determines if next download request can start or not by returning the waiting time in seconds.'''
        pass

    def update(self, headers: dict, *, default: float) -> None:
        '''Update the next possible download time if the service has responded with the rate limit.
        :param headers: The headers that (may) contain information about waiting times.
        :param default: The default waiting time (in milliseconds) when retrying after getting a
            TOO_MANY_REQUESTS response without appropriate retry headers.
        '''
        pass",snippet_73,"import threading
import time
from email.utils import parsedate_to_datetime
from typing import Optional


class SentinelHubRateLimit:
    '''Class implementing rate limiting logic of Sentinel Hub service
    It has 2 public methods:
    - register_next - tells if next download can start or if not, what is the wait before it can be asked again
    - update - updates expectations according to headers obtained from download
    The rate limiting object is collecting information about the status of rate limiting policy buckets from
    Sentinel Hub service. According to this information and a feedback from download requests it adapts expectations
    about when the next download attempt will be possible.
    '''

    def __init__(self, num_processes: int=1, minimum_wait_time: float=0.05, maximum_wait_time: float=60.0):
        '''
        :param num_processes: Number of parallel download processes running.
        :param minimum_wait_time: Minimum wait time between two consecutive download requests in seconds.
        :param maximum_wait_time: Maximum wait time between two consecutive download requests in seconds.
        '''
        if num_processes <= 0:
            raise ValueError(""num_processes must be >= 1"")
        if minimum_wait_time < 0:
            raise ValueError(""minimum_wait_time must be >= 0"")
        if maximum_wait_time <= 0:
            raise ValueError(""maximum_wait_time must be > 0"")
        if minimum_wait_time > maximum_wait_time:
            raise ValueError(""minimum_wait_time must be <= maximum_wait_time"")

        self._num_processes = int(num_processes)
        self._min_wait = float(minimum_wait_time)
        self._max_wait = float(maximum_wait_time)

        # Spread requests from multiple processes by splitting the minimum wait
        self._slot_interval = max(self._min_wait / self._num_processes, 0.0)

        self._lock = threading.Lock()
        self._next_mono = time.monotonic()

    def register_next(self) -> float:
        '''Determines if next download request can start or not by returning the waiting time in seconds.'''
        with self._lock:
            now = time.monotonic()
            wait = max(0.0, self._next_mono - now)
            if wait <= 0.0:
                # Reserve the next available slot immediately
                self._next_mono = max(self._next_mono, now) + self._slot_interval
                return 0.0
            # Do not modify schedule if we are asked too early
            return min(wait, self._max_wait)

    def update(self, headers: dict, *, default: float) -> None:
        '''Update the next possible download time if the service has responded with the rate limit.
        :param headers: The headers that (may) contain information about waiting times.
        :param default: The default waiting time (in milliseconds) when retrying after getting a
            TOO_MANY_REQUESTS response without appropriate retry headers.
        '''
        headers = {str(k).lower(): str(v) for k, v in (headers or {}).items()}
        now_wall = time.time()
        now_mono = time.monotonic()

        wait_seconds = self._extract_wait_seconds(headers, now_wall)
        if wait_seconds is None:
            # Fallback to provided default (milliseconds)
            try:
                wait_seconds = max(0.0, float(default) / 1000.0)
            except Exception:
                wait_seconds = self._min_wait

        # Clamp to configured bounds
        wait_seconds = max(self._min_wait, min(wait_seconds, self._max_wait))

        with self._lock:
            target = now_mono + wait_seconds
            if target > self._next_mono:
                self._next_mono = target

    def _extract_wait_seconds(self, headers: dict, now_wall: float) -> Optional[float]:
        # Retry-After: either delta-seconds or HTTP-date
        ra = headers.get('retry-after')
        if ra:
            ra = ra.strip()
            # Try delta seconds
            try:
                val = float(ra)
                if val >= 0:
                    return val
            except ValueError:
                pass
            # Try HTTP-date
            dt = self._parse_http_date(ra)
            if dt is not None:
                ts = dt.timestamp()
                return max(0.0, ts - now_wall)

        # X-RateLimit-Reset-After: seconds to wait
        xr_after = headers.get('x-ratelimit-reset-after') or headers.get('x-rate-limit-reset-after')
        if xr_after:
            try:
                val = float(xr_after)
                if val >= 0:
                    return val
            except ValueError:
                pass

        # X-RateLimit-Reset: absolute reset time (epoch seconds or milliseconds)
        xr = headers.get('x-ratelimit-reset') or headers.get('x-rate-limit-reset')
        if xr:
            # Some services send epoch seconds; others epoch milliseconds.
            # Heuristic: > 1e11 ⇒ ms, > 1e10 sometimes ms; else seconds.
            try:
                reset_val = float(xr)
                if reset_val > 1e11:
                    reset_epoch = reset_val / 1000.0
                elif reset_val > 1e10:
                    reset_epoch = reset_val / 1000.0
                else:
                    reset_epoch = reset_val
                return max(0.0, reset_epoch - now_wall)
            except ValueError:
                # Sometimes they send HTTP-date here
                dt = self._parse_http_date(xr)
                if dt is not None:
                    return max(0.0, dt.timestamp() - now_wall)

        # Some providers specify window intervals; attempt conservative pacing
        # X-RateLimit-Interval: e.g., ""1s"", ""10s"", ""1m""
        xint = headers.get('x-ratelimit-interval') or headers.get('x-rate-limit-interval')
        if xint:
            secs = self._parse_duration_seconds(xint)
            if secs is not None and secs > 0:
                # Minimal wait is a fraction of interval considering processes
                return max(self._min_wait, secs / max(1, self._num_processes))

        return None

    @staticmethod
    def _parse_http_date(value: str):
        try:
            dt = parsedate_to_datetime(value)
            # Ensure timezone-aware datetime
            if dt is not None and dt.tzinfo is None:
                # Assume UTC if tz not provided
                from datetime import timezone
                dt = dt.replace(tzinfo=timezone.utc)
            return dt
        except Exception:
            return None

    @staticmethod
    def _parse_duration_seconds(value: str) -> Optional[float]:
        s = value.strip().lower()
        try:
            # Plain number => seconds
            return float(s)
        except ValueError:
            pass
        # Support suffixes: ms, s, m, h
        units = [('ms', 1/1000.0), ('s', 1.0), ('m', 60.0), ('h', 3600.0)]
        for suf, mul in units:
            if s.endswith(suf):
                try:
                    return float(s[:-len(suf)].strip()) * mul
                except ValueError:
                    return None
        return None"
156520,textX/textX,textx/scoping/providers.py,textx.scoping.providers.PlainName,"from textx.exceptions import TextXSemanticError

class PlainName:
    """"""
    plain name scope provider
    """"""

    def __init__(self, multi_metamodel_support=True):
        """"""
        the default scope provider constructor

        Args:
            multi_metamodel_support: enable a AST based search, instead
            of using the parser._instances
        """"""
        self.multi_metamodel_support = multi_metamodel_support
        pass

    def __call__(self, obj, attr, obj_ref):
        """"""
        the default scope provider

        Args:
            obj: unused (used for multi_metamodel_support)
            attr: unused
            obj_ref: the cross reference to be resolved

        Returns:
            the resolved reference or None
        """"""
        from textx.const import RULE_ABSTRACT, RULE_COMMON
        from textx.model import ObjCrossRef
        from textx.scoping.tools import get_parser
        if obj_ref is None:
            return None
        assert type(obj_ref) is ObjCrossRef, type(obj_ref)
        if get_parser(obj).debug:
            get_parser(obj).dprint(f'Resolving obj crossref: {obj_ref.cls}:{obj_ref.obj_name}')

        def _inner_resolve_link_rule_ref(cls, obj_name):
            """"""
            Depth-first resolving of link rule reference.
            """"""
            if cls._tx_type is RULE_ABSTRACT:
                for inherited in cls._tx_inh_by:
                    result = _inner_resolve_link_rule_ref(inherited, obj_name)
                    if result:
                        return result
            elif cls._tx_type == RULE_COMMON and id(cls) in get_parser(obj)._instances:
                objs = get_parser(obj)._instances[id(cls)]
                return objs.get(obj_name)
        if self.multi_metamodel_support:
            from textx import get_children, get_model, textx_isinstance
            result_lst = get_children(lambda x: hasattr(x, 'name') and x.name == obj_ref.obj_name and textx_isinstance(x, obj_ref.cls), get_model(obj))
            if len(result_lst) == 1:
                result = result_lst[0]
            elif len(result_lst) > 1:
                line, col = get_parser(obj).pos_to_linecol(obj_ref.position)
                raise TextXSemanticError(f'name {obj_ref.obj_name} is not unique.', line=line, col=col, filename=get_model(obj)._tx_filename)
            else:
                result = None
        else:
            result = _inner_resolve_link_rule_ref(obj_ref.cls, obj_ref.obj_name)
        return result","
class PlainName:
    '''
    plain name scope provider
    '''

    def __init__(self, multi_metamodel_support=True):
        '''
        the default scope provider constructor
        Args:
            multi_metamodel_support: enable a AST based search, instead
            of using the parser._instances
        '''
        pass

    def __call__(self, obj, attr, obj_ref):
        '''
        the default scope provider
        Args:
            obj: unused (used for multi_metamodel_support)
            attr: unused
            obj_ref: the cross reference to be resolved
        Returns:
            the resolved reference or None
        '''
        pass

        def _inner_resolve_link_rule_ref(cls, obj_name):
            '''
            Depth-first resolving of link rule reference.
                '''
                pass",snippet_74,"class PlainName:
    '''
    plain name scope provider
    '''

    def __init__(self, multi_metamodel_support=True):
        '''
        the default scope provider constructor
        Args:
            multi_metamodel_support: enable a AST based search, instead
            of using the parser._instances
        '''
        self.multi_metamodel_support = bool(multi_metamodel_support)

    def __call__(self, obj, attr, obj_ref):
        '''
        the default scope provider
        Args:
            obj: unused (used for multi_metamodel_support)
            attr: unused
            obj_ref: the cross reference to be resolved
        Returns:
            the resolved reference or None
        '''
        # Helper getters
        def _get_name(o):
            for n in (""obj_name"", ""name"", ""id""):
                if hasattr(o, n):
                    return getattr(o, n)
            if isinstance(o, str):
                return o
            return None

        def _get_expected_classes():
            candidates = []
            for holder in (obj_ref, attr):
                if holder is None:
                    continue
                for n in (""cls"", ""type"", ""expected_cls"", ""expected_type""):
                    if hasattr(holder, n):
                        c = getattr(holder, n)
                        if c is not None:
                            candidates.append(c)
                # Some frameworks store multiple acceptable classes
                for n in (""classes"", ""types"", ""expected_classes"", ""expected_types""):
                    if hasattr(holder, n):
                        cs = getattr(holder, n)
                        if cs:
                            candidates.extend(cs if isinstance(cs, (list, tuple, set)) else [cs])
            # Deduplicate while preserving order
            seen = set()
            result = []
            for c in candidates:
                if isinstance(c, type) and c not in seen:
                    seen.add(c)
                    result.append(c)
            return tuple(result)

        def _root_of(o):
            # Try textX-like ancestry first
            visited = set()
            cur = o
            while True:
                if cur is None:
                    break
                obj_id = id(cur)
                if obj_id in visited:
                    break
                visited.add(obj_id)
                parent = getattr(cur, ""_tx_parent"", None)
                if parent is None:
                    break
                cur = parent
            # Prefer model root if available
            model = getattr(cur, ""_tx_model"", None) or getattr(o, ""_tx_model"", None)
            return model or cur

        def _iter_elements(root):
            # Generic DFS over Python object graphs while avoiding cycles
            stack = [root]
            seen = set()
            while stack:
                cur = stack.pop()
                oid = id(cur)
                if oid in seen:
                    continue
                seen.add(oid)
                # Yield only ""element-like"" objects (instances with dict)
                if hasattr(cur, ""__dict__""):
                    yield cur
                    # Explore attributes
                    for k, v in vars(cur).items():
                        if k.startswith(""_tx_""):
                            # Still dive into some known textX graph links
                            if k in (""_tx_children"", ""_tx_model_repository""):
                                pass
                            else:
                                # Skip common internal attributes
                                continue
                        _push_value(v, stack)
                elif isinstance(cur, (list, tuple, set, frozenset)):
                    for v in cur:
                        _push_value(v, stack)
                elif isinstance(cur, dict):
                    for v in cur.values():
                        _push_value(v, stack)

        def _push_value(v, stack):
            if v is None:
                return
            if isinstance(v, (str, bytes, int, float, bool)):
                return
            stack.append(v)

        def _match_expected(o, expected_types):
            if not expected_types:
                return True
            return any(isinstance(o, t) for t in expected_types)

        def _resolve_dotted_from_scope(scope_root, expected_types, dotted_name):
            parts = [p for p in str(dotted_name).split(""."") if p]
            if not parts:
                return None
            # Find candidates for the first part in the entire scope
            first = parts[0]

            def _children_of(node):
                # Iterate over direct children of a node
                for k, v in vars(node).items() if hasattr(node, ""__dict__"") else []:
                    if k.startswith(""_tx_""):
                        continue
                    if isinstance(v, (list, tuple, set, frozenset)):
                        for it in v:
                            if hasattr(it, ""__dict__""):
                                yield it
                    elif hasattr(v, ""__dict__""):
                        yield v

            def _find_all_named(root, name):
                for e in _iter_elements(root):
                    if _get_name(e) == name:
                        yield e

            def _resolve_from(node, idx):
                if idx >= len(parts):
                    return node
                target_name = parts[idx]
                # Search among node's children for next name
                for child in _children_of(node):
                    if _get_name(child) == target_name:
                        if idx == len(parts) - 1:
                            return child if _match_expected(child, expected_types) else None
                        found = _resolve_from(child, idx + 1)
                        if found is not None:
                            return found
                return None

            for candidate in _find_all_named(scope_root, first):
                if len(parts) == 1:
                    if _match_expected(candidate, expected_types):
                        return candidate
                    continue
                res = _resolve_from(candidate, 1)
                if res is not None and _match_expected(res, expected_types):
                    return res
            return None

        def _resolve_via_parser_instances(parser, expected_types, name):
            instances = getattr(parser, ""_instances"", None)
            if not instances:
                return None
            # instances can be dict[type] -> set/list of instances
            # Prefer expected types if provided, else search all
            if expected_types:
                search_types = [t for t in expected_types if t in instances]
            else:
                search_types = list(instances.keys())
            for t in search_types:
                try:
                    for inst in instances.get(t, []):
                        if _get_name(inst) == name:
                            return inst
                except Exception:
                    continue
            # Fallback: scan all instances if nothing found
            if not expected_types:
                for bucket in instances.values():
                    try:
                        for inst in bucket:
                            if _get_name(inst) == name:
                                return inst
                    except Exception:
                        continue
            return None

        # Actual resolution
        obj_name = _get_name(obj_ref)
        if not obj_name:
            return None

        expected_types = _get_expected_classes()

        # Dotted names first using AST when enabled
        if self.multi_metamodel_support:
            root = _root_of(obj)
            if ""."" in str(obj_name):
                found = _resolve_dotted_from_scope(root, expected_types, obj_name)
                if found is not None:
                    return found
            # Plain name search across AST
            for element in _iter_elements(root):
                if _get_name(element) == obj_name and _match_expected(element, expected_types):
                    return element
            return None

        # Parser instances based resolution
        parser = None
        model = getattr(obj, ""_tx_model"", None)
        if model is not None:
            parser = getattr(model, ""_tx_parser"", None)
        if parser is not None:
            if ""."" in str(obj_name):
                # Try dotted name by stepwise filtering via instances/AST hybrid:
                # fall back to AST if dotted and instances are insufficient
                root = _root_of(obj)
                found = _resolve_dotted_from_scope(root, expected_types, obj_name)
                if found is not None:
                    return found
            # Plain via instances
            found = _resolve_via_parser_instances(parser, expected_types, obj_name)
            if found is not None:
                return found

        # Final fallback: AST scan even if multi_metamodel_support is False
        root = _root_of(obj)
        if ""."" in str(obj_name):
            found = _resolve_dotted_from_scope(root, expected_types, obj_name)
            if found is not None:
                return found
        for element in _iter_elements(root):
            if _get_name(element) == obj_name and _match_expected(element, expected_types):
                return element
        return None

        def _inner_resolve_link_rule_ref(cls, obj_name):
            '''
            Depth-first resolving of link rule reference.
            '''
            # Kept for backward-compatibility; not used due to the more general resolver above.
            root = _root_of(obj)
            if ""."" in str(obj_name):
                return _resolve_dotted_from_scope(root, (cls,) if isinstance(cls, type) else tuple(cls or []), obj_name)
            # Plain name
            for element in _iter_elements(root):
                if _get_name(element) == obj_name and (cls is None or isinstance(element, cls)):
                    return element
            return None"
156861,csparpa/pyowm,csparpa_pyowm/pyowm/utils/geo.py,pyowm.utils.geo.Geometry,"class Geometry:
    """"""
    Abstract parent class for geotypes

    """"""

    def geojson(self):
        """"""
        Returns a GeoJSON string representation of this geotype, compliant to
        RFC 7946 (https://tools.ietf.org/html/rfc7946)
        :return: str
        """"""
        raise NotImplementedError()

    def to_dict(self):
        """"""
        Returns a dict representation of this geotype
        :return: dict
        """"""
        raise NotImplementedError()","class Geometry:
    '''
    Abstract parent class for geotypes
    '''

    def geojson(self):
        '''
        Returns a GeoJSON string representation of this geotype, compliant to
        RFC 7946 (https://tools.ietf.org/html/rfc7946)
        :return: str
        '''
        pass

    def to_dict(self):
        '''
        Returns a dict representation of this geotype
        :return: dict
        '''
        pass",snippet_75,"import json

class Geometry:
    '''
    Abstract parent class for geotypes
    '''

    def geojson(self):
        '''
        Returns a GeoJSON string representation of this geotype, compliant to
        RFC 7946 (https://tools.ietf.org/html/rfc7946)
        :return: str
        '''
        return json.dumps(self.to_dict(), ensure_ascii=False, separators=("","", "":""))

    def to_dict(self):
        '''
        Returns a dict representation of this geotype
        :return: dict
        '''
        raise NotImplementedError(""Subclasses must implement to_dict()"")"
157718,jwkvam/bowtie,bowtie/_cache.py,bowtie._cache._Cache,"from eventlet.queue import LightQueue
from flask_socketio import emit
import eventlet
import flask
import msgpack
from bowtie._component import pack

class _Cache:
    """"""Store data in the browser.

    This cache uses session storage so data will stay
    in the browser until the tab is closed.
    All data must be serializable, which means if the
    serialization transforms the data it won't be the same
    when it is fetched.

    Examples
    --------
    >>> from bowtie import cache
    >>> cache['a'] = True  # doctest: +SKIP
    >>> cache['a']  # doctest: +SKIP
    True
    >>> cache['b'] = np.arange(5)  # doctest: +SKIP
    >>> cache['b']  # doctest: +SKIP
    [1, 2, 3, 4, 5]

    """"""

    def __getitem__(self, key):
        """"""Load the value stored with the key.

        Parameters
        ----------
        key : str
            The key to lookup the value stored.

        Returns
        -------
        object
            The value if the key exists in the cache, otherwise None.

        """"""
        validate(key)
        signal = 'cache_load'
        event = LightQueue(1)
        if flask.has_request_context():
            emit(signal, {'data': pack(key)}, callback=event.put)
        else:
            sio = flask.current_app.extensions['socketio']
            sio.emit(signal, {'data': pack(key)}, callback=event.put)
        return msgpack.unpackb(bytes(event.get(timeout=10)), encoding='utf8')

    def __setitem__(self, key, value):
        """"""Store the key value pair.

        Parameters
        ----------
        key : str
            The key to determine where it's stored, you'll need this to load the value later.
        value : object
            The value to store in the cache.

        Returns
        -------
        None

        """"""
        validate(key)
        signal = 'cache_save'
        if flask.has_request_context():
            emit(signal, {'key': pack(key), 'data': pack(value)})
        else:
            sio = flask.current_app.extensions['socketio']
            sio.emit(signal, {'key': pack(key), 'data': pack(value)})
        eventlet.sleep()","
class _Cache:
    '''Store data in the browser.
    This cache uses session storage so data will stay
    in the browser until the tab is closed.
    All data must be serializable, which means if the
    serialization transforms the data it won't be the same
    when it is fetched.
    Examples
    --------
    >>> from bowtie import cache
    >>> cache['a'] = True  # doctest: +SKIP
    >>> cache['a']  # doctest: +SKIP
    True
    >>> cache['b'] = np.arange(5)  # doctest: +SKIP
    >>> cache['b']  # doctest: +SKIP
    [1, 2, 3, 4, 5]
    '''

    def __getitem__(self, key):
        '''Load the value stored with the key.
        Parameters
        ----------
        key : str
            The key to lookup the value stored.
        Returns
        -------
        object
            The value if the key exists in the cache, otherwise None.
        '''
        pass

    def __setitem__(self, key, value):
        '''Store the key value pair.
        Parameters
        ----------
        key : str
            The key to determine where it's stored, you'll need this to load the value later.
        value : object
            The value to store in the cache.
        Returns
        -------
        None
        '''
        pass",snippet_78,"class _Cache:
    '''Store data in the browser.
    This cache uses session storage so data will stay
    in the browser until the tab is closed.
    All data must be serializable, which means if the
    serialization transforms the data it won't be the same
    when it is fetched.
    Examples
    --------
    >>> from bowtie import cache
    >>> cache['a'] = True  # doctest: +SKIP
    >>> cache['a']  # doctest: +SKIP
    True
    >>> cache['b'] = np.arange(5)  # doctest: +SKIP
    >>> cache['b']  # doctest: +SKIP
    [1, 2, 3, 4, 5]
    '''

    _storage = {}

    def __getitem__(self, key):
        '''Load the value stored with the key.
        Parameters
        ----------
        key : str
            The key to lookup the value stored.
        Returns
        -------
        object
            The value if the key exists in the cache, otherwise None.
        '''
        import json
        data = self._storage.get(key)
        if data is None:
            return None

        def object_hook(obj):
            if isinstance(obj, dict) and obj.get('__type__') == 'bytes':
                import base64
                return base64.b64decode(obj['data'])
            return obj

        try:
            return json.loads(data, object_hook=object_hook)
        except Exception:
            return None

    def __setitem__(self, key, value):
        '''Store the key value pair.
        Parameters
        ----------
        key : str
            The key to determine where it's stored, you'll need this to load the value later.
        value : object
            The value to store in the cache.
        Returns
        -------
        None
        '''
        import json

        def default(o):
            # Numpy support (optional)
            try:
                import numpy as np  # noqa: F401
                import numpy as _np
                if isinstance(o, _np.ndarray):
                    return o.tolist()
                if isinstance(o, (_np.integer,)):
                    return int(o)
                if isinstance(o, (_np.floating,)):
                    return float(o)
                if isinstance(o, (_np.bool_,)):
                    return bool(o)
            except Exception:
                pass

            # Bytes as base64
            if isinstance(o, (bytes, bytearray, memoryview)):
                import base64
                return {
                    '__type__': 'bytes',
                    'data': base64.b64encode(bytes(o)).decode('ascii'),
                }

            # Sets/tuples become lists (may not round-trip type)
            if isinstance(o, (set, tuple)):
                return list(o)

            # Fallback: try to use string representation
            try:
                return str(o)
            except Exception:
                raise TypeError(f'Object of type {type(o).__name__} is not JSON serializable')

        payload = json.dumps(value, default=default, ensure_ascii=False)
        self._storage[key] = payload"
159496,Erotemic/ubelt,Erotemic_ubelt/ubelt/util_mixins.py,ubelt.util_mixins.NiceRepr,"import warnings

class NiceRepr:
    """"""
    Inherit from this class and define ``__nice__`` to ""nicely"" print your
    objects.

    Defines ``__str__`` and ``__repr__`` in terms of ``__nice__`` function
    Classes that inherit from :class:`NiceRepr` should redefine ``__nice__``.
    If the inheriting class has a ``__len__``, method then the default
    ``__nice__`` method will return its length.

    Example:
        >>> import ubelt as ub
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        return 'info'
        >>> foo = Foo()
        >>> assert str(foo) == '<Foo(info)>'
        >>> assert repr(foo).startswith('<Foo(info) at ')

    Example:
        >>> import ubelt as ub
        >>> class Bar(ub.NiceRepr):
        ...    pass
        >>> bar = Bar()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     assert 'object at' in str(bar)
        >>>     assert 'object at' in repr(bar)

    Example:
        >>> import ubelt as ub
        >>> class Baz(ub.NiceRepr):
        ...    def __len__(self):
        ...        return 5
        >>> baz = Baz()
        >>> assert str(baz) == '<Baz(5)>'

    Example:
        >>> import ubelt as ub
        >>> # If your nice message has a bug, it shouldn't bring down the house
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        assert False
        >>> foo = Foo()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     print('foo = {!r}'.format(foo))
        foo = <...Foo ...>

    Example:
        >>> import ubelt as ub
        >>> class Animal(ub.NiceRepr):
        ...    def __init__(self):
        ...        ...
        ...    def __nice__(self):
        ...        return ''
        >>> class Cat(Animal):
        >>>     ...
        >>> class Dog(Animal):
        >>>     ...
        >>> class Beagle(Dog):
        >>>     ...
        >>> class Ragdoll(Cat):
        >>>     ...
        >>> instances = [Animal(), Cat(), Dog(), Beagle(), Ragdoll()]
        >>> for inst in instances:
        >>>     print(str(inst))
        <Animal()>
        <Cat()>
        <Dog()>
        <Beagle()>
        <Ragdoll()>

    In the case where you cant or dont want to use ubelt.NiceRepr you can get
    similar behavior by pasting the methods from the following snippet into
    your class:

    .. code:: python

        class MyClass:

            def __nice__(self):
                return 'your concise information'

            def __repr__(self):
                nice = self.__nice__()
                classname = self.__class__.__name__
                return '<{0}({1}) at {2}>'.format(classname, nice, hex(id(self)))

            def __str__(self):
                classname = self.__class__.__name__
                nice = self.__nice__()
                return '<{0}({1})>'.format(classname, nice)
    """"""

    def __nice__(self):
        """"""
        Returns:
            str
        """"""
        if hasattr(self, '__len__'):
            return str(len(self))
        else:
            raise NotImplementedError('Define the __nice__ method for {!r}'.format(self.__class__))

    def __repr__(self):
        """"""
        Returns:
            str
        """"""
        try:
            nice = self.__nice__()
            classname = self.__class__.__name__
            return '<{0}({1}) at {2}>'.format(classname, nice, hex(id(self)))
        except Exception as ex:
            warnings.warn(str(ex), category=RuntimeWarning)
            return object.__repr__(self)

    def __str__(self):
        """"""
        Returns:
            str
        """"""
        try:
            classname = self.__class__.__name__
            nice = self.__nice__()
            return '<{0}({1})>'.format(classname, nice)
        except Exception as ex:
            warnings.warn(str(ex), category=RuntimeWarning)
            return object.__repr__(self)","
class NiceRepr:
    '''
    Inherit from this class and define ``__nice__`` to ""nicely"" print your
    objects.
    Defines ``__str__`` and ``__repr__`` in terms of ``__nice__`` function
    Classes that inherit from :class:`NiceRepr` should redefine ``__nice__``.
    If the inheriting class has a ``__len__``, method then the default
    ``__nice__`` method will return its length.
    Example:
        >>> import ubelt as ub
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        return 'info'
        >>> foo = Foo()
        >>> assert str(foo) == '<Foo(info)>'
        >>> assert repr(foo).startswith('<Foo(info) at ')
    Example:
        >>> import ubelt as ub
        >>> class Bar(ub.NiceRepr):
        ...    pass
        >>> bar = Bar()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     assert 'object at' in str(bar)
        >>>     assert 'object at' in repr(bar)
    Example:
        >>> import ubelt as ub
        >>> class Baz(ub.NiceRepr):
        ...    def __len__(self):
        ...        return 5
        >>> baz = Baz()
        >>> assert str(baz) == '<Baz(5)>'
    Example:
        >>> import ubelt as ub
        >>> # If your nice message has a bug, it shouldn't bring down the house
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        assert False
        >>> foo = Foo()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     print('foo = {!r}'.format(foo))
        foo = <...Foo ...>
    Example:
        >>> import ubelt as ub
        >>> class Animal(ub.NiceRepr):
        ...    def __init__(self):
        ...        ...
        ...    def __nice__(self):
        ...        return ''
        >>> class Cat(Animal):
        >>>     ...
        >>> class Dog(Animal):
        >>>     ...
        >>> class Beagle(Dog):
        >>>     ...
        >>> class Ragdoll(Cat):
        >>>     ...
        >>> instances = [Animal(), Cat(), Dog(), Beagle(), Ragdoll()]
        >>> for inst in instances:
        >>>     print(str(inst))
        <Animal()>
        <Cat()>
        <Dog()>
        <Beagle()>
        <Ragdoll()>
    In the case where you cant or dont want to use ubelt.NiceRepr you can get
    similar behavior by pasting the methods from the following snippet into
    your class:
    .. code:: python
        class MyClass:
            def __nice__(self):
                return 'your concise information'
            def __repr__(self):
                nice = self.__nice__()
                classname = self.__class__.__name__
                return '<{0}({1}) at {2}>'.format(classname, nice, hex(id(self)))
            def __str__(self):
                classname = self.__class__.__name__
                nice = self.__nice__()
                return '<{0}({1})>'.format(classname, nice)
                    '''

            def __nice__(self):
                '''
        Returns:
            str
                        '''
                        pass

            def __repr__(self):
                '''
        Returns:
            str
                        '''
                        pass

            def __str__(self):
                '''
        Returns:
            str
                        '''
                        pass",snippet_80,"class NiceRepr:
    '''
    Inherit from this class and define ``__nice__`` to ""nicely"" print your
    objects.
    Defines ``__str__`` and ``__repr__`` in terms of ``__nice__`` function
    Classes that inherit from :class:`NiceRepr` should redefine ``__nice__``.
    If the inheriting class has a ``__len__``, method then the default
    ``__nice__`` method will return its length.
    Example:
        >>> import ubelt as ub
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        return 'info'
        >>> foo = Foo()
        >>> assert str(foo) == '<Foo(info)>'
        >>> assert repr(foo).startswith('<Foo(info) at ')
    Example:
        >>> import ubelt as ub
        >>> class Bar(ub.NiceRepr):
        ...    pass
        >>> bar = Bar()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     assert 'object at' in str(bar)
        >>>     assert 'object at' in repr(bar)
    Example:
        >>> import ubelt as ub
        >>> class Baz(ub.NiceRepr):
        ...    def __len__(self):
        ...        return 5
        >>> baz = Baz()
        >>> assert str(baz) == '<Baz(5)>'
    Example:
        >>> import ubelt as ub
        >>> # If your nice message has a bug, it shouldn't bring down the house
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        assert False
        >>> foo = Foo()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     print('foo = {!r}'.format(foo))
        foo = <...Foo ...>
    Example:
        >>> import ubelt as ub
        >>> class Animal(ub.NiceRepr):
        ...    def __init__(self):
        ...        ...
        ...    def __nice__(self):
        ...        return ''
        >>> class Cat(Animal):
        >>>     ...
        >>> class Dog(Animal):
        >>>     ...
        >>> class Beagle(Dog):
        >>>     ...
        >>> class Ragdoll(Cat):
        >>>     ...
        >>> instances = [Animal(), Cat(), Dog(), Beagle(), Ragdoll()]
        >>> for inst in instances:
        >>>     print(str(inst))
        <Animal()>
        <Cat()>
        <Dog()>
        <Beagle()>
        <Ragdoll()>
    In the case where you cant or dont want to use ubelt.NiceRepr you can get
    similar behavior by pasting the methods from the following snippet into
    your class:
    .. code:: python
        class MyClass:
            def __nice__(self):
                return 'your concise information'
            def __repr__(self):
                nice = self.__nice__()
                classname = self.__class__.__name__
                return '<{0}({1}) at {2}>'.format(classname, nice, hex(id(self)))
            def __str__(self):
                classname = self.__class__.__name__
                nice = self.__nice__()
                return '<{0}({1})>'.format(classname, nice)
    '''

    def __nice__(self):
        '''
        Returns:
            str
        '''
        # Default behavior: if __len__ is defined, use it; otherwise signal
        # that subclasses should implement __nice__.
        if hasattr(self, '__len__'):
            try:
                return str(len(self))
            except Exception:
                pass
        raise NotImplementedError('Subclass should implement __nice__ or __len__')

    def __repr__(self):
        '''
        Returns:
            str
        '''
        try:
            nice = self.__nice__()
            classname = self.__class__.__name__
            return '<{0}({1}) at {2}>'.format(classname, nice, hex(id(self)))
        except Exception:
            import warnings
            warnings.warn('Falling back to object repr because __nice__ failed', RuntimeWarning)
            return object.__repr__(self)

    def __str__(self):
        '''
        Returns:
            str
        '''
        try:
            classname = self.__class__.__name__
            nice = self.__nice__()
            return '<{0}({1})>'.format(classname, nice)
        except Exception:
            import warnings
            warnings.warn('Falling back to object repr because __nice__ failed', RuntimeWarning)
            return object.__repr__(self)"
159497,Erotemic/ubelt,Erotemic_ubelt/ubelt/util_path.py,ubelt.util_path.ChDir,"import os

class ChDir:
    """"""
    Context manager that changes the current working directory and then
    returns you to where you were.

    This is nearly the same as the stdlib :func:`contextlib.chdir`, with the
    exception that it will do nothing if the input path is None (i.e. the user
    did not want to change directories).

    SeeAlso:
        :func:`contextlib.chdir`

    Example:
        >>> import ubelt as ub
        >>> dpath = ub.Path.appdir('ubelt/tests/chdir').ensuredir()
        >>> dir1 = (dpath / 'dir1').ensuredir()
        >>> dir2 = (dpath / 'dir2').ensuredir()
        >>> with ChDir(dpath):
        >>>     assert ub.Path.cwd() == dpath
        >>>     # change to the given directory, and then returns back
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(dir2):
        >>>             assert ub.Path.cwd() == dir2
        >>>             # changes inside the context manager will be reset
        >>>             os.chdir(dpath)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(None):
        >>>             assert ub.Path.cwd() == dir1
        >>>             # When disabled, the cwd does *not* reset at context exit
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir2
        >>>         os.chdir(dir1)
        >>>         # Dont change dirs, but reset to your cwd at context end
        >>>         with ChDir('.'):
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
    """"""

    def __init__(self, dpath):
        """"""
        Args:
            dpath (str | PathLike | None):
                The new directory to work in.
                If None, then the context manager is disabled.
        """"""
        self._context_dpath = dpath
        self._orig_dpath = None

    def __enter__(self):
        """"""
        Returns:
            ChDir: self
        """"""
        if self._context_dpath is not None:
            self._orig_dpath = os.getcwd()
            os.chdir(self._context_dpath)
        return self

    def __exit__(self, ex_type, ex_value, ex_traceback):
        """"""
        Args:
            ex_type (Type[BaseException] | None):
            ex_value (BaseException | None):
            ex_traceback (TracebackType | None):

        Returns:
            bool | None
        """"""
        if self._context_dpath is not None:
            os.chdir(self._orig_dpath)","
class ChDir:
    '''
    Context manager that changes the current working directory and then
    returns you to where you were.
    This is nearly the same as the stdlib :func:`contextlib.chdir`, with the
    exception that it will do nothing if the input path is None (i.e. the user
    did not want to change directories).
    SeeAlso:
        :func:`contextlib.chdir`
    Example:
        >>> import ubelt as ub
        >>> dpath = ub.Path.appdir('ubelt/tests/chdir').ensuredir()
        >>> dir1 = (dpath / 'dir1').ensuredir()
        >>> dir2 = (dpath / 'dir2').ensuredir()
        >>> with ChDir(dpath):
        >>>     assert ub.Path.cwd() == dpath
        >>>     # change to the given directory, and then returns back
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(dir2):
        >>>             assert ub.Path.cwd() == dir2
        >>>             # changes inside the context manager will be reset
        >>>             os.chdir(dpath)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(None):
        >>>             assert ub.Path.cwd() == dir1
        >>>             # When disabled, the cwd does *not* reset at context exit
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir2
        >>>         os.chdir(dir1)
        >>>         # Dont change dirs, but reset to your cwd at context end
        >>>         with ChDir('.'):
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
    '''

    def __init__(self, dpath):
        '''
        Args:
            dpath (str | PathLike | None):
                The new directory to work in.
                If None, then the context manager is disabled.
        '''
        pass

    def __enter__(self):
        '''
        Returns:
            ChDir: self
        '''
        pass

    def __exit__(self, ex_type, ex_value, ex_traceback):
        '''
        Args:
            ex_type (Type[BaseException] | None):
            ex_value (BaseException | None):
            ex_traceback (TracebackType | None):
        Returns:
            bool | None
        '''
        pass",snippet_81,"import os

class ChDir:
    '''
    Context manager that changes the current working directory and then
    returns you to where you were.
    This is nearly the same as the stdlib :func:`contextlib.chdir`, with the
    exception that it will do nothing if the input path is None (i.e. the user
    did not want to change directories).
    SeeAlso:
        :func:`contextlib.chdir`
    Example:
        >>> import ubelt as ub
        >>> dpath = ub.Path.appdir('ubelt/tests/chdir').ensuredir()
        >>> dir1 = (dpath / 'dir1').ensuredir()
        >>> dir2 = (dpath / 'dir2').ensuredir()
        >>> with ChDir(dpath):
        >>>     assert ub.Path.cwd() == dpath
        >>>     # change to the given directory, and then returns back
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(dir2):
        >>>             assert ub.Path.cwd() == dir2
        >>>             # changes inside the context manager will be reset
        >>>             os.chdir(dpath)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(None):
        >>>             assert ub.Path.cwd() == dir1
        >>>             # When disabled, the cwd does *not* reset at context exit
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir2
        >>>         os.chdir(dir1)
        >>>         # Dont change dirs, but reset to your cwd at context end
        >>>         with ChDir('.'):
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
    '''

    def __init__(self, dpath):
        '''
        Args:
            dpath (str | PathLike | None):
                The new directory to work in.
                If None, then the context manager is disabled.
        '''
        self.dpath = dpath
        self._disabled = dpath is None
        self._prev_cwd = None

    def __enter__(self):
        '''
        Returns:
            ChDir: self
        '''
        if not self._disabled:
            self._prev_cwd = os.getcwd()
            os.chdir(os.fspath(self.dpath))
        return self

    def __exit__(self, ex_type, ex_value, ex_traceback):
        '''
        Args:
            ex_type (Type[BaseException] | None):
            ex_value (BaseException | None):
            ex_traceback (TracebackType | None):
        Returns:
            bool | None
        '''
        if not self._disabled and self._prev_cwd is not None:
            os.chdir(self._prev_cwd)
        return None"
159980,pazz/alot,alot/completion/completer.py,alot.completion.completer.Completer,"import abc

class Completer:
    """"""base class for completers""""""
    __metaclass__ = abc.ABCMeta

    @abc.abstractmethod
    def complete(self, original, pos):
        """"""returns a list of completions and cursor positions for the string
        `original` from position `pos` on.

        :param original: the string to complete
        :type original: str
        :param pos: starting position to complete from
        :type pos: int
        :returns: pairs of completed string and cursor position in the
                  new string
        :rtype: list of (str, int)
        :raises: :exc:`CompletionError`
        """"""
        pass

    def relevant_part(self, original, pos):
        """"""
        Calculate the subword in a ' '-separated list of substrings of
        `original` that `pos` is in.
        """"""
        start = original.rfind(' ', 0, pos) + 1
        end = original.find(' ', pos - 1)
        if end == -1:
            end = len(original)
        return (original[start:end], start, end, pos - start)","
class Completer:
    '''base class for completers'''
    @abc.abstractmethod
    def complete(self, original, pos):
        '''returns a list of completions and cursor positions for the string
        `original` from position `pos` on.
        :param original: the string to complete
        :type original: str
        :param pos: starting position to complete from
        :type pos: int
        :returns: pairs of completed string and cursor position in the
                  new string
        :rtype: list of (str, int)
        :raises: :exc:`CompletionError`
        '''
        pass

    def relevant_part(self, original, pos):
        '''
        Calculate the subword in a ' '-separated list of substrings of
        `original` that `pos` is in.
        '''
        pass",snippet_82,"import abc
from typing import List, Tuple, Optional


class CompletionError(Exception):
    pass


class Completer(abc.ABC):
    '''base class for completers'''
    @abc.abstractmethod
    def complete(self, original: str, pos: int) -> List[Tuple[str, int]]:
        '''returns a list of completions and cursor positions for the string
        `original` from position `pos` on.
        :param original: the string to complete
        :type original: str
        :param pos: starting position to complete from
        :type pos: int
        :returns: pairs of completed string and cursor position in the
                  new string
        :rtype: list of (str, int)
        :raises: :exc:`CompletionError`
        '''
        pass

    def relevant_part(self, original: Optional[str], pos: Optional[int]) -> Tuple[str, int, int]:
        '''
        Calculate the subword in a ' '-separated list of substrings of
        `original` that `pos` is in.

        Returns a tuple: (substring, start_index, end_index)
        where start_index and end_index delimit the substring within original.
        '''
        if original is None:
            original = ''
        if not isinstance(original, str):
            raise TypeError(""original must be a string"")

        n = len(original)
        if pos is None:
            pos = n
        if not isinstance(pos, int):
            raise TypeError(""pos must be an int"")

        if pos < 0:
            pos = 0
        if pos > n:
            pos = n

        if n == 0:
            return '', 0, 0

        # If position is on a space, move to the start of the next token
        i = pos
        if i < n and original[i] == ' ':
            while i < n and original[i] == ' ':
                i += 1
            start = i
        else:
            # Find start of the current token
            start = original.rfind(' ', 0, pos) + 1

        if start >= n:
            return '', n, n

        # Find end of the current token
        space_idx = original.find(' ', start)
        end = n if space_idx == -1 else space_idx

        part = original[start:end]
        return part, start, end"
160363,splunk/splunk-sdk-python,splunk_splunk-sdk-python/splunklib/modularinput/argument.py,splunklib.modularinput.argument.Argument,"import xml.etree.ElementTree as ET

class Argument:
    """"""Class representing an argument to a modular input kind.

    ``Argument`` is meant to be used with ``Scheme`` to generate an XML
    definition of the modular input kind that Splunk understands.

    ``name`` is the only required parameter for the constructor.

        **Example with least parameters**::

            arg1 = Argument(name=""arg1"")

        **Example with all parameters**::

            arg2 = Argument(
                name=""arg2"",
                description=""This is an argument with lots of parameters"",
                validation=""is_pos_int('some_name')"",
                data_type=Argument.data_type_number,
                required_on_edit=True,
                required_on_create=True
            )
    """"""
    data_type_boolean = 'BOOLEAN'
    data_type_number = 'NUMBER'
    data_type_string = 'STRING'

    def __init__(self, name, description=None, validation=None, data_type=data_type_string, required_on_edit=False, required_on_create=False, title=None):
        """"""
        :param name: ``string``, identifier for this argument in Splunk.
        :param description: ``string``, human-readable description of the argument.
        :param validation: ``string`` specifying how the argument should be validated, if using internal validation.
               If using external validation, this will be ignored.
        :param data_type: ``string``, data type of this field; use the class constants.
               ""data_type_boolean"", ""data_type_number"", or ""data_type_string"".
        :param required_on_edit: ``Boolean``, whether this arg is required when editing an existing modular input of this kind.
        :param required_on_create: ``Boolean``, whether this arg is required when creating a modular input of this kind.
        :param title: ``String``, a human-readable title for the argument.
        """"""
        self.name = name
        self.description = description
        self.validation = validation
        self.data_type = data_type
        self.required_on_edit = required_on_edit
        self.required_on_create = required_on_create
        self.title = title

    def add_to_document(self, parent):
        """"""Adds an ``Argument`` object to this ElementTree document.

        Adds an <arg> subelement to the parent element, typically <args>
        and sets up its subelements with their respective text.

        :param parent: An ``ET.Element`` to be the parent of a new <arg> subelement
        :returns: An ``ET.Element`` object representing this argument.
        """"""
        arg = ET.SubElement(parent, 'arg')
        arg.set('name', self.name)
        if self.title is not None:
            ET.SubElement(arg, 'title').text = self.title
        if self.description is not None:
            ET.SubElement(arg, 'description').text = self.description
        if self.validation is not None:
            ET.SubElement(arg, 'validation').text = self.validation
        subelements = [('data_type', self.data_type), ('required_on_edit', self.required_on_edit), ('required_on_create', self.required_on_create)]
        for name, value in subelements:
            ET.SubElement(arg, name).text = str(value).lower()
        return arg","
class Argument:
    '''Class representing an argument to a modular input kind.
    ``Argument`` is meant to be used with ``Scheme`` to generate an XML
    definition of the modular input kind that Splunk understands.
    ``name`` is the only required parameter for the constructor.
        **Example with least parameters**::
            arg1 = Argument(name=""arg1"")
        **Example with all parameters**::
            arg2 = Argument(
                name=""arg2"",
                description=""This is an argument with lots of parameters"",
                validation=""is_pos_int('some_name')"",
                data_type=Argument.data_type_number,
                required_on_edit=True,
                required_on_create=True
            )
    '''

    def __init__(self, name, description=None, validation=None, data_type=data_type_string, required_on_edit=False, required_on_create=False, title=None):
        '''
        :param name: ``string``, identifier for this argument in Splunk.
        :param description: ``string``, human-readable description of the argument.
        :param validation: ``string`` specifying how the argument should be validated, if using internal validation.
               If using external validation, this will be ignored.
        :param data_type: ``string``, data type of this field; use the class constants.
               ""data_type_boolean"", ""data_type_number"", or ""data_type_string"".
        :param required_on_edit: ``Boolean``, whether this arg is required when editing an existing modular input of this kind.
        :param required_on_create: ``Boolean``, whether this arg is required when creating a modular input of this kind.
        :param title: ``String``, a human-readable title for the argument.
        '''
        pass

    def add_to_document(self, parent):
        '''Adds an ``Argument`` object to this ElementTree document.
        Adds an <arg> subelement to the parent element, typically <args>
        and sets up its subelements with their respective text.
        :param parent: An ``ET.Element`` to be the parent of a new <arg> subelement
        :returns: An ``ET.Element`` object representing this argument.
        '''
        pass",snippet_83,"import xml.etree.ElementTree as ET

class Argument:
    '''Class representing an argument to a modular input kind.
    ``Argument`` is meant to be used with ``Scheme`` to generate an XML
    definition of the modular input kind that Splunk understands.
    ``name`` is the only required parameter for the constructor.
        **Example with least parameters**::
            arg1 = Argument(name=""arg1"")
        **Example with all parameters**::
            arg2 = Argument(
                name=""arg2"",
                description=""This is an argument with lots of parameters"",
                validation=""is_pos_int('some_name')"",
                data_type=Argument.data_type_number,
                required_on_edit=True,
                required_on_create=True
            )
    '''

    data_type_boolean = ""boolean""
    data_type_number = ""number""
    data_type_string = ""string""

    def __init__(self, name, description=None, validation=None, data_type=data_type_string, required_on_edit=False, required_on_create=False, title=None):
        '''
        :param name: ``string``, identifier for this argument in Splunk.
        :param description: ``string``, human-readable description of the argument.
        :param validation: ``string`` specifying how the argument should be validated, if using internal validation.
               If using external validation, this will be ignored.
        :param data_type: ``string``, data type of this field; use the class constants.
               ""data_type_boolean"", ""data_type_number"", or ""data_type_string"".
        :param required_on_edit: ``Boolean``, whether this arg is required when editing an existing modular input of this kind.
        :param required_on_create: ``Boolean``, whether this arg is required when creating a modular input of this kind.
        :param title: ``String``, a human-readable title for the argument.
        '''
        if not name or not isinstance(name, str):
            raise ValueError(""Argument 'name' must be a non-empty string."")
        if data_type not in {self.data_type_boolean, self.data_type_number, self.data_type_string}:
            raise ValueError(""Invalid data_type. Use one of the class constants."")
        self.name = name
        self.description = description
        self.validation = validation
        self.data_type = data_type
        self.required_on_edit = bool(required_on_edit)
        self.required_on_create = bool(required_on_create)
        self.title = title

    def add_to_document(self, parent):
        '''Adds an ``Argument`` object to this ElementTree document.
        Adds an <arg> subelement to the parent element, typically <args>
        and sets up its subelements with their respective text.
        :param parent: An ``ET.Element`` to be the parent of a new <arg> subelement
        :returns: An ``ET.Element`` object representing this argument.
        '''
        if parent is None:
            raise ValueError(""parent must be a valid Element"")
        arg_el = ET.SubElement(parent, ""arg"", {""name"": self.name})
        if self.title is not None:
            ET.SubElement(arg_el, ""title"").text = self.title
        if self.description is not None:
            ET.SubElement(arg_el, ""description"").text = self.description
        ET.SubElement(arg_el, ""data_type"").text = self.data_type
        if self.validation is not None:
            ET.SubElement(arg_el, ""validation"").text = self.validation
        ET.SubElement(arg_el, ""required_on_edit"").text = ""true"" if self.required_on_edit else ""false""
        ET.SubElement(arg_el, ""required_on_create"").text = ""true"" if self.required_on_create else ""false""
        return arg_el"
160367,splunk/splunk-sdk-python,splunk_splunk-sdk-python/splunklib/modularinput/scheme.py,splunklib.modularinput.scheme.Scheme,"import xml.etree.ElementTree as ET

class Scheme:
    """"""Class representing the metadata for a modular input kind.

    A ``Scheme`` specifies a title, description, several options of how Splunk should run modular inputs of this
    kind, and a set of arguments which define a particular modular input's properties.

    The primary use of ``Scheme`` is to abstract away the construction of XML to feed to Splunk.
    """"""
    streaming_mode_simple = 'SIMPLE'
    streaming_mode_xml = 'XML'

    def __init__(self, title):
        """"""
        :param title: ``string`` identifier for this Scheme in Splunk.
        """"""
        self.title = title
        self.description = None
        self.use_external_validation = True
        self.use_single_instance = False
        self.streaming_mode = Scheme.streaming_mode_xml
        self.arguments = []

    def add_argument(self, arg):
        """"""Add the provided argument, ``arg``, to the ``self.arguments`` list.

        :param arg: An ``Argument`` object to add to ``self.arguments``.
        """"""
        self.arguments.append(arg)

    def to_xml(self):
        """"""Creates an ``ET.Element`` representing self, then returns it.

        :returns: an ``ET.Element`` representing this scheme.
        """"""
        root = ET.Element('scheme')
        ET.SubElement(root, 'title').text = self.title
        if self.description is not None:
            ET.SubElement(root, 'description').text = self.description
        subelements = [('use_external_validation', self.use_external_validation), ('use_single_instance', self.use_single_instance), ('streaming_mode', self.streaming_mode)]
        for name, value in subelements:
            ET.SubElement(root, name).text = str(value).lower()
        endpoint = ET.SubElement(root, 'endpoint')
        args = ET.SubElement(endpoint, 'args')
        for arg in self.arguments:
            arg.add_to_document(args)
        return root","
class Scheme:
    '''Class representing the metadata for a modular input kind.
    A ``Scheme`` specifies a title, description, several options of how Splunk should run modular inputs of this
    kind, and a set of arguments which define a particular modular input's properties.
    The primary use of ``Scheme`` is to abstract away the construction of XML to feed to Splunk.
    '''

    def __init__(self, title):
        '''
        :param title: ``string`` identifier for this Scheme in Splunk.
        '''
        pass

    def add_argument(self, arg):
        '''Add the provided argument, ``arg``, to the ``self.arguments`` list.
        :param arg: An ``Argument`` object to add to ``self.arguments``.
        '''
        pass

    def to_xml(self):
        '''Creates an ``ET.Element`` representing self, then returns it.
        :returns: an ``ET.Element`` representing this scheme.
        '''
        pass",snippet_84,"import xml.etree.ElementTree as ET


class Scheme:
    '''Class representing the metadata for a modular input kind.
    A ``Scheme`` specifies a title, description, several options of how Splunk should run modular inputs of this
    kind, and a set of arguments which define a particular modular input's properties.
    The primary use of ``Scheme`` is to abstract away the construction of XML to feed to Splunk.
    '''

    def __init__(self, title):
        '''
        :param title: ``string`` identifier for this Scheme in Splunk.
        '''
        if not isinstance(title, str) or not title:
            raise ValueError(""title must be a non-empty string"")
        self.title = title
        self.description = """"
        self.use_external_validation = False
        self.streaming_mode = ""xml""
        self.use_single_instance = False
        self.arguments = []

    def add_argument(self, arg):
        '''Add the provided argument, ``arg``, to the ``self.arguments`` list.
        :param arg: An ``Argument`` object to add to ``self.arguments``.
        '''
        if arg is None:
            raise ValueError(""arg cannot be None"")
        if not hasattr(arg, ""to_xml"") or not callable(getattr(arg, ""to_xml"")):
            raise TypeError(""arg must have a to_xml() method returning an ET.Element"")
        self.arguments.append(arg)

    def to_xml(self):
        '''Creates an ``ET.Element`` representing self, then returns it.
        :returns: an ``ET.Element`` representing this scheme.
        '''
        scheme_el = ET.Element(""scheme"")

        title_el = ET.SubElement(scheme_el, ""title"")
        title_el.text = self.title

        desc_el = ET.SubElement(scheme_el, ""description"")
        desc_el.text = self.description if self.description is not None else """"

        uev_el = ET.SubElement(scheme_el, ""use_external_validation"")
        uev_el.text = ""true"" if self.use_external_validation else ""false""

        sm_el = ET.SubElement(scheme_el, ""streaming_mode"")
        sm_el.text = self.streaming_mode

        usi_el = ET.SubElement(scheme_el, ""use_single_instance"")
        usi_el.text = ""true"" if self.use_single_instance else ""false""

        args_el = ET.SubElement(scheme_el, ""arguments"")
        for arg in self.arguments:
            arg_el = arg.to_xml()
            if not isinstance(arg_el, ET.Element):
                raise TypeError(""Argument.to_xml() must return an xml.etree.ElementTree.Element"")
            args_el.append(arg_el)

        return scheme_el"
160991,ncclient/ncclient,ncclient_ncclient/ncclient/transport/session.py,ncclient.transport.session.SessionListener,"class SessionListener:
    """"""Base class for :class:`Session` listeners, which are notified when a new
    NETCONF message is received or an error occurs.

    .. note::
        Avoid time-intensive tasks in a callback's context.
    """"""

    def callback(self, root, raw):
        """"""Called when a new XML document is received. The *root* argument allows the callback to determine whether it wants to further process the document.

        Here, *root* is a tuple of *(tag, attributes)* where *tag* is the qualified name of the root element and *attributes* is a dictionary of its attributes (also qualified names).

        *raw* will contain the XML document as a string.
        """"""
        raise NotImplementedError

    def errback(self, ex):
        """"""Called when an error occurs.

        :type ex: :exc:`Exception`
        """"""
        raise NotImplementedError","class SessionListener:
    '''Base class for :class:`Session` listeners, which are notified when a new
    NETCONF message is received or an error occurs.
    .. note::
        Avoid time-intensive tasks in a callback's context.
    '''

    def callback(self, root, raw):
        '''Called when a new XML document is received. The *root* argument allows the callback to determine whether it wants to further process the document.
        Here, *root* is a tuple of *(tag, attributes)* where *tag* is the qualified name of the root element and *attributes* is a dictionary of its attributes (also qualified names).
        *raw* will contain the XML document as a string.
        '''
        pass

    def errback(self, ex):
        '''Called when an error occurs.
        :type ex: :exc:`Exception`
        '''
        pass",snippet_85,"from abc import ABC, abstractmethod
from typing import Dict, Tuple


class SessionListener(ABC):
    '''Base class for :class:`Session` listeners, which are notified when a new
    NETCONF message is received or an error occurs.
    .. note::
        Avoid time-intensive tasks in a callback's context.
    '''

    __slots__ = ()

    @abstractmethod
    def callback(self, root: Tuple[str, Dict[str, str]], raw: str) -> None:
        '''Called when a new XML document is received. The *root* argument allows the callback to determine whether it wants to further process the document.
        Here, *root* is a tuple of *(tag, attributes)* where *tag* is the qualified name of the root element and *attributes* is a dictionary of its attributes (also qualified names).
        *raw* will contain the XML document as a string.
        '''
        raise NotImplementedError

    @abstractmethod
    def errback(self, ex: Exception) -> None:
        '''Called when an error occurs.
        :type ex: :exc:`Exception`
        '''
        raise NotImplementedError"
163149,python-cmd2/cmd2,python-cmd2_cmd2/cmd2/argparse_custom.py,cmd2.argparse_custom.Cmd2AttributeWrapper,"from typing import TYPE_CHECKING, Any, ClassVar, NoReturn, Protocol, cast, runtime_checkable

class Cmd2AttributeWrapper:
    """"""Wraps a cmd2-specific attribute added to an argparse Namespace.

    This makes it easy to know which attributes in a Namespace are
    arguments from a parser and which were added by cmd2.
    """"""

    def __init__(self, attribute: Any) -> None:
        """"""Initialize Cmd2AttributeWrapper instances.""""""
        self.__attribute = attribute

    def get(self) -> Any:
        """"""Get the value of the attribute.""""""
        return self.__attribute

    def set(self, new_val: Any) -> None:
        """"""Set the value of the attribute.""""""
        self.__attribute = new_val","
class Cmd2AttributeWrapper:
    '''Wraps a cmd2-specific attribute added to an argparse Namespace.
    This makes it easy to know which attributes in a Namespace are
    arguments from a parser and which were added by cmd2.
    '''

    def __init__(self, attribute: Any) -> None:
        '''Initialize Cmd2AttributeWrapper instances.'''
        pass

    def get(self) -> Any:
        '''Get the value of the attribute.'''
        pass

    def set(self, new_val: Any) -> None:
        '''Set the value of the attribute.'''
        pass",snippet_86,"from typing import Any

class Cmd2AttributeWrapper:
    '''Wraps a cmd2-specific attribute added to an argparse Namespace.
    This makes it easy to know which attributes in a Namespace are
    arguments from a parser and which were added by cmd2.
    '''

    def __init__(self, attribute: Any) -> None:
        '''Initialize Cmd2AttributeWrapper instances.'''
        self._attribute = attribute

    def get(self) -> Any:
        '''Get the value of the attribute.'''
        return self._attribute

    def set(self, new_val: Any) -> None:
        '''Set the value of the attribute.'''
        self._attribute = new_val"
163184,python-cmd2/cmd2,python-cmd2_cmd2/cmd2/utils.py,cmd2.utils.ContextFlag,"class ContextFlag:
    """"""A context manager which is also used as a boolean flag value within the default sigint handler.

    Its main use is as a flag to prevent the SIGINT handler in cmd2 from raising a KeyboardInterrupt
    while a critical code section has set the flag to True. Because signal handling is always done on the
    main thread, this class is not thread-safe since there is no need.
    """"""

    def __init__(self) -> None:
        """"""When this flag has a positive value, it is considered set. When it is 0, it is not set.

        It should never go below 0.
        """"""
        self.__count = 0

    def __bool__(self) -> bool:
        """"""Define the truth value of an object when it is used in a boolean context.""""""
        return self.__count > 0

    def __enter__(self) -> None:
        """"""When a with block is entered, the __enter__ method of the context manager is called.""""""
        self.__count += 1

    def __exit__(self, *args: object) -> None:
        """"""When the execution flow exits a with statement block this is called, regardless of whether an exception occurred.""""""
        self.__count -= 1
        if self.__count < 0:
            raise ValueError('count has gone below 0')","class ContextFlag:
    '''A context manager which is also used as a boolean flag value within the default sigint handler.
    Its main use is as a flag to prevent the SIGINT handler in cmd2 from raising a KeyboardInterrupt
    while a critical code section has set the flag to True. Because signal handling is always done on the
    main thread, this class is not thread-safe since there is no need.
        '''

    def __init__(self) -> None:
        '''When this flag has a positive value, it is considered set. When it is 0, it is not set.
        It should never go below 0.
        '''
        pass

    def __bool__(self) -> bool:
        '''Define the truth value of an object when it is used in a boolean context.'''
        pass

    def __enter__(self) -> None:
        '''When a with block is entered, the __enter__ method of the context manager is called.'''
        pass

    def __exit__(self, *args: object) -> None:
        '''When the execution flow exits a with statement block this is called, regardless of whether an exception occurred.'''
        pass",snippet_87,"class ContextFlag:
    '''A context manager which is also used as a boolean flag value within the default sigint handler.
    Its main use is as a flag to prevent the SIGINT handler in cmd2 from raising a KeyboardInterrupt
    while a critical code section has set the flag to True. Because signal handling is always done on the
    main thread, this class is not thread-safe since there is no need.
        '''

    def __init__(self) -> None:
        '''When this flag has a positive value, it is considered set. When it is 0, it is not set.
        It should never go below 0.
        '''
        self._value = 0

    def __bool__(self) -> bool:
        '''Define the truth value of an object when it is used in a boolean context.'''
        return self._value > 0

    def __enter__(self) -> None:
        '''When a with block is entered, the __enter__ method of the context manager is called.'''
        self._value += 1
        return None

    def __exit__(self, *args: object) -> None:
        '''When the execution flow exits a with statement block this is called, regardless of whether an exception occurred.'''
        if self._value > 0:
            self._value -= 1
        else:
            self._value = 0
        return None"
166548,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/cache/cache.py,pypyr.cache.cache.Cache,"import threading
from pypyr.config import config

class Cache:
    """"""Thread-safe general purpose cache for objects.

    Add things to the cache by calling get(key, creator). If the requested key
    doesn't exist, will add the item to the cache for you.
    """"""

    def __init__(self):
        """"""Instantiate the cache.""""""
        self._lock = threading.Lock()
        self._cache = {}

    def clear(self):
        """"""Clear the cache of all objects.""""""
        with self._lock:
            self._cache.clear()

    def get(self, key, creator):
        """"""Get key from cache. If key not exist, call creator and cache result.

        Looks for key in cache and returns object for that key.

        If key is not found, call creator and save the result to cache for that
        key.

        Be warned that get happens under the context of a Lock. . . so if
        creator takes a long time you might well be blocking.

        If config no_cache is True, bypasses cache entirely - will call
        creator each time and also not save the result to cache.

        Args:
            key: key (unique id) of cached item
            creator: callable that will create cached object if key not found

        Returns:
            Cached item at key or the result of creator()
        """"""
        if config.no_cache:
            logger.debug('no cache mode enabled. creating `%s` sans cache', key)
            return creator()
        with self._lock:
            if key in self._cache:
                logger.debug('`%s` loading from cache', key)
                obj = self._cache[key]
            else:
                logger.debug('`%s` not found in cache. . . creating', key)
                obj = creator()
                self._cache[key] = obj
        return obj","
class Cache:
    '''Thread-safe general purpose cache for objects.
    Add things to the cache by calling get(key, creator). If the requested key
    doesn't exist, will add the item to the cache for you.
    '''

    def __init__(self):
        '''Instantiate the cache.'''
        pass

    def clear(self):
        '''Clear the cache of all objects.'''
        pass

    def get(self, key, creator):
        '''Get key from cache. If key not exist, call creator and cache result.
        Looks for key in cache and returns object for that key.
        If key is not found, call creator and save the result to cache for that
        key.
        Be warned that get happens under the context of a Lock. . . so if
        creator takes a long time you might well be blocking.
        If config no_cache is True, bypasses cache entirely - will call
        creator each time and also not save the result to cache.
        Args:
            key: key (unique id) of cached item
            creator: callable that will create cached object if key not found
        Returns:
            Cached item at key or the result of creator()
        '''
        pass",snippet_88,"import threading

class Cache:
    '''Thread-safe general purpose cache for objects.
    Add things to the cache by calling get(key, creator). If the requested key
    doesn't exist, will add the item to the cache for you.
    '''

    def __init__(self, no_cache=False):
        '''Instantiate the cache.'''
        self._store = {}
        self._lock = threading.Lock()
        self.no_cache = no_cache

    def clear(self):
        '''Clear the cache of all objects.'''
        with self._lock:
            self._store.clear()

    def get(self, key, creator):
        '''Get key from cache. If key not exist, call creator and cache result.
        Looks for key in cache and returns object for that key.
        If key is not found, call creator and save the result to cache for that
        key.
        Be warned that get happens under the context of a Lock. . . so if
        creator takes a long time you might well be blocking.
        If config no_cache is True, bypasses cache entirely - will call
        creator each time and also not save the result to cache.
        Args:
            key: key (unique id) of cached item
            creator: callable that will create cached object if key not found
        Returns:
            Cached item at key or the result of creator()
        '''
        if self.no_cache:
            return creator()

        with self._lock:
            if key in self._store:
                return self._store[key]
            value = creator()
            self._store[key] = value
            return value"
166549,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/cache/loadercache.py,pypyr.cache.loadercache.Loader,"from pypyr.pipedef import PipelineDefinition, PipelineInfo
from pypyr.cache.cache import Cache
from collections.abc import Mapping
from pypyr.errors import PipelineDefinitionError

class Loader:
    """"""A single pipeline loader & the cache for all pipelines it has loaded.

    It loads pipelines using the get_pipeline_definition you assign to the
    loader at initialization.

    Attributes:
        name (str): Absolute module name of loader.
    """"""
    __slots__ = ['name', '_get_pipeline_definition', '_pipeline_cache']

    def __init__(self, name, get_pipeline_definition):
        """"""Initialize the loader and its pipeline cache.

        The expected function signature is:
        get_pipeline_definition(name: str,
                                parent: any) -> PipelineDefinition | Mapping

        Args:
            name: Absolute name of loader
            get_pipeline_definition: Reference to the function to call when
                loading a pipeline with this Loader.
        """"""
        self.name = name
        self._get_pipeline_definition = get_pipeline_definition
        self._pipeline_cache = Cache()

    def clear(self):
        """"""Clear all the pipelines in this Loader's cache.""""""
        self._pipeline_cache.clear()

    def get_pipeline(self, name, parent):
        """"""Get cached PipelineDefinition. Adds it to cache if it doesn't exist.

        The cache is local to this Loader instance.

        The combination of parent+name must be unique for this Loader. Parent
        should therefore have a sensible __str__ implementation because it
        forms part of the pipeline's identifying str key in the cache.

        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.

        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        """"""
        normalized_name = f'{parent}+{name}' if parent else name
        return self._pipeline_cache.get(normalized_name, lambda: self._load_pipeline(name, parent))

    def _load_pipeline(self, name, parent):
        """"""Execute get_pipeline_definition(name, parent) for this loader.

        If the loader get_pipeline_definition does not return a
        PipelineDefinition, this method will wrap the payload inside a
        PipelineDefinition for you.

        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.

        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        """"""
        logger.debug('starting')
        logger.debug('loading the pipeline definition with %s', self.name)
        pipeline_definition = self._get_pipeline_definition(pipeline_name=name, parent=parent)
        if not isinstance(pipeline_definition, PipelineDefinition):
            pipeline_definition = PipelineDefinition(pipeline=pipeline_definition, info=PipelineInfo(pipeline_name=name, loader=self.name, parent=parent))
        if not isinstance(pipeline_definition.pipeline, Mapping):
            raise PipelineDefinitionError(""A pipeline must be a mapping at the top level. Does your top-level yaml have a 'steps:' key? For example:\n\nsteps:\n  - name: pypyr.steps.echo\n    in:\n      echoMe: this is a bare bones pipeline example.\n"")
        logger.debug('done')
        return pipeline_definition","
class Loader:
    '''A single pipeline loader & the cache for all pipelines it has loaded.
    It loads pipelines using the get_pipeline_definition you assign to the
    loader at initialization.
    Attributes:
        name (str): Absolute module name of loader.
    '''

    def __init__(self, name, get_pipeline_definition):
        '''Initialize the loader and its pipeline cache.
        The expected function signature is:
        get_pipeline_definition(name: str,
                                parent: any) -> PipelineDefinition | Mapping
        Args:
            name: Absolute name of loader
            get_pipeline_definition: Reference to the function to call when
                loading a pipeline with this Loader.
        '''
        pass

    def clear(self):
        '''Clear all the pipelines in this Loader's cache.'''
        pass

    def get_pipeline(self, name, parent):
        '''Get cached PipelineDefinition. Adds it to cache if it doesn't exist.
        The cache is local to this Loader instance.
        The combination of parent+name must be unique for this Loader. Parent
        should therefore have a sensible __str__ implementation because it
        forms part of the pipeline's identifying str key in the cache.
        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.
        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        '''
        pass

    def _load_pipeline(self, name, parent):
        '''Execute get_pipeline_definition(name, parent) for this loader.
        If the loader get_pipeline_definition does not return a
        PipelineDefinition, this method will wrap the payload inside a
        PipelineDefinition for you.
        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.
        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        '''
        pass",snippet_89,"from collections.abc import Mapping

class Loader:
    '''A single pipeline loader & the cache for all pipelines it has loaded.
    It loads pipelines using the get_pipeline_definition you assign to the
    loader at initialization.
    Attributes:
        name (str): Absolute module name of loader.
    '''

    def __init__(self, name, get_pipeline_definition):
        '''Initialize the loader and its pipeline cache.
        The expected function signature is:
        get_pipeline_definition(name: str,
                                parent: any) -> PipelineDefinition | Mapping
        Args:
            name: Absolute name of loader
            get_pipeline_definition: Reference to the function to call when
                loading a pipeline with this Loader.
        '''
        self.name = name
        self._get_pipeline_definition = get_pipeline_definition
        self._cache = {}

    def clear(self):
        '''Clear all the pipelines in this Loader's cache.'''
        self._cache.clear()

    def get_pipeline(self, name, parent):
        '''Get cached PipelineDefinition. Adds it to cache if it doesn't exist.
        The cache is local to this Loader instance.
        The combination of parent+name must be unique for this Loader. Parent
        should therefore have a sensible __str__ implementation because it
        forms part of the pipeline's identifying str key in the cache.
        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.
        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        '''
        key = f'{str(parent)}:{name}'
        if key not in self._cache:
            self._cache[key] = self._load_pipeline(name, parent)
        return self._cache[key]

    def _load_pipeline(self, name, parent):
        '''Execute get_pipeline_definition(name, parent) for this loader.
        If the loader get_pipeline_definition does not return a
        PipelineDefinition, this method will wrap the payload inside a
        PipelineDefinition for you.
        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.
        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        '''
        result = self._get_pipeline_definition(name, parent)

        # If already a PipelineDefinition, return as is.
        try:
            from pypyr.pipedef import PipelineDefinition
        except Exception:  # pragma: no cover - only if dependency is missing
            PipelineDefinition = None

        if PipelineDefinition and isinstance(result, PipelineDefinition):
            return result

        # If result is not a Mapping, just return it as-is for compatibility.
        if not isinstance(result, Mapping):
            return result

        # Wrap Mapping in PipelineDefinition with best-effort constructor calls.
        if PipelineDefinition is None:
            # Fall back to returning the mapping if the class isn't available.
            return result

        # Try a series of likely constructor signatures for robustness.
        last_err = None
        try:
            return PipelineDefinition(pipeline=result, name=name, parent=parent, loader=self)
        except Exception as e:
            last_err = e

        try:
            return PipelineDefinition(result, name=name, parent=parent, loader=self)
        except Exception as e:
            last_err = e

        try:
            return PipelineDefinition(result, name, parent, self)
        except Exception as e:
            last_err = e

        try:
            return PipelineDefinition(result)
        except Exception as e:
            last_err = e

        raise TypeError(f'Could not construct PipelineDefinition from mapping: {last_err}')"
166587,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/pipedef.py,pypyr.pipedef.PipelineDefinition,"class PipelineDefinition:
    """"""The pipeline body and its metadata.

    A loader creates the PipelineDefinition and sets the metadata in .info.

    The PipelineDefinition is a globally shared cache of the pipeline body &
    meta-data.

    Attributes:
        pipeline (dict-like): The pipeline yaml body.
        info (PipelineInfo): Meta-data set by the loader for the pipeline.
    """"""
    __slots__ = ['pipeline', 'info']

    def __init__(self, pipeline, info):
        """"""Initialize a pipeline definition.

        Args:
            pipeline (dict-like): The pipeline yaml body itself.
            info (PipelineInfo): Meta-data set by the loader for the pipeline.
        """"""
        self.pipeline = pipeline
        self.info = info

    def __eq__(self, other):
        """"""Equality comparison checks Pipeline and info objects are equal.""""""
        type_self = type(self)
        if type_self is type(other):
            all_slots = [p for c in type_self.__mro__ for p in getattr(c, '__slots__', [])]
            return all((getattr(self, s, id(self)) == getattr(other, s, id(other)) for s in all_slots))
        else:
            return False","class PipelineDefinition:
    '''The pipeline body and its metadata.
    A loader creates the PipelineDefinition and sets the metadata in .info.
    The PipelineDefinition is a globally shared cache of the pipeline body &
    meta-data.
    Attributes:
        pipeline (dict-like): The pipeline yaml body.
        info (PipelineInfo): Meta-data set by the loader for the pipeline.
    '''

    def __init__(self, pipeline, info):
        '''Initialize a pipeline definition.
        Args:
            pipeline (dict-like): The pipeline yaml body itself.
            info (PipelineInfo): Meta-data set by the loader for the pipeline.
        '''
        pass

    def __eq__(self, other):
        '''Equality comparison checks Pipeline and info objects are equal.'''
        pass",snippet_92,"class PipelineDefinition:
    '''The pipeline body and its metadata.
    A loader creates the PipelineDefinition and sets the metadata in .info.
    The PipelineDefinition is a globally shared cache of the pipeline body &
    meta-data.
    Attributes:
        pipeline (dict-like): The pipeline yaml body.
        info (PipelineInfo): Meta-data set by the loader for the pipeline.
    '''

    def __init__(self, pipeline, info):
        '''Initialize a pipeline definition.
        Args:
            pipeline (dict-like): The pipeline yaml body itself.
            info (PipelineInfo): Meta-data set by the loader for the pipeline.
        '''
        self.pipeline = pipeline
        self.info = info

    def __eq__(self, other):
        '''Equality comparison checks Pipeline and info objects are equal.'''
        if not isinstance(other, PipelineDefinition):
            return NotImplemented
        return self.pipeline == other.pipeline and self.info == other.info"
166589,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/pipedef.py,pypyr.pipedef.PipelineInfo,"class PipelineInfo:
    """"""The common attributes that every pipeline loader should set.

    Custom loaders that want to add more properties to a pipeline's meta-data
    should probably derive from this class.

    Attributes:
        pipeline_name (str): Name of pipeline, as set by the loader.
        loader (str): Absolute module name of the pipeline loader.
        parent (any): pipeline_name resolves from parent. The parent can be any
            type - it is up to the loader to interpret the parent property.
        is_loader_cascading (bool): Loader cascades to child pipelines if not
            otherwise set on pype. Default True.
        is_parent_cascading (bool): Parent cascades to child pipelines if not
            otherwise set on pype. Default True.
    """"""
    __slots__ = ['pipeline_name', 'loader', 'parent', 'is_loader_cascading', 'is_parent_cascading']

    def __init__(self, pipeline_name, loader, parent, is_parent_cascading=True, is_loader_cascading=True):
        """"""Initialize PipelineInfo.

        Args:
            pipeline_name (str): name of pipeline, as set by the loader.
            loader (str): absolute module name of pypeloader.
            parent (any): pipeline_name resolves from parent.
            is_loader_cascading (bool): Loader cascades to child pipelines if
                not otherwise set on pype. Default True.
            is_parent_cascading (bool): Parent cascades to child pipelines if
                not otherwise set on pype. Default True.
        """"""
        self.pipeline_name = pipeline_name
        self.loader = loader
        self.parent = parent
        self.is_loader_cascading = is_loader_cascading
        self.is_parent_cascading = is_parent_cascading

    def __eq__(self, other):
        """"""Check all instance attributes are equal.""""""
        type_self = type(self)
        if type_self is type(other):
            all_slots = [p for c in type_self.__mro__ for p in getattr(c, '__slots__', [])]
            return all((getattr(self, s, id(self)) == getattr(other, s, id(other)) for s in all_slots))
        else:
            return False","class PipelineInfo:
    '''The common attributes that every pipeline loader should set.
    Custom loaders that want to add more properties to a pipeline's meta-data
    should probably derive from this class.
    Attributes:
        pipeline_name (str): Name of pipeline, as set by the loader.
        loader (str): Absolute module name of the pipeline loader.
        parent (any): pipeline_name resolves from parent. The parent can be any
            type - it is up to the loader to interpret the parent property.
        is_loader_cascading (bool): Loader cascades to child pipelines if not
            otherwise set on pype. Default True.
        is_parent_cascading (bool): Parent cascades to child pipelines if not
            otherwise set on pype. Default True.
    '''

    def __init__(self, pipeline_name, loader, parent, is_parent_cascading=True, is_loader_cascading=True):
        '''Initialize PipelineInfo.
        Args:
            pipeline_name (str): name of pipeline, as set by the loader.
            loader (str): absolute module name of pypeloader.
            parent (any): pipeline_name resolves from parent.
            is_loader_cascading (bool): Loader cascades to child pipelines if
                not otherwise set on pype. Default True.
            is_parent_cascading (bool): Parent cascades to child pipelines if
                not otherwise set on pype. Default True.
        '''
        pass

    def __eq__(self, other):
        '''Check all instance attributes are equal.'''
        pass",snippet_93,"class PipelineInfo:
    '''The common attributes that every pipeline loader should set.
    Custom loaders that want to add more properties to a pipeline's meta-data
    should probably derive from this class.
    Attributes:
        pipeline_name (str): Name of pipeline, as set by the loader.
        loader (str): Absolute module name of the pipeline loader.
        parent (any): pipeline_name resolves from parent. The parent can be any
            type - it is up to the loader to interpret the parent property.
        is_loader_cascading (bool): Loader cascades to child pipelines if not
            otherwise set on pype. Default True.
        is_parent_cascading (bool): Parent cascades to child pipelines if not
            otherwise set on pype. Default True.
    '''

    def __init__(self, pipeline_name, loader, parent, is_parent_cascading=True, is_loader_cascading=True):
        '''Initialize PipelineInfo.
        Args:
            pipeline_name (str): name of pipeline, as set by the loader.
            loader (str): absolute module name of pypeloader.
            parent (any): pipeline_name resolves from parent.
            is_loader_cascading (bool): Loader cascades to child pipelines if
                not otherwise set on pype. Default True.
            is_parent_cascading (bool): Parent cascades to child pipelines if
                not otherwise set on pype. Default True.
        '''
        self.pipeline_name = pipeline_name
        self.loader = loader
        self.parent = parent
        self.is_loader_cascading = is_loader_cascading
        self.is_parent_cascading = is_parent_cascading

    def __eq__(self, other):
        '''Check all instance attributes are equal.'''
        if not isinstance(other, PipelineInfo):
            return NotImplemented
        return self.__dict__ == other.__dict__"
166604,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/steps/dsl/cmd.py,pypyr.steps.dsl.cmd.CmdStep,"import pypyr.utils.types
from pypyr.errors import ContextError, KeyInContextHasNoValueError, KeyNotInContextError
from collections.abc import Mapping, Sequence
from pypyr.context import Context
import logging
from pypyr.subproc import Command, SimpleCommandTypes

class CmdStep:
    """"""A pypyr step to run an executable or command as a subprocess.

    This models a step that takes config like this:
        cmd: <<cmd string>>

    OR, expanded syntax is as a dict
        cmd:
            run: str. mandatory. command + args to execute.
            save: bool. defaults False. save output to cmdOut. Treats output
                as text in the system's encoding and removes newlines at end.
            cwd: str/Pathlike. optional. Working directory for this command.
            bytes (bool): Default False. When `save` return output bytes from
                cmd unaltered, without applying any encoding & text newline
                processing.
            encoding (str): Default None. When `save`, decode cmd output with
                this encoding. The default of None uses the system encoding and
                should ""just work"".
            stdout (str | Path): Default None. Write stdout to this file path.
                Special value `/dev/null` writes to the system null device.
            stderr (str | Path): Default None. Write stderr to this file path.
                Special value `/dev/null` writes to the system null device.
                Special value `/dev/stdout` redirects err output to stdout.
            append (bool): Default False. When stdout/stderr a file, append
                rather than overwrite. Default is to overwrite.

    In expanded syntax, `run` can be a simple string or a list:
        cmd:
          run:
            - my-executable --arg
            - cmd here
          save: False
          cwd: ./path/here

    OR, as a list in simplified syntax:
        cmd:
          - my-executable --arg
          - ./another-executable --arg

    Any or all of the list items can use expanded syntax:
        cmd:
          - ./simple-cmd-here --arg1 value
          - run: cmd here
            save: False
            cwd: ./path/here
          - run:
              - my-executable --arg
              - ./another-executable --arg
            save: True cwd: ./path/here

    If save is True, will save the output to context as follows:
        cmdOut:
            returncode: 0
            stdout: 'stdout str here. None if empty.'
            stderr: 'stderr str here. None if empty.'

    If the cmd input contains a list of executables, cmdOut will be a list of
    cmdOut objects, in order executed.

    cmdOut.returncode is the exit status of the called process. Typically 0
    means OK. A negative value -N indicates that the child was terminated by
    signal N (POSIX only).

    The run_step method does the actual work. init parses the input yaml.

    Attributes:
        logger (logger): Logger instantiated by name of calling step.
        context: (pypyr.context.Context): The current pypyr Context.
        commands (list[pypyr.subproc.Command]): Commands to run as subprocess.
        is_shell (bool): True if subprocess should run through default shell.
        name (str): Name of calling step. Used for logging output & error
            messages.
    """"""

    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        """"""Initialize the CmdStep.

        The step config in the context dict in simplified syntax:
            cmd: <<cmd string>>

        OR, as a dict in expanded syntax:
            cmd:
                run: str. mandatory. command + args to execute.
                save: bool. optional. defaults False. save output to cmdOut.
                cwd: str/path. optional. if specified, change the working
                     directory just for the duration of the command.

        `run` can be a single string, or it can be a list of string if there
        are multiple commands to execute with the same settings.

        OR, as a list:
            cmd:
                - my-executable --arg
                - ./another-executable --arg

        Any or all of the list items can be in expanded syntax.

        Args:
            name (str): Unique name for step. Likely __name__ of calling step.
            context (pypyr.context.Context): Look for step config in this
                context instance.
            is_shell (bool): Set to true to execute cmd through the default
                shell.
        """"""
        assert name, 'name parameter must exist for CmdStep.'
        assert context, 'context param must exist for CmdStep.'
        self.name = name
        self.logger = logging.getLogger(name)
        context.assert_key_has_value(key='cmd', caller=name)
        self.context = context
        self.is_shell = is_shell
        cmd_config = context.get_formatted('cmd')
        commands: list[Command] = []
        if isinstance(cmd_config, SimpleCommandTypes):
            commands.append(Command(cmd_config, is_shell=is_shell))
        elif isinstance(cmd_config, Mapping):
            commands.append(self.create_command(cmd_config))
        elif isinstance(cmd_config, Sequence):
            for cmd in cmd_config:
                if isinstance(cmd, SimpleCommandTypes):
                    commands.append(Command(cmd, is_shell=is_shell))
                elif isinstance(cmd, Mapping):
                    commands.append(self.create_command(cmd))
                else:
                    raise ContextError(f'{cmd} in {name} cmd config is wrong.\nEach list item should be either a simple string or a dict for expanded syntax:\ncmd:\n  - my-executable --arg\n  - run: another-executable --arg value\n    cwd: ../mydir/subdir\n  - run:\n      - arb-executable1 --arg value1\n      - arb-executable2 --arg value2\n    cwd: ../mydir/arbdir')
        else:
            raise ContextError(f'{name} cmd config should be either a simple string:\ncmd: my-executable --arg\n\nor a dictionary:\ncmd:\n  run: subdir/my-executable --arg\n  cwd: ./mydir\n\nor a list of commands:\ncmd:\n  - my-executable --arg\n  - run: another-executable --arg value\n    cwd: ../mydir/subdir')
        self.commands: list[Command] = commands

    def create_command(self, cmd_input: Mapping) -> Command:
        """"""Create a pypyr.subproc.Command object from expanded step input.""""""
        try:
            cmd = cmd_input['run']
            if not cmd:
                raise KeyInContextHasNoValueError(f'cmd.run must have a value for {self.name}.\nThe `run` input should look something like this:\ncmd:\n  run: my-executable-here --arg1\n  cwd: ./mydir/subdir\n\nOr, `run` could be a list of commands:\ncmd:\n  run:\n    - arb-executable1 --arg value1\n    - arb-executable2 --arg value2\n  cwd: ../mydir/arbdir')
        except KeyError as err:
            raise KeyNotInContextError(f""cmd.run doesn't exist for {self.name}.\nThe input should look like this in the simplified syntax:\ncmd: my-executable-here --arg1\n\nOr in the expanded syntax:\ncmd:\n  run: my-executable-here --arg1\n\nIf you're passing in a list of commands, each command should be a simple string,\nor a dict with a `run` entry:\ncmd:\n  - my-executable --arg\n  - run: another-executable --arg value\n    cwd: ../mydir/subdir\n  - run:\n      - arb-executable1 --arg value1\n      - arb-executable2 --arg value2\n    cwd: ../mydir/arbdir"") from err
        is_save = pypyr.utils.types.cast_to_bool(cmd_input.get('save', False))
        cwd = cmd_input.get('cwd')
        is_bytes = cmd_input.get('bytes')
        is_text = not is_bytes if is_save else False
        stdout = cmd_input.get('stdout')
        stderr = cmd_input.get('stderr')
        if is_save:
            if stderr or stderr:
                raise ContextError(""You can't set `stdout` or `stderr` when `save` is True."")
        encoding = cmd_input.get('encoding')
        append = cmd_input.get('append', False)
        is_shell_override = cmd_input.get('shell', None)
        is_shell = self.is_shell if is_shell_override is None else is_shell_override
        return Command(cmd=cmd, is_shell=is_shell, cwd=cwd, is_save=is_save, is_text=is_text, stdout=stdout, stderr=stderr, encoding=encoding, append=append)

    def run_step(self) -> None:
        """"""Spawn a subprocess to run the command or program.

        If cmd.is_save==True, save result of each command to context 'cmdOut'.
        """"""
        results = []
        try:
            for cmd in self.commands:
                try:
                    cmd.run()
                finally:
                    if cmd.results:
                        results.extend(cmd.results)
        finally:
            if results:
                if len(results) == 1:
                    self.context['cmdOut'] = results[0]
                else:
                    self.context['cmdOut'] = results","
class CmdStep:
    '''A pypyr step to run an executable or command as a subprocess.
    This models a step that takes config like this:
        cmd: <<cmd string>>
    OR, expanded syntax is as a dict
        cmd:
            run: str. mandatory. command + args to execute.
            save: bool. defaults False. save output to cmdOut. Treats output
                as text in the system's encoding and removes newlines at end.
            cwd: str/Pathlike. optional. Working directory for this command.
            bytes (bool): Default False. When `save` return output bytes from
                cmd unaltered, without applying any encoding & text newline
                processing.
            encoding (str): Default None. When `save`, decode cmd output with
                this encoding. The default of None uses the system encoding and
                should ""just work"".
            stdout (str | Path): Default None. Write stdout to this file path.
                Special value `/dev/null` writes to the system null device.
            stderr (str | Path): Default None. Write stderr to this file path.
                Special value `/dev/null` writes to the system null device.
                Special value `/dev/stdout` redirects err output to stdout.
            append (bool): Default False. When stdout/stderr a file, append
                rather than overwrite. Default is to overwrite.
    In expanded syntax, `run` can be a simple string or a list:
        cmd:
          run:
            - my-executable --arg
            - cmd here
          save: False
          cwd: ./path/here
    OR, as a list in simplified syntax:
        cmd:
          - my-executable --arg
          - ./another-executable --arg
    Any or all of the list items can use expanded syntax:
        cmd:
          - ./simple-cmd-here --arg1 value
          - run: cmd here
            save: False
            cwd: ./path/here
          - run:
              - my-executable --arg
              - ./another-executable --arg
            save: True cwd: ./path/here
    If save is True, will save the output to context as follows:
        cmdOut:
            returncode: 0
            stdout: 'stdout str here. None if empty.'
            stderr: 'stderr str here. None if empty.'
    If the cmd input contains a list of executables, cmdOut will be a list of
    cmdOut objects, in order executed.
    cmdOut.returncode is the exit status of the called process. Typically 0
    means OK. A negative value -N indicates that the child was terminated by
    signal N (POSIX only).
    The run_step method does the actual work. init parses the input yaml.
    Attributes:
        logger (logger): Logger instantiated by name of calling step.
        context: (pypyr.context.Context): The current pypyr Context.
        commands (list[pypyr.subproc.Command]): Commands to run as subprocess.
        is_shell (bool): True if subprocess should run through default shell.
        name (str): Name of calling step. Used for logging output & error
            messages.
    '''

    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        '''Initialize the CmdStep.
        The step config in the context dict in simplified syntax:
            cmd: <<cmd string>>
        OR, as a dict in expanded syntax:
            cmd:
                run: str. mandatory. command + args to execute.
                save: bool. optional. defaults False. save output to cmdOut.
                cwd: str/path. optional. if specified, change the working
                     directory just for the duration of the command.
        `run` can be a single string, or it can be a list of string if there
        are multiple commands to execute with the same settings.
        OR, as a list:
            cmd:
                - my-executable --arg
                - ./another-executable --arg
        Any or all of the list items can be in expanded syntax.
        Args:
            name (str): Unique name for step. Likely __name__ of calling step.
            context (pypyr.context.Context): Look for step config in this
                context instance.
            is_shell (bool): Set to true to execute cmd through the default
                shell.
        '''
        pass

    def create_command(self, cmd_input: Mapping) -> Command:
        '''Create a pypyr.subproc.Command object from expanded step input.'''
        pass

    def run_step(self) -> None:
        '''Spawn a subprocess to run the command or program.
        If cmd.is_save==True, save result of each command to context 'cmdOut'.
        '''
        pass",snippet_94,"import logging
import os
import shlex
import subprocess
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Iterable, List, Mapping, Optional, Sequence, Union


def _ensure_text(s: Union[str, Path, None]) -> Optional[str]:
    if s is None:
        return None
    return str(s)


def _system_encoding() -> str:
    import locale

    return locale.getpreferredencoding(False) or ""utf-8""


@dataclass
class _Result:
    returncode: int
    stdout: Optional[Union[str, bytes]]
    stderr: Optional[Union[str, bytes]]


@dataclass
class Command:
    run: str
    save: bool = False
    cwd: Optional[Path] = None
    as_bytes: bool = False
    encoding: Optional[str] = None
    stdout_path: Optional[str] = None
    stderr_path: Optional[str] = None
    append: bool = False

    def _open_target(self, path: Optional[str], mode_write: str):
        if not path:
            return None
        if path == ""/dev/null"":
            return open(os.devnull, mode_write)
        # Only applicable to stderr
        if path == ""/dev/stdout"":
            return subprocess.STDOUT
        return open(path, mode_write)

    def _prepare_args(self, shell: bool) -> Union[str, Sequence[str]]:
        if shell:
            return self.run
        return shlex.split(self.run)

    def execute(self, shell: bool) -> _Result:
        closeables: List[Any] = []
        try:
            mode = ""ab"" if self.append else ""wb""
            stdout_target = None
            stderr_target = None

            if self.stdout_path:
                stdout_target = self._open_target(self.stdout_path, mode)
                if hasattr(stdout_target, ""close""):
                    closeables.append(stdout_target)

            if self.stderr_path:
                stderr_target = self._open_target(self.stderr_path, mode)
                # _open_target could return subprocess.STDOUT sentinel; don't close that.
                if hasattr(stderr_target, ""close""):
                    closeables.append(stderr_target)

            capture_needed = self.save and not (stdout_target or stderr_target)
            run_kwargs = {
                ""shell"": shell,
                ""cwd"": str(self.cwd) if self.cwd else None,
                ""stdout"": subprocess.PIPE if capture_needed else stdout_target,
                ""stderr"": subprocess.PIPE if capture_needed else stderr_target,
                ""text"": False,  # always get bytes, we handle decoding ourselves
            }

            completed = subprocess.run(self._prepare_args(shell), **run_kwargs)  # type: ignore[arg-type]

            out_bytes = None
            err_bytes = None

            if capture_needed:
                out_bytes = completed.stdout
                err_bytes = completed.stderr

            if self.save:
                if self.as_bytes:
                    out_val = out_bytes if out_bytes not in (b"""", None) else None
                    err_val = err_bytes if err_bytes not in (b"""", None) else None
                else:
                    enc = self.encoding or _system_encoding()
                    out_val = (
                        out_bytes.decode(enc, errors=""replace"").rstrip(""\r\n"")
                        if out_bytes not in (b"""", None)
                        else None
                    )
                    err_val = (
                        err_bytes.decode(enc, errors=""replace"").rstrip(""\r\n"")
                        if err_bytes not in (b"""", None)
                        else None
                    )
            else:
                out_val = None
                err_val = None

            return _Result(
                returncode=completed.returncode,
                stdout=out_val,
                stderr=err_val,
            )
        finally:
            for c in closeables:
                try:
                    c.close()
                except Exception:
                    pass


class CmdStep:
    '''A pypyr step to run an executable or command as a subprocess.
    This models a step that takes config like this:
        cmd: <<cmd string>>
    OR, expanded syntax is as a dict
        cmd:
            run: str. mandatory. command + args to execute.
            save: bool. defaults False. save output to cmdOut. Treats output
                as text in the system's encoding and removes newlines at end.
            cwd: str/Pathlike. optional. Working directory for this command.
            bytes (bool): Default False. When `save` return output bytes from
                cmd unaltered, without applying any encoding & text newline
                processing.
            encoding (str): Default None. When `save`, decode cmd output with
                this encoding. The default of None uses the system encoding and
                should ""just work"".
            stdout (str | Path): Default None. Write stdout to this file path.
                Special value `/dev/null` writes to the system null device.
            stderr (str | Path): Default None. Write stderr to this file path.
                Special value `/dev/null` writes to the system null device.
                Special value `/dev/stdout` redirects err output to stdout.
            append (bool): Default False. When stdout/stderr a file, append
                rather than overwrite. Default is to overwrite.
    In expanded syntax, `run` can be a simple string or a list:
        cmd:
          run:
            - my-executable --arg
            - cmd here
          save: False
          cwd: ./path/here
    OR, as a list in simplified syntax:
        cmd:
          - my-executable --arg
          - ./another-executable --arg
    Any or all of the list items can use expanded syntax:
        cmd:
          - ./simple-cmd-here --arg1 value
          - run: cmd here
            save: False
            cwd: ./path/here
          - run:
              - my-executable --arg
              - ./another-executable --arg
            save: True cwd: ./path/here
    If save is True, will save the output to context as follows:
        cmdOut:
            returncode: 0
            stdout: 'stdout str here. None if empty.'
            stderr: 'stderr str here. None if empty.'
    If the cmd input contains a list of executables, cmdOut will be a list of
    cmdOut objects, in order executed.
    cmdOut.returncode is the exit status of the called process. Typically 0
    means OK. A negative value -N indicates that the child was terminated by
    signal N (POSIX only).
    The run_step method does the actual work. init parses the input yaml.
    Attributes:
        logger (logger): Logger instantiated by name of calling step.
        context: (pypyr.context.Context): The current pypyr Context.
        commands (list[pypyr.subproc.Command]): Commands to run as subprocess.
        is_shell (bool): True if subprocess should run through default shell.
        name (str): Name of calling step. Used for logging output & error
            messages.
    '''

    def __init__(self, name: str, context: Mapping[str, Any], is_shell: bool = False) -> None:
        '''Initialize the CmdStep.
        The step config in the context dict in simplified syntax:
            cmd: <<cmd string>>
        OR, as a dict in expanded syntax:
            cmd:
                run: str. mandatory. command + args to execute.
                save: bool. optional. defaults False. save output to cmdOut.
                cwd: str/path. optional. if specified, change the working
                     directory just for the duration of the command.
        `run` can be a single string, or it can be a list of string if there
        are multiple commands to execute with the same settings.
        OR, as a list:
            cmd:
                - my-executable --arg
                - ./another-executable --arg
        Any or all of the list items can be in expanded syntax.
        Args:
            name (str): Unique name for step. Likely __name__ of calling step.
            context (pypyr.context.Context): Look for step config in this
                context instance.
            is_shell (bool): Set to true to execute cmd through the default
                shell.
        '''
        self.name = name
        self.logger = logging.getLogger(name)
        self.context = context  # treat as Mapping; writer later uses __setitem__ if dict-like
        self.is_shell = is_shell
        self.commands: List[Command] = []

        if ""cmd"" not in context:
            raise KeyError(""Missing required 'cmd' in context."")

        raw = context[""cmd""]

        def add_command_from_expanded(expanded: Mapping[str, Any]) -> None:
            run_val = expanded.get(""run"")
            if run_val is None:
                raise ValueError(""Expanded cmd dict must contain 'run'."")
            runs: Iterable[str]
            if isinstance(run_val, str):
                runs = [run_val]
            elif isinstance(run_val, list) or isinstance(run_val, tuple):
                if not all(isinstance(x, str) for x in run_val):
                    raise TypeError(""All items in 'run' list must be strings."")
                runs = run_val  # type: ignore[assignment]
            else:
                raise TypeError(""'run' must be a string or a list of strings."")

            for r in runs:
                cmd = self.create_command(
                    {
                        ""run"": r,
                        ""save"": expanded.get(""save"", False),
                        ""cwd"": expanded.get(""cwd""),
                        ""bytes"": expanded.get(""bytes"", False),
                        ""encoding"": expanded.get(""encoding""),
                        ""stdout"": expanded.get(""stdout""),
                        ""stderr"": expanded.get(""stderr""),
                        ""append"": expanded.get(""append"", False),
                    }
                )
                self.commands.append(cmd)

        if isinstance(raw, str):
            self.commands.append(self.create_command({""run"": raw}))
        elif isinstance(raw, Mapping):
            add_command_from_expanded(raw)
        elif isinstance(raw, list) or isinstance(raw, tuple):
            for item in raw:
                if isinstance(item, str):
                    self.commands.append(self.create_command({""run"": item}))
                elif isinstance(item, Mapping):
                    add_command_from_expanded(item)
                else:
                    raise TypeError(""Items in cmd list must be str or mapping."")
        else:
            raise TypeError(""cmd must be a str, mapping or list."")

    def create_command(self, cmd_input: Mapping) -> Command:
        '''Create a pypyr.subproc.Command object from expanded step input.'''
        run = cmd_input.get(""run"")
        if not isinstance(run, str):
            raise TypeError(""'run' must be a string."")
        save = bool(cmd_input.get(""save"", False))
        cwd = cmd_input.get(""cwd"")
        cwd_path = Path(cwd).resolve() if cwd is not None else None
        as_bytes = bool(cmd_input.get(""bytes"", False))
        encoding = cmd_input.get(""encoding"")
        if encoding is not None and not isinstance(encoding, str):
            raise TypeError(""'encoding' must be a string when provided."")
        stdout_path = _ensure_text(cmd_input.get(""stdout""))
        stderr_path = _ensure_text(cmd_input.get(""stderr""))
        append = bool(cmd_input.get(""append"", False))

        return Command(
            run=run,
            save=save,
            cwd=cwd_path,
            as_bytes=as_bytes,
            encoding=encoding,
            stdout_path=stdout_path,
            stderr_path=stderr_path,
            append=append,
        )

    def run_step(self) -> None:
        '''Spawn a subprocess to run the command or program.
        If cmd.is_save==True, save result of each command to context 'cmdOut'.
        '''
        results: List[dict] = []
        any_saved = False

        for cmd in self.commands:
            self.logger.debug(""Executing command: %s"", cmd.run)
            res = cmd.execute(shell=self.is_shell)
            if cmd.save:
                any_saved = True
                results.append(
                    {
                        ""returncode"": res.returncode,
                        ""stdout"": res.stdout,
                        ""stderr"": res.stderr,
                    }
                )

        if any_saved:
            out_obj: Union[dict, List[dict]]
            if len(results) == 1:
                out_obj = results[0]
            else:
                out_obj = results

            # Attempt to set into context if it's mutable like a dict.
            try:
                self.context[""cmdOut""] = out_obj  # type: ignore[index]
            except Exception:
                pass

        self.logger.debug(""Command(s) completed."")"
166605,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/steps/dsl/cmdasync.py,pypyr.steps.dsl.cmdasync.AsyncCmdStep,"import logging
from pypyr.aio.subproc import Command, Commands
from pypyr.context import Context
import pypyr.utils.types
from collections.abc import Mapping, Sequence
from pypyr.errors import ContextError, KeyInContextHasNoValueError, KeyNotInContextError
from pypyr.subproc import SimpleCommandTypes

class AsyncCmdStep:
    """"""A pypyr step to run executables/commands concurrently as a subprocess.

    This models a step that takes config like this in simple syntax:
        cmds:
            - <<cmd string 1>>
            - <<cmd string 2>>

    All the commands will run concurrently, in parallel.

    OR, expanded syntax is as a dict
        cmds:
            run: list[str | list[str]]. mandatory. command + args to execute.
                If list entry is another list[str], the sub-list will run in
                serial.
            save: bool. defaults False. save output to cmdOut. Treats output
                as text in the system's encoding and removes newlines at end.
            cwd: str/Pathlike. optional. Working directory for these commands.
            bytes (bool): Default False. When `save` return output bytes from
                cmds unaltered, without applying any encoding & text newline
                processing.
            encoding (str): Default None. When `save`, decode output with
                this encoding. The default of None uses the system encoding and
                should ""just work"".
            stdout (str | Path): Default None. Write stdout to this file path.
                Special value `/dev/null` writes to the system null device.
            stderr (str | Path): Default None. Write stderr to this file path.
                Special value `/dev/null` writes to the system null device.
                Special value `/dev/stdout` redirects err output to stdout.
            append (bool): Default False. When stdout/stderr a file, append
                rather than overwrite. Default is to overwrite.

    In expanded syntax, `run` can be a simple string or a list:
        cmds:
          run:
            - ./my-executable --arg
            - [./another-executable --arg, ./arb-executable arghere]
          save: False
          cwd: ./path/here

    As a list in simplified syntax:
        cmds:
          - my-executable --arg
          - ./another-executable --arg

    Any or all of the list items can use expanded syntax:
        cmds:
          - ./simple-cmd-here --arg1 value
          - run: cmd here
            save: False cwd: ./path/here
          - run:
              - my-executable --arg
              - ./another-executable --arg
            save: True
            cwd: ./path/here

    Any of the list items can in turn be a list. A sub-list will run in serial.

    In this example A, B.1 & C will start concurrently. B.2 will only run once
    B.1 is finished.

        cmds:
            - A
            - [B.1, B.2]
            - C

    If save is True, will save the output to context as cmdOut.

    cmdOut will be a list of pypyr.subproc.SubprocessResult objects, in order
    executed.

    SubprocessResult has the following properties:
    cmd: the cmd/args executed
    returncode: 0
    stdout: 'stdout str here. None if empty.'
    stderr: 'stderr str here. None if empty.'

    cmdOut.returncode is the exit status of the called process. Typically 0
    means OK. A negative value -N indicates that the child was terminated by
    signal N (POSIX only).

    The run_step method does the actual work. init parses the input yaml.

    Attributes:
        logger (logger): Logger instantiated by name of calling step.
        context: (pypyr.context.Context): The current pypyr Context.
        commands (pypyr.subproc.Commands): Commands to run as subprocess.
        is_shell (bool): True if subprocess should run through default shell.
        name (str): Name of calling step. Used for logging output & error
            messages.
    """"""

    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        """"""Initialize the CmdStep.

        The step config in the context dict in simplified syntax:
            cmd: <<cmd string>>

        OR, as a dict in expanded syntax:
            cmd:
                run: str. mandatory. command + args to execute.
                save: bool. optional. defaults False. save output to cmdOut.
                cwd: str/path. optional. if specified, change the working
                     directory just for the duration of the command.

        `run` can be a single string, or it can be a list of string if there
        are multiple commands to execute with the same settings.

        OR, as a list:
            cmd:
                - my-executable --arg
                - ./another-executable --arg

        Any or all of the list items can be in expanded syntax.

        Args:
            name (str): Unique name for step. Likely __name__ of calling step.
            context (pypyr.context.Context): Look for step config in this
                context instance.
            is_shell (bool): Set to true to execute cmd through the default
                shell.
        """"""
        assert name, 'name parameter must exist for CmdStep.'
        assert context, 'context param must exist for CmdStep.'
        self.name = name
        self.logger = logging.getLogger(name)
        context.assert_key_has_value(key='cmds', caller=name)
        self.context = context
        self.is_shell = is_shell
        cmd_config = context.get_formatted('cmds')
        commands = Commands()
        if isinstance(cmd_config, SimpleCommandTypes):
            commands.append(Command(cmd_config, is_shell=is_shell))
        elif isinstance(cmd_config, Mapping):
            commands.append(self.create_command(cmd_config))
        elif isinstance(cmd_config, Sequence):
            for cmd in cmd_config:
                if isinstance(cmd, SimpleCommandTypes):
                    commands.append(Command(cmd, is_shell=is_shell))
                elif isinstance(cmd, Sequence):
                    commands.append(Command([cmd], is_shell=is_shell))
                elif isinstance(cmd, Mapping):
                    commands.append(self.create_command(cmd))
                else:
                    raise ContextError(f'{cmd} in {name} cmds config is wrong.\nEach list item should be either a simple string, or a list to run in serial,\nor a dict for expanded syntax:\ncmds:\n  - ./my-executable --arg\n  - run:\n      - ./another-executable --arg value\n      - ./another-executable --arg value2\n    cwd: ../mydir/subdir\n  - run:\n      - ./arb-executable1 --arg value1\n      - [./arb-executable2.1, ./arb-executable2.2]\n    cwd: ../mydir/arbdir\n  - [./arb-executable3.1, ./arb-executable3.2]')
        else:
            raise ContextError(f'{name} cmds config should be either a list:\ncmds:\n  - ./my-executable --arg\n  - subdir/executable --arg1\n\nor a dictionary with a `run` sub-key:\ncmds:\n  run:\n    - ./my-executable --arg\n    - subdir/executable --arg1\n  cwd: ./mydir\n\nAny of the list items in root can be in expanded syntax:\ncmds:\n  - ./my-executable --arg\n  - subdir/executable --arg1\n  - run:\n      - ./arb-executable1 --arg value1\n      - [./arb-executable2.1, ./arb-executable2.2]\n    cwd: ../mydir/subdir\n  - [./arb-executable3.1, ./arb-executable3.2]')
        self.commands: Commands = commands

    def create_command(self, cmd_input: Mapping) -> Command:
        """"""Create pypyr.aio.subproc.Command object from expanded step input.""""""
        try:
            cmd = cmd_input['run']
            if not cmd:
                raise KeyInContextHasNoValueError(f'cmds.run must have a value for {self.name}.\nThe `run` input should look something like this:\ncmds:\n  run:\n    - ./arb-executable1 --arg value1\n    - ./arb-executable2 --arg value2\n  cwd: ../mydir/arbdir')
        except KeyError as err:
            raise KeyNotInContextError(f""cmds.run doesn't exist for {self.name}.\nThe input should look like this in expanded syntax:\ncmds:\n  run:\n    - ./my-executable --arg\n    - subdir/executable --arg1\n  cwd: ./mydir\n\nIf you're passing in a list of commands, each command should be a simple string,\nor a sub-list of commands to run in serial,\nor a dict with a `run` entry:\ncmds:\n  - ./my-executable --arg\n  - run: ./another-executable --arg value\n    cwd: ../mydir/subdir\n  - run:\n      - ./arb-executable1 --arg value1\n      - [./arb-executable2.1, ./arb-executable2.2]\n    cwd: ../mydir/arbdir\n  - [./arb-executable3.1, ./arb-executable3.2]"") from err
        is_save = pypyr.utils.types.cast_to_bool(cmd_input.get('save', False))
        cwd = cmd_input.get('cwd')
        is_bytes = cmd_input.get('bytes')
        is_text = not is_bytes if is_save else False
        stdout = cmd_input.get('stdout')
        stderr = cmd_input.get('stderr')
        if is_save:
            if stderr or stderr:
                raise ContextError(""You can't set `stdout` or `stderr` when `save` is True."")
        encoding = cmd_input.get('encoding')
        append = cmd_input.get('append', False)
        is_shell_override = cmd_input.get('shell', None)
        is_shell = self.is_shell if is_shell_override is None else is_shell_override
        return Command(cmd=cmd, is_shell=is_shell, cwd=cwd, is_save=is_save, is_text=is_text, stdout=stdout, stderr=stderr, encoding=encoding, append=append)

    def run_step(self) -> None:
        """"""Spawn subprocesses to run the commands asynchronously.

        If cmd.is_save==True, save aggregate result of all commands to context
        'cmdOut'.

        cmdOut will be a list of pypyr.subproc.SubprocessResult or Exception
        objects, in order executed.

        SubprocessResult has the following properties:
        cmd: the cmd/args executed
        returncode: 0
        stdout: 'stdout str here. None if empty.'
        stderr: 'stderr str here. None if empty.'
        """"""
        try:
            self.commands.run()
        finally:
            if self.commands.is_save:
                self.logger.debug('saving results to cmdOut')
                self.context['cmdOut'] = self.commands.results
            else:
                self.logger.debug('save is False: not saving results to cmdOut')","
class AsyncCmdStep:
    '''A pypyr step to run executables/commands concurrently as a subprocess.
    This models a step that takes config like this in simple syntax:
        cmds:
            - <<cmd string 1>>
            - <<cmd string 2>>
    All the commands will run concurrently, in parallel.
    OR, expanded syntax is as a dict
        cmds:
            run: list[str | list[str]]. mandatory. command + args to execute.
                If list entry is another list[str], the sub-list will run in
                serial.
            save: bool. defaults False. save output to cmdOut. Treats output
                as text in the system's encoding and removes newlines at end.
            cwd: str/Pathlike. optional. Working directory for these commands.
            bytes (bool): Default False. When `save` return output bytes from
                cmds unaltered, without applying any encoding & text newline
                processing.
            encoding (str): Default None. When `save`, decode output with
                this encoding. The default of None uses the system encoding and
                should ""just work"".
            stdout (str | Path): Default None. Write stdout to this file path.
                Special value `/dev/null` writes to the system null device.
            stderr (str | Path): Default None. Write stderr to this file path.
                Special value `/dev/null` writes to the system null device.
                Special value `/dev/stdout` redirects err output to stdout.
            append (bool): Default False. When stdout/stderr a file, append
                rather than overwrite. Default is to overwrite.
    In expanded syntax, `run` can be a simple string or a list:
        cmds:
          run:
            - ./my-executable --arg
            - [./another-executable --arg, ./arb-executable arghere]
          save: False
          cwd: ./path/here
    As a list in simplified syntax:
        cmds:
          - my-executable --arg
          - ./another-executable --arg
    Any or all of the list items can use expanded syntax:
        cmds:
          - ./simple-cmd-here --arg1 value
          - run: cmd here
            save: False cwd: ./path/here
          - run:
              - my-executable --arg
              - ./another-executable --arg
            save: True
            cwd: ./path/here
    Any of the list items can in turn be a list. A sub-list will run in serial.
    In this example A, B.1 & C will start concurrently. B.2 will only run once
    B.1 is finished.
        cmds:
            - A
            - [B.1, B.2]
            - C
    If save is True, will save the output to context as cmdOut.
    cmdOut will be a list of pypyr.subproc.SubprocessResult objects, in order
    executed.
    SubprocessResult has the following properties:
    cmd: the cmd/args executed
    returncode: 0
    stdout: 'stdout str here. None if empty.'
    stderr: 'stderr str here. None if empty.'
    cmdOut.returncode is the exit status of the called process. Typically 0
    means OK. A negative value -N indicates that the child was terminated by
    signal N (POSIX only).
    The run_step method does the actual work. init parses the input yaml.
    Attributes:
        logger (logger): Logger instantiated by name of calling step.
        context: (pypyr.context.Context): The current pypyr Context.
        commands (pypyr.subproc.Commands): Commands to run as subprocess.
        is_shell (bool): True if subprocess should run through default shell.
        name (str): Name of calling step. Used for logging output & error
            messages.
    '''

    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        '''Initialize the CmdStep.
        The step config in the context dict in simplified syntax:
            cmd: <<cmd string>>
        OR, as a dict in expanded syntax:
            cmd:
                run: str. mandatory. command + args to execute.
                save: bool. optional. defaults False. save output to cmdOut.
                cwd: str/path. optional. if specified, change the working
                     directory just for the duration of the command.
        `run` can be a single string, or it can be a list of string if there
        are multiple commands to execute with the same settings.
        OR, as a list:
            cmd:
                - my-executable --arg
                - ./another-executable --arg
        Any or all of the list items can be in expanded syntax.
        Args:
            name (str): Unique name for step. Likely __name__ of calling step.
            context (pypyr.context.Context): Look for step config in this
                context instance.
            is_shell (bool): Set to true to execute cmd through the default
                shell.
        '''
        pass

    def create_command(self, cmd_input: Mapping) -> Command:
        '''Create pypyr.aio.subproc.Command object from expanded step input.'''
        pass

    def run_step(self) -> None:
        '''Spawn subprocesses to run the commands asynchronously.
        If cmd.is_save==True, save aggregate result of all commands to context
        'cmdOut'.
        cmdOut will be a list of pypyr.subproc.SubprocessResult or Exception
        objects, in order executed.
        SubprocessResult has the following properties:
        cmd: the cmd/args executed
        returncode: 0
        stdout: 'stdout str here. None if empty.'
        stderr: 'stderr str here. None if empty.'
        '''
        pass",snippet_95,"from __future__ import annotations

import asyncio
import locale
import os
import shlex
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Iterable, List, Mapping, MutableMapping, Optional, Sequence, Tuple, Union


@dataclass
class SubprocessResult:
    cmd: Union[str, Sequence[str]]
    returncode: int
    stdout: Optional[Union[str, bytes]]
    stderr: Optional[Union[str, bytes]]


@dataclass
class _CommandConfig:
    run: Union[str, Sequence[str]]
    save: bool = False
    cwd: Optional[Union[str, Path]] = None
    is_bytes: bool = False
    encoding: Optional[str] = None
    stdout: Optional[Union[str, Path]] = None
    stderr: Optional[Union[str, Path]] = None
    append: bool = False


try:
    # pypyr Context is mapping-like; we only rely on mapping interface.
    from pypyr.context import Context  # type: ignore
except Exception:  # pragma: no cover
    Context = MutableMapping  # type: ignore


class AsyncCmdStep:
    '''A pypyr step to run executables/commands concurrently as a subprocess.
    This models a step that takes config like this in simple syntax:
        cmds:
            - <<cmd string 1>>
            - <<cmd string 2>>
    All the commands will run concurrently, in parallel.
    OR, expanded syntax is as a dict
        cmds:
            run: list[str | list[str]]. mandatory. command + args to execute.
                If list entry is another list[str], the sub-list will run in
                serial.
            save: bool. defaults False. save output to cmdOut. Treats output
                as text in the system's encoding and removes newlines at end.
            cwd: str/Pathlike. optional. Working directory for these commands.
            bytes (bool): Default False. When `save` return output bytes from
                cmds unaltered, without applying any encoding & text newline
                processing.
            encoding (str): Default None. When `save`, decode output with
                this encoding. The default of None uses the system encoding and
                should ""just work"".
            stdout (str | Path): Default None. Write stdout to this file path.
                Special value `/dev/null` writes to the system null device.
            stderr (str | Path): Default None. Write stderr to this file path.
                Special value `/dev/null` writes to the system null device.
                Special value `/dev/stdout` redirects err output to stdout.
            append (bool): Default False. When stdout/stderr a file, append
                rather than overwrite. Default is to overwrite.
    In expanded syntax, `run` can be a simple string or a list:
        cmds:
          run:
            - ./my-executable --arg
            - [./another-executable --arg, ./arb-executable arghere]
          save: False
          cwd: ./path/here
    As a list in simplified syntax:
        cmds:
          - my-executable --arg
          - ./another-executable --arg
    Any or all of the list items can use expanded syntax:
        cmds:
          - ./simple-cmd-here --arg1 value
          - run: cmd here
            save: False cwd: ./path/here
          - run:
              - my-executable --arg
              - ./another-executable --arg
            save: True
            cwd: ./path/here
    Any of the list items can in turn be a list. A sub-list will run in serial.
    In this example A, B.1 & C will start concurrently. B.2 will only run once
    B.1 is finished.
        cmds:
            - A
            - [B.1, B.2]
            - C
    If save is True, will save the output to context as cmdOut.
    cmdOut will be a list of pypyr.subproc.SubprocessResult objects, in order
    executed.
    SubprocessResult has the following properties:
    cmd: the cmd/args executed
    returncode: 0
    stdout: 'stdout str here. None if empty.'
    stderr: 'stderr str here. None if empty.'
    cmdOut.returncode is the exit status of the called process. Typically 0
    means OK. A negative value -N indicates that the child was terminated by
    signal N (POSIX only).
    The run_step method does the actual work. init parses the input yaml.
    Attributes:
        logger (logger): Logger instantiated by name of calling step.
        context: (pypyr.context.Context): The current pypyr Context.
        commands (pypyr.subproc.Commands): Commands to run as subprocess.
        is_shell (bool): True if subprocess should run through default shell.
        name (str): Name of calling step. Used for logging output & error
            messages.
    '''

    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        '''Initialize the CmdStep.
        The step config in the context dict in simplified syntax:
            cmd: <<cmd string>>
        OR, as a dict in expanded syntax:
            cmd:
                run: str. mandatory. command + args to execute.
                save: bool. optional. defaults False. save output to cmdOut.
                cwd: str/path. optional. if specified, change the working
                     directory just for the duration of the command.
        `run` can be a single string, or it can be a list of string if there
        are multiple commands to execute with the same settings.
        OR, as a list:
            cmd:
                - my-executable --arg
                - ./another-executable --arg
        Any or all of the list items can be in expanded syntax.
        Args:
            name (str): Unique name for step. Likely __name__ of calling step.
            context (pypyr.context.Context): Look for step config in this
                context instance.
            is_shell (bool): Set to true to execute cmd through the default
                shell.
        '''
        self.name = name
        self.context = context
        self.is_shell = bool(is_shell)

        cfg = None
        # Support both 'cmds' (preferred) and 'cmd' (fallback)
        if isinstance(context, Mapping):
            if 'cmds' in context:
                cfg = context['cmds']
            elif 'cmd' in context:
                cfg = context['cmd']
        if cfg is None:
            raise KeyError('AsyncCmdStep expects ""cmds"" (or ""cmd"") in context.')

        # Build execution plan: list of units. A unit is either CommandConfig or list[CommandConfig] (serial group).
        self._units: List[Union[_CommandConfig, List[_CommandConfig]]] = []
        self._parse_config(cfg)
        self._save_any = any(c.save for c in self._iter_all_commands())

    def create_command(self, cmd_input: Mapping) -> _CommandConfig:
        '''Create pypyr.aio.subproc.Command object from expanded step input.'''
        if not isinstance(cmd_input, Mapping):
            raise TypeError('Expanded syntax must be a mapping/dict.')

        run = cmd_input.get('run')
        if run is None:
            raise KeyError('Expanded syntax requires ""run"".')

        save = bool(cmd_input.get('save', False))
        is_bytes = bool(cmd_input.get('bytes', False))
        encoding = cmd_input.get('encoding')
        if encoding is not None and not isinstance(encoding, str):
            raise TypeError('""encoding"" must be a string or None.')
        cwd = cmd_input.get('cwd')
        stdout = cmd_input.get('stdout')
        stderr = cmd_input.get('stderr')
        append = bool(cmd_input.get('append', False))

        # The run value can be str or sequence[str]; return a base config;
        # caller will clone as needed when fan-out.
        return _CommandConfig(
            run=run,
            save=save,
            cwd=cwd,
            is_bytes=is_bytes,
            encoding=encoding,
            stdout=stdout,
            stderr=stderr,
            append=append,
        )

    def run_step(self) -> None:
        '''Spawn subprocesses to run the commands asynchronously.
        If cmd.is_save==True, save aggregate result of all commands to context
        'cmdOut'.
        cmdOut will be a list of pypyr.subproc.SubprocessResult or Exception
        objects, in order executed.
        SubprocessResult has the following properties:
        cmd: the cmd/args executed
        returncode: 0
        stdout: 'stdout str here. None if empty.'
        stderr: 'stderr str here. None if empty.'
        '''
        async def runner():
            # Pre-allocate result list in start order
            total = sum(1 if isinstance(u, _CommandConfig) else len(u) for u in self._units)
            results: List[Union[SubprocessResult, Exception, None]] = [None] * total

            # Index mapping: for each unit, the slice where to place completed results
            slices: List[Tuple[int, int]] = []
            cursor = 0
            for u in self._units:
                if isinstance(u, _CommandConfig):
                    slices.append((cursor, cursor + 1))
                    cursor += 1
                else:
                    n = len(u)
                    slices.append((cursor, cursor + n))
                    cursor += n

            async def run_one(cfg: _CommandConfig) -> Union[SubprocessResult, Exception]:
                try:
                    return await self._run_command(cfg)
                except Exception as ex:  # capture exception as per spec
                    return ex

            async def run_serial(group: List[_CommandConfig]) -> List[Union[SubprocessResult, Exception]]:
                out: List[Union[SubprocessResult, Exception]] = []
                for cfg in group:
                    out.append(await run_one(cfg))
                return out

            tasks: List[asyncio.Task] = []
            # Map task to its slot index/slice
            task_slots: List[Tuple[int, int, bool]] = []  # start, end, is_serial

            for idx, unit in enumerate(self._units):
                if isinstance(unit, _CommandConfig):
                    t = asyncio.create_task(run_one(unit))
                    tasks.append(t)
                    start, end = slices[idx]
                    task_slots.append((start, end, False))
                else:
                    t = asyncio.create_task(run_serial(unit))
                    tasks.append(t)
                    start, end = slices[idx]
                    task_slots.append((start, end, True))

            # Wait for all concurrently
            finished = await asyncio.gather(*tasks, return_exceptions=False)
            # Place results into their positions
            for (start, end, is_serial), value in zip(task_slots, finished):
                if not is_serial:
                    results[start] = value  # type: ignore[index]
                else:
                    seq = value  # type: ignore[assignment]
                    for i, v in enumerate(seq):
                        results[start + i] = v

            # Filter Nones (shouldn't be any)
            flat_results: List[Union[SubprocessResult, Exception]] = [r for r in results if r is not None]  # type: ignore[list-item]

            if self._save_any and isinstance(self.context, MutableMapping):
                self.context['cmdOut'] = flat_results

        asyncio.run(runner())

    # --------------- internals -----------------

    def _parse_config(self, cfg: Any) -> None:
        # cfg can be:
        # - str: single cmd
        # - list: items can be str, list[str], dict (expanded), or list[dict? not supported -> treat as list[str])
        # - dict (expanded) with run: str | list[str|list[str]]
        def normalize_cmd_str_or_seq(run: Union[str, Sequence[str]]) -> Union[str, Sequence[str]]:
            if isinstance(run, str):
                return run
            # ensure sequence[str]
            return list(run)

        if isinstance(cfg, str):
            self._units.append(_CommandConfig(run=cfg))
            return

        if isinstance(cfg, Mapping):
            base = self.create_command(cfg)
            run = base.run
            if isinstance(run, str):
                base.run = normalize_cmd_str_or_seq(run)
                self._units.append(base)
            else:
                # run is a list; entries can be str or list[str] (serial subgroup)
                for entry in list(run):
                    if isinstance(entry, str):
                        self._units.append(_CommandConfig(
                            run=normalize_cmd_str_or_seq(entry),
                            save=base.save,
                            cwd=base.cwd,
                            is_bytes=base.is_bytes,
                            encoding=base.encoding,
                            stdout=base.stdout,
                            stderr=base.stderr,
                            append=base.append,
                        ))
                    elif isinstance(entry, Sequence):
                        group: List[_CommandConfig] = []
                        for sub in list(entry):
                            if not isinstance(sub, str):
                                raise TypeError('Nested ""run"" sub-list must contain strings.')
                            group.append(_CommandConfig(
                                run=normalize_cmd_str_or_seq(sub),
                                save=base.save,
                                cwd=base.cwd,
                                is_bytes=base.is_bytes,
                                encoding=base.encoding,
                                stdout=base.stdout,
                                stderr=base.stderr,
                                append=base.append,
                            ))
                        self._units.append(group)
                    else:
                        raise TypeError('Items in ""run"" list must be str or list[str].')
            return

        if isinstance(cfg, Sequence):
            for item in cfg:
                if isinstance(item, str):
                    self._units.append(_CommandConfig(run=item))
                elif isinstance(item, Mapping):
                    # expanded syntax per item
                    self._parse_config(item)
                elif isinstance(item, Sequence):
                    # serial group
                    group: List[_CommandConfig] = []
                    for sub in item:
                        if isinstance(sub, str):
                            group.append(_CommandConfig(run=sub))
                        elif isinstance(sub, Mapping):
                            # allow expanded per-subcommand inside group
                            subcfg = self.create_command(sub)
                            run = subcfg.run
                            if isinstance(run, str):
                                subcfg.run = run
                                group.append(subcfg)
                            else:
                                # If expanded sub has run list, we flatten:
                                for subrun in list(run):
                                    if not isinstance(subrun, str):
                                        raise TypeError('Nested run list inside serial group must be strings.')
                                    group.append(_CommandConfig(
                                        run=subrun,
                                        save=subcfg.save,
                                        cwd=subcfg.cwd,
                                        is_bytes=subcfg.is_bytes,
                                        encoding=subcfg.encoding,
                                        stdout=subcfg.stdout,
                                        stderr=subcfg.stderr,
                                        append=subcfg.append,
                                    ))
                        else:
                            raise TypeError('Serial group entries must be str or mapping.')
                    self._units.append(group)
                else:
                    raise TypeError('cmds list entries must be str, list[str], or mapping.')
            return

        raise TypeError('Unsupported configuration type for cmds/cmd.')

    def _iter_all_commands(self) -> Iterable[_CommandConfig]:
        for u in self._units:
            if isinstance(u, _CommandConfig):
                yield u
            else:
                for c in u:
                    yield c

    async def _run_command(self, cfg: _CommandConfig) -> SubprocessResult:
        shell = self.is_shell
        cwd = str(cfg.cwd) if cfg.cwd is not None else None

        # stdout/stderr redirection
        stdout_spec, stdout_handle = self._map_redirect(cfg.stdout, cfg.append)
        stderr_spec, stderr_handle = self._map_redirect(cfg.stderr, cfg.append, allow_stdout=True)

        # choose PIPE when saving and not redirected
        want_capture = cfg.save and stdout_spec is None
        want_err_capture = cfg.save and (stderr_spec is None or stderr_spec == asyncio.subprocess.STDOUT)

        if want_capture:
            stdout_param = asyncio.subprocess.PIPE
        else:
            stdout_param = stdout_spec

        if want_err_capture:
            stderr_param = asyncio.subprocess.PIPE if stderr_spec is None else stderr_spec
        else:
            stderr_param = stderr_spec

        # Build command invocation
        if isinstance(cfg.run, str):
            cmd_display = cfg.run
            if shell:
                proc = await asyncio.create_subprocess_shell(
                    cfg.run,
                    stdout=stdout_param,
                    stderr=stderr_param,
                    cwd=cwd
                )
            else:
                args = shlex.split(cfg.run)
                proc = await asyncio.create_subprocess_exec(
                    *args,
                    stdout=stdout_param,
                    stderr=stderr_param,
                    cwd=cwd
                )
        else:
            # sequence args
            args_seq = list(cfg.run)
            cmd_display = args_seq
            if shell:
                # If shell=True with list, join into a string
                cmd_str = ' '.join(shlex.quote(s) for s in args_seq)
                proc = await asyncio.create_subprocess_shell(
                    cmd_str,
                    stdout=stdout_param,
                    stderr=stderr_param,
                    cwd=cwd
                )
            else:
                proc = await asyncio.create_subprocess_exec(
                    *args_seq,
                    stdout=stdout_param,
                    stderr=stderr_param,
                    cwd=cwd
                )

        out_b, err_b = await proc.communicate()

        # Close any opened file handles
        if stdout_handle is not None:
            try:
                stdout_handle.close()
            except Exception:
                pass
        if stderr_handle is not None and stderr_handle is not stdout_handle:
            try:
                stderr_handle.close()
            except Exception:
                pass

        # Prepare outputs
        if cfg.save:
            if cfg.is_bytes:
                out_v = out_b if isinstance(out_b, (bytes, bytearray)) and out_b else None
                err_v = err_b if isinstance(err_b, (bytes, bytearray)) and err_b else None
            else:
                enc = cfg.encoding or locale.getpreferredencoding(False)
                out_v = out_b.decode(enc).rstrip('\n') if isinstance(out_b, (bytes, bytearray)) and out_b is not None else None
                err_v = err_b.decode(enc).rstrip('\n') if isinstance(err_b, (bytes, bytearray)) and err_b is not None else None
        else:
            out_v = None
            err_v = None

        return SubprocessResult(
            cmd=cmd_display,
            returncode=proc.returncode,
            stdout=out_v,
            stderr=err_v
        )

    def _map_redirect(self, target: Optional[Union[str, Path]], append: bool, allow_stdout: bool=False):
        # Returns (spec, handle) for asyncio subprocess stdout/stderr param
        if target is None:
            return None, None
        t = str(target)

        if t == '/dev/null':
            return asyncio.subprocess.DEVNULL, None

        if allow_stdout and t == '/dev/stdout':
            return asyncio.subprocess.STDOUT, None

        # File path
        mode = 'ab' if append else 'wb'
        # Ensure parent dir exists
        p = Path(t)
        p.parent.mkdir(parents=True, exist_ok=True)
        f = p.open(mode)
        return f, f"
167547,automl/HpBandSter,automl_HpBandSter/hpbandster/optimizers/learning_curve_models/base.py,hpbandster.optimizers.learning_curve_models.base.LCModel,"class LCModel:
    """"""
        base class for simple learning curve models
    """"""

    def fit(self, times, losses, configs=None):
        """"""
            function to train the model on the observed data

            Parameters:
            -----------

            times: list
                list of numpy arrays of the timesteps for each curve
            losses: list
                list of numpy arrays of the loss (the actual learning curve)
            configs: list or None
                list of the configurations for each sample. Each element
                has to be a numpy array. Set to None, if no configuration
                information is available.
        """"""
        raise NotImplementedError()

    def predict_unseen(self, times, config):
        """"""
            predict the loss of an unseen configuration

            Parameters:
            -----------

            times: numpy array
                times where to predict the loss
            config: numpy array
                the numerical representation of the config

            Returns:
            --------

            mean and variance prediction at input times for the given config
        """"""
        raise NotImplementedError()

    def extend_partial(self, times, obs_times, obs_losses, config=None):
        """"""
            extends a partially observed curve

            Parameters:
            -----------

            times: numpy array
                times where to predict the loss
            obs_times: numpy array
                times where the curve has already been observed
            obs_losses: numpy array
                corresponding observed losses
            config: numpy array
                numerical reperesentation of the config; None if no config
                information is available

            Returns:
            --------

            mean and variance prediction at input times


        """"""","class LCModel:
    '''
        base class for simple learning curve models
            '''

    def fit(self, times, losses, configs=None):
        '''
            function to train the model on the observed data
            Parameters:
            -----------
            times: list
                list of numpy arrays of the timesteps for each curve
            losses: list
                list of numpy arrays of the loss (the actual learning curve)
            configs: list or None
                list of the configurations for each sample. Each element
                has to be a numpy array. Set to None, if no configuration
                information is available.
        '''
        pass

    def predict_unseen(self, times, config):
        '''
            predict the loss of an unseen configuration
            Parameters:
            -----------
            times: numpy array
                times where to predict the loss
            config: numpy array
                the numerical representation of the config
            Returns:
            --------
            mean and variance prediction at input times for the given config
        '''
        pass

    def extend_partial(self, times, obs_times, obs_losses, config=None):
        '''
            extends a partially observed curve
            Parameters:
            -----------
            times: numpy array
                times where to predict the loss
            obs_times: numpy array
                times where the curve has already been observed
            obs_losses: numpy array
                corresponding observed losses
            config: numpy array
                numerical reperesentation of the config; None if no config
                information is available
            Returns:
            --------
            mean and variance prediction at input times

        '''
        pass",snippet_98,"import numpy as np


class LCModel:
    '''
        base class for simple learning curve models
            '''

    def __init__(self, k_neighbors=10, min_variance=1e-8, eps=1e-12, obs_weight=5.0, exact_tol=0.0):
        self.k_neighbors = k_neighbors
        self.min_variance = min_variance
        self.eps = eps
        self.obs_weight = obs_weight
        self.exact_tol = exact_tol
        self._fitted = False
        self._train_times = None
        self._train_losses = None
        self._configs = None

    def fit(self, times, losses, configs=None):
        '''
            function to train the model on the observed data
            Parameters:
            -----------
            times: list
                list of numpy arrays of the timesteps for each curve
            losses: list
                list of numpy arrays of the loss (the actual learning curve)
            configs: list or None
                list of the configurations for each sample. Each element
                has to be a numpy array. Set to None, if no configuration
                information is available.
        '''
        if not isinstance(times, list) or not isinstance(losses, list):
            raise TypeError(""times and losses must be lists of numpy arrays."")
        if len(times) != len(losses):
            raise ValueError(""times and losses must have the same length."")
        if len(times) == 0:
            raise ValueError(""times and losses must be non-empty."")

        flat_t = []
        flat_y = []
        for t_arr, y_arr in zip(times, losses):
            t_arr = np.asarray(t_arr).ravel()
            y_arr = np.asarray(y_arr).ravel()
            if t_arr.size != y_arr.size:
                raise ValueError(""Each times array must match the size of the corresponding losses array."")
            if t_arr.size == 0:
                continue
            mask = np.isfinite(t_arr) & np.isfinite(y_arr)
            t_arr = t_arr[mask]
            y_arr = y_arr[mask]
            if t_arr.size == 0:
                continue
            flat_t.append(t_arr.astype(float))
            flat_y.append(y_arr.astype(float))

        if len(flat_t) == 0:
            raise ValueError(""After filtering, no valid training data points remain."")

        self._train_times = np.concatenate(flat_t, axis=0)
        self._train_losses = np.concatenate(flat_y, axis=0)

        # Sort by time for potential performance benefit (not strictly required)
        order = np.argsort(self._train_times)
        self._train_times = self._train_times[order]
        self._train_losses = self._train_losses[order]

        # Store configs as-is for API compatibility; not used in this baseline
        if configs is not None:
            if not isinstance(configs, list) or len(configs) != len(times):
                raise ValueError(""configs must be a list of the same length as times and losses, or None."")
            self._configs = configs
        else:
            self._configs = None

        self._fitted = True
        return self

    def predict_unseen(self, times, config):
        '''
            predict the loss of an unseen configuration
            Parameters:
            -----------
            times: numpy array
                times where to predict the loss
            config: numpy array
                the numerical representation of the config
            Returns:
            --------
            mean and variance prediction at input times for the given config
        '''
        self._check_fitted()
        q_times = self._ensure_1d_array(times, ""times"")
        means, vars_ = self._knn_predict(q_times,
                                         support_times=self._train_times,
                                         support_values=self._train_losses,
                                         k=self.k_neighbors,
                                         obs_weight=1.0)
        return means, vars_

    def extend_partial(self, times, obs_times, obs_losses, config=None):
        '''
            extends a partially observed curve
            Parameters:
            -----------
            times: numpy array
                times where to predict the loss
            obs_times: numpy array
                times where the curve has already been observed
            obs_losses: numpy array
                corresponding observed losses
            config: numpy array
                numerical reperesentation of the config; None if no config
                information is available
            Returns:
            --------
            mean and variance prediction at input times

        '''
        self._check_fitted()
        q_times = self._ensure_1d_array(times, ""times"")
        obs_t = self._ensure_1d_array(obs_times, ""obs_times"")
        obs_y = self._ensure_1d_array(obs_losses, ""obs_losses"")
        if obs_t.size != obs_y.size:
            raise ValueError(""obs_times and obs_losses must have the same length."")

        if obs_t.size == 0:
            return self.predict_unseen(q_times, config)

        # Combine observed partial data with training support
        support_times = np.concatenate([obs_t, self._train_times], axis=0)
        support_values = np.concatenate([obs_y, self._train_losses], axis=0)
        means = np.empty_like(q_times, dtype=float)
        vars_ = np.empty_like(q_times, dtype=float)

        # If a query time exactly matches an observed time (within tol), return the observation
        if self.exact_tol > 0.0:
            # vectorized exact matches handling
            for i, tq in enumerate(q_times):
                diffs = np.abs(obs_t - tq)
                idx = np.argmin(diffs)
                if diffs[idx] <= self.exact_tol:
                    means[i] = obs_y[idx]
                    vars_[i] = 0.0
                else:
                    m, v = self._knn_predict_single(
                        tq,
                        support_times=support_times,
                        support_values=support_values,
                        k=self.k_neighbors,
                        obs_weight=self.obs_weight,
                        obs_count=obs_t.size
                    )
                    means[i] = m
                    vars_[i] = v
        else:
            # Also handle exact equality without tolerance
            for i, tq in enumerate(q_times):
                eq_mask = (obs_t == tq)
                if np.any(eq_mask):
                    means[i] = obs_y[np.where(eq_mask)[0][0]]
                    vars_[i] = 0.0
                else:
                    m, v = self._knn_predict_single(
                        tq,
                        support_times=support_times,
                        support_values=support_values,
                        k=self.k_neighbors,
                        obs_weight=self.obs_weight,
                        obs_count=obs_t.size
                    )
                    means[i] = m
                    vars_[i] = v

        return means, vars_

    # --------- internal helpers ---------

    def _check_fitted(self):
        if not self._fitted or self._train_times is None or self._train_losses is None:
            raise RuntimeError(""Model is not fitted. Call fit(...) before prediction."")

    @staticmethod
    def _ensure_1d_array(arr, name):
        arr = np.asarray(arr)
        if arr.ndim == 0:
            arr = arr.reshape(1)
        elif arr.ndim > 1:
            arr = arr.ravel()
        if arr.size == 0:
            return arr.astype(float)
        if not np.issubdtype(arr.dtype, np.number):
            raise TypeError(f""{name} must be numeric."")
        return arr.astype(float)

    def _knn_predict(self, query_times, support_times, support_values, k, obs_weight=1.0, obs_count=0):
        means = np.empty_like(query_times, dtype=float)
        vars_ = np.empty_like(query_times, dtype=float)
        for i, tq in enumerate(query_times):
            m, v = self._knn_predict_single(
                tq,
                support_times=support_times,
                support_values=support_values,
                k=k,
                obs_weight=obs_weight,
                obs_count=obs_count
            )
            means[i] = m
            vars_[i] = v
        return means, vars_

    def _knn_predict_single(self, tq, support_times, support_values, k, obs_weight=1.0, obs_count=0):
        n = support_times.size
        if n == 0:
            return np.nan, np.nan
        k_eff = int(max(1, min(k, n)))

        # Compute absolute time distances
        d = np.abs(support_times - tq)

        # Find k nearest neighbors
        if k_eff < n:
            idx = np.argpartition(d, k_eff - 1)[:k_eff]
            # sort selected by distance for stability
            idx = idx[np.argsort(d[idx])]
        else:
            idx = np.argsort(d)

        sel_times = support_times[idx]
        sel_values = support_values[idx]
        sel_d = np.abs(sel_times - tq)

        # Inverse distance weights with epsilon for stability
        w = 1.0 / (sel_d + self.eps)

        # Boost weights for observed points if obs_count > 0
        if obs_count > 0:
            obs_mask = idx < obs_count
            w = np.where(obs_mask, w * obs_weight, w)

        # Normalize weights
        w_sum = np.sum(w)
        if not np.isfinite(w_sum) or w_sum <= 0.0:
            w = np.full_like(w, 1.0 / w.size)

        mean = np.sum(w * sel_values) / np.sum(w)

        # Weighted variance
        diff = sel_values - mean
        # Use stable computation with small ridge to avoid zero variance
        var_num = np.sum(w * diff * diff)
        var_den = np.sum(w)
        var = var_num / max(self.eps, var_den)
        var = float(max(var, self.min_variance))
        return float(mean), var"
168574,romanz/trezor-agent,romanz_trezor-agent/libagent/age/client.py,libagent.age.client.Client,"from cryptography.hazmat.primitives.kdf.hkdf import HKDF
from cryptography.hazmat.primitives import hashes

class Client:
    """"""Sign messages and get public keys from a hardware device.""""""

    def __init__(self, device):
        """"""C-tor.""""""
        self.device = device

    def pubkey(self, identity, ecdh=False):
        """"""Return public key as VerifyingKey object.""""""
        with self.device:
            pubkey = bytes(self.device.pubkey(ecdh=ecdh, identity=identity))
            assert len(pubkey) == 32
            return pubkey

    def ecdh(self, identity, peer_pubkey):
        """"""Derive shared secret using ECDH from peer public key.""""""
        log.info('please confirm AGE decryption on %s for ""%s""...', self.device, identity.to_string())
        with self.device:
            assert len(peer_pubkey) == 32
            result, self_pubkey = self.device.ecdh_with_pubkey(pubkey=b'@' + peer_pubkey, identity=identity)
            assert result[:1] == b'\x04'
            hkdf = HKDF(algorithm=hashes.SHA256(), length=32, salt=peer_pubkey + self_pubkey, info=b'age-encryption.org/v1/X25519')
            return hkdf.derive(result[1:])","
class Client:
    '''Sign messages and get public keys from a hardware device.'''

    def __init__(self, device):
        '''C-tor.'''
        pass

    def pubkey(self, identity, ecdh=False):
        '''Return public key as VerifyingKey object.'''
        pass

    def ecdh(self, identity, peer_pubkey):
        '''Derive shared secret using ECDH from peer public key.'''
        pass",snippet_100,"import binascii
from ecdsa import VerifyingKey, SECP256k1


class Client:
    '''Sign messages and get public keys from a hardware device.'''

    def __init__(self, device):
        '''C-tor.'''
        self._dev = device

    def pubkey(self, identity, ecdh=False):
        '''Return public key as VerifyingKey object.'''
        key_bytes = self._get_pubkey_bytes(identity, ecdh)
        return self._vk_from_bytes(key_bytes)

    def ecdh(self, identity, peer_pubkey):
        '''Derive shared secret using ECDH from peer public key.'''
        peer_bytes = self._ensure_compressed_pubkey(peer_pubkey)
        fn = (
            getattr(self._dev, ""ecdh"", None)
            or getattr(self._dev, ""derive"", None)
            or getattr(self._dev, ""derive_ecdh"", None)
            or getattr(self._dev, ""ecdh_shared_secret"", None)
        )
        if fn is None:
            raise AttributeError(""Device does not support ECDH"")
        secret = fn(identity, peer_bytes)
        if not isinstance(secret, (bytes, bytearray)):
            raise TypeError(""Device ECDH must return bytes"")
        return bytes(secret)

    # internal helpers

    def _get_pubkey_bytes(self, identity, ecdh):
        candidates = [
            (""pubkey"", {""ecdh"": ecdh}),
            (""pubkey"", {}),
            (""get_pubkey"", {""ecdh"": ecdh}),
            (""get_pubkey"", {}),
            (""get_public_key"", {""ecdh"": ecdh}),
            (""get_public_key"", {}),
        ]
        for name, kwargs in candidates:
            fn = getattr(self._dev, name, None)
            if fn is None:
                continue
            try:
                res = fn(identity, **kwargs)
            except TypeError:
                # try positional ecdh if keyword not accepted
                try:
                    res = fn(identity, ecdh)
                except TypeError:
                    res = fn(identity)
            if not isinstance(res, (bytes, bytearray)):
                raise TypeError(""Device pubkey method must return bytes"")
            return bytes(res)
        raise AttributeError(""Device does not expose a pubkey retrieval method"")

    def _vk_from_bytes(self, key_bytes):
        b = bytes(key_bytes)
        if len(b) == 33:
            return VerifyingKey.from_string(b, curve=SECP256k1, encoding=""compressed"")
        if len(b) == 65 and b[0] == 0x04:
            return VerifyingKey.from_string(b[1:], curve=SECP256k1, encoding=""uncompressed"")
        if len(b) == 64:
            return VerifyingKey.from_string(b, curve=SECP256k1, encoding=""uncompressed"")
        raise ValueError(""Unsupported public key length/format"")

    def _ensure_compressed_pubkey(self, peer_pubkey):
        # Accept VerifyingKey, bytes (compressed/uncompressed/raw), or hex string
        if isinstance(peer_pubkey, VerifyingKey):
            return peer_pubkey.to_string(encoding=""compressed"")
        if isinstance(peer_pubkey, str):
            s = peer_pubkey.strip().lower()
            if s.startswith(""0x""):
                s = s[2:]
            b = binascii.unhexlify(s)
        elif isinstance(peer_pubkey, (bytes, bytearray)):
            b = bytes(peer_pubkey)
        else:
            raise TypeError(""peer_pubkey must be VerifyingKey, bytes or hex string"")
        if len(b) == 33 and b[0] in (2, 3):
            return b
        if len(b) == 65 and b[0] == 0x04:
            # uncompressed with prefix, compress
            x = b[1:33]
            y = b[33:65]
            prefix = 0x02 | (y[-1] & 1)
            return bytes([prefix]) + x
        if len(b) == 64:
            # raw x||y, compress
            x = b[:32]
            y = b[32:]
            prefix = 0x02 | (y[-1] & 1)
            return bytes([prefix]) + x
        # If it's an unexpected length, try parsing as VerifyingKey via helper
        vk = self._vk_from_bytes(b)
        return vk.to_string(encoding=""compressed"")"
168596,romanz/trezor-agent,romanz_trezor-agent/libagent/ssh/__init__.py,libagent.ssh.ClosableNamedTemporaryFile,"import tempfile
import os

class ClosableNamedTemporaryFile:
    """"""Creates a temporary file that is not deleted when the file is closed.

    This allows the file to be opened with an exclusive lock, but used by other programs before
    it is deleted
    """"""

    def __init__(self):
        """"""Create a temporary file.""""""
        self.file = tempfile.NamedTemporaryFile(prefix='trezor-ssh-pubkey-', mode='w', delete=False)
        self.name = self.file.name

    def write(self, buf):
        """"""Write `buf` to the file.""""""
        self.file.write(buf)

    def close(self):
        """"""Closes the file, allowing it to be opened by other programs. Does not delete the file.""""""
        self.file.close()

    def __del__(self):
        """"""Deletes the temporary file.""""""
        try:
            os.unlink(self.file.name)
        except OSError:
            log.warning('Failed to delete temporary file: %s', self.file.name)","
class ClosableNamedTemporaryFile:
    '''Creates a temporary file that is not deleted when the file is closed.
    This allows the file to be opened with an exclusive lock, but used by other programs before
    it is deleted
    '''

    def __init__(self):
        '''Create a temporary file.'''
        pass

    def write(self, buf):
        '''Write `buf` to the file.'''
        pass

    def close(self):
        '''Closes the file, allowing it to be opened by other programs. Does not delete the file.'''
        pass

    def __del__(self):
        '''Deletes the temporary file.'''
        pass",snippet_101,"import tempfile
import os
import atexit

class ClosableNamedTemporaryFile:
    '''Creates a temporary file that is not deleted when the file is closed.
    This allows the file to be opened with an exclusive lock, but used by other programs before
    it is deleted
    '''

    def __init__(self):
        '''Create a temporary file.'''
        tmp = tempfile.NamedTemporaryFile(mode='wb', delete=False)
        self._fh = tmp
        self.name = tmp.name
        self._closed = False
        atexit.register(self._cleanup)

    def write(self, buf):
        '''Write `buf` to the file.'''
        if self._fh is None or self._fh.closed:
            raise ValueError(""I/O operation on closed file."")
        if isinstance(buf, str):
            buf = buf.encode('utf-8')
        self._fh.write(buf)
        self._fh.flush()

    def close(self):
        '''Closes the file, allowing it to be opened by other programs. Does not delete the file.'''
        if self._fh and not self._fh.closed:
            try:
                self._fh.flush()
                os.fsync(self._fh.fileno())
            except Exception:
                pass
            self._fh.close()
        self._closed = True

    def _cleanup(self):
        try:
            if self._fh and not self._fh.closed:
                self._fh.close()
        except Exception:
            pass
        try:
            if self.name and os.path.exists(self.name):
                os.remove(self.name)
        except Exception:
            pass

    def __del__(self):
        '''Deletes the temporary file.'''
        self._cleanup()"
168603,romanz/trezor-agent,romanz_trezor-agent/libagent/util.py,libagent.util.ExpiringCache,"import time

class ExpiringCache:
    """"""Simple cache with a deadline.""""""

    def __init__(self, seconds, timer=time.time):
        """"""C-tor.""""""
        self.duration = seconds
        self.timer = timer
        self.value = None
        self.set(None)

    def get(self):
        """"""Returns existing value, or None if deadline has expired.""""""
        if self.timer() > self.deadline:
            self.value = None
        return self.value

    def set(self, value):
        """"""Set new value and reset the deadline for expiration.""""""
        self.deadline = self.timer() + self.duration
        self.value = value","
class ExpiringCache:
    '''Simple cache with a deadline.'''

    def __init__(self, seconds, timer=time.time):
        '''C-tor.'''
        pass

    def get(self):
        '''Returns existing value, or None if deadline has expired.'''
        pass

    def set(self, value):
        '''Set new value and reset the deadline for expiration.'''
        pass",snippet_102,"import time

class ExpiringCache:
    '''Simple cache with a deadline.'''

    def __init__(self, seconds, timer=time.time):
        '''C-tor.'''
        self.seconds = float(seconds)
        self.timer = timer
        self._value = None
        self._deadline = None

    def get(self):
        '''Returns existing value, or None if deadline has expired.'''
        if self._deadline is None:
            return None
        if self.timer() >= self._deadline:
            self._value = None
            self._deadline = None
            return None
        return self._value

    def set(self, value):
        '''Set new value and reset the deadline for expiration.'''
        self._value = value
        self._deadline = self.timer() + self.seconds"
168604,romanz/trezor-agent,romanz_trezor-agent/libagent/util.py,libagent.util.Reader,"import struct
import contextlib

class Reader:
    """"""Read basic type objects out of given stream.""""""

    def __init__(self, stream):
        """"""Create a non-capturing reader.""""""
        self.s = stream
        self._captured = None

    def readfmt(self, fmt):
        """"""Read a specified object, using a struct format string.""""""
        size = struct.calcsize(fmt)
        blob = self.read(size)
        obj, = struct.unpack(fmt, blob)
        return obj

    def read(self, size=None):
        """"""Read `size` bytes from stream.""""""
        blob = self.s.read(size)
        if size is not None and len(blob) < size:
            raise EOFError
        if self._captured:
            self._captured.write(blob)
        return blob

    @contextlib.contextmanager
    def capture(self, stream):
        """"""Capture all data read during this context.""""""
        self._captured = stream
        try:
            yield
        finally:
            self._captured = None","
class Reader:
    '''Read basic type objects out of given stream.'''

    def __init__(self, stream):
        '''Create a non-capturing reader.'''
        pass

    def readfmt(self, fmt):
        '''Read a specified object, using a struct format string.'''
        pass

    def readfmt(self, fmt):
        '''Read `size` bytes from stream.'''
        pass
    @contextlib.contextmanager
    def capture(self, stream):
        '''Capture all data read during this context.'''
        pass",snippet_103,"import contextlib
import struct

class Reader:
    '''Read basic type objects out of given stream.'''

    def __init__(self, stream):
        '''Create a non-capturing reader.'''
        self._stream = stream
        self._capture_stream = None

    def _read_exact(self, size):
        if size < 0:
            raise ValueError(""size must be non-negative"")
        chunks = []
        remaining = size
        while remaining:
            chunk = self._stream.read(remaining)
            if not chunk:
                raise EOFError(f""unexpected end of stream; wanted {size} bytes, got {size - remaining}"")
            chunks.append(chunk)
            remaining -= len(chunk)
        data = b"""".join(chunks)
        if self._capture_stream is not None and data:
            self._capture_stream.write(data)
        return data

    def readfmt(self, fmt):
        '''Read a specified object, using a struct format string, or read `size` bytes if fmt is int.'''
        if isinstance(fmt, int):
            return self._read_exact(fmt)
        if isinstance(fmt, str):
            size = struct.calcsize(fmt)
            data = self._read_exact(size)
            values = struct.unpack(fmt, data)
            return values[0] if len(values) == 1 else values
        raise TypeError(""fmt must be an int (size) or a struct format string"")

    @contextlib.contextmanager
    def capture(self, stream):
        '''Capture all data read during this context.'''
        prev = self._capture_stream
        self._capture_stream = stream
        try:
            yield
        finally:
            self._capture_stream = prev"
171012,indygreg/python-zstandard,indygreg_python-zstandard/zstandard/backend_cffi.py,zstandard.backend_cffi.BufferSegment,"class BufferSegment:
    """"""Represents a segment within a ``BufferWithSegments``.

    This type is essentially a reference to N bytes within a
    ``BufferWithSegments``.

    The object conforms to the buffer protocol.
    """"""

    @property
    def offset(self):
        """"""The byte offset of this segment within its parent buffer.""""""
        raise NotImplementedError()

    def __len__(self):
        """"""Obtain the length of the segment, in bytes.""""""
        raise NotImplementedError()

    def tobytes(self):
        """"""Obtain bytes copy of this segment.""""""
        raise NotImplementedError()","class BufferSegment:
    '''Represents a segment within a ``BufferWithSegments``.
    This type is essentially a reference to N bytes within a
    ``BufferWithSegments``.
    The object conforms to the buffer protocol.
    '''
    @property
    def offset(self):
        '''The byte offset of this segment within its parent buffer.'''
        pass

    def __len__(self):
        '''Obtain the length of the segment, in bytes.'''
        pass

    def tobytes(self):
        '''Obtain bytes copy of this segment.'''
        pass",snippet_104,"class BufferSegment:
    '''Represents a segment within a ``BufferWithSegments``.
    This type is essentially a reference to N bytes within a
    ``BufferWithSegments``.
    The object conforms to the buffer protocol.
    '''
    def __init__(self, buffer, offset=0, length=None):
        mv = memoryview(buffer)
        if mv.ndim != 1:
            mv = mv.cast('B')
        if offset < 0:
            raise ValueError(""offset must be non-negative"")
        total_len = mv.nbytes
        if length is None:
            length = total_len - offset
        if length < 0:
            raise ValueError(""length must be non-negative"")
        end = offset + length
        if end > total_len:
            raise ValueError(""offset + length exceeds buffer size"")
        self._parent_view = mv
        self._offset = int(offset)
        # Slice in bytes; memoryview slicing is by elements (1 byte here)
        self._view = mv[self._offset:end]

    @property
    def offset(self):
        '''The byte offset of this segment within its parent buffer.'''
        return self._offset

    def __len__(self):
        '''Obtain the length of the segment, in bytes.'''
        return self._view.nbytes

    def tobytes(self):
        '''Obtain bytes copy of this segment.'''
        return self._view.tobytes()

    # Optional conveniences
    def __bytes__(self):
        return self.tobytes()

    def __repr__(self):
        return f""{self.__class__.__name__}(len={len(self)}, offset={self.offset})""

    def __getitem__(self, key):
        return self._view[key]"
171015,indygreg/python-zstandard,indygreg_python-zstandard/zstandard/backend_cffi.py,zstandard.backend_cffi.BufferWithSegmentsCollection,"class BufferWithSegmentsCollection:
    """"""A virtual spanning view over multiple BufferWithSegments.

    Instances are constructed from 1 or more :py:class:`BufferWithSegments`
    instances. The resulting object behaves like an ordered sequence whose
    members are the segments within each ``BufferWithSegments``.

    If the object is composed of 2 ``BufferWithSegments`` instances with the
    first having 2 segments and the second have 3 segments, then ``b[0]``
    and ``b[1]`` access segments in the first object and ``b[2]``, ``b[3]``,
    and ``b[4]`` access segments from the second.
    """"""

    def __len__(self):
        """"""The number of segments within all ``BufferWithSegments``.""""""
        raise NotImplementedError()

    def __getitem__(self, i):
        """"""Obtain the ``BufferSegment`` at an offset.""""""
        raise NotImplementedError()","class BufferWithSegmentsCollection:
    '''A virtual spanning view over multiple BufferWithSegments.
    Instances are constructed from 1 or more :py:class:`BufferWithSegments`
    instances. The resulting object behaves like an ordered sequence whose
    members are the segments within each ``BufferWithSegments``.
    If the object is composed of 2 ``BufferWithSegments`` instances with the
    first having 2 segments and the second have 3 segments, then ``b[0]``
    and ``b[1]`` access segments in the first object and ``b[2]``, ``b[3]``,
    and ``b[4]`` access segments from the second.
    '''

    def __len__(self):
        '''The number of segments within all ``BufferWithSegments``.'''
        pass

    def __getitem__(self, i):
        '''Obtain the ``BufferSegment`` at an offset.'''
        pass",snippet_105,"class BufferWithSegmentsCollection:
    '''A virtual spanning view over multiple BufferWithSegments.
    Instances are constructed from 1 or more :py:class:`BufferWithSegments`
    instances. The resulting object behaves like an ordered sequence whose
    members are the segments within each ``BufferWithSegments``.
    If the object is composed of 2 ``BufferWithSegments`` instances with the
    first having 2 segments and the second have 3 segments, then ``b[0]``
    and ``b[1]`` access segments in the first object and ``b[2]``, ``b[3]``,
    and ``b[4]`` access segments from the second.
    '''

    def __init__(self, *buffers):
        if len(buffers) == 1 and hasattr(buffers[0], '__iter__') and not hasattr(buffers[0], '__getitem__'):
            buffers = tuple(buffers[0])
        self._buffers = list(buffers)
        self._cum = []
        total = 0
        for b in self._buffers:
            n = len(b)
            total += n
            self._cum.append(total)
        self._len = total

    def __len__(self):
        '''The number of segments within all ``BufferWithSegments``.'''
        return self._len

    def __getitem__(self, i):
        '''Obtain the ``BufferSegment`` at an offset.'''
        if isinstance(i, slice):
            start, stop, step = i.indices(self._len)
            if step == 1:
                # Fast path: gather contiguous ranges from underlying buffers
                result = []
                if start >= stop:
                    return result
                cur = start
                while cur < stop:
                    # find buffer containing cur
                    buf_idx = self._find_buffer_index(cur)
                    prev_cum = 0 if buf_idx == 0 else self._cum[buf_idx - 1]
                    local_start = cur - prev_cum
                    # maximum local index we can take from this buffer
                    local_end_cap = self._cum[buf_idx] - prev_cum
                    take = min(local_end_cap, local_start + (stop - cur))
                    # extend from local_start to take (exclusive)
                    b = self._buffers[buf_idx]
                    result.extend(b[local_start:take])
                    cur += (take - local_start)
                return result
            else:
                return [self[idx] for idx in range(start, stop, step)]
        # integer index
        idx = i
        if idx < 0:
            idx += self._len
        if idx < 0 or idx >= self._len:
            raise IndexError('index out of range')
        buf_idx = self._find_buffer_index(idx)
        prev_cum = 0 if buf_idx == 0 else self._cum[buf_idx - 1]
        local_idx = idx - prev_cum
        return self._buffers[buf_idx][local_idx]

    def _find_buffer_index(self, global_index):
        # binary search over cumulative counts
        lo, hi = 0, len(self._cum)
        while lo < hi:
            mid = (lo + hi) // 2
            if global_index < self._cum[mid]:
                hi = mid
            else:
                lo = mid + 1
        return lo"
171185,ssalentin/plip,ssalentin_plip/plip/exchange/xml.py,plip.exchange.xml.XMLStorage,"class XMLStorage:
    """"""Generic class for storing XML data from PLIP XML files.""""""

    @staticmethod
    def getdata(tree, location, force_string=False):
        """"""Gets XML data from a specific element and handles types.""""""
        found = tree.xpath('%s/text()' % location)
        if not found:
            return None
        else:
            data = found[0]
        if force_string:
            return data
        if data == 'True':
            return True
        elif data == 'False':
            return False
        else:
            try:
                return int(data)
            except ValueError:
                try:
                    return float(data)
                except ValueError:
                    return data

    @staticmethod
    def getcoordinates(tree, location):
        """"""Gets coordinates from a specific element in PLIP XML""""""
        return tuple((float(x) for x in tree.xpath('.//%s/*/text()' % location)))","class XMLStorage:
    '''Generic class for storing XML data from PLIP XML files.'''
    @staticmethod
    def getdata(tree, location, force_string=False):
        '''Gets XML data from a specific element and handles types.'''
        pass
    @staticmethod
    def getcoordinates(tree, location):
        '''Gets coordinates from a specific element in PLIP XML'''
        pass",snippet_106,"class XMLStorage:
    '''Generic class for storing XML data from PLIP XML files.'''

    @staticmethod
    def _root(tree):
        try:
            return tree.getroot()
        except AttributeError:
            return tree

    @staticmethod
    def _coerce_value(text, force_string=False):
        if text is None:
            return None
        s = str(text).strip()
        if s == """":
            return None
        if force_string:
            return s
        lower = s.lower()
        if lower in {""true"", ""yes""}:
            return True
        if lower in {""false"", ""no""}:
            return False
        # Int
        try:
            if s.isdigit() or (s.startswith((""+"", ""-"")) and s[1:].isdigit()):
                return int(s)
        except Exception:
            pass
        # Float
        try:
            return float(s)
        except Exception:
            pass
        return s

    @staticmethod
    def getdata(tree, location, force_string=False):
        '''Gets XML data from a specific element and handles types.'''
        root = XMLStorage._root(tree)

        # Support attribute selector at end: path/@attr
        attr_name = None
        attr_path = None
        if ""/@"" in location:
            attr_path, attr_name = location.rsplit(""/@"", 1)

        try:
            if attr_name:
                elems = list(root.findall(attr_path))
                if not elems:
                    return None
                values = [e.get(attr_name) for e in elems]
                values = [XMLStorage._coerce_value(v, force_string) for v in values]
            else:
                elems = list(root.findall(location))
                if not elems:
                    return None
                values = [XMLStorage._coerce_value(e.text, force_string) for e in elems]
        except Exception:
            return None

        # Collapse singletons
        if len(values) == 1:
            return values[0]
        return values

    @staticmethod
    def getcoordinates(tree, location):
        '''Gets coordinates from a specific element in PLIP XML'''
        import re

        root = XMLStorage._root(tree)

        try:
            elems = list(root.findall(location))
        except Exception:
            return None
        if not elems:
            return None

        def parse_coords(elem):
            # 1) Attributes x,y,z
            for keyset in ((""x"", ""y"", ""z""), (""X"", ""Y"", ""Z"")):
                if all(k in elem.attrib for k in keyset):
                    try:
                        return (
                            float(elem.attrib[keyset[0]]),
                            float(elem.attrib[keyset[1]]),
                            float(elem.attrib[keyset[2]]),
                        )
                    except Exception:
                        pass
            # 2) Child elements <x>,<y>,<z>
            def child_text(name):
                c = elem.find(name)
                return None if c is None else c.text
            for names in ((""x"", ""y"", ""z""), (""X"", ""Y"", ""Z"")):
                xt, yt, zt = child_text(names[0]), child_text(names[1]), child_text(names[2])
                if xt is not None and yt is not None and zt is not None:
                    try:
                        return (float(xt), float(yt), float(zt))
                    except Exception:
                        pass
            # 3) Text with three numbers
            txt = elem.text.strip() if elem.text else """"
            if txt:
                nums = re.findall(r""[+-]?\d+(?:\.\d+)?(?:[eE][+-]?\d+)?"", txt.replace("","", "" ""))
                if len(nums) >= 3:
                    try:
                        return (float(nums[0]), float(nums[1]), float(nums[2]))
                    except Exception:
                        pass
            return None

        coords_list = [parse_coords(e) for e in elems]
        coords_list = [c for c in coords_list if c is not None]

        if not coords_list:
            return None
        if len(elems) == 1:
            return coords_list[0]
        return coords_list"
172857,cogeotiff/rio-tiler,rio_tiler/colormap.py,rio_tiler.colormap.ColorMaps,"import attr
from rio_tiler.errors import ColorMapAlreadyRegistered, InvalidColorFormat, InvalidColorMapName, InvalidFormat
import json
import numpy
from typing import Dict, List, Sequence, Tuple, Union
import pathlib
from rio_tiler.types import ColorMapType, DataMaskType, DiscreteColorMapType, GDALColorMapType, IntervalColorMapType

@attr.s(frozen=True)
class ColorMaps:
    """"""Default Colormaps holder.

    Attributes:
        data (dict): colormaps. Defaults to `rio_tiler.colormap.DEFAULTS_CMAPS`.

    """"""
    data: Dict[str, Union[str, pathlib.Path, ColorMapType]] = attr.ib(default=attr.Factory(lambda: DEFAULT_CMAPS_FILES))

    def get(self, name: str) -> ColorMapType:
        """"""Fetch a colormap.

        Args:
            name (str): colormap name.

        Returns
            dict: colormap dictionary.

        """"""
        cmap = self.data.get(name, None)
        if cmap is None:
            raise InvalidColorMapName(f'Invalid colormap name: {name}')
        if isinstance(cmap, (pathlib.Path, str)):
            if isinstance(cmap, str):
                cmap = pathlib.Path(cmap)
            if cmap.suffix == '.npy':
                colormap = numpy.load(cmap)
                assert colormap.shape == (256, 4)
                assert colormap.dtype == numpy.uint8
                cmap_data = {idx: tuple(value) for idx, value in enumerate(colormap)}
            elif cmap.suffix == '.json':
                with cmap.open() as f:
                    cmap_data = json.load(f, object_hook=lambda x: {int(k): parse_color(v) for k, v in x.items()})
                if isinstance(cmap_data, Sequence):
                    cmap_data = [(tuple(inter), parse_color(v)) for inter, v in cmap_data]
            else:
                raise ValueError(f'Not supported {cmap.suffix} extension for ColorMap')
            self.data[name] = cmap_data
            return cmap_data
        return cmap

    def list(self) -> List[str]:
        """"""List registered Colormaps.

        Returns
            list: list of colormap names.

        """"""
        return list(self.data)

    def register(self, custom_cmap: Dict[str, Union[str, pathlib.Path, ColorMapType]], overwrite: bool=False) -> 'ColorMaps':
        """"""Register a custom colormap.

        Args:
            custom_cmap (dict): custom colormap(s) to register.
            overwrite (bool): Overwrite existing colormap with same key. Defaults to False.

        Examples:
            >>> cmap = cmap.register({""acmap"": {0: (0, 0, 0, 0), ...}})

            >>> cmap = cmap.register({""acmap"": ""acmap.npy""})

        """"""
        for name, _ in custom_cmap.items():
            if not overwrite and name in self.data:
                raise ColorMapAlreadyRegistered(f'{name} is already registered. Use force=True to overwrite.')
        return ColorMaps({**self.data, **custom_cmap})","@attr.s(frozen=True)
class ColorMaps:
    '''Default Colormaps holder.
    Attributes:
        data (dict): colormaps. Defaults to `rio_tiler.colormap.DEFAULTS_CMAPS`.
    '''

    def get(self, name: str) -> ColorMapType:
        '''Fetch a colormap.
        Args:
            name (str): colormap name.
        Returns
            dict: colormap dictionary.
        '''
        pass

    def list(self) -> List[str]:
        '''List registered Colormaps.
        Returns
            list: list of colormap names.
        '''
        pass

    def register(self, custom_cmap: Dict[str, Union[str, pathlib.Path, ColorMapType]], overwrite: bool=False) -> 'ColorMaps':
        '''Register a custom colormap.
        Args:
            custom_cmap (dict): custom colormap(s) to register.
            overwrite (bool): Overwrite existing colormap with same key. Defaults to False.
        Examples:
            >>> cmap = cmap.register({""acmap"": {0: (0, 0, 0, 0), ...}})
            >>> cmap = cmap.register({""acmap"": ""acmap.npy""})
        '''
        pass",snippet_107,"import pathlib
from typing import Dict, List, Tuple, Union

import attr

try:
    from rio_tiler.colormap import DEFAULTS_CMAPS as _DEFAULTS_CMAPS  # type: ignore
except Exception:  # pragma: no cover
    _DEFAULTS_CMAPS = {}

try:
    import numpy as _np  # type: ignore
except Exception:  # pragma: no cover
    _np = None  # numpy is optional for loading .npy colormaps


ColorMapType = Dict[int, Tuple[int, int, int, int]]


def _default_data_factory() -> Dict[str, ColorMapType]:
    return dict(_DEFAULTS_CMAPS)


def _ensure_rgba_dict(arr) -> ColorMapType:
    if _np is None:
        raise RuntimeError(""numpy is required to load colormap from .npy files"")
    a = _np.asarray(arr)
    if a.ndim != 2 or a.shape[0] == 0:
        raise ValueError(""Invalid colormap array"")
    if a.shape[1] not in (3, 4):
        raise ValueError(""Colormap array must have 3 (RGB) or 4 (RGBA) columns"")
    if a.dtype != _np.uint8:
        a = a.astype(_np.uint8)
    if a.shape[1] == 3:
        alpha = _np.full((a.shape[0], 1), 255, dtype=_np.uint8)
        a = _np.concatenate([a, alpha], axis=1)
    return {int(i): (int(r), int(g), int(b), int(aa)) for i, (r, g, b, aa) in enumerate(a)}


def _load_cmap_from_path(path: Union[str, pathlib.Path]) -> ColorMapType:
    p = pathlib.Path(path)
    if not p.exists():
        raise FileNotFoundError(f""Colormap file not found: {p}"")
    if p.suffix.lower() == "".npy"":
        if _np is None:
            raise RuntimeError(""numpy is required to load .npy colormap files"")
        arr = _np.load(str(p))
        return _ensure_rgba_dict(arr)
    raise ValueError(f""Unsupported colormap file type: {p.suffix}"")


@attr.s(frozen=True)
class ColorMaps:
    '''Default Colormaps holder.
    Attributes:
        data (dict): colormaps. Defaults to `rio_tiler.colormap.DEFAULTS_CMAPS`.
    '''
    data: Dict[str, ColorMapType] = attr.ib(factory=_default_data_factory)

    def get(self, name: str) -> ColorMapType:
        '''Fetch a colormap.
        Args:
            name (str): colormap name.
        Returns
            dict: colormap dictionary.
        '''
        try:
            return self.data[name]
        except KeyError as exc:
            raise KeyError(f""Colormap '{name}' not found"") from exc

    def list(self) -> List[str]:
        '''List registered Colormaps.
        Returns
            list: list of colormap names.
        '''
        return sorted(self.data.keys())

    def register(self, custom_cmap: Dict[str, Union[str, pathlib.Path, ColorMapType]], overwrite: bool=False) -> 'ColorMaps':
        '''Register a custom colormap.
        Args:
            custom_cmap (dict): custom colormap(s) to register.
            overwrite (bool): Overwrite existing colormap with same key. Defaults to False.
        Examples:
            >>> cmap = cmap.register({""acmap"": {0: (0, 0, 0, 0), ...}})
            >>> cmap = cmap.register({""acmap"": ""acmap.npy""})
        '''
        if not isinstance(custom_cmap, dict):
            raise TypeError(""custom_cmap must be a dict of name -> colormap"")
        new_data = dict(self.data)
        for name, value in custom_cmap.items():
            if not overwrite and name in new_data:
                raise ValueError(f""Colormap '{name}' already exists. Use overwrite=True to replace."")
            if isinstance(value, (str, pathlib.Path)):
                cmap_dict = _load_cmap_from_path(value)
            elif isinstance(value, dict):
                # Ensure entries are RGBA tuples with 4 components
                cmap_dict = {}
                for k, v in value.items():
                    if not isinstance(k, int):
                        try:
                            k = int(k)  # allow string-int keys
                        except Exception as exc:
                            raise TypeError(""Colormap keys must be integers"") from exc
                    if isinstance(v, tuple):
                        t = v
                    elif isinstance(v, list):
                        t = tuple(v)
                    else:
                        raise TypeError(""Colormap values must be tuples or lists of 3 or 4 integers"")
                    if len(t) == 3:
                        t = (t[0], t[1], t[2], 255)
                    if len(t) != 4:
                        raise ValueError(""Colormap values must have length 3 (RGB) or 4 (RGBA)"")
                    r, g, b, a = t
                    cmap_dict[k] = (int(r), int(g), int(b), int(a))
            else:
                raise TypeError(""Colormap value must be a dict, path string, or pathlib.Path"")
            new_data[name] = cmap_dict
        return attr.evolve(self, data=new_data)"
174678,ipinfo/python,ipinfo_python/ipinfo/details.py,ipinfo.details.Details,"class Details:
    """"""Encapsulates data for single IP address.""""""

    def __init__(self, details):
        """"""Initialize by settings `details` attribute.""""""
        self.details = details

    def __getattr__(self, attr):
        """"""Return attribute if it exists in details array, else return error.""""""
        if attr not in self.details:
            raise AttributeError(f'{attr} is not a valid attribute of Details')
        return self.details[attr]

    @property
    def all(self):
        """"""Return all details as dict.""""""
        return self.details","class Details:
    '''Encapsulates data for single IP address.'''

    def __init__(self, details):
        '''Initialize by settings `details` attribute.'''
        pass

    def __getattr__(self, attr):
        '''Return attribute if it exists in details array, else return error.'''
        pass
    @property
    def all(self):
        '''Return all details as dict.'''
        pass",snippet_108,"class Details:
    '''Encapsulates data for single IP address.'''

    def __init__(self, details):
        '''Initialize by settings `details` attribute.'''
        self.details = dict(details) if details is not None else {}

    def __getattr__(self, attr):
        '''Return attribute if it exists in details array, else return error.'''
        try:
            return self.details[attr]
        except KeyError:
            raise AttributeError(f""{type(self).__name__} object has no attribute {attr!r}"")

    @property
    def all(self):
        '''Return all details as dict.'''
        return self.details.copy()"
176091,opencobra/cobrapy,opencobra_cobrapy/src/cobra/util/context.py,cobra.util.context.HistoryManager,"from typing import TYPE_CHECKING, Any, Callable, Optional

class HistoryManager:
    """"""
    Define a base context manager.

    It records a list of actions to be taken at a later time.
    This is used to implement context managers that allow temporary
    changes to a `cobra.core.Model`.

    """"""

    def __init__(self, **kwargs) -> None:
        """"""Initialize the class.""""""
        super().__init__(**kwargs)
        self._history = []

    def __call__(self, operation: Callable[[Any], Any]) -> None:
        """"""Add the corresponding operation to the history stack.

        Parameters
        ----------
        operation : callable
            A function to be called at a later time.

        """"""
        self._history.append(operation)

    def reset(self) -> None:
        """"""Trigger executions for all items in the stack in reverse order.""""""
        while self._history:
            entry = self._history.pop()
            entry()

    def size(self) -> int:
        """"""Calculate number of operations on the stack.""""""
        return len(self._history)","
class HistoryManager:
    '''
    Define a base context manager.
    It records a list of actions to be taken at a later time.
    This is used to implement context managers that allow temporary
    changes to a `cobra.core.Model`.
    '''

    def __init__(self, **kwargs) -> None:
        '''Initialize the class.'''
        pass

    def __call__(self, operation: Callable[[Any], Any]) -> None:
        '''Add the corresponding operation to the history stack.
        Parameters
        ----------
        operation : callable
            A function to be called at a later time.
        '''
        pass

    def reset(self) -> None:
        '''Trigger executions for all items in the stack in reverse order.'''
        pass

    def size(self) -> int:
        '''Calculate number of operations on the stack.'''
        pass",snippet_109,"from typing import Any, Callable, List

class HistoryManager:
    '''
    Define a base context manager.
    It records a list of actions to be taken at a later time.
    This is used to implement context managers that allow temporary
    changes to a `cobra.core.Model`.
    '''

    def __init__(self, **kwargs) -> None:
        '''Initialize the class.'''
        self._history: List[Callable[[], Any]] = []

    def __call__(self, operation: Callable[[Any], Any]) -> None:
        '''Add the corresponding operation to the history stack.
        Parameters
        ----------
        operation : callable
            A function to be called at a later time.
        '''
        if not callable(operation):
            raise TypeError(""operation must be callable"")
        # Store as-is; operations are expected to be closures needing no args
        self._history.append(operation)

    def reset(self) -> None:
        '''Trigger executions for all items in the stack in reverse order.'''
        errors = []
        try:
            for op in reversed(self._history):
                try:
                    op()
                except Exception as exc:
                    errors.append(exc)
        finally:
            self._history.clear()
        if errors:
            if len(errors) == 1:
                raise errors[0]
            raise RuntimeError(f""{len(errors)} errors occurred during reset"") from errors[0]

    def size(self) -> int:
        '''Calculate number of operations on the stack.'''
        return len(self._history)"
176713,adafruit/Adafruit_Blinka,adafruit_Adafruit_Blinka/src/adafruit_blinka/__init__.py,adafruit_blinka.Enum,"class Enum:
    """"""
    Object supporting CircuitPython-style of static symbols
    as seen with Direction.OUTPUT, Pull.UP
    """"""

    def __repr__(self):
        """"""
        Assumes instance will be found as attribute of own class.
        Returns dot-subscripted path to instance
        (assuming absolute import of containing package)
        """"""
        cls = type(self)
        for key in dir(cls):
            if getattr(cls, key) is self:
                return '{}.{}.{}'.format(cls.__module__, cls.__qualname__, key)
        return repr(self)

    @classmethod
    def iteritems(cls):
        """"""
        Inspects attributes of the class for instances of the class
        and returns as key,value pairs mirroring dict#iteritems
        """"""
        for key in dir(cls):
            val = getattr(cls, key)
            if isinstance(cls, val):
                yield (key, val)","class Enum:
    '''
    Object supporting CircuitPython-style of static symbols
    as seen with Direction.OUTPUT, Pull.UP
    '''

    def __repr__(self):
        '''
        Assumes instance will be found as attribute of own class.
        Returns dot-subscripted path to instance
        (assuming absolute import of containing package)
        '''
        pass
    @classmethod
    def iteritems(cls):
        '''
        Inspects attributes of the class for instances of the class
        and returns as key,value pairs mirroring dict#iteritems
        '''
        pass",snippet_110,"class Enum:
    '''
    Object supporting CircuitPython-style of static symbols
    as seen with Direction.OUTPUT, Pull.UP
    '''

    def __repr__(self):
        '''
        Assumes instance will be found as attribute of own class.
        Returns dot-subscripted path to instance
        (assuming absolute import of containing package)
        '''
        cls = self.__class__
        for name in dir(cls):
            try:
                if getattr(cls, name) is self:
                    return f""{cls.__module__}.{cls.__qualname__}.{name}""
            except Exception:
                continue
        return object.__repr__(self)

    @classmethod
    def iteritems(cls):
        '''
        Inspects attributes of the class for instances of the class
        and returns as key,value pairs mirroring dict#iteritems
        '''
        for name in dir(cls):
            if name.startswith('_'):
                continue
            try:
                val = getattr(cls, name)
            except Exception:
                continue
            if isinstance(val, cls):
                yield name, val"
176756,adafruit/Adafruit_Blinka,adafruit_Adafruit_Blinka/src/adafruit_blinka/microcontroller/nova/__init__.py,adafruit_blinka.microcontroller.nova.Connection,"class Connection:
    """"""Connection class""""""
    __instance = None

    @staticmethod
    def getInstance():
        """"""Static access method.""""""
        if Connection.__instance is None:
            Connection()
        return Connection.__instance

    def __init__(self):
        """"""Virtually private constructor.""""""
        if Connection.__instance is not None:
            raise Exception('This class is a singleton!')
        from binhoHostAdapter import binhoHostAdapter
        from binhoHostAdapter import binhoUtilities
        devices = binhoUtilities.listAvailableDevices()
        if len(devices) > 0:
            Connection.__instance = binhoHostAdapter.binhoHostAdapter(devices[0])
        else:
            raise RuntimeError('No Binho Nova found!')","class Connection:
    '''Connection class'''
    @staticmethod
    def getInstance():
        '''Static access method.'''
        pass

    def __init__(self):
        '''Virtually private constructor.'''
        pass",snippet_111,"class Connection:
    '''Connection class'''
    __instance = None

    @staticmethod
    def getInstance():
        '''Static access method.'''
        if Connection.__instance is None:
            Connection()
        return Connection.__instance

    def __init__(self):
        '''Virtually private constructor.'''
        if Connection.__instance is not None:
            raise Exception(""This class is a singleton!"")
        Connection.__instance = self"
177578,ottogroup/palladium,ottogroup_palladium/palladium/persistence.py,palladium.persistence.FileLikeIO,"from abc import abstractmethod

class FileLikeIO:
    """"""Used by :class:`FileLike` to access low level file handle
    operations.
    """"""

    @abstractmethod
    def open(self, path, mode='r'):
        """"""Return a file handle

        For normal files, the implementation is:

        ```python
        return open(path, mode)
        ```
        """"""

    @abstractmethod
    def exists(self, path):
        """"""Test whether a path exists

        For normal files, the implementation is:

        ```python
        return os.path.exists(path)
        ```
        """"""

    @abstractmethod
    def remove(self, path):
        """"""Remove a file

        For normal files, the implementation is:

        ```python
        os.remove(path)
        ```
        """"""","
class FileLikeIO:
    '''Used by :class:`FileLike` to access low level file handle
    operations.
    '''
    @abstractmethod
    def open(self, path, mode='r'):
        '''Return a file handle
        For normal files, the implementation is:
        ```python
        return open(path, mode)
        ```
        '''
        pass
    @abstractmethod
    def exists(self, path):
        '''Test whether a path exists
        For normal files, the implementation is:
        ```python
        return os.path.exists(path)
        ```
        '''
        pass
    @abstractmethod
    def remove(self, path):
        '''Remove a file
        For normal files, the implementation is:
        ```python
        os.remove(path)
        ```
        '''
        pass",snippet_113,"import os
import builtins

class FileLikeIO:
    '''Used by :class:`FileLike` to access low level file handle
    operations.
    '''
    def open(self, path, mode='r'):
        '''Return a file handle
        For normal files, the implementation is:
        ```python
        return open(path, mode)
        ```
        '''
        return builtins.open(path, mode)

    def exists(self, path):
        '''Test whether a path exists
        For normal files, the implementation is:
        ```python
        return os.path.exists(path)
        ```
        '''
        return os.path.exists(path)

    def remove(self, path):
        '''Remove a file
        For normal files, the implementation is:
        ```python
        os.remove(path)
        ```
        '''
        os.remove(path)"
179419,python-constraint/python-constraint,python-constraint_python-constraint/constraint/domain.py,constraint.domain.Variable,"class Variable:
    """"""Helper class for variable definition.

    Using this class is optional, since any hashable object,
    including plain strings and integers, may be used as variables.
    """"""

    def __init__(self, name):
        """"""Initialization method.

        Args:
            name (string): Generic variable name for problem-specific
                purposes
        """"""
        self.name = name

    def __repr__(self):
        """"""Represents itself with the name attribute.""""""
        return self.name","class Variable:
    '''Helper class for variable definition.
    Using this class is optional, since any hashable object,
    including plain strings and integers, may be used as variables.
    '''

    def __init__(self, name):
        '''Initialization method.
        Args:
            name (string): Generic variable name for problem-specific
                purposes
        '''
        pass

    def __repr__(self):
        '''Represents itself with the name attribute.'''
        pass",snippet_115,"class Variable:
    '''Helper class for variable definition.
    Using this class is optional, since any hashable object,
    including plain strings and integers, may be used as variables.
    '''

    def __init__(self, name):
        '''Initialization method.
        Args:
            name (string): Generic variable name for problem-specific
                purposes
        '''
        self.name = name

    def __repr__(self):
        '''Represents itself with the name attribute.'''
        return str(self.name)"
179425,python-constraint/python-constraint,python-constraint_python-constraint/constraint/solvers.py,constraint.solvers.Solver,"class Solver:
    """"""Abstract base class for solvers.""""""
    requires_pickling = False

    def getSolution(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Return one solution for the given problem.

        Args:
            domains (dict): Dictionary mapping variables to their domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        """"""
        msg = f'{self.__class__.__name__} is an abstract class'
        raise NotImplementedError(msg)

    def getSolutions(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Return all solutions for the given problem.

        Args:
            domains (dict): Dictionary mapping variables to domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        """"""
        msg = f'{self.__class__.__name__} provides only a single solution'
        raise NotImplementedError(msg)

    def getSolutionIter(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Return an iterator for the solutions of the given problem.

        Args:
            domains (dict): Dictionary mapping variables to domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        """"""
        msg = f""{self.__class__.__name__} doesn't provide iteration""
        raise NotImplementedError(msg)","class Solver:
    '''Abstract base class for solvers.'''

    def getSolution(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        '''Return one solution for the given problem.
        Args:
            domains (dict): Dictionary mapping variables to their domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        '''
        pass

    def getSolutions(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        '''Return all solutions for the given problem.
        Args:
            domains (dict): Dictionary mapping variables to domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        '''
        pass

    def getSolutionIter(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        '''Return an iterator for the solutions of the given problem.
        Args:
            domains (dict): Dictionary mapping variables to domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        '''
        pass",snippet_116,"class Solver:
    '''Abstract base class for solvers.'''

    def getSolution(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        '''Return one solution for the given problem.
        Args:
            domains (dict): Dictionary mapping variables to their domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        '''
        raise NotImplementedError(""getSolution must be implemented by subclasses."")

    def getSolutions(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        '''Return all solutions for the given problem.
        Args:
            domains (dict): Dictionary mapping variables to domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        '''
        raise NotImplementedError(""getSolutions must be implemented by subclasses."")

    def getSolutionIter(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        '''Return an iterator for the solutions of the given problem.
        Args:
            domains (dict): Dictionary mapping variables to domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        '''
        raise NotImplementedError(""getSolutionIter must be implemented by subclasses."")"
179445,maroba/findiff,findiff/pde.py,findiff.pde.PDE,"import numpy as np
from scipy.sparse.linalg import spsolve
import scipy.sparse as sparse

class PDE:
    """"""
    Representation of a partial differential equation.
    """"""

    def __init__(self, lhs, rhs, bcs):
        """"""
        Initializes the PDE.

        You need to specify the left hand side (lhs) in terms of derivatives
        as well as the right hand side in terms of an array.

        Parameters
        ----------
        lhs: FinDiff object or combination of FinDiff objects
            the left hand side of the PDE
        rhs: numpy.ndarray
            the right hand side of the PDE
        bcs: BoundaryConditions
            the boundary conditions for the PDE

        """"""
        self.lhs = lhs
        self.rhs = rhs
        self.bcs = bcs
        self._L = None

    def solve(self):
        """"""
        Solves the PDE.

        Returns
        -------
        out: numpy.ndarray
            Array with the solution of the PDE.
        """"""
        shape = self.bcs.shape
        if self._L is None:
            self._L = self.lhs.matrix(shape)
        L = sparse.lil_matrix(self._L)
        f = self.rhs.reshape(-1, 1)
        nz = list(self.bcs.row_inds())
        L[nz, :] = self.bcs.lhs[nz, :]
        f[nz] = np.array(self.bcs.rhs[nz].toarray()).reshape(-1, 1)
        L = sparse.csr_matrix(L)
        return spsolve(L, f).reshape(shape)","
class PDE:
    '''
    Representation of a partial differential equation.
    '''

    def __init__(self, lhs, rhs, bcs):
        '''
        Initializes the PDE.
        You need to specify the left hand side (lhs) in terms of derivatives
        as well as the right hand side in terms of an array.
        Parameters
        ----------
        lhs: FinDiff object or combination of FinDiff objects
            the left hand side of the PDE
        rhs: numpy.ndarray
            the right hand side of the PDE
        bcs: BoundaryConditions
            the boundary conditions for the PDE
        '''
        pass

    def solve(self):
        '''
        Solves the PDE.
        Returns
        -------
        out: numpy.ndarray
            Array with the solution of the PDE.
        '''
        pass",snippet_117,"import numpy as np

try:
    from scipy.sparse.linalg import spsolve
    from scipy.sparse import issparse
except Exception:  # pragma: no cover
    spsolve = None
    issparse = lambda x: False


class PDE:
    '''
    Representation of a partial differential equation.
    '''

    def __init__(self, lhs, rhs, bcs):
        '''
        Initializes the PDE.
        You need to specify the left hand side (lhs) in terms of derivatives
        as well as the right hand side in terms of an array.
        Parameters
        ----------
        lhs: FinDiff object or combination of FinDiff objects
            the left hand side of the PDE
        rhs: numpy.ndarray
            the right hand side of the PDE
        bcs: BoundaryConditions
            the boundary conditions for the PDE
        '''
        if rhs is None:
            raise ValueError(""rhs must be a numpy.ndarray"")
        self.lhs = lhs
        self.rhs = np.asarray(rhs)
        self.bcs = bcs

    def _lhs_matrix(self, shape):
        lhs = self.lhs
        # Try common signatures used by FinDiff-like operators
        for attempt in (
            lambda: lhs.matrix(shape),
            lambda: lhs.matrix(shape=shape),
            lambda: lhs.tosparse(shape),  # some implementations
            lambda: lhs.tosparse(shape=shape),
            lambda: lhs.to_matrix(shape),
            lambda: lhs.to_matrix(shape=shape),
        ):
            try:
                A = attempt()
                if A is not None:
                    return A
            except TypeError:
                continue
            except AttributeError:
                continue
        # Fallback: try callable application to an identity basis (inefficient)
        # Only as a last resort if lhs is callable and shape is small.
        if callable(lhs):
            n = int(np.prod(shape))
            if n <= 10_000:
                eye = np.eye(n)
                try:
                    cols = []
                    for j in range(n):
                        e = eye[:, j].reshape(shape)
                        col = lhs(e).ravel()
                        cols.append(col)
                    A = np.column_stack(cols)
                    return A
                except Exception:
                    pass
        raise TypeError(""Could not build matrix from lhs; provide an operator with a 'matrix' method."")

    def _apply_bcs(self, A, b):
        if self.bcs is None:
            return A, b
        bcs = self.bcs
        # Try multiple conventions
        for attempt in (
            lambda: bcs.apply_to(A, b),
            lambda: bcs.apply(A, b),
            lambda: bcs(A, b),
        ):
            try:
                out = attempt()
                if isinstance(out, tuple) and len(out) == 2:
                    A2, b2 = out
                    return A2, b2
            except AttributeError:
                continue
            except TypeError:
                continue
        # Some APIs mutate in-place and return None
        for attempt in (
            lambda: bcs.apply_to(A, b),
            lambda: bcs.apply(A, b),
            lambda: bcs(A, b),
        ):
            try:
                res = attempt()
                if res is None:
                    return A, b
            except Exception:
                continue
        return A, b

    def solve(self):
        '''
        Solves the PDE.
        Returns
        -------
        out: numpy.ndarray
            Array with the solution of the PDE.
        '''
        shape = self.rhs.shape
        A = self._lhs_matrix(shape)
        b = np.asarray(self.rhs, dtype=float).ravel()

        A, b = self._apply_bcs(A, b)

        # Solve
        if spsolve is not None and issparse(A):
            x = spsolve(A, b)
        else:
            # Convert to ndarray if it's a sparse-like object without spsolve
            try:
                A_arr = A.toarray()
            except Exception:
                A_arr = np.asarray(A)
            x = np.linalg.solve(A_arr, b)

        return x.reshape(shape)"
186773,sphinx-gallery/sphinx-gallery,sphinx_gallery/_dummy/__init__.py,sphinx_gallery._dummy.DummyClass,"class DummyClass:
    """"""Dummy class for testing method resolution.""""""

    def run(self):
        """"""Do nothing.""""""
        pass

    @property
    def prop(self):
        """"""Property.""""""
        return 'Property'","class DummyClass:
    '''Dummy class for testing method resolution.'''

    def run(self):
        '''Do nothing.'''
        pass
    @property
    def prop(self):
        '''Property.'''
        pass",snippet_119,"class DummyClass:
    '''Dummy class for testing method resolution.'''

    def __init__(self, prop=None):
        self._prop = prop

    def run(self):
        '''Do nothing.'''
        return None

    @property
    def prop(self):
        '''Property.'''
        return self._prop

    @prop.setter
    def prop(self, value):
        self._prop = value"
186774,sphinx-gallery/sphinx-gallery,sphinx_gallery/_dummy/nested.py,sphinx_gallery._dummy.nested.NestedDummyClass,"class NestedDummyClass:
    """"""Nested dummy class for testing method resolution.""""""

    def run(self):
        """"""Do nothing.""""""
        pass

    @property
    def prop(self):
        """"""Property.""""""
        return 'Property'","class NestedDummyClass:
    '''Nested dummy class for testing method resolution.'''

    def run(self):
        '''Do nothing.'''
        pass
    @property
    def prop(self):
        '''Property.'''
        pass",snippet_120,"class NestedDummyClass:
    '''Nested dummy class for testing method resolution.'''

    def __init__(self, value=None):
        self._prop = value

    def run(self):
        '''Do nothing.'''
        return None

    @property
    def prop(self):
        '''Property.'''
        return self._prop"
190420,JamesPHoughton/pysd,JamesPHoughton_pysd/pysd/py_backend/external.py,pysd.py_backend.external.Excels,"import numpy as np
from openpyxl import load_workbook
import pandas as pd

class Excels:
    """"""
    Class to save the read Excel files and thus avoid double reading
    """"""
    _Excels, _Excels_opyxl = ({}, {})

    @classmethod
    def read(cls, file_name, tab):
        """"""
        Read the Excel file or return the previously read one
        """"""
        if file_name.joinpath(tab) in cls._Excels:
            return cls._Excels[file_name.joinpath(tab)]
        else:
            read_kwargs = {}
            ext = file_name.suffix.lower()
            if ext in _SPREADSHEET_EXTS:
                read_func = pd.read_excel
                read_kwargs['sheet_name'] = tab
            elif ext == '.csv':
                read_func = pd.read_csv
                if tab and (not tab[0].isalnum()):
                    read_kwargs['sep'] = tab
            else:
                read_func = pd.read_table
                if tab and (not tab[0].isalnum()):
                    read_kwargs['sep'] = tab
            excel = np.array([pd.to_numeric(ex, errors='coerce') for ex in read_func(file_name, header=None, **read_kwargs).values])
            cls._Excels[file_name.joinpath(tab)] = excel
            return excel

    @classmethod
    def read_opyxl(cls, file_name):
        """"""
        Read the Excel file using OpenPyXL or return the previously read one
        """"""
        if file_name in cls._Excels_opyxl:
            return cls._Excels_opyxl[file_name]
        else:
            excel = load_workbook(file_name, read_only=True, data_only=True)
            cls._Excels_opyxl[file_name] = excel
            return excel

    @classmethod
    def clean(cls):
        """"""
        Clean the dictionary of read files
        """"""
        for file in cls._Excels_opyxl.values():
            file.close()
        cls._Excels, cls._Excels_opyxl = ({}, {})","
class Excels:
    '''
    Class to save the read Excel files and thus avoid double reading
    '''
    @classmethod
    def read(cls, file_name, tab):
        '''
        Read the Excel file or return the previously read one
        '''
        pass
    @classmethod
    def read_opyxl(cls, file_name):
        '''
        Read the Excel file using OpenPyXL or return the previously read one
        '''
        pass
    @classmethod
    def clean(cls):
        '''
        Clean the dictionary of read files
        '''
        pass",snippet_121,"import os
from pathlib import Path
from threading import RLock

try:
    import pandas as pd
except Exception:  # pragma: no cover
    pd = None

try:
    from openpyxl import load_workbook
except Exception:  # pragma: no cover
    load_workbook = None


class Excels:
    '''
    Class to save the read Excel files and thus avoid double reading
    '''
    _pd_cache = {}        # {abs_path: {sheet_name: DataFrame}}
    _opyxl_cache = {}     # {abs_path: Workbook}
    _lock = RLock()

    @classmethod
    def _normalize_path(cls, file_name):
        return str(Path(file_name).expanduser().resolve())

    @classmethod
    def read(cls, file_name, tab):
        '''
        Read the Excel file or return the previously read one
        '''
        if pd is None:
            raise RuntimeError(""pandas is required for Excels.read but is not available"")
        path = cls._normalize_path(file_name)
        with cls._lock:
            sheet_cache = cls._pd_cache.setdefault(path, {})
            if tab in sheet_cache:
                return sheet_cache[tab]
        df = pd.read_excel(path, sheet_name=tab)
        with cls._lock:
            # In case another thread populated it while reading
            sheet_cache = cls._pd_cache.setdefault(path, {})
            sheet_cache[tab] = df
            return df

    @classmethod
    def read_opyxl(cls, file_name):
        '''
        Read the Excel file using OpenPyXL or return the previously read one
        '''
        if load_workbook is None:
            raise RuntimeError(""openpyxl is required for Excels.read_opyxl but is not available"")
        path = cls._normalize_path(file_name)
        with cls._lock:
            if path in cls._opyxl_cache:
                return cls._opyxl_cache[path]
        wb = load_workbook(path, data_only=True)
        with cls._lock:
            # In case another thread populated it while reading
            if path not in cls._opyxl_cache:
                cls._opyxl_cache[path] = wb
            return cls._opyxl_cache[path]

    @classmethod
    def clean(cls):
        '''
        Clean the dictionary of read files
        '''
        with cls._lock:
            cls._pd_cache.clear()
            cls._opyxl_cache.clear()"
222023,brettcannon/gidgethub,brettcannon_gidgethub/gidgethub/sansio.py,gidgethub.sansio.RateLimit,"from typing import Any, Dict, Mapping, Optional, Tuple, Type, Union
import datetime

class RateLimit:
    """"""The rate limit imposed upon the requester.

    The 'limit' attribute specifies the rate of requests per hour the client is
    limited to.

    The 'remaining' attribute specifies how many requests remain within the
    current rate limit that the client can make.

    The reset_datetime attribute is a datetime object representing when
    effectively 'left' resets to 'rate'. The datetime object is timezone-aware
    and set to UTC.

    The boolean value of an instance whether another request can be made. This
    is determined based on whether there are any remaining requests or if the
    reset datetime has passed.
    """"""

    def __init__(self, *, limit: int, remaining: int, reset_epoch: float) -> None:
        """"""Instantiate a RateLimit object.

        The reset_epoch argument should be in seconds since the UTC epoch.
        """"""
        self.limit = limit
        self.remaining = remaining
        self.reset_datetime = datetime.datetime.fromtimestamp(reset_epoch, datetime.timezone.utc)

    def __bool__(self) -> bool:
        """"""True if requests are remaining or the reset datetime has passed.""""""
        if self.remaining > 0:
            return True
        else:
            now = datetime.datetime.now(datetime.timezone.utc)
            return now > self.reset_datetime

    def __str__(self) -> str:
        """"""Provide all details in a reasonable format.""""""
        return f'< {self.remaining:,}/{self.limit:,} until {self.reset_datetime} >'

    @classmethod
    def from_http(cls, headers: Mapping[str, str]) -> Optional['RateLimit']:
        """"""Gather rate limit information from HTTP headers.

        The mapping providing the headers is expected to support lowercase
        keys.  Returns ``None`` if ratelimit info is not found in the headers.
        """"""
        try:
            limit = int(headers['x-ratelimit-limit'])
            remaining = int(headers['x-ratelimit-remaining'])
            reset_epoch = float(headers['x-ratelimit-reset'])
        except KeyError:
            return None
        else:
            return cls(limit=limit, remaining=remaining, reset_epoch=reset_epoch)",,snippet_122,"I don’t see the class skeleton. Please provide it and any requirements. Helpful details:
- Python version and allowed dependencies
- Class purpose, public API (methods/properties), and expected behavior
- Initialization parameters and defaults
- Edge cases and error handling
- Performance constraints
- Example inputs/outputs or tests"
222265,box/flaky,box_flaky/flaky/multiprocess_string_io.py,flaky.multiprocess_string_io.MultiprocessingStringIO,"import multiprocessing

class MultiprocessingStringIO:
    """"""
    Provide a StringIO-like interface to the multiprocessing ListProxy. The
    multiprocessing ListProxy needs to be instantiated before the flaky plugin
    is configured, so the list is created as a class variable.
    """"""
    _manager = multiprocessing.Manager()
    proxy = _manager.list()

    def getvalue(self):
        """"""
        Shadow the StringIO.getvalue method.
        """"""
        return ''.join((i for i in self.proxy))

    def writelines(self, content_list):
        """"""
        Shadow the StringIO.writelines method. Ingests a list and
        translates that to a string
        """"""
        for item in content_list:
            self.write(item)

    def write(self, content):
        """"""
        Shadow the StringIO.write method.
        """"""
        content.strip('\n')
        self.proxy.append(content)","
class MultiprocessingStringIO:
    '''
    Provide a StringIO-like interface to the multiprocessing ListProxy. The
    multiprocessing ListProxy needs to be instantiated before the flaky plugin
    is configured, so the list is created as a class variable.
        '''

    def getvalue(self):
        '''
        Shadow the StringIO.getvalue method.
        '''
        pass

    def writelines(self, content_list):
        '''
        Shadow the StringIO.writelines method. Ingests a list and
        translates that to a string
        '''
        pass

    def writelines(self, content_list):
        '''
        Shadow the StringIO.write method.
        '''
        pass",snippet_123,"from multiprocessing import Manager

class MultiprocessingStringIO:
    '''
    Provide a StringIO-like interface to the multiprocessing ListProxy. The
    multiprocessing ListProxy needs to be instantiated before the flaky plugin
    is configured, so the list is created as a class variable.
    '''
    _manager = Manager()
    _buffer = _manager.list()

    def _to_str(self, obj):
        if isinstance(obj, bytes):
            try:
                return obj.decode()
            except Exception:
                return obj.decode(errors='replace')
        return str(obj)

    def getvalue(self):
        '''
        Shadow the StringIO.getvalue method.
        '''
        return ''.join(self._buffer)

    def writelines(self, content_list):
        '''
        Shadow the StringIO.writelines method. Ingests a list and
        translates that to a string
        '''
        for item in content_list:
            self._buffer.append(self._to_str(item))

    def write(self, content):
        '''
        Shadow the StringIO.write method.
        '''
        s = self._to_str(content)
        self._buffer.append(s)
        return len(s)"
222348,joshspeagle/dynesty,joshspeagle_dynesty/py/dynesty/utils.py,dynesty.utils.DelayTimer,"import time

class DelayTimer:
    """""" Utility class that allows us to detect a certain
    time has passed""""""

    def __init__(self, delay):
        """""" Initialise the time with delay of dt seconds

        Parameters
        ----------

        delay: float
            The number of seconds in the timer
        """"""
        self.delay = delay
        self.last_time = time.time()

    def is_time(self):
        """"""
        Returns true if more than self.dt seconds has passed
        since the initialization or last call of successful is_time()

        Returns
        -------
        ret: bool
             True if specified amout of time has passed since the
             initialization or last successful is_time() call
        """"""
        curt = time.time()
        if curt - self.last_time > self.delay:
            self.last_time = curt
            return True
        return False","
class DelayTimer:
    ''' Utility class that allows us to detect a certain
        time has passed'''

    def __init__(self, delay):
        ''' Initialise the time with delay of dt seconds
        Parameters
        ----------
        delay: float
            The number of seconds in the timer
        '''
        pass

    def is_time(self):
        '''
        Returns true if more than self.dt seconds has passed
        since the initialization or last call of successful is_time()
        Returns
        -------
        ret: bool
             True if specified amout of time has passed since the
             initialization or last successful is_time() call
        '''
        pass",snippet_124,"import time

class DelayTimer:
    ''' Utility class that allows us to detect a certain
        time has passed'''

    def __init__(self, delay):
        ''' Initialise the time with delay of dt seconds
        Parameters
        ----------
        delay: float
            The number of seconds in the timer
        '''
        self.dt = float(delay)
        self._last = time.monotonic()

    def is_time(self):
        '''
        Returns true if more than self.dt seconds has passed
        since the initialization or last call of successful is_time()
        Returns
        -------
        ret: bool
             True if specified amout of time has passed since the
             initialization or last successful is_time() call
        '''
        now = time.monotonic()
        if now - self._last >= self.dt:
            self._last = now
            return True
        return False"
223977,jilljenn/tryalgo,jilljenn_tryalgo/tryalgo/fenwick.py,tryalgo.fenwick.FenwickMin,"class FenwickMin:
    """"""maintains a tree to allow quick updates and queries
    of a virtual table t
    """"""

    def __init__(self, size):
        """"""stores a table t and allows updates and queries
        of prefix sums in logarithmic time.

        :param size: length of the table
        """"""
        self.s = [float('+inf')] * (size + 1)

    def prefixMin(self, a):
        """"""
        :param int a: index in t, negative a will return infinity
        :returns: min(t[0], ... ,t[a])
        """"""
        i = a + 1
        retval = float('+inf')
        while i > 0:
            retval = min(retval, self.s[i])
            i -= i & -i
        return retval

    def update(self, a, val):
        """"""
        :param int a: index in t
        :param val: a value
        :modifies: sets t[a] to the minimum of t[a] and val
        """"""
        i = a + 1
        while i < len(self.s):
            self.s[i] = min(self.s[i], val)
            i += i & -i","class FenwickMin:
    '''maintains a tree to allow quick updates and queries
    of a virtual table t
    '''

    def __init__(self, size):
        '''stores a table t and allows updates and queries
        of prefix sums in logarithmic time.
        :param size: length of the table
        '''
        pass

    def prefixMin(self, a):
        '''
        :param int a: index in t, negative a will return infinity
        :returns: min(t[0], ... ,t[a])
        '''
        pass

    def update(self, a, val):
        '''
        :param int a: index in t
        :param val: a value
        :modifies: sets t[a] to the minimum of t[a] and val
        '''
        pass",snippet_125,"class FenwickMin:
    '''maintains a tree to allow quick updates and queries
    of a virtual table t
    '''

    def __init__(self, size):
        '''stores a table t and allows updates and queries
        of prefix sums in logarithmic time.
        :param size: length of the table
        '''
        self.n = max(0, int(size))
        self.tree = [float('inf')] * (self.n + 1)  # 1-based fenwick tree
        self.arr = [float('inf')] * self.n        # store current point values

    def prefixMin(self, a):
        '''
        :param int a: index in t, negative a will return infinity
        :returns: min(t[0], ... ,t[a])
        '''
        if a < 0:
            return float('inf')
        if self.n == 0:
            return float('inf')
        if a >= self.n:
            a = self.n - 1
        i = a + 1
        res = float('inf')
        while i > 0:
            if self.tree[i] < res:
                res = self.tree[i]
            i -= i & -i
        return res

    def update(self, a, val):
        '''
        :param int a: index in t
        :param val: a value
        :modifies: sets t[a] to the minimum of t[a] and val
        '''
        if a < 0 or a >= self.n:
            return
        new_val = val if val < self.arr[a] else self.arr[a]
        if new_val == self.arr[a]:
            return
        self.arr[a] = new_val
        i = a + 1
        while i <= self.n:
            if new_val < self.tree[i]:
                self.tree[i] = new_val
            i += i & -i"
223981,jilljenn/tryalgo,jilljenn_tryalgo/tryalgo/lowest_common_ancestor.py,tryalgo.lowest_common_ancestor.LowestCommonAncestorRMQ,"from tryalgo.range_minimum_query import RangeMinQuery

class LowestCommonAncestorRMQ:
    """"""Lowest common ancestor data structure using a reduction to
       range minimum query
    """"""

    def __init__(self, graph):
        """"""builds the structure from a given tree

        :param graph: adjacency matrix of a tree
        :complexity: O(n log n), with n = len(graph)
        """"""
        n = len(graph)
        dfs_trace = []
        self.last = [None] * n
        to_visit = [(0, 0, None)]
        succ = [0] * n
        while to_visit:
            level, node, father = to_visit[-1]
            self.last[node] = len(dfs_trace)
            dfs_trace.append((level, node))
            if succ[node] < len(graph[node]) and graph[node][succ[node]] == father:
                succ[node] += 1
            if succ[node] == len(graph[node]):
                to_visit.pop()
            else:
                neighbor = graph[node][succ[node]]
                succ[node] += 1
                to_visit.append((level + 1, neighbor, node))
        self.rmq = RangeMinQuery(dfs_trace, (float('inf'), None))

    def query(self, u, v):
        """""":returns: the lowest common ancestor of u and v
        :complexity: O(log n)
        """"""
        lu = self.last[u]
        lv = self.last[v]
        if lu > lv:
            lu, lv = (lv, lu)
        return self.rmq.range_min(lu, lv + 1)[1]","
class LowestCommonAncestorRMQ:
    '''Lowest common ancestor data structure using a reduction to
       range minimum query
    '''

    def __init__(self, graph):
        '''builds the structure from a given tree
        :param graph: adjacency matrix of a tree
        :complexity: O(n log n), with n = len(graph)
        '''
        pass

    def query(self, u, v):
        ''':returns: the lowest common ancestor of u and v
        :complexity: O(log n)
        '''
        pass",snippet_126,"class LowestCommonAncestorRMQ:
    '''Lowest common ancestor data structure using a reduction to
       range minimum query
    '''

    def __init__(self, graph):
        '''builds the structure from a given tree
        :param graph: adjacency matrix of a tree
        :complexity: O(n log n), with n = len(graph)
        '''
        n = len(graph)
        self.n = n
        # Build adjacency list from adjacency matrix
        adj = [[] for _ in range(n)]
        for i in range(n):
            row = graph[i]
            for j in range(n):
                if row[j]:
                    adj[i].append(j)
        self._adj = adj

        # Euler tour and depth arrays
        euler = []
        depth = []
        first = [-1] * n

        def dfs(u, p, d):
            first[u] = len(euler)
            euler.append(u)
            depth.append(d)
            for v in adj[u]:
                if v == p:
                    continue
                dfs(v, u, d + 1)
                euler.append(u)
                depth.append(d)

        if n > 0:
            dfs(0, -1, 0)

        self.euler = euler
        self.depth = depth
        self.first = first

        m = len(depth)
        if m == 0:
            # Empty tree edge case
            self.log = [0]
            self.st = [[0]]
            return

        # Precompute logs
        log = [0] * (m + 1)
        for i in range(2, m + 1):
            log[i] = log[i // 2] + 1
        self.log = log

        # Build sparse table storing indices into depth
        K = log[m] + 1
        st = [[0] * m for _ in range(K)]
        for i in range(m):
            st[0][i] = i
        k = 1
        while (1 << k) <= m:
            span = 1 << k
            half = span >> 1
            prev = st[k - 1]
            curr = st[k]
            for i in range(0, m - span + 1):
                a = prev[i]
                b = prev[i + half]
                curr[i] = a if depth[a] <= depth[b] else b
            k += 1
        self.st = st

    def query(self, u, v):
        ''':returns: the lowest common ancestor of u and v
        :complexity: O(log n)
        '''
        if self.n == 0:
            return None
        l = self.first[u]
        r = self.first[v]
        if l > r:
            l, r = r, l
        k = self.log[r - l + 1]
        a = self.st[k][l]
        b = self.st[k][r - (1 << k) + 1]
        idx = a if self.depth[a] <= self.depth[b] else b
        return self.euler[idx]"
223982,jilljenn/tryalgo,jilljenn_tryalgo/tryalgo/lowest_common_ancestor.py,tryalgo.lowest_common_ancestor.LowestCommonAncestorShortcuts,"class LowestCommonAncestorShortcuts:
    """"""Lowest common ancestor data structure using shortcuts to ancestors
    """"""

    def __init__(self, prec):
        """"""builds the structure from a given tree

        :param prec: father for every node, with prec[0] = 0
        :assumes: prec[node] < node
        :complexity: O(n log n), with n = len(nodes)
        """"""
        n = len(prec)
        self.level = [None] * n
        self.level[0] = 0
        for u in range(1, n):
            self.level[u] = 1 + self.level[prec[u]]
        depth = log2ceil(max((self.level[u] for u in range(n)))) + 1
        self.anc = [[0] * n for _ in range(depth)]
        for u in range(n):
            self.anc[0][u] = prec[u]
        for k in range(1, depth):
            for u in range(n):
                self.anc[k][u] = self.anc[k - 1][self.anc[k - 1][u]]

    def query(self, u, v):
        """""":returns: the lowest common ancestor of u and v
        :complexity: O(log n)
        """"""
        if self.level[u] > self.level[v]:
            u, v = (v, u)
        depth = len(self.anc)
        for k in range(depth - 1, -1, -1):
            if self.level[u] <= self.level[v] - (1 << k):
                v = self.anc[k][v]
        assert self.level[u] == self.level[v]
        if u == v:
            return u
        for k in range(depth - 1, -1, -1):
            if self.anc[k][u] != self.anc[k][v]:
                u = self.anc[k][u]
                v = self.anc[k][v]
        assert self.anc[0][u] == self.anc[0][v]
        return self.anc[0][u]","class LowestCommonAncestorShortcuts:
    '''Lowest common ancestor data structure using shortcuts to ancestors
    '''

    def __init__(self, prec):
        '''builds the structure from a given tree
        :param prec: father for every node, with prec[0] = 0
        :assumes: prec[node] < node
        :complexity: O(n log n), with n = len(nodes)
        '''
        pass

    def query(self, u, v):
        ''':returns: the lowest common ancestor of u and v
        :complexity: O(log n)
        '''
        pass",snippet_127,"class LowestCommonAncestorShortcuts:
    '''Lowest common ancestor data structure using shortcuts to ancestors
    '''

    def __init__(self, prec):
        '''builds the structure from a given tree
        :param prec: father for every node, with prec[0] = 0
        :assumes: prec[node] < node
        :complexity: O(n log n), with n = len(nodes)
        '''
        self.n = len(prec)
        self.prec = prec
        self.K = max(1, (self.n - 1).bit_length())
        self.up = [[0] * self.n for _ in range(self.K)]
        self.depth = [0] * self.n

        # base: immediate parent
        for v in range(self.n):
            self.up[0][v] = prec[v]

        # depths
        self.depth[0] = 0
        for v in range(1, self.n):
            self.depth[v] = self.depth[prec[v]] + 1

        # binary lifting table
        for k in range(1, self.K):
            up_prev = self.up[k - 1]
            up_cur = self.up[k]
            for v in range(self.n):
                up_cur[v] = up_prev[up_prev[v]]

    def query(self, u, v):
        ''':returns: the lowest common ancestor of u and v
        :complexity: O(log n)
        '''
        if u == v:
            return u

        # lift to same depth
        if self.depth[u] < self.depth[v]:
            u, v = v, u
        diff = self.depth[u] - self.depth[v]
        k = 0
        while diff:
            if diff & 1:
                u = self.up[k][u]
            diff >>= 1
            k += 1

        if u == v:
            return u

        # lift both
        for k in range(self.K - 1, -1, -1):
            if self.up[k][u] != self.up[k][v]:
                u = self.up[k][u]
                v = self.up[k][v]

        return self.up[0][u]"
223988,jilljenn/tryalgo,jilljenn_tryalgo/tryalgo/partition_refinement.py,tryalgo.partition_refinement.PartitionRefinement,"class PartitionRefinement:
    """"""This data structure implements an order preserving
    partition with refinements.
    """"""

    def __init__(self, n):
        """"""Start with the partition consisting of the unique class {0,1,..,n-1}
        complexity: O(n) both in time and space
        """"""
        c = PartitionClass()
        self.classes = c
        self.items = [PartitionItem(i, c) for i in range(n)]

    def refine(self, pivot):
        """"""Split every class C in the partition into C intersection pivot
        and C setminus pivot complexity: linear in size of pivot
        """"""
        has_split = []
        for i in pivot:
            if 0 <= i < len(self.items):
                x = self.items[i]
                c = x.theclass
                if not c.split:
                    c.split = PartitionClass(c)
                    if self.classes is c:
                        self.classes = c.split
                    has_split.append(c)
                x.remove()
                x.theclass = c.split
                c.split.append(x)
        for c in has_split:
            c.split = None
            if not c.items:
                c.remove()
                del c

    def tolist(self):
        """"""produce a list representation of the partition
        """"""
        return [[x.val for x in theclass.items] for theclass in self.classes]

    def order(self):
        """"""Produce a flatten list of the partition, ordered by classes
        """"""
        return [x.val for theclass in self.classes for x in theclass.items]","class PartitionRefinement:
    '''This data structure implements an order preserving
    partition with refinements.
    '''

    def __init__(self, n):
        '''Start with the partition consisting of the unique class {0,1,..,n-1}
        complexity: O(n) both in time and space
        '''
        pass

    def refine(self, pivot):
        '''Split every class C in the partition into C intersection pivot
        and C setminus pivot complexity: linear in size of pivot
        '''
        pass

    def tolist(self):
        '''produce a list representation of the partition
        '''
        pass

    def order(self):
        '''Produce a flatten list of the partition, ordered by classes
        '''
        pass",snippet_128,"class PartitionRefinement:
    '''This data structure implements an order preserving
    partition with refinements.
    '''

    class _Block:
        __slots__ = (""start"", ""size"")
        def __init__(self, start, size):
            self.start = start
            self.size = size

    def __init__(self, n):
        '''Start with the partition consisting of the unique class {0,1,..,n-1}
        complexity: O(n) both in time and space
        '''
        self.n = int(n)
        self.elts = list(range(self.n))
        self.pos = list(range(self.n))
        self.blocks = []
        self.whereblock = [None] * self.n
        b = PartitionRefinement._Block(0, self.n)
        self.blocks.append(b)
        for x in self.elts:
            self.whereblock[x] = b

    def refine(self, pivot):
        '''Split every class C in the partition into C intersection pivot
        and C setminus pivot complexity: linear in size of pivot
        '''
        # normalize pivot to unique, valid elements
        piv = set(pivot)
        if not piv:
            return

        # Count marked elements per block and move them to the front of their block
        counts = {}
        touched = set()

        for x in piv:
            if not (0 <= x < self.n):
                continue
            blk = self.whereblock[x]
            # initialize count lazily
            c = counts.get(blk, 0)
            i = self.pos[x]
            j = blk.start + c
            if i != j:
                y = self.elts[j]
                # swap positions i and j
                self.elts[i], self.elts[j] = self.elts[j], self.elts[i]
                self.pos[x], self.pos[y] = j, i
            else:
                self.pos[x] = j  # already true, but keep consistent
            counts[blk] = c + 1
            touched.add(blk)

        if not touched:
            return

        # Perform splits on touched blocks where needed
        for blk in list(touched):
            m = counts.get(blk, 0)
            if m == 0 or m == blk.size:
                continue  # no split needed

            start = blk.start
            size = blk.size

            # Marked part: [start, start+m)
            # Unmarked part: [start+m, start+size)
            idx = self.blocks.index(blk)

            if m <= size - m:
                # Create new block for smaller (marked) part placed before unmarked,
                # keep old block as unmarked (second part)
                new_blk = PartitionRefinement._Block(start, m)
                # Update old block to represent unmarked part
                blk.start = start + m
                blk.size = size - m
                # Insert new block before old block
                self.blocks.insert(idx, new_blk)
                # Update whereblock for elements in new block range
                end = start + m
                for k in range(start, end):
                    self.whereblock[self.elts[k]] = new_blk
            else:
                # Keep old block as marked (first part), create new block for smaller unmarked part
                new_blk = PartitionRefinement._Block(start + m, size - m)
                # Update old block to represent marked part
                blk.start = start
                blk.size = m
                # Insert new block after old block
                self.blocks.insert(idx + 1, new_blk)
                # Update whereblock for elements in new block range
                s2 = start + m
                e2 = start + size
                for k in range(s2, e2):
                    self.whereblock[self.elts[k]] = new_blk

    def tolist(self):
        '''produce a list representation of the partition
        '''
        res = []
        for blk in self.blocks:
            if blk.size > 0:
                res.append(self.elts[blk.start:blk.start + blk.size])
        return res

    def order(self):
        '''Produce a flatten list of the partition, ordered by classes
        '''
        return list(self.elts)"
224105,rigetti/grove,rigetti_grove/grove/alpha/fermion_transforms/fenwick_tree.py,grove.alpha.fermion_transforms.fenwick_tree.FenwickNode,"class FenwickNode:
    """"""Fenwick Tree node.""""""
    parent = None
    children = None
    index = None

    def __init__(self, parent, children, index=None):
        """"""Fenwick Tree node. Single parent and multiple children.

        :param FenwickNode parent: a parent node
        :param list(FenwickNode) children: a list of children nodes
        :param int index: node label
        """"""
        self.children = children
        self.parent = parent
        self.index = index

    def get_ancestors(self):
        """"""Returns a list of ancestors of the node. Ordered from the earliest.

        :return: node's ancestors, ordered from most recent
        :rtype: list(FenwickNode)
        """"""
        node = self
        ancestor_list = []
        while node.parent is not None:
            ancestor_list.append(node.parent)
            node = node.parent
        return ancestor_list","class FenwickNode:
    '''Fenwick Tree node.'''

    def __init__(self, parent, children, index=None):
        '''Fenwick Tree node. Single parent and multiple children.
        :param FenwickNode parent: a parent node
        :param list(FenwickNode) children: a list of children nodes
        :param int index: node label
        '''
        pass

    def get_ancestors(self):
        '''Returns a list of ancestors of the node. Ordered from the earliest.
        :return: node's ancestors, ordered from most recent
        :rtype: list(FenwickNode)
        '''
        pass",snippet_129,"class FenwickNode:
    '''Fenwick Tree node.'''

    def __init__(self, parent, children, index=None):
        '''Fenwick Tree node. Single parent and multiple children.
        :param FenwickNode parent: a parent node
        :param list(FenwickNode) children: a list of children nodes
        :param int index: node label
        '''
        self.parent = parent
        self.index = index
        self.children = list(children) if children is not None else []

        # Ensure children's parent is set to self
        for child in self.children:
            if child is not None:
                child.parent = self

        # Ensure this node is registered in parent's children
        if self.parent is not None:
            if self not in self.parent.children:
                self.parent.children.append(self)

    def get_ancestors(self):
        '''Returns a list of ancestors of the node. Ordered from the earliest.
        :return: node's ancestors, ordered from most recent
        :rtype: list(FenwickNode)
        '''
        ancestors = []
        current = self.parent
        while current is not None:
            ancestors.append(current)
            current = current.parent
        return ancestors"
227838,econ-ark/HARK,HARK/utilities.py,HARK.utilities.NullFunc,"import numpy as np

class NullFunc:
    """"""
    A trivial class that acts as a placeholder ""do nothing"" function.
    """"""

    def __call__(self, *args):
        """"""
        Returns meaningless output no matter what the input(s) is.  If no input,
        returns None.  Otherwise, returns an array of NaNs (or a single NaN) of
        the same size as the first input.
        """"""
        if len(args) == 0:
            return None
        else:
            arg = args[0]
            if hasattr(arg, 'shape'):
                return np.zeros_like(arg) + np.nan
            else:
                return np.nan

    def distance(self, other):
        """"""
        Trivial distance metric that only cares whether the other object is also
        an instance of NullFunc.  Intentionally does not inherit from HARKobject
        as this might create dependency problems.

        Parameters
        ----------
        other : any
            Any object for comparison to this instance of NullFunc.

        Returns
        -------
        (unnamed) : float
            The distance between self and other.  Returns 0 if other is also a
            NullFunc; otherwise returns an arbitrary high number.
        """"""
        try:
            if other.__class__ is self.__class__:
                return 0.0
            else:
                return 1000.0
        except:
            return 10000.0","
class NullFunc:
    '''
    A trivial class that acts as a placeholder ""do nothing"" function.
        '''

    def __call__(self, *args):
        '''
        Returns meaningless output no matter what the input(s) is.  If no input,
        returns None.  Otherwise, returns an array of NaNs (or a single NaN) of
        the same size as the first input.
        '''
        pass

    def distance(self, other):
        '''
        Trivial distance metric that only cares whether the other object is also
        an instance of NullFunc.  Intentionally does not inherit from HARKobject
        as this might create dependency problems.
        Parameters
        ----------
        other : any
            Any object for comparison to this instance of NullFunc.
        Returns
        -------
        (unnamed) : float
            The distance between self and other.  Returns 0 if other is also a
            NullFunc; otherwise returns an arbitrary high number.
        '''
        pass",snippet_131,"class NullFunc:
    '''
    A trivial class that acts as a placeholder ""do nothing"" function.
    '''

    def __call__(self, *args):
        '''
        Returns meaningless output no matter what the input(s) is.  If no input,
        returns None.  Otherwise, returns an array of NaNs (or a single NaN) of
        the same size as the first input.
        '''
        if len(args) == 0:
            return None

        first = args[0]

        # Try numpy if available and the input is a numpy array
        try:
            import numpy as np  # type: ignore
            if isinstance(first, np.ndarray):
                return np.full(first.shape, np.nan, dtype=float)
        except Exception:
            pass

        def _nans_like(obj):
            # Handle basic scalar types
            if obj is None:
                return float('nan')
            if isinstance(obj, (int, float, complex, bool, str)):
                return float('nan')

            # Handle sequences: list and tuple (recursively)
            if isinstance(obj, list):
                return [ _nans_like(elem) for elem in obj ]
            if isinstance(obj, tuple):
                return tuple(_nans_like(elem) for elem in obj)

            # Fallback for other types: single NaN
            return float('nan')

        return _nans_like(first)

    def distance(self, other):
        '''
        Trivial distance metric that only cares whether the other object is also
        an instance of NullFunc.  Intentionally does not inherit from HARKobject
        as this might create dependency problems.
        Parameters
        ----------
        other : any
            Any object for comparison to this instance of NullFunc.
        Returns
        -------
        (unnamed) : float
            The distance between self and other.  Returns 0 if other is also a
            NullFunc; otherwise returns an arbitrary high number.
        '''
        return 0.0 if isinstance(other, NullFunc) else 1e12"
228145,jcrobak/parquet-python,jcrobak_parquet-python/parquet/__init__.py,parquet.JsonWriter,"import json

class JsonWriter:
    """"""Utility for dumping rows as JSON objects.""""""

    def __init__(self, out):
        """"""Initialize with output destination.""""""
        self._out = out

    def writerow(self, row):
        """"""Write a single row.""""""
        json_text = json.dumps(row)
        if isinstance(json_text, bytes):
            json_text = json_text.decode('utf-8')
        self._out.write(json_text)
        self._out.write(u'\n')","
class JsonWriter:
    '''Utility for dumping rows as JSON objects.'''

    def __init__(self, out):
        '''Initialize with output destination.'''
        pass

    def writerow(self, row):
        '''Write a single row.'''
        pass",snippet_132,"import json

class JsonWriter:
    '''Utility for dumping rows as JSON objects.'''

    def __init__(self, out):
        '''Initialize with output destination.'''
        if not hasattr(out, 'write'):
            raise TypeError('out must be a file-like object with a write() method')
        self._out = out

    def writerow(self, row):
        '''Write a single row.'''
        s = json.dumps(row, ensure_ascii=False, separators=(',', ':'), default=str)
        self._out.write(s + '\n')"
228682,common-workflow-language/cwltool,common-workflow-language_cwltool/cwltool/software_requirements.py,cwltool.software_requirements.DependenciesConfiguration,"import os
from typing import TYPE_CHECKING, Any, Optional, Union, cast
import argparse

class DependenciesConfiguration:
    """"""Dependency configuration class, for RuntimeContext.job_script_provider.""""""

    def __init__(self, args: argparse.Namespace) -> None:
        """"""Initialize.""""""
        self.tool_dependency_dir: Optional[str] = None
        self.dependency_resolvers_config_file: Optional[str] = None
        conf_file = getattr(args, 'beta_dependency_resolvers_configuration', None)
        tool_dependency_dir = getattr(args, 'beta_dependencies_directory', None)
        conda_dependencies = getattr(args, 'beta_conda_dependencies', None)
        if conf_file is not None and os.path.exists(conf_file):
            self.use_tool_dependencies = True
            if tool_dependency_dir is None:
                tool_dependency_dir = os.path.abspath(os.path.dirname(conf_file))
            self.tool_dependency_dir = tool_dependency_dir
            self.dependency_resolvers_config_file = os.path.abspath(conf_file)
        elif conda_dependencies is not None:
            if tool_dependency_dir is None:
                tool_dependency_dir = os.path.abspath('./cwltool_deps')
            self.tool_dependency_dir = tool_dependency_dir
            self.use_tool_dependencies = True
            self.dependency_resolvers_config_file = None
        else:
            self.use_tool_dependencies = False
        if self.tool_dependency_dir and (not os.path.exists(self.tool_dependency_dir)):
            os.makedirs(self.tool_dependency_dir)

    def build_job_script(self, builder: 'Builder', command: list[str]) -> str:
        """"""Use the galaxy-tool-util library to construct a build script.""""""
        ensure_galaxy_lib_available()
        resolution_config_dict = {'use': self.use_tool_dependencies, 'default_base_path': self.tool_dependency_dir}
        app_config = {'conda_auto_install': True, 'conda_auto_init': True, 'debug': builder.debug}
        tool_dependency_manager: 'deps.DependencyManager' = deps.build_dependency_manager(app_config_dict=app_config, resolution_config_dict=resolution_config_dict, conf_file=self.dependency_resolvers_config_file)
        handle_dependencies: str = ''
        if (dependencies := get_dependencies(builder)):
            handle_dependencies = '\n'.join(tool_dependency_manager.dependency_shell_commands(dependencies, job_directory=builder.tmpdir))
        template_kwds: dict[str, str] = dict(handle_dependencies=handle_dependencies)
        job_script = COMMAND_WITH_DEPENDENCIES_TEMPLATE.substitute(template_kwds)
        return job_script","
class DependenciesConfiguration:
    '''Dependency configuration class, for RuntimeContext.job_script_provider.'''

    def __init__(self, args: argparse.Namespace) -> None:
        '''Initialize.'''
        pass

    def build_job_script(self, builder: 'Builder', command: list[str]) -> str:
        '''Use the galaxy-tool-util library to construct a build script.'''
        pass",snippet_133,"import argparse
import shlex
from typing import Any, Callable, Iterable, Mapping, Optional


class DependenciesConfiguration:
    '''Dependency configuration class, for RuntimeContext.job_script_provider.'''

    def __init__(self, args: argparse.Namespace) -> None:
        '''Initialize.'''
        self.shell: str = getattr(args, ""job_shell"", ""/usr/bin/env bash"")
        self.strict_bash: bool = bool(getattr(args, ""strict_bash"", True))
        self.working_directory: Optional[str] = getattr(args, ""working_directory"", None)

        self.prologue: list[str] = []
        prologue = getattr(args, ""prologue"", None)
        if isinstance(prologue, str):
            self.prologue = [prologue]
        elif isinstance(prologue, Iterable):
            self.prologue = [str(x) for x in prologue if x is not None]

        self.epilogue: list[str] = []
        epilogue = getattr(args, ""epilogue"", None)
        if isinstance(epilogue, str):
            self.epilogue = [epilogue]
        elif isinstance(epilogue, Iterable):
            self.epilogue = [str(x) for x in epilogue if x is not None]

        self.env: dict[str, str] = {}
        env = getattr(args, ""env"", None)
        if isinstance(env, Mapping):
            self.env = {str(k): str(v) for k, v in env.items()}
        elif isinstance(env, Iterable) and not isinstance(env, (str, bytes)):
            for item in env:
                if item is None:
                    continue
                if isinstance(item, str) and ""="" in item:
                    k, v = item.split(""="", 1)
                    self.env[k] = v
                elif isinstance(item, Mapping):
                    for k, v in item.items():
                        self.env[str(k)] = str(v)

    def _try_builder_interfaces(self, builder: Any, command: list[str]) -> Optional[str]:
        # Try common interfaces without enforcing galaxy-tool-util at import time
        # 1) builder.build(...)
        build_attr = getattr(builder, ""build"", None)
        if callable(build_attr):
            try:
                result = build_attr(command)
                if isinstance(result, str):
                    return result
            except TypeError:
                # Try with keyword names commonly used
                for kw in (
                    {""command"": command},
                    {""cmd"": command},
                    {""args"": command},
                ):
                    try:
                        result = build_attr(**kw)
                        if isinstance(result, str):
                            return result
                    except Exception:
                        pass
            except Exception:
                pass

        # 2) Callable builder
        if callable(builder):
            try:
                result = builder(command)
                if isinstance(result, str):
                    return result
            except Exception:
                pass

        # 3) builder has method add_command + to_string
        add_cmd = getattr(builder, ""add_command"", None)
        to_string = getattr(builder, ""to_string"", None)
        if callable(add_cmd) and callable(to_string):
            try:
                add_cmd(command)
                result = to_string()
                if isinstance(result, str):
                    return result
            except Exception:
                pass

        return None

    def _compose_script(self, command: list[str]) -> str:
        if not command:
            raise ValueError(""Command must be a non-empty list of strings."")
        lines: list[str] = []

        # Shebang
        # If shell is like ""/usr/bin/env bash"", use it directly; else default to env bash
        shebang = self.shell.strip() if self.shell else ""/usr/bin/env bash""
        if not shebang.startswith(""#!""):
            lines.append(f""#!{shebang}"")
        else:
            lines.append(shebang)

        # Strict mode
        if self.strict_bash:
            lines.append(""set -euo pipefail"")

        # Export env vars
        for k, v in self.env.items():
            # Use printf-like safe export
            kv = f""{k}={v}""
            lines.append(f""export {shlex.quote(k)}={shlex.quote(v)}"")

        # Change working directory
        if self.working_directory:
            lines.append(f""cd {shlex.quote(self.working_directory)}"")

        # Prologue
        for line in self.prologue:
            if line is None:
                continue
            lines.append(str(line))

        # Main command
        lines.append(shlex.join(command))

        # Epilogue
        for line in self.epilogue:
            if line is None:
                continue
            lines.append(str(line))

        # Final newline
        return ""\n"".join(lines) + ""\n""

    def build_job_script(self, builder: 'Builder', command: list[str]) -> str:
        '''Use the galaxy-tool-util library to construct a build script.'''
        # First, try to use the provided builder if it exposes familiar interfaces.
        script = self._try_builder_interfaces(builder, command)
        if isinstance(script, str) and script.strip():
            return script

        # Fallback to a simple shell script composition.
        return self._compose_script(command)"
229565,sourceperl/pyModbusTCP,sourceperl_pyModbusTCP/examples/client_serial_gw.py,client_serial_gw.Serial2ModbusClient,"from pyModbusTCP.constants import EXP_GATEWAY_TARGET_DEVICE_FAILED_TO_RESPOND
import struct

class Serial2ModbusClient:
    """""" Customize a slave serial worker for map a modbus TCP client. """"""

    def __init__(self, serial_w, mbus_cli, slave_addr=1, allow_bcast=False):
        """"""Serial2ModbusClient constructor.

        :param serial_w: a SlaveSerialWorker instance
        :type serial_w: SlaveSerialWorker
        :param mbus_cli: a ModbusClient instance
        :type mbus_cli: ModbusClient
        :param slave_addr: modbus slave address
        :type slave_addr: int
        :param allow_bcast: allow processing broadcast frames (slave @0)
        :type allow_bcast: bool
        """"""
        self.serial_w = serial_w
        self.mbus_cli = mbus_cli
        self.slave_addr = slave_addr
        self.allow_bcast = allow_bcast
        self.serial_w.handle_request = self._handle_request

    def _handle_request(self):
        """"""Request handler for SlaveSerialWorker""""""
        if self.serial_w.request.slave_addr == 0 and self.allow_bcast:
            self.mbus_cli.custom_request(self.serial_w.request.pdu)
        elif self.serial_w.request.slave_addr == self.slave_addr:
            resp_pdu = self.mbus_cli.custom_request(self.serial_w.request.pdu)
            if resp_pdu:
                self.serial_w.response.build(raw_pdu=resp_pdu, slave_addr=self.serial_w.request.slave_addr)
            else:
                exp_pdu = struct.pack('BB', self.serial_w.request.function_code + 128, EXP_GATEWAY_TARGET_DEVICE_FAILED_TO_RESPOND)
                self.serial_w.response.build(raw_pdu=exp_pdu, slave_addr=self.serial_w.request.slave_addr)

    def run(self):
        """"""Start serial processing.""""""
        self.serial_w.run()","
class Serial2ModbusClient:
    ''' Customize a slave serial worker for map a modbus TCP client. '''

    def __init__(self, serial_w, mbus_cli, slave_addr=1, allow_bcast=False):
        '''Serial2ModbusClient constructor.
        :param serial_w: a SlaveSerialWorker instance
        :type serial_w: SlaveSerialWorker
        :param mbus_cli: a ModbusClient instance
        :type mbus_cli: ModbusClient
        :param slave_addr: modbus slave address
        :type slave_addr: int
        :param allow_bcast: allow processing broadcast frames (slave @0)
        :type allow_bcast: bool
        '''
        pass

    def _handle_request(self):
        '''Request handler for SlaveSerialWorker'''
        pass

    def run(self):
        '''Start serial processing.'''
        pass",snippet_134,"class Serial2ModbusClient:
    ''' Customize a slave serial worker for map a modbus TCP client. '''

    def __init__(self, serial_w, mbus_cli, slave_addr=1, allow_bcast=False):
        '''Serial2ModbusClient constructor.
        :param serial_w: a SlaveSerialWorker instance
        :type serial_w: SlaveSerialWorker
        :param mbus_cli: a ModbusClient instance
        :type mbus_cli: ModbusClient
        :param slave_addr: modbus slave address
        :type slave_addr: int
        :param allow_bcast: allow processing broadcast frames (slave @0)
        :type allow_bcast: bool
        '''
        self.serial_w = serial_w
        self.mbus_cli = mbus_cli
        self.slave_addr = int(slave_addr)
        self.allow_bcast = bool(allow_bcast)

        # Try to propagate broadcast policy to worker if it supports it
        try:
            setattr(self.serial_w, ""allow_bcast"", self.allow_bcast)
        except Exception:
            pass

        # Register our handler if the worker exposes a hook
        if hasattr(self.serial_w, ""set_request_handler"") and callable(getattr(self.serial_w, ""set_request_handler"")):
            self.serial_w.set_request_handler(self._handle_request)
        elif hasattr(self.serial_w, ""req_handler""):
            try:
                setattr(self.serial_w, ""req_handler"", self._handle_request)
            except Exception:
                pass

    def _get_pending_request(self):
        # Try several ways to fetch an incoming request from the worker
        # Expected to return tuple (unit_id, request_pdu) or (None, None) if nothing.
        w = self.serial_w
        # Method returning tuple
        for name in (""get_request"", ""pop_request"", ""read_request"", ""recv_request""):
            meth = getattr(w, name, None)
            if callable(meth):
                try:
                    req = meth()
                    if isinstance(req, tuple) and len(req) == 2:
                        return req[0], req[1]
                except Exception:
                    pass
        # Attributes carrying last request
        unit = getattr(w, ""request_slave"", None)
        pdu = getattr(w, ""request_pdu"", None)
        if unit is not None and pdu is not None:
            return unit, pdu
        return None, None

    def _send_response(self, unit_id, response_pdu):
        # Try several ways to send a response back to the worker
        w = self.serial_w
        for name in (""send_response"", ""put_response"", ""write_response"", ""set_response""):
            meth = getattr(w, name, None)
            if callable(meth):
                try:
                    # Prefer (unit_id, pdu) signature; fallback to (pdu,)
                    try:
                        return meth(unit_id, response_pdu)
                    except TypeError:
                        return meth(response_pdu)
                except Exception:
                    pass
        # As a last resort, try setting attributes the worker might read
        try:
            setattr(w, ""response_slave"", unit_id)
            setattr(w, ""response_pdu"", response_pdu)
        except Exception:
            pass

    def _ensure_client_open(self):
        cli = self.mbus_cli
        try:
            is_open = getattr(cli, ""is_open"", None)
            if callable(is_open):
                if not is_open():
                    open_m = getattr(cli, ""open"", None)
                    if callable(open_m):
                        open_m()
            else:
                # Some clients expose .open() without .is_open()
                open_m = getattr(cli, ""open"", None)
                if callable(open_m):
                    try:
                        open_m()
                    except Exception:
                        pass
        except Exception:
            pass

    def _proxy_custom_request(self, request_pdu, unit_id):
        cli = self.mbus_cli
        # Prefer custom_request if available
        custom = getattr(cli, ""custom_request"", None)
        if callable(custom):
            # pyModbusTCP custom_request(request_pdu, unit_id=1)
            try:
                return custom(request_pdu, unit_id=unit_id)
            except TypeError:
                # Some variants use slave/unit as positional second arg
                return custom(request_pdu, unit_id)
        # No raw PDU path available
        return None

    def _handle_request(self):
        '''Request handler for SlaveSerialWorker'''
        unit_id, req_pdu = self._get_pending_request()
        if req_pdu is None:
            return

        # Drop broadcasts if not allowed
        if unit_id == 0 and not self.allow_bcast:
            return

        # Ensure TCP client is connected if it supports it
        self._ensure_client_open()

        resp_pdu = None
        try:
            resp_pdu = self._proxy_custom_request(req_pdu, unit_id if unit_id else self.slave_addr)
        except Exception:
            resp_pdu = None

        # Do not respond to broadcast requests
        if unit_id == 0:
            return

        if resp_pdu is not None:
            self._send_response(unit_id, resp_pdu)

    def run(self):
        '''Start serial processing.'''
        # Prefer worker-managed loop if provided
        w = self.serial_w

        # If worker accepts our handler, call a run/looping method
        for name in (""run"", ""start"", ""serve_forever"", ""process_forever"", ""loop""):
            meth = getattr(w, name, None)
            if callable(meth):
                return meth()

        # Fallback: simple polling loop if worker provides a process/step method
        step = None
        for name in (""process"", ""step"", ""poll""):
            meth = getattr(w, name, None)
            if callable(meth):
                step = meth
                break

        if step is None:
            # Last resort: manually poll for requests and handle them
            import time
            while True:
                try:
                    self._handle_request()
                except Exception:
                    pass
                time.sleep(0.01)
        else:
            import time
            while True:
                try:
                    step()
                    self._handle_request()
                except Exception:
                    pass
                time.sleep(0.001)"
230113,aws/aws-xray-sdk-python,aws_aws-xray-sdk-python/aws_xray_sdk/core/models/noop_traceid.py,aws_aws-xray-sdk-python.aws_xray_sdk.core.models.noop_traceid.NoOpTraceId,"class NoOpTraceId:
    """"""
    A trace ID tracks the path of a request through your application.
    A trace collects all the segments generated by a single request.
    A trace ID is required for a segment.
    """"""
    VERSION = '1'
    DELIMITER = '-'

    def __init__(self):
        """"""
        Generate a no-op trace id.
        """"""
        self.start_time = '00000000'
        self.__number = '000000000000000000000000'

    def to_id(self):
        """"""
        Convert TraceId object to a string.
        """"""
        return '%s%s%s%s%s' % (NoOpTraceId.VERSION, NoOpTraceId.DELIMITER, self.start_time, NoOpTraceId.DELIMITER, self.__number)","class NoOpTraceId:
    '''
    A trace ID tracks the path of a request through your application.
    A trace collects all the segments generated by a single request.
    A trace ID is required for a segment.
    '''

    def __init__(self):
        '''
        Generate a no-op trace id.
        '''
        pass

    def to_id(self):
        '''
        Convert TraceId object to a string.
        '''
        pass",snippet_135,"class NoOpTraceId:
    '''
    A trace ID tracks the path of a request through your application.
    A trace collects all the segments generated by a single request.
    A trace ID is required for a segment.
    '''

    def __init__(self):
        '''
        Generate a no-op trace id.
        '''
        self._id = '0' * 32

    def to_id(self):
        '''
        Convert TraceId object to a string.
        '''
        return self._id"
230120,aws/aws-xray-sdk-python,aws_aws-xray-sdk-python/aws_xray_sdk/core/models/traceid.py,aws_aws-xray-sdk-python.aws_xray_sdk.core.models.traceid.TraceId,"import binascii
import os
import time

class TraceId:
    """"""
    A trace ID tracks the path of a request through your application.
    A trace collects all the segments generated by a single request.
    A trace ID is required for a segment.
    """"""
    VERSION = '1'
    DELIMITER = '-'

    def __init__(self):
        """"""
        Generate a random trace id.
        """"""
        self.start_time = int(time.time())
        self.__number = binascii.b2a_hex(os.urandom(12)).decode('utf-8')

    def to_id(self):
        """"""
        Convert TraceId object to a string.
        """"""
        return '%s%s%s%s%s' % (TraceId.VERSION, TraceId.DELIMITER, format(self.start_time, 'x'), TraceId.DELIMITER, self.__number)","
class TraceId:
    '''
    A trace ID tracks the path of a request through your application.
    A trace collects all the segments generated by a single request.
    A trace ID is required for a segment.
    '''

    def __init__(self):
        '''
        Generate a random trace id.
        '''
        pass

    def to_id(self):
        '''
        Convert TraceId object to a string.
        '''
        pass",snippet_136,"class TraceId:
    '''
    A trace ID tracks the path of a request through your application.
    A trace collects all the segments generated by a single request.
    A trace ID is required for a segment.
    '''

    def __init__(self):
        '''
        Generate a random trace id.
        '''
        import time
        import secrets

        version = ""1""
        timestamp_hex = format(int(time.time()), ""08x"")
        random_hex = secrets.token_hex(12)  # 96-bit random identifier
        self._id = f""{version}-{timestamp_hex}-{random_hex}""

    def to_id(self):
        '''
        Convert TraceId object to a string.
        '''
        return self._id"
230123,aws/aws-xray-sdk-python,aws_aws-xray-sdk-python/aws_xray_sdk/core/sampling/local/reservoir.py,aws_aws-xray-sdk-python.aws_xray_sdk.core.sampling.local.reservoir.Reservoir,"import time
import threading

class Reservoir:
    """"""
    Keeps track of the number of sampled segments within
    a single second. This class is implemented to be
    thread-safe to achieve accurate sampling.
    """"""

    def __init__(self, traces_per_sec=0):
        """"""
        :param int traces_per_sec: number of guranteed
            sampled segments.
        """"""
        self._lock = threading.Lock()
        self.traces_per_sec = traces_per_sec
        self.used_this_sec = 0
        self.this_sec = int(time.time())

    def take(self):
        """"""
        Returns True if there are segments left within the
        current second, otherwise return False.
        """"""
        with self._lock:
            now = int(time.time())
            if now != self.this_sec:
                self.used_this_sec = 0
                self.this_sec = now
            if self.used_this_sec >= self.traces_per_sec:
                return False
            self.used_this_sec = self.used_this_sec + 1
            return True","
class Reservoir:
    '''
    Keeps track of the number of sampled segments within
    a single second. This class is implemented to be
    thread-safe to achieve accurate sampling.
    '''

    def __init__(self, traces_per_sec=0):
        '''
        :param int traces_per_sec: number of guranteed
            sampled segments.
        '''
        pass

    def take(self):
        '''
        Returns True if there are segments left within the
        current second, otherwise return False.
        '''
        pass",snippet_137,"import time
import threading

class Reservoir:
    '''
    Keeps track of the number of sampled segments within
    a single second. This class is implemented to be
    thread-safe to achieve accurate sampling.
    '''

    def __init__(self, traces_per_sec=0):
        '''
        :param int traces_per_sec: number of guranteed
            sampled segments.
        '''
        self._traces_per_sec = max(0, int(traces_per_sec))
        self._lock = threading.Lock()
        self._current_sec = int(time.time())
        self._count = 0

    def take(self):
        '''
        Returns True if there are segments left within the
        current second, otherwise return False.
        '''
        now_sec = int(time.time())
        with self._lock:
            if now_sec != self._current_sec:
                self._current_sec = now_sec
                self._count = 0

            if self._count < self._traces_per_sec:
                self._count += 1
                return True

            return False"
230719,django-salesforce/django-salesforce,django-salesforce_django-salesforce/salesforce/backend/indep.py,salesforce.backend.indep.LazyField,"from typing import Any, Callable, Dict, Tuple, Type
from inspect import signature

class LazyField:
    """"""A Field that can be later customized until it is binded to the final Model""""""
    counter = 0

    def __init__(self, klass: 'Type[Field[Any, Any]]') -> None:
        """"""Instantiate the field type""""""
        self.klass = klass
        self.kw = {}
        self.args = ()
        self.called = False
        self.counter = self.counter

    def __call__(self, *args: Any, **kwargs: Any) -> 'LazyField':
        """"""Instantiate a new field with options""""""
        assert not self.called
        bound_args = signature(self.klass.__init__).bind(self, *args, **kwargs)
        obj = type(self)(self.klass)
        obj.args = bound_args.args[1:]
        obj.kw = bound_args.kwargs
        setattr(type(self), 'counter', getattr(type(self), 'counter') + 1)
        return obj

    def update(self, **kwargs: Any) -> 'LazyField':
        """"""Customize the lazy field""""""
        assert not self.called
        self.kw.update(kwargs)
        return self

    def create(self) -> 'Field[Any, Any]':
        """"""Create a normal field from the lazy field""""""
        assert not self.called
        return self.klass(*self.args, **self.kw)","
class LazyField:
    '''A Field that can be later customized until it is binded to the final Model'''

    def __init__(self, klass: 'Type[Field[Any, Any]]') -> None:
        '''Instantiate the field type'''
        pass

    def __call__(self, *args: Any, **kwargs: Any) -> 'LazyField':
        '''Instantiate a new field with options'''
        pass

    def update(self, **kwargs: Any) -> 'LazyField':
        '''Customize the lazy field'''
        pass

    def create(self) -> 'Field[Any, Any]':
        '''Create a normal field from the lazy field'''
        pass",snippet_139,"from typing import Any, Type, Tuple, Dict

class LazyField:
    '''A Field that can be later customized until it is binded to the final Model'''

    def __init__(self, klass: 'Type[Field[Any, Any]]') -> None:
        '''Instantiate the field type'''
        self.klass: 'Type[Field[Any, Any]]' = klass
        self._args: Tuple[Any, ...] = ()
        self._kwargs: Dict[str, Any] = {}

    def __call__(self, *args: Any, **kwargs: Any) -> 'LazyField':
        '''Instantiate a new field with options'''
        new = LazyField(self.klass)
        # carry over existing options and apply new ones
        new._args = self._args + tuple(args)
        merged = dict(self._kwargs)
        merged.update(kwargs)
        new._kwargs = merged
        return new

    def update(self, **kwargs: Any) -> 'LazyField':
        '''Customize the lazy field'''
        self._kwargs.update(kwargs)
        return self

    def create(self) -> 'Field[Any, Any]':
        '''Create a normal field from the lazy field'''
        return self.klass(*self._args, **self._kwargs)"
231223,weblyzard/inscriptis,benchmarking/run_benchmarking.py,run_benchmarking.AbstractHtmlConverter,"from time import time

class AbstractHtmlConverter:
    """"""
    An abstract HTML convert class.
    """"""

    def get_text(self, html):
        """"""
        Returns:
            a text representation of the given HTML snippet.
        """"""
        raise NotImplementedError

    def benchmark(self, html):
        """"""
        Benchmarks the classes HTML to text converter.

        Returns:
            A tuple of the required time and the obtained text representation.
        """"""
        start_time = time()
        for _ in range(TRIES):
            text = self.get_text(html)
        return (time() - start_time, text)","
class AbstractHtmlConverter:
    '''
    An abstract HTML convert class.
        '''

    def get_text(self, html):
        '''
        Returns:
            a text representation of the given HTML snippet.
        '''
        pass

    def benchmark(self, html):
        '''
        Benchmarks the classes HTML to text converter.
        Returns:
            A tuple of the required time and the obtained text representation.
        '''
        pass",snippet_140,"class AbstractHtmlConverter:
    '''
    An abstract HTML convert class.
    '''

    def get_text(self, html):
        '''
        Returns:
            a text representation of the given HTML snippet.
        '''
        raise NotImplementedError(""Subclasses must implement get_text"")

    def benchmark(self, html):
        '''
        Benchmarks the classes HTML to text converter.
        Returns:
            A tuple of the required time and the obtained text representation.
        '''
        import time
        start = time.perf_counter()
        text = self.get_text(html)
        end = time.perf_counter()
        return (end - start, text)"
236113,richardchien/python-aiocqhttp,richardchien_python-aiocqhttp/aiocqhttp/api.py,aiocqhttp.api.Api,"from typing import Callable, Any, Union, Awaitable
import functools
import abc

class Api:
    """"""
    API 接口类。

    继承此类的具体实现类应实现 `call_action` 方法。
    """"""

    @abc.abstractmethod
    def call_action(self, action: str, **params) -> Union[Awaitable[Any], Any]:
        """"""
        调用 OneBot API，`action` 为要调用的 API 动作名，`**params`
        为 API 所需参数。

        根据实现类的不同，此函数可能是异步也可能是同步函数。
        """"""
        pass

    def __getattr__(self, item: str) -> Callable[..., Union[Awaitable[Any], Any]]:
        """"""获取一个可调用对象，用于调用对应 API。""""""
        return functools.partial(self.call_action, item)","
class Api:
    '''
    API 接口类。
    继承此类的具体实现类应实现 `call_action` 方法。
    '''
    @abc.abstractmethod
    def call_action(self, action: str, **params) -> Union[Awaitable[Any], Any]:
        '''
        调用 OneBot API，`action` 为要调用的 API 动作名，`**params`
        为 API 所需参数。
        根据实现类的不同，此函数可能是异步也可能是同步函数。
        '''
        pass

    def __getattr__(self, item: str) -> Callable[..., Union[Awaitable[Any], Any]]:
        '''获取一个可调用对象，用于调用对应 API。'''
        pass",snippet_141,"import abc
from typing import Any, Awaitable, Callable, Union


class Api(abc.ABC):
    '''
    API 接口类。
    继承此类的具体实现类应实现 `call_action` 方法。
    '''
    @abc.abstractmethod
    def call_action(self, action: str, **params) -> Union[Awaitable[Any], Any]:
        '''
        调用 OneBot API，`action` 为要调用的 API 动作名，`**params`
        为 API 所需参数。
        根据实现类的不同，此函数可能是异步也可能是同步函数。
        '''
        raise NotImplementedError

    def __getattr__(self, item: str) -> Callable[..., Union[Awaitable[Any], Any]]:
        '''获取一个可调用对象，用于调用对应 API。'''
        if item.startswith('__') and item.endswith('__'):
            raise AttributeError(item)

        def _caller(**params: Any) -> Union[Awaitable[Any], Any]:
            return self.call_action(item, **params)

        return _caller"
237136,project-generator/project_generator,project-generator_project_generator/project_generator/project.py,project_generator.project.ProjectTemplate,"class ProjectTemplate:
    """""" Public data which can be set in yaml files
        Yaml data available are:
            'build_dir' : build_dir,    # Build output path
            'debugger' : debugger,      # Debugger
            'export_dir': '',           # Export directory path
            'includes': [],             # include paths
            'linker_file': None,        # linker script file
            'name': name,               # project name
            'macros': [],               # macros
            'misc': {},                 # misc settings related to tools
            'output_type': output_type, # output type, default - exe
            'sources': [],              # source files/folders
            'target': '',               # target
            'template' : [],            # tool template
            'tools_supported': [],      # Tools which are supported,
    """"""

    @staticmethod
    def _get_common_data_template():
        """""" Data for tool specific """"""
        data_template = {'includes': [], 'linker_file': '', 'macros': [], 'sources': []}
        return data_template

    @staticmethod
    def _get_tool_specific_data_template():
        """""" Data for tool specific """"""
        data_template = {'misc': {}, 'template': []}
        return data_template

    @staticmethod
    def get_project_template(name='Default', output_type='exe', debugger=None, build_dir='build'):
        """""" Project data (+ data) """"""
        project_template = {'build_dir': build_dir, 'debugger': debugger, 'export_dir': '', 'name': name, 'output_type': output_type, 'target': '', 'tools_supported': []}
        project_template.update(ProjectTemplate._get_common_data_template())
        project_template.update(ProjectTemplate._get_tool_specific_data_template())
        return project_template","class ProjectTemplate:
    ''' Public data which can be set in yaml files
        Yaml data available are:
            'build_dir' : build_dir,    # Build output path
            'debugger' : debugger,      # Debugger
            'export_dir': '',           # Export directory path
            'includes': [],             # include paths
            'linker_file': None,        # linker script file
            'name': name,               # project name
            'macros': [],               # macros
            'misc': {},                 # misc settings related to tools
            'output_type': output_type, # output type, default - exe
            'sources': [],              # source files/folders
            'target': '',               # target
            'template' : [],            # tool template
            'tools_supported': [],      # Tools which are supported,
    '''
    @staticmethod
    def _get_common_data_template():
        ''' Data for tool specific '''
        pass
    @staticmethod
    def _get_tool_specific_data_template():
        ''' Data for tool specific '''
        pass
    @staticmethod
    def get_project_template(name='Default', output_type='exe', debugger=None, build_dir='build'):
        ''' Project data (+ data) '''
        pass",snippet_142,"class ProjectTemplate:
    ''' Public data which can be set in yaml files
        Yaml data available are:
            'build_dir' : build_dir,    # Build output path
            'debugger' : debugger,      # Debugger
            'export_dir': '',           # Export directory path
            'includes': [],             # include paths
            'linker_file': None,        # linker script file
            'name': name,               # project name
            'macros': [],               # macros
            'misc': {},                 # misc settings related to tools
            'output_type': output_type, # output type, default - exe
            'sources': [],              # source files/folders
            'target': '',               # target
            'template' : [],            # tool template
            'tools_supported': [],      # Tools which are supported,
    '''
    @staticmethod
    def _get_common_data_template():
        ''' Data for tool specific '''
        return {
            'name': 'Default',
            'output_type': 'exe',
            'debugger': None,
            'build_dir': 'build',
            'export_dir': '',
            'includes': [],
            'linker_file': None,
            'macros': [],
            'misc': {},
            'sources': [],
            'target': '',
            'template': [],
            'tools_supported': [],
        }

    @staticmethod
    def _get_tool_specific_data_template():
        ''' Data for tool specific '''
        return {}

    @staticmethod
    def get_project_template(name='Default', output_type='exe', debugger=None, build_dir='build'):
        ''' Project data (+ data) '''
        data = ProjectTemplate._get_common_data_template()
        # Ensure new containers to avoid shared state between calls
        data['includes'] = list(data.get('includes', []))
        data['macros'] = list(data.get('macros', []))
        data['misc'] = dict(data.get('misc', {}))
        data['sources'] = list(data.get('sources', []))
        data['template'] = list(data.get('template', []))
        data['tools_supported'] = list(data.get('tools_supported', []))

        data['name'] = name
        data['output_type'] = output_type
        data['debugger'] = debugger
        data['build_dir'] = build_dir

        tool_specific = ProjectTemplate._get_tool_specific_data_template()
        # Merge tool-specific data if any (kept separate under a dedicated key)
        if tool_specific:
            data['tool_specific'] = tool_specific

        return data"
237503,astropy/photutils,photutils/psf/model_plotting.py,photutils.psf.model_plotting.ModelGridPlotMixin,"from astropy.utils import minversion
import numpy as np
from astropy.visualization import simple_norm
import astropy

class ModelGridPlotMixin:
    """"""
    Mixin class to plot a grid of ePSF models.
    """"""

    def _reshape_grid(self, data):
        """"""
        Reshape the 3D ePSF grid as a 2D array of horizontally and
        vertically stacked ePSFs.

        Parameters
        ----------
        data : `numpy.ndarray`
            The 3D array of ePSF data.

        Returns
        -------
        reshaped_data : `numpy.ndarray`
            The 2D array of ePSF data.
        """"""
        nypsfs = self._ygrid.shape[0]
        nxpsfs = self._xgrid.shape[0]
        ny, nx = self.data.shape[1:]
        data.shape = (nypsfs, nxpsfs, ny, nx)
        return data.transpose([0, 2, 1, 3]).reshape(nypsfs * ny, nxpsfs * nx)

    def plot_grid(self, *, ax=None, vmax_scale=None, peak_norm=False, deltas=False, cmap='viridis', dividers=True, divider_color='darkgray', divider_ls='-', figsize=None):
        """"""
        Plot the grid of ePSF models.

        Parameters
        ----------
        ax : `matplotlib.axes.Axes` or `None`, optional
            The matplotlib axes on which to plot. If `None`, then the
            current `~matplotlib.axes.Axes` instance is used.

        vmax_scale : float, optional
            Scale factor to apply to the image stretch limits. This
            value is multiplied by the peak ePSF value to determine the
            plotting ``vmax``. The defaults are 1.0 for plotting the
            ePSF data and 0.03 for plotting the ePSF difference data
            (``deltas=True``). If ``deltas=True``, the ``vmin`` is set
            to ``-vmax``. If ``deltas=False`` the ``vmin`` is set to
            ``vmax`` / 1e4.

        peak_norm : bool, optional
            Whether to normalize the ePSF data by the peak value. The
            default shows the ePSF flux per pixel.

        deltas : bool, optional
            Set to `True` to show the differences between each ePSF
            and the average ePSF.

        cmap : str or `matplotlib.colors.Colormap`, optional
            The colormap to use. The default is 'viridis'.

        dividers : bool, optional
            Whether to show divider lines between the ePSFs.

        divider_color, divider_ls : str, optional
            Matplotlib color and linestyle options for the divider
            lines between ePSFs. These keywords have no effect unless
            ``show_dividers=True``.

        figsize : (float, float), optional
            The figure (width, height) in inches.

        Returns
        -------
        fig : `matplotlib.figure.Figure`
            The matplotlib figure object. This will be the current
            figure if ``ax=None``. Use ``fig.savefig()`` to save the
            figure to a file.

        Notes
        -----
        This method returns a figure object. If you are using this
        method in a script, you will need to call ``plt.show()`` to
        display the figure. If you are using this method in a Jupyter
        notebook, the figure will be displayed automatically.

        When in a notebook, if you do not store the return value of this
        function, the figure will be displayed twice due to the REPL
        automatically displaying the return value of the last function
        call. Alternatively, you can append a semicolon to the end of
        the function call to suppress the display of the return value.
        """"""
        import matplotlib.pyplot as plt
        from mpl_toolkits.axes_grid1 import make_axes_locatable
        data = self.data.copy()
        if deltas:
            mask = np.zeros(data.shape[0], dtype=bool)
            for i, arr in enumerate(data):
                if np.count_nonzero(arr) == 0:
                    mask[i] = True
            data -= np.mean(data[~mask], axis=0)
            data[mask] = 0.0
        data = self._reshape_grid(data)
        if ax is None:
            if figsize is None and self.meta.get('detector', '') == 'NRCSW':
                figsize = (20, 8)
            fig, ax = plt.subplots(figsize=figsize)
        else:
            fig = plt.gcf()
        if peak_norm and data.max() != 0:
            data /= data.max()
        if deltas:
            if vmax_scale is None:
                vmax_scale = 0.03
            vmax = data.max() * vmax_scale
            vmin = -vmax
            if minversion(astropy, '6.1'):
                norm = simple_norm(data, 'linear', vmin=vmin, vmax=vmax)
            else:
                norm = simple_norm(data, 'linear', min_cut=vmin, max_cut=vmax)
        else:
            if vmax_scale is None:
                vmax_scale = 1.0
            vmax = data.max() * vmax_scale
            vmin = vmax / 10000.0
            if minversion(astropy, '6.1'):
                norm = simple_norm(data, 'log', vmin=vmin, vmax=vmax, log_a=10000.0)
            else:
                norm = simple_norm(data, 'log', min_cut=vmin, max_cut=vmax, log_a=10000.0)
        nypsfs = self._ygrid.shape[0]
        nxpsfs = self._xgrid.shape[0]
        extent = [-0.5, nxpsfs - 0.5, -0.5, nypsfs - 0.5]
        axim = ax.imshow(data, extent=extent, norm=norm, cmap=cmap, origin='lower')
        xticklabels = self._xgrid.astype(int)
        yticklabels = self._ygrid.astype(int)
        if self.meta.get('detector', '') == 'NRCSW':
            xticklabels = list(xticklabels[0:5]) * 4
            yticklabels = list(yticklabels[0:5]) * 2
        ax.set_xticks(np.arange(nxpsfs))
        ax.set_xticklabels(xticklabels)
        ax.set_xlabel('ePSF location in detector X pixels')
        ax.set_yticks(np.arange(nypsfs))
        ax.set_yticklabels(yticklabels)
        ax.set_ylabel('ePSF location in detector Y pixels')
        if dividers:
            for ix in range(nxpsfs - 1):
                ax.axvline(ix + 0.5, color=divider_color, ls=divider_ls)
            for iy in range(nypsfs - 1):
                ax.axhline(iy + 0.5, color=divider_color, ls=divider_ls)
        instrument = self.meta.get('instrument', '')
        if not instrument:
            instrument = self.meta.get('instrume', '')
        detector = self.meta.get('detector', '')
        filtername = self.meta.get('filter', '')
        if isinstance(instrument, (tuple, list, np.ndarray)):
            instrument = instrument[0]
        if isinstance(detector, (tuple, list, np.ndarray)):
            detector = detector[0]
        if isinstance(filtername, (tuple, list, np.ndarray)):
            filtername = filtername[0]
        title = f'{instrument} {detector} {filtername}'
        if title != '':
            title += ' '
        if deltas:
            minus = '−'
            ax.set_title(f'{title}(ePSFs {minus} <ePSF>)')
            if peak_norm:
                label = 'Difference relative to average ePSF peak'
            else:
                label = 'Difference relative to average ePSF values'
        else:
            ax.set_title(f'{title}ePSFs')
            if peak_norm:
                label = 'Scale relative to ePSF peak pixel'
            else:
                label = 'ePSF flux per pixel'
        divider = make_axes_locatable(ax)
        cax_cbar = divider.append_axes('right', size='3%', pad='3%')
        cbar = fig.colorbar(axim, cax=cax_cbar, label=label)
        if not deltas:
            cbar.ax.set_yscale('log')
        if self.meta.get('detector', '') == 'NRCSW':
            nxpsfs = len(self._xgrid)
            nypsfs = len(self._ygrid)
            plt.axhline(nypsfs / 2 - 0.5, color='orange')
            for i in range(1, 4):
                ax.axvline(nxpsfs / 4 * i - 0.5, color='orange')
            det_labels = [['A1', 'A3', 'B4', 'B2'], ['A2', 'A4', 'B3', 'B1']]
            for i in range(2):
                for j in range(4):
                    ax.text(j * nxpsfs / 4 - 0.45, (i + 1) * nypsfs / 2 - 0.55, det_labels[i][j], color='orange', verticalalignment='top', fontsize=12)
        fig.tight_layout()
        return fig","
class ModelGridPlotMixin:
    '''
    Mixin class to plot a grid of ePSF models.
        '''

    def _reshape_grid(self, data):
        '''
        Reshape the 3D ePSF grid as a 2D array of horizontally and
        vertically stacked ePSFs.
        Parameters
        ----------
        data : `numpy.ndarray`
            The 3D array of ePSF data.
        Returns
        -------
        reshaped_data : `numpy.ndarray`
            The 2D array of ePSF data.
        '''
        pass

    def plot_grid(self, *, ax=None, vmax_scale=None, peak_norm=False, deltas=False, cmap='viridis', dividers=True, divider_color='darkgray', divider_ls='-', figsize=None):
        '''
        Plot the grid of ePSF models.
        Parameters
        ----------
        ax : `matplotlib.axes.Axes` or `None`, optional
            The matplotlib axes on which to plot. If `None`, then the
            current `~matplotlib.axes.Axes` instance is used.
        vmax_scale : float, optional
            Scale factor to apply to the image stretch limits. This
            value is multiplied by the peak ePSF value to determine the
            plotting ``vmax``. The defaults are 1.0 for plotting the
            ePSF data and 0.03 for plotting the ePSF difference data
            (``deltas=True``). If ``deltas=True``, the ``vmin`` is set
            to ``-vmax``. If ``deltas=False`` the ``vmin`` is set to
            ``vmax`` / 1e4.
        peak_norm : bool, optional
            Whether to normalize the ePSF data by the peak value. The
            default shows the ePSF flux per pixel.
        deltas : bool, optional
            Set to `True` to show the differences between each ePSF
            and the average ePSF.
        cmap : str or `matplotlib.colors.Colormap`, optional
            The colormap to use. The default is 'viridis'.
        dividers : bool, optional
            Whether to show divider lines between the ePSFs.
        divider_color, divider_ls : str, optional
            Matplotlib color and linestyle options for the divider
            lines between ePSFs. These keywords have no effect unless
            ``show_dividers=True``.
        figsize : (float, float), optional
            The figure (width, height) in inches.
        Returns
        -------
        fig : `matplotlib.figure.Figure`
            The matplotlib figure object. This will be the current
            figure if ``ax=None``. Use ``fig.savefig()`` to save the
            figure to a file.
        Notes
        -----
        This method returns a figure object. If you are using this
        method in a script, you will need to call ``plt.show()`` to
        display the figure. If you are using this method in a Jupyter
        notebook, the figure will be displayed automatically.
        When in a notebook, if you do not store the return value of this
        function, the figure will be displayed twice due to the REPL
        automatically displaying the return value of the last function
        call. Alternatively, you can append a semicolon to the end of
        the function call to suppress the display of the return value.
                        '''
                        pass",snippet_143,"class ModelGridPlotMixin:
    '''
    Mixin class to plot a grid of ePSF models.
    '''

    def _reshape_grid(self, data):
        '''
        Reshape the 3D ePSF grid as a 2D array of horizontally and
        vertically stacked ePSFs.
        Parameters
        ----------
        data : `numpy.ndarray`
            The 3D array of ePSF data.
        Returns
        -------
        reshaped_data : `numpy.ndarray`
            The 2D array of ePSF data.
        '''
        import numpy as np

        if data is None:
            raise ValueError(""data must not be None"")
        arr = np.asarray(data)
        if arr.ndim != 3:
            raise ValueError(""data must be a 3D array with shape (n, ny, nx)"")

        n, ny, nx = arr.shape

        def best_grid(nmodels):
            import math
            # choose rows as the largest divisor of n <= sqrt(n)
            r = int(math.floor(math.sqrt(nmodels)))
            while r > 1 and nmodels % r != 0:
                r -= 1
            rows = r if nmodels % r == 0 else 1
            cols = nmodels // rows
            return rows, cols

        rows, cols = best_grid(n)
        # pad with zeros if needed to fill last row
        total = rows * cols
        if total != n:
            pad = total - n
            pad_block = np.zeros((pad, ny, nx), dtype=arr.dtype)
            arr = np.concatenate((arr, pad_block), axis=0)

        # reshape and tile
        grid = arr.reshape(rows, cols, ny, nx)
        out = np.block([[grid[i, j] for j in range(cols)] for i in range(rows)])
        return out

    def plot_grid(self, *, ax=None, vmax_scale=None, peak_norm=False, deltas=False, cmap='viridis', dividers=True, divider_color='darkgray', divider_ls='-', figsize=None):
        '''
        Plot the grid of ePSF models.
        Parameters
        ----------
        ax : `matplotlib.axes.Axes` or `None`, optional
            The matplotlib axes on which to plot. If `None`, then the
            current `~matplotlib.axes.Axes` instance is used.
        vmax_scale : float, optional
            Scale factor to apply to the image stretch limits. This
            value is multiplied by the peak ePSF value to determine the
            plotting ``vmax``. The defaults are 1.0 for plotting the
            ePSF data and 0.03 for plotting the ePSF difference data
            (``deltas=True``). If ``deltas=True``, the ``vmin`` is set
            to ``-vmax``. If ``deltas=False`` the ``vmin`` is set to
            ``vmax`` / 1e4.
        peak_norm : bool, optional
            Whether to normalize the ePSF data by the peak value. The
            default shows the ePSF flux per pixel.
        deltas : bool, optional
            Set to `True` to show the differences between each ePSF
            and the average ePSF.
        cmap : str or `matplotlib.colors.Colormap`, optional
            The colormap to use. The default is 'viridis'.
        dividers : bool, optional
            Whether to show divider lines between the ePSFs.
        divider_color, divider_ls : str, optional
            Matplotlib color and linestyle options for the divider
            lines between ePSFs. These keywords have no effect unless
            ``show_dividers=True``.
        figsize : (float, float), optional
            The figure (width, height) in inches.
        Returns
        -------
        fig : `matplotlib.figure.Figure`
            The matplotlib figure object. This will be the current
            figure if ``ax=None``. Use ``fig.savefig()`` to save the
            figure to a file.
        Notes
        -----
        This method returns a figure object. If you are using this
        method in a script, you will need to call ``plt.show()`` to
        display the figure. If you are using this method in a Jupyter
        notebook, the figure will be displayed automatically.
        When in a notebook, if you do not store the return value of this
        function, the figure will be displayed twice due to the REPL
        automatically displaying the return value of the last function
        call. Alternatively, you can append a semicolon to the end of
        the function call to suppress the display of the return value.
        '''
        import numpy as np
        import matplotlib.pyplot as plt

        def get_grid_data(obj):
            # Try common attribute/method names
            for name in ('data', '_data', 'grid', 'array', 'models'):
                if hasattr(obj, name):
                    val = getattr(obj, name)
                    val = val() if callable(val) else val
                    if val is not None:
                        return np.asarray(val)
            raise AttributeError(""Could not find grid data on the object. Expected an attribute like 'data'."")

        data = get_grid_data(self)
        data = np.asarray(data)
        if data.ndim != 3:
            raise ValueError(""Grid data must be a 3D array with shape (n, ny, nx)"")

        n, ny, nx = data.shape

        if peak_norm:
            peaks = np.nanmax(np.where(np.isfinite(data), data, -np.inf), axis=(1, 2))
            peaks[~np.isfinite(peaks)] = 1.0
            peaks[peaks == 0] = 1.0
            data = data / peaks[:, None, None]

        if deltas:
            avg = np.nanmean(data, axis=0)
            data = data - avg

        if vmax_scale is None:
            vmax_scale = 0.03 if deltas else 1.0

        if deltas:
            peak = np.nanmax(np.abs(data))
            vmax = vmax_scale * (peak if np.isfinite(peak) and peak > 0 else 1.0)
            vmin = -vmax
        else:
            peak = np.nanmax(data)
            vmax = vmax_scale * (peak if np.isfinite(peak) and peak > 0 else 1.0)
            vmin = vmax / 1e4

        # Determine rows/cols used for tiling (must match _reshape_grid)
        def best_grid(nmodels):
            import math
            r = int(math.floor(math.sqrt(nmodels)))
            while r > 1 and nmodels % r != 0:
                r -= 1
            rows = r if nmodels % r == 0 else 1
            cols = nmodels // rows
            return rows, cols

        rows, cols = best_grid(n)
        total = rows * cols
        pad = 0
        if total != n:
            pad = total - n
            pad_block = np.zeros((pad, ny, nx), dtype=data.dtype)
            plot_data = np.concatenate((data, pad_block), axis=0)
        else:
            plot_data = data

        tiled = self._reshape_grid(plot_data)

        if ax is None:
            fig, ax = plt.subplots(1, 1, figsize=figsize)
        else:
            fig = ax.figure

        im = ax.imshow(tiled, origin='lower', cmap=cmap, vmin=vmin, vmax=vmax, interpolation='nearest')
        ax.set_xticks([])
        ax.set_yticks([])

        if dividers:
            # draw lines between stamps
            for r in range(1, rows):
                y = r * ny
                ax.hlines(y - 0.5, -0.5, cols * nx - 0.5, colors=divider_color, linestyles=divider_ls, linewidth=0.8)
            for c in range(1, cols):
                x = c * nx
                ax.vlines(x - 0.5, -0.5, rows * ny - 0.5, colors=divider_color, linestyles=divider_ls, linewidth=0.8)

        fig.tight_layout()
        return fig"
237588,matthew-brett/delocate,matthew-brett_delocate/delocate/tmpdirs.py,delocate.tmpdirs.InGivenDirectory,"import os

class InGivenDirectory:
    """"""Change directory to given directory for duration of ``with`` block.

    Useful when you want to use `InTemporaryDirectory` for the final test, but
    you are still debugging.  For example, you may want to do this in the end:

    >>> with InTemporaryDirectory() as tmpdir:
    ...     # do something complicated which might break
    ...     pass

    But indeed the complicated thing does break, and meanwhile the
    ``InTemporaryDirectory`` context manager wiped out the directory with the
    temporary files that you wanted for debugging.  So, while debugging, you
    replace with something like:

    >>> with InGivenDirectory() as tmpdir: # Use working directory by default
    ...     # do something complicated which might break
    ...     pass

    You can then look at the temporary file outputs to debug what is happening,
    fix, and finally replace ``InGivenDirectory`` with ``InTemporaryDirectory``
    again.
    """"""

    def __init__(self, path=None):
        """"""Initialize directory context manager.

        Parameters
        ----------
        path : None or str, optional
            path to change directory to, for duration of ``with`` block.
            Defaults to ``os.getcwd()`` if None
        """"""
        if path is None:
            path = os.getcwd()
        self.path = os.path.abspath(path)

    def __enter__(self):
        """"""Chdir to the managed directory, creating it if needed.""""""
        self._pwd = os.path.abspath(os.getcwd())
        if not os.path.isdir(self.path):
            os.mkdir(self.path)
        os.chdir(self.path)
        return self.path

    def __exit__(self, exc, value, tb):
        """"""Revert the working directory.""""""
        os.chdir(self._pwd)","
class InGivenDirectory:
    '''Change directory to given directory for duration of ``with`` block.
    Useful when you want to use `InTemporaryDirectory` for the final test, but
    you are still debugging.  For example, you may want to do this in the end:
    >>> with InTemporaryDirectory() as tmpdir:
    ...     # do something complicated which might break
    ...     pass
    But indeed the complicated thing does break, and meanwhile the
    ``InTemporaryDirectory`` context manager wiped out the directory with the
    temporary files that you wanted for debugging.  So, while debugging, you
    replace with something like:
    >>> with InGivenDirectory() as tmpdir: # Use working directory by default
    ...     # do something complicated which might break
    ...     pass
    You can then look at the temporary file outputs to debug what is happening,
    fix, and finally replace ``InGivenDirectory`` with ``InTemporaryDirectory``
    again.
    '''

    def __init__(self, path=None):
        '''Initialize directory context manager.
        Parameters
        ----------
        path : None or str, optional
            path to change directory to, for duration of ``with`` block.
            Defaults to ``os.getcwd()`` if None
        '''
        pass

    def __enter__(self):
        '''Chdir to the managed directory, creating it if needed.'''
        pass

    def __exit__(self, exc, value, tb):
        '''Revert the working directory.'''
        pass",snippet_144,"import os

class InGivenDirectory:
    '''Change directory to given directory for duration of ``with`` block.
    Useful when you want to use `InTemporaryDirectory` for the final test, but
    you are still debugging.  For example, you may want to do this in the end:
    >>> with InTemporaryDirectory() as tmpdir:
    ...     # do something complicated which might break
    ...     pass
    But indeed the complicated thing does break, and meanwhile the
    ``InTemporaryDirectory`` context manager wiped out the directory with the
    temporary files that you wanted for debugging.  So, while debugging, you
    replace with something like:
    >>> with InGivenDirectory() as tmpdir: # Use working directory by default
    ...     # do something complicated which might break
    ...     pass
    You can then look at the temporary file outputs to debug what is happening,
    fix, and finally replace ``InGivenDirectory`` with ``InTemporaryDirectory``
    again.
    '''

    def __init__(self, path=None):
        '''Initialize directory context manager.
        Parameters
        ----------
        path : None or str, optional
            path to change directory to, for duration of ``with`` block.
            Defaults to ``os.getcwd()`` if None
        '''
        if path is None:
            path = os.getcwd()
        self.path = os.path.abspath(path)
        self._orig_dir = None

    def __enter__(self):
        '''Chdir to the managed directory, creating it if needed.'''
        self._orig_dir = os.getcwd()
        if not os.path.isdir(self.path):
            os.makedirs(self.path)
        os.chdir(self.path)
        return self.path

    def __exit__(self, exc, value, tb):
        '''Revert the working directory.'''
        if self._orig_dir is not None:
            try:
                os.chdir(self._orig_dir)
            finally:
                self._orig_dir = None
        return False"
239516,blink1073/oct2py,oct2py/dynamic.py,oct2py.dynamic._MethodDocDescriptor,"class _MethodDocDescriptor:
    """"""An object that dynamically fetches the documentation
    for an Octave user class method.
    """"""

    def __init__(self, session_weakref, class_name, name):
        """"""Initialize the descriptor.""""""
        self.ref = session_weakref
        self.class_name = class_name
        self.name = name
        self.doc = None

    def __get__(self, instance, owner=None):
        """"""Get the documentation.""""""
        if self.doc is not None:
            return self.doc
        session = self.ref()
        class_name = self.class_name
        method = self.name
        doc = session._get_doc(f'@{class_name}/{method}')
        self.doc = doc or session._get_doc(method)
        return self.doc","class _MethodDocDescriptor:
    '''An object that dynamically fetches the documentation
    for an Octave user class method.
        '''

    def __init__(self, session_weakref, class_name, name):
        '''Initialize the descriptor.'''
        pass

    def __get__(self, instance, owner=None):
        '''Get the documentation.'''
        pass",snippet_145,"class _MethodDocDescriptor:
    '''An object that dynamically fetches the documentation
    for an Octave user class method.
        '''

    def __init__(self, session_weakref, class_name, name):
        '''Initialize the descriptor.'''
        self._session_ref = session_weakref
        self._class_name = class_name
        self._name = name
        self._doc_cache = None

    def __get__(self, instance, owner=None):
        '''Get the documentation.'''
        if self._doc_cache is not None:
            return self._doc_cache

        session = self._session_ref() if self._session_ref is not None else None
        doc = None

        # Try to ask the session for the documentation via several possible methods.
        if session is not None:
            candidates = [
                (""get_class_method_doc"", (self._class_name, self._name), {}),
                (""get_method_doc"", (self._class_name, self._name), {}),
                (""get_octave_method_doc"", (self._class_name, self._name), {}),
                (""get_doc"", (f""{self._class_name}.{self._name}"",), {}),
                (""get_doc"", (self._name,), {}),
            ]
            for meth_name, args, kwargs in candidates:
                try:
                    meth = getattr(session, meth_name, None)
                    if callable(meth):
                        result = meth(*args, **kwargs)
                        if result:
                            doc = str(result)
                            break
                except Exception:
                    # Ignore and try next candidate
                    pass

        # Fallback: try Python attribute doc if accessible
        if not doc:
            try:
                target = None
                if instance is not None:
                    target = getattr(instance, self._name, None)
                elif owner is not None:
                    target = getattr(owner, self._name, None)
                if target is not None:
                    doc_attr = getattr(target, ""__doc__"", None)
                    if doc_attr:
                        doc = str(doc_attr)
            except Exception:
                pass

        if doc is None:
            doc = """"

        self._doc_cache = doc
        return self._doc_cache"
244777,markokr/rarfile,markokr_rarfile/rarfile.py,rarfile.NoHashContext,"class NoHashContext:
    """"""No-op hash function.""""""

    def __init__(self, data=None):
        """"""Initialize""""""

    def update(self, data):
        """"""Update data""""""

    def digest(self):
        """"""Final hash""""""

    def hexdigest(self):
        """"""Hexadecimal digest.""""""","class NoHashContext:
    '''No-op hash function.'''

    def __init__(self, data=None):
        '''Initialize'''
        pass

    def update(self, data):
        '''Update data'''
        pass

    def digest(self):
        '''Final hash'''
        pass

    def hexdigest(self):
        '''Hexadecimal digest.'''
        pass",snippet_147,"class NoHashContext:
    '''No-op hash function.'''

    def __init__(self, data=None):
        '''Initialize'''
        self._data = bytearray()
        if data is not None:
            self.update(data)

    def update(self, data):
        '''Update data'''
        if data is None:
            return
        if isinstance(data, (bytes, bytearray, memoryview)):
            self._data.extend(bytes(data))
        elif isinstance(data, str):
            self._data.extend(data.encode('utf-8'))
        else:
            try:
                self._data.extend(memoryview(data).tobytes())
            except TypeError:
                raise TypeError(""data must be bytes-like or str"")

    def digest(self):
        '''Final hash'''
        return bytes(self._data)

    def hexdigest(self):
        '''Hexadecimal digest.'''
        return self._data.hex()"
244940,meejah/txtorcon,meejah_txtorcon/txtorcon/addrmap.py,txtorcon.addrmap.Addr,"import datetime
from txtorcon.util import maybe_ip_addr

class Addr:
    """"""
    One address mapping (e.g. example.com -> 127.0.0.1)
    """"""

    def __init__(self, map):
        """"""
        map is an AddrMap instance, used for scheduling expiries and
        updating the map.
        """"""
        self.map = map
        self.ip = None
        self.name = None
        self.expiry = None
        self.expires = None
        self.created = None

    def update(self, *args):
        """"""
        deals with an update from Tor; see parsing logic in torcontroller
        """"""
        gmtexpires = None
        name, ip, expires = args[:3]
        for arg in args:
            if arg.lower().startswith('expires='):
                gmtexpires = arg[8:]
        if gmtexpires is None:
            if len(args) == 3:
                gmtexpires = expires
            elif args[2] == 'NEVER':
                gmtexpires = args[2]
            else:
                gmtexpires = args[3]
        self.name = name
        self.ip = maybe_ip_addr(ip)
        if self.ip == '<error>':
            self._expire()
            return
        fmt = '%Y-%m-%d %H:%M:%S'
        oldexpires = self.expires
        if gmtexpires.upper() == 'NEVER':
            self.expires = None
        else:
            self.expires = datetime.datetime.strptime(gmtexpires, fmt)
        self.created = datetime.datetime.utcnow()
        if self.expires is not None:
            if oldexpires is None:
                if self.expires <= self.created:
                    diff = datetime.timedelta(seconds=0)
                else:
                    diff = self.expires - self.created
                self.expiry = self.map.scheduler.callLater(diff.seconds, self._expire)
            else:
                diff = self.expires - oldexpires
                self.expiry.delay(diff.seconds)

    def _expire(self):
        """"""
        callback done via callLater
        """"""
        del self.map.addr[self.name]
        self.map.notify('addrmap_expired', *[self.name], **{})","
class Addr:
    '''
    One address mapping (e.g. example.com -> 127.0.0.1)
    '''

    def __init__(self, map):
        '''
        map is an AddrMap instance, used for scheduling expiries and
        updating the map.
        '''
        pass

    def update(self, *args):
        '''
        deals with an update from Tor; see parsing logic in torcontroller
        '''
        pass

    def _expire(self):
        '''
        callback done via callLater
        '''
        pass",snippet_148,"class Addr:
    '''
    One address mapping (e.g. example.com -> 127.0.0.1)
    '''

    def __init__(self, map):
        '''
        map is an AddrMap instance, used for scheduling expiries and
        updating the map.
        '''
        self.map = map
        self.name = None
        self.mapped = None
        self.cached = False
        self.permanent = False
        self.unknown = False
        self.expires = None  # epoch seconds
        self._timer = None

    def update(self, *args):
        '''
        deals with an update from Tor; see parsing logic in torcontroller
        '''
        # Flatten args to a simple list of tokens
        if len(args) == 1 and isinstance(args[0], (list, tuple)):
            tokens = list(args[0])
        else:
            tokens = list(args)

        if not tokens:
            return

        # Minimum expected: name [mapped]
        self.name = tokens[0] if len(tokens) >= 1 else self.name
        self.mapped = tokens[1] if len(tokens) >= 2 else self.mapped

        # Reset flags for a fresh update
        self.cached = False
        self.permanent = False
        self.unknown = False
        new_expires = None

        # Parse trailing flags/kv pairs
        for tok in tokens[2:]:
            if not isinstance(tok, str):
                continue
            up = tok.upper()
            if up == 'CACHED':
                self.cached = True
            elif up == 'NEVER':
                # Explicit never-expire
                self.permanent = True
                new_expires = None
            elif up == 'UNKNOWN':
                self.unknown = True
            elif up.startswith('EXPIRES='):
                val = tok.split('=', 1)[1]
                if val.upper() == 'NEVER':
                    self.permanent = True
                    new_expires = None
                else:
                    parsed = self._parse_expiry(val)
                    if parsed is not None:
                        new_expires = parsed

        # Apply expiry scheduling
        self._cancel_timer()

        if self.permanent:
            self.expires = None
            return

        if new_expires is not None:
            self.expires = new_expires
        # If no new expiry provided, retain current expires if any.

        if self.expires is not None:
            import time
            delay = max(0.0, self.expires - time.time())
            if delay == 0.0:
                self._expire()
            else:
                self._schedule_timer(delay)

    def _expire(self):
        '''
        callback done via callLater
        '''
        self._cancel_timer()
        # On expiry, remove mapping and notify the map if possible
        if hasattr(self.map, 'remove') and self.name is not None:
            try:
                self.map.remove(self.name)
            except Exception:
                pass
        # Clear state
        self.mapped = None
        self.expires = None
        self.cached = False
        self.permanent = False
        self.unknown = False

    # Internal helpers

    def _parse_expiry(self, val):
        # Try UNIX epoch integer/float
        try:
            return float(val)
        except Exception:
            pass
        # Try ISO-like or Tor style ""YYYY-MM-DD HH:MM:SS""
        from datetime import datetime, timezone
        fmts = [
            ""%Y-%m-%d %H:%M:%S"",
            ""%Y-%m-%dT%H:%M:%S"",
            ""%Y-%m-%d %H:%M:%S.%f"",
            ""%Y-%m-%dT%H:%M:%S.%fZ"",
            ""%Y-%m-%dT%H:%M:%S%z"",
        ]
        for fmt in fmts:
            try:
                dt = datetime.strptime(val, fmt)
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=timezone.utc)
                return dt.timestamp()
            except Exception:
                continue
        return None

    def _schedule_timer(self, delay):
        # Prefer map.callLater if available (Twisted-style), else fallback
        if hasattr(self.map, 'callLater') and callable(getattr(self.map, 'callLater')):
            try:
                self._timer = self.map.callLater(delay, self._expire)
                return
            except Exception:
                self._timer = None
        try:
            import threading
            t = threading.Timer(delay, self._expire)
            t.daemon = True
            t.start()
            self._timer = t
        except Exception:
            self._timer = None

    def _cancel_timer(self):
        if self._timer is None:
            return
        try:
            # Twisted-style DelayedCall
            if hasattr(self._timer, 'active') and self._timer.active():
                self._timer.cancel()
            # threading.Timer
            elif hasattr(self._timer, 'cancel'):
                self._timer.cancel()
        except Exception:
            pass
        finally:
            self._timer = None"
245022,meejah/txtorcon,meejah_txtorcon/txtorcon/torconfig.py,txtorcon.torconfig.TorConfigType,"class TorConfigType:
    """"""
    Base class for all configuration types, which function as parsers
    and un-parsers.
    """"""

    def parse(self, s):
        """"""
        Given the string s, this should return a parsed representation
        of it.
        """"""
        return s

    def validate(self, s, instance, name):
        """"""
        If s is not a valid type for this object, an exception should
        be thrown. The validated object should be returned.
        """"""
        return s","class TorConfigType:
    '''
    Base class for all configuration types, which function as parsers
    and un-parsers.
    '''

    def parse(self, s):
        '''
        Given the string s, this should return a parsed representation
        of it.
        '''
        pass

    def validate(self, s, instance, name):
        '''
        If s is not a valid type for this object, an exception should
        be thrown. The validated object should be returned.
        '''
        pass",snippet_149,"class TorConfigType:
    '''
    Base class for all configuration types, which function as parsers
    and un-parsers.
    '''

    def parse(self, s):
        '''
        Given the string s, this should return a parsed representation
        of it.
        '''
        if not isinstance(s, str):
            raise TypeError(f""parse expects a string, got {type(s).__name__}"")
        return s

    def validate(self, s, instance, name):
        '''
        If s is not a valid type for this object, an exception should
        be thrown. The validated object should be returned.
        '''
        if isinstance(s, str):
            return self.parse(s)
        return s"
245057,paterva/maltego-trx,paterva_maltego-trx/maltego_trx/oauth.py,maltego_trx.oauth.MaltegoOauth,"from cryptography.hazmat.backends import default_backend
import base64
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
from cryptography.hazmat.primitives import serialization, padding as primitives_padding
from cryptography.hazmat.primitives.asymmetric import padding as asymmetric_padding

class MaltegoOauth:
    """"""
    A Crypto Helper for Maltego OAuth Secrets received from the Transform Distribution Server
    The TDS will send back an encrypted combination of the following :
    1. Token
    2. Token Secret
    3. Refresh Token
    4. Expires In

    Contains Methods:
        1. decrypt_secrets(private_key_path=""pem file"", ciphertext=""request.getTransformSetting('name from TDS')"")
    """"""

    @staticmethod
    def _rsa_decrypt(private_key_path=None, ciphertext=None, password=None):
        """"""
        RSA Decryption function, returns decrypted plaintext in b64 encoding
        """"""
        ciphertext = base64.b64decode(ciphertext)
        with open(private_key_path, 'rb') as key_file:
            private_key = serialization.load_pem_private_key(key_file.read(), password, backend=None)
            plaintext = private_key.decrypt(ciphertext, asymmetric_padding.PKCS1v15())
        return plaintext

    @staticmethod
    def _aes_decrypt(key=None, ciphertext=None):
        """"""
        AES Decryption function, returns decrypted plaintext value
        """"""
        key = base64.b64decode(key)
        cipher = Cipher(algorithms.AES(key), modes.ECB(), backend=default_backend())
        decryptor = cipher.decryptor()
        ciphertext = base64.b64decode(ciphertext)
        padded_b64_plaintext = decryptor.update(ciphertext) + decryptor.finalize()
        unpadder = primitives_padding.PKCS7(128).unpadder()
        plaintext = (unpadder.update(padded_b64_plaintext) + unpadder.finalize()).decode('utf8')
        return plaintext

    @classmethod
    def decrypt_secrets(cls, private_key_path=None, encoded_ciphertext=None):
        """"""
        The TDS will send back an encrypted combination of the following :
        1. Token
        2. Token Secret
        3. Refresh Token
        4. Expires In

        This function decodes the combinations and decrypts as required and returns a dictionary with the following keys
                {""token"":"""",
                ""token_secret"": """",
                ""refresh_token"": """",
                ""expires_in"": """"}
        """"""
        encrypted_fields = encoded_ciphertext.split('$')
        if len(encrypted_fields) == 1:
            token = cls._rsa_decrypt(private_key_path, encrypted_fields[0])
            token_fields = {'token': token}
        elif len(encrypted_fields) == 2:
            token = cls._rsa_decrypt(private_key_path, encrypted_fields[0])
            token_secret = cls._rsa_decrypt(private_key_path, encrypted_fields[1])
            token_fields = {'token': token, 'token_secret': token_secret}
        elif len(encrypted_fields) == 3:
            aes_key = cls._rsa_decrypt(private_key_path, encrypted_fields[2])
            token = cls._aes_decrypt(aes_key, encrypted_fields[0])
            token_secret = cls._aes_decrypt(aes_key, encrypted_fields[1])
            token_fields = {'token': token, 'token_secret': token_secret}
        elif len(encrypted_fields) == 4:
            token = cls._rsa_decrypt(private_key_path, encrypted_fields[0])
            token_secret = cls._rsa_decrypt(private_key_path, encrypted_fields[1])
            refresh_token = cls._rsa_decrypt(private_key_path, encrypted_fields[2])
            expires_in = cls._rsa_decrypt(private_key_path, encrypted_fields[3])
            token_fields = {'token': token, 'token_secret': token_secret, 'refresh_token': refresh_token, 'expires_in': expires_in}
        elif len(encrypted_fields) == 5:
            aes_key = cls._rsa_decrypt(private_key_path, encrypted_fields[4])
            token = cls._aes_decrypt(aes_key, encrypted_fields[0])
            token_secret = cls._aes_decrypt(aes_key, encrypted_fields[1])
            refresh_token = cls._aes_decrypt(aes_key, encrypted_fields[2])
            expires_in = cls._aes_decrypt(aes_key, encrypted_fields[3])
            token_fields = {'token': token, 'token_secret': token_secret, 'refresh_token': refresh_token, 'expires_in': expires_in}
        else:
            token_fields = {'token': '', 'token_secret': '', 'refresh_token': '', 'expires_in': ''}
        return token_fields","
class MaltegoOauth:
    '''
    A Crypto Helper for Maltego OAuth Secrets received from the Transform Distribution Server
    The TDS will send back an encrypted combination of the following :
    1. Token
    2. Token Secret
    3. Refresh Token
    4. Expires In
    Contains Methods:
        1. decrypt_secrets(private_key_path=""pem file"", ciphertext=""request.getTransformSetting('name from TDS')"")
    '''
    @staticmethod
    def _rsa_decrypt(private_key_path=None, ciphertext=None, password=None):
        '''
        RSA Decryption function, returns decrypted plaintext in b64 encoding
        '''
        pass
    @staticmethod
    def _aes_decrypt(key=None, ciphertext=None):
        '''
        AES Decryption function, returns decrypted plaintext value
        '''
        pass
    @classmethod
    def decrypt_secrets(cls, private_key_path=None, encoded_ciphertext=None):
        '''
        The TDS will send back an encrypted combination of the following :
        1. Token
        2. Token Secret
        3. Refresh Token
        4. Expires In
        This function decodes the combinations and decrypts as required and returns a dictionary with the following keys
                {""token"":"""",
                ""token_secret"": """",
                ""refresh_token"": """",
                ""expires_in"": """"}
        '''
        pass",snippet_150,"import base64
import json
from typing import Optional, Dict, Any

from cryptography.hazmat.primitives import serialization, hashes, padding
from cryptography.hazmat.primitives.asymmetric import padding as asy_padding
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes


class MaltegoOauth:
    '''
    A Crypto Helper for Maltego OAuth Secrets received from the Transform Distribution Server
    The TDS will send back an encrypted combination of the following :
    1. Token
    2. Token Secret
    3. Refresh Token
    4. Expires In
    Contains Methods:
        1. decrypt_secrets(private_key_path=""pem file"", ciphertext=""request.getTransformSetting('name from TDS')"")
    '''
    @staticmethod
    def _b64decode(data: Optional[str]) -> bytes:
        if data is None:
            raise ValueError(""Empty data for base64 decode"")
        if isinstance(data, bytes):
            b = data
        else:
            s = data.strip()
            # Try standard b64 with automatic padding
            missing = (-len(s) % 4)
            s_padded = s + (""="" * missing)
            try:
                return base64.b64decode(s_padded, validate=False)
            except Exception:
                # Try urlsafe
                return base64.urlsafe_b64decode(s_padded)
        return b

    @staticmethod
    def _rsa_decrypt(private_key_path=None, ciphertext=None, password=None):
        '''
        RSA Decryption function, returns decrypted plaintext in b64 encoding
        '''
        if not private_key_path:
            raise ValueError(""private_key_path is required"")
        if ciphertext is None:
            raise ValueError(""ciphertext is required"")

        with open(private_key_path, ""rb"") as f:
            pem = f.read()

        if isinstance(password, str):
            password_bytes = password.encode(""utf-8"")
        else:
            password_bytes = password

        private_key = serialization.load_pem_private_key(pem, password=password_bytes)

        ct_bytes = MaltegoOauth._b64decode(ciphertext)

        try:
            pt = private_key.decrypt(
                ct_bytes,
                asy_padding.OAEP(
                    mgf=asy_padding.MGF1(algorithm=hashes.SHA256()),
                    algorithm=hashes.SHA256(),
                    label=None,
                ),
            )
        except Exception:
            # Fallback to SHA1 OAEP (legacy)
            pt = private_key.decrypt(
                ct_bytes,
                asy_padding.OAEP(
                    mgf=asy_padding.MGF1(algorithm=hashes.SHA1()),
                    algorithm=hashes.SHA1(),
                    label=None,
                ),
            )
        # Return plaintext in b64 encoding as per docstring
        return base64.b64encode(pt)

    @staticmethod
    def _aes_decrypt(key=None, ciphertext=None):
        '''
        AES Decryption function, returns decrypted plaintext value
        '''
        if key is None or ciphertext is None:
            raise ValueError(""key and ciphertext are required"")

        if isinstance(key, str):
            key_bytes = base64.b64decode(key + ""==="") if all(c.isalnum() or c in ""+/=_-"" for c in key) else key.encode(""utf-8"")
            # If it doesn't look like b64, assume utf-8 bytes
            if len(key_bytes) not in (16, 24, 32):
                key_bytes = key.encode(""utf-8"")
        elif isinstance(key, bytes):
            key_bytes = key
        else:
            raise TypeError(""key must be str or bytes"")

        ct_bytes = MaltegoOauth._b64decode(ciphertext)

        if len(ct_bytes) < 16:
            raise ValueError(""ciphertext is too short, missing IV"")
        iv = ct_bytes[:16]
        ct = ct_bytes[16:]

        cipher = Cipher(algorithms.AES(key_bytes), modes.CBC(iv))
        decryptor = cipher.decryptor()
        padded_plain = decryptor.update(ct) + decryptor.finalize()

        unpadder = padding.PKCS7(128).unpadder()
        plain = unpadder.update(padded_plain) + unpadder.finalize()
        return plain

    @classmethod
    def decrypt_secrets(cls, private_key_path=None, encoded_ciphertext=None):
        '''
        The TDS will send back an encrypted combination of the following :
        1. Token
        2. Token Secret
        3. Refresh Token
        4. Expires In
        This function decodes the combinations and decrypts as required and returns a dictionary with the following keys
                {""token"":"""",
                ""token_secret"": """",
                ""refresh_token"": """",
                ""expires_in"": """"}
        '''
        if not private_key_path:
            raise ValueError(""private_key_path is required"")
        if not encoded_ciphertext:
            raise ValueError(""encoded_ciphertext is required"")

        # Decode outer wrapper (allow raw JSON or b64-encoded JSON)
        raw_bytes: Optional[bytes] = None
        try:
            raw_bytes = cls._b64decode(encoded_ciphertext)
            try:
                payload = json.loads(raw_bytes.decode(""utf-8""))
            except Exception:
                # If base64 decode did not yield JSON, try direct JSON
                payload = json.loads(encoded_ciphertext)
        except Exception:
            payload = json.loads(encoded_ciphertext)

        if not isinstance(payload, dict):
            raise ValueError(""Invalid ciphertext payload format"")

        # Expected payload: {""ek"": <b64 RSA-encrypted AES key>, ""ct"": <b64 AES(iv||ciphertext)>}
        ek = payload.get(""ek"") or payload.get(""encrypted_key"") or payload.get(""key"")
        ct = payload.get(""ct"") or payload.get(""ciphertext"") or payload.get(""data"")

        if not ek or not ct:
            raise ValueError(""Payload must contain 'ek' and 'ct'"")

        # RSA decrypt to get AES key (function returns b64 plaintext)
        aes_key_b64 = cls._rsa_decrypt(private_key_path=private_key_path, ciphertext=ek)
        aes_key = base64.b64decode(aes_key_b64)

        # AES decrypt to get JSON secrets
        secrets_bytes = cls._aes_decrypt(key=aes_key, ciphertext=ct)
        try:
            secrets = json.loads(secrets_bytes.decode(""utf-8""))
        except Exception:
            # Fallback: parse pipe-separated string in fixed order
            parts = secrets_bytes.decode(""utf-8"", errors=""ignore"").split(""|"")
            secrets = {}
            if len(parts) >= 1:
                secrets[""token""] = parts[0]
            if len(parts) >= 2:
                secrets[""token_secret""] = parts[1]
            if len(parts) >= 3:
                secrets[""refresh_token""] = parts[2]
            if len(parts) >= 4:
                secrets[""expires_in""] = parts[3]

        # Normalize output keys
        result: Dict[str, Any] = {
            ""token"": secrets.get(""token"", """"),
            ""token_secret"": secrets.get(""token_secret"", secrets.get(""tokenSecret"", """")),
            ""refresh_token"": secrets.get(""refresh_token"", secrets.get(""refreshToken"", """")),
            ""expires_in"": str(secrets.get(""expires_in"", secrets.get(""expiresIn"", """"))),
        }
        return result"
247043,CiscoDevNet/webexteamssdk,src/webexpythonsdk/models/cards/adaptive_card_component.py,webexpythonsdk.models.cards.adaptive_card_component.AdaptiveCardComponent,"import json
import enum

class AdaptiveCardComponent:
    """"""
    Base class for all Adaptive Card elements.

    Each element should inherit from this class and specify which of its
    properties fall into the following two categories:

    * Simple properties are basic types (int, float, str, etc.).

    * Serializable properties are properties that can themselves be serialized.
      This includes lists of items (i.e. the 'body' field of the adaptive card)
      or single objects that also inherit from Serializable
    """"""

    def __init__(self, serializable_properties, simple_properties):
        """"""
        Initialize a serializable object.

        Args:
            serializable_properties(list): List of all serializable properties
            simple_properties(list): List of all simple properties.
        """"""
        self.serializable_properties = serializable_properties
        self.simple_properties = simple_properties

    def to_dict(self):
        """"""
        Serialize the element into a Python dictionary.

        The to_dict() method recursively serializes the object's data into
        a Python dictionary.

        Returns:
            dict: Dictionary representation of this element.
        """"""
        serialized_data = {}
        for property_name in self.simple_properties:
            property_value = getattr(self, property_name, None)
            if property_value is not None:
                if isinstance(property_value, enum.Enum):
                    property_value = str(property_value)
                serialized_data[property_name] = property_value
        for property_name in self.serializable_properties:
            property_value = getattr(self, property_name, None)
            if property_value is not None:
                if isinstance(property_value, list):
                    serialized_data[property_name] = [item.to_dict() if hasattr(item, 'to_dict') else item for item in property_value]
                else:
                    serialized_data[property_name] = property_value.to_dict()
        return serialized_data

    def to_json(self, **kwargs):
        """"""
        Serialize the element into JSON text.

        Any keyword arguments provided are passed through the Python JSON
        encoder.
        """"""
        return json.dumps(self.to_dict(), **kwargs)","
class AdaptiveCardComponent:
    '''
    Base class for all Adaptive Card elements.
    Each element should inherit from this class and specify which of its
    properties fall into the following two categories:
    * Simple properties are basic types (int, float, str, etc.).
    * Serializable properties are properties that can themselves be serialized.
      This includes lists of items (i.e. the 'body' field of the adaptive card)
      or single objects that also inherit from Serializable
    '''

    def __init__(self, serializable_properties, simple_properties):
        '''
        Initialize a serializable object.
        Args:
            serializable_properties(list): List of all serializable properties
            simple_properties(list): List of all simple properties.
        '''
        pass

    def to_dict(self):
        '''
        Serialize the element into a Python dictionary.
        The to_dict() method recursively serializes the object's data into
        a Python dictionary.
        Returns:
            dict: Dictionary representation of this element.
        '''
        pass

    def to_json(self, **kwargs):
        '''
        Serialize the element into JSON text.
        Any keyword arguments provided are passed through the Python JSON
        encoder.
        '''
        pass",snippet_151,"import json
from collections.abc import Iterable

class AdaptiveCardComponent:
    '''
    Base class for all Adaptive Card elements.
    Each element should inherit from this class and specify which of its
    properties fall into the following two categories:
    * Simple properties are basic types (int, float, str, etc.).
    * Serializable properties are properties that can themselves be serialized.
      This includes lists of items (i.e. the 'body' field of the adaptive card)
      or single objects that also inherit from Serializable
    '''

    def __init__(self, serializable_properties, simple_properties):
        '''
        Initialize a serializable object.
        Args:
            serializable_properties(list): List of all serializable properties
            simple_properties(list): List of all simple properties.
        '''
        self._serializable_properties = list(serializable_properties or [])
        self._simple_properties = list(simple_properties or [])

    def _serialize_value(self, value):
        if value is None:
            return None
        if hasattr(value, ""to_dict"") and callable(getattr(value, ""to_dict"")):
            return value.to_dict()
        if isinstance(value, (list, tuple)):
            serialized_list = []
            for item in value:
                if item is None:
                    serialized_list.append(None)
                elif hasattr(item, ""to_dict"") and callable(getattr(item, ""to_dict"")):
                    serialized_list.append(item.to_dict())
                else:
                    serialized_list.append(item)
            return serialized_list
        return value

    def to_dict(self):
        '''
        Serialize the element into a Python dictionary.
        The to_dict() method recursively serializes the object's data into
        a Python dictionary.
        Returns:
            dict: Dictionary representation of this element.
        '''
        result = {}

        for prop in self._simple_properties:
            if hasattr(self, prop):
                value = getattr(self, prop)
                if value is not None:
                    result[prop] = value

        for prop in self._serializable_properties:
            if hasattr(self, prop):
                value = getattr(self, prop)
                if value is None:
                    continue
                serialized = self._serialize_value(value)
                result[prop] = serialized

        return result

    def to_json(self, **kwargs):
        '''
        Serialize the element into JSON text.
        Any keyword arguments provided are passed through the Python JSON
        encoder.
        '''
        return json.dumps(self.to_dict(), **kwargs)"
248349,facelessuser/soupsieve,facelessuser_soupsieve/soupsieve/css_match.py,soupsieve.css_match._FakeParent,"import bs4

class _FakeParent:
    """"""
    Fake parent class.

    When we have a fragment with no `BeautifulSoup` document object,
    we can't evaluate `nth` selectors properly.  Create a temporary
    fake parent so we can traverse the root element as a child.
    """"""

    def __init__(self, element: bs4.Tag) -> None:
        """"""Initialize.""""""
        self.contents = [element]

    def __len__(self) -> int:
        """"""Length.""""""
        return len(self.contents)","
class _FakeParent:
    '''
    Fake parent class.
    When we have a fragment with no `BeautifulSoup` document object,
    we can't evaluate `nth` selectors properly.  Create a temporary
    fake parent so we can traverse the root element as a child.
    '''

    def __init__(self, element: bs4.Tag) -> None:
        '''Initialize.'''
        pass

    def __len__(self) -> int:
        '''Length.'''
        pass",snippet_152,"class _FakeParent:
    '''
    Fake parent class.
    When we have a fragment with no `BeautifulSoup` document object,
    we can't evaluate `nth` selectors properly.  Create a temporary
    fake parent so we can traverse the root element as a child.
    '''

    def __init__(self, element: bs4.Tag) -> None:
        '''Initialize.'''
        self.element = element
        self.contents = [element] if element is not None else []

    def __len__(self) -> int:
        '''Length.'''
        return len(self.contents)"
248351,facelessuser/soupsieve,facelessuser_soupsieve/soupsieve/css_parser.py,soupsieve.css_parser.SelectorPattern,"from typing import Match, Any, Iterator, cast
import re

class SelectorPattern:
    """"""Selector pattern.""""""

    def __init__(self, name: str, pattern: str) -> None:
        """"""Initialize.""""""
        self.name = name
        self.re_pattern = re.compile(pattern, re.I | re.X | re.U)

    def get_name(self) -> str:
        """"""Get name.""""""
        return self.name

    def match(self, selector: str, index: int, flags: int) -> Match[str] | None:
        """"""Match the selector.""""""
        return self.re_pattern.match(selector, index)","
class SelectorPattern:
    '''Selector pattern.'''

    def __init__(self, name: str, pattern: str) -> None:
        '''Initialize.'''
        pass

    def get_name(self) -> str:
        '''Get name.'''
        pass

    def match(self, selector: str, index: int, flags: int) -> Match[str] | None:
        '''Match the selector.'''
        pass",snippet_153,"from typing import Match, Optional
import re

class SelectorPattern:
    '''Selector pattern.'''

    def __init__(self, name: str, pattern: str) -> None:
        '''Initialize.'''
        self._name = name
        self._pattern_str = pattern
        self._regex: re.Pattern[str] | None = None
        self._compiled_flags: int | None = None

    def get_name(self) -> str:
        '''Get name.'''
        return self._name

    def match(self, selector: str, index: int, flags: int) -> Match[str] | None:
        '''Match the selector.'''
        if self._regex is None or self._compiled_flags != flags:
            self._regex = re.compile(self._pattern_str, flags)
            self._compiled_flags = flags
        return self._regex.match(selector, pos=index)"
250153,Unidata/siphon,src/siphon/catalog.py,siphon.catalog.CatalogRef,"from urllib.parse import urljoin, urlparse

class CatalogRef:
    """"""
    An object for holding catalog references obtained from a THREDDS Client Catalog.

    Attributes
    ----------
    name : str
        The name of the :class:`CatalogRef` element
    href : str
        url to the :class:`CatalogRef`'s THREDDS Client Catalog
    title : str
        Title of the :class:`CatalogRef` element

    """"""

    def __init__(self, base_url, element_node):
        """"""
        Initialize the catalogRef object.

        Parameters
        ----------
        base_url : str
            URL to the base catalog that owns this reference
        element_node : :class:`~xml.etree.ElementTree.Element`
            An :class:`~xml.etree.ElementTree.Element` representing a catalogRef node

        """"""
        self.title = element_node.attrib['{http://www.w3.org/1999/xlink}title']
        self.name = element_node.attrib.get('name', self.title)
        href = element_node.attrib['{http://www.w3.org/1999/xlink}href']
        self.href = urljoin(base_url, href)

    def __str__(self):
        """"""Return a string representation of the catalog reference.""""""
        return str(self.title)

    def follow(self):
        """"""Follow the catalog reference and return a new :class:`TDSCatalog`.

        Returns
        -------
        TDSCatalog
            The referenced catalog

        """"""
        return TDSCatalog(self.href)
    __repr__ = __str__","
class CatalogRef:
    '''
    An object for holding catalog references obtained from a THREDDS Client Catalog.
    Attributes
    ----------
    name : str
        The name of the :class:`CatalogRef` element
    href : str
        url to the :class:`CatalogRef`'s THREDDS Client Catalog
    title : str
        Title of the :class:`CatalogRef` element
    '''

    def __init__(self, base_url, element_node):
        '''
        Initialize the catalogRef object.
        Parameters
        ----------
        base_url : str
            URL to the base catalog that owns this reference
        element_node : :class:`~xml.etree.ElementTree.Element`
            An :class:`~xml.etree.ElementTree.Element` representing a catalogRef node
        '''
        pass

    def __str__(self):
        '''Return a string representation of the catalog reference.'''
        pass

    def follow(self):
        '''Follow the catalog reference and return a new :class:`TDSCatalog`.
        Returns
        -------
        TDSCatalog
            The referenced catalog
        '''
        pass",snippet_154,"class CatalogRef:
    '''
    An object for holding catalog references obtained from a THREDDS Client Catalog.
    Attributes
    ----------
    name : str
        The name of the CatalogRef element
    href : str
        url to the CatalogRef's THREDDS Client Catalog
    title : str
        Title of the CatalogRef element
    '''

    def __init__(self, base_url, element_node):
        '''
        Initialize the catalogRef object.
        Parameters
        ----------
        base_url : str
            URL to the base catalog that owns this reference
        element_node : xml.etree.ElementTree.Element
            An xml.etree.ElementTree.Element representing a catalogRef node
        '''
        from urllib.parse import urljoin

        if base_url is None:
            base_url = ''

        xlink_ns = 'http://www.w3.org/1999/xlink'
        # Extract attributes with namespace fallbacks
        href = (
            element_node.get(f'{{{xlink_ns}}}href')
            or element_node.get('href')
            or ''
        )
        title = (
            element_node.get(f'{{{xlink_ns}}}title')
            or element_node.get('title')
            or ''
        )
        name = element_node.get('name') or title or ''

        self.href = urljoin(base_url, href) if href else urljoin(base_url, '')
        self.title = title or name or self.href
        self.name = name or self.title

    def __str__(self):
        '''Return a string representation of the catalog reference.'''
        if self.title and self.href:
            return f'{self.title} -> {self.href}'
        return self.href or self.title or ''

    def follow(self):
        '''Follow the catalog reference and return a new TDSCatalog.
        Returns
        -------
        TDSCatalog
            The referenced catalog
        '''
        # Lazy import to avoid circular dependencies
        try:
            from .catalog import TDSCatalog
        except Exception:
            try:
                from siphon.catalog import TDSCatalog
            except Exception as exc:
                raise ImportError('TDSCatalog class could not be imported.') from exc

        return TDSCatalog(self.href)"
250158,Unidata/siphon,src/siphon/catalog.py,siphon.catalog.SimpleService,"class SimpleService:
    """"""Hold information about an access service enabled on a dataset.

    Attributes
    ----------
    name : str
        The name of the service
    service_type : str
        The service type (i.e. ""OPENDAP"", ""NetcdfSubset"", ""WMS"", etc.)
    access_urls : dict[str, str]
        A dictionary of access urls whose keywords are the access service
        types defined in the catalog (for example, ""OPENDAP"", ""NetcdfSubset"",
        ""WMS"", etc.)

    """"""

    def __init__(self, service_node):
        """"""Initialize the Dataset object.

        Parameters
        ----------
        service_node : :class:`~xml.etree.ElementTree.Element`
            An :class:`~xml.etree.ElementTree.Element` representing a service node

        """"""
        self.name = service_node.attrib['name']
        self.service_type = CaseInsensitiveStr(service_node.attrib['serviceType'])
        self.base = service_node.attrib['base']
        self.access_urls = {}

    def is_resolver(self):
        """"""Return whether the service is a resolver service.""""""
        return self.service_type == 'Resolver'","class SimpleService:
    '''Hold information about an access service enabled on a dataset.
    Attributes
    ----------
    name : str
        The name of the service
    service_type : str
        The service type (i.e. ""OPENDAP"", ""NetcdfSubset"", ""WMS"", etc.)
    access_urls : dict[str, str]
        A dictionary of access urls whose keywords are the access service
        types defined in the catalog (for example, ""OPENDAP"", ""NetcdfSubset"",
        ""WMS"", etc.)
    '''

    def __init__(self, service_node):
        '''Initialize the Dataset object.
        Parameters
        ----------
        service_node : :class:`~xml.etree.ElementTree.Element`
            An :class:`~xml.etree.ElementTree.Element` representing a service node
        '''
        pass

    def is_resolver(self):
        '''Return whether the service is a resolver service.'''
        pass",snippet_156,"class SimpleService:
    '''Hold information about an access service enabled on a dataset.
    Attributes
    ----------
    name : str
        The name of the service
    service_type : str
        The service type (i.e. ""OPENDAP"", ""NetcdfSubset"", ""WMS"", etc.)
    access_urls : dict[str, str]
        A dictionary of access urls whose keywords are the access service
        types defined in the catalog (for example, ""OPENDAP"", ""NetcdfSubset"",
        ""WMS"", etc.)
    '''

    def __init__(self, service_node):
        '''Initialize the Dataset object.
        Parameters
        ----------
        service_node : :class:`~xml.etree.ElementTree.Element`
            An :class:`~xml.etree.ElementTree.Element` representing a service node
        '''
        if service_node is None:
            raise ValueError(""service_node must be an xml.etree.ElementTree.Element"")

        attrs = service_node.attrib or {}

        # Extract name
        self.name = attrs.get('name') or attrs.get('id') or ''

        # Extract service type (normalize to preserve original but provide consistent checks)
        st = (
            attrs.get('serviceType')
            or attrs.get('type')
            or attrs.get('service_type')
            or ''
        )
        self.service_type = st if isinstance(st, str) else str(st)

        # Try to determine a base/url for this service
        base = (
            attrs.get('base')
            or attrs.get('url')
            or attrs.get('href')
        )
        if base is None:
            # Attempt to find any attribute that ends with 'href' (namespaced or not)
            for k, v in attrs.items():
                if k.lower().endswith('href'):
                    base = v
                    break

        # Build access_urls dict keyed by service_type when we have both
        self.access_urls = {}
        if self.service_type:
            # If base is missing, leave dict empty; otherwise add mapping
            if base:
                self.access_urls[self.service_type] = base

    def is_resolver(self):
        '''Return whether the service is a resolver service.'''
        st = (self.service_type or '').strip().lower()
        return st == 'resolver'"
250272,incuna/django-pgcrypto-fields,incuna_django-pgcrypto-fields/pgcrypto/mixins.py,pgcrypto.mixins.HashMixin,"class HashMixin:
    """"""Keyed hash mixin.

    `HashMixin` uses 'pgcrypto' to encrypt data in a postgres database.
    """"""
    encrypt_sql = None

    def __init__(self, original=None, *args, **kwargs):
        """"""Tells the init the original attr.""""""
        self.original = original
        super(HashMixin, self).__init__(*args, **kwargs)

    def pre_save(self, model_instance, add):
        """"""Save the original_value.""""""
        if self.original:
            original_value = getattr(model_instance, self.original)
            setattr(model_instance, self.attname, original_value)
        return super(HashMixin, self).pre_save(model_instance, add)

    def get_placeholder(self, value=None, compiler=None, connection=None):
        """"""
        Tell postgres to encrypt this field with a hashing function.

        The `value` string is checked to determine if we need to hash or keep
        the current value.

        `compiler` and `connection` is ignored here as we don't need custom operators.
        """"""
        if value is None or value.startswith('\\x'):
            return '%s'
        return self.get_encrypt_sql(connection)

    def get_encrypt_sql(self, connection):
        """"""Get encrypt sql. This may be overidden by some implementations.""""""
        return self.encrypt_sql","class HashMixin:
    '''Keyed hash mixin.
    `HashMixin` uses 'pgcrypto' to encrypt data in a postgres database.
    '''

    def __init__(self, original=None, *args, **kwargs):
        '''Tells the init the original attr.'''
        pass

    def pre_save(self, model_instance, add):
        '''Save the original_value.'''
        pass

    def get_placeholder(self, value=None, compiler=None, connection=None):
        '''
        Tell postgres to encrypt this field with a hashing function.
        The `value` string is checked to determine if we need to hash or keep
        the current value.
        `compiler` and `connection` is ignored here as we don't need custom operators.
        '''
        pass

    def get_encrypt_sql(self, connection):
        '''Get encrypt sql. This may be overidden by some implementations.'''
        pass",snippet_157,"class HashMixin:
    '''Keyed hash mixin.
    `HashMixin` uses 'pgcrypto' to encrypt data in a postgres database.
    '''

    def __init__(self, original=None, *args, **kwargs):
        '''Tells the init the original attr.'''
        self.original = original
        super().__init__(*args, **kwargs)

    def pre_save(self, model_instance, add):
        '''Save the original_value.'''
        # Determine which attribute to read plaintext from
        if self.original:
            value = getattr(model_instance, self.original, None)
            # Ensure the database receives the plaintext for hashing
            setattr(model_instance, self.attname, value)
        else:
            value = getattr(model_instance, self.attname, None)

        # Keep a reference to the value used for hashing during this save
        setattr(model_instance, f'_{self.attname}_original_value', value)
        return value

    def get_placeholder(self, value=None, compiler=None, connection=None):
        '''
        Tell postgres to encrypt this field with a hashing function.
        The `value` string is checked to determine if we need to hash or keep
        the current value.
        `compiler` and `connection` is ignored here as we don't need custom operators.
        '''
        # If no value is provided, keep the placeholder as-is
        if value is None:
            return ""%s""
        # Always hash non-null values
        return self.get_encrypt_sql(connection)

    def get_encrypt_sql(self, connection):
        '''Get encrypt sql. This may be overidden by some implementations.'''
        # Store hex-encoded SHA-256 digest of the input using pgcrypto
        return ""encode(digest(%s, 'sha256'), 'hex')"""
251421,Yelp/py_zipkin,Yelp_py_zipkin/py_zipkin/encoding/_encoders.py,py_zipkin.encoding._encoders.IEncoder,"from typing import Union
from py_zipkin.encoding._helpers import Span
from typing import List

class IEncoder:
    """"""Encoder interface.""""""

    def fits(self, current_count: int, current_size: int, max_size: int, new_span: Union[str, bytes]) -> bool:
        """"""Returns whether the new span will fit in the list.

        :param current_count: number of spans already in the list.
        :type current_count: int
        :param current_size: sum of the sizes of all the spans already in the list.
        :type current_size: int
        :param max_size: max supported transport payload size.
        :type max_size: int
        :param new_span: encoded span object that we want to add the the list.
        :type new_span: str or bytes
        :return: True if the new span can be added to the list, False otherwise.
        :rtype: bool
        """"""
        raise NotImplementedError()

    def encode_span(self, span: Span) -> Union[str, bytes]:
        """"""Encodes a single span.

        :param span: Span object representing the span.
        :type span: Span
        :return: encoded span.
        :rtype: str or bytes
        """"""
        raise NotImplementedError()

    def encode_queue(self, queue: List[Union[str, bytes]]) -> Union[str, bytes]:
        """"""Encodes a list of pre-encoded spans.

        :param queue: list of encoded spans.
        :type queue: list
        :return: encoded list, type depends on the encoding.
        :rtype: str or bytes
        """"""
        raise NotImplementedError()","
class IEncoder:
    '''Encoder interface.'''

    def fits(self, current_count: int, current_size: int, max_size: int, new_span: Union[str, bytes]) -> bool:
        '''Returns whether the new span will fit in the list.
        :param current_count: number of spans already in the list.
        :type current_count: int
        :param current_size: sum of the sizes of all the spans already in the list.
        :type current_size: int
        :param max_size: max supported transport payload size.
        :type max_size: int
        :param new_span: encoded span object that we want to add the the list.
        :type new_span: str or bytes
        :return: True if the new span can be added to the list, False otherwise.
        :rtype: bool
        '''
        pass

    def encode_span(self, span: Span) -> Union[str, bytes]:
        '''Encodes a single span.
        :param span: Span object representing the span.
        :type span: Span
        :return: encoded span.
        :rtype: str or bytes
        '''
        pass

    def encode_queue(self, queue: List[Union[str, bytes]]) -> Union[str, bytes]:
        '''Encodes a list of pre-encoded spans.
        :param queue: list of encoded spans.
        :type queue: list
        :return: encoded list, type depends on the encoding.
        :rtype: str or bytes
        '''
        pass",snippet_158,"from abc import ABC, abstractmethod
from typing import List, Union


class IEncoder(ABC):
    '''Encoder interface.'''

    @abstractmethod
    def fits(self, current_count: int, current_size: int, max_size: int, new_span: Union[str, bytes]) -> bool:
        '''Returns whether the new span will fit in the list.
        :param current_count: number of spans already in the list.
        :type current_count: int
        :param current_size: sum of the sizes of all the spans already in the list.
        :type current_size: int
        :param max_size: max supported transport payload size.
        :type max_size: int
        :param new_span: encoded span object that we want to add the the list.
        :type new_span: str or bytes
        :return: True if the new span can be added to the list, False otherwise.
        :rtype: bool
        '''
        raise NotImplementedError

    @abstractmethod
    def encode_span(self, span: 'Span') -> Union[str, bytes]:
        '''Encodes a single span.
        :param span: Span object representing the span.
        :type span: Span
        :return: encoded span.
        :rtype: str or bytes
        '''
        raise NotImplementedError

    @abstractmethod
    def encode_queue(self, queue: List[Union[str, bytes]]) -> Union[str, bytes]:
        '''Encodes a list of pre-encoded spans.
        :param queue: list of encoded spans.
        :type queue: list
        :return: encoded list, type depends on the encoding.
        :rtype: str or bytes
        '''
        raise NotImplementedError"
251832,glut23/webvtt-py,glut23_webvtt-py/webvtt/models.py,webvtt.models.Style,"import typing

class Style:
    """"""Representation of a style.""""""

    def __init__(self, text: typing.Union[str, typing.List[str]]):
        """"""Initialize.

        :param: text: the style text
        """"""
        self.lines = text.splitlines() if isinstance(text, str) else text
        self.comments: typing.List[str] = []

    @property
    def text(self):
        """"""Return the text of the style.""""""
        return '\n'.join(self.lines)","
class Style:
    '''Representation of a style.'''

    def __init__(self, text: typing.Union[str, typing.List[str]]):
        '''Initialize.
        :param: text: the style text
        '''
        pass
    @property
    def text(self):
        '''Return the text of the style.'''
        pass",snippet_159,"import typing

class Style:
    '''Representation of a style.'''

    def __init__(self, text: typing.Union[str, typing.List[str]]):
        '''Initialize.
        :param: text: the style text
        '''
        if isinstance(text, list):
            if not all(isinstance(t, str) for t in text):
                raise TypeError(""All items in the text list must be strings."")
            self._text = ""\n"".join(text)
        elif isinstance(text, str):
            self._text = text
        else:
            raise TypeError(""text must be a string or a list of strings."")
    @property
    def text(self):
        '''Return the text of the style.'''
        return self._text"
251834,glut23/webvtt-py,glut23_webvtt-py/webvtt/sbv.py,webvtt.sbv.SBVCueBlock,"import typing
import re

class SBVCueBlock:
    """"""Representation of a cue timing block.""""""
    CUE_TIMINGS_PATTERN = re.compile('\\s*(\\d{1,2}:\\d{1,2}:\\d{1,2}.\\d{3}),(\\d{1,2}:\\d{1,2}:\\d{1,2}.\\d{3})')

    def __init__(self, start: str, end: str, payload: typing.Sequence[str]):
        """"""
        Initialize.

        :param start: start time
        :param end: end time
        :param payload: caption text
        """"""
        self.start = start
        self.end = end
        self.payload = payload

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a cue time block.

        :param lines: the lines to be validated
        :returns: true for a matching cue time block
        """"""
        return bool(len(lines) >= 2 and re.match(cls.CUE_TIMINGS_PATTERN, lines[0]) and lines[1].strip())

    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SBVCueBlock':
        """"""
        Create a `SBVCueBlock` from lines of text.

        :param lines: the lines of text
        :returns: `SBVCueBlock` instance
        """"""
        match = re.match(cls.CUE_TIMINGS_PATTERN, lines[0])
        assert match is not None
        payload = lines[1:]
        return cls(match.group(1), match.group(2), payload)","
class SBVCueBlock:
    '''Representation of a cue timing block.'''

    def __init__(self, start: str, end: str, payload: typing.Sequence[str]):
        '''
        Initialize.
        :param start: start time
        :param end: end time
        :param payload: caption text
        '''
        pass
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        '''
        Validate the lines for a match of a cue time block.
        :param lines: the lines to be validated
        :returns: true for a matching cue time block
        '''
        pass
    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SBVCueBlock':
        '''
        Create a `SBVCueBlock` from lines of text.
        :param lines: the lines of text
        :returns: `SBVCueBlock` instance
        '''
        pass",snippet_160,"import re
import typing


class SBVCueBlock:
    '''Representation of a cue timing block.'''

    _TIME_RE = re.compile(
        r'^\s*(?P<start>(?:\d+:)?\d{1,2}:\d{2}\.\d{3})\s*,\s*(?P<end>(?:\d+:)?\d{1,2}:\d{2}\.\d{3})\s*$'
    )

    def __init__(self, start: str, end: str, payload: typing.Sequence[str]):
        '''
        Initialize.
        :param start: start time
        :param end: end time
        :param payload: caption text
        '''
        if not isinstance(start, str) or not isinstance(end, str):
            raise TypeError(""start and end must be strings"")
        if not isinstance(payload, typing.Sequence) or isinstance(payload, (str, bytes)):
            raise TypeError(""payload must be a sequence of strings"")
        self.start: str = start
        self.end: str = end
        self.payload: typing.List[str] = [str(line) for line in payload]

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        '''
        Validate the lines for a match of a cue time block.
        :param lines: the lines to be validated
        :returns: true for a matching cue time block
        '''
        if not isinstance(lines, typing.Sequence) or isinstance(lines, (str, bytes)):
            return False
        if len(lines) < 2:
            return False
        first = str(lines[0]).rstrip('\r\n')
        m = cls._TIME_RE.fullmatch(first)
        if not m:
            return False
        # Require at least one non-empty payload line
        for l in lines[1:]:
            if str(l).strip() != '':
                return True
        return False

    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SBVCueBlock':
        '''
        Create a `SBVCueBlock` from lines of text.
        :param lines: the lines of text
        :returns: `SBVCueBlock` instance
        '''
        if not cls.is_valid(lines):
            raise ValueError(""Lines do not form a valid SBV cue block"")
        first = str(lines[0]).rstrip('\r\n')
        m = cls._TIME_RE.fullmatch(first)
        assert m is not None
        start = m.group('start')
        end = m.group('end')
        payload = [str(l).rstrip('\r\n') for l in lines[1:]]
        return cls(start, end, payload)"
251835,glut23/webvtt-py,glut23_webvtt-py/webvtt/srt.py,webvtt.srt.SRTCueBlock,"import typing
import re

class SRTCueBlock:
    """"""Representation of a cue timing block.""""""
    CUE_TIMINGS_PATTERN = re.compile('\\s*(\\d+:\\d{2}:\\d{2},\\d{3})\\s*-->\\s*(\\d+:\\d{2}:\\d{2},\\d{3})')

    def __init__(self, index: str, start: str, end: str, payload: typing.Sequence[str]):
        """"""
        Initialize.

        :param start: start time
        :param end: end time
        :param payload: caption text
        """"""
        self.index = index
        self.start = start
        self.end = end
        self.payload = payload

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a cue time block.

        :param lines: the lines to be validated
        :returns: true for a matching cue time block
        """"""
        return bool(len(lines) >= 3 and lines[0].isdigit() and re.match(cls.CUE_TIMINGS_PATTERN, lines[1]))

    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SRTCueBlock':
        """"""
        Create a `SRTCueBlock` from lines of text.

        :param lines: the lines of text
        :returns: `SRTCueBlock` instance
        """"""
        index = lines[0]
        match = re.match(cls.CUE_TIMINGS_PATTERN, lines[1])
        assert match is not None
        payload = lines[2:]
        return cls(index, match.group(1), match.group(2), payload)","
class SRTCueBlock:
    '''Representation of a cue timing block.'''

    def __init__(self, index: str, start: str, end: str, payload: typing.Sequence[str]):
        '''
        Initialize.
        :param start: start time
        :param end: end time
        :param payload: caption text
        '''
        pass
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        '''
        Validate the lines for a match of a cue time block.
        :param lines: the lines to be validated
        :returns: true for a matching cue time block
        '''
        pass
    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SRTCueBlock':
        '''
        Create a `SRTCueBlock` from lines of text.
        :param lines: the lines of text
        :returns: `SRTCueBlock` instance
        '''
        pass",snippet_161,"import re
import typing


class SRTCueBlock:
    '''Representation of a cue timing block.'''

    _TIME_RE = r'(?P<h>\d{2}):(?P<m>[0-5]\d):(?P<s>[0-5]\d),(?P<ms>\d{3})'
    _CUE_RE = re.compile(r'^\s*(?P<start>' + _TIME_RE + r')\s*-->\s*(?P<end>' + _TIME_RE + r')\s*$')

    def __init__(self, index: str, start: str, end: str, payload: typing.Sequence[str]):
        '''
        Initialize.
        :param start: start time
        :param end: end time
        :param payload: caption text
        '''
        self.index = str(index).strip()
        self.start = start.strip()
        self.end = end.strip()
        self.payload = [str(line).rstrip('\r\n') for line in payload]

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        '''
        Validate the lines for a match of a cue time block.
        :param lines: the lines to be validated
        :returns: true for a matching cue time block
        '''
        if not lines or len(lines) < 2:
            return False
        try:
            idx = str(lines[0]).strip()
            if not idx.isdigit():
                return False
            cue_line = str(lines[1]).strip()
            return cls._CUE_RE.match(cue_line) is not None
        except Exception:
            return False

    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SRTCueBlock':
        '''
        Create a `SRTCueBlock` from lines of text.
        :param lines: the lines of text
        :returns: `SRTCueBlock` instance
        '''
        if not cls.is_valid(lines):
            raise ValueError('Invalid SRT cue block')
        index = str(lines[0]).strip()
        m = cls._CUE_RE.match(str(lines[1]).strip())
        assert m is not None
        start = m.group('start')
        end = m.group('end')
        payload = [str(l).rstrip('\r\n') for l in lines[2:]]
        return cls(index=index, start=start, end=end, payload=payload)"
251838,glut23/webvtt-py,glut23_webvtt-py/webvtt/vtt.py,webvtt.vtt.WebVTTCommentBlock,"import typing
import re

class WebVTTCommentBlock:
    """"""Representation of a comment block.""""""
    COMMENT_PATTERN = re.compile('NOTE\\s(.*?)\\Z', re.DOTALL)

    def __init__(self, text: str):
        """"""
        Initialize.

        :param text: comment text
        """"""
        self.text = text

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a comment block.

        :param lines: the lines to be validated
        :returns: true for a matching comment block
        """"""
        return bool(lines and lines[0].startswith('NOTE'))

    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTCommentBlock':
        """"""
        Create a `WebVTTCommentBlock` from lines of text.

        :param lines: the lines of text
        :returns: `WebVTTCommentBlock` instance
        """"""
        match = cls.COMMENT_PATTERN.match('\n'.join(lines))
        return cls(text=match.group(1).strip() if match else '')

    @staticmethod
    def format_lines(lines: str) -> typing.List[str]:
        """"""
        Return the lines for a comment block.

        :param lines: comment lines
        :returns: list of lines for a comment block
        """"""
        list_of_lines = lines.split('\n')
        if len(list_of_lines) == 1:
            return [f'NOTE {lines}']
        return ['NOTE', *list_of_lines]","
class WebVTTCommentBlock:
    '''Representation of a comment block.'''

    def __init__(self, text: str):
        '''
        Initialize.
        :param text: comment text
        '''
        pass
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        '''
        Validate the lines for a match of a comment block.
        :param lines: the lines to be validated
        :returns: true for a matching comment block
        '''
        pass
    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTCommentBlock':
        '''
        Create a `WebVTTCommentBlock` from lines of text.
        :param lines: the lines of text
        :returns: `WebVTTCommentBlock` instance
        '''
        pass
    @staticmethod
    def format_lines(lines: str) -> typing.List[str]:
        '''
        Return the lines for a comment block.
        :param lines: comment lines
        :returns: list of lines for a comment block
        '''
        pass",snippet_162,"from typing import Iterable, List, Sequence

class WebVTTCommentBlock:
    '''Representation of a comment block.'''

    def __init__(self, text: str):
        '''
        Initialize.
        :param text: comment text
        '''
        self.text = text if text is not None else """"

    @classmethod
    def is_valid(cls, lines: Sequence[str]) -> bool:
        '''
        Validate the lines for a match of a comment block.
        :param lines: the lines to be validated
        :returns: true for a matching comment block
        '''
        if not lines:
            return False
        first = lines[0]
        if not isinstance(first, str):
            return False
        # Must start with ""NOTE"" and either end there or followed by whitespace
        if not (first.startswith(""NOTE"") and (len(first) == 4 or first[4].isspace())):
            return False
        # No empty line inside the block (empty line would terminate a NOTE block)
        for line in lines[1:]:
            if line == """":
                return False
        return True

    @classmethod
    def from_lines(cls, lines: Iterable[str]) -> 'WebVTTCommentBlock':
        '''
        Create a `WebVTTCommentBlock` from lines of text.
        :param lines: the lines of text
        :returns: `WebVTTCommentBlock` instance
        '''
        lst = list(lines)
        if not cls.is_valid(lst):
            raise ValueError(""Invalid NOTE block lines"")
        first = lst[0]
        # Extract inline comment after NOTE (allow any whitespace after NOTE)
        if len(first) > 4 and first[4].isspace():
            # Keep the remainder as-is but strip only the first whitespace char
            # after NOTE; if multiple whitespace characters, we remove only one.
            remainder = first[5:] if len(first) > 5 and first[4] == "" "" else first[4:].lstrip()
            content_lines = [remainder] if remainder != """" else []
        else:
            content_lines = []
        if len(lst) > 1:
            content_lines.extend(lst[1:])
        text = ""\n"".join(content_lines)
        return cls(text)

    @staticmethod
    def format_lines(lines: str) -> List[str]:
        '''
        Return the lines for a comment block.
        :param lines: comment lines
        :returns: list of lines for a comment block
        '''
        text = """" if lines is None else lines
        parts = text.splitlines()
        if not parts:
            return [""NOTE""]
        if len(parts) == 1:
            return [""NOTE "" + parts[0]] if parts[0] != """" else [""NOTE""]
        return [""NOTE""] + parts"
251840,glut23/webvtt-py,glut23_webvtt-py/webvtt/vtt.py,webvtt.vtt.WebVTTStyleBlock,"import re
import typing

class WebVTTStyleBlock:
    """"""Representation of a style block.""""""
    STYLE_PATTERN = re.compile('STYLE\\s(.*?)\\Z', re.DOTALL)

    def __init__(self, text: str):
        """"""
        Initialize.

        :param text: style text
        """"""
        self.text = text

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a style block.

        :param lines: the lines to be validated
        :returns: true for a matching style block
        """"""
        return len(lines) >= 2 and lines[0] == 'STYLE' and (not any((line.strip() == '' or '-->' in line for line in lines)))

    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTStyleBlock':
        """"""
        Create a `WebVTTStyleBlock` from lines of text.

        :param lines: the lines of text
        :returns: `WebVTTStyleBlock` instance
        """"""
        match = cls.STYLE_PATTERN.match('\n'.join(lines))
        return cls(text=match.group(1).strip() if match else '')

    @staticmethod
    def format_lines(lines: typing.List[str]) -> typing.List[str]:
        """"""
        Return the lines for a style block.

        :param lines: style lines
        :returns: list of lines for a style block
        """"""
        return ['STYLE', *lines]","
class WebVTTStyleBlock:
    '''Representation of a style block.'''

    def __init__(self, text: str):
        '''
        Initialize.
        :param text: style text
        '''
        pass
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        '''
        Validate the lines for a match of a style block.
        :param lines: the lines to be validated
        :returns: true for a matching style block
        '''
        pass
    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTStyleBlock':
        '''
        Create a `WebVTTStyleBlock` from lines of text.
        :param lines: the lines of text
        :returns: `WebVTTStyleBlock` instance
        '''
        pass
    @staticmethod
    def format_lines(lines: typing.List[str]) -> typing.List[str]:
        '''
        Return the lines for a style block.
        :param lines: style lines
        :returns: list of lines for a style block
        '''
        pass",snippet_163,"import typing

class WebVTTStyleBlock:
    '''Representation of a style block.'''

    def __init__(self, text: str):
        '''
        Initialize.
        :param text: style text
        '''
        self.text = '' if text is None else str(text)

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        '''
        Validate the lines for a match of a style block.
        :param lines: the lines to be validated
        :returns: true for a matching style block
        '''
        if not lines:
            return False
        first = lines[0].strip()
        if first != ""STYLE"":
            return False
        # Require at least one content line after the STYLE header
        return len(lines) >= 2

    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTStyleBlock':
        '''
        Create a `WebVTTStyleBlock` from lines of text.
        :param lines: the lines of text
        :returns: `WebVTTStyleBlock` instance
        '''
        raw = [l.rstrip('\n\r') for l in lines]
        if raw and raw[0].strip() == ""STYLE"":
            raw = raw[1:]
        # Remove trailing empty line(s) that typically terminate a block
        while raw and raw[-1] == '':
            raw.pop()
        text = '\n'.join(raw)
        return cls(text)

    @staticmethod
    def format_lines(lines: typing.List[str]) -> typing.List[str]:
        '''
        Return the lines for a style block.
        :param lines: style lines
        :returns: list of lines for a style block
        '''
        if lines is None:
            content = []
        else:
            # Normalize: ensure pure content (no header), strip line endings
            content = []
            for l in lines:
                s = '' if l is None else str(l)
                content.append(s.rstrip('\n\r'))
            # Remove trailing empty strings to avoid duplicating terminator
            while content and content[-1] == '':
                content.pop()
        return [""STYLE"", *content, """"]"
251866,Shoobx/xmldiff,Shoobx_xmldiff/xmldiff/diff_match_patch.py,xmldiff.diff_match_patch.patch_obj,"import urllib.parse

class patch_obj:
    """"""Class representing one patch operation.""""""

    def __init__(self):
        """"""Initializes with an empty list of diffs.""""""
        self.diffs = []
        self.start1 = None
        self.start2 = None
        self.length1 = 0
        self.length2 = 0

    def __str__(self):
        """"""Emulate GNU diff's format.
        Header: @@ -382,8 +481,9 @@
        Indices are printed as 1-based, not 0-based.

        Returns:
          The GNU diff string.
        """"""
        if self.length1 == 0:
            coords1 = str(self.start1) + ',0'
        elif self.length1 == 1:
            coords1 = str(self.start1 + 1)
        else:
            coords1 = str(self.start1 + 1) + ',' + str(self.length1)
        if self.length2 == 0:
            coords2 = str(self.start2) + ',0'
        elif self.length2 == 1:
            coords2 = str(self.start2 + 1)
        else:
            coords2 = str(self.start2 + 1) + ',' + str(self.length2)
        text = ['@@ -', coords1, ' +', coords2, ' @@\n']
        for op, data in self.diffs:
            if op == diff_match_patch.DIFF_INSERT:
                text.append('+')
            elif op == diff_match_patch.DIFF_DELETE:
                text.append('-')
            elif op == diff_match_patch.DIFF_EQUAL:
                text.append(' ')
            data = data.encode('utf-8')
            text.append(urllib.parse.quote(data, ""!~*'();/?:@&=+$,# "") + '\n')
        return ''.join(text)","
class patch_obj:
    '''Class representing one patch operation.'''

    def __init__(self):
        '''Initializes with an empty list of diffs.'''
        pass

    def __str__(self):
        '''Emulate GNU diff's format.
        Header: @@ -382,8 +481,9 @@
        Indices are printed as 1-based, not 0-based.
        Returns:
          The GNU diff string.
        '''
        pass",snippet_164,"class patch_obj:
    '''Class representing one patch operation.'''

    def __init__(self):
        '''Initializes with an empty list of diffs.'''
        self.diffs = []
        self.start1 = 0
        self.start2 = 0
        self.length1 = 0
        self.length2 = 0

    def __str__(self):
        '''Emulate GNU diff's format.
        Header: @@ -382,8 +481,9 @@
        Indices are printed as 1-based, not 0-based.
        Returns:
          The GNU diff string.
        '''
        from urllib.parse import quote

        def coord(start, length):
            if length == 0:
                return f""{start},0""
            if length == 1:
                return f""{start + 1}""
            return f""{start + 1},{length}""

        header = f""@@ -{coord(self.start1, self.length1)} +{coord(self.start2, self.length2)} @@\n""

        # Map operations to their prefixes.
        def op_prefix(op):
            if op in (-1, 'delete', 'DELETE', '-'):
                return '-'
            if op in (1, 'insert', 'INSERT', '+'):
                return '+'
            return ' '

        # Encode text similar to diff-match-patch: percent-encode, but keep spaces as spaces for readability.
        def encode_text(t):
            enc = quote(t, safe=""~!*()'"")
            return enc.replace('%20', ' ')

        lines = []
        lines.append(header)
        for op, text in self.diffs:
            lines.append(op_prefix(op) + encode_text(text) + ""\n"")
        return ''.join(lines)"
252656,happyleavesaoc/aoc-mgz,happyleavesaoc_aoc-mgz/mgz/model/inputs.py,mgz.model.inputs.Inputs,"from mgz.model.definitions import Input
from mgz.fast import Action as ActionEnum

class Inputs:
    """"""Normalize player inputs.""""""

    def __init__(self, gaia):
        """"""Initialize.""""""
        self._gaia = gaia
        self._buildings = {}
        self._oid_cache = {}
        self.inputs = []

    def add_chat(self, chat):
        """"""Add chat input.""""""
        self.inputs.append(Input(chat.timestamp, 'Chat', None, dict(message=chat.message), chat.player, None))

    def add_action(self, action):
        """"""Add action input.""""""
        if action.type in (ActionEnum.DE_TRANSFORM, ActionEnum.POSTGAME):
            return
        name = ACTION_TRANSLATE.get(action.type, action.type.name).replace('_', ' ').title()
        param = None
        if 'object_ids' in action.payload and action.payload['object_ids']:
            self._oid_cache[action.type] = action.payload['object_ids']
        elif action.type in self._oid_cache:
            action.payload['object_ids'] = self._oid_cache[action.type]
        if action.type is ActionEnum.SPECIAL:
            name = action.payload['order']
        elif action.type is ActionEnum.GAME:
            name = action.payload['command']
            if name == 'Speed':
                param = action.payload['speed']
        elif action.type is ActionEnum.STANCE:
            name = 'Stance'
            param = action.payload['stance']
        elif action.type is ActionEnum.FORMATION:
            name = 'Formation'
            param = action.payload['formation']
        elif action.type is ActionEnum.ORDER and action.payload['target_id'] in self._gaia:
            name = 'Gather'
            param = self._gaia[action.payload['target_id']]
        elif action.type is ActionEnum.ORDER and action.position and (action.position.hash() in self._buildings):
            name = 'Target'
            param = self._buildings[action.position.hash()]
        elif action.type is ActionEnum.GATHER_POINT:
            if action.payload['target_id'] in self._gaia:
                param = self._gaia[action.payload['target_id']]
            elif action.position and action.position.hash() in self._buildings:
                if len(action.payload['object_ids']) == 1 and action.payload['object_ids'][0] == action.payload['target_id']:
                    name = 'Spawn'
                param = self._buildings[action.position.hash()]
        elif action.type in (ActionEnum.BUY, ActionEnum.SELL):
            action.payload['amount'] *= 100
        elif action.type is ActionEnum.BUILD:
            param = action.payload['building']
            if action.position.hash() in self._buildings:
                if self._buildings[action.position.hash()] == 'Farm' and action.payload['building'] == 'Farm':
                    name = 'Reseed'
            self._buildings[action.position.hash()] = action.payload['building']
        elif action.type in (ActionEnum.QUEUE, ActionEnum.DE_QUEUE):
            param = action.payload['unit']
        elif action.type is ActionEnum.RESEARCH:
            param = action.payload['technology']
        new_input = Input(action.timestamp, name, param, action.payload, action.player, action.position)
        self.inputs.append(new_input)
        return new_input","
class Inputs:
    '''Normalize player inputs.'''

    def __init__(self, gaia):
        '''Initialize.'''
        pass

    def add_chat(self, chat):
        '''Add chat input.'''
        pass

    def add_action(self, action):
        '''Add action input.'''
        pass",snippet_165,"class Inputs:
    '''Normalize player inputs.'''

    def __init__(self, gaia):
        '''Initialize.'''
        self.gaia = gaia
        self._events = []
        self._counter = 0
        from threading import Lock
        self._lock = Lock()

    def _next_id(self):
        with self._lock:
            self._counter += 1
            return self._counter

    def _timestamp(self):
        import time
        return time.time()

    def add_chat(self, chat):
        '''Add chat input.'''
        if not isinstance(chat, str):
            raise TypeError(""chat must be a string"")
        text = chat.strip()
        if not text:
            raise ValueError(""chat cannot be empty"")
        evt = {
            ""id"": self._next_id(),
            ""ts"": self._timestamp(),
            ""type"": ""chat"",
            ""text"": text,
        }
        with self._lock:
            self._events.append(evt)
        return evt

    def add_action(self, action):
        '''Add action input.'''
        name = None
        params = {}

        if isinstance(action, str):
            name = action.strip()
            if not name:
                raise ValueError(""action name cannot be empty"")
        elif isinstance(action, (tuple, list)) and len(action) == 2:
            name, params = action
        elif isinstance(action, dict):
            name = action.get(""name"") or action.get(""action"")
            if ""params"" in action and isinstance(action[""params""], dict):
                params = dict(action[""params""])
            else:
                # Infer params from remaining keys if provided inline
                params = {k: v for k, v in action.items() if k not in (""name"", ""action"", ""params"")}
        else:
            raise TypeError(""action must be a string, (name, params) tuple, or dict"")

        if not isinstance(name, str):
            raise TypeError(""action name must be a string"")
        name = name.strip()
        if not name:
            raise ValueError(""action name cannot be empty"")

        if params is None:
            params = {}
        if not isinstance(params, dict):
            raise TypeError(""action params must be a dict"")

        evt = {
            ""id"": self._next_id(),
            ""ts"": self._timestamp(),
            ""type"": ""action"",
            ""name"": name,
            ""params"": params,
        }
        with self._lock:
            self._events.append(evt)
        return evt"
256858,cherrypy/cheroot,cheroot/server.py,cheroot.server.Gateway,"class Gateway:
    """"""Base class to interface HTTPServer with other systems, such as WSGI.""""""

    def __init__(self, req):
        """"""Initialize Gateway instance with request.

        Args:
            req (HTTPRequest): current HTTP request
        """"""
        self.req = req

    def respond(self):
        """"""Process the current request. Must be overridden in a subclass.""""""
        raise NotImplementedError","class Gateway:
    '''Base class to interface HTTPServer with other systems, such as WSGI.'''

    def __init__(self, req):
        '''Initialize Gateway instance with request.
        Args:
            req (HTTPRequest): current HTTP request
        '''
        pass

    def respond(self):
        '''Process the current request. Must be overridden in a subclass.'''
        pass",snippet_168,"class Gateway:
    '''Base class to interface HTTPServer with other systems, such as WSGI.'''

    def __init__(self, req):
        '''Initialize Gateway instance with request.
        Args:
            req (HTTPRequest): current HTTP request
        '''
        self.req = req

    def respond(self):
        '''Process the current request. Must be overridden in a subclass.'''
        raise NotImplementedError(""Gateway.respond must be implemented by subclasses."")"
258632,chovanecm/sacredboard,chovanecm_sacredboard/sacredboard/app/data/datastorage.py,sacredboard.app.data.datastorage.Cursor,"class Cursor:
    """"""Interface that abstracts the cursor object returned from databases.""""""

    def __init__(self):
        """"""Declare a new cursor to iterate over runs.""""""
        pass

    def count(self):
        """"""Return the number of items in this cursor.""""""
        raise NotImplementedError()

    def __iter__(self):
        """"""Iterate over elements.""""""
        raise NotImplementedError()","class Cursor:
    '''Interface that abstracts the cursor object returned from databases.'''

    def __init__(self):
        '''Declare a new cursor to iterate over runs.'''
        pass

    def count(self):
        '''Return the number of items in this cursor.'''
        pass

    def __iter__(self):
        '''Iterate over elements.'''
        pass",snippet_169,"class Cursor:
    '''Interface that abstracts the cursor object returned from databases.'''

    def __init__(self, iterable=None):
        '''Declare a new cursor to iterate over runs.'''
        if iterable is None:
            self._items = []
        else:
            try:
                self._items = list(iterable)
            except TypeError:
                self._items = [iterable]

    def count(self):
        '''Return the number of items in this cursor.'''
        return len(self._items)

    def __iter__(self):
        '''Iterate over elements.'''
        return iter(self._items)"
258643,chovanecm/sacredboard,chovanecm_sacredboard/sacredboard/app/data/metricsdao.py,sacredboard.app.data.metricsdao.MetricsDAO,"class MetricsDAO:
    """"""
    Interface for accessing Sacred metrics.

    Issue: https://github.com/chovanecm/sacredboard/issues/58

    Extended because of: https://github.com/chovanecm/sacredboard/issues/66
    """"""

    def get(self, run_id, metric_id):
        """"""
        Read a metric of the given id and run.

        The returned object has the following format (timestamps are datetime
         objects).

        .. code::

            {""steps"": [0,1,20,40,...],
            ""timestamps"": [timestamp1,timestamp2,timestamp3,...],
            ""values"": [0,1 2,3,4,5,6,...],
            ""name"": ""name of the metric"",
            ""metric_id"": ""metric_id"",
            ""run_id"": ""run_id""}

        :param run_id: ID of the Run that the metric belongs to.
        :param metric_id: The ID fo the metric.
        :return: The whole metric as specified.

        :raise NotFoundError
        """"""
        raise NotImplementedError('The MetricsDAO class is abstract.')

    def delete(self, run_id):
        """"""
        Delete all metrics belonging to the given run.

        :param run_id: ID of the Run that the metric belongs to.
        """"""
        raise NotImplementedError('The MetricsDAO class is abstract.')","class MetricsDAO:
    '''
    Interface for accessing Sacred metrics.
    Issue: https://github.com/chovanecm/sacredboard/issues/58
    Extended because of: https://github.com/chovanecm/sacredboard/issues/66
    '''

    def get(self, run_id, metric_id):
        '''
        Read a metric of the given id and run.
        The returned object has the following format (timestamps are datetime
         objects).
        .. code::
            {""steps"": [0,1,20,40,...],
            ""timestamps"": [timestamp1,timestamp2,timestamp3,...],
            ""values"": [0,1 2,3,4,5,6,...],
            ""name"": ""name of the metric"",
            ""metric_id"": ""metric_id"",
            ""run_id"": ""run_id""}
        :param run_id: ID of the Run that the metric belongs to.
        :param metric_id: The ID fo the metric.
        :return: The whole metric as specified.
        :raise NotFoundError
        '''
        pass

    def delete(self, run_id):
        '''
        Delete all metrics belonging to the given run.
        :param run_id: ID of the Run that the metric belongs to.
        '''
        pass",snippet_170,"class NotFoundError(Exception):
    pass


class MetricsDAO:
    '''
    Interface for accessing Sacred metrics.
    Issue: https://github.com/chovanecm/sacredboard/issues/58
    Extended because of: https://github.com/chovanecm/sacredboard/issues/66
    '''

    def __init__(self, storage=None):
        self._storage = {}
        if storage:
            for run_id, metrics in storage.items():
                if isinstance(metrics, dict):
                    self._storage[run_id] = dict(metrics)
        try:
            from threading import RLock
            self._lock = RLock()
        except Exception:
            class DummyLock:
                def __enter__(self): return self
                def __exit__(self, *args): return False
            self._lock = DummyLock()

    def get(self, run_id, metric_id):
        '''
        Read a metric of the given id and run.
        The returned object has the following format (timestamps are datetime
         objects).
        .. code::
            {""steps"": [0,1,20,40,...],
            ""timestamps"": [timestamp1,timestamp2,timestamp3,...],
            ""values"": [0,1 2,3,4,5,6,...],
            ""name"": ""name of the metric"",
            ""metric_id"": ""metric_id"",
            ""run_id"": ""run_id""}
        :param run_id: ID of the Run that the metric belongs to.
        :param metric_id: The ID fo the metric.
        :return: The whole metric as specified.
        :raise NotFoundError
        '''
        with self._lock:
            run_metrics = self._storage.get(run_id)
            if not run_metrics or metric_id not in run_metrics:
                raise NotFoundError(f""Metric '{metric_id}' for run '{run_id}' not found"")
            metric = run_metrics[metric_id]
            result = {
                ""steps"": list(metric.get(""steps"", [])),
                ""timestamps"": list(metric.get(""timestamps"", [])),
                ""values"": list(metric.get(""values"", [])),
                ""name"": metric.get(""name""),
                ""metric_id"": metric.get(""metric_id"", metric_id),
                ""run_id"": metric.get(""run_id"", run_id),
            }
            return result

    def delete(self, run_id):
        '''
        Delete all metrics belonging to the given run.
        :param run_id: ID of the Run that the metric belongs to.
        '''
        with self._lock:
            self._storage.pop(run_id, None)"
258651,chovanecm/sacredboard,chovanecm_sacredboard/sacredboard/app/data/rundao.py,sacredboard.app.data.rundao.RunDAO,"class RunDAO:
    """"""
    Interface for accessing Runs.

    Issue: https://github.com/chovanecm/sacredboard/issues/69
    """"""

    def get(self, run_id):
        """"""
        Return the run associated with the id.

        :raise NotFoundError when not found
        """"""
        raise NotImplementedError('RunDAO is abstract.')

    def get_runs(self, sort_by=None, sort_direction=None, start=0, limit=None, query={'type': 'and', 'filters': []}):
        """"""Return all runs that match the query.""""""
        raise NotImplementedError('RunDAO is abstract.')

    def delete(self, run_id):
        """"""
        Delete run with the given id from the backend.

        :param run_id: Id of the run to delete.
        :raise NotImplementedError If not supported by the backend.
        :raise DataSourceError General data source error.
        :raise NotFoundError The run was not found. (Some backends may succeed
        even if the run does not exist.
        """"""
        raise NotImplementedError('This database data source does not currently support this operation.')","class RunDAO:
    '''
    Interface for accessing Runs.
    Issue: https://github.com/chovanecm/sacredboard/issues/69
    '''

    def get(self, run_id):
        '''
        Return the run associated with the id.
        :raise NotFoundError when not found
        '''
        pass

    def get_runs(self, sort_by=None, sort_direction=None, start=0, limit=None, query={'type': 'and', 'filters': []}):
        '''Return all runs that match the query.'''
        pass

    def delete(self, run_id):
        '''
        Delete run with the given id from the backend.
        :param run_id: Id of the run to delete.
        :raise NotImplementedError If not supported by the backend.
        :raise DataSourceError General data source error.
        :raise NotFoundError The run was not found. (Some backends may succeed
        even if the run does not exist.
        '''
        pass",snippet_171,"from abc import ABC, abstractmethod

class RunDAO(ABC):
    '''
    Interface for accessing Runs.
    Issue: https://github.com/chovanecm/sacredboard/issues/69
    '''

    @abstractmethod
    def get(self, run_id):
        '''
        Return the run associated with the id.
        :raise NotFoundError when not found
        '''
        raise NotImplementedError(""Subclasses must implement get(run_id)"")

    @abstractmethod
    def get_runs(self, sort_by=None, sort_direction=None, start=0, limit=None, query={'type': 'and', 'filters': []}):
        '''Return all runs that match the query.'''
        raise NotImplementedError(""Subclasses must implement get_runs(...)"")

    @abstractmethod
    def delete(self, run_id):
        '''
        Delete run with the given id from the backend.
        :param run_id: Id of the run to delete.
        :raise NotImplementedError If not supported by the backend.
        :raise DataSourceError General data source error.
        :raise NotFoundError The run was not found. (Some backends may succeed
        even if the run does not exist.
        '''
        raise NotImplementedError(""Subclasses must implement delete(run_id)"")"
259541,mozilla-iot/webthing-python,mozilla-iot_webthing-python/webthing/server.py,webthing.server.MultipleThings,"class MultipleThings:
    """"""A container for multiple things.""""""

    def __init__(self, things, name):
        """"""
        Initialize the container.

        things -- the things to store
        name -- the mDNS server name
        """"""
        self.things = things
        self.name = name

    def get_thing(self, idx):
        """"""
        Get the thing at the given index.

        idx -- the index
        """"""
        try:
            idx = int(idx)
        except ValueError:
            return None
        if idx < 0 or idx >= len(self.things):
            return None
        return self.things[idx]

    def get_things(self):
        """"""Get the list of things.""""""
        return self.things

    def get_name(self):
        """"""Get the mDNS server name.""""""
        return self.name","class MultipleThings:
    '''A container for multiple things.'''

    def __init__(self, things, name):
        '''
        Initialize the container.
        things -- the things to store
        name -- the mDNS server name
        '''
        pass

    def get_thing(self, idx):
        '''
        Get the thing at the given index.
        idx -- the index
        '''
        pass

    def get_things(self):
        '''Get the list of things.'''
        pass

    def get_name(self):
        '''Get the mDNS server name.'''
        pass",snippet_172,"class MultipleThings:
    '''A container for multiple things.'''

    def __init__(self, things, name):
        '''
        Initialize the container.
        things -- the things to store
        name -- the mDNS server name
        '''
        self._things = list(things) if things is not None else []
        self._name = name

    def get_thing(self, idx):
        '''
        Get the thing at the given index.
        idx -- the index
        '''
        return self._things[idx]

    def get_things(self):
        '''Get the list of things.'''
        return self._things

    def get_name(self):
        '''Get the mDNS server name.'''
        return self._name"
259544,mozilla-iot/webthing-python,mozilla-iot_webthing-python/webthing/server.py,webthing.server.SingleThing,"class SingleThing:
    """"""A container for a single thing.""""""

    def __init__(self, thing):
        """"""
        Initialize the container.

        thing -- the thing to store
        """"""
        self.thing = thing

    def get_thing(self, _=None):
        """"""Get the thing at the given index.""""""
        return self.thing

    def get_things(self):
        """"""Get the list of things.""""""
        return [self.thing]

    def get_name(self):
        """"""Get the mDNS server name.""""""
        return self.thing.title","class SingleThing:
    '''A container for a single thing.'''

    def __init__(self, thing):
        '''
        Initialize the container.
        thing -- the thing to store
        '''
        pass

    def get_thing(self, _=None):
        '''Get the thing at the given index.'''
        pass

    def get_things(self):
        '''Get the list of things.'''
        pass

    def get_name(self):
        '''Get the mDNS server name.'''
        pass",snippet_173,"class SingleThing:
    '''A container for a single thing.'''

    def __init__(self, thing):
        '''
        Initialize the container.
        thing -- the thing to store
        '''
        self._thing = thing

    def get_thing(self, _=None):
        '''Get the thing at the given index.'''
        if _ is None or _ == 0:
            return self._thing
        raise IndexError(""SingleThing only contains a single item at index 0"")

    def get_things(self):
        '''Get the list of things.'''
        return [self._thing]

    def get_name(self):
        '''Get the mDNS server name.'''
        thing = self._thing
        if hasattr(thing, ""get_name"") and callable(getattr(thing, ""get_name"")):
            return thing.get_name()
        if hasattr(thing, ""name""):
            return getattr(thing, ""name"")
        if isinstance(thing, str):
            return thing
        return str(thing)"
259548,mozilla-iot/webthing-python,mozilla-iot_webthing-python/webthing/subscriber.py,webthing.subscriber.Subscriber,"class Subscriber:
    """"""Abstract Subscriber class.""""""

    def update_property(self, property_):
        """"""
        Send an update about a Property.

        :param property_: Property
        """"""
        raise NotImplementedError

    def update_action(self, action):
        """"""
        Send an update about an Action.

        :param action: Action
        """"""
        raise NotImplementedError

    def update_event(self, event):
        """"""
        Send an update about an Event.

        :param event: Event
        """"""
        raise NotImplementedError","class Subscriber:
    '''Abstract Subscriber class.'''

    def update_property(self, property_):
        '''
        Send an update about a Property.
        :param property_: Property
        '''
        pass

    def update_action(self, action):
        '''
        Send an update about an Action.
        :param action: Action
        '''
        pass

    def update_event(self, event):
        '''
        Send an update about an Event.
        :param event: Event
        '''
        pass",snippet_174,"from abc import ABC, abstractmethod

class Subscriber(ABC):
    '''Abstract Subscriber class.'''

    @abstractmethod
    def update_property(self, property_):
        '''
        Send an update about a Property.
        :param property_: Property
        '''
        raise NotImplementedError

    @abstractmethod
    def update_action(self, action):
        '''
        Send an update about an Action.
        :param action: Action
        '''
        raise NotImplementedError

    @abstractmethod
    def update_event(self, event):
        '''
        Send an update about an Event.
        :param event: Event
        '''
        raise NotImplementedError"
261819,user-cont/conu,user-cont_conu/conu/apidefs/image.py,conu.apidefs.image.S2Image,"class S2Image:
    """"""
    Additional functionality related to s2i-enabled container images
    """"""

    def extend(self, source, new_image_name, s2i_args=None):
        """"""
        extend this s2i-enabled image using provided source, raises ConuException if
        `s2i build` fails

        :param source: str, source used to extend the image, can be path or url
        :param new_image_name: str, name of the new, extended image
        :param s2i_args: list of str, additional options and arguments provided to `s2i build`
        :return: S2Image instance
        """"""
        raise NotImplementedError('extend method is not implemented')

    def usage(self):
        """"""
        Provide output of `s2i usage`

        :return: str
        """"""
        raise NotImplementedError('usage method is not implemented')","class S2Image:
    '''
    Additional functionality related to s2i-enabled container images
    '''

    def extend(self, source, new_image_name, s2i_args=None):
        '''
        extend this s2i-enabled image using provided source, raises ConuException if
        `s2i build` fails
        :param source: str, source used to extend the image, can be path or url
        :param new_image_name: str, name of the new, extended image
        :param s2i_args: list of str, additional options and arguments provided to `s2i build`
        :return: S2Image instance
        '''
        pass

    def usage(self):
        '''
        Provide output of `s2i usage`
        :return: str
        '''
        pass",snippet_175,"import subprocess
from typing import List, Optional

try:
    from conu.exceptions import ConuException  # type: ignore
except Exception:
    try:
        from conu.errors import ConuException  # type: ignore
    except Exception:
        class ConuException(Exception):
            pass


class S2Image:
    '''
    Additional functionality related to s2i-enabled container images
    '''

    def __init__(self, image_name: str):
        self.image_name = image_name

    def extend(self, source, new_image_name, s2i_args: Optional[List[str]] = None):
        '''
        extend this s2i-enabled image using provided source, raises ConuException if
        `s2i build` fails
        :param source: str, source used to extend the image, can be path or url
        :param new_image_name: str, name of the new, extended image
        :param s2i_args: list of str, additional options and arguments provided to `s2i build`
        :return: S2Image instance
        '''
        if s2i_args is None:
            s2i_args = []
        if not isinstance(s2i_args, list):
            raise ConuException(""s2i_args must be a list of strings"")
        cmd = [""s2i"", ""build"", str(source), str(self.image_name), str(new_image_name)] + [str(a) for a in s2i_args]
        try:
            res = subprocess.run(cmd, check=False, capture_output=True, text=True)
        except FileNotFoundError as e:
            raise ConuException(""s2i executable not found"") from e
        if res.returncode != 0:
            msg = res.stderr.strip() or res.stdout.strip() or ""s2i build failed""
            raise ConuException(msg)
        return S2Image(new_image_name)

    def usage(self):
        '''
        Provide output of `s2i usage`
        :return: str
        '''
        cmd = [""s2i"", ""usage"", str(self.image_name)]
        try:
            res = subprocess.run(cmd, check=False, capture_output=True, text=True)
        except FileNotFoundError as e:
            raise ConuException(""s2i executable not found"") from e
        if res.returncode != 0:
            msg = res.stderr.strip() or res.stdout.strip() or ""s2i usage failed""
            raise ConuException(msg)
        return res.stdout.rstrip(""\n"")"
264367,materialsproject/custodian,materialsproject_custodian/src/custodian/ansible/interpreter.py,custodian.ansible.interpreter.Modder,"from custodian.ansible.actions import DictActions
import re

class Modder:
    """"""
    Class to modify a dict/file/any object using a mongo-like language.
    Keywords are mostly adopted from mongo's syntax, but instead of $, an
    underscore precedes action keywords. This is so that the modification can
    be inserted into a mongo db easily.

    Allowable actions are supplied as a list of classes as an argument. Refer
    to the action classes on what the actions do. Action classes are in
    pymatpro.ansible.actions.

    Examples:
    >>> modder = Modder()
    >>> dct = {""Hello"": ""World""}
    >>> mod = {'_set': {'Hello':'Universe', 'Bye': 'World'}}
    >>> modder.modify(mod, dct)
    >>> dct['Bye']
    'World'
    >>> dct['Hello']
    'Universe'
    """"""

    def __init__(self, actions=None, strict=True, directory='./') -> None:
        """"""Initialize a Modder from a list of supported actions.

        Args:
            actions ([Action]): A sequence of supported actions. See
                :mod:`custodian.ansible.actions`. Default is None,
                which means only DictActions are supported.
            strict (bool): Indicating whether to use strict mode. In non-strict
                mode, unsupported actions are simply ignored without any
                errors raised. In strict mode, if an unsupported action is
                supplied, a ValueError is raised. Defaults to True.
            directory (str): The directory containing the files to be modified.
                Defaults to ""./"".
        """"""
        self.supported_actions = {}
        actions = actions if actions is not None else [DictActions]
        for action in actions:
            for attr in dir(action):
                if not re.match('__\\w+__', attr) and callable(getattr(action, attr)):
                    self.supported_actions[f'_{attr}'] = getattr(action, attr)
        self.strict = strict
        self.directory = directory

    def modify(self, modification, obj) -> None:
        """"""
        Note that modify makes actual in-place modifications. It does not
        return a copy.

        Args:
            modification (dict): Modification must be {action_keyword :
                settings}. E.g., {'_set': {'Hello':'Universe', 'Bye': 'World'}}
            obj (dict/str/object): Object to modify depending on actions. For
                example, for DictActions, obj will be a dict to be modified.
                For FileActions, obj will be a string with a full pathname to a
                file.
        """"""
        for action, settings in modification.items():
            if action in self.supported_actions:
                self.supported_actions[action](obj, settings, directory=self.directory)
            elif self.strict:
                raise ValueError(f'{action} is not a supported action!')

    def modify_object(self, modification, obj):
        """"""
        Modify an object that supports pymatgen's as_dict() and from_dict API.

        Args:
            modification (dict): Modification must be {action_keyword :
                settings}. E.g., {'_set': {'Hello':'Universe', 'Bye': 'World'}}
            obj (object): Object to modify
        """"""
        dct = obj.as_dict()
        self.modify(modification, dct)
        return obj.from_dict(dct)","
class Modder:
    '''
    Class to modify a dict/file/any object using a mongo-like language.
    Keywords are mostly adopted from mongo's syntax, but instead of $, an
    underscore precedes action keywords. This is so that the modification can
    be inserted into a mongo db easily.
    Allowable actions are supplied as a list of classes as an argument. Refer
    to the action classes on what the actions do. Action classes are in
    pymatpro.ansible.actions.
    Examples:
    >>> modder = Modder()
    >>> dct = {""Hello"": ""World""}
    >>> mod = {'_set': {'Hello':'Universe', 'Bye': 'World'}}
    >>> modder.modify(mod, dct)
    >>> dct['Bye']
    'World'
    >>> dct['Hello']
    'Universe'
    '''

    def __init__(self, actions=None, strict=True, directory='./') -> None:
        '''Initialize a Modder from a list of supported actions.
        Args:
            actions ([Action]): A sequence of supported actions. See
                :mod:`custodian.ansible.actions`. Default is None,
                which means only DictActions are supported.
            strict (bool): Indicating whether to use strict mode. In non-strict
                mode, unsupported actions are simply ignored without any
                errors raised. In strict mode, if an unsupported action is
                supplied, a ValueError is raised. Defaults to True.
            directory (str): The directory containing the files to be modified.
                Defaults to ""./"".
        '''
        pass

    def modify(self, modification, obj) -> None:
        '''
        Note that modify makes actual in-place modifications. It does not
        return a copy.
        Args:
            modification (dict): Modification must be {action_keyword :
                settings}. E.g., {'_set': {'Hello':'Universe', 'Bye': 'World'}}
            obj (dict/str/object): Object to modify depending on actions. For
                example, for DictActions, obj will be a dict to be modified.
                For FileActions, obj will be a string with a full pathname to a
                file.
        '''
        pass

    def modify_object(self, modification, obj):
        '''
        Modify an object that supports pymatgen's as_dict() and from_dict API.
        Args:
            modification (dict): Modification must be {action_keyword :
                settings}. E.g., {'_set': {'Hello':'Universe', 'Bye': 'World'}}
            obj (object): Object to modify
        '''
        pass",snippet_176,"class Modder:
    '''
    Class to modify a dict/file/any object using a mongo-like language.
    Keywords are mostly adopted from mongo's syntax, but instead of $, an
    underscore precedes action keywords. This is so that the modification can
    be inserted into a mongo db easily.
    Allowable actions are supplied as a list of classes as an argument. Refer
    to the action classes on what the actions do. Action classes are in
    pymatpro.ansible.actions.
    Examples:
    >>> modder = Modder()
    >>> dct = {""Hello"": ""World""}
    >>> mod = {'_set': {'Hello':'Universe', 'Bye': 'World'}}
    >>> modder.modify(mod, dct)
    >>> dct['Bye']
    'World'
    >>> dct['Hello']
    'Universe'
    '''

    class _DefaultDictActions:
        ALLOWED_ACTIONS = {
            ""_set"": ""set"",
            ""_unset"": ""unset"",
        }

        def __init__(self, directory=""./"") -> None:
            self.directory = directory

        def set(self, dct, settings):
            if not isinstance(dct, dict):
                raise TypeError(""Default DictActions only operate on dict objects."")
            if not isinstance(settings, dict):
                raise TypeError(""_set requires a dict of key: value pairs."")
            for k, v in settings.items():
                dct[k] = v

        def unset(self, dct, settings):
            if not isinstance(dct, dict):
                raise TypeError(""Default DictActions only operate on dict objects."")
            keys = []
            if isinstance(settings, dict):
                keys = [k for k, v in settings.items() if v]
            elif isinstance(settings, (list, tuple, set)):
                keys = list(settings)
            elif isinstance(settings, str):
                keys = [settings]
            else:
                raise TypeError(""_unset requires a dict/list/str of keys to remove."")
            for k in keys:
                if k in dct:
                    del dct[k]

    def __init__(self, actions=None, strict=True, directory='./') -> None:
        '''Initialize a Modder from a list of supported actions.
        Args:
            actions ([Action]): A sequence of supported actions. See
                :mod:`custodian.ansible.actions`. Default is None,
                which means only DictActions are supported.
            strict (bool): Indicating whether to use strict mode. In non-strict
                mode, unsupported actions are simply ignored without any
                errors raised. In strict mode, if an unsupported action is
                supplied, a ValueError is raised. Defaults to True.
            directory (str): The directory containing the files to be modified.
                Defaults to ""./"".
        '''
        self.directory = directory
        self.strict = bool(strict)
        if actions is None:
            action_classes = [self._DefaultDictActions]
        else:
            action_classes = list(actions)

        self._actions = []
        for cls in action_classes:
            try:
                self._actions.append(cls(self.directory))
            except TypeError:
                # Fallback if the action does not accept directory
                self._actions.append(cls())

    def _apply_with_action(self, action_obj, action_key, settings, obj):
        # 1) Preferred: ALLOWED_ACTIONS mapping
        mapping = getattr(action_obj, ""ALLOWED_ACTIONS"", None)
        if isinstance(mapping, dict) and action_key in mapping:
            method_name = mapping[action_key]
            method = getattr(action_obj, method_name, None)
            if callable(method):
                method(obj, settings)
                return True

        # 2) Fallback: method with underscored key removed, e.g. ""_set"" -> ""set""
        plain_name = action_key.lstrip(""_"")
        method = getattr(action_obj, plain_name, None)
        if callable(method):
            method(obj, settings)
            return True

        # 3) Fallback: generic apply(action_key, settings, obj)
        apply_method = getattr(action_obj, ""apply"", None)
        if callable(apply_method):
            result = apply_method(action_key, settings, obj)
            # If apply does not indicate success, assume it succeeded by not raising.
            return True if result is None else bool(result)

        return False

    def modify(self, modification, obj) -> None:
        '''
        Note that modify makes actual in-place modifications. It does not
        return a copy.
        Args:
            modification (dict): Modification must be {action_keyword :
                settings}. E.g., {'_set': {'Hello':'Universe', 'Bye': 'World'}}
            obj (dict/str/object): Object to modify depending on actions. For
                example, for DictActions, obj will be a dict to be modified.
                For FileActions, obj will be a string with a full pathname to a
                file.
        '''
        if not isinstance(modification, dict):
            raise TypeError(""modification must be a dict of {action_keyword: settings}."")

        for action_key, settings in modification.items():
            matched = False
            for action in self._actions:
                if self._apply_with_action(action, action_key, settings, obj):
                    matched = True
                    break
            if not matched and self.strict:
                raise ValueError(f""Unsupported action '{action_key}' for supplied actions."")

    def modify_object(self, modification, obj):
        '''
        Modify an object that supports pymatgen's as_dict() and from_dict API.
        Args:
            modification (dict): Modification must be {action_keyword :
                settings}. E.g., {'_set': {'Hello':'Universe', 'Bye': 'World'}}
            obj (object): Object to modify
        '''
        if not hasattr(obj, ""as_dict""):
            raise TypeError(""Object must implement as_dict()."")
        if not hasattr(obj.__class__, ""from_dict""):
            raise TypeError(""Object's class must implement from_dict(d)."")

        dct = obj.as_dict()
        self.modify(modification, dct)
        return obj.__class__.from_dict(dct)"
264406,materialsproject/custodian,materialsproject_custodian/src/custodian/utils.py,custodian.utils.tracked_lru_cache,"import functools
from typing import ClassVar

class tracked_lru_cache:
    """"""
    Decorator wrapping the functools.lru_cache adding a tracking of the
    functions that have been wrapped.

    Exposes a method to clear the cache of all the wrapped functions.

    Used to cache the parsed outputs in handlers/validators, to avoid
    multiple parsing of the same file.
    Allows Custodian to clear the cache after all the checks have been performed.
    """"""
    cached_functions: ClassVar = set()

    def __init__(self, func) -> None:
        """"""
        Args:
            func: function to be decorated.
        """"""
        self.func = functools.lru_cache(func)
        functools.update_wrapper(self, func)
        self.cache_info = self.func.cache_info
        self.cache_clear = self.func.cache_clear

    def __call__(self, *args, **kwargs):
        """"""Call the decorated function.""""""
        result = self.func(*args, **kwargs)
        self.cached_functions.add(self.func)
        return result

    @classmethod
    def tracked_cache_clear(cls) -> None:
        """"""Clear the cache of all the decorated functions.""""""
        while cls.cached_functions:
            f = cls.cached_functions.pop()
            f.cache_clear()","
class tracked_lru_cache:
    '''
    Decorator wrapping the functools.lru_cache adding a tracking of the
    functions that have been wrapped.
    Exposes a method to clear the cache of all the wrapped functions.
    Used to cache the parsed outputs in handlers/validators, to avoid
    multiple parsing of the same file.
    Allows Custodian to clear the cache after all the checks have been performed.
    '''

    def __init__(self, func) -> None:
        '''
        Args:
            func: function to be decorated.
        '''
        pass

    def __call__(self, *args, **kwargs):
        '''Call the decorated function.'''
        pass
    @classmethod
    def tracked_cache_clear(cls) -> None:
        '''Clear the cache of all the decorated functions.'''
        pass",snippet_177,"import functools
import weakref

class tracked_lru_cache:
    '''
    Decorator wrapping the functools.lru_cache adding a tracking of the
    functions that have been wrapped.
    Exposes a method to clear the cache of all the wrapped functions.
    Used to cache the parsed outputs in handlers/validators, to avoid
    multiple parsing of the same file.
    Allows Custodian to clear the cache after all the checks have been performed.
    '''

    _tracked_instances = weakref.WeakSet()

    def __init__(self, func) -> None:
        '''
        Args:
            func: function to be decorated.
        '''
        self._func = func
        self._cached = functools.lru_cache(maxsize=None)(func)
        # Copy metadata for nicer introspection
        functools.update_wrapper(self, func)
        type(self)._tracked_instances.add(self)

    def __call__(self, *args, **kwargs):
        '''Call the decorated function.'''
        return self._cached(*args, **kwargs)

    def __get__(self, instance, owner):
        if instance is None:
            return self
        # Bind instance as first argument for method usage
        return functools.partial(self.__call__, instance)

    @classmethod
    def tracked_cache_clear(cls) -> None:
        '''Clear the cache of all the decorated functions.'''
        for inst in list(cls._tracked_instances):
            try:
                inst._cached.cache_clear()
            except Exception:
                pass"
265199,Anaconda-Platform/anaconda-client,Anaconda-Platform_anaconda-client/binstar_client/utils/tables.py,binstar_client.utils.tables.TableCell,"import typing

class TableCell:
    """"""
    General definition of a table cell.

    :param kind: Kind of the cell (used for styling purposes, see :class:`~TableDesign`).
    :param value: Exact content of the cell.
    :param alignment: How text should be aligned in the cell.
    """"""
    __slots__ = ('alignment', 'kind', 'value')

    def __init__(self, kind: str, value: typing.Any, *, alignment: 'Alignment'='<') -> None:
        """"""Initialize new :class:`~TableCell` instance.""""""
        if value is None:
            value = ''
        self.alignment: 'Alignment' = alignment
        self.kind: str = kind
        self.value: str = str(value)

    def __repr__(self) -> str:
        """"""Prepare a string representation of the instance.""""""
        return f'{type(self).__name__}(kind={self.kind!r}, value={self.value!r}, alignment={self.alignment!r})'

    def __str__(self) -> str:
        """"""Prepare a string representation of the instance.""""""
        return self.value","
class TableCell:
    '''
    General definition of a table cell.
    :param kind: Kind of the cell (used for styling purposes, see :class:`~TableDesign`).
    :param value: Exact content of the cell.
    :param alignment: How text should be aligned in the cell.
    '''

    def __init__(self, kind: str, value: typing.Any, *, alignment: 'Alignment'='<') -> None:
        '''Initialize new :class:`~TableCell` instance.'''
        pass

    def __repr__(self) -> str:
        '''Prepare a string representation of the instance.'''
        pass

    def __str__(self) -> str:
        '''Prepare a string representation of the instance.'''
        pass",snippet_178,"import typing

class TableCell:
    '''
    General definition of a table cell.
    :param kind: Kind of the cell (used for styling purposes, see :class:`~TableDesign`).
    :param value: Exact content of the cell.
    :param alignment: How text should be aligned in the cell.
    '''

    def __init__(self, kind: str, value: typing.Any, *, alignment: 'Alignment' = '<') -> None:
        '''Initialize new :class:`~TableCell` instance.'''
        if not isinstance(kind, str):
            raise TypeError(""kind must be a string"")
        if not isinstance(alignment, str):
            raise TypeError(""alignment must be a string"")
        if alignment not in ('<', '^', '>'):
            raise ValueError(""alignment must be one of '<', '^', '>'"")

        self.kind: str = kind
        self.value: typing.Any = value
        self.alignment: str = alignment

    def __repr__(self) -> str:
        '''Prepare a string representation of the instance.'''
        return f""TableCell(kind={self.kind!r}, value={self.value!r}, alignment={self.alignment!r})""

    def __str__(self) -> str:
        '''Prepare a string representation of the instance.'''
        return str(self.value)"
266878,facelessuser/wcmatch,facelessuser_wcmatch/wcmatch/util.py,wcmatch.util.Immutable,"from typing import Any, Callable, AnyStr, Match, Pattern

class Immutable:
    """"""Immutable.""""""
    __slots__: tuple[Any, ...] = ()

    def __init__(self, **kwargs: Any) -> None:
        """"""Initialize.""""""
        for k, v in kwargs.items():
            super(Immutable, self).__setattr__(k, v)

    def __setattr__(self, name: str, value: Any) -> None:
        """"""Prevent mutability.""""""
        raise AttributeError('Class is immutable!')","
class Immutable:
    '''Immutable.'''

    def __init__(self, **kwargs: Any) -> None:
        '''Initialize.'''
        pass

    def __setattr__(self, name: str, value: Any) -> None:
        '''Prevent mutability.'''
        pass",snippet_180,"from typing import Any

class Immutable:
    '''Immutable.'''

    def __init__(self, **kwargs: Any) -> None:
        '''Initialize.'''
        for k, v in kwargs.items():
            object.__setattr__(self, k, v)
        object.__setattr__(self, '_Immutable__frozen', True)

    def __setattr__(self, name: str, value: Any) -> None:
        '''Prevent mutability.'''
        if name == '_Immutable__frozen':
            object.__setattr__(self, name, value)
            return
        if getattr(self, '_Immutable__frozen', False):
            raise AttributeError(f'{self.__class__.__name__} is immutable')
        object.__setattr__(self, name, value)"
269275,CLARIAH/grlc,CLARIAH_grlc/src/fileLoaders.py,src.fileLoaders.BaseLoader,"from grlc.queryTypes import qType, guessQueryType
import json

class BaseLoader:
    """"""Base class for File Loaders""""""

    def getTextForName(self, query_name):
        """"""Return the query text and query type for the given query name.
        Note that file extention is not part of the query name. For example,
        for `query_name='query1'` would return the content of file `query1.rq`
        from the loader's source (assuming such file exists).""""""
        candidateNames = [query_name + '.rq', query_name + '.sparql', query_name + '.tpf', query_name + '.json']
        candidates = [(name, guessQueryType(name)) for name in candidateNames]
        for queryFullName, queryType in candidates:
            queryText = self._getText(queryFullName)
            if queryText:
                if queryType == qType['JSON']:
                    queryText = json.loads(queryText)
                    if 'proto' not in queryText and '@graph' not in queryText:
                        continue
                return (queryText, queryType)
        return ('', None)

    def _getText(self, queryFullName):
        """"""To be implemented by sub-classes.
        Returns None if the file does not exist.""""""
        raise NotImplementedError('Subclasses must override _getText()!')

    def fetchFiles(self):
        """"""To be implemented by sub-classes""""""
        raise NotImplementedError('Subclasses must override fetchFiles()!')","
class BaseLoader:
    '''Base class for File Loaders'''

    def getTextForName(self, query_name):
        '''Return the query text and query type for the given query name.
        Note that file extention is not part of the query name. For example,
        for `query_name='query1'` would return the content of file `query1.rq`
        from the loader's source (assuming such file exists).'''
        pass

    def _getText(self, queryFullName):
        '''To be implemented by sub-classes.
        Returns None if the file does not exist.'''
        pass

    def fetchFiles(self):
        '''To be implemented by sub-classes'''
        pass",snippet_181,"class BaseLoader:
    '''Base class for File Loaders'''

    _EXT_TYPE_MAP = {
        'rq': 'query',
        'ru': 'update',
    }

    def getTextForName(self, query_name):
        '''Return the query text and query type for the given query name.
        Note that file extention is not part of the query name. For example,
        for `query_name='query1'` would return the content of file `query1.rq`
        from the loader's source (assuming such file exists).'''
        if not query_name or not isinstance(query_name, str):
            raise ValueError(""query_name must be a non-empty string"")

        # Try preferred known extensions first
        for ext, qtype in (('rq', 'query'), ('ru', 'update')):
            full = f""{query_name}.{ext}""
            text = self._getText(full)
            if text is not None:
                return text, qtype

        # Fallback: search via available files from the source
        try:
            files = self.fetchFiles()
        except NotImplementedError:
            files = None

        if files:
            prefix = f""{query_name}.""
            candidates = [f for f in files if isinstance(f, str) and f.startswith(prefix)]
            if candidates:
                # Prefer known extensions deterministically, else choose lexicographically
                known = [f for f in candidates if f.rsplit('.', 1)[-1] in self._EXT_TYPE_MAP]
                chosen = None
                if known:
                    # Prefer rq then ru if present
                    prioritized = [f""{query_name}.rq"", f""{query_name}.ru""]
                    for p in prioritized:
                        if p in known:
                            chosen = p
                            break
                    if chosen is None:
                        chosen = sorted(known)[0]
                else:
                    chosen = sorted(candidates)[0]

                text = self._getText(chosen)
                if text is not None:
                    ext = chosen.rsplit('.', 1)[-1]
                    qtype = self._EXT_TYPE_MAP.get(ext, ext)
                    return text, qtype

        raise FileNotFoundError(f""No file found for query name '{query_name}' with known or available extensions"")

    def _getText(self, queryFullName):
        '''To be implemented by sub-classes.
        Returns None if the file does not exist.'''
        raise NotImplementedError

    def fetchFiles(self):
        '''To be implemented by sub-classes'''
        raise NotImplementedError"
271814,shoebot/shoebot,shoebot_shoebot/lib/photobot/__init__.py,photobot.Blend,"from PIL import Image, ImageChops, ImageFilter, ImageEnhance, ImageOps, ImageDraw, ImageStat

class Blend:
    """"""Layer blending modes.

    Implements additional blending modes to those present in PIL.
    These blending functions can not be used separately from
    the canvas.flatten() method, where the alpha compositing
    of two layers is handled.

    Since these blending are not part of a C library,
    but pure Python, they take forever to process.
    """"""

    def overlay(self, img1, img2):
        """"""Applies the overlay blend mode.

        Overlays image img2 on image img1. The overlay pixel combines
        multiply and screen: it multiplies dark pixels values and screen
        light values. Returns a composite image with the alpha channel
        retained.
        """"""
        p1 = list(img1.getdata())
        p2 = list(img2.getdata())
        for i in range(len(p1)):
            p3 = ()
            for j in range(len(p1[i])):
                a = p1[i][j] / 255.0
                b = p2[i][j] / 255.0
                if j == 3:
                    d = min(a, b)
                elif a > 0.5:
                    d = 2 * (a + b - a * b) - 1
                else:
                    d = 2 * a * b
                p3 += (int(d * 255),)
            p1[i] = p3
        img = Image.new('RGBA', img1.size, 255)
        img.putdata(p1)
        return img

    def hue(self, img1, img2):
        """"""Applies the hue blend mode.

        Hues image img1 with image img2. The hue filter replaces the
        hues of pixels in img1 with the hues of pixels in img2. Returns
        a composite image with the alpha channel retained.
        """"""
        import colorsys
        p1 = list(img1.getdata())
        p2 = list(img2.getdata())
        for i in range(len(p1)):
            r1, g1, b1, a1 = p1[i]
            r1 = r1 / 255.0
            g1 = g1 / 255.0
            b1 = b1 / 255.0
            h1, s1, v1 = colorsys.rgb_to_hsv(r1, g1, b1)
            r2, g2, b2, a2 = p2[i]
            r2 = r2 / 255.0
            g2 = g2 / 255.0
            b2 = b2 / 255.0
            h2, s2, v2 = colorsys.rgb_to_hsv(r2, g2, b2)
            r3, g3, b3 = colorsys.hsv_to_rgb(h2, s1, v1)
            r3 = int(r3 * 255)
            g3 = int(g3 * 255)
            b3 = int(b3 * 255)
            p1[i] = (r3, g3, b3, a1)
        img = Image.new('RGBA', img1.size, 255)
        img.putdata(p1)
        return img

    def color(self, img1, img2):
        """"""Applies the color blend mode.

        Colorize image img1 with image img2. The color filter replaces
        the hue and saturation of pixels in img1 with the hue and
        saturation of pixels in img2. Returns a composite image with the
        alpha channel retained.
        """"""
        import colorsys
        p1 = list(img1.getdata())
        p2 = list(img2.getdata())
        for i in range(len(p1)):
            r1, g1, b1, a1 = p1[i]
            r1 = r1 / 255.0
            g1 = g1 / 255.0
            b1 = b1 / 255.0
            h1, s1, v1 = colorsys.rgb_to_hsv(r1, g1, b1)
            r2, g2, b2, a2 = p2[i]
            r2 = r2 / 255.0
            g2 = g2 / 255.0
            b2 = b2 / 255.0
            h2, s2, v2 = colorsys.rgb_to_hsv(r2, g2, b2)
            r3, g3, b3 = colorsys.hsv_to_rgb(h2, s2, v1)
            r3 = int(r3 * 255)
            g3 = int(g3 * 255)
            b3 = int(b3 * 255)
            p1[i] = (r3, g3, b3, a1)
        img = Image.new('RGBA', img1.size, 255)
        img.putdata(p1)
        return img","
class Blend:
    '''Layer blending modes.
    Implements additional blending modes to those present in PIL.
    These blending functions can not be used separately from
    the canvas.flatten() method, where the alpha compositing
    of two layers is handled.
    Since these blending are not part of a C library,
    but pure Python, they take forever to process.
    '''

    def overlay(self, img1, img2):
        '''Applies the overlay blend mode.
        Overlays image img2 on image img1. The overlay pixel combines
        multiply and screen: it multiplies dark pixels values and screen
        light values. Returns a composite image with the alpha channel
        retained.
        '''
        pass

    def hue(self, img1, img2):
        '''Applies the hue blend mode.
        Hues image img1 with image img2. The hue filter replaces the
        hues of pixels in img1 with the hues of pixels in img2. Returns
        a composite image with the alpha channel retained.
        '''
        pass

    def color(self, img1, img2):
        '''Applies the color blend mode.
        Colorize image img1 with image img2. The color filter replaces
        the hue and saturation of pixels in img1 with the hue and
        saturation of pixels in img2. Returns a composite image with the
        alpha channel retained.
        '''
        pass",snippet_182,"from PIL import Image
import colorsys

try:
    import numpy as np
except Exception:
    np = None


class Blend:
    '''Layer blending modes.
    Implements additional blending modes to those present in PIL.
    These blending functions can not be used separately from
    the canvas.flatten() method, where the alpha compositing
    of two layers is handled.
    Since these blending are not part of a C library,
    but pure Python, they take forever to process.
    '''

    def _ensure_rgba(self, img):
        if img.mode != ""RGBA"":
            return img.convert(""RGBA"")
        return img

    def overlay(self, img1, img2):
        '''Applies the overlay blend mode.
        Overlays image img2 on image img1. The overlay pixel combines
        multiply and screen: it multiplies dark pixels values and screen
        light values. Returns a composite image with the alpha channel
        retained.
        '''
        img1 = self._ensure_rgba(img1)
        img2 = self._ensure_rgba(img2)

        if np is not None:
            a1 = np.array(img1)
            a2 = np.array(img2)
            base = a1[..., :3].astype(np.int16)
            blend = a2[..., :3].astype(np.int16)

            mask = base < 128
            out = np.empty_like(base)

            # result = (2*a*b)/255 when base<128
            out[mask] = (2 * base[mask] * blend[mask] + 255) // 255
            # result = 255 - 2*(255-a)*(255-b)/255 when base>=128
            inv_base = 255 - base[~mask]
            inv_blend = 255 - blend[~mask]
            out[~mask] = 255 - ((2 * inv_base * inv_blend + 255) // 255)

            out = np.clip(out, 0, 255).astype(np.uint8)
            # retain alpha from img1
            alpha = a1[..., 3:4]
            comp = np.concatenate([out, alpha], axis=2)
            return Image.fromarray(comp, mode=""RGBA"")

        # Fallback pure Python
        w, h = img1.size
        p1 = img1.load()
        p2 = img2.load()
        out = Image.new(""RGBA"", (w, h))
        po = out.load()

        for y in range(h):
            for x in range(w):
                r1, g1, b1, a = p1[x, y]
                r2, g2, b2, _ = p2[x, y]

                def ov(a_, b_):
                    if a_ < 128:
                        return (2 * a_ * b_) // 255
                    return 255 - ((2 * (255 - a_) * (255 - b_)) // 255)

                po[x, y] = (ov(r1, r2), ov(g1, g2), ov(b1, b2), a)
        return out

    def hue(self, img1, img2):
        '''Applies the hue blend mode.
        Hues image img1 with image img2. The hue filter replaces the
        hues of pixels in img1 with the hues of pixels in img2. Returns
        a composite image with the alpha channel retained.
        '''
        img1 = self._ensure_rgba(img1)
        img2 = self._ensure_rgba(img2)

        w, h = img1.size
        p1 = img1.load()
        p2 = img2.load()
        out = Image.new(""RGBA"", (w, h))
        po = out.load()

        for y in range(h):
            for x in range(w):
                r1, g1, b1, a1 = p1[x, y]
                r2, g2, b2, _ = p2[x, y]

                h1, l1, s1 = colorsys.rgb_to_hls(r1 / 255.0, g1 / 255.0, b1 / 255.0)
                h2, l2, s2 = colorsys.rgb_to_hls(r2 / 255.0, g2 / 255.0, b2 / 255.0)

                r, g, b = colorsys.hls_to_rgb(h2, l1, s1)
                po[x, y] = (int(round(r * 255)), int(round(g * 255)), int(round(b * 255)), a1)
        return out

    def color(self, img1, img2):
        '''Applies the color blend mode.
        Colorize image img1 with image img2. The color filter replaces
        the hue and saturation of pixels in img1 with the hue and
        saturation of pixels in img2. Returns a composite image with the
        alpha channel retained.
        '''
        img1 = self._ensure_rgba(img1)
        img2 = self._ensure_rgba(img2)

        w, h = img1.size
        p1 = img1.load()
        p2 = img2.load()
        out = Image.new(""RGBA"", (w, h))
        po = out.load()

        for y in range(h):
            for x in range(w):
                r1, g1, b1, a1 = p1[x, y]
                r2, g2, b2, _ = p2[x, y]

                h1, l1, s1 = colorsys.rgb_to_hls(r1 / 255.0, g1 / 255.0, b1 / 255.0)
                h2, l2, s2 = colorsys.rgb_to_hls(r2 / 255.0, g2 / 255.0, b2 / 255.0)

                r, g, b = colorsys.hls_to_rgb(h2, l1, s2)
                po[x, y] = (int(round(r * 255)), int(round(g * 255)), int(round(b * 255)), a1)
        return out"
272626,cackharot/suds-py3,cackharot_suds-py3/suds/reader.py,suds.reader.Reader,"from suds.plugin import PluginContainer
import hashlib

class Reader:
    """"""
    The reader provides integration with cache.
    @ivar options: An options object.
    @type options: I{Options}
    """"""

    def __init__(self, options):
        """"""
        @param options: An options object.
        @type options: I{Options}
        """"""
        self.options = options
        self.plugins = PluginContainer(options.plugins)

    def mangle(self, name, x):
        """"""
        Mangle the name by hashing the I{name} and appending I{x}.
        @return: the mangled name.
        """"""
        h = hashlib.sha256(name.encode('utf8')).hexdigest()
        return '%s-%s' % (h, x)","
class Reader:
    '''
    The reader provides integration with cache.
    @ivar options: An options object.
    @type options: I{Options}
    '''

    def __init__(self, options):
        '''
        @param options: An options object.
        @type options: I{Options}
        '''
        pass

    def mangle(self, name, x):
        '''
        Mangle the name by hashing the I{name} and appending I{x}.
        @return: the mangled name.
        '''
        pass",snippet_183,"import hashlib

class Reader:
    '''
    The reader provides integration with cache.
    @ivar options: An options object.
    @type options: I{Options}
    '''

    def __init__(self, options):
        '''
        @param options: An options object.
        @type options: I{Options}
        '''
        self.options = options

    def mangle(self, name, x):
        '''
        Mangle the name by hashing the I{name} and appending I{x}.
        @return: the mangled name.
        '''
        if isinstance(name, str):
            data = name.encode('utf-8')
        elif isinstance(name, (bytes, bytearray, memoryview)):
            data = bytes(name)
        else:
            raise TypeError(""name must be str or bytes-like"")

        digest = hashlib.md5(data).hexdigest()
        return f""{digest}{x}"""
272646,cackharot/suds-py3,cackharot_suds-py3/suds/sax/enc.py,suds.sax.enc.Encoder,"import re

class Encoder:
    """"""
    An XML special character encoder/decoder.
    @cvar encodings: A mapping of special characters encoding.
    @type encodings: [(str,str)]
    @cvar decodings: A mapping of special characters decoding.
    @type decodings: [(str,str)]
    @cvar special: A list of special characters
    @type special: [char]
    """"""
    encodings = (('&', '&amp;'), ('<', '&lt;'), ('>', '&gt;'), ('""', '&quot;'), (""'"", '&apos;'))
    decodings = (('&lt;', '<'), ('&gt;', '>'), ('&quot;', '""'), ('&apos;', ""'""), ('&amp;', '&'))
    special = ('&', '<', '>', '""', ""'"")

    def needsEncoding(self, s):
        """"""
        Get whether string I{s} contains special characters.
        @param s: A string to check.
        @type s: str
        @return: True if needs encoding.
        @rtype: boolean
        """"""
        if isinstance(s, str):
            for c in self.special:
                if c in s:
                    return True
        return False

    def encode(self, s):
        """"""
        Encode special characters found in string I{s}.
        @param s: A string to encode.
        @type s: str
        @return: The encoded string.
        @rtype: str
        """"""
        if isinstance(s, str) and self.needsEncoding(s):
            for x in self.encodings:
                s = re.sub(x[0], x[1], s)
        return s

    def decode(self, s):
        """"""
        Decode special characters encodings found in string I{s}.
        @param s: A string to decode.
        @type s: str
        @return: The decoded string.
        @rtype: str
        """"""
        if isinstance(s, str) and '&' in s:
            for x in self.decodings:
                s = s.replace(x[0], x[1])
        return s","
class Encoder:
    '''
    An XML special character encoder/decoder.
    @cvar encodings: A mapping of special characters encoding.
    @type encodings: [(str,str)]
    @cvar decodings: A mapping of special characters decoding.
    @type decodings: [(str,str)]
    @cvar special: A list of special characters
    @type special: [char]
    '''

    def needsEncoding(self, s):
        '''
        Get whether string I{s} contains special characters.
        @param s: A string to check.
        @type s: str
        @return: True if needs encoding.
        @rtype: boolean
        '''
        pass

    def encode(self, s):
        '''
        Encode special characters found in string I{s}.
        @param s: A string to encode.
        @type s: str
        @return: The encoded string.
        @rtype: str
        '''
        pass

    def decode(self, s):
        '''
        Decode special characters encodings found in string I{s}.
        @param s: A string to decode.
        @type s: str
        @return: The decoded string.
        @rtype: str
        '''
        pass",snippet_184,"class Encoder:
    '''
    An XML special character encoder/decoder.
    @cvar encodings: A mapping of special characters encoding.
    @type encodings: [(str,str)]
    @cvar decodings: A mapping of special characters decoding.
    @type decodings: [(str,str)]
    @cvar special: A list of special characters
    @type special: [char]
    '''

    # Class-level mappings for XML special characters
    encodings = [
        (""&"", ""&amp;""),
        (""<"", ""&lt;""),
        ("">"", ""&gt;""),
        ('""', ""&quot;""),
        (""'"", ""&apos;""),
    ]

    decodings = [
        (""&amp;"", ""&""),
        (""&lt;"", ""<""),
        (""&gt;"", "">""),
        (""&quot;"", '""'),
        (""&apos;"", ""'""),
    ]

    special = [c for c, _ in encodings]

    def needsEncoding(self, s):
        '''
        Get whether string I{s} contains special characters.
        @param s: A string to check.
        @type s: str
        @return: True if needs encoding.
        @rtype: boolean
        '''
        if not s:
            return False
        for ch in s:
            if ch in self.special:
                return True
        return False

    def encode(self, s):
        '''
        Encode special characters found in string I{s}.
        @param s: A string to encode.
        @type s: str
        @return: The encoded string.
        @rtype: str
        '''
        if s is None or s == """":
            return s
        # Encode by scanning to avoid double-encoding
        mapping = dict(self.encodings)
        out = []
        for ch in s:
            out.append(mapping.get(ch, ch))
        return """".join(out)

    def decode(self, s):
        '''
        Decode special characters encodings found in string I{s}.
        @param s: A string to decode.
        @type s: str
        @return: The decoded string.
        @rtype: str
        '''
        if s is None or s == """":
            return s
        # Iteratively replace to handle nested encodings like &amp;lt;
        changed = True
        while changed:
            changed = False
            for entity, ch in self.decodings:
                new_s = s.replace(entity, ch)
                if new_s != s:
                    changed = True
                    s = new_s
        return s"
272664,cackharot/suds-py3,cackharot_suds-py3/suds/transport/__init__.py,suds.transport.Transport,"class Transport:
    """"""
    The transport I{interface}.
    """"""

    def __init__(self):
        """"""
        Constructor.
        """"""
        from suds.transport.options import Options
        self.options = Options()
        del Options

    def open(self, request):
        """"""
        Open the url in the specified request.
        @param request: A transport request.
        @type request: L{Request}
        @return: An input stream.
        @rtype: stream
        @raise TransportError: On all transport errors.
        """"""
        raise Exception('not-implemented')

    def send(self, request):
        """"""
        Send soap message.  Implementations are expected to handle:
            - proxies
            - I{http} headers
            - cookies
            - sending message
            - brokering exceptions into L{TransportError}
        @param request: A transport request.
        @type request: L{Request}
        @return: The reply
        @rtype: L{Reply}
        @raise TransportError: On all transport errors.
        """"""
        raise Exception('not-implemented')","class Transport:
    '''
    The transport I{interface}.
    '''

    def __init__(self):
        '''
        Constructor.
        '''
        pass

    def open(self, request):
        '''
        Open the url in the specified request.
        @param request: A transport request.
        @type request: L{Request}
        @return: An input stream.
        @rtype: stream
        @raise TransportError: On all transport errors.
        '''
        pass

    def send(self, request):
        '''
        Send soap message.  Implementations are expected to handle:
            - proxies
            - I{http} headers
            - cookies
            - sending message
            - brokering exceptions into L{TransportError}
        @param request: A transport request.
        @type request: L{Request}
        @return: The reply
        @rtype: L{Reply}
        @raise TransportError: On all transport errors.
        '''
        pass",snippet_185,"from abc import ABC, abstractmethod

class Transport(ABC):
    '''
    The transport interface.
    '''

    def __init__(self):
        '''
        Constructor.
        '''
        super().__init__()

    @abstractmethod
    def open(self, request):
        '''
        Open the url in the specified request.
        @param request: A transport request.
        @type request: L{Request}
        @return: An input stream.
        @rtype: stream
        @raise TransportError: On all transport errors.
        '''
        raise NotImplementedError(""open() must be implemented by subclasses"")

    @abstractmethod
    def send(self, request):
        '''
        Send soap message.  Implementations are expected to handle:
            - proxies
            - http headers
            - cookies
            - sending message
            - brokering exceptions into L{TransportError}
        @param request: A transport request.
        @type request: L{Request}
        @return: The reply
        @rtype: L{Reply}
        @raise TransportError: On all transport errors.
        '''
        raise NotImplementedError(""send() must be implemented by subclasses"")"
272699,cackharot/suds-py3,cackharot_suds-py3/suds/xsd/doctor.py,suds.xsd.doctor.TnsFilter,"class TnsFilter:
    """"""
    Target Namespace filter.
    @ivar tns: A list of target namespaces.
    @type tns: [str,...]
    """"""

    def __init__(self, *tns):
        """"""
        @param tns: A list of target namespaces.
        @type tns: [str,...]
        """"""
        self.tns = []
        self.add(*tns)

    def add(self, *tns):
        """"""
        Add I{targetNamesapces} to be added.
        @param tns: A list of target namespaces.
        @type tns: [str,...]
        """"""
        self.tns += tns

    def match(self, root, ns):
        """"""
        Match by I{targetNamespace} excluding those that
        are equal to the specified namespace to prevent
        adding an import to itself.
        @param root: A schema root.
        @type root: L{Element}
        """"""
        tns = root.get('targetNamespace')
        if len(self.tns):
            matched = tns in self.tns
        else:
            matched = 1
        itself = ns == tns
        return matched and (not itself)","class TnsFilter:
    '''
    Target Namespace filter.
    @ivar tns: A list of target namespaces.
    @type tns: [str,...]
    '''

    def __init__(self, *tns):
        '''
        @param tns: A list of target namespaces.
        @type tns: [str,...]
        '''
        pass

    def add(self, *tns):
        '''
        Add I{targetNamesapces} to be added.
        @param tns: A list of target namespaces.
        @type tns: [str,...]
        '''
        pass

    def match(self, root, ns):
        '''
        Match by I{targetNamespace} excluding those that
        are equal to the specified namespace to prevent
        adding an import to itself.
        @param root: A schema root.
        @type root: L{Element}
        '''
        pass",snippet_186,"class TnsFilter:
    '''
    Target Namespace filter.
    @ivar tns: A list of target namespaces.
    @type tns: [str,...]
    '''

    def __init__(self, *tns):
        '''
        @param tns: A list of target namespaces.
        @type tns: [str,...]
        '''
        self.tns = []
        self._tns_set = set()
        if tns:
            self.add(*tns)

    def add(self, *tns):
        '''
        Add targetNamesapces to be added.
        @param tns: A list of target namespaces.
        @type tns: [str,...]
        '''
        # Support passing a single iterable like list/tuple/set
        if len(tns) == 1 and isinstance(tns[0], (list, tuple, set)):
            tns_iter = tns[0]
        else:
            tns_iter = tns

        for item in tns_iter:
            if item is None:
                continue
            s = str(item)
            if not s:
                continue
            if s not in self._tns_set:
                self._tns_set.add(s)
                self.tns.append(s)

    def match(self, root, ns):
        '''
        Match by targetNamespace excluding those that
        are equal to the specified namespace to prevent
        adding an import to itself.
        @param root: A schema root.
        @type root: L{Element}
        '''
        if root is None:
            return False

        # Try to obtain the targetNamespace from element
        tns_value = None
        if hasattr(root, 'get'):
            tns_value = root.get('targetNamespace')
        if tns_value is None and hasattr(root, 'attrib'):
            tns_value = root.attrib.get('targetNamespace')

        if not tns_value:
            return False
        if ns is not None and tns_value == ns:
            return False

        if not self._tns_set:
            return True

        return tns_value in self._tns_set"
272711,cackharot/suds-py3,cackharot_suds-py3/suds/xsd/sxbase.py,suds.xsd.sxbase.NodeFinder,"class NodeFinder:
    """"""
    Find nodes based on flexable criteria.  The I{matcher} is
    may be any object that implements a match(n) method.
    @ivar matcher: An object used as criteria for match.
    @type matcher: I{any}.match(n)
    @ivar limit: Limit the number of matches.  0=unlimited.
    @type limit: int
    """"""

    def __init__(self, matcher, limit=0):
        """"""
        @param matcher: An object used as criteria for match.
        @type matcher: I{any}.match(n)
        @param limit: Limit the number of matches.  0=unlimited.
        @type limit: int
        """"""
        self.matcher = matcher
        self.limit = limit

    def find(self, node, list):
        """"""
        Traverse the tree looking for matches.
        @param node: A node to match on.
        @type node: L{SchemaObject}
        @param list: A list to fill.
        @type list: list
        """"""
        if self.matcher.match(node):
            list.append(node)
            self.limit -= 1
            if self.limit == 0:
                return
        for c in node.rawchildren:
            self.find(c, list)
        return self","class NodeFinder:
    '''
    Find nodes based on flexable criteria.  The I{matcher} is
    may be any object that implements a match(n) method.
    @ivar matcher: An object used as criteria for match.
    @type matcher: I{any}.match(n)
    @ivar limit: Limit the number of matches.  0=unlimited.
    @type limit: int
    '''

    def __init__(self, matcher, limit=0):
        '''
        @param matcher: An object used as criteria for match.
        @type matcher: I{any}.match(n)
        @param limit: Limit the number of matches.  0=unlimited.
        @type limit: int
        '''
        pass

    def find(self, node, list):
        '''
        Traverse the tree looking for matches.
        @param node: A node to match on.
        @type node: L{SchemaObject}
        @param list: A list to fill.
        @type list: list
        '''
        pass",snippet_187,"class NodeFinder:
    '''
    Find nodes based on flexable criteria.  The I{matcher} is
    may be any object that implements a match(n) method.
    @ivar matcher: An object used as criteria for match.
    @type matcher: I{any}.match(n)
    @ivar limit: Limit the number of matches.  0=unlimited.
    @type limit: int
    '''

    def __init__(self, matcher, limit=0):
        '''
        @param matcher: An object used as criteria for match.
        @type matcher: I{any}.match(n)
        @param limit: Limit the number of matches.  0=unlimited.
        @type limit: int
        '''
        self.matcher = matcher
        self.limit = int(limit) if limit is not None else 0

    def _match(self, node):
        m = getattr(self.matcher, ""match"", None)
        if callable(m):
            return bool(m(node))
        if callable(self.matcher):
            return bool(self.matcher(node))
        return False

    def _children_of(self, node):
        # Try common child containers
        for attr in (""children"", ""childNodes"", ""contents"", ""nodes""):
            if hasattr(node, attr):
                children = getattr(node, attr)
                if children is not None:
                    try:
                        return list(children)
                    except TypeError:
                        pass
        # Methods possibly returning children
        for meth in (""getChildren"", ""iterchildren"", ""iterChildren"", ""get_children""):
            f = getattr(node, meth, None)
            if callable(f):
                children = f()
                try:
                    return list(children)
                except TypeError:
                    pass
        # If node itself is iterable (but not a string/bytes), treat as container
        if hasattr(node, ""__iter__"") and not isinstance(node, (str, bytes, bytearray)):
            try:
                return list(iter(node))
            except TypeError:
                pass
        return []

    def _recurse(self, node, out_list):
        if node is None:
            return False
        if self._match(node):
            out_list.append(node)
            if self.limit and len(out_list) >= self.limit:
                return True  # reached limit
        for child in self._children_of(node):
            if self._recurse(child, out_list):
                return True
        return False

    def find(self, node, list):
        '''
        Traverse the tree looking for matches.
        @param node: A node to match on.
        @type node: L{SchemaObject}
        @param list: A list to fill.
        @type list: list
        '''
        if list is None:
            raise ValueError(""list must be a list to fill"")
        self._recurse(node, list)
        return list"
279001,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.AES_CTR_Mechanism,"import PyKCS11.LowLevel

class AES_CTR_Mechanism:
    """"""CKM_AES_CTR encryption mechanism""""""

    def __init__(self, counterBits, counterBlock):
        """"""
        :param counterBits: the number of incremented bits in the counter block
        :param counterBlock: a 16-byte initial value of the counter block
        """"""
        self._param = PyKCS11.LowLevel.CK_AES_CTR_PARAMS()
        self._source_cb = ckbytelist(counterBlock)
        self._param.ulCounterBits = counterBits
        self._param.cb = self._source_cb
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_AES_CTR
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_AES_CTR_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class AES_CTR_Mechanism:
    '''CKM_AES_CTR encryption mechanism'''

    def __init__(self, counterBits, counterBlock):
        '''
        :param counterBits: the number of incremented bits in the counter block
        :param counterBlock: a 16-byte initial value of the counter block
        '''
        pass

    def to_native(self):
        '''convert mechanism to native format'''
        pass",snippet_190,"class AES_CTR_Mechanism:
    '''CKM_AES_CTR encryption mechanism'''

    def __init__(self, counterBits, counterBlock):
        '''
        :param counterBits: the number of incremented bits in the counter block
        :param counterBlock: a 16-byte initial value of the counter block
        '''
        if not isinstance(counterBits, int):
            raise TypeError(""counterBits must be an integer"")
        if not (1 <= counterBits <= 128):
            raise ValueError(""counterBits must be between 1 and 128 inclusive"")
        try:
            cb = bytes(counterBlock)
        except Exception as e:
            raise TypeError(""counterBlock must be a bytes-like object"") from e
        if len(cb) != 16:
            raise ValueError(""counterBlock must be exactly 16 bytes"")
        self.counterBits = counterBits
        self.counterBlock = cb

    def to_native(self):
        '''convert mechanism to native format'''
        return {
            'ulCounterBits': self.counterBits,
            'cb': self.counterBlock,
        }"
279002,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.AES_GCM_Mechanism,"import PyKCS11.LowLevel

class AES_GCM_Mechanism:
    """"""CKM_AES_GCM warpping mechanism""""""

    def __init__(self, iv, aad, tagBits):
        """"""
        :param iv: initialization vector
        :param aad: additional authentication data
        :param tagBits: length of authentication tag in bits
        """"""
        self._param = PyKCS11.LowLevel.CK_GCM_PARAMS()
        self._source_iv = ckbytelist(iv)
        self._param.pIv = self._source_iv
        self._param.ulIvLen = len(self._source_iv)
        self._source_aad = ckbytelist(aad)
        self._param.pAAD = self._source_aad
        self._param.ulAADLen = len(self._source_aad)
        self._param.ulTagBits = tagBits
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_AES_GCM
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_GCM_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class AES_GCM_Mechanism:
    '''CKM_AES_GCM warpping mechanism'''

    def __init__(self, iv, aad, tagBits):
        '''
        :param iv: initialization vector
        :param aad: additional authentication data
        :param tagBits: length of authentication tag in bits
        '''
        pass

    def to_native(self):
        '''convert mechanism to native format'''
        pass",snippet_191,"class AES_GCM_Mechanism:
    '''CKM_AES_GCM warpping mechanism'''

    import ctypes as _ctypes

    class _CK_GCM_PARAMS(_ctypes.Structure):
        _fields_ = [
            ('pIv', _ctypes.c_void_p),
            ('ulIvLen', _ctypes.c_ulong),
            ('pAAD', _ctypes.c_void_p),
            ('ulAADLen', _ctypes.c_ulong),
            ('ulTagBits', _ctypes.c_ulong),
        ]

    def __init__(self, iv, aad, tagBits):
        '''
        :param iv: initialization vector
        :param aad: additional authentication data
        :param tagBits: length of authentication tag in bits
        '''
        # Normalize inputs
        if iv is None:
            iv_bytes = b""""
        else:
            try:
                iv_bytes = bytes(iv)
            except Exception as e:
                raise TypeError(""iv must be bytes-like or None"") from e

        if aad is None:
            aad_bytes = b""""
        else:
            try:
                aad_bytes = bytes(aad)
            except Exception as e:
                raise TypeError(""aad must be bytes-like or None"") from e

        if not isinstance(tagBits, int) or tagBits < 0:
            raise ValueError(""tagBits must be a non-negative integer"")

        # Keep original for reference
        self.iv = iv_bytes
        self.aad = aad_bytes
        self.tagBits = int(tagBits)

        # Prepare native buffers and structure; keep buffers alive on the instance
        self._iv_buf = (self._ctypes.c_ubyte * len(iv_bytes))(*iv_bytes) if iv_bytes else None
        self._aad_buf = (self._ctypes.c_ubyte * len(aad_bytes))(*aad_bytes) if aad_bytes else None

        pIv = self._ctypes.cast(self._iv_buf, self._ctypes.c_void_p) if self._iv_buf is not None else self._ctypes.c_void_p()
        pAAD = self._ctypes.cast(self._aad_buf, self._ctypes.c_void_p) if self._aad_buf is not None else self._ctypes.c_void_p()

        self._params = self._CK_GCM_PARAMS(
            pIv=pIv,
            ulIvLen=len(iv_bytes),
            pAAD=pAAD,
            ulAADLen=len(aad_bytes),
            ulTagBits=self.tagBits,
        )

    def to_native(self):
        '''convert mechanism to native format'''
        return self._params"
279010,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.CONCATENATE_BASE_AND_KEY_Mechanism,"import PyKCS11.LowLevel

class CONCATENATE_BASE_AND_KEY_Mechanism:
    """"""CKM_CONCATENATE_BASE_AND_KEY key derivation mechanism""""""

    def __init__(self, encKey):
        """"""
        :param encKey: a handle of encryption key
        """"""
        self._encKey = encKey
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_CONCATENATE_BASE_AND_KEY
        self._mech.pParameter = self._encKey
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_OBJECT_HANDLE_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class CONCATENATE_BASE_AND_KEY_Mechanism:
    '''CKM_CONCATENATE_BASE_AND_KEY key derivation mechanism'''

    def __init__(self, encKey):
        '''
        :param encKey: a handle of encryption key
        '''
        pass

    def to_native(self):
        '''convert mechanism to native format'''
        pass",snippet_192,"class CONCATENATE_BASE_AND_KEY_Mechanism:
    '''CKM_CONCATENATE_BASE_AND_KEY key derivation mechanism'''

    def __init__(self, encKey):
        '''
        :param encKey: a handle of encryption key
        '''
        if isinstance(encKey, bool) or not isinstance(encKey, int):
            raise TypeError(""encKey must be an integer handle"")
        if encKey < 0:
            raise ValueError(""encKey must be non-negative"")
        self.encKey = encKey

    def to_native(self):
        '''convert mechanism to native format'''
        return {
            ""mechanism"": ""CKM_CONCATENATE_BASE_AND_KEY"",
            ""parameter"": self.encKey,
        }"
279012,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.CkClass,"class CkClass:
    """"""
    Base class for CK_* classes
    """"""
    flags_dict = {}
    fields = {}
    flags = 0

    def flags2text(self):
        """"""
        parse the `self.flags` field and create a list of `CKF_*` strings
        corresponding to bits set in flags

        :return: a list of strings
        :rtype: list
        """"""
        r = []
        for k, v in self.flags_dict.items():
            if self.flags & k:
                r.append(v)
        return r

    def state2text(self):
        """"""
        Dummy method. Will be overwriden if necessary
        """"""
        return ''

    def to_dict(self):
        """"""
        convert the fields of the object into a dictionnary
        """"""
        dico = {}
        for field in self.fields:
            if field == 'flags':
                dico[field] = self.flags2text()
            elif field == 'state':
                dico[field] = self.state2text()
            else:
                dico[field] = self.__dict__[field]
        return dico

    def __str__(self):
        """"""
        text representation of the object
        """"""
        dico = self.to_dict()
        lines = []
        for key in sorted(dico.keys()):
            ck_type = self.fields[key]
            if ck_type == 'flags':
                flags = ', '.join(dico[key])
                lines.append(f'{key}: {flags}')
            elif ck_type == 'pair':
                p1, p2 = dico[key]
                lines.append(f'{key}: {p1}.{p2}')
            else:
                lines.append(f'{key}: {dico[key]}')
        return '\n'.join(lines)","class CkClass:
    '''
    Base class for CK_* classes
        '''

    def flags2text(self):
        '''
        parse the `self.flags` field and create a list of `CKF_*` strings
        corresponding to bits set in flags
        :return: a list of strings
        :rtype: list
        '''
        pass

    def state2text(self):
        '''
        Dummy method. Will be overwriden if necessary
        '''
        pass

    def to_dict(self):
        '''
        convert the fields of the object into a dictionnary
        '''
        pass

    def __str__(self):
        '''
        text representation of the object
        '''
        pass",snippet_193,"class CkClass:
    '''
    Base class for CK_* classes
    '''

    def _get_module(self):
        import sys
        return sys.modules.get(self.__class__.__module__)

    def _iter_flag_defs(self):
        # Priority: explicit mappings on the instance/class if provided
        # 1) FLAG_MAP: dict {bit:int -> name:str}
        flag_map = getattr(self, 'FLAG_MAP', None) or getattr(self.__class__, 'FLAG_MAP', None)
        if isinstance(flag_map, dict):
            for bit, name in flag_map.items():
                if isinstance(bit, int) and isinstance(name, str):
                    yield (bit, name)
            return

        # 2) FLAGS: iterable of (bit, name) or name->bit
        flags = getattr(self, 'FLAGS', None) or getattr(self.__class__, 'FLAGS', None)
        if flags is not None:
            try:
                for item in flags:
                    if isinstance(item, tuple) and len(item) == 2:
                        a, b = item
                        if isinstance(a, int) and isinstance(b, str):
                            yield (a, b)
                        elif isinstance(a, str) and isinstance(b, int):
                            yield (b, a)
                    elif isinstance(item, int):
                        # If only bit is given, synthesize a name
                        yield (item, f'CKF_0x{item:X}')
                    elif isinstance(item, str):
                        # Try resolve name from class/module
                        value = getattr(self, item, None)
                        if not isinstance(value, int):
                            value = getattr(self.__class__, item, None)
                        if not isinstance(value, int):
                            mod = self._get_module()
                            value = getattr(mod, item, None) if mod else None
                        if isinstance(value, int):
                            yield (value, item)
            except TypeError:
                pass
            return

        # 3) Fallback: scan class and module for CKF_* integer constants
        seen = {}
        # from class
        for name in dir(self.__class__):
            if not name.startswith('CKF_'):
                continue
            val = getattr(self.__class__, name, None)
            if isinstance(val, int):
                seen.setdefault(val, name)
        # from module
        mod = self._get_module()
        if mod:
            for name in dir(mod):
                if not name.startswith('CKF_'):
                    continue
                val = getattr(mod, name, None)
                if isinstance(val, int) and val not in seen:
                    seen[val] = name
        for bit, name in seen.items():
            yield (bit, name)

    def flags2text(self):
        '''
        parse the `self.flags` field and create a list of `CKF_*` strings
        corresponding to bits set in flags
        :return: a list of strings
        :rtype: list
        '''
        f = getattr(self, 'flags', 0) or 0
        try:
            f_int = int(f)
        except Exception:
            f_int = 0
        pairs = list(self._iter_flag_defs())
        # Sort by bit value for deterministic ordering
        pairs.sort(key=lambda x: (x[0], x[1]))
        result = []
        used_bits = 0
        for bit, name in pairs:
            if bit and (f_int & bit) == bit:
                result.append(name)
                used_bits |= bit
        # If there are remaining bits without names, include generic CKF_0x...
        remaining = f_int & ~used_bits
        if remaining:
            # Decompose remaining into individual set bits
            x = remaining
            while x:
                b = x & -x
                result.append(f'CKF_0x{b:X}')
                x ^= b
        return result

    def state2text(self):
        '''
        Dummy method. Will be overwriden if necessary
        '''
        state = getattr(self, 'state', None)
        if state is None:
            return None
        # Priority: explicit mappings
        state_map = getattr(self, 'STATE_MAP', None) or getattr(self.__class__, 'STATE_MAP', None)
        if isinstance(state_map, dict):
            return state_map.get(state) or state_map.get(int(state), None)

        # Try STATES iterable similar to FLAGS
        states = getattr(self, 'STATES', None) or getattr(self.__class__, 'STATES', None)
        if states is not None:
            try:
                for item in states:
                    if isinstance(item, tuple) and len(item) == 2:
                        a, b = item
                        if isinstance(a, int) and a == state and isinstance(b, str):
                            return b
                        if isinstance(b, int) and b == state and isinstance(a, str):
                            return a
                    elif isinstance(item, str):
                        val = getattr(self, item, None)
                        if not isinstance(val, int):
                            val = getattr(self.__class__, item, None)
                        if not isinstance(val, int):
                            mod = self._get_module()
                            val = getattr(mod, item, None) if mod else None
                        if isinstance(val, int) and val == state:
                            return item
            except TypeError:
                pass

        # Fallback: search module/class for CKS_* exact match
        mod = self._get_module()
        candidates = []
        for name in dir(self.__class__):
            if name.startswith('CKS_'):
                val = getattr(self.__class__, name, None)
                if isinstance(val, int) and val == state:
                    candidates.append(name)
        if mod:
            for name in dir(mod):
                if name.startswith('CKS_'):
                    val = getattr(mod, name, None)
                    if isinstance(val, int) and val == state:
                        candidates.append(name)
        if candidates:
            # Deterministic: smallest lexicographically
            candidates.sort()
            return candidates[0]

        # Default textual representation
        try:
            return f'0x{int(state):X}'
        except Exception:
            return str(state)

    def to_dict(self):
        '''
        convert the fields of the object into a dictionnary
        '''
        def convert(val):
            if isinstance(val, CkClass):
                return val.to_dict()
            if hasattr(val, 'to_dict') and callable(val.to_dict):
                try:
                    return val.to_dict()
                except Exception:
                    pass
            if isinstance(val, (list, tuple)):
                return type(val)(convert(v) for v in val)
            if isinstance(val, memoryview):
                return bytes(val)
            return val

        data = {}
        for k, v in vars(self).items():
            if k.startswith('_') or callable(v):
                continue
            data[k] = convert(v)

        # Add textual helpers if applicable
        if 'flags' in data:
            try:
                data['flags_text'] = self.flags2text()
            except Exception:
                pass
        if 'state' in data:
            try:
                data['state_text'] = self.state2text()
            except Exception:
                pass

        return data

    def __str__(self):
        '''
        text representation of the object
        '''
        try:
            d = self.to_dict()
        except Exception:
            d = {k: v for k, v in vars(self).items() if not k.startswith('_') and not callable(v)}
        items = ', '.join(f'{k}={repr(v)}' for k, v in sorted(d.items()))
        return f'{self.__class__.__name__}({items})'"
279015,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.EXTRACT_KEY_FROM_KEY_Mechanism,"import PyKCS11.LowLevel

class EXTRACT_KEY_FROM_KEY_Mechanism:
    """"""CKM_EXTRACT_KEY_FROM_KEY key derivation mechanism""""""

    def __init__(self, extractParams):
        """"""
        :param extractParams: the index of the first bit of the original
        key to be used in the newly-derived key.  For example if
        extractParams=5 then the 5 first bits are skipped and not used.
        """"""
        self._param = PyKCS11.LowLevel.CK_EXTRACT_PARAMS()
        self._param.assign(extractParams)
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_EXTRACT_KEY_FROM_KEY
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_EXTRACT_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class EXTRACT_KEY_FROM_KEY_Mechanism:
    '''CKM_EXTRACT_KEY_FROM_KEY key derivation mechanism'''

    def __init__(self, extractParams):
        '''
        :param extractParams: the index of the first bit of the original
        key to be used in the newly-derived key.  For example if
        extractParams=5 then the 5 first bits are skipped and not used.
        '''
        pass

    def to_native(self):
        '''convert mechanism to native format'''
        pass",snippet_194,"class EXTRACT_KEY_FROM_KEY_Mechanism:
    '''CKM_EXTRACT_KEY_FROM_KEY key derivation mechanism'''

    def __init__(self, extractParams):
        '''
        :param extractParams: the index of the first bit of the original
        key to be used in the newly-derived key.  For example if
        extractParams=5 then the 5 first bits are skipped and not used.
        '''
        if not isinstance(extractParams, int):
            raise TypeError(""extractParams must be an integer"")
        if extractParams < 0:
            raise ValueError(""extractParams must be non-negative"")
        self.extractParams = extractParams

    def to_native(self):
        '''convert mechanism to native format'''
        import struct
        import ctypes

        size = ctypes.sizeof(ctypes.c_ulong)
        if size == 4:
            fmt = ""=L""
            max_val = (1 << 32) - 1
        elif size == 8:
            fmt = ""=Q""
            max_val = (1 << 64) - 1
        else:
            raise RuntimeError(""Unsupported c_ulong size"")

        if self.extractParams > max_val:
            raise OverflowError(""extractParams too large for native CK_ULONG"")

        return struct.pack(fmt, self.extractParams)"
279016,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.KEY_DERIVATION_STRING_DATA_MechanismBase,"import PyKCS11.LowLevel

class KEY_DERIVATION_STRING_DATA_MechanismBase:
    """"""Base class for mechanisms using derivation string data""""""

    def __init__(self, data, mechType):
        """"""
        :param data: a byte array to concatenate the key with
        :param mechType: mechanism type
        """"""
        self._param = PyKCS11.LowLevel.CK_KEY_DERIVATION_STRING_DATA()
        self._data = ckbytelist(data)
        self._param.pData = self._data
        self._param.ulLen = len(self._data)
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = mechType
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_KEY_DERIVATION_STRING_DATA_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class KEY_DERIVATION_STRING_DATA_MechanismBase:
    '''Base class for mechanisms using derivation string data'''

    def __init__(self, data, mechType):
        '''
        :param data: a byte array to concatenate the key with
        :param mechType: mechanism type
        '''
        pass

    def to_native(self):
        '''convert mechanism to native format'''
        pass",snippet_195,"class KEY_DERIVATION_STRING_DATA_MechanismBase:
    '''Base class for mechanisms using derivation string data'''

    def __init__(self, data, mechType):
        '''
        :param data: a byte array to concatenate the key with
        :param mechType: mechanism type
        '''
        if not isinstance(mechType, int):
            raise TypeError(""mechType must be an int"")
        if data is None:
            raise ValueError(""data must not be None"")
        if isinstance(data, str):
            raise TypeError(""data must be bytes-like, not str"")
        try:
            data_bytes = bytes(data)
        except Exception as e:
            raise TypeError(""data must be bytes-like"") from e

        self.data = data_bytes
        self.mechType = mechType
        self.length = len(self.data)

    def to_native(self):
        '''convert mechanism to native format'''
        return {
            'mech_type': self.mechType,
            'parameter': self.data,
            'length': self.length,
        }"
279017,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.Mechanism,"import PyKCS11.LowLevel

class Mechanism:
    """"""Wraps CK_MECHANISM""""""

    def __init__(self, mechanism, param=None):
        """"""
        :param mechanism: the mechanism to be used
        :type mechanism: integer, any `CKM_*` value
        :param param: data to be used as crypto operation parameter
          (i.e. the IV for some algorithms)
        :type param: string or list/tuple of bytes

        :see: :func:`Session.decrypt`, :func:`Session.sign`
        """"""
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = mechanism
        self._param = None
        if param:
            self._param = ckbytelist(param)
            self._mech.pParameter = self._param
            self._mech.ulParameterLen = len(param)

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class Mechanism:
    '''Wraps CK_MECHANISM'''

    def __init__(self, mechanism, param=None):
        '''
        :param mechanism: the mechanism to be used
        :type mechanism: integer, any `CKM_*` value
        :param param: data to be used as crypto operation parameter
          (i.e. the IV for some algorithms)
        :type param: string or list/tuple of bytes
        :see: :func:`Session.decrypt`, :func:`Session.sign`
        '''
        pass

    def to_native(self):
        '''convert mechanism to native format'''
        pass",snippet_196,"class Mechanism:
    '''Wraps CK_MECHANISM'''

    def __init__(self, mechanism, param=None):
        '''
        :param mechanism: the mechanism to be used
        :type mechanism: integer, any `CKM_*` value
        :param param: data to be used as crypto operation parameter
          (i.e. the IV for some algorithms)
        :type param: string or list/tuple of bytes
        :see: :func:`Session.decrypt`, :func:`Session.sign`
        '''
        if not isinstance(mechanism, int):
            raise TypeError(""mechanism must be an integer"")
        self.mechanism = mechanism
        self.param = self._normalize_param(param)

    def _normalize_param(self, param):
        if param is None:
            return None

        # bytes-like directly
        if isinstance(param, (bytes, bytearray, memoryview)):
            return bytes(param)

        # string -> encode as utf-8
        if isinstance(param, str):
            return param.encode('utf-8')

        # list/tuple handling
        if isinstance(param, (list, tuple)):
            # If elements are bytes-like, join
            if all(isinstance(x, (bytes, bytearray, memoryview)) for x in param):
                return b"""".join(bytes(x) for x in param)
            # If elements are ints (0-255), make bytes
            if all(isinstance(x, int) for x in param):
                return bytes(param)
            raise TypeError(""param list/tuple must contain bytes-like objects or ints"")

        # Iterable of ints (not list/tuple), attempt to convert
        try:
            from collections.abc import Iterable
            if isinstance(param, Iterable):
                # Try to convert to bytes; will raise if not ints 0-255
                return bytes(param)
        except Exception:
            pass

        raise TypeError(""Unsupported param type"")

    def to_native(self):
        '''convert mechanism to native format'''
        if self.param is None:
            return (self.mechanism, None, 0)
        return (self.mechanism, self.param, len(self.param))"
279020,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.RSAOAEPMechanism,"import PyKCS11.LowLevel

class RSAOAEPMechanism:
    """"""RSA OAEP Wrapping mechanism""""""

    def __init__(self, hashAlg, mgf, label=None):
        """"""
        :param hashAlg: the hash algorithm to use (like `CKM_SHA256`)
        :param mgf: the mask generation function to use (like
          `CKG_MGF1_SHA256`)
        :param label: the (optional) label to use
        """"""
        self._param = PyKCS11.LowLevel.CK_RSA_PKCS_OAEP_PARAMS()
        self._param.hashAlg = hashAlg
        self._param.mgf = mgf
        self._source = None
        self._param.source = CKZ_DATA_SPECIFIED
        if label:
            self._source = ckbytelist(label)
            self._param.ulSourceDataLen = len(self._source)
        else:
            self._param.ulSourceDataLen = 0
        self._param.pSourceData = self._source
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_RSA_PKCS_OAEP
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_RSA_PKCS_OAEP_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class RSAOAEPMechanism:
    '''RSA OAEP Wrapping mechanism'''

    def __init__(self, hashAlg, mgf, label=None):
        '''
        :param hashAlg: the hash algorithm to use (like `CKM_SHA256`)
        :param mgf: the mask generation function to use (like
          `CKG_MGF1_SHA256`)
        :param label: the (optional) label to use
        '''
        pass

    def to_native(self):
        '''convert mechanism to native format'''
        pass",snippet_197,"class RSAOAEPMechanism:
    '''RSA OAEP Wrapping mechanism'''

    def __init__(self, hashAlg, mgf, label=None):
        '''
        :param hashAlg: the hash algorithm to use (like `CKM_SHA256`)
        :param mgf: the mask generation function to use (like
          `CKG_MGF1_SHA256`)
        :param label: the (optional) label to use
        '''
        self.hashAlg = self._coerce_mech(hashAlg, ""hashAlg"")
        self.mgf = self._coerce_mech(mgf, ""mgf"")
        self.label = self._coerce_label(label)

    def _coerce_mech(self, value, name):
        if hasattr(value, ""value""):
            value = value.value
        try:
            ivalue = int(value)
        except Exception as e:
            raise TypeError(f""{name} must be an int or enum-like with 'value'"") from e
        if ivalue < 0:
            raise ValueError(f""{name} must be a non-negative integer"")
        return ivalue

    def _coerce_label(self, label):
        if label is None:
            return b""""
        if isinstance(label, bytes):
            return label
        if isinstance(label, str):
            return label.encode(""utf-8"")
        try:
            return bytes(label)
        except Exception as e:
            raise TypeError(""label must be bytes-like, str, or None"") from e

    def to_native(self):
        '''convert mechanism to native format'''
        return {
            ""hashAlg"": self.hashAlg,
            ""mgf"": self.mgf,
            ""label"": self.label,
        }"
279021,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.RSA_PSS_Mechanism,"import PyKCS11.LowLevel

class RSA_PSS_Mechanism:
    """"""RSA PSS Wrapping mechanism""""""

    def __init__(self, mecha, hashAlg, mgf, sLen):
        """"""
        :param mecha: the mechanism to use (like
          `CKM_SHA384_RSA_PKCS_PSS`)
        :param hashAlg: the hash algorithm to use (like `CKM_SHA384`)
        :param mgf: the mask generation function to use (like
          `CKG_MGF1_SHA384`)
        :param sLen: length, in bytes, of the salt value used in the PSS
          encoding (like 0 or the message length)
        """"""
        self._param = PyKCS11.LowLevel.CK_RSA_PKCS_PSS_PARAMS()
        self._param.hashAlg = hashAlg
        self._param.mgf = mgf
        self._param.sLen = sLen
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = mecha
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_RSA_PKCS_PSS_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class RSA_PSS_Mechanism:
    '''RSA PSS Wrapping mechanism'''

    def __init__(self, mecha, hashAlg, mgf, sLen):
        '''
        :param mecha: the mechanism to use (like
          `CKM_SHA384_RSA_PKCS_PSS`)
        :param hashAlg: the hash algorithm to use (like `CKM_SHA384`)
        :param mgf: the mask generation function to use (like
          `CKG_MGF1_SHA384`)
        :param sLen: length, in bytes, of the salt value used in the PSS
          encoding (like 0 or the message length)
        '''
        pass

    def to_native(self):
        '''convert mechanism to native format'''
        pass",snippet_198,"class RSA_PSS_Mechanism:
    '''RSA PSS Wrapping mechanism'''

    def __init__(self, mecha, hashAlg, mgf, sLen):
        '''
        :param mecha: the mechanism to use (like
          `CKM_SHA384_RSA_PKCS_PSS`)
        :param hashAlg: the hash algorithm to use (like `CKM_SHA384`)
        :param mgf: the mask generation function to use (like
          `CKG_MGF1_SHA384`)
        :param sLen: length, in bytes, of the salt value used in the PSS
          encoding (like 0 or the message length)
        '''
        if sLen is None:
            raise ValueError(""sLen must not be None"")
        if isinstance(sLen, bool) or not isinstance(sLen, int):
            raise TypeError(""sLen must be an integer"")
        if sLen < 0:
            raise ValueError(""sLen must be >= 0"")

        self.mecha = mecha
        self.hashAlg = hashAlg
        self.mgf = mgf
        self.sLen = sLen

    def to_native(self):
        '''convert mechanism to native format'''
        return (
            self.mecha,
            {
                'hashAlg': self.hashAlg,
                'mgf': self.mgf,
                'sLen': self.sLen,
            }
        )"
281845,datadotworld/data.world-py,datadotworld_data.world-py/datadotworld/datadotworld.py,datadotworld.datadotworld.UriParam,"class UriParam:
    """"""Represents a URI value as a parameter to a SPARQL query""""""

    def __init__(self, uri):
        """"""
        Initialize the UriParam value
        :param uri: the uri value to wrap
        """"""
        self._uri = uri

    def __repr__(self):
        """"""
        The official string representation for the URI
        :return: the string representation for the URI
        """"""
        return self._uri","class UriParam:
    '''Represents a URI value as a parameter to a SPARQL query'''

    def __init__(self, uri):
        '''
        Initialize the UriParam value
        :param uri: the uri value to wrap
        '''
        pass

    def __repr__(self):
        '''
        The official string representation for the URI
        :return: the string representation for the URI
        '''
        pass",snippet_199,"class UriParam:
    '''Represents a URI value as a parameter to a SPARQL query'''

    def __init__(self, uri):
        '''
        Initialize the UriParam value
        :param uri: the uri value to wrap
        '''
        if not isinstance(uri, str):
            raise TypeError(""uri must be a string"")
        s = uri.strip()
        if s.startswith(""<"") and s.endswith("">""):
            s = s[1:-1].strip()
        if not s:
            raise ValueError(""uri cannot be empty"")
        self._uri = s

    def __repr__(self):
        '''
        The official string representation for the URI
        :return: the string representation for the URI
        '''
        return f""<{self._uri}>"""
282529,quora/qcore,quora_qcore/qcore/events.py,qcore.events.EventInterceptor,"class EventInterceptor:
    """"""A context object helping to temporarily intercept
    a set of events on an object exposing a set of event hooks.

    """"""

    def __init__(self, source, **events):
        """"""
        Constructor.

        :param source: the object exposing a set of event hook properies
        :param events: a set of event_hook_name=event_handler pairs specifying
                       which events to intercept.
        """"""
        self.source = source
        self.events = events

    def __enter__(self):
        """"""Starts event interception.""""""
        source = self.source
        for name, handler in self.events.items():
            hook = getattr(source, name)
            hook.subscribe(handler)

    def __exit__(self, typ, value, traceback):
        """"""Stops event interception.""""""
        source = self.source
        for name, handler in self.events.items():
            hook = getattr(source, name)
            hook.unsubscribe(handler)","class EventInterceptor:
    '''A context object helping to temporarily intercept
    a set of events on an object exposing a set of event hooks.
    '''

    def __init__(self, source, **events):
        '''
        Constructor.
        :param source: the object exposing a set of event hook properies
        :param events: a set of event_hook_name=event_handler pairs specifying
                       which events to intercept.
        '''
        pass

    def __enter__(self):
        '''Starts event interception.'''
        pass

    def __exit__(self, typ, value, traceback):
        '''Stops event interception.'''
        pass",snippet_200,"class EventInterceptor:
    '''A context object helping to temporarily intercept
    a set of events on an object exposing a set of event hooks.
    '''

    def __init__(self, source, **events):
        '''
        Constructor.
        :param source: the object exposing a set of event hook properies
        :param events: a set of event_hook_name=event_handler pairs specifying
                       which events to intercept.
        '''
        self._source = source
        self._events = dict(events)
        self._originals = {}
        self._entered = False

    def __enter__(self):
        '''Starts event interception.'''
        if self._entered:
            return self
        applied = []
        try:
            for name, handler in self._events.items():
                original = getattr(self._source, name)
                self._originals[name] = original
                setattr(self._source, name, handler)
                applied.append(name)
            self._entered = True
            return self
        except Exception:
            # rollback any changes already applied
            for name in reversed(applied):
                try:
                    setattr(self._source, name, self._originals[name])
                except Exception:
                    pass
            self._originals.clear()
            raise

    def __exit__(self, typ, value, traceback):
        '''Stops event interception.'''
        if self._entered:
            for name, original in self._originals.items():
                try:
                    setattr(self._source, name, original)
                except Exception:
                    pass
            self._originals.clear()
            self._entered = False
        return False"
284811,empymod/empymod,empymod/filters.py,empymod.filters.DigitalFilter,"import numpy as np
import os

class DigitalFilter:
    """"""Simple Class for Digital Linear Filters.


    Parameters
    ----------
    name : str
        Name of the DFL.

    savename = str
        Name with which the filter is saved. If None (default) it is set to the
        same value as `name`.

    filter_coeff = list of str
        By default, the following filter coefficients are checked:

            ``filter_coeff = ['j0', 'j1', 'sin', 'cos']``

        This accounts for the standard Hankel and Fourier DLF in CSEM
        modelling. However, additional coefficient names can be provided via
        this parameter (in list format).

    """"""

    def __init__(self, name, savename=None, filter_coeff=None):
        """"""Add filter name.""""""
        self.name = name
        if savename is None:
            self.savename = name
        else:
            self.savename = savename
        self.filter_coeff = ['j0', 'j1', 'sin', 'cos']
        if filter_coeff is not None:
            self.filter_coeff.extend(filter_coeff)

    def tofile(self, path='filters'):
        """"""Save filter values to ASCII-files.

        Store the filter base and the filter coefficients in separate files
        in the directory `path`; `path` can be a relative or absolute path.

        Examples
        --------
        >>> import empymod
        >>> # Load a filter
        >>> filt = empymod.filters.Hankel().wer_201_2018
        >>> # Save it to pure ASCII-files
        >>> filt.tofile()
        >>> # This will save the following three files:
        >>> #    ./filters/wer_201_2018_base.txt
        >>> #    ./filters/wer_201_2018_j0.txt
        >>> #    ./filters/wer_201_2018_j1.txt

        """"""
        name = self.savename
        path = os.path.abspath(path)
        os.makedirs(path, exist_ok=True)
        basefile = os.path.join(path, name + '_base.txt')
        with open(basefile, 'w') as f:
            self.base.tofile(f, sep='\n')
        for val in self.filter_coeff:
            if hasattr(self, val):
                attrfile = os.path.join(path, name + '_' + val + '.txt')
                with open(attrfile, 'w') as f:
                    getattr(self, val).tofile(f, sep='\n')

    def fromfile(self, path='filters'):
        """"""Load filter values from ASCII-files.

        Load filter base and filter coefficients from ASCII files in the
        directory `path`; `path` can be a relative or absolute path.

        Examples
        --------
        >>> import empymod
        >>> # Create an empty filter;
        >>> # Name has to be the base of the text files
        >>> filt = empymod.filters.DigitalFilter('my-filter')
        >>> # Load the ASCII-files
        >>> filt.fromfile()
        >>> # This will load the following three files:
        >>> #    ./filters/my-filter_base.txt
        >>> #    ./filters/my-filter_j0.txt
        >>> #    ./filters/my-filter_j1.txt
        >>> # and store them in filt.base, filt.j0, and filt.j1.

        """"""
        name = self.savename
        path = os.path.abspath(path)
        basefile = os.path.join(path, name + '_base.txt')
        with open(basefile, 'r') as f:
            self.base = np.fromfile(f, sep='\n')
        for val in self.filter_coeff:
            attrfile = os.path.join(path, name + '_' + val + '.txt')
            if os.path.isfile(attrfile):
                with open(attrfile, 'r') as f:
                    setattr(self, val, np.fromfile(f, sep='\n'))
        self.factor = np.around([self.base[1] / self.base[0]], 15)","
class DigitalFilter:
    '''Simple Class for Digital Linear Filters.

    Parameters
    ----------
    name : str
        Name of the DFL.
    savename = str
        Name with which the filter is saved. If None (default) it is set to the
        same value as `name`.
    filter_coeff = list of str
        By default, the following filter coefficients are checked:
            ``filter_coeff = ['j0', 'j1', 'sin', 'cos']``
        This accounts for the standard Hankel and Fourier DLF in CSEM
        modelling. However, additional coefficient names can be provided via
        this parameter (in list format).
    '''

    def __init__(self, name, savename=None, filter_coeff=None):
        '''Add filter name.'''
        pass

    def tofile(self, path='filters'):
        '''Save filter values to ASCII-files.
        Store the filter base and the filter coefficients in separate files
        in the directory `path`; `path` can be a relative or absolute path.
        Examples
        --------
        >>> import empymod
        >>> # Load a filter
        >>> filt = empymod.filters.Hankel().wer_201_2018
        >>> # Save it to pure ASCII-files
        >>> filt.tofile()
        >>> # This will save the following three files:
        >>> #    ./filters/wer_201_2018_base.txt
        >>> #    ./filters/wer_201_2018_j0.txt
        >>> #    ./filters/wer_201_2018_j1.txt
        '''
        pass

    def fromfile(self, path='filters'):
        '''Load filter values from ASCII-files.
        Load filter base and filter coefficients from ASCII files in the
        directory `path`; `path` can be a relative or absolute path.
        Examples
        --------
        >>> import empymod
        >>> # Create an empty filter;
        >>> # Name has to be the base of the text files
        >>> filt = empymod.filters.DigitalFilter('my-filter')
        >>> # Load the ASCII-files
        >>> filt.fromfile()
        >>> # This will load the following three files:
        >>> #    ./filters/my-filter_base.txt
        >>> #    ./filters/my-filter_j0.txt
        >>> #    ./filters/my-filter_j1.txt
        >>> # and store them in filt.base, filt.j0, and filt.j1.
        '''
        pass",snippet_201,"import os
import numpy as np

class DigitalFilter:
    '''Simple Class for Digital Linear Filters.

    Parameters
    ----------
    name : str
        Name of the DFL.
    savename = str
        Name with which the filter is saved. If None (default) it is set to the
        same value as `name`.
    filter_coeff = list of str
        By default, the following filter coefficients are checked:
            ``filter_coeff = ['j0', 'j1', 'sin', 'cos']``
        This accounts for the standard Hankel and Fourier DLF in CSEM
        modelling. However, additional coefficient names can be provided via
        this parameter (in list format).
    '''

    def __init__(self, name, savename=None, filter_coeff=None):
        '''Add filter name.'''
        self.name = str(name)
        self.savename = str(savename) if savename is not None else self.name
        self.filter_coeff = list(filter_coeff) if filter_coeff is not None else ['j0', 'j1', 'sin', 'cos']

        self.base = None
        for coeff in self.filter_coeff:
            setattr(self, coeff, None)

    def tofile(self, path='filters'):
        '''Save filter values to ASCII-files.
        Store the filter base and the filter coefficients in separate files
        in the directory `path`; `path` can be a relative or absolute path.
        Examples
        --------
        >>> import empymod
        >>> # Load a filter
        >>> filt = empymod.filters.Hankel().wer_201_2018
        >>> # Save it to pure ASCII-files
        >>> filt.tofile()
        >>> # This will save the following three files:
        >>> #    ./filters/wer_201_2018_base.txt
        >>> #    ./filters/wer_201_2018_j0.txt
        >>> #    ./filters/wer_201_2018_j1.txt
        '''
        # Ensure directory exists
        os.makedirs(path, exist_ok=True)

        if self.base is None:
            raise ValueError(""Filter 'base' is not set; cannot save to file."")

        basefile = os.path.join(path, f""{self.savename}_base.txt"")
        np.savetxt(basefile, np.asarray(self.base), fmt=""%.18e"")

        for coeff in self.filter_coeff:
            if hasattr(self, coeff):
                data = getattr(self, coeff)
                if data is not None:
                    coeffile = os.path.join(path, f""{self.savename}_{coeff}.txt"")
                    np.savetxt(coeffile, np.asarray(data), fmt=""%.18e"")

    def fromfile(self, path='filters'):
        '''Load filter values from ASCII-files.
        Load filter base and filter coefficients from ASCII files in the
        directory `path`; `path` can be a relative or absolute path.
        Examples
        --------
        >>> import empymod
        >>> # Create an empty filter;
        >>> # Name has to be the base of the text files
        >>> filt = empymod.filters.DigitalFilter('my-filter')
        >>> # Load the ASCII-files
        >>> filt.fromfile()
        >>> # This will load the following three files:
        >>> #    ./filters/my-filter_base.txt
        >>> #    ./filters/my-filter_j0.txt
        >>> #    ./filters/my-filter_j1.txt
        >>> # and store them in filt.base, filt.j0, and filt.j1.
        '''
        basefile = os.path.join(path, f""{self.savename}_base.txt"")
        if not os.path.isfile(basefile):
            raise FileNotFoundError(f""Base file not found: {basefile}"")
        self.base = np.loadtxt(basefile)

        for coeff in self.filter_coeff:
            coeffile = os.path.join(path, f""{self.savename}_{coeff}.txt"")
            if os.path.isfile(coeffile):
                setattr(self, coeff, np.loadtxt(coeffile))"
285266,Samreay/ChainConsumer,src/chainconsumer/kde.py,chainconsumer.kde.MegKDE,"import numpy as np
from scipy import spatial

class MegKDE:
    """"""Matched Elliptical Gaussian Kernel Density Estimator

    Adapted from the algorithm specified in the BAMBIS's model specified Wolf 2017
    to support weighted samples.
    """"""

    def __init__(self, train: np.ndarray, weights: np.ndarray | None=None, truncation: float=3.0, nmin: int=4, factor: float=1.0):
        """"""
        Args:
            train (np.ndarray): The training data set. Should be a 1D array of samples or a 2D array
                of shape (n_samples, n_dim).
            weights (np.ndarray, optional): An array of weights. If not specified, equal weights are assumed.
            truncation (float, optional): The maximum deviation (in sigma) to use points in the KDE
            nmin (int, optional): The minimum number of points required to estimate the density
            factor (float, optional): Send bandwidth to this factor of the data estimate
        """"""
        self.truncation = truncation
        self.nmin = nmin
        self.train = train
        if len(train.shape) == 1:
            train = np.atleast_2d(train).T
        self.num_points, self.num_dim = train.shape
        if weights is None:
            weights = np.ones(self.num_points)
        self.weights = weights
        self.mean = np.average(train, weights=weights, axis=0)
        dx = train - self.mean
        cov = np.atleast_2d(np.cov(dx.T, aweights=weights))
        self.A = np.linalg.cholesky(np.linalg.inv(cov))
        self.d = np.dot(dx, self.A)
        self.tree = spatial.cKDTree(self.d)
        self.sigma = 2.0 * factor * np.power(self.num_points, -1.0 / (4 + self.num_dim))
        self.sigma_fact = -0.5 / (self.sigma * self.sigma)

    def evaluate(self, data: np.ndarray) -> np.ndarray:
        """"""Estimate un-normalised probability density at target points

        Args:
            data (np.ndarray): 2D array of shape (n_samples, n_dim).

        Returns:
            np.ndarray: A `(n_samples)` length array of estimates

        """"""
        if len(data.shape) == 1 and self.num_dim == 1:
            data = np.atleast_2d(data).T
        _d = np.dot(data - self.mean, self.A)
        neighbors = self.tree.query_ball_point(_d, self.sigma * self.truncation)
        out = []
        for i, n in enumerate(neighbors):
            if len(n) >= self.nmin:
                diff = self.d[n, :] - _d[i]
                distsq = np.sum(diff * diff, axis=1)
            else:
                dist, n = self.tree.query(_d[i], k=self.nmin)
                distsq = dist * dist
            out.append(np.sum(self.weights[n] * np.exp(self.sigma_fact * distsq)))
        return np.array(out)","
class MegKDE:
    '''Matched Elliptical Gaussian Kernel Density Estimator
    Adapted from the algorithm specified in the BAMBIS's model specified Wolf 2017
    to support weighted samples.
    '''

    def __init__(self, train: np.ndarray, weights: np.ndarray | None=None, truncation: float=3.0, nmin: int=4, factor: float=1.0):
        '''
        Args:
            train (np.ndarray): The training data set. Should be a 1D array of samples or a 2D array
                of shape (n_samples, n_dim).
            weights (np.ndarray, optional): An array of weights. If not specified, equal weights are assumed.
            truncation (float, optional): The maximum deviation (in sigma) to use points in the KDE
            nmin (int, optional): The minimum number of points required to estimate the density
            factor (float, optional): Send bandwidth to this factor of the data estimate
        '''
        pass

    def evaluate(self, data: np.ndarray) -> np.ndarray:
        '''Estimate un-normalised probability density at target points
        Args:
            data (np.ndarray): 2D array of shape (n_samples, n_dim).
        Returns:
            np.ndarray: A `(n_samples)` length array of estimates
        '''
        pass",snippet_203,"class MegKDE:
    '''Matched Elliptical Gaussian Kernel Density Estimator
    Adapted from the algorithm specified in the BAMBIS's model specified Wolf 2017
    to support weighted samples.
    '''

    def __init__(self, train: np.ndarray, weights: np.ndarray | None=None, truncation: float=3.0, nmin: int=4, factor: float=1.0):
        '''
        Args:
            train (np.ndarray): The training data set. Should be a 1D array of samples or a 2D array
                of shape (n_samples, n_dim).
            weights (np.ndarray, optional): An array of weights. If not specified, equal weights are assumed.
            truncation (float, optional): The maximum deviation (in sigma) to use points in the KDE
            nmin (int, optional): The minimum number of points required to estimate the density
            factor (float, optional): Send bandwidth to this factor of the data estimate
        '''
        if train is None:
            raise ValueError(""train must be provided"")
        X = np.asarray(train)
        if X.ndim == 1:
            X = X.reshape(-1, 1)
        elif X.ndim != 2:
            raise ValueError(""train must be 1D or 2D array"")
        n, d = X.shape

        if weights is None:
            w = np.ones(n, dtype=float)
        else:
            w = np.asarray(weights, dtype=float).reshape(-1)
            if w.shape[0] != n:
                raise ValueError(""weights must have same length as number of samples"")
        if np.any(w < 0):
            raise ValueError(""weights must be non-negative"")
        wsum = float(np.sum(w))
        if not np.isfinite(wsum) or wsum <= 0:
            raise ValueError(""sum of weights must be positive and finite"")

        mu = (w[:, None] * X).sum(axis=0) / wsum
        Xm = X - mu
        # Weighted covariance (second moment)
        # Use sum(w * outer) / sum(w)
        cov = (Xm.T * w) @ Xm / wsum
        # Ensure symmetry
        cov = 0.5 * (cov + cov.T)
        # Jitter for numerical stability
        eps = 1e-12
        jitter = eps * np.trace(cov) / d if np.isfinite(np.trace(cov)) and np.trace(cov) > 0 else eps
        cov = cov + np.eye(d) * jitter
        # Bandwidth matrix
        H = (factor ** 2) * cov
        # Inverse via Cholesky
        try:
            L = np.linalg.cholesky(H)
        except np.linalg.LinAlgError:
            # Add more jitter if needed
            diag_boost = max(jitter, 1e-9)
            H = H + np.eye(d) * diag_boost
            L = np.linalg.cholesky(H)
        Linv = np.linalg.inv(L)
        invH = Linv.T @ Linv

        self.train = X
        self.weights = w
        self._wsum = wsum
        self._invH = invH
        self._trunc2 = float(truncation) ** 2
        self._nmin = int(max(1, nmin))
        self._d = d
        self._n = n

    def evaluate(self, data: np.ndarray) -> np.ndarray:
        '''Estimate un-normalised probability density at target points
        Args:
            data (np.ndarray): 2D array of shape (n_samples, n_dim).
        Returns:
            np.ndarray: A `(n_samples)` length array of estimates
        '''
        Xq = np.asarray(data)
        if Xq.ndim == 1:
            Xq = Xq.reshape(-1, self._d)
        if Xq.ndim != 2 or Xq.shape[1] != self._d:
            raise ValueError(f""data must be of shape (n_samples, {self._d})"")
        m = Xq.shape[0]
        invH = self._invH
        T = self.train
        w = self.weights
        trunc2 = self._trunc2
        nmin = self._nmin

        # Compute squared Mahalanobis distances for all pairs
        # d2[i, j] = (Xq[i]-T[j])^T invH (Xq[i]-T[j])
        # Use broadcasting; may be memory heavy for very large inputs.
        diff = Xq[:, None, :] - T[None, :, :]
        tmp = diff @ invH
        d2 = np.sum(tmp * diff, axis=2)

        # Exponentials
        out = np.zeros(m, dtype=float)
        exp_full = np.exp(-0.5 * d2)

        # Apply truncation; ensure at least nmin contributors
        for i in range(m):
            mask = d2[i] <= trunc2
            if np.count_nonzero(mask) < nmin:
                # take nmin smallest distances
                idx = np.argpartition(d2[i], nmin - 1)[:nmin]
                val = np.dot(w[idx], exp_full[i, idx])
            else:
                idx = np.nonzero(mask)[0]
                val = np.dot(w[idx], exp_full[i, idx])
            out[i] = val

        return out"
285844,4Catalyzer/flask-resty,4Catalyzer_flask-resty/flask_resty/authentication.py,flask_resty.authentication.AuthenticationBase,"class AuthenticationBase:
    """"""Base class for API authentication components.

    Authentication components are responsible for extracting the request
    credentials, if any. They should raise a 401 if the credentials are
    invalid, but should provide `None` for unauthenticated users.

    Flask-RESTy provides an implementation using `JSON Web Tokens`_  but you
    can use any authentication component by extending
    :py:class:`AuthenticationBase` and implementing
    :py:meth:`get_request_credentials`.

    .. _JSON Web Tokens: https://jwt.io/
    """"""

    def authenticate_request(self):
        """"""Store the request credentials in the
        :py:class:`flask.ctx.AppContext`.

        .. warning::

            No validation is performed by Flask-RESTy. It is up to the
            implementor to validate the request in
            :py:meth:`get_request_credentials`.
        """"""
        set_request_credentials(self.get_request_credentials())

    def get_request_credentials(self):
        """"""Get the credentials for the current request.

        Typically this is done by inspecting :py:data:`flask.request`.

        .. warning::

            Implementing classes **must** raise an exception on authentication
            failure. A 401 Unauthorized :py:class:`ApiError` is recommended.

        :return: The credentials for the current request.
        """"""
        raise NotImplementedError()","class AuthenticationBase:
    '''Base class for API authentication components.
    Authentication components are responsible for extracting the request
    credentials, if any. They should raise a 401 if the credentials are
    invalid, but should provide `None` for unauthenticated users.
    Flask-RESTy provides an implementation using `JSON Web Tokens`_  but you
    can use any authentication component by extending
    :py:class:`AuthenticationBase` and implementing
    :py:meth:`get_request_credentials`.
    .. _JSON Web Tokens: https://jwt.io/
    '''

    def authenticate_request(self):
        '''Store the request credentials in the
        :py:class:`flask.ctx.AppContext`.
        .. warning::
            No validation is performed by Flask-RESTy. It is up to the
            implementor to validate the request in
            :py:meth:`get_request_credentials`.
        '''
        pass

    def get_request_credentials(self):
        '''Get the credentials for the current request.
        Typically this is done by inspecting :py:data:`flask.request`.
        .. warning::
            Implementing classes **must** raise an exception on authentication
            failure. A 401 Unauthorized :py:class:`ApiError` is recommended.
        :return: The credentials for the current request.
        '''
        pass",snippet_204,"class AuthenticationBase:
    '''Base class for API authentication components.
    Authentication components are responsible for extracting the request
    credentials, if any. They should raise a 401 if the credentials are
    invalid, but should provide `None` for unauthenticated users.
    Flask-RESTy provides an implementation using `JSON Web Tokens`_  but you
    can use any authentication component by extending
    :py:class:`AuthenticationBase` and implementing
    :py:meth:`get_request_credentials`.
    .. _JSON Web Tokens: https://jwt.io/
    '''

    def authenticate_request(self):
        '''Store the request credentials in the
        :py:class:`flask.ctx.AppContext`.
        .. warning::
            No validation is performed by Flask-RESTy. It is up to the
            implementor to validate the request in
            :py:meth:`get_request_credentials`.
        '''
        from flask import g

        credentials = self.get_request_credentials()
        setattr(g, ""request_credentials"", credentials)
        return credentials

    def get_request_credentials(self):
        '''Get the credentials for the current request.
        Typically this is done by inspecting :py:data:`flask.request`.
        .. warning::
            Implementing classes **must** raise an exception on authentication
            failure. A 401 Unauthorized :py:class:`ApiError` is recommended.
        :return: The credentials for the current request.
        '''
        raise NotImplementedError(""Subclasses must implement get_request_credentials()"")"
285856,4Catalyzer/flask-resty,4Catalyzer_flask-resty/flask_resty/filtering.py,flask_resty.filtering.ArgFilterBase,"class ArgFilterBase:
    """"""An abstract specification of a filter from a query argument.

    Implementing classes must provide :py:meth:`maybe_set_arg_name` and
    :py:meth:`filter_query`.
    """"""

    def maybe_set_arg_name(self, arg_name):
        """"""Set the name of the argument to which this filter is bound.

        :param str arg_name: The name of the field to filter against.
        :raises: :py:class:`NotImplementedError` if no implementation is
            provided.
        """"""
        raise NotImplementedError()

    def filter_query(self, query, view, arg_value):
        """"""Filter the query.

        :param query: The query to filter.
        :type query: :py:class:`sqlalchemy.orm.query.Query`
        :param view: The view with the model we wish to filter for.
        :type view: :py:class:`ModelView`
        :param str arg_value: The filter specification
        :return: The filtered query
        :rtype: :py:class:`sqlalchemy.orm.query.Query`
        :raises: :py:class:`NotImplementedError` if no implementation is
            provided.
        """"""
        raise NotImplementedError()","class ArgFilterBase:
    '''An abstract specification of a filter from a query argument.
    Implementing classes must provide :py:meth:`maybe_set_arg_name` and
    :py:meth:`filter_query`.
    '''

    def maybe_set_arg_name(self, arg_name):
        '''Set the name of the argument to which this filter is bound.
        :param str arg_name: The name of the field to filter against.
        :raises: :py:class:`NotImplementedError` if no implementation is
            provided.
        '''
        pass

    def filter_query(self, query, view, arg_value):
        '''Filter the query.
        :param query: The query to filter.
        :type query: :py:class:`sqlalchemy.orm.query.Query`
        :param view: The view with the model we wish to filter for.
        :type view: :py:class:`ModelView`
        :param str arg_value: The filter specification
        :return: The filtered query
        :rtype: :py:class:`sqlalchemy.orm.query.Query`
        :raises: :py:class:`NotImplementedError` if no implementation is
            provided.
        '''
        pass",snippet_205,"class ArgFilterBase:
    '''An abstract specification of a filter from a query argument.
    Implementing classes must provide :py:meth:`maybe_set_arg_name` and
    :py:meth:`filter_query`.
    '''

    def maybe_set_arg_name(self, arg_name):
        '''Set the name of the argument to which this filter is bound.
        :param str arg_name: The name of the field to filter against.
        :raises: :py:class:`NotImplementedError` if no implementation is
            provided.
        '''
        raise NotImplementedError(""maybe_set_arg_name must be implemented by subclasses."")

    def filter_query(self, query, view, arg_value):
        '''Filter the query.
        :param query: The query to filter.
        :type query: :py:class:`sqlalchemy.orm.query.Query`
        :param view: The view with the model we wish to filter for.
        :type view: :py:class:`ModelView`
        :param str arg_value: The filter specification
        :return: The filtered query
        :rtype: :py:class:`sqlalchemy.orm.query.Query`
        :raises: :py:class:`NotImplementedError` if no implementation is
            provided.
        '''
        raise NotImplementedError(""filter_query must be implemented by subclasses."")"
285958,tcalmant/python-javaobj,tcalmant_python-javaobj/javaobj/constants.py,javaobj.constants.StreamCodeDebug,"class StreamCodeDebug:
    """"""
    Codes utility methods
    """"""

    @staticmethod
    def op_id(op_id):
        """"""
        Returns the name of the given OP Code
        :param op_id: OP Code
        :return: Name of the OP Code
        """"""
        try:
            return TerminalCode(op_id).name
        except ValueError:
            return '<unknown TC:{0}>'.format(op_id)

    @staticmethod
    def type_code(type_id):
        """"""
        Returns the name of the given Type Code
        :param type_id: Type code
        :return: Name of the type code
        """"""
        try:
            return TypeCode(type_id).name
        except ValueError:
            return '<unknown TypeCode:{0}>'.format(type_id)

    @staticmethod
    def flags(flags):
        """"""
        Returns the names of the class description flags found in the given
        integer

        :param flags: A class description flag entry
        :return: The flags names as a single string
        """"""
        names = sorted((key.name for key in ClassDescFlags if key & flags))
        return ', '.join(names)","class StreamCodeDebug:
    '''
    Codes utility methods
    '''
    @staticmethod
    def op_id(op_id):
        '''
        Returns the name of the given OP Code
        :param op_id: OP Code
        :return: Name of the OP Code
        '''
        pass
    @staticmethod
    def type_code(type_id):
        '''
        Returns the name of the given Type Code
        :param type_id: Type code
        :return: Name of the type code
        '''
        pass
    @staticmethod
    def flags(flags):
        '''
        Returns the names of the class description flags found in the given
        integer
        :param flags: A class description flag entry
        :return: The flags names as a single string
        '''
        pass",snippet_206,"class StreamCodeDebug:
    '''
    Codes utility methods
    '''

    _OPCODES = {
        0x70: ""TC_NULL"",
        0x71: ""TC_REFERENCE"",
        0x72: ""TC_CLASSDESC"",
        0x73: ""TC_OBJECT"",
        0x74: ""TC_STRING"",
        0x75: ""TC_ARRAY"",
        0x76: ""TC_CLASS"",
        0x77: ""TC_BLOCKDATA"",
        0x78: ""TC_ENDBLOCKDATA"",
        0x79: ""TC_RESET"",
        0x7A: ""TC_BLOCKDATALONG"",
        0x7B: ""TC_EXCEPTION"",
        0x7C: ""TC_LONGSTRING"",
        0x7D: ""TC_PROXYCLASSDESC"",
        0x7E: ""TC_ENUM"",
    }

    _TYPECODES = {
        'B': ""byte"",
        'C': ""char"",
        'D': ""double"",
        'F': ""float"",
        'I': ""int"",
        'J': ""long"",
        'L': ""object"",
        'S': ""short"",
        'Z': ""boolean"",
        '[': ""array"",
    }

    _FLAGS = [
        (0x01, ""SC_WRITE_METHOD""),
        (0x02, ""SC_SERIALIZABLE""),
        (0x04, ""SC_EXTERNALIZABLE""),
        (0x08, ""SC_BLOCK_DATA""),
        (0x10, ""SC_ENUM""),
    ]

    @staticmethod
    def op_id(op_id):
        '''
        Returns the name of the given OP Code
        :param op_id: OP Code
        :return: Name of the OP Code
        '''
        # Normalize integers possibly given as bytes
        if isinstance(op_id, (bytes, bytearray)) and len(op_id) == 1:
            op_id = op_id[0]
        try:
            op_int = int(op_id)
        except Exception:
            return ""UNKNOWN""
        return StreamCodeDebug._OPCODES.get(op_int & 0xFF, ""UNKNOWN"")

    @staticmethod
    def type_code(type_id):
        '''
        Returns the name of the given Type Code
        :param type_id: Type code
        :return: Name of the type code
        '''
        if isinstance(type_id, (bytes, bytearray)):
            if len(type_id) == 0:
                return ""UNKNOWN""
            type_id = chr(type_id[0])
        elif isinstance(type_id, int):
            if 0 <= type_id <= 0x10FFFF:
                type_id = chr(type_id)
            else:
                return ""UNKNOWN""
        elif not isinstance(type_id, str) or len(type_id) == 0:
            return ""UNKNOWN""
        return StreamCodeDebug._TYPECODES.get(type_id[0], ""UNKNOWN"")

    @staticmethod
    def flags(flags):
        '''
        Returns the names of the class description flags found in the given
        integer
        :param flags: A class description flag entry
        :return: The flags names as a single string
        '''
        try:
            val = int(flags)
        except Exception:
            return ""UNKNOWN""
        names = [name for bit, name in StreamCodeDebug._FLAGS if val & bit]
        return "" | "".join(names) if names else ""0"""
285972,tcalmant/python-javaobj,tcalmant_python-javaobj/javaobj/v2/api.py,javaobj.v2.api.IJavaStreamParser,"class IJavaStreamParser:
    """"""
    API of the Java stream parser
    """"""

    def run(self):
        """"""
        Parses the input stream
        """"""
        raise NotImplementedError

    def dump(self, content):
        """"""
        Dumps to a string the given objects
        """"""
        raise NotImplementedError

    def _read_content(self, type_code, block_data, class_desc=None):
        """"""
        Parses the next content. Use with care (use only in a transformer)
        """"""","class IJavaStreamParser:
    '''
    API of the Java stream parser
    '''

    def run(self):
        '''
        Parses the input stream
        '''
        pass

    def dump(self, content):
        '''
        Dumps to a string the given objects
        '''
        pass

    def _read_content(self, type_code, block_data, class_desc=None):
        '''
        Parses the next content. Use with care (use only in a transformer)
        '''
        pass",snippet_207,"from abc import ABC, abstractmethod
from typing import Any, Optional


class IJavaStreamParser(ABC):
    '''
    API of the Java stream parser
    '''

    @abstractmethod
    def run(self) -> Any:
        '''
        Parses the input stream
        '''
        raise NotImplementedError

    @abstractmethod
    def dump(self, content: Any) -> str:
        '''
        Dumps to a string the given objects
        '''
        raise NotImplementedError

    @abstractmethod
    def _read_content(self, type_code: int, block_data: bytes, class_desc: Optional[Any] = None) -> Any:
        '''
        Parses the next content. Use with care (use only in a transformer)
        '''
        raise NotImplementedError"
287233,materialsvirtuallab/monty,materialsvirtuallab_monty/src/monty/design_patterns.py,monty.design_patterns.NullFile,"import os

class NullFile:
    """"""A file object that is associated to /dev/null.""""""

    def __new__(cls):
        """"""
        Pass through.
        """"""
        return open(os.devnull, 'w')

    def __init__(self):
        """"""no-op""""""","
class NullFile:
    '''A file object that is associated to /dev/null.'''

    def __new__(cls):
        '''
        Pass through.
        '''
        pass

    def __init__(self):
        '''no-op'''
        pass",snippet_208,"import os

class NullFile:
    '''A file object that is associated to /dev/null.'''

    def __new__(cls):
        '''
        Pass through.
        '''
        return open(os.devnull, 'w+b')

    def __init__(self):
        '''no-op'''
        pass"
287394,common-workflow-language/schema_salad,common-workflow-language_schema_salad/schema_salad/cpp_codegen.py,schema_salad.cpp_codegen.ClassDefinition,"from typing import IO, Any, Optional, Union, cast

class ClassDefinition:
    """"""Prototype of a class.""""""

    def __init__(self, name: str):
        """"""Initialize the class definition with a name.""""""
        self.fullName = name
        self.extends: list[dict[str, str]] = []
        self.specializationTypes: list[str] = []
        self.allfields: list[FieldDefinition] = []
        self.fields: list[FieldDefinition] = []
        self.abstract = False
        self.namespace, self.classname = split_name(name)
        self.namespace = safenamespacename(self.namespace)
        self.classname = safename(self.classname)

    def writeFwdDeclaration(self, target: IO[str], fullInd: str, ind: str) -> None:
        """"""Write forward declaration.""""""
        target.write(f'{fullInd}namespace {self.namespace} {{ struct {self.classname}; }}\n')

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, common_namespace: str) -> None:
        """"""Write definition of the class.""""""
        target.write(f'{fullInd}namespace {self.namespace} {{\n')
        target.write(f'{fullInd}struct {self.classname}')
        extends = list(map(safename2, self.extends))
        override = ''
        virtual = 'virtual '
        if len(self.extends) > 0:
            target.write(f'\n{fullInd}{ind}: ')
            target.write(f'\n{fullInd}{ind}, '.join(extends))
            override = ' override'
            virtual = ''
        target.write(' {\n')
        for field in self.fields:
            field.writeDefinition(target, fullInd + ind, ind, self.namespace)
        if self.abstract:
            target.write(f'{fullInd}{ind}virtual ~{self.classname}() = 0;\n')
        else:
            target.write(f'{fullInd}{ind}{virtual}~{self.classname}(){override} = default;\n')
        target.write(f'{fullInd}{ind}{virtual}auto toYaml([[maybe_unused]] {common_namespace}::store_config const& config) const -> YAML::Node{override};\n')
        target.write(f'{fullInd}{ind}{virtual}void fromYaml(YAML::Node const& n){override};\n')
        target.write(f'{fullInd}}};\n')
        target.write(f'{fullInd}}}\n\n')

    def writeImplDefinition(self, target: IO[str], fullInd: str, ind: str, common_namespace: str) -> None:
        """"""Write definition with implementation.""""""
        extends = list(map(safename2, self.extends))
        if self.abstract:
            target.write(f'{fullInd}inline {self.namespace}::{self.classname}::~{self.classname}() = default;\n')
        target.write(f'{fullInd}inline auto {self.namespace}::{self.classname}::toYaml([[maybe_unused]] ::{common_namespace}::store_config const& config) const -> YAML::Node {{\n{fullInd}{ind}using ::{common_namespace}::toYaml;\n{fullInd}{ind}auto n = YAML::Node{{}};\n{fullInd}{ind}if (config.generateTags) {{\n{fullInd}{ind}{ind}n.SetTag(""{self.classname}"");\n{fullInd}{ind}}}\n')
        for e in extends:
            target.write(f'{fullInd}{ind}n = mergeYaml(n, {e}::toYaml(config));\n')
        for field in self.fields:
            fieldname = safename(field.name)
            target.write(f'{fullInd}{ind}{{\n')
            target.write(f'{fullInd}{ind}{ind} auto member = toYaml(*{fieldname}, config);\n')
            if field.typeDSL:
                target.write(f'{fullInd}{ind}{ind} member = simplifyType(member, config);\n')
            target.write(f'{fullInd}{ind}{ind} member = convertListToMap(member, {q(field.mapSubject)}, {q(field.mapPredicate)}, config);\n')
            target.write(f'{fullInd}{ind}{ind}addYamlField(n, {q(field.name)}, member);\n')
            target.write(f'{fullInd}{ind}}}\n')
        target.write(f'{fullInd}{ind}return n;\n{fullInd}}}\n')
        functionname = f'{self.namespace}::{self.classname}::fromYaml'
        target.write(f'{fullInd}inline void {functionname}([[maybe_unused]] YAML::Node const& n) {{\n{fullInd}{ind}using ::{common_namespace}::fromYaml;\n')
        for e in extends:
            target.write(f'{fullInd}{ind}{e}::fromYaml(n);\n')
        for field in self.fields:
            fieldname = safename(field.name)
            expandType = ''
            if field.typeDSL:
                expandType = 'expandType'
            target.write(f'{fullInd}{ind}{{\n{fullInd}{ind}{ind}auto nodeAsList = convertMapToList(n[{q(field.name)}], {q(field.mapSubject)}, {q(field.mapPredicate)});\n{fullInd}{ind}{ind}auto expandedNode = {expandType}(nodeAsList);\n{fullInd}{ind}{ind}fromYaml(expandedNode, *{fieldname});\n{fullInd}{ind}}}\n')
        target.write(f'{fullInd}}}\n')
        if not self.abstract:
            e = f'::{self.namespace}::{self.classname}'
            target.write(f'namespace {common_namespace} {{\ntemplate <>\nstruct DetectAndExtractFromYaml<{e}> {{\n    auto operator()(YAML::Node const& n) const -> std::optional<{e}> {{\n        if (!n.IsDefined()) return std::nullopt;\n        if (!n.IsMap()) return std::nullopt;\n        auto res = {e}{{}};\n\n')
            for field in self.fields:
                fieldname = safename(field.name)
                target.write(f'        if constexpr (::{common_namespace}::IsConstant<decltype(res.{fieldname})::value_t>::value) try {{\n            fromYaml(n[{q(field.name)}], *res.{fieldname});\n            fromYaml(n, res);\n            return res;\n        }} catch(...) {{}}\n\n')
            target.write('        return std::nullopt;\n    }\n};\n}\n')","
class ClassDefinition:
    '''Prototype of a class.'''

    def __init__(self, name: str):
        '''Initialize the class definition with a name.'''
        pass

    def writeFwdDeclaration(self, target: IO[str], fullInd: str, ind: str) -> None:
        '''Write forward declaration.'''
        pass

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, common_namespace: str) -> None:
        '''Write definition of the class.'''
        pass

    def writeImplDefinition(self, target: IO[str], fullInd: str, ind: str, common_namespace: str) -> None:
        '''Write definition with implementation.'''
        pass",snippet_209,"from typing import IO, Any


class ClassDefinition:
    '''Prototype of a class.'''

    def __init__(self, name: str):
        '''Initialize the class definition with a name.'''
        if not isinstance(name, str) or not name:
            raise ValueError(""name must be a non-empty string"")
        self.name = name

    def writeFwdDeclaration(self, target: IO[str], fullInd: str, ind: str) -> None:
        '''Write forward declaration.'''
        del ind
        target.write(f""{fullInd}class {self.name};\n"")

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, common_namespace: str) -> None:
        '''Write definition of the class.'''
        ns_opened = False
        if common_namespace:
            target.write(f""{fullInd}namespace {common_namespace} {{\n"")
            ns_opened = True

        target.write(f""{fullInd}class {self.name} {{\n"")
        target.write(f""{fullInd}{ind}public:\n"")
        target.write(f""{fullInd}{ind}{ind}{self.name}() = default;\n"")
        target.write(f""{fullInd}{ind}{ind}virtual ~{self.name}() = default;\n"")
        target.write(f""{fullInd}}};\n"")

        if ns_opened:
            target.write(f""{fullInd}}} // namespace {common_namespace}\n"")

    def writeImplDefinition(self, target: IO[str], fullInd: str, ind: str, common_namespace: str) -> None:
        '''Write definition with implementation.'''
        ns_opened = False
        if common_namespace:
            target.write(f""{fullInd}namespace {common_namespace} {{\n"")
            ns_opened = True

        target.write(f""{fullInd}class {self.name} {{\n"")
        target.write(f""{fullInd}{ind}public:\n"")
        target.write(f""{fullInd}{ind}{ind}{self.name}() {{}}\n"")
        target.write(f""{fullInd}{ind}{ind}virtual ~{self.name}() {{}}\n"")
        target.write(f""{fullInd}}};\n"")

        if ns_opened:
            target.write(f""{fullInd}}} // namespace {common_namespace}\n"")"
287396,common-workflow-language/schema_salad,common-workflow-language_schema_salad/schema_salad/cpp_codegen.py,schema_salad.cpp_codegen.EnumDefinition,"from typing import IO, Any, Optional, Union, cast

class EnumDefinition:
    """"""Prototype of a enum.""""""

    def __init__(self, name: str, values: list[str]):
        """"""Initialize enum definition with a name and possible values.""""""
        self.name = name
        self.values = values
        self.namespace, self.classname = split_name(name)
        self.namespace = safenamespacename(self.namespace)
        self.classname = safename(self.classname)

    def writeDefinition(self, target: IO[str], ind: str, common_namespace: str) -> None:
        """"""Write enum definition to output.""""""
        namespace = ''
        if len(self.name.split('#')) == 2:
            namespace, classname = split_name(self.name)
            namespace = safenamespacename(namespace)
            classname = safename(classname)
            name = namespace + '::' + classname
        else:
            name = safename(self.name)
            classname = name
        if len(namespace) > 0:
            target.write(f'namespace {namespace} {{\n')
        target.write(f'enum class {classname} : unsigned int {{\n{ind}')
        target.write(f',\n{ind}'.join(map(safename, self.values)))
        target.write('\n};\n')
        target.write(f'inline auto to_string({classname} v) {{\n')
        target.write(f'{ind}static auto m = std::vector<std::string_view> {{\n')
        target.write(f'{ind}    ""')
        target.write(f'"",\n{ind}    ""'.join(self.values))
        target.write(f'""\n{ind}}};\n')
        target.write(f'{ind}using U = std::underlying_type_t<{name}>;\n')
        target.write(f'{ind}return m.at(static_cast<U>(v));\n}}\n')
        if len(namespace) > 0:
            target.write('}\n')
        target.write(f'inline void to_enum(std::string_view v, {name}& out) {{\n')
        target.write(f'{ind}static auto m = std::map<std::string, {name}, std::less<>> {{\n')
        for v in self.values:
            target.write(f'{ind}{ind}{{{q(v)}, {name}::{safename(v)}}},\n')
        target.write(f'{ind}}};\n{ind}auto iter = m.find(v);\n')
        target.write(f'{ind}if (iter == m.end()) throw bool{{}};\n')
        target.write(f'{ind}out = iter->second;\n}}\n')
        target.write(f'namespace {common_namespace} {{\n')
        target.write(f'inline auto toYaml({name} v, [[maybe_unused]] ::{common_namespace}::store_config const& config) {{\n')
        target.write(f'{ind}auto n = YAML::Node{{std::string{{to_string(v)}}}};\n')
        target.write(f'{ind}if (config.generateTags) n.SetTag(""{name}"");\n')
        target.write(f'{ind}return n;\n}}\n')
        target.write(f'inline void fromYaml(YAML::Node n, {name}& out) {{\n')
        target.write(f'{ind}to_enum(n.as<std::string>(), out);\n}}\n')
        if len(self.values):
            target.write(f'template <> struct IsConstant<{name}> : std::true_type {{}};\n')
        target.write('}\n')
        target.write('\n')","
class EnumDefinition:
    '''Prototype of a enum.'''

    def __init__(self, name: str, values: list[str]):
        '''Initialize enum definition with a name and possible values.'''
        pass

    def writeDefinition(self, target: IO[str], ind: str, common_namespace: str) -> None:
        '''Write enum definition to output.'''
        pass",snippet_210,"from typing import IO, List
import keyword
import re

class EnumDefinition:
    '''Prototype of a enum.'''

    def __init__(self, name: str, values: List[str]):
        '''Initialize enum definition with a name and possible values.'''
        if not isinstance(name, str) or not name:
            raise ValueError(""Enum name must be a non-empty string"")
        if not isinstance(values, list) or not all(isinstance(v, str) for v in values):
            raise ValueError(""Enum values must be a list of strings"")
        self.name = name
        self.values = values[:]

    def _sanitize_member(self, value: str) -> str:
        s = re.sub(r'\W+', '_', value).strip('_')
        if not s:
            s = ""VALUE""
        if s[0].isdigit():
            s = f""_{s}""
        s = s.upper()
        if keyword.iskeyword(s):
            s = f""{s}_""
        return s

    def writeDefinition(self, target: IO[str], ind: str, common_namespace: str) -> None:
        '''Write enum definition to output.'''
        # optional comment with namespace info
        if common_namespace:
            target.write(f""{ind}# namespace: {common_namespace}\n"")
        target.write(f""{ind}class {self.name}(Enum):\n"")
        if not self.values:
            target.write(f""{ind}{ind}pass\n"")
            return
        used = {}
        for i, val in enumerate(self.values):
            member = self._sanitize_member(val)
            base = member
            suffix = 1
            while member in used:
                suffix += 1
                member = f""{base}_{suffix}""
            used[member] = True
            target.write(f""{ind}{ind}{member} = {val!r}\n"")"
287397,common-workflow-language/schema_salad,common-workflow-language_schema_salad/schema_salad/cpp_codegen.py,schema_salad.cpp_codegen.FieldDefinition,"from typing import IO, Any, Optional, Union, cast

class FieldDefinition:
    """"""Prototype of a single field from a class definition.""""""

    def __init__(self, name: str, typeStr: str, optional: bool, mapSubject: str, mapPredicate: str, typeDSL: bool):
        """"""Initialize field definition.

        Creates a new field with name, its type, optional and which field to use to convert
        from list to map (or empty if it is not possible)
        """"""
        self.name = name
        self.typeStr = typeStr
        self.optional = optional
        self.mapSubject = mapSubject
        self.mapPredicate = mapPredicate
        self.typeDSL = typeDSL

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, namespace: str) -> None:
        """"""Write a C++ definition for the class field.""""""
        name = safename(self.name)
        typeStr = self.typeStr.replace(namespace + '::', '')
        target.write(f'{fullInd}heap_object<{typeStr}> {name};\n')","
class FieldDefinition:
    '''Prototype of a single field from a class definition.'''

    def __init__(self, name: str, typeStr: str, optional: bool, mapSubject: str, mapPredicate: str, typeDSL: bool):
        '''Initialize field definition.
        Creates a new field with name, its type, optional and which field to use to convert
        from list to map (or empty if it is not possible)
        '''
        pass

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, namespace: str) -> None:
        '''Write a C++ definition for the class field.'''
        pass",snippet_211,"from typing import IO, Any, Optional


class FieldDefinition:
    '''Prototype of a single field from a class definition.'''

    def __init__(self, name: str, typeStr: str, optional: bool, mapSubject: str, mapPredicate: str, typeDSL: bool):
        '''Initialize field definition.
        Creates a new field with name, its type, optional and which field to use to convert
        from list to map (or empty if it is not possible)
        '''
        self.name = name
        self.typeStr = typeStr.strip()
        self.optional = bool(optional)
        self.mapSubject = (mapSubject or """").strip()
        self.mapPredicate = (mapPredicate or """").strip()
        self.typeDSL = bool(typeDSL)

    def _is_builtin(self, t: str) -> bool:
        builtin = {
            ""void"",
            ""bool"",
            ""char"",
            ""signed char"",
            ""unsigned char"",
            ""short"",
            ""short int"",
            ""signed short"",
            ""signed short int"",
            ""unsigned short"",
            ""unsigned short int"",
            ""int"",
            ""signed"",
            ""signed int"",
            ""unsigned"",
            ""unsigned int"",
            ""long"",
            ""long int"",
            ""unsigned long"",
            ""unsigned long int"",
            ""long long"",
            ""long long int"",
            ""unsigned long long"",
            ""unsigned long long int"",
            ""float"",
            ""double"",
            ""long double"",
            ""size_t"",
            ""ptrdiff_t"",
            ""std::string"",
            ""std::wstring"",
            ""std::u16string"",
            ""std::u32string"",
        }
        std_containers_prefixes = (""std::"",)
        if t in builtin:
            return True
        if t.startswith(std_containers_prefixes):
            return True
        return False

    def _normalize_identifier(self, t: str) -> str:
        t = t.strip()
        if t == ""string"":
            return ""std::string""
        return t

    def _resolve_simple(self, t: str, namespace: str) -> str:
        t = self._normalize_identifier(t)
        if self.typeDSL and ""::"" not in t and not self._is_builtin(t):
            ns = namespace.strip()
            if ns:
                return f""{ns}::{t}""
        return t

    def _split_top_level(self, s: str) -> list[str]:
        parts = []
        depth = 0
        current = []
        for ch in s:
            if ch == '<' or ch == '[':
                depth += 1
                current.append(ch)
            elif ch == '>' or ch == ']':
                depth -= 1
                current.append(ch)
            elif ch == ',' and depth == 0:
                parts.append("""".join(current).strip())
                current = []
            else:
                current.append(ch)
        if current:
            parts.append("""".join(current).strip())
        return parts

    def _parse_generic(self, t: str) -> Optional[tuple[str, list[str]]]:
        t = t.strip()
        # Accept forms: name<...> or name[...]
        if '<' in t and t.endswith('>'):
            head, rest = t.split('<', 1)
            inner = rest[:-1]
            return head.strip(), self._split_top_level(inner)
        if '[' in t and t.endswith(']'):
            head, rest = t.split('[', 1)
            inner = rest[:-1]
            return head.strip(), self._split_top_level(inner)
        return None

    def _resolve_type(self, t: str, namespace: str) -> str:
        t = t.strip()
        parsed = self._parse_generic(t)
        if parsed:
            head, args = parsed
            head_l = head.lower()
            if head_l in (""vector"", ""list"", ""array"", ""deque""):
                cpp_head = {
                    ""vector"": ""std::vector"",
                    ""list"": ""std::vector"",
                    ""array"": ""std::array"",
                    ""deque"": ""std::deque"",
                }[head_l]
                resolved_args = [self._resolve_type(arg, namespace) for arg in args]
                return f""{cpp_head}<{', '.join(resolved_args)}>""
            if head_l in (""map"", ""unordered_map"", ""dict"", ""dictionary""):
                cpp_head = {
                    ""map"": ""std::map"",
                    ""unordered_map"": ""std::unordered_map"",
                    ""dict"": ""std::map"",
                    ""dictionary"": ""std::map"",
                }[head_l]
                resolved_args = [self._resolve_type(arg, namespace) for arg in args]
                return f""{cpp_head}<{', '.join(resolved_args)}>""
            if head_l in (""optional"",):
                resolved_args = [self._resolve_type(arg, namespace) for arg in args]
                return f""std::optional<{', '.join(resolved_args)}>""
            # Generic unknown template: resolve head then args
            resolved_head = self._resolve_simple(head, namespace)
            resolved_args = [self._resolve_type(arg, namespace) for arg in args]
            return f""{resolved_head}<{', '.join(resolved_args)}>""
        # Not generic
        return self._resolve_simple(t, namespace)

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, namespace: str) -> None:
        '''Write a C++ definition for the class field.'''
        if self.mapSubject and self.mapPredicate:
            base_type = f""std::map<{self._resolve_type(self.mapSubject, namespace)}, {self._resolve_type(self.mapPredicate, namespace)}>""
        else:
            base_type = self._resolve_type(self.typeStr, namespace)

        if self.optional and not base_type.startswith(""std::optional<""):
            base_type = f""std::optional<{base_type}>""

        line = f""{fullInd}{base_type} {self.name};\n""
        target.write(line)"
287519,kinegratii/borax,kinegratii_borax/borax/calendars/utils.py,borax.calendars.utils.ThreeNineUtils,"from borax.calendars.lunardate import LunarDate, TextUtils, TermUtils
from collections import OrderedDict
from datetime import date, datetime, timedelta
from typing import Union, Dict

class ThreeNineUtils:
    """"""三伏数九天工具函数
    """"""

    @staticmethod
    def get_39days(year: int) -> Dict[str, date]:
        """"""获取公历year年的三伏数九天对应的公历日期。
        """"""
        day13 = TermUtils.day_start_from_term(year, '夏至', 3, '庚')
        day23 = day13 + timedelta(days=10)
        day33 = TermUtils.day_start_from_term(year, '立秋', 1, '庚')
        day19 = TermUtils.day_start_from_term(year, '冬至', 0)
        days = OrderedDict({'初伏': day13, '中伏': day23, '末伏': day33, '一九': day19})
        for i, dc in enumerate(TextUtils.DAYS_CN[1:10], start=1):
            days[f'{dc}九'] = day19 + timedelta(days=(i - 1) * 9)
        return days

    @staticmethod
    def get_39label(date_obj: Union[date, LunarDate]) -> str:
        """"""返回三伏数九天对应的标签，如果不是，返回空字符串。
        """"""
        if isinstance(date_obj, LunarDate):
            sd = date_obj.to_solar_date()
        else:
            sd = date_obj
        if sd.month in (4, 5, 6, 10, 11):
            return ''
        year = sd.year - bool(sd.month < 4)
        days = ThreeNineUtils.get_39days(year)
        for vs in list(days.items()):
            label, sd = vs
            range_len = -1
            if label in ['初伏', '末伏']:
                range_len = 10
            elif label == '中伏':
                range_len = (days['末伏'] - days['中伏']).days
            elif '九' in label:
                range_len = 9
            offset = (date_obj - sd).days
            if 0 <= offset <= range_len - 1:
                return f'{label}第{offset + 1}天'
        return ''","
class ThreeNineUtils:
    '''三伏数九天工具函数
    '''
    @staticmethod
    def get_39days(year: int) -> Dict[str, date]:
        '''获取公历year年的三伏数九天对应的公历日期。
        '''
        pass
    @staticmethod
    def get_39label(date_obj: Union[date, LunarDate]) -> str:
        '''返回三伏数九天对应的标签，如果不是，返回空字符串。
        '''
        pass",snippet_212,"from datetime import date, timedelta
from typing import Dict, Union

try:
    # pip install lunar-python
    from lunar_python import Solar
    _HAS_LUNAR = True
except Exception:
    _HAS_LUNAR = False

try:
    # optional: pip install lunardate
    from lunardate import LunarDate  # type: ignore
except Exception:
    class LunarDate:  # type: ignore
        pass


class ThreeNineUtils:
    '''三伏数九天工具函数
    '''
    @staticmethod
    def _ensure_dep():
        if not _HAS_LUNAR:
            raise RuntimeError(""lunar-python package is required for ThreeNineUtils"")

    @staticmethod
    def _to_solar_date(date_obj: Union[date, LunarDate]) -> date:
        if isinstance(date_obj, date):
            return date_obj
        # handle lunardate.LunarDate if available
        if hasattr(date_obj, ""toSolarDate"") and callable(getattr(date_obj, ""toSolarDate"")):
            return date_obj.toSolarDate()  # type: ignore[attr-defined]
        # fallback: try attributes commonly present: year, month, day and a converter
        raise TypeError(""Unsupported date object type; expected datetime.date or lunardate.LunarDate"")

    @staticmethod
    def _get_label_for_date(d: date) -> str:
        ThreeNineUtils._ensure_dep()
        s = Solar.fromYmd(d.year, d.month, d.day)
        l = s.getLunar()
        fu = l.getFu()
        if fu is not None:
            # 初伏/中伏/末伏 第n天
            name = fu.getName()  # 初伏/中伏/末伏
            idx = fu.getIndex()  # 1..10/20
            return f""{name}第{idx}天""
        sj = l.getShuJiu()
        if sj is not None:
            # 一九..九九 第n天
            name = sj.getName()  # 一九..九九
            idx = sj.getIndex()  # 1..9
            return f""{name}第{idx}天""
        return """"

    @staticmethod
    def get_39days(year: int) -> Dict[str, date]:
        '''获取公历year年的三伏数九天对应的公历日期。
        返回字典：键为标签（如“初伏第1天”、“一九第3天”），值为对应的公历日期。
        仅包含该公历年的日期。
        '''
        ThreeNineUtils._ensure_dep()
        start = date(year, 1, 1)
        end = date(year, 12, 31)
        res: Dict[str, date] = {}
        d = start
        one_day = timedelta(days=1)
        while d <= end:
            label = ThreeNineUtils._get_label_for_date(d)
            if label:
                res[label] = d
            d += one_day
        return res

    @staticmethod
    def get_39label(date_obj: Union[date, LunarDate]) -> str:
        '''返回三伏数九天对应的标签，如果不是，返回空字符串。
        '''
        d = ThreeNineUtils._to_solar_date(date_obj)
        return ThreeNineUtils._get_label_for_date(d)"
292643,swistakm/graceful,swistakm_graceful/src/graceful/authentication.py,graceful.authentication.BaseAuthenticationMiddleware,"class BaseAuthenticationMiddleware:
    """"""Base class for all authentication middleware classes.

    Args:
        user_storage (BaseUserStorage): a storage object used to retrieve
            user object using their identifier lookup.
        name (str): custom name of the authentication middleware useful
            for handling custom user storage backends. Defaults to middleware
            class name.

    .. versionadded:: 0.4.0
    """"""
    challenge = None
    only_with_storage = False

    def __init__(self, user_storage=None, name=None):
        """"""Initialize authentication middleware.""""""
        self.user_storage = user_storage
        self.name = name if name else self.__class__.__name__
        if self.only_with_storage and (not isinstance(self.user_storage, BaseUserStorage)):
            raise ValueError('{} authentication middleware requires valid storage. Got {}.'.format(self.__class__.__name__, self.user_storage))

    def process_resource(self, req, resp, resource, uri_kwargs=None):
        """"""Process resource after routing to it.

        This is basic falcon middleware handler.

        Args:
            req (falcon.Request): request object
            resp (falcon.Response): response object
            resource (object): resource object matched by falcon router
            uri_kwargs (dict): additional keyword argument from uri template.
                For ``falcon<1.0.0`` this is always ``None``
        """"""
        if 'user' in req.context:
            return
        identifier = self.identify(req, resp, resource, uri_kwargs)
        user = self.try_storage(identifier, req, resp, resource, uri_kwargs)
        if user is not None:
            req.context['user'] = user
        elif self.challenge is not None:
            req.context.setdefault('challenges', list()).append(self.challenge)

    def identify(self, req, resp, resource, uri_kwargs):
        """"""Identify the user that made the request.

        Args:
            req (falcon.Request): request object
            resp (falcon.Response): response object
            resource (object): resource object matched by falcon router
            uri_kwargs (dict): additional keyword argument from uri template.
                For ``falcon<1.0.0`` this is always ``None``

        Returns:
            object: a user object (preferably a dictionary).
        """"""
        raise NotImplementedError

    def try_storage(self, identifier, req, resp, resource, uri_kwargs):
        """"""Try to find user in configured user storage object.

        Args:
            identifier: User identifier.

        Returns:
            user object.
        """"""
        if identifier is None:
            user = None
        elif self.user_storage is not None:
            user = self.user_storage.get_user(self, identifier, req, resp, resource, uri_kwargs)
        elif self.user_storage is None and (not self.only_with_storage):
            user = {'identified_with': self, 'identifier': identifier}
        else:
            user = None
        return user","class BaseAuthenticationMiddleware:
    '''Base class for all authentication middleware classes.
    Args:
        user_storage (BaseUserStorage): a storage object used to retrieve
            user object using their identifier lookup.
        name (str): custom name of the authentication middleware useful
            for handling custom user storage backends. Defaults to middleware
            class name.
    .. versionadded:: 0.4.0
    '''

    def __init__(self, user_storage=None, name=None):
        '''Initialize authentication middleware.'''
        pass

    def process_resource(self, req, resp, resource, uri_kwargs=None):
        '''Process resource after routing to it.
        This is basic falcon middleware handler.
        Args:
            req (falcon.Request): request object
            resp (falcon.Response): response object
            resource (object): resource object matched by falcon router
            uri_kwargs (dict): additional keyword argument from uri template.
                For ``falcon<1.0.0`` this is always ``None``
        '''
        pass

    def identify(self, req, resp, resource, uri_kwargs):
        '''Identify the user that made the request.
        Args:
            req (falcon.Request): request object
            resp (falcon.Response): response object
            resource (object): resource object matched by falcon router
            uri_kwargs (dict): additional keyword argument from uri template.
                For ``falcon<1.0.0`` this is always ``None``
        Returns:
            object: a user object (preferably a dictionary).
        '''
        pass

    def try_storage(self, identifier, req, resp, resource, uri_kwargs):
        '''Try to find user in configured user storage object.
        Args:
            identifier: User identifier.
        Returns:
            user object.
        '''
        pass",snippet_213,"class BaseAuthenticationMiddleware:
    '''Base class for all authentication middleware classes.
    Args:
        user_storage (BaseUserStorage): a storage object used to retrieve
            user object using their identifier lookup.
        name (str): custom name of the authentication middleware useful
            for handling custom user storage backends. Defaults to middleware
            class name.
    .. versionadded:: 0.4.0
    '''

    def __init__(self, user_storage=None, name=None):
        '''Initialize authentication middleware.'''
        self.user_storage = user_storage
        self.name = name or self.__class__.__name__

    def process_resource(self, req, resp, resource, uri_kwargs=None):
        '''Process resource after routing to it.
        This is basic falcon middleware handler.
        Args:
            req (falcon.Request): request object
            resp (falcon.Response): response object
            resource (object): resource object matched by falcon router
            uri_kwargs (dict): additional keyword argument from uri template.
                For ``falcon<1.0.0`` this is always ``None``
        '''
        user = self.identify(req, resp, resource, uri_kwargs)
        # Attach identified user onto the request in a best-effort manner
        try:
            if hasattr(req, 'context') and req.context is not None:
                ctx = req.context
                # dict-like context
                if isinstance(ctx, dict):
                    ctx['user'] = user
                else:
                    # object-like context
                    try:
                        setattr(ctx, 'user', user)
                    except Exception:
                        pass
        except Exception:
            pass
        # Also attach directly on request for convenience
        try:
            setattr(req, 'user', user)
        except Exception:
            pass

    def identify(self, req, resp, resource, uri_kwargs):
        '''Identify the user that made the request.
        Args:
            req (falcon.Request): request object
            resp (falcon.Response): response object
            resource (object): resource object matched by falcon router
            uri_kwargs (dict): additional keyword argument from uri template.
                For ``falcon<1.0.0`` this is always ``None``
        Returns:
            object: a user object (preferably a dictionary).
        '''
        return None

    def try_storage(self, identifier, req, resp, resource, uri_kwargs):
        '''Try to find user in configured user storage object.
        Args:
            identifier: User identifier.
        Returns:
            user object.
        '''
        if identifier is None or self.user_storage is None:
            return None

        storage = self.user_storage

        # If storage is callable (factory/lookup), try calling with identifier
        try:
            if callable(storage):
                user = storage(identifier)
                if user is not None:
                    return user
        except Exception:
            pass

        # Try common lookup methods
        methods = (
            'get',
            'get_user',
            'get_by_id',
            'find',
            'lookup',
            'retrieve',
            'fetch',
        )
        for m in methods:
            try:
                meth = getattr(storage, m, None)
                if callable(meth):
                    user = meth(identifier)
                    if user is not None:
                        return user
            except Exception:
                continue

        # Mapping-style access
        try:
            if isinstance(storage, dict):
                return storage.get(identifier)
        except Exception:
            pass

        try:
            # __getitem__ style
            return storage[identifier]  # type: ignore[index]
        except Exception:
            pass

        return None"
292765,pytroll/pyspectral,pytroll_pyspectral/rsr_convert_scripts/avhrr1_rsr.py,avhrr1_rsr.AvhrrRSR,"import numpy as np
from pyspectral.config import get_config
from xlrd import open_workbook
import os

class AvhrrRSR:
    """"""Container for the NOAA AVHRR-1 RSR data.""""""

    def __init__(self, wavespace='wavelength'):
        """"""Initialize the AVHRR-1 RSR class.""""""
        options = get_config()
        self.avhrr_path = options['avhrr/1'].get('path')
        if not os.path.exists(self.avhrr_path):
            self.avhrr1_path = os.path.join(DATA_PATH, options['avhrr/1'].get('filename'))
        self.output_dir = options.get('rsr_dir', './')
        self.rsr = {}
        for satname in AVHRR1_SATELLITES:
            self.rsr[satname] = {}
            for chname in AVHRR_BAND_NAMES['avhrr/1']:
                self.rsr[satname][chname] = {'wavelength': None, 'response': None}
        self._load()
        self.wavespace = wavespace
        if wavespace not in ['wavelength', 'wavenumber']:
            raise AttributeError('wavespace has to be either ' + ""'wavelength' or 'wavenumber'!"")
        self.unit = 'micrometer'
        if wavespace == 'wavenumber':
            self.convert2wavenumber()

    def _load(self, scale=1.0):
        """"""Load the AVHRR RSR data for the band requested.""""""
        wb_ = open_workbook(self.avhrr_path)
        sheet_names = []
        for sheet in wb_.sheets():
            if sheet.name in ['Kleespies Data']:
                print('Skip sheet...')
                continue
            ch_name = CHANNEL_NAMES.get(sheet.name.strip())
            if not ch_name:
                break
            sheet_names.append(sheet.name.strip())
            header = sheet.col_values(0, start_rowx=0, end_rowx=2)
            platform_name = header[0].strip('# ')
            unit = header[1].split('Wavelength (')[1].strip(')')
            scale = get_scale_from_unit(unit)
            wvl = sheet.col_values(0, start_rowx=2)
            is_comment = True
            idx = 0
            while is_comment:
                item = wvl[::-1][idx]
                if isinstance(item, str):
                    idx = idx + 1
                else:
                    break
            ndim = len(wvl) - idx
            wvl = wvl[0:ndim]
            if platform_name == 'TIROS-N':
                wvl = adjust_typo_avhrr1_srf_only_xls_file(platform_name, wvl)
            response = sheet.col_values(1, start_rowx=2, end_rowx=2 + ndim)
            wavelength = np.array(wvl) * scale
            response = np.array(response)
            self.rsr[platform_name][ch_name]['wavelength'] = wavelength
            self.rsr[platform_name][ch_name]['response'] = response","
class AvhrrRSR:
    '''Container for the NOAA AVHRR-1 RSR data.'''

    def __init__(self, wavespace='wavelength'):
        '''Initialize the AVHRR-1 RSR class.'''
        pass

    def _load(self, scale=1.0):
        '''Load the AVHRR RSR data for the band requested.'''
        pass",snippet_215,"import math
import numpy as np

class AvhrrRSR:
    '''Container for the NOAA AVHRR-1 RSR data.'''

    def __init__(self, wavespace='wavelength'):
        '''Initialize the AVHRR-1 RSR class.'''
        ws = str(wavespace).lower()
        if ws not in ('wavelength', 'wavenumber'):
            raise ValueError(""wavespace must be 'wavelength' or 'wavenumber'"")
        self.wavespace = ws
        self._centers_um = {
            1: 0.63,   # approximate band centers (micrometers)
            2: 0.86,
            3: 3.75,
            4: 10.8,
        }
        self._fwhm_um = {
            1: 0.10,   # approximate full-width at half-maximum (micrometers)
            2: 0.30,
            3: 0.38,
            4: 1.00,
        }
        self._samples = 1001
        self.data = None

    def _load(self, scale=1.0):
        '''Load the AVHRR RSR data for the band requested.'''
        if not (isinstance(scale, (int, float)) and np.isfinite(scale) and scale > 0):
            raise ValueError(""scale must be a positive finite number"")

        def gaussian_rsr(lam_um, center_um, fwhm_um):
            # Gaussian defined by FWHM: exp(-4 ln(2) * ((x-mu)^2 / fwhm^2))
            rsr = np.exp(-4.0 * math.log(2.0) * ((lam_um - center_um) ** 2) / (fwhm_um ** 2))
            # normalize to peak of 1.0 then scale
            peak = rsr.max()
            if peak > 0:
                rsr = rsr / peak
            rsr = np.clip(rsr * scale, 0.0, 1.0)
            return rsr

        out = {'wavespace': self.wavespace, 'channels': {}}

        for ch in sorted(self._centers_um):
            c = self._centers_um[ch]
            w = self._fwhm_um[ch]
            lam_min = max(1e-6, c - 3.0 * w)
            lam_max = c + 3.0 * w
            lam = np.linspace(lam_min, lam_max, self._samples, dtype=float)
            rsr = gaussian_rsr(lam, c, w)

            if self.wavespace == 'wavelength':
                x = lam  # micrometers
            else:
                # Convert to wavenumber in cm^-1: nu = 1 / lambda(cm) = 1e4 / lambda(um)
                nu = 1e4 / lam
                # Ensure increasing x for consistency
                order = np.argsort(nu)
                x = nu[order]
                rsr = rsr[order]

            out['channels'][ch] = {'x': x, 'rsr': rsr}

        self.data = out
        return out"
293144,Kentzo/Power,Kentzo_Power/power/common.py,power.common.PowerManagementObserver,"from abc import ABCMeta, abstractmethod

class PowerManagementObserver:
    """"""
    Base class for PowerManagement observers.
    Do not make assumptions in what thread or event loop these methods are called.
    """"""
    __metaclass__ = ABCMeta

    @abstractmethod
    def on_power_sources_change(self, power_management):
        """"""
        @param power_management: Instance of PowerManagement posted notification
        """"""
        pass

    @abstractmethod
    def on_time_remaining_change(self, power_management):
        """"""
        @param power_management: Instance of PowerManagement posted notification
        """"""
        pass","
class PowerManagementObserver:
    '''
    Base class for PowerManagement observers.
    Do not make assumptions in what thread or event loop these methods are called.
    '''
    @abstractmethod
    def on_power_sources_change(self, power_management):
        '''
        @param power_management: Instance of PowerManagement posted notification
        '''
        pass
    @abstractmethod
    def on_time_remaining_change(self, power_management):
        '''
        @param power_management: Instance of PowerManagement posted notification
        '''
        pass",snippet_216,"from abc import ABC, abstractmethod
from typing import Any


class PowerManagementObserver(ABC):
    '''
    Base class for PowerManagement observers.
    Do not make assumptions in what thread or event loop these methods are called.
    '''

    @abstractmethod
    def on_power_sources_change(self, power_management: Any) -> None:
        '''
        @param power_management: Instance of PowerManagement posted notification
        '''
        raise NotImplementedError

    @abstractmethod
    def on_time_remaining_change(self, power_management: Any) -> None:
        '''
        @param power_management: Instance of PowerManagement posted notification
        '''
        raise NotImplementedError"
293849,hasgeek/coaster,hasgeek_coaster/src/coaster/logger.py,coaster.logger.FilteredValueIndicator,"class FilteredValueIndicator:
    """"""Represent a filtered value.""""""

    def __str__(self) -> str:
        """"""Filter str.""""""
        return '[Filtered]'

    def __repr__(self) -> str:
        """"""Filter repr.""""""
        return '[Filtered]'","class FilteredValueIndicator:
    '''Represent a filtered value.'''

    def __str__(self) -> str:
        '''Filter str.'''
        pass

    def __repr__(self) -> str:
        '''Filter repr.'''
        pass",snippet_217,"class FilteredValueIndicator:
    '''Represent a filtered value.'''

    def __str__(self) -> str:
        '''Filter str.'''
        return ""[Filtered]""

    def __repr__(self) -> str:
        '''Filter repr.'''
        return ""[Filtered]"""
293851,hasgeek/coaster,hasgeek_coaster/src/coaster/logger.py,coaster.logger.RepeatValueIndicator,"class RepeatValueIndicator:
    """"""Represent a repeating value.""""""

    def __init__(self, key: str) -> None:
        """"""Init with key.""""""
        self.key = key

    def __repr__(self) -> str:
        """"""Return representation.""""""
        return f'<same as prior {self.key!r}>'
    __str__ = __repr__","class RepeatValueIndicator:
    '''Represent a repeating value.'''

    def __init__(self, key: str) -> None:
        '''Init with key.'''
        pass

    def __repr__(self) -> str:
        '''Return representation.'''
        pass",snippet_218,"class RepeatValueIndicator:
    '''Represent a repeating value.'''

    def __init__(self, key: str) -> None:
        '''Init with key.'''
        if not isinstance(key, str):
            raise TypeError(""key must be a str"")
        if not key:
            raise ValueError(""key cannot be empty"")
        self.key = key

    def __repr__(self) -> str:
        '''Return representation.'''
        return f""RepeatValueIndicator({self.key!r})"""
295935,cloudsmith-io/cloudsmith-cli,cloudsmith-io_cloudsmith-cli/cloudsmith_cli/core/ratelimits.py,cloudsmith_cli.core.ratelimits.RateLimitsInfo,"import datetime

class RateLimitsInfo:
    """"""Data for rate limits.""""""
    interval = None
    limit = None
    remaining = None
    reset = None
    throttled = None

    def __str__(self):
        """"""Get rate limit information as text.""""""
        return 'Throttled: %(throttled)s, Remaining: %(remaining)d/%(limit)d, Interval: %(interval)f, Reset: %(reset)s' % {'throttled': 'Yes' if self.throttled else 'No', 'remaining': self.remaining, 'limit': self.limit, 'interval': self.interval, 'reset': self.reset}

    @classmethod
    def from_dict(cls, data):
        """"""Create RateLimitsInfo from a dictionary.""""""
        info = RateLimitsInfo()
        if 'interval' in data:
            info.interval = float(data['interval'])
        if 'limit' in data:
            info.limit = int(data['limit'])
        if 'remaining' in data:
            info.remaining = int(data['remaining'])
        if 'reset' in data:
            info.reset = datetime.datetime.utcfromtimestamp(int(data['reset']))
        if 'throtted' in data:
            info.throttled = bool(data['throttled'])
        else:
            info.throttled = info.remaining == 0
        return info

    @classmethod
    def from_headers(cls, headers):
        """"""Create RateLimitsInfo from HTTP headers.""""""
        try:
            data = {'interval': headers['X-RateLimit-Interval'], 'limit': headers['X-RateLimit-Limit'], 'remaining': headers['X-RateLimit-Remaining'], 'reset': headers['X-RateLimit-Reset']}
        except KeyError:
            data = {}
        return cls.from_dict(data)","
class RateLimitsInfo:
    '''Data for rate limits.'''

    def __str__(self):
        '''Get rate limit information as text.'''
        pass
    @classmethod
    def from_dict(cls, data):
        '''Create RateLimitsInfo from a dictionary.'''
        pass
    @classmethod
    def from_headers(cls, headers):
        '''Create RateLimitsInfo from HTTP headers.'''
        pass",snippet_220,"import datetime
import time
from email.utils import parsedate_to_datetime


class RateLimitsInfo:
    '''Data for rate limits.'''

    __slots__ = (
        'limit',
        'remaining',
        'used',
        'reset_epoch',
        'reset_after',
        'retry_after',
        'resource',
        'bucket',
        'window',
        'raw',
    )

    def __init__(
        self,
        limit=None,
        remaining=None,
        used=None,
        reset_epoch=None,
        reset_after=None,
        retry_after=None,
        resource=None,
        bucket=None,
        window=None,
        raw=None,
    ):
        self.limit = limit
        self.remaining = remaining
        self.used = used
        self.reset_epoch = reset_epoch
        self.reset_after = reset_after
        self.retry_after = retry_after
        self.resource = resource
        self.bucket = bucket
        self.window = window
        self.raw = raw or {}

    def __str__(self):
        parts = []
        if self.limit is not None:
            parts.append(f""limit={self.limit}"")
        if self.remaining is not None:
            parts.append(f""remaining={self.remaining}"")
        if self.used is not None:
            parts.append(f""used={self.used}"")
        if self.reset_epoch is not None:
            try:
                dt = datetime.datetime.utcfromtimestamp(int(self.reset_epoch))
                parts.append(f""reset_at={dt.isoformat()}Z"")
            except Exception:
                parts.append(f""reset_epoch={self.reset_epoch}"")
        if self.reset_after is not None:
            parts.append(f""reset_after={self.reset_after}s"")
        if self.retry_after is not None:
            parts.append(f""retry_after={self.retry_after}s"")
        if self.resource:
            parts.append(f""resource={self.resource}"")
        if self.bucket:
            parts.append(f""bucket={self.bucket}"")
        if self.window:
            parts.append(f""window={self.window}"")
        return "", "".join(parts) if parts else ""No rate limit data""

    @classmethod
    def from_dict(cls, data):
        '''Create RateLimitsInfo from a dictionary.'''
        if data is None:
            return cls(raw={})
        if not isinstance(data, dict):
            raise TypeError(""data must be a dict"")

        def to_int(val):
            if val is None or val == """":
                return None
            try:
                return int(val)
            except Exception:
                try:
                    return int(float(val))
                except Exception:
                    return None

        def to_float(val):
            if val is None or val == """":
                return None
            try:
                return float(val)
            except Exception:
                return None

        # Normalize keys to lower without spaces/underscores/dashes
        norm = {}
        for k, v in data.items():
            nk = str(k).strip().lower().replace(""-"", ""_"").replace("" "", ""_"")
            norm[nk] = v

        # Gather values with common aliases
        limit = to_int(norm.get(""limit"") or norm.get(""x_ratelimit_limit""))
        remaining = to_int(norm.get(""remaining"") or norm.get(""x_ratelimit_remaining""))
        used = to_int(norm.get(""used"") or norm.get(""x_ratelimit_used""))
        reset_epoch = to_int(norm.get(""reset"") or norm.get(""reset_epoch"") or norm.get(""x_ratelimit_reset""))
        reset_after = to_float(norm.get(""reset_after"") or norm.get(""x_ratelimit_reset_after""))
        retry_after = to_float(norm.get(""retry_after""))

        resource = norm.get(""resource"") or norm.get(""x_ratelimit_resource"")
        bucket = norm.get(""bucket"") or norm.get(""x_ratelimit_bucket"")
        window = norm.get(""window"") or norm.get(""period"") or norm.get(""interval"")

        return cls(
            limit=limit,
            remaining=remaining,
            used=used,
            reset_epoch=reset_epoch,
            reset_after=reset_after,
            retry_after=retry_after,
            resource=resource,
            bucket=bucket,
            window=window,
            raw=dict(data),
        )

    @classmethod
    def from_headers(cls, headers):
        '''Create RateLimitsInfo from HTTP headers.'''
        if headers is None:
            return cls(raw={})
        # Support any mapping-like object
        try:
            items = headers.items()
        except Exception:
            raise TypeError(""headers must be a mapping with .items()"")

        # Normalize header keys to lowercase
        norm = {}
        for k, v in items:
            nk = str(k).strip().lower()
            norm[nk] = v

        def get_first(name):
            v = norm.get(name)
            # Some clients may join multiple header values with commas
            if isinstance(v, str) and "","" in v and name != ""date"":
                # Take the first token for numeric fields
                parts = [p.strip() for p in v.split("","")]
                return parts[0] if parts else v
            return v

        def to_int(val):
            if val is None or val == """":
                return None
            try:
                return int(val)
            except Exception:
                try:
                    return int(float(val))
                except Exception:
                    return None

        def to_float(val):
            if val is None or val == """":
                return None
            try:
                return float(val)
            except Exception:
                return None

        limit = to_int(get_first(""x-ratelimit-limit""))
        remaining = to_int(get_first(""x-ratelimit-remaining""))
        used = to_int(get_first(""x-ratelimit-used""))
        reset_epoch = to_int(get_first(""x-ratelimit-reset""))
        reset_after = to_float(get_first(""x-ratelimit-reset-after""))

        # Retry-After can be seconds or HTTP-date
        retry_after_val = get_first(""retry-after"")
        retry_after = None
        if retry_after_val is not None and retry_after_val != """":
            # Try numeric seconds first
            ra_seconds = to_float(retry_after_val)
            if ra_seconds is not None:
                retry_after = ra_seconds
            else:
                try:
                    dt = parsedate_to_datetime(retry_after_val)
                    if dt.tzinfo is None:
                        # Assume UTC if no tz info
                        dt = dt.replace(tzinfo=datetime.timezone.utc)
                    now = datetime.datetime.now(datetime.timezone.utc)
                    delta = (dt - now).total_seconds()
                    retry_after = max(0.0, delta)
                except Exception:
                    retry_after = None

        resource = get_first(""x-ratelimit-resource"")
        bucket = get_first(""x-ratelimit-bucket"")
        window = get_first(""x-ratelimit-window"") or get_first(""ratelimit-window"")

        return cls(
            limit=limit,
            remaining=remaining,
            used=used,
            reset_epoch=reset_epoch,
            reset_after=reset_after,
            retry_after=retry_after,
            resource=resource,
            bucket=bucket,
            window=window,
            raw=dict(headers),
        )"
296505,CZ-NIC/python-rt,CZ-NIC_python-rt/rt/rest2.py,rt.rest2.Attachment,"import dataclasses
import base64

@dataclasses.dataclass
class Attachment:
    """"""Dataclass representing an attachment.""""""
    file_name: str
    file_type: str
    file_content: bytes

    def to_dict(self) -> dict[str, str]:
        """"""Convert to a dictionary for submitting to the REST API.""""""
        return {'FileName': self.file_name, 'FileType': self.file_type, 'FileContent': base64.b64encode(self.file_content).decode('utf-8')}

    def multipart_form_element(self) -> tuple[str, bytes, str]:
        """"""Convert to a tuple as required for multipart-form-data submission.""""""
        return (self.file_name, self.file_content, self.file_type)","@dataclasses.dataclass
class Attachment:
    '''Dataclass representing an attachment.'''

    def to_dict(self) -> dict[str, str]:
        '''Convert to a dictionary for submitting to the REST API.'''
        pass

    def multipart_form_element(self) -> tuple[str, bytes, str]:
        '''Convert to a tuple as required for multipart-form-data submission.'''
        pass",snippet_221,"import dataclasses
import base64
from typing import Union


@dataclasses.dataclass
class Attachment:
    '''Dataclass representing an attachment.'''
    filename: str
    content: Union[bytes, str]
    content_type: str = 'application/octet-stream'

    def __post_init__(self) -> None:
        if isinstance(self.content, str):
            self.content = self.content.encode('utf-8')

    def to_dict(self) -> dict[str, str]:
        '''Convert to a dictionary for submitting to the REST API.'''
        return {
            'filename': self.filename,
            'content': base64.b64encode(self.content).decode('ascii'),
            'content_type': self.content_type,
        }

    def multipart_form_element(self) -> tuple[str, bytes, str]:
        '''Convert to a tuple as required for multipart-form-data submission.'''
        return (self.filename, self.content, self.content_type)"
297960,theolind/pymysensors,theolind_pymysensors/mysensors/transport.py,mysensors.transport.Transport,"class Transport:
    """"""Handle gateway transport.

    I/O is allowed in this class. This class should host methods that
    are related to the gateway transport type.
    """"""

    def __init__(self, gateway, connect, timeout=1.0, reconnect_timeout=10.0, **kwargs):
        """"""Set up transport.""""""
        self._connect = connect
        self.can_log = False
        self.connect_task = None
        self.gateway = gateway
        self.protocol = None
        self.reconnect_timeout = reconnect_timeout
        self.timeout = timeout

    def disconnect(self):
        """"""Disconnect from the transport.""""""
        if not self.protocol or not self.protocol.transport:
            self.protocol = None
            return
        _LOGGER.info('Disconnecting from gateway')
        self.protocol.transport.close()
        self.protocol = None

    def send(self, message):
        """"""Write a message to the gateway.""""""
        if not message or not self.protocol or (not self.protocol.transport):
            return
        if not self.can_log:
            _LOGGER.debug('Sending %s', message.strip())
        try:
            self.protocol.transport.write(message.encode())
        except OSError as exc:
            _LOGGER.error('Failed writing to transport %s: %s', self.protocol.transport, exc)
            self.protocol.transport.close()
            self.protocol.conn_lost_callback()","class Transport:
    '''Handle gateway transport.
    I/O is allowed in this class. This class should host methods that
    are related to the gateway transport type.
    '''

    def __init__(self, gateway, connect, timeout=1.0, reconnect_timeout=10.0, **kwargs):
        '''Set up transport.'''
        pass

    def disconnect(self):
        '''Disconnect from the transport.'''
        pass

    def send(self, message):
        '''Write a message to the gateway.'''
        pass",snippet_222,"import time
import threading
import socket


class Transport:
    '''Handle gateway transport.
    I/O is allowed in this class. This class should host methods that
    are related to the gateway transport type.
    '''

    def __init__(self, gateway, connect, timeout=1.0, reconnect_timeout=10.0, **kwargs):
        '''Set up transport.'''
        if not callable(connect):
            raise TypeError(""connect must be a callable that returns a connection object"")
        self.gateway = gateway
        self._connect_func = connect
        self.timeout = float(timeout) if timeout is not None else None
        self.reconnect_timeout = float(reconnect_timeout) if reconnect_timeout is not None else 0.0
        self.kwargs = dict(kwargs) if kwargs else {}
        self._conn = None
        self._lock = threading.RLock()
        self._closed = False
        self._connect()

    def _set_timeout_if_possible(self, conn):
        if conn is None:
            return
        # socket-like
        try:
            if hasattr(conn, ""settimeout""):
                conn.settimeout(self.timeout)
                return
        except Exception:
            pass
        # file-like: nothing generic to set

    def _connect(self):
        with self._lock:
            if self._closed:
                raise RuntimeError(""Transport is closed"")
            conn = self._connect_func(self.gateway, **self.kwargs)
            self._set_timeout_if_possible(conn)
            self._conn = conn

    def _close_conn(self, conn):
        try:
            if conn is None:
                return
            if hasattr(conn, ""close""):
                conn.close()
        except Exception:
            pass

    def disconnect(self):
        '''Disconnect from the transport.'''
        with self._lock:
            self._closed = True
            conn, self._conn = self._conn, None
        self._close_conn(conn)

    def _ensure_connected(self, start_time=None):
        with self._lock:
            if self._closed:
                raise RuntimeError(""Transport is closed"")
            if self._conn is not None:
                return
        # attempt reconnects outside lock to avoid blocking other operations
        deadline = None if self.reconnect_timeout is None else (
            (start_time or time.monotonic()) + float(self.reconnect_timeout)
        )
        delay = 0.05
        while True:
            try:
                self._connect()
                return
            except Exception:
                if deadline is not None and time.monotonic() >= deadline:
                    raise
                time.sleep(delay)
                delay = min(delay * 2, 1.0)

    def _send_bytes(self, data):
        conn = self._conn
        if conn is None:
            raise ConnectionError(""Not connected"")
        # Preferred order: sendall, send, write(+flush)
        if hasattr(conn, ""sendall""):
            conn.sendall(data)
            return
        if hasattr(conn, ""send""):
            total = 0
            n = len(data)
            while total < n:
                sent = conn.send(data[total:])
                if sent is None:
                    raise RuntimeError(""send returned None"")
                if sent == 0:
                    raise ConnectionError(""Socket connection broken"")
                total += sent
            return
        if hasattr(conn, ""write""):
            written = conn.write(data)
            if hasattr(conn, ""flush""):
                try:
                    conn.flush()
                except Exception:
                    pass
            if written is not None and written != len(data):
                raise IOError(""Short write"")
            return
        raise TypeError(""Connection object does not support sending data"")

    def send(self, message):
        '''Write a message to the gateway.'''
        if message is None:
            raise ValueError(""message cannot be None"")
        if isinstance(message, str):
            data = message.encode(""utf-8"")
        elif isinstance(message, (bytes, bytearray, memoryview)):
            data = bytes(message)
        else:
            data = str(message).encode(""utf-8"")

        with self._lock:
            if self._closed:
                raise RuntimeError(""Transport is closed"")

        start = time.monotonic()
        try:
            self._ensure_connected(start_time=start)
            with self._lock:
                self._send_bytes(data)
            return

        except Exception:
            # Attempt reconnect loop and retry send
            deadline = start + float(self.reconnect_timeout)
            last_err = None
            delay = 0.05
            while time.monotonic() < deadline:
                try:
                    # Close old conn (if any) and reconnect
                    old = None
                    with self._lock:
                        old, self._conn = self._conn, None
                    self._close_conn(old)
                    self._ensure_connected(start_time=start)
                    with self._lock:
                        self._send_bytes(data)
                    return
                except Exception as e:
                    last_err = e
                    time.sleep(delay)
                    delay = min(delay * 2, 1.0)
            if last_err:
                raise last_err
            raise ConnectionError(""Failed to send message within reconnect timeout"")"
298148,edx/event-tracking,edx_event-tracking/eventtracking/backends/logger.py,eventtracking.backends.logger.LoggerBackend,"import logging
import json

class LoggerBackend:
    """"""
    Event tracker backend that uses a python logger.

    Events are logged to the INFO level as JSON strings.
    """"""

    def __init__(self, **kwargs):
        """"""
        Event tracker backend that uses a python logger.

        `name` is an identifier for the logger, which should have
            been configured using the default python mechanisms.
        """"""
        name = kwargs.get('name', None)
        self.max_event_size = kwargs.get('max_event_size', MAX_EVENT_SIZE)
        self.event_logger = logging.getLogger(name)
        level = kwargs.get('level', 'info')
        self.log = getattr(self.event_logger, level.lower())

    def send(self, event):
        """"""Send the event to the standard python logger""""""
        event_str = json.dumps(event, cls=DateTimeJSONEncoder)
        if self.max_event_size is None or len(event_str) <= self.max_event_size:
            self.log(event_str)","
class LoggerBackend:
    '''
    Event tracker backend that uses a python logger.
    Events are logged to the INFO level as JSON strings.
    '''

    def __init__(self, **kwargs):
        '''
        Event tracker backend that uses a python logger.
        `name` is an identifier for the logger, which should have
            been configured using the default python mechanisms.
        '''
        pass

    def send(self, event):
        '''Send the event to the standard python logger'''
        pass",snippet_223,"import logging
import json


class LoggerBackend:
    '''
    Event tracker backend that uses a python logger.
    Events are logged to the INFO level as JSON strings.
    '''

    def __init__(self, **kwargs):
        '''
        Event tracker backend that uses a python logger.
        `name` is an identifier for the logger, which should have
            been configured using the default python mechanisms.
        '''
        name = kwargs.get('name')
        logger = kwargs.get('logger')
        self.level = kwargs.get('level', logging.INFO)
        self.ensure_ascii = kwargs.get('ensure_ascii', False)
        self.json_encoder = kwargs.get('json_encoder', None)

        if logger is not None:
            self.logger = logger
        else:
            self.logger = logging.getLogger(name) if name else logging.getLogger()

    def _json_default(self, obj):
        for attr in ('to_dict', 'dict', 'model_dump'):
            fn = getattr(obj, attr, None)
            if callable(fn):
                try:
                    return fn()
                except Exception:
                    pass
        try:
            return vars(obj)
        except Exception:
            return str(obj)

    def send(self, event):
        '''Send the event to the standard python logger'''
        try:
            payload = json.dumps(
                event,
                ensure_ascii=self.ensure_ascii,
                cls=self.json_encoder,
                default=self._json_default,
                separators=(',', ':')
            )
        except Exception:
            payload = json.dumps(
                {'error': 'failed to serialize event', 'repr': repr(event)},
                ensure_ascii=self.ensure_ascii,
                separators=(',', ':')
            )
        self.logger.log(self.level, payload)"
303020,UpCloudLtd/upcloud-python-api,UpCloudLtd_upcloud-python-api/upcloud_api/cloud_manager/host_mixin.py,upcloud_api.cloud_manager.host_mixin.HostManager,"from upcloud_api.api import API
from upcloud_api.host import Host

class HostManager:
    """"""
    Functions for managing hosts. Intended to be used as a mixin for CloudManager.
    """"""
    api: API

    def get_hosts(self):
        """"""
        Returns a list of available hosts, along with basic statistics of them when available.
        """"""
        url = '/host'
        res = self.api.get_request(url)
        return [Host(**host) for host in res['hosts']['host']]

    def get_host(self, id: str) -> Host:
        """"""
        Returns detailed information about a specific host.
        """"""
        url = f'/host/{id}'
        res = self.api.get_request(url)
        return Host(**res['host'])

    def modify_host(self, host: str, description: str) -> Host:
        """"""
        Modifies description of a specific host.
        """"""
        url = f'/host/{host}'
        body = {'host': {'description': description}}
        res = self.api.patch_request(url, body)
        return Host(**res['host'])","
class HostManager:
    '''
    Functions for managing hosts. Intended to be used as a mixin for CloudManager.
    '''

    def get_hosts(self):
        '''
        Returns a list of available hosts, along with basic statistics of them when available.
        '''
        pass

    def get_hosts(self):
        '''
        Returns detailed information about a specific host.
        '''
        pass

    def modify_host(self, host: str, description: str) -> Host:
        '''
        Modifies description of a specific host.
        '''
        pass",snippet_224,"from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional


@dataclass
class Host:
    name: str
    description: Optional[str] = None
    stats: Dict[str, Any] = field(default_factory=dict)


class HostManager:
    '''
    Functions for managing hosts. Intended to be used as a mixin for CloudManager.
    '''

    def _ensure_host_store(self) -> None:
        if not hasattr(self, ""_hosts""):
            self._hosts: Dict[str, Host] = {}

    def get_hosts(self) -> List[Host]:
        '''
        Returns a list of available hosts, along with basic statistics of them when available.
        '''
        self._ensure_host_store()
        return list(self._hosts.values())

    def get_host(self, host: str) -> Host:
        '''
        Returns detailed information about a specific host.
        '''
        self._ensure_host_store()
        try:
            return self._hosts[host]
        except KeyError as e:
            raise KeyError(f""Host not found: {host}"") from e

    def modify_host(self, host: str, description: str) -> Host:
        '''
        Modifies description of a specific host.
        '''
        self._ensure_host_store()
        if host not in self._hosts:
            self._hosts[host] = Host(name=host, description=description)
        else:
            self._hosts[host].description = description
        return self._hosts[host]"
304970,threeML/astromodels,threeML_astromodels/astromodels/functions/template_model.py,astromodels.functions.template_model.TemplateFile,"import numpy as np
import h5py
from dataclasses import dataclass
import collections
from typing import Dict, List, Optional, Union

@dataclass
class TemplateFile:
    """"""
    simple container to read and write
    the data to an hdf5 file

    """"""
    name: str
    description: str
    grid: np.ndarray
    parameters: Dict[str, np.ndarray]
    parameter_order: List[str]
    energies: np.ndarray
    interpolation_degree: int
    spline_smoothing_factor: float

    def save(self, file_name: str):
        """"""
        serialize the contents to a file

        :param file_name:
        :type file_name: str
        :returns:

        """"""
        with h5py.File(file_name, 'w') as f:
            f.attrs['name'] = self.name
            f.attrs['description'] = self.description
            f.attrs['interpolation_degree'] = self.interpolation_degree
            f.attrs['spline_smoothing_factor'] = self.spline_smoothing_factor
            f.create_dataset('energies', data=self.energies, compression='gzip')
            f.create_dataset('grid', data=self.grid, compression='gzip')
            dt = h5py.special_dtype(vlen=str)
            po = np.array(self.parameter_order, dtype=dt)
            f.create_dataset('parameter_order', data=po)
            par_group = f.create_group('parameters')
            for k in self.parameter_order:
                par_group.create_dataset(k, data=self.parameters[k], compression='gzip')

    @classmethod
    def from_file(cls, file_name: str):
        """"""
        read contents from a file

        :param cls:
        :type cls:
        :param file_name:
        :type file_name: str
        :returns:

        """"""
        with h5py.File(file_name, 'r') as f:
            name = f.attrs['name']
            description = f.attrs['description']
            interpolation_degree = f.attrs['interpolation_degree']
            spline_smoothing_factor = f.attrs['spline_smoothing_factor']
            energies = f['energies'][()]
            parameter_order = f['parameter_order'][()]
            grid = f['grid'][()]
            parameters = collections.OrderedDict()
            for k in parameter_order:
                parameters[k] = f['parameters'][k][()]
        return cls(name=name, description=description, interpolation_degree=interpolation_degree, spline_smoothing_factor=spline_smoothing_factor, energies=energies, parameter_order=parameter_order, parameters=parameters, grid=grid)","@dataclass
class TemplateFile:
    '''
    simple container to read and write
    the data to an hdf5 file
    '''

    def save(self, file_name: str):
        '''
        serialize the contents to a file
        :param file_name:
        :type file_name: str
        :returns:
        '''
        pass
    @classmethod
    def from_file(cls, file_name: str):
        '''
        read contents from a file
        :param cls:
        :type cls:
        :param file_name:
        :type file_name: str
        :returns:
        '''
        pass",snippet_227,"from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any, Dict

import pickle


@dataclass
class TemplateFile:
    '''
    simple container to read and write
    the data to an hdf5 file
    '''
    data: Dict[str, Any] = field(default_factory=dict)

    def save(self, file_name: str):
        '''
        serialize the contents to a file
        :param file_name:
        :type file_name: str
        :returns:
        '''
        try:
            import h5py
        except Exception as e:
            raise RuntimeError(""h5py is required to save to HDF5 files"") from e

        def _write_value(group, name: str, value: Any):
            # Dictionaries become groups
            if isinstance(value, dict):
                subgrp = group.create_group(name)
                subgrp.attrs[""__type__""] = ""dict""
                for k, v in value.items():
                    _write_value(subgrp, str(k), v)
                return

            # Primitive or array-like via native h5py support
            try:
                group.create_dataset(name, data=value)
                return
            except Exception:
                pass

            # Fallback: pickle arbitrary Python objects as variable-length bytes
            try:
                dt = h5py.vlen_dtype(bytes)
                ds = group.create_dataset(name, data=pickle.dumps(value, protocol=pickle.HIGHEST_PROTOCOL), dtype=dt)
                ds.attrs[""__pickled__""] = True
            except Exception as e:
                raise TypeError(f""Unable to serialize key '{name}' of type {type(value)}"") from e

        with h5py.File(file_name, ""w"") as f:
            f.attrs[""__templatefile__""] = True
            root = f.create_group(""data"")
            root.attrs[""__type__""] = ""dict""
            for k, v in self.data.items():
                _write_value(root, str(k), v)

    @classmethod
    def from_file(cls, file_name: str):
        '''
        read contents from a file
        :param cls:
        :type cls:
        :param file_name:
        :type file_name: str
        :returns:
        '''
        try:
            import h5py
        except Exception as e:
            raise RuntimeError(""h5py is required to read HDF5 files"") from e

        def _read_node(obj):
            # Group -> dict
            if isinstance(obj, h5py.Group):
                result = {}
                for name, item in obj.items():
                    result[name] = _read_node(item)
                return result

            # Dataset -> possibly pickled, else native value
            if isinstance(obj, h5py.Dataset):
                if obj.attrs.get(""__pickled__"", False):
                    raw = obj[()]
                    if isinstance(raw, (bytes, bytearray, memoryview)):
                        data_bytes = bytes(raw)
                    else:
                        data_bytes = bytes(raw)  # best-effort
                    return pickle.loads(data_bytes)
                else:
                    val = obj[()]
                    # Convert 0-dim numpy scalars to Python scalars if possible
                    try:
                        return val.item()
                    except Exception:
                        return val

            raise TypeError(""Unsupported HDF5 object encountered"")

        with h5py.File(file_name, ""r"") as f:
            if ""data"" not in f:
                # Backward/loose compatibility: read whole file root if not present
                root_group = f
            else:
                root_group = f[""data""]

            contents = _read_node(root_group)
            # If root_group is 'data' with dict marker, contents is a dict of user data
            # If we read from file root, ensure dict type
            if isinstance(contents, dict):
                return cls(data=contents)
            else:
                return cls(data={""data"": contents})"
305605,NASA-AMMOS/AIT-Core,ait/core/tlm.py,core.tlm.PacketContext,"class PacketContext:
    """"""PacketContext

    A PacketContext provides a simple wrapper around a Packet so that
    field accesses of the form:

        packet.fieldname

    may also be specified as:

        packet[fieldname]

    This latter syntax allows a PacketContext to be used as a symbol
    table when evaluating PacketExpressions.
    """"""
    __slots__ = ['_packet']

    def __init__(self, packet):
        """"""Creates a new PacketContext for the given Packet.""""""
        self._packet = packet

    def __getitem__(self, name):
        """"""Returns packet[name]""""""
        result = None
        if self._packet._hasattr(name):
            result = self._packet._getattr(name)
        else:
            msg = ""Packet '%s' has no field '%s'""
            values = (self._packet._defn.name, name)
            raise KeyError(msg % values)
        return result","class PacketContext:
    '''PacketContext
    A PacketContext provides a simple wrapper around a Packet so that
    field accesses of the form:
        packet.fieldname
    may also be specified as:
        packet[fieldname]
    This latter syntax allows a PacketContext to be used as a symbol
    table when evaluating PacketExpressions.
    '''

    def __init__(self, packet):
        '''Creates a new PacketContext for the given Packet.'''
        pass

    def __getitem__(self, name):
        '''Returns packet[name]'''
        pass",snippet_229,"class PacketContext:
    '''PacketContext
    A PacketContext provides a simple wrapper around a Packet so that
    field accesses of the form:
        packet.fieldname
    may also be specified as:
        packet[fieldname]
    This latter syntax allows a PacketContext to be used as a symbol
    table when evaluating PacketExpressions.
    '''

    def __init__(self, packet):
        '''Creates a new PacketContext for the given Packet.'''
        self._packet = packet

    def __getitem__(self, name):
        '''Returns packet[name]'''
        return self._packet[name]"
305610,NASA-AMMOS/AIT-Core,ait/core/tlm.py,core.tlm.RawPacket,"class RawPacket:
    """"""RawPacket

    Wraps a packet such that:

        packet.raw.fieldname

    returns the value of fieldname as a raw value with no enumeration
    substitutions or DN to EU conversions applied.
    """"""
    __slots__ = ['_packet']

    def __init__(self, packet):
        """"""Creates a new RawPacket based on the given Packet.""""""
        self._packet = packet

    def __getattr__(self, fieldname):
        """"""Returns the value of the given packet fieldname as a raw
        value with no DN to EU conversion applied.
        """"""
        return self._packet._getattr(fieldname, raw=True)","class RawPacket:
    '''RawPacket
    Wraps a packet such that:
        packet.raw.fieldname
    returns the value of fieldname as a raw value with no enumeration
    substitutions or DN to EU conversions applied.
    '''

    def __init__(self, packet):
        '''Creates a new RawPacket based on the given Packet.'''
        pass

    def __getattr__(self, fieldname):
        '''Returns the value of the given packet fieldname as a raw
        value with no DN to EU conversion applied.
        '''
        pass",snippet_230,"class RawPacket:
    '''RawPacket
    Wraps a packet such that:
        packet.raw.fieldname
    returns the value of fieldname as a raw value with no enumeration
    substitutions or DN to EU conversions applied.
    '''

    def __init__(self, packet):
        '''Creates a new RawPacket based on the given Packet.'''
        self._packet = packet

    def __getattr__(self, fieldname):
        '''Returns the value of the given packet fieldname as a raw
        value with no DN to EU conversion applied.
        '''
        p = self._packet

        def try_call(fn):
            try:
                return True, fn()
            except Exception:
                return False, None

        attempts = [
            # Common explicit API styles
            lambda: p.get_raw_value(fieldname),
            lambda: p.get_value(fieldname, raw=True),
            lambda: p.get(fieldname, raw=True),
            # Field accessor objects
            lambda: getattr(p, 'get_field')(fieldname).raw,
            lambda: getattr(p, 'get_field')(fieldname).raw_value,
            # Indexing by field name
            lambda: p[fieldname].raw,
            lambda: p[fieldname].raw_value,
            # Field maps
            lambda: p.fields[fieldname].raw,
            lambda: p.fields[fieldname].raw_value,
            # Internal maps
            lambda: p._raw[fieldname],
            lambda: p._raw_values[fieldname],
            lambda: p._values[fieldname]['raw'],
            # Alternate field containers
            lambda: getattr(getattr(p, 'fields_map'), fieldname).raw,
            # Attribute-based field objects
            lambda: getattr(getattr(p, fieldname), 'raw'),
            lambda: getattr(getattr(p, fieldname), 'raw_value'),
            lambda: getattr(getattr(p, fieldname), 'dn'),
        ]

        for fn in attempts:
            ok, val = try_call(fn)
            if ok:
                return val

        # Fallback: delegate to the packet's attribute if it exists
        try:
            return getattr(p, fieldname)
        except AttributeError:
            raise AttributeError(f""No raw value found for field '{fieldname}'"") from None"
305613,NASA-AMMOS/AIT-Core,ait/core/tlm.py,core.tlm.WordArray,"class WordArray:
    """"""WordArrays are somewhat analogous to Python bytearrays, but
    currently much more limited in functionality.  They provide a
    readonly view of a bytearray addressable and iterable as a sequence
    of 16-bit words.  This is convenient for telemetry processing as
    packets are often more naturally addressable on word, as opposed to
    byte, boundaries.
    """"""
    __slots__ = ['_bytes']

    def __init__(self, bytes):
        """"""Creates a new wordarray from the given bytearray.

        The given bytearray should contain an even number of bytes.  If
        odd, the last byte is ignored.
        """"""
        self._bytes = bytes

    def __getitem__(self, key):
        """"""Returns the words in this wordarray at the given Python slice
        or word at the given integer index.""""""
        length = len(self)
        if isinstance(key, slice):
            return [self[n] for n in range(*key.indices(length))]
        elif isinstance(key, int):
            if key < 0:
                key += length
            if key >= length:
                msg = 'wordarray index (%d) is out of range [0 %d].'
                raise IndexError(msg % (key, length - 1))
            index = 2 * key
            return self._bytes[index] << 8 | self._bytes[index + 1]
        else:
            raise TypeError('wordarray indices must be integers.')

    def __len__(self):
        """"""Returns the number of words in this wordarray.""""""
        return len(self._bytes) / 2","class WordArray:
    '''WordArrays are somewhat analogous to Python bytearrays, but
    currently much more limited in functionality.  They provide a
    readonly view of a bytearray addressable and iterable as a sequence
    of 16-bit words.  This is convenient for telemetry processing as
    packets are often more naturally addressable on word, as opposed to
    byte, boundaries.
    '''

    def __init__(self, bytes):
        '''Creates a new wordarray from the given bytearray.
        The given bytearray should contain an even number of bytes.  If
        odd, the last byte is ignored.
        '''
        pass

    def __getitem__(self, key):
        '''Returns the words in this wordarray at the given Python slice
        or word at the given integer index.'''
        pass

    def __len__(self):
        '''Returns the number of words in this wordarray.'''
        pass",snippet_231,"class WordArray:
    '''WordArrays are somewhat analogous to Python bytearrays, but
    currently much more limited in functionality.  They provide a
    readonly view of a bytearray addressable and iterable as a sequence
    of 16-bit words.  This is convenient for telemetry processing as
    packets are often more naturally addressable on word, as opposed to
    byte, boundaries.
    '''

    def __init__(self, bytes_):
        '''Creates a new wordarray from the given bytearray.
        The given bytearray should contain an even number of bytes.  If
        odd, the last byte is ignored.
        '''
        mv = memoryview(bytes_)
        even_len = (len(mv) // 2) * 2
        self._view = mv[:even_len]

    def __getitem__(self, key):
        '''Returns the words in this wordarray at the given Python slice
        or word at the given integer index.'''
        n = len(self)
        if isinstance(key, slice):
            indices = range(n)[key]
            return [self[i] for i in indices]
        # integer index
        idx = key
        if idx < 0:
            idx += n
        if idx < 0 or idx >= n:
            raise IndexError('WordArray index out of range')
        off = idx * 2
        hi = self._view[off]
        lo = self._view[off + 1]
        return (hi << 8) | lo

    def __len__(self):
        '''Returns the number of words in this wordarray.'''
        return len(self._view) // 2"
306020,ihmeuw/vivarium,ihmeuw_vivarium/src/vivarium/framework/lookup/interpolation.py,vivarium.framework.lookup.interpolation.Order0Interp,"import numpy as np
import pandas as pd
from collections.abc import Hashable, Sequence

class Order0Interp:
    """"""A callable that returns the result of order 0 interpolation over input data.

    Attributes
    ----------
    data
        The data from which to build the interpolation.
    value_columns
        Columns to be interpolated.
    extrapolate
        Whether or not to extrapolate beyond the edge of supplied bins.
    parameter_bins
        A dictionary where they keys are a tuple of the form
        (column name used in call, column name for left bin edge, column name for right bin edge)
        and the values are dictionaries of the form {""bins"": [ordered left edges of bins],
        ""max"": max right edge (used when extrapolation not allowed)}.

    """"""

    def __init__(self, data: pd.DataFrame, continuous_parameters: Sequence[Sequence[str]], value_columns: list[str], extrapolate: bool, validate: bool):
        """"""
        Parameters
        ----------
        data
            Data frame used to build interpolation.
        continuous_parameters
            Parameter columns. Should be of form (column name used in call,
            column name for left bin edge, column name for right bin edge)
            or column name. Assumes left bin edges are inclusive and
            right exclusive.
        value_columns
            Columns to be interpolated.
        extrapolate
            Whether or not to extrapolate beyond the edge of supplied bins.
        validate
            Whether or not to validate the data.
        """"""
        if validate:
            check_data_complete(data, continuous_parameters)
        self.data = data.copy()
        self.value_columns = value_columns
        self.extrapolate = extrapolate
        self.parameter_bins = {}
        for p in continuous_parameters:
            left_edge = self.data[p[1]].drop_duplicates().sort_values()
            max_right = self.data[p[2]].drop_duplicates().max()
            self.parameter_bins[tuple(p)] = {'bins': left_edge.reset_index(drop=True), 'max': max_right}

    def __call__(self, interpolants: pd.DataFrame) -> pd.DataFrame:
        """"""Find the bins for each parameter for each interpolant in interpolants
        and return the values from data there.

        Parameters
        ----------
        interpolants
            Data frame containing the parameters to interpolate..

        Returns
        -------
            A table with the interpolated values for the given interpolants.
        """"""
        interpolant_bins = pd.DataFrame(index=interpolants.index)
        merge_cols = []
        for cols, d in self.parameter_bins.items():
            bins = d['bins']
            max_right = d['max']
            merge_cols.append(cols[1])
            interpolant_col = interpolants[cols[0]]
            if not self.extrapolate and (interpolant_col.min() < bins[0] or interpolant_col.max() >= max_right):
                raise ValueError(f'Extrapolation outside of bins used to set up interpolation is only allowed when explicitly set in creation of Interpolation. Extrapolation is currently off for this interpolation, and parameter {cols[0]} includes data outside of original bins.')
            bin_indices = np.digitize(interpolant_col, bins.tolist())
            bin_indices[bin_indices > 0] -= 1
            interpolant_bins[cols[1]] = bins.loc[bin_indices].values
        index = interpolant_bins.index
        interp_vals = interpolant_bins.merge(self.data, how='left', on=merge_cols).set_index(index)
        return interp_vals[self.value_columns]","
class Order0Interp:
    '''A callable that returns the result of order 0 interpolation over input data.
    Attributes
    ----------
    data
        The data from which to build the interpolation.
    value_columns
        Columns to be interpolated.
    extrapolate
        Whether or not to extrapolate beyond the edge of supplied bins.
    parameter_bins
        A dictionary where they keys are a tuple of the form
        (column name used in call, column name for left bin edge, column name for right bin edge)
        and the values are dictionaries of the form {""bins"": [ordered left edges of bins],
        ""max"": max right edge (used when extrapolation not allowed)}.
    '''

    def __init__(self, data: pd.DataFrame, continuous_parameters: Sequence[Sequence[str]], value_columns: list[str], extrapolate: bool, validate: bool):
        '''
        Parameters
        ----------
        data
            Data frame used to build interpolation.
        continuous_parameters
            Parameter columns. Should be of form (column name used in call,
            column name for left bin edge, column name for right bin edge)
            or column name. Assumes left bin edges are inclusive and
            right exclusive.
        value_columns
            Columns to be interpolated.
        extrapolate
            Whether or not to extrapolate beyond the edge of supplied bins.
        validate
            Whether or not to validate the data.
        '''
        pass

    def __call__(self, interpolants: pd.DataFrame) -> pd.DataFrame:
        '''Find the bins for each parameter for each interpolant in interpolants
        and return the values from data there.
        Parameters
        ----------
        interpolants
            Data frame containing the parameters to interpolate..
        Returns
        -------
            A table with the interpolated values for the given interpolants.
        '''
        pass",snippet_232,"import pandas as pd
import numpy as np
from typing import Sequence, Union, Tuple


class Order0Interp:
    '''A callable that returns the result of order 0 interpolation over input data.
    Attributes
    ----------
    data
        The data from which to build the interpolation.
    value_columns
        Columns to be interpolated.
    extrapolate
        Whether or not to extrapolate beyond the edge of supplied bins.
    parameter_bins
        A dictionary where they keys are a tuple of the form
        (column name used in call, column name for left bin edge, column name for right bin edge)
        and the values are dictionaries of the form {""bins"": [ordered left edges of bins],
        ""max"": max right edge (used when extrapolation not allowed)}.
    '''

    def __init__(self, data: pd.DataFrame, continuous_parameters: Sequence[Sequence[str]], value_columns: list[str], extrapolate: bool, validate: bool):
        '''
        Parameters
        ----------
        data
            Data frame used to build interpolation.
        continuous_parameters
            Parameter columns. Should be of form (column name used in call,
            column name for left bin edge, column name for right bin edge)
            or column name. Assumes left bin edges are inclusive and
            right exclusive.
        value_columns
            Columns to be interpolated.
        extrapolate
            Whether or not to extrapolate beyond the edge of supplied bins.
        validate
            Whether or not to validate the data.
        '''
        self.data = data.copy()
        self.value_columns = list(value_columns)
        self.extrapolate = bool(extrapolate)

        # Separate continuous (triplet) and discrete (single name) parameters
        cont_params: list[Tuple[str, str, str]] = []
        disc_params: list[str] = []

        for p in continuous_parameters:
            if isinstance(p, (list, tuple)):
                if len(p) == 3:
                    cont_params.append((p[0], p[1], p[2]))
                elif len(p) == 1:
                    disc_params.append(p[0])
                else:
                    raise ValueError(""Each parameter must be either a single column name or a triplet (call_name, left_edge_col, right_edge_col)."")
            elif isinstance(p, str):
                disc_params.append(p)
            else:
                raise ValueError(""Parameter entries must be sequences of strings or a string."")

        # Build parameter_bins for continuous parameters
        self.parameter_bins: dict[tuple[str, str, str], dict[str, Union[list[float], float]]] = {}
        for call_name, left_col, right_col in cont_params:
            if left_col not in self.data.columns or right_col not in self.data.columns:
                raise KeyError(f""Missing bin edge columns '{left_col}' or '{right_col}' in data for parameter '{call_name}'."")
            left_edges = np.sort(self.data[left_col].dropna().unique())
            max_right = float(np.nanmax(self.data[right_col].values))
            self.parameter_bins[(call_name, left_col, right_col)] = {
                ""bins"": left_edges.tolist(),
                ""max"": max_right,
            }

            if validate:
                if left_edges.size == 0:
                    raise ValueError(f""No left bin edges found for parameter '{call_name}'."")
                if not np.all(np.diff(left_edges) > 0):
                    raise ValueError(f""Left bin edges for parameter '{call_name}' must be strictly increasing."")
                if max_right <= left_edges[-1]:
                    raise ValueError(f""Max right edge for parameter '{call_name}' must be greater than the last left edge."")
                # Ensure at least one row per left edge exists
                counts = self.data[left_col].value_counts()
                for v in left_edges:
                    if counts.get(v, 0) == 0:
                        raise ValueError(f""No data rows for left edge {v} in parameter '{call_name}'."")

        # Store discrete parameters
        self.discrete_parameters: list[str] = []
        for c in disc_params:
            if c not in self.data.columns:
                raise KeyError(f""Discrete parameter column '{c}' not found in data."")
            self.discrete_parameters.append(c)

        # Validate value columns
        missing_values = [c for c in self.value_columns if c not in self.data.columns]
        if missing_values:
            raise KeyError(f""Value columns not found in data: {missing_values}"")

    def __call__(self, interpolants: pd.DataFrame) -> pd.DataFrame:
        '''Find the bins for each parameter for each interpolant in interpolants
        and return the values from data there.
        Parameters
        ----------
        interpolants
            Data frame containing the parameters to interpolate..
        Returns
        -------
            A table with the interpolated values for the given interpolants.
        '''
        if not isinstance(interpolants, pd.DataFrame):
            raise TypeError(""interpolants must be a pandas DataFrame."")

        tmp = interpolants.copy()
        idx_name = tmp.index.name
        tmp = tmp.reset_index(drop=False).rename(columns={tmp.index.name or ""index"": ""__row_id__""}) if idx_name is None else tmp.reset_index().rename(columns={idx_name: ""__row_id__""})

        # Compute bin assignments for continuous parameters
        bin_assign_cols = {}
        for (call_name, left_col, right_col), meta in self.parameter_bins.items():
            if call_name not in tmp.columns:
                raise KeyError(f""Interpolants missing required parameter '{call_name}'."")
            values = tmp[call_name].astype(float).to_numpy()
            bins = np.array(meta[""bins""], dtype=float)
            max_right = float(meta[""max""])

            # searchsorted returns insertion index to maintain order
            # For value in [bins[i], bins[i+1]) we want index i
            idxs = np.searchsorted(bins, values, side=""right"") - 1

            # Handle out-of-range
            low_mask = values < bins[0]
            high_mask = values >= max_right  # right edge exclusive
            if self.extrapolate:
                idxs = np.clip(idxs, 0, len(bins) - 1)
            else:
                idxs[low_mask] = -1
                # For values between last left edge and max_right, idx is valid
                # For values >= max_right, set to -1
                idxs[high_mask] = -1

            # Map to left edge values; -1 -> NaN
            left_assigned = np.where(idxs >= 0, bins[np.clip(idxs, 0, len(bins) - 1)], np.nan)
            assign_col = f""__bin__{left_col}""
            tmp[assign_col] = left_assigned
            bin_assign_cols[left_col] = assign_col

        # Prepare merge keys: continuous left edge columns and discrete columns
        merge_left = []
        merge_right = []
        for left_col, assign_col in bin_assign_cols.items():
            merge_left.append(assign_col)
            merge_right.append(left_col)

        for col in self.discrete_parameters:
            if col not in tmp.columns:
                raise KeyError(f""Interpolants missing required discrete parameter '{col}'."")
            merge_left.append(col)
            merge_right.append(col)

        # If no parameters, just broadcast values
        if not merge_left:
            result = pd.DataFrame(self.data[self.value_columns].iloc[:1].values.repeat(len(tmp), axis=0), columns=self.value_columns)
            result[""__row_id__""] = tmp[""__row_id__""] if ""__row_id__"" in tmp.columns else np.arange(len(tmp))
        else:
            merged = tmp.merge(self.data[self.value_columns + merge_right], left_on=merge_left, right_on=merge_right, how=""left"", copy=False)

            # Preserve row order
            result = merged[[""__row_id__"", *self.value_columns]].copy()

        # Restore original index order
        if ""__row_id__"" in interpolants.reset_index().columns:
            # Unlikely name collision; ensure unique mapping
            # We'll sort by position introduced earlier
            pass

        result = result.set_index(""__row_id__"").sort_index()
        # If original had a named index, keep it; else range
        return result[self.value_columns]"
306151,lago-project/lago,lago-project_lago/lago/templates.py,lago.templates.Template,"class Template:
    """"""
    Disk image template class

    Attributes:
        name (str): Name of this template
        _versions (dict(str:TemplateVersion)): versions for this template
    """"""

    def __init__(self, name, versions):
        """"""
        Args:
            name (str): Name of the template
            versions (dict(str:TemplateVersion)): dictionary with the
                version_name: :class:`TemplateVersion` pairs for this template
        """"""
        self.name = name
        self._versions = versions

    def get_version(self, ver_name=None):
        """"""
        Get the given version for this template, or the latest

        Args:
            ver_name (str or None): Version to retieve, None for the latest

        Returns:
            TemplateVersion: The version matching the given name or the latest
                one
        """"""
        if ver_name is None:
            return self.get_latest_version()
        return self._versions[ver_name]

    def get_latest_version(self):
        """"""
        Retrieves the latest version for this template, the latest being the
        one with the newest timestamp

        Returns:
            TemplateVersion
        """"""
        return max(self._versions.values(), key=lambda x: x.timestamp())","class Template:
    '''
    Disk image template class
    Attributes:
        name (str): Name of this template
        _versions (dict(str:TemplateVersion)): versions for this template
    '''

    def __init__(self, name, versions):
        '''
        Args:
            name (str): Name of the template
            versions (dict(str:TemplateVersion)): dictionary with the
                version_name: :class:`TemplateVersion` pairs for this template
        '''
        pass

    def get_version(self, ver_name=None):
        '''
        Get the given version for this template, or the latest
        Args:
            ver_name (str or None): Version to retieve, None for the latest
        Returns:
            TemplateVersion: The version matching the given name or the latest
                one
        '''
        pass

    def get_latest_version(self):
        '''
        Retrieves the latest version for this template, the latest being the
        one with the newest timestamp
        Returns:
            TemplateVersion
        '''
        pass",snippet_233,"class Template:
    '''
    Disk image template class
    Attributes:
        name (str): Name of this template
        _versions (dict(str:TemplateVersion)): versions for this template
    '''

    def __init__(self, name, versions):
        '''
        Args:
            name (str): Name of the template
            versions (dict(str:TemplateVersion)): dictionary with the
                version_name: :class:`TemplateVersion` pairs for this template
        '''
        self.name = name
        self._versions = dict(versions or {})

    def get_version(self, ver_name=None):
        '''
        Get the given version for this template, or the latest
        Args:
            ver_name (str or None): Version to retieve, None for the latest
        Returns:
            TemplateVersion: The version matching the given name or the latest
                one
        '''
        if ver_name is None:
            return self.get_latest_version()
        return self._versions[ver_name]

    def get_latest_version(self):
        '''
        Retrieves the latest version for this template, the latest being the
        one with the newest timestamp
        Returns:
            TemplateVersion
        '''
        if not self._versions:
            return None

        def _timestamp(ver):
            for attr in (""timestamp"", ""time"", ""created"", ""created_at"", ""date""):
                if hasattr(ver, attr):
                    return getattr(ver, attr)
            return None

        best_ver = None
        best_ts = None
        for ver in self._versions.values():
            ts = _timestamp(ver)
            if best_ver is None:
                best_ver, best_ts = ver, ts
                continue
            if best_ts is None and ts is None:
                continue
            if best_ts is None:
                best_ver, best_ts = ver, ts
                continue
            if ts is None:
                continue
            if ts > best_ts:
                best_ver, best_ts = ver, ts

        return best_ver"
311056,geronimp/graftM,geronimp_graftM/graftm/deduplicator.py,graftm.deduplicator.Deduplicator,"class Deduplicator:
    """"""Deduplicates sequences""""""

    def deduplicate(self, aligned_sequence_objects):
        """"""Sort the given aligned_sequence objects into an array of arrays,
        where input sequences are grouped iff they have the same sequence

        Parameters
        ----------
        aligned_sequence_objects: array of Sequence objects
            input sequences

        Returns
        -------
        Array of arrays of Sequence objects""""""
        sequence_to_groups = {}
        for s in aligned_sequence_objects:
            try:
                sequence_to_groups[s.seq].append(s)
            except KeyError:
                sequence_to_groups[s.seq] = [s]
        return list(sequence_to_groups.values())

    def lca_taxonomy(self, deduplicated_sequences, taxonomy_hash):
        """"""Given a set of deduplicated sequences and a taxonomy hash,
        return the respective LCAs of taxonomy

        Parameters
        ----------
        deduplicated_sequences: Array of arrays of Sequence objects
            as output from deduplicate()
        taxonomy_hash: dictionary 
            of sequence names to taxonomy array (i.e. array of str)

        Returns
        -------
        Array of taxonomy LCAs""""""
        to_return = []
        for dup_group in deduplicated_sequences:
            lca = taxonomy_hash[dup_group[0].name]
            for s in dup_group[1:]:
                for i, tax in enumerate(taxonomy_hash[s.name]):
                    if i >= len(lca) or tax != lca[i]:
                        lca = lca[:i]
                        break
                if len(lca) > len(taxonomy_hash[s.name]):
                    lca = lca[:len(taxonomy_hash[s.name])]
            to_return.append(lca)
        return to_return","class Deduplicator:
    '''Deduplicates sequences'''

    def deduplicate(self, aligned_sequence_objects):
        '''Sort the given aligned_sequence objects into an array of arrays,
        where input sequences are grouped iff they have the same sequence
        Parameters
        ----------
        aligned_sequence_objects: array of Sequence objects
            input sequences
        Returns
        -------
        Array of arrays of Sequence objects'''
        pass

    def lca_taxonomy(self, deduplicated_sequences, taxonomy_hash):
        '''Given a set of deduplicated sequences and a taxonomy hash,
        return the respective LCAs of taxonomy
        Parameters
        ----------
        deduplicated_sequences: Array of arrays of Sequence objects
            as output from deduplicate()
        taxonomy_hash: dictionary 
            of sequence names to taxonomy array (i.e. array of str)
        Returns
        -------
        Array of taxonomy LCAs'''
        pass",snippet_235,"from collections import OrderedDict

class Deduplicator:
    '''Deduplicates sequences'''

    def _get_sequence_str(self, obj):
        # Try common attributes/methods to fetch aligned sequence string
        for attr in ('aligned_sequence', 'sequence', 'seq', 'sequence_string'):
            if hasattr(obj, attr):
                val = getattr(obj, attr)
                if callable(val):
                    try:
                        val = val()
                    except TypeError:
                        pass
                if isinstance(val, str):
                    return val
        # Fallback to str(obj)
        return str(obj)

    def _get_name(self, obj):
        for attr in ('name', 'id', 'identifier', 'header', 'label'):
            if hasattr(obj, attr):
                val = getattr(obj, attr)
                if callable(val):
                    try:
                        val = val()
                    except TypeError:
                        pass
                if isinstance(val, str):
                    return val
        return str(obj)

    def deduplicate(self, aligned_sequence_objects):
        '''Sort the given aligned_sequence objects into an array of arrays,
        where input sequences are grouped iff they have the same sequence
        Parameters
        ----------
        aligned_sequence_objects: array of Sequence objects
            input sequences
        Returns
        -------
        Array of arrays of Sequence objects'''
        groups = OrderedDict()
        for obj in aligned_sequence_objects or []:
            key = self._get_sequence_str(obj)
            groups.setdefault(key, []).append(obj)
        return list(groups.values())

    def lca_taxonomy(self, deduplicated_sequences, taxonomy_hash):
        '''Given a set of deduplicated sequences and a taxonomy hash,
        return the respective LCAs of taxonomy
        Parameters
        ----------
        deduplicated_sequences: Array of arrays of Sequence objects
            as output from deduplicate()
        taxonomy_hash: dictionary 
            of sequence names to taxonomy array (i.e. array of str)
        Returns
        -------
        Array of taxonomy LCAs'''
        if not deduplicated_sequences:
            return []

        def lca_of_arrays(arrays):
            if not arrays:
                return []
            # Start with the first taxonomy, then reduce by common prefix
            lca = list(arrays[0])
            for arr in arrays[1:]:
                max_len = min(len(lca), len(arr))
                i = 0
                while i < max_len and lca[i] == arr[i]:
                    i += 1
                lca = lca[:i]
                if not lca:
                    break
            return lca

        results = []
        for group in deduplicated_sequences:
            tax_arrays = []
            for obj in group:
                name = self._get_name(obj)
                if name in taxonomy_hash:
                    tx = taxonomy_hash.get(name)
                    if isinstance(tx, (list, tuple)):
                        tax_arrays.append(list(tx))
            results.append(lca_of_arrays(tax_arrays))
        return results"
311087,geronimp/graftM,geronimp_graftM/graftm/search_table.py,graftm.search_table.SearchTableWriter,"import logging
from graftm.sequence_search_results import SequenceSearchResult

class SearchTableWriter:
    """"""
    Class for writing the search output OTU table. Basically a summary
    of hits to the HMM/Diamond searched in the following format:

             #ID    Metagenome_1    Metagenome_2    ...
            HMM1    50              6
            HMM2    195             41
            HMM3    2               20120
            ...

    You just need to specify a series of SequenceSearchResult objects, and an
    output path.
    """"""

    def _interpret_hits(self, results_list, base_list):
        """"""Sort reads that hit multiple HMMs to the databases to which they had
        the highest bit score. Return a dictionary containing HMMs as keys, and
        number of hits as the values.

        This function is set up so that the read names could easily be returned
        instead of numbers, for future development of GraftM

        Parameters
        ----------
        results_list: list
            Iterable if SequenceSearchResult objects. e.g.
                [SequenceSearchResult_1, SequenceSearchResult_2, ...]

        base_list: list
            Iterable of the basenames for each sequence file provided to graftM
            e.g.
                [sample_1, sample_2, ...]

        Returns
        -------
        dictionary:
            Contains samples as entries. The value for each sample is another
            dictionary with HMM as the key, and number of hits as values:
                {""sample_1"":{HMM_1: 12
                             HMM_2: 35
                             HMM_3: 1258
                             ...}
                 ...
                }

        """"""
        logging.debug('Sorting reads into HMMs by bit score')
        run_results = {}
        for base, results in zip(base_list, results_list):
            search_results = {}
            for search in results():
                search_list = list(search.each([SequenceSearchResult.QUERY_ID_FIELD, SequenceSearchResult.ALIGNMENT_BIT_SCORE, SequenceSearchResult.HMM_NAME_FIELD]))
                for hit in search_list:
                    if hit[0] in search_results:
                        if float(hit[1]) > search_results[hit[0]][0]:
                            search_results[hit[0]] = [float(hit[1]), hit[2]]
                    else:
                        search_results[hit[0]] = [float(hit[1]), hit[2]]
            run_results[base] = search_results
        db_count = {}
        for run in run_results.keys():
            run_count = {}
            for entry in list(run_results[run].values()):
                key = entry[1]
                if key in run_count:
                    run_count[key] += 1
                else:
                    run_count[key] = 1
            db_count[run] = run_count
        return db_count

    def _write_results(self, db_count, output_path):
        """"""Write the table to the output_path directory

        db_count: dict
            Contains samples as entries. The value for each sample is another
            dictionary with HMM as the key, and number of hits as values:
                {""sample_1"":{HMM_1: 12
                             HMM_2: 35
                             HMM_3: 1258
                             ...}
                 ...
                }

        output_path: str
            Path to output file to which the resultant output file will be
            written to.
        """"""
        logging.debug('Writing search otu table to file: %s' % output_path)
        output_dict = {}
        for idx, value_dict in enumerate(db_count.values()):
            for database, count in value_dict.items():
                if database in output_dict:
                    output_dict[database].append(str(count))
                else:
                    output_dict[database] = ['0'] * idx + [str(count)]
            for key, item in output_dict.items():
                if len(item) == idx:
                    output_dict[key].append('0')
        with open(output_path, 'w') as out:
            out.write('\t'.join(['#ID'] + list(db_count.keys())) + '\n')
            for key, item in output_dict.items():
                out.write('%s\t%s' % (key, '\t'.join(item)) + '\n')

    def build_search_otu_table(self, search_results_list, base_list, output_path):
        """"""
        Build an OTU from SequenceSearchResult objects

        Parameters
        ----------
        search_results_list: list
            Iterable if SequenceSearchResult objects. e.g.
                [SequenceSearchResult_1, SequenceSearchResult_2, ...]
        base_list: list
            Iterable of the basenames for each sequence file provided to graftM
            e.g.
                [sample_1, sample_2, ...]
        output_path: str
            Path to output file to which the resultant output file will be
            written to.
        """"""
        db_count = self._interpret_hits(search_results_list, base_list)
        self._write_results(db_count, output_path)","
class SearchTableWriter:
    '''
    Class for writing the search output OTU table. Basically a summary
    of hits to the HMM/Diamond searched in the following format:
             #ID    Metagenome_1    Metagenome_2    ...
            HMM1    50              6
            HMM2    195             41
            HMM3    2               20120
            ...
    You just need to specify a series of SequenceSearchResult objects, and an
    output path.
    '''

    def _interpret_hits(self, results_list, base_list):
        '''Sort reads that hit multiple HMMs to the databases to which they had
        the highest bit score. Return a dictionary containing HMMs as keys, and
        number of hits as the values.
        This function is set up so that the read names could easily be returned
        instead of numbers, for future development of GraftM
        Parameters
        ----------
        results_list: list
            Iterable if SequenceSearchResult objects. e.g.
                [SequenceSearchResult_1, SequenceSearchResult_2, ...]
        base_list: list
            Iterable of the basenames for each sequence file provided to graftM
            e.g.
                [sample_1, sample_2, ...]
        Returns
        -------
        dictionary:
            Contains samples as entries. The value for each sample is another
            dictionary with HMM as the key, and number of hits as values:
                {""sample_1"":{HMM_1: 12
                             HMM_2: 35
                             HMM_3: 1258
                             ...}
                 ...
                }
        '''
        pass

    def _write_results(self, db_count, output_path):
        '''Write the table to the output_path directory
        db_count: dict
            Contains samples as entries. The value for each sample is another
            dictionary with HMM as the key, and number of hits as values:
                {""sample_1"":{HMM_1: 12
                             HMM_2: 35
                             HMM_3: 1258
                             ...}
                 ...
                }
        output_path: str
            Path to output file to which the resultant output file will be
            written to.
        '''
        pass

    def build_search_otu_table(self, search_results_list, base_list, output_path):
        '''
        Build an OTU from SequenceSearchResult objects
        Parameters
        ----------
        search_results_list: list
            Iterable if SequenceSearchResult objects. e.g.
                [SequenceSearchResult_1, SequenceSearchResult_2, ...]
        base_list: list
            Iterable of the basenames for each sequence file provided to graftM
            e.g.
                [sample_1, sample_2, ...]
        output_path: str
            Path to output file to which the resultant output file will be
            written to.
        '''
        pass",snippet_236,"class SearchTableWriter:
    '''
    Class for writing the search output OTU table. Basically a summary
    of hits to the HMM/Diamond searched in the following format:
             #ID    Metagenome_1    Metagenome_2    ...
            HMM1    50              6
            HMM2    195             41
            HMM3    2               20120
            ...
    You just need to specify a series of SequenceSearchResult objects, and an
    output path.
    '''

    def _interpret_hits(self, results_list, base_list):
        '''Sort reads that hit multiple HMMs to the databases to which they had
        the highest bit score. Return a dictionary containing HMMs as keys, and
        number of hits as the values.
        This function is set up so that the read names could easily be returned
        instead of numbers, for future development of GraftM
        Parameters
        ----------
        results_list: list
            Iterable if SequenceSearchResult objects. e.g.
                [SequenceSearchResult_1, SequenceSearchResult_2, ...]
        base_list: list
            Iterable of the basenames for each sequence file provided to graftM
            e.g.
                [sample_1, sample_2, ...]
        Returns
        -------
        dictionary:
            Contains samples as entries. The value for each sample is another
            dictionary with HMM as the key, and number of hits as values:
                {""sample_1"":{HMM_1: 12
                             HMM_2: 35
                             HMM_3: 1258
                             ...}
                 ...
                }
        '''
        if results_list is None:
            results_list = []
        if base_list is None:
            base_list = []
        if len(results_list) != len(base_list):
            raise ValueError(""results_list and base_list must be the same length"")

        def _get_attr_or_key(obj, names, default=None):
            for n in names:
                if hasattr(obj, n):
                    return getattr(obj, n)
                if isinstance(obj, dict) and n in obj:
                    return obj[n]
            return default

        def _iterate_hits_from_result(result):
            # Try to detect mapping of read -> hits
            if isinstance(result, dict):
                # assume keys are read ids and values are iterable of hit dicts/objects
                for read_id, hits in result.items():
                    if hits is None:
                        continue
                    for h in hits:
                        yield read_id, h
                return

            # named attribute with mapping
            mapping = _get_attr_or_key(result, ['per_read_hits', 'by_read', 'read_hits', 'mapping'], None)
            if isinstance(mapping, dict):
                for read_id, hits in mapping.items():
                    if hits is None:
                        continue
                    for h in hits:
                        yield read_id, h
                return

            # flat iterable of hits
            hits_iter = _get_attr_or_key(result, ['hits', 'results', 'records', 'alignments', 'matches'], None)
            if hits_iter is None and hasattr(result, '__iter__'):
                hits_iter = result

            if hits_iter is not None:
                for h in hits_iter:
                    read_id = _get_attr_or_key(h, ['query', 'read', 'read_id', 'qseqid', 'query_id', 'name', 'qid'])
                    yield read_id, h
                return

            # nothing found
            return

        def _extract_hit_fields(hit):
            hmm = _get_attr_or_key(hit, ['hmm', 'target', 'db', 'subject', 'sseqid', 'target_id', 'hmm_id', 'reference', 'model'])
            score = _get_attr_or_key(hit, ['bit_score', 'bitscore', 'score', 'bitScore', 'bits', 'bitscore_raw'])
            # Try convert score
            try:
                score = float(score) if score is not None else None
            except Exception:
                score = None
            return hmm, score

        db_count = {}
        for sample_name, result in zip(base_list, results_list):
            sample_counts = {}
            # Group by read
            per_read_hits = {}
            for read_id, hit in _iterate_hits_from_result(result):
                if read_id is None:
                    # best effort: treat missing read id as unique per hit object
                    read_id = id(hit)
                hmm, score = _extract_hit_fields(hit)
                if hmm is None:
                    continue
                if score is None:
                    # If no score available, treat as 0 to allow deterministic tie-break
                    score = 0.0
                per_read_hits.setdefault(read_id, {}).setdefault(hmm, -float('inf'))
                if score > per_read_hits[read_id][hmm]:
                    per_read_hits[read_id][hmm] = score

            # Decide best HMM per read
            for read_id, hmm_scores in per_read_hits.items():
                if not hmm_scores:
                    continue
                # pick hmm with max score; tie-breaker by lexicographic order of hmm id
                max_score = max(hmm_scores.values())
                best_hmms = [h for h, s in hmm_scores.items() if s == max_score]
                best_hmm = sorted(best_hmms)[0]
                sample_counts[best_hmm] = sample_counts.get(best_hmm, 0) + 1

            db_count[sample_name] = sample_counts

        return db_count

    def _write_results(self, db_count, output_path):
        '''Write the table to the output_path directory
        db_count: dict
            Contains samples as entries. The value for each sample is another
            dictionary with HMM as the key, and number of hits as values:
                {""sample_1"":{HMM_1: 12
                             HMM_2: 35
                             HMM_3: 1258
                             ...}
                 ...
                }
        output_path: str
            Path to output file to which the resultant output file will be
            written to.
        '''
        if db_count is None:
            db_count = {}

        samples = list(db_count.keys())
        # Gather all HMMs present across samples
        all_hmms = set()
        for counts in db_count.values():
            all_hmms.update(counts.keys())
        sorted_hmms = sorted(all_hmms)

        with open(output_path, 'w', encoding='utf-8') as fh:
            # Header
            fh.write(""#ID"")
            if samples:
                fh.write(""\t"" + ""\t"".join(samples))
            fh.write(""\n"")

            # Rows
            for hmm in sorted_hmms:
                row = [hmm]
                for s in samples:
                    row.append(str(db_count.get(s, {}).get(hmm, 0)))
                fh.write(""\t"".join(row) + ""\n"")

    def build_search_otu_table(self, search_results_list, base_list, output_path):
        '''
        Build an OTU from SequenceSearchResult objects
        Parameters
        ----------
        search_results_list: list
            Iterable if SequenceSearchResult objects. e.g.
                [SequenceSearchResult_1, SequenceSearchResult_2, ...]
        base_list: list
            Iterable of the basenames for each sequence file provided to graftM
            e.g.
                [sample_1, sample_2, ...]
        output_path: str
            Path to output file to which the resultant output file will be
            written to.
        '''
        db_count = self._interpret_hits(search_results_list, base_list)
        self._write_results(db_count, output_path)"
311100,geronimp/graftM,geronimp_graftM/graftm/tree_decorator.py,graftm.tree_decorator.TreeDecorator,"from graftm.greengenes_taxonomy import GreenGenesTaxonomy, MalformedGreenGenesTaxonomyException
from graftm.getaxnseq import Getaxnseq
import logging
from graftm.taxonomy_cleaner import TaxonomyCleaner

class TreeDecorator:
    """"""
    A class that conservatively decorates trees with taxonomy, or any other
    hierarchical annotation. If all tips descending from a node within the
    provided tree have consistent taxonomy, it will be decorated with that
    taxonomy (or annotation of any type).
    """"""

    def __init__(self, tree, taxonomy, seqinfo=None):
        """"""
        Parameters
        ----------
        tree        : dendropy.Tree

            dendropy.Tree object
        taxonomy    : string
            Path to a file containing taxonomy information about the tree,
            either in Greengenes or taxtastic format (seqinfo file must also
            be provided if taxonomy is in taxtastic format).
        seqinfo     : string
            Path to a seqinfo file. This is a .csv file with the first column
            denoting the sequence name, and the second column, its most resolved
            taxonomic rank.
        """"""
        self.encountered_nodes = {}
        self.encountered_taxonomies = set()
        self.tree = tree
        logging.info('Reading in taxonomy')
        if seqinfo:
            logging.info('Importing taxtastic taxonomy from files: %s and %s' % (taxonomy, seqinfo))
            gtns = Getaxnseq()
            self.taxonomy = gtns.read_taxtastic_taxonomy_and_seqinfo(open(taxonomy), open(seqinfo))
        else:
            try:
                logging.info('Reading Greengenes style taxonomy')
                self.taxonomy = GreenGenesTaxonomy.read_file(taxonomy).taxonomy
            except MalformedGreenGenesTaxonomyException:
                raise Exception('Failed to read taxonomy as a Greengenes                                  formatted file. Was a taxtastic style                                  taxonomy provided with no seqinfo file?')

    def _write_consensus_strings(self, output):
        """"""
        Writes the taxonomy of each leaf to a file. If the leaf has no
        taxonomy, a taxonomy string will be created using the annotations
        provided to the ancestor nodes of that leaf (meaning, it will be
        decorated).

        Parameters
        ----------
        output    : string
            File to which the taxonomy strings for each leaf in the tree will
            be written in Greengenes format, e.g.
                637960147    mcrA; Euryarchaeota_mcrA; Methanomicrobia
                637699780    mcrA; Euryarchaeota_mcrA; Methanomicrobia
        """"""
        logging.info('Writing decorated taxonomy to file: %s' % output)
        with open(output, 'w') as out:
            for tip in self.tree.leaf_nodes():
                tax_name = tip.taxon.label.replace(' ', '_')
                if tip.taxon.label in self.taxonomy:
                    tax_string = '; '.join(self.taxonomy[tax_name])
                else:
                    ancestor_list = []
                    for ancestor in tip.ancestor_iter():
                        if ancestor.label:
                            split_node_name = ancestor.label.split(':')
                            if len(split_node_name) == 2:
                                ancestor_list += list(reversed(split_node_name[1].split('; ')))
                            elif len(split_node_name) == 1:
                                try:
                                    float(split_node_name[0])
                                except ValueError:
                                    ancestor_list += list(reversed(split_node_name[0].split('; ')))
                            else:
                                raise Exception('Malformed node name: %s' % ancestor.label)
                    tax_list = list(reversed(ancestor_list))
                    if len(tax_list) < 1:
                        logging.warning('No taxonomy found for species %s!' % tax_name)
                        tax_string = 'Unknown'
                    else:
                        tax_string = '; '.join(tax_list)
                output_line = '%s\t%s\n' % (tax_name, tax_string)
                out.write(output_line)

    def _rename(self, node, name):
        """"""
        Rename an internal node of the tree. If an annotation is already
        present, append the new annotation to the end of it. If a bootstrap
        value is present, add annotations are added after a "":"" as per standard
        newick format.

        Parameters
        ----------
        node: dendropy.Node
            dendropy.Node object
        name    : string
            Annotation to rename the node with.
        """"""
        if node.label:
            try:
                float(node.label)
                new_label = '%s:%s' % (node.label, name)
            except ValueError:
                new_label = '%s; %s' % (node.label, name)
            node.label = new_label
        else:
            node.label = name

    def decorate(self, output_tree, output_tax, unique_names):
        """"""
        Decorate a tree with taxonomy. This code does not allow inconsistent
        taxonomy within a clade. If one sequence in a clade has a different
        annotation to the rest, it will split the clade. Paraphyletic group
        names are distinguished if unique_names = True using a simple tally of
        each group (see unique_names below).

        Parameters
        ----------
        output_tree        : string
            File to which the decorated tree will be written.
        output_tax         : string
            File to which the taxonomy strings for each tip in the tree will be
            written.
        unique_names       : boolean
            True indicating that a unique number will be appended to the end of
            a taxonomic rank if it is found more than once in the tree
            (i.e. it is paraphyletic in the tree). If false, multiple clades
            may be assigned with the same name.
        """"""
        logging.info('Decorating tree')
        encountered_taxonomies = {}
        tc = TaxonomyCleaner()
        for node in self.tree.preorder_internal_node_iter(exclude_seed_node=True):
            max_tax_string_length = 0
            for tip in node.leaf_nodes():
                tip_label = tip.taxon.label.replace(' ', '_')
                if tip_label in self.taxonomy:
                    tax_string_length = len(self.taxonomy[tip.taxon.label.replace(' ', '_')])
                    if tax_string_length > max_tax_string_length:
                        max_tax_string_length = tax_string_length
            logging.debug('Number of ranks found for node: %i' % max_tax_string_length)
            tax_string_array = []
            for rank in range(max_tax_string_length):
                rank_tax = []
                for tip in node.leaf_nodes():
                    tip_label = tip.taxon.label.replace(' ', '_')
                    if tip_label in self.taxonomy:
                        tip_tax = self.taxonomy[tip_label]
                        if len(tip_tax) > rank:
                            tip_rank = tip_tax[rank]
                            if tip_rank not in rank_tax:
                                rank_tax.append(tip_rank)
                consistent_taxonomy = len(rank_tax) == 1
                if consistent_taxonomy:
                    tax = rank_tax.pop()
                    logging.debug('Consistent taxonomy found for node: %s' % tax)
                    if tax not in tc.meaningless_taxonomic_names:
                        if unique_names:
                            if tax in encountered_taxonomies:
                                encountered_taxonomies[tax] += 0
                                tax = '%s_%i' % (tax, encountered_taxonomies[tax])
                            else:
                                encountered_taxonomies[tax] = 0
                        tax_string_array.append(tax)
            if any(tax_string_array):
                index = 0
                for anc in node.ancestor_iter():
                    try:
                        index += anc.tax
                    except:
                        continue
                tax_string_array = tax_string_array[index:]
                if any(tax_string_array):
                    self._rename(node, '; '.join(tax_string_array))
                node.tax = len(tax_string_array)
        logging.info('Writing decorated tree to file: %s' % output_tree)
        if output_tree:
            self.tree.write(path=output_tree, schema='newick')
        if output_tax:
            self._write_consensus_strings(output_tax)","
class TreeDecorator:
    '''
    A class that conservatively decorates trees with taxonomy, or any other
    hierarchical annotation. If all tips descending from a node within the
    provided tree have consistent taxonomy, it will be decorated with that
    taxonomy (or annotation of any type).
    '''

    def __init__(self, tree, taxonomy, seqinfo=None):
        '''
        Parameters
        ----------
        tree        : dendropy.Tree
            dendropy.Tree object
        taxonomy    : string
            Path to a file containing taxonomy information about the tree,
            either in Greengenes or taxtastic format (seqinfo file must also
            be provided if taxonomy is in taxtastic format).
        seqinfo     : string
            Path to a seqinfo file. This is a .csv file with the first column
            denoting the sequence name, and the second column, its most resolved
            taxonomic rank.
        '''
        pass

    def _write_consensus_strings(self, output):
        '''
        Writes the taxonomy of each leaf to a file. If the leaf has no
        taxonomy, a taxonomy string will be created using the annotations
        provided to the ancestor nodes of that leaf (meaning, it will be
        decorated).
        Parameters
        ----------
        output    : string
            File to which the taxonomy strings for each leaf in the tree will
            be written in Greengenes format, e.g.
                637960147    mcrA; Euryarchaeota_mcrA; Methanomicrobia
                637699780    mcrA; Euryarchaeota_mcrA; Methanomicrobia
        '''
        pass

    def _rename(self, node, name):
        '''
        Rename an internal node of the tree. If an annotation is already
        present, append the new annotation to the end of it. If a bootstrap
        value is present, add annotations are added after a "":"" as per standard
        newick format.
        Parameters
        ----------
        node: dendropy.Node
            dendropy.Node object
        name    : string
            Annotation to rename the node with.
        '''
        pass

    def decorate(self, output_tree, output_tax, unique_names):
        '''
        Decorate a tree with taxonomy. This code does not allow inconsistent
        taxonomy within a clade. If one sequence in a clade has a different
        annotation to the rest, it will split the clade. Paraphyletic group
        names are distinguished if unique_names = True using a simple tally of
        each group (see unique_names below).
        Parameters
        ----------
        output_tree        : string
            File to which the decorated tree will be written.
        output_tax         : string
            File to which the taxonomy strings for each tip in the tree will be
            written.
        unique_names       : boolean
            True indicating that a unique number will be appended to the end of
            a taxonomic rank if it is found more than once in the tree
            (i.e. it is paraphyletic in the tree). If false, multiple clades
            may be assigned with the same name.
        '''
        pass",snippet_237,"class TreeDecorator:
    '''
    A class that conservatively decorates trees with taxonomy, or any other
    hierarchical annotation. If all tips descending from a node within the
    provided tree have consistent taxonomy, it will be decorated with that
    taxonomy (or annotation of any type).
    '''

    def __init__(self, tree, taxonomy, seqinfo=None):
        '''
        Parameters
        ----------
        tree        : dendropy.Tree
            dendropy.Tree object
        taxonomy    : string
            Path to a file containing taxonomy information about the tree,
            either in Greengenes or taxtastic format (seqinfo file must also
            be provided if taxonomy is in taxtastic format).
        seqinfo     : string
            Path to a seqinfo file. This is a .csv file with the first column
            denoting the sequence name, and the second column, its most resolved
            taxonomic rank.
        '''
        self.tree = tree
        self.seqinfo_path = seqinfo
        self.taxonomy_path = taxonomy
        self.taxonomy_map = self._load_taxonomy_file(taxonomy)

    def _load_taxonomy_file(self, path):
        tx = {}
        if path is None:
            return tx
        import csv
        # Try TSV first, then CSV
        with open(path, ""r"", newline="""") as fh:
            sample = fh.read(4096)
            fh.seek(0)
            dialect = None
            try:
                dialect = csv.Sniffer().sniff(sample, delimiters=""\t,;"")
            except Exception:
                pass
            if dialect is not None:
                reader = csv.reader(fh, dialect)
            else:
                reader = (line.rstrip(""\n"") for line in fh)
                reader = (l.split(""\t"") if ""\t"" in l else l.split("","") for l in reader)
            for row in reader:
                if not row:
                    continue
                if isinstance(row, str):
                    row = [r for r in row.split(""\t"") if r] or [r for r in row.split("","") if r]
                # Expect at least two columns: id and taxonomy string
                if len(row) == 1:
                    # Try to split on whitespace between id and taxonomy
                    parts = row[0].strip().split(None, 1)
                    if len(parts) == 2:
                        key, val = parts
                    else:
                        # Skip malformed lines
                        continue
                else:
                    key = row[0].strip()
                    val = row[1].strip()
                if not key:
                    continue
                # Normalize taxonomy separators to ""; "" and strip parts
                parts = [p.strip() for p in self._split_tax_string(val)]
                if parts:
                    tx[key] = parts
        return tx

    def _split_tax_string(self, s):
        if s is None:
            return []
        # Accept separators like ';', '; ', '|', or ',' (but avoid splitting on commas within names if quoted)
        raw = [seg for seg in s.replace(""|"", "";"").split("";"")]
        cleaned = []
        for seg in raw:
            seg = seg.strip()
            if not seg:
                continue
            cleaned.append(seg)
        return cleaned

    def _tip_name(self, node):
        # Try common dendropy attributes to get the tip name
        name = None
        try:
            if getattr(node, ""taxon"", None) is not None:
                name = getattr(node.taxon, ""label"", None)
        except Exception:
            name = None
        if not name:
            name = getattr(node, ""label"", None)
        if not name:
            name = getattr(node, ""leaf_label"", None)
        return name

    def _is_leaf(self, node):
        try:
            return node.is_leaf()
        except Exception:
            # Fallback: no children means leaf
            ch = getattr(node, ""child_nodes"", None)
            if callable(ch):
                return len(ch()) == 0
            children = getattr(node, ""child_nodes"", []) or getattr(node, ""children"", [])
            try:
                return len(children) == 0
            except Exception:
                return False

    def _iter_leaves(self):
        # DendroPy provides leaf_node_iter
        try:
            yield from self.tree.leaf_node_iter()
            return
        except Exception:
            pass
        try:
            for n in self.tree.leaf_nodes():
                yield n
            return
        except Exception:
            pass
        # Fallback: traverse all nodes and yield leaves
        yield from (n for n in self._postorder_nodes() if self._is_leaf(n))

    def _postorder_nodes(self):
        try:
            yield from self.tree.postorder_node_iter()
            return
        except Exception:
            pass
        # Fallback: recursive traversal from seed/root
        root = getattr(self.tree, ""seed_node"", None) or getattr(self.tree, ""root"", None)
        if root is None:
            return
        yield from self._postorder_from(root)

    def _children(self, node):
        ch_meth = getattr(node, ""child_nodes"", None)
        if callable(ch_meth):
            return list(ch_meth())
        children = getattr(node, ""child_nodes"", None)
        if children is None:
            children = getattr(node, ""children"", [])
        return list(children) if children is not None else []

    def _parent(self, node):
        return getattr(node, ""parent_node"", None) or getattr(node, ""parent"", None)

    def _postorder_from(self, node):
        for c in self._children(node):
            yield from self._postorder_from(c)
        yield node

    def _write_consensus_strings(self, output):
        '''
        Writes the taxonomy of each leaf to a file. If the leaf has no
        taxonomy, a taxonomy string will be created using the annotations
        provided to the ancestor nodes of that leaf (meaning, it will be
        decorated).
        Parameters
        ----------
        output    : string
            File to which the taxonomy strings for each leaf in the tree will
            be written in Greengenes format, e.g.
                637960147    mcrA; Euryarchaeota_mcrA; Methanomicrobia
                637699780    mcrA; Euryarchaeota_mcrA; Methanomicrobia
        '''
        with open(output, ""w"") as out:
            for leaf in self._iter_leaves():
                name = self._tip_name(leaf) or """"
                tparts = self.taxonomy_map.get(name, None)
                if not tparts:
                    # Build from ancestor annotations
                    tparts = self._taxonomy_from_ancestors(leaf)
                tx_str = ""; "".join(tparts) if tparts else """"
                out.write(f""{name}\t{tx_str}\n"")

    def _taxonomy_from_ancestors(self, leaf):
        # Walk from root towards leaf collecting annotation labels
        parts = []
        seen = set()
        # Build path to root
        chain = []
        n = self._parent(leaf)
        while n is not None:
            chain.append(n)
            n = self._parent(n)
        for node in reversed(chain):
            lbl = getattr(node, ""label"", None)
            if not lbl:
                continue
            ann = self._extract_annotation_from_label(lbl)
            if not ann:
                continue
            if ann not in seen:
                parts.append(ann)
                seen.add(ann)
        return parts

    def _extract_annotation_from_label(self, label):
        # If bootstrap-like numeric with annotation after "":"", take the suffix
        if "":"" in label:
            # assume everything after the first "":"" is annotation; trim spaces
            candidate = label.split("":"", 1)[1].strip()
            if candidate:
                return candidate
        # Otherwise, if label contains semicolons, take last segment as annotation name
        if "";"" in label:
            segs = [s.strip() for s in label.split("";"") if s.strip()]
            if segs:
                return segs[-1]
        # If the label is purely numeric, treat as bootstrap only
        import re
        if re.fullmatch(r""\s*-?\d+(\.\d+)?\s*"", label):
            return """"
        return label.strip()

    def _rename(self, node, name):
        '''
        Rename an internal node of the tree. If an annotation is already
        present, append the new annotation to the end of it. If a bootstrap
        value is present, add annotations are added after a "":"" as per standard
        newick format.
        Parameters
        ----------
        node: dendropy.Node
            dendropy.Node object
        name    : string
            Annotation to rename the node with.
        '''
        current = getattr(node, ""label"", None)
        if not current:
            node.label = name
            return
        import re
        if re.fullmatch(r""\s*-?\d+(\.\d+)?\s*"", current):
            node.label = f""{current}:{name}""
            return
        # If already has ""bootstrap:annotation"" form, append ""; name"" to annotation part
        if "":"" in current:
            left, right = current.split("":"", 1)
            right = right.strip()
            if right:
                node.label = f""{left}:{right}; {name}""
            else:
                node.label = f""{left}:{name}""
            return
        # Otherwise append with semicolon
        node.label = f""{current}; {name}""

    def decorate(self, output_tree, output_tax, unique_names):
        '''
        Decorate a tree with taxonomy. This code does not allow inconsistent
        taxonomy within a clade. If one sequence in a clade has a different
        annotation to the rest, it will split the clade. Paraphyletic group
        names are distinguished if unique_names = True using a simple tally of
        each group (see unique_names below).
        Parameters
        ----------
        output_tree        : string
            File to which the decorated tree will be written.
        output_tax         : string
            File to which the taxonomy strings for each tip in the tree will be
            written.
        unique_names       : boolean
            True indicating that a unique number will be appended to the end of
            a taxonomic rank if it is found more than once in the tree
            (i.e. it is paraphyletic in the tree). If false, multiple clades
            may be assigned with the same name.
        '''
        # Preload leaf taxonomy lists
        leaf_tax_lists = {}
        for leaf in self._iter_leaves():
            nm = self._tip_name(leaf)
            parts = self.taxonomy_map.get(nm, [])
            leaf_tax_lists[leaf] = parts

        # Helper to get descendant leaf taxonomy lists for a node
        def descendant_tax_lists(node):
            # collect leaves below node
            stack = [node]
            lists = []
            while stack:
                cur = stack.pop()
                if self._is_leaf(cur):
                    lists.append(leaf_tax_lists.get(cur, []))
                else:
                    stack.extend(self._children(cur))
            return lists

        def common_prefix(list_of_lists):
            if not list_of_lists:
                return []
            # Filter out empty lists
            nonempty = [lst for lst in list_of_lists if lst]
            if not nonempty:
                return []
            # Compute shared prefix across all lists
            prefix = []
            idx = 0
            while True:
                # Check that all lists have idx and equal element
                try:
                    cand = nonempty[0][idx]
                except IndexError:
                    break
                ok = True
                for lst in nonempty[1:]:
                    if len(lst) <= idx or lst[idx] != cand:
                        ok = False
                        break
                if not ok:
                    break
                prefix.append(cand)
                idx += 1
            return prefix

        name_counts = {}
        # Postorder so children evaluated before parents
        for node in self._postorder_nodes():
            if self._is_leaf(node):
                continue
            lists = descendant_tax_lists(node)
            prefix = common_prefix(lists)
            if not prefix:
                continue
            base = prefix[-1]
            if unique_names:
                cnt = name_counts.get(base, 0) + 1
                name_counts[base] = cnt
                name = f""{base}"" if cnt == 1 else f""{base}_{cnt}""
            else:
                name = base
            self._rename(node, name)

        # Write outputs
        # Tree
        wrote = False
        try:
            # DendroPy-style
            self.tree.write(path=output_tree, schema=""newick"")
            wrote = True
        except Exception:
            try:
                with open(output_tree, ""w"") as fh:
                    fh.write(str(self.tree))
                wrote = True
            except Exception:
                wrote = False
        # Taxonomy strings per leaf
        self._write_consensus_strings(output_tax)
        return wrote"
311641,abhishek-ram/pyas2-lib,abhishek-ram_pyas2-lib/pyas2lib/as2.py,pyas2lib.as2.Organization,"from pyas2lib.constants import AS2_VERSION, ASYNCHRONOUS_MDN, DIGEST_ALGORITHMS, EDIINT_FEATURES, ENCRYPTION_ALGORITHMS, KEY_ENCRYPTION_ALGORITHMS, MDN_CONFIRM_TEXT, MDN_FAILED_TEXT, MDN_MODES, SIGNATUR_ALGORITHMS, SYNCHRONOUS_MDN
from pyas2lib.utils import canonicalize, extract_first_part, make_mime_boundary, mime_to_bytes, pem_to_der, quote_as2name, split_pem, unquote_as2name, verify_certificate_chain
from pyas2lib.exceptions import AS2Exception, DuplicateDocument, ImproperlyConfigured, InsufficientSecurityError, IntegrityError, MDNNotFound, PartnerNotFound
from oscrypto import asymmetric
from dataclasses import dataclass

@dataclass
class Organization:
    """"""
    Class represents an AS2 organization and defines the certificates and
    settings to be used when sending and receiving messages.

    :param as2_name: The unique AS2 name for this organization

    :param sign_key: A byte string of the pkcs12 encoded key pair
        used for signing outbound messages and MDNs.

    :param sign_key_pass: The password for decrypting the `sign_key`

    :param decrypt_key:  A byte string of the pkcs12 encoded key pair
        used for decrypting inbound messages.

    :param decrypt_key_pass: The password for decrypting the `decrypt_key`

    :param mdn_url: The URL where the receiver is expected to post
        asynchronous MDNs.

    :param domain:
        Optional domain if given provides the portion of the message id
        after the '@'.  It defaults to the locally defined hostname.
    """"""
    as2_name: str
    sign_key: bytes = None
    sign_key_pass: str = None
    decrypt_key: bytes = None
    decrypt_key_pass: str = None
    mdn_url: str = None
    mdn_confirm_text: str = MDN_CONFIRM_TEXT
    domain: str = None

    def __post_init__(self):
        """"""Run the post initialisation checks for this class.""""""
        if self.sign_key:
            self.sign_key = self.load_key(self.sign_key, self.sign_key_pass)
        if self.decrypt_key:
            self.decrypt_key = self.load_key(self.decrypt_key, self.decrypt_key_pass)

    @staticmethod
    def load_key(key_str: bytes, key_pass: str):
        """"""Function to load password protected key file in p12 or pem format.""""""
        try:
            key, cert, _ = asymmetric.load_pkcs12(key_str, key_pass)
        except ValueError as e:
            if e.args[0] == 'Password provided is invalid':
                raise AS2Exception('Password not valid for Private Key.') from e
            key, cert = (None, None)
            for kc in split_pem(key_str):
                try:
                    cert = asymmetric.load_certificate(kc)
                except (ValueError, TypeError) as e:
                    try:
                        key = asymmetric.load_private_key(kc, key_pass)
                    except OSError:
                        raise AS2Exception('Invalid Private Key or password is not correct.') from e
        if not key or not cert:
            raise AS2Exception('Invalid Private key file or Public key not included.')
        return (key, cert)",,snippet_238,"I don’t have the class skeleton or requirements. Please provide:
- The class name and full skeleton (methods, attributes, docstrings)
- Expected behavior and edge cases
- Any interfaces/protocols to follow
- Example usage or unit tests
- Constraints (Python version, dependencies, performance expectations)

I’ll implement it accordingly without explanations."
311642,abhishek-ram/pyas2-lib,abhishek-ram_pyas2-lib/pyas2lib/as2.py,pyas2lib.as2.Partner,"from oscrypto import asymmetric
from pyas2lib.exceptions import AS2Exception, DuplicateDocument, ImproperlyConfigured, InsufficientSecurityError, IntegrityError, MDNNotFound, PartnerNotFound
from pyas2lib.utils import canonicalize, extract_first_part, make_mime_boundary, mime_to_bytes, pem_to_der, quote_as2name, split_pem, unquote_as2name, verify_certificate_chain
from pyas2lib.constants import AS2_VERSION, ASYNCHRONOUS_MDN, DIGEST_ALGORITHMS, EDIINT_FEATURES, ENCRYPTION_ALGORITHMS, KEY_ENCRYPTION_ALGORITHMS, MDN_CONFIRM_TEXT, MDN_FAILED_TEXT, MDN_MODES, SIGNATUR_ALGORITHMS, SYNCHRONOUS_MDN
from dataclasses import dataclass

@dataclass
class Partner:
    """"""
    Class represents an AS2 partner and defines the certificates and
    settings to be used when sending and receiving messages.

    :param as2_name: The unique AS2 name for this partner.

    :param verify_cert: A byte string of the certificate to be used for
        verifying signatures of inbound messages and MDNs.

    :param verify_cert_ca: A byte string of the ca certificate if any of
        the verification cert

    :param encrypt_cert: A byte string of the certificate to be used for
        encrypting outbound message.

    :param encrypt_cert_ca: A byte string of the ca certificate if any of
        the encryption cert

    :param validate_certs: Set this flag to `False` to disable validations of
        the encryption and verification certificates. (default `True`)

    :param compress: Set this flag to `True` to compress outgoing
        messages. (default `False`)

    :param sign: Set this flag to `True` to sign outgoing
        messages. (default `False`)

    :param digest_alg: The digest algorithm to be used for generating the
        signature. (default ""sha256"")

    :param encrypt: Set this flag to `True` to encrypt outgoing
        messages. (default `False`)

    :param enc_alg:
        The encryption algorithm to be used. (default `""tripledes_192_cbc""`)

    :param mdn_mode: The mode to be used for receiving the MDN.
        Set to `None` for no MDN, `'SYNC'` for synchronous and `'ASYNC'`
        for asynchronous. (default `None`)

    :param mdn_digest_alg: The digest algorithm to be used by the receiver
        for signing the MDN. Use `None` for unsigned MDN. (default `None`)

    :param mdn_confirm_text: The text to be used in the MDN for successfully
        processed messages received from this partner.

    :param canonicalize_as_binary: force binary canonicalization for this partner

    :param sign_alg: The signing algorithm to be used for generating the
        signature. (default `rsassa_pkcs1v15`)

    :param key_enc_alg: The key encryption algorithm to be used.
        (default `rsaes_pkcs1v15`)

    """"""
    as2_name: str
    verify_cert: bytes = None
    verify_cert_ca: bytes = None
    encrypt_cert: bytes = None
    encrypt_cert_ca: bytes = None
    validate_certs: bool = True
    compress: bool = False
    encrypt: bool = False
    enc_alg: str = 'tripledes_192_cbc'
    sign: bool = False
    digest_alg: str = 'sha256'
    mdn_mode: str = None
    mdn_digest_alg: str = None
    mdn_confirm_text: str = MDN_CONFIRM_TEXT
    ignore_self_signed: bool = True
    canonicalize_as_binary: bool = False
    sign_alg: str = 'rsassa_pkcs1v15'
    key_enc_alg: str = 'rsaes_pkcs1v15'

    def __post_init__(self):
        """"""Run the post initialisation checks for this class.""""""
        if self.digest_alg and self.digest_alg not in DIGEST_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported Digest Algorithm {self.digest_alg}, must be one of {DIGEST_ALGORITHMS}')
        if self.enc_alg and self.enc_alg not in ENCRYPTION_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported Encryption Algorithm {self.enc_alg}, must be one of {ENCRYPTION_ALGORITHMS}')
        if self.mdn_mode and self.mdn_mode not in MDN_MODES:
            raise ImproperlyConfigured(f'Unsupported MDN Mode {self.mdn_mode}, must be one of {MDN_MODES}')
        if self.mdn_digest_alg and self.mdn_digest_alg not in DIGEST_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported MDN Digest Algorithm {self.mdn_digest_alg}, must be one of {DIGEST_ALGORITHMS}')
        if self.sign_alg and self.sign_alg not in SIGNATUR_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported Signature Algorithm {self.sign_alg}, must be one of {SIGNATUR_ALGORITHMS}')
        if self.key_enc_alg and self.key_enc_alg not in KEY_ENCRYPTION_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported Key Encryption Algorithm {self.key_enc_alg}, must be one of {KEY_ENCRYPTION_ALGORITHMS}')

    def load_verify_cert(self):
        """"""Load the verification certificate of the partner and returned the parsed cert.""""""
        if self.validate_certs:
            cert = pem_to_der(self.verify_cert, return_multiple=False)
            if self.verify_cert_ca:
                trust_roots = pem_to_der(self.verify_cert_ca)
            else:
                trust_roots = []
            verify_certificate_chain(cert, trust_roots, ignore_self_signed=self.ignore_self_signed)
        return asymmetric.load_certificate(self.verify_cert)

    def load_encrypt_cert(self):
        """"""Load the encryption certificate of the partner and returned the parsed cert.""""""
        if self.validate_certs:
            cert = pem_to_der(self.encrypt_cert, return_multiple=False)
            if self.encrypt_cert_ca:
                trust_roots = pem_to_der(self.encrypt_cert_ca)
            else:
                trust_roots = []
            verify_certificate_chain(cert, trust_roots, ignore_self_signed=self.ignore_self_signed)
        return asymmetric.load_certificate(self.encrypt_cert)","@dataclass
class Partner:
    '''
    Class represents an AS2 partner and defines the certificates and
    settings to be used when sending and receiving messages.
    :param as2_name: The unique AS2 name for this partner.
    :param verify_cert: A byte string of the certificate to be used for
        verifying signatures of inbound messages and MDNs.
    :param verify_cert_ca: A byte string of the ca certificate if any of
        the verification cert
    :param encrypt_cert: A byte string of the certificate to be used for
        encrypting outbound message.
    :param encrypt_cert_ca: A byte string of the ca certificate if any of
        the encryption cert
    :param validate_certs: Set this flag to `False` to disable validations of
        the encryption and verification certificates. (default `True`)
    :param compress: Set this flag to `True` to compress outgoing
        messages. (default `False`)
    :param sign: Set this flag to `True` to sign outgoing
        messages. (default `False`)
    :param digest_alg: The digest algorithm to be used for generating the
        signature. (default ""sha256"")
    :param encrypt: Set this flag to `True` to encrypt outgoing
        messages. (default `False`)
    :param enc_alg:
        The encryption algorithm to be used. (default `""tripledes_192_cbc""`)
    :param mdn_mode: The mode to be used for receiving the MDN.
        Set to `None` for no MDN, `'SYNC'` for synchronous and `'ASYNC'`
        for asynchronous. (default `None`)
    :param mdn_digest_alg: The digest algorithm to be used by the receiver
        for signing the MDN. Use `None` for unsigned MDN. (default `None`)
    :param mdn_confirm_text: The text to be used in the MDN for successfully
        processed messages received from this partner.
    :param canonicalize_as_binary: force binary canonicalization for this partner
    :param sign_alg: The signing algorithm to be used for generating the
        signature. (default `rsassa_pkcs1v15`)
    :param key_enc_alg: The key encryption algorithm to be used.
        (default `rsaes_pkcs1v15`)
    '''

    def __post_init__(self):
        '''Run the post initialisation checks for this class.'''
        pass

    def load_verify_cert(self):
        '''Load the verification certificate of the partner and returned the parsed cert.'''
        pass

    def load_encrypt_cert(self):
        '''Load the encryption certificate of the partner and returned the parsed cert.'''
        pass",snippet_239,"from dataclasses import dataclass, field
from typing import Optional
from datetime import datetime, timezone

try:
    from cryptography import x509
    from cryptography.hazmat.primitives import hashes
except Exception as e:  # pragma: no cover
    x509 = None  # type: ignore


@dataclass
class Partner:
    '''
    Class represents an AS2 partner and defines the certificates and
    settings to be used when sending and receiving messages.
    :param as2_name: The unique AS2 name for this partner.
    :param verify_cert: A byte string of the certificate to be used for
        verifying signatures of inbound messages and MDNs.
    :param verify_cert_ca: A byte string of the ca certificate if any of
        the verification cert
    :param encrypt_cert: A byte string of the certificate to be used for
        encrypting outbound message.
    :param encrypt_cert_ca: A byte string of the ca certificate if any of
        the encryption cert
    :param validate_certs: Set this flag to `False` to disable validations of
        the encryption and verification certificates. (default `True`)
    :param compress: Set this flag to `True` to compress outgoing
        messages. (default `False`)
    :param sign: Set this flag to `True` to sign outgoing
        messages. (default `False`)
    :param digest_alg: The digest algorithm to be used for generating the
        signature. (default ""sha256"")
    :param encrypt: Set this flag to `True` to encrypt outgoing
        messages. (default `False`)
    :param enc_alg:
        The encryption algorithm to be used. (default `""tripledes_192_cbc""`)
    :param mdn_mode: The mode to be used for receiving the MDN.
        Set to `None` for no MDN, `'SYNC'` for synchronous and `'ASYNC'`
        for asynchronous. (default `None`)
    :param mdn_digest_alg: The digest algorithm to be used by the receiver
        for signing the MDN. Use `None` for unsigned MDN. (default `None`)
    :param mdn_confirm_text: The text to be used in the MDN for successfully
        processed messages received from this partner.
    :param canonicalize_as_binary: force binary canonicalization for this partner
    :param sign_alg: The signing algorithm to be used for generating the
        signature. (default `rsassa_pkcs1v15`)
    :param key_enc_alg: The key encryption algorithm to be used.
        (default `rsaes_pkcs1v15`)
    '''

    as2_name: str
    verify_cert: Optional[bytes] = None
    verify_cert_ca: Optional[bytes] = None
    encrypt_cert: Optional[bytes] = None
    encrypt_cert_ca: Optional[bytes] = None

    validate_certs: bool = True

    compress: bool = False
    sign: bool = False
    digest_alg: str = ""sha256""

    encrypt: bool = False
    enc_alg: str = ""tripledes_192_cbc""

    mdn_mode: Optional[str] = None
    mdn_digest_alg: Optional[str] = None
    mdn_confirm_text: Optional[str] = None

    canonicalize_as_binary: bool = False

    sign_alg: str = ""rsassa_pkcs1v15""
    key_enc_alg: str = ""rsaes_pkcs1v15""

    _allowed_digests: set = field(init=False, repr=False, default_factory=lambda: {""sha1"", ""sha224"", ""sha256"", ""sha384"", ""sha512""})
    _allowed_enc_algs: set = field(init=False, repr=False, default_factory=lambda: {""aes_128_cbc"", ""aes_192_cbc"", ""aes_256_cbc"", ""tripledes_192_cbc""})
    _allowed_sign_algs: set = field(init=False, repr=False, default_factory=lambda: {""rsassa_pkcs1v15"", ""rsassa_pss""})
    _allowed_key_enc_algs: set = field(init=False, repr=False, default_factory=lambda: {""rsaes_pkcs1v15"", ""rsaes_oaep""})
    _allowed_mdn_modes: set = field(init=False, repr=False, default_factory=lambda: {None, ""SYNC"", ""ASYNC""})

    def __post_init__(self):
        if not isinstance(self.as2_name, str) or not self.as2_name.strip():
            raise ValueError(""as2_name must be a non-empty string."")

        if self.mdn_mode is not None:
            self.mdn_mode = self.mdn_mode.upper()
        if self.mdn_mode not in self._allowed_mdn_modes:
            raise ValueError(""mdn_mode must be None, 'SYNC', or 'ASYNC'."")

        if self.digest_alg not in self._allowed_digests:
            raise ValueError(f""digest_alg must be one of {sorted(self._allowed_digests)}."")

        if self.mdn_digest_alg is not None and self.mdn_digest_alg not in self._allowed_digests:
            raise ValueError(f""mdn_digest_alg must be one of {sorted(self._allowed_digests)} or None."")

        if self.enc_alg not in self._allowed_enc_algs:
            raise ValueError(f""enc_alg must be one of {sorted(self._allowed_enc_algs)}."")

        if self.sign_alg not in self._allowed_sign_algs:
            raise ValueError(f""sign_alg must be one of {sorted(self._allowed_sign_algs)}."")

        if self.key_enc_alg not in self._allowed_key_enc_algs:
            raise ValueError(f""key_enc_alg must be one of {sorted(self._allowed_key_enc_algs)}."")

        if self.encrypt and not self.encrypt_cert:
            raise ValueError(""encrypt is True but no encrypt_cert was provided."")

        if self.validate_certs:
            # Validate cert availability vs. expected usage
            if self.verify_cert is not None:
                self._validate_cert_bytes(self.verify_cert, ""verify_cert"")
            if self.encrypt_cert is not None:
                self._validate_cert_bytes(self.encrypt_cert, ""encrypt_cert"")
            if self.verify_cert_ca is not None:
                self._validate_cert_bytes(self.verify_cert_ca, ""verify_cert_ca"")
            if self.encrypt_cert_ca is not None:
                self._validate_cert_bytes(self.encrypt_cert_ca, ""encrypt_cert_ca"")

            # Basic chain relationship checks if both cert and CA are provided
            if self.verify_cert and self.verify_cert_ca:
                leaf = self._parse_cert(self.verify_cert)
                ca = self._parse_cert(self.verify_cert_ca)
                if leaf.issuer.rfc4514_string() != ca.subject.rfc4514_string():
                    # Not strictly required to match (it could be intermediate), but this is a minimal sanity check
                    pass
                self._check_cert_validity(leaf)
                self._check_cert_validity(ca)
            elif self.verify_cert:
                self._check_cert_validity(self._parse_cert(self.verify_cert))

            if self.encrypt_cert and self.encrypt_cert_ca:
                leaf = self._parse_cert(self.encrypt_cert)
                ca = self._parse_cert(self.encrypt_cert_ca)
                if leaf.issuer.rfc4514_string() != ca.subject.rfc4514_string():
                    pass
                self._check_cert_validity(leaf)
                self._check_cert_validity(ca)
            elif self.encrypt_cert:
                self._check_cert_validity(self._parse_cert(self.encrypt_cert))

    def load_verify_cert(self):
        '''Load the verification certificate of the partner and returned the parsed cert.'''
        if self.verify_cert is None:
            return None
        cert = self._parse_cert(self.verify_cert)
        if self.validate_certs:
            self._check_cert_validity(cert)
        return cert

    def load_encrypt_cert(self):
        '''Load the encryption certificate of the partner and returned the parsed cert.'''
        if self.encrypt_cert is None:
            return None
        cert = self._parse_cert(self.encrypt_cert)
        if self.validate_certs:
            self._check_cert_validity(cert)
        return cert

    def _parse_cert(self, data: bytes):
        if x509 is None:
            raise RuntimeError(""cryptography package is required to parse certificates."")
        if not isinstance(data, (bytes, bytearray)):
            raise TypeError(""Certificate data must be bytes."")
        blob = bytes(data)
        # Heuristic: PEM contains BEGIN
        try:
            if b""-----BEGIN"" in blob:
                return x509.load_pem_x509_certificate(blob)
            else:
                return x509.load_der_x509_certificate(blob)
        except Exception as e:
            raise ValueError(""Invalid certificate data."") from e

    def _check_cert_validity(self, cert):
        now = datetime.now(timezone.utc)
        # cryptography returns naive datetimes in older versions; normalize to aware UTC
        not_before = cert.not_valid_before
        not_after = cert.not_valid_after
        if not_before.tzinfo is None:
            not_before = not_before.replace(tzinfo=timezone.utc)
        if not_after.tzinfo is None:
            not_after = not_after.replace(tzinfo=timezone.utc)
        if now < not_before:
            raise ValueError(""Certificate is not yet valid."")
        if now > not_after:
            raise ValueError(""Certificate has expired."")

    def _validate_cert_bytes(self, data: bytes, field_name: str):
        if data is None:
            return
        if not isinstance(data, (bytes, bytearray)):
            raise TypeError(f""{field_name} must be bytes."")
        # Quick parse check
        self._parse_cert(bytes(data))"
311862,reportportal/client-Python,reportportal_client-Python/reportportal_client/core/rp_issues.py,reportportal_client.core.rp_issues.ExternalIssue,"class ExternalIssue:
    """"""This class represents external(BTS) system issue.""""""

    def __init__(self, bts_url=None, bts_project=None, submit_date=None, ticket_id=None, url=None):
        """"""Initialize instance attributes.

        :param bts_url:     Bug tracker system URL
        :param bts_project: Bug tracker system project
        :param submit_date: Bug submission date
        :param ticket_id:   Unique ID of the ticket at the BTS
        :param url:         URL to the ticket(bug)
        """"""
        self.bts_url = bts_url
        self.bts_project = bts_project
        self.submit_date = submit_date
        self.ticket_id = ticket_id
        self.url = url

    @property
    def payload(self):
        """"""Form the correct dictionary for the BTS issue.""""""
        return {'btsUrl': self.bts_url, 'btsProject': self.bts_project, 'submitDate': self.submit_date, 'ticketId': self.ticket_id, 'url': self.url}","class ExternalIssue:
    '''This class represents external(BTS) system issue.'''

    def __init__(self, bts_url=None, bts_project=None, submit_date=None, ticket_id=None, url=None):
        '''Initialize instance attributes.
        :param bts_url:     Bug tracker system URL
        :param bts_project: Bug tracker system project
        :param submit_date: Bug submission date
        :param ticket_id:   Unique ID of the ticket at the BTS
        :param url:         URL to the ticket(bug)
        '''
        pass
    @property
    def payload(self):
        '''Form the correct dictionary for the BTS issue.'''
        pass",snippet_240,"class ExternalIssue:
    '''This class represents external(BTS) system issue.'''

    def __init__(self, bts_url=None, bts_project=None, submit_date=None, ticket_id=None, url=None):
        '''Initialize instance attributes.
        :param bts_url:     Bug tracker system URL
        :param bts_project: Bug tracker system project
        :param submit_date: Bug submission date
        :param ticket_id:   Unique ID of the ticket at the BTS
        :param url:         URL to the ticket(bug)
        '''
        self.bts_url = bts_url
        self.bts_project = bts_project
        self.submit_date = submit_date
        self.ticket_id = ticket_id
        self.url = url

    @property
    def payload(self):
        '''Form the correct dictionary for the BTS issue.'''
        from datetime import date, datetime

        def _isoformat_date(value):
            if value is None:
                return None
            if isinstance(value, (datetime, date)):
                return value.isoformat()
            return str(value)

        data = {}
        if self.bts_url is not None:
            data['bts_url'] = self.bts_url
        if self.bts_project is not None:
            data['bts_project'] = self.bts_project
        if self.submit_date is not None:
            iso_date = _isoformat_date(self.submit_date)
            if iso_date is not None:
                data['submit_date'] = iso_date
        if self.ticket_id is not None:
            data['ticket_id'] = str(self.ticket_id)
        if self.url is not None:
            data['url'] = self.url
        return dict(data)"
311863,reportportal/client-Python,reportportal_client-Python/reportportal_client/core/rp_issues.py,reportportal_client.core.rp_issues.Issue,"class Issue:
    """"""This class represents an issue that can be attached to test result.""""""

    def __init__(self, issue_type, comment=None, auto_analyzed=False, ignore_analyzer=True):
        """"""Initialize instance attributes.

        :param issue_type:      Issue type locator. Allowable values: ""pb***"",
                                ""ab***"", ""si***"", ""ti***"", ""nd001"". Where ***
                                is locator id.
        :param comment:         Issue comments
        :param auto_analyzed:   Indicator that the issue has been marked with
                                the RP auto analyzer
        :param ignore_analyzer: Flag that forces RP analyzer to ignore this
                                issue
        """"""
        self._external_issues = []
        self.auto_analyzed = auto_analyzed
        self.comment = comment
        self.ignore_analyzer = ignore_analyzer
        self.issue_type = issue_type

    def external_issue_add(self, issue):
        """"""Add external system issue to the issue.""""""
        self._external_issues.append(issue.payload)

    @property
    def payload(self):
        """"""Form the correct dictionary for the issue.""""""
        return {'autoAnalyzed': self.auto_analyzed, 'comment': self.comment, 'externalSystemIssues': self._external_issues, 'ignoreAnalyzer': self.ignore_analyzer, 'issueType': self.issue_type}","class Issue:
    '''This class represents an issue that can be attached to test result.'''

    def __init__(self, issue_type, comment=None, auto_analyzed=False, ignore_analyzer=True):
        '''Initialize instance attributes.
        :param issue_type:      Issue type locator. Allowable values: ""pb***"",
                                ""ab***"", ""si***"", ""ti***"", ""nd001"". Where ***
                                is locator id.
        :param comment:         Issue comments
        :param auto_analyzed:   Indicator that the issue has been marked with
                                the RP auto analyzer
        :param ignore_analyzer: Flag that forces RP analyzer to ignore this
                                issue
        '''
        pass

    def external_issue_add(self, issue):
        '''Add external system issue to the issue.'''
        pass
    @property
    def payload(self):
        '''Form the correct dictionary for the issue.'''
        pass",snippet_241,"class Issue:
    '''This class represents an issue that can be attached to test result.'''

    _ALLOWED_ISSUE_TYPE_PREFIXES = {""pb"", ""ab"", ""si"", ""ti""}
    _EXTERNAL_ISSUE_FIELDS = {""submitter"", ""systemId"", ""ticketId"", ""url"", ""btsProject"", ""btsUrl""}

    def __init__(self, issue_type, comment=None, auto_analyzed=False, ignore_analyzer=True):
        '''Initialize instance attributes.
        :param issue_type:      Issue type locator. Allowable values: ""pb***"",
                                ""ab***"", ""si***"", ""ti***"", ""nd001"". Where ***
                                is locator id.
        :param comment:         Issue comments
        :param auto_analyzed:   Indicator that the issue has been marked with
                                the RP auto analyzer
        :param ignore_analyzer: Flag that forces RP analyzer to ignore this
                                issue
        '''
        if not isinstance(issue_type, str):
            raise TypeError(""issue_type must be a string"")

        issue_type_lc = issue_type.strip().lower()
        if issue_type_lc == ""nd001"":
            validated_issue_type = ""nd001""
        else:
            if len(issue_type_lc) != 5:
                raise ValueError('issue_type must have format ""<prefix><3 digits>"", e.g., ""pb001"" or be ""nd001""')
            prefix, digits = issue_type_lc[:2], issue_type_lc[2:]
            if prefix not in self._ALLOWED_ISSUE_TYPE_PREFIXES or not digits.isdigit():
                raise ValueError('issue_type must start with one of {""pb"",""ab"",""si"",""ti""} followed by 3 digits, or be ""nd001""')
            validated_issue_type = issue_type_lc

        self._issue_type = validated_issue_type
        self._comment = None if comment is None else str(comment)
        self._auto_analyzed = bool(auto_analyzed)
        self._ignore_analyzer = bool(ignore_analyzer)
        self._external_system_issues = []

    def external_issue_add(self, issue):
        '''Add external system issue to the issue.'''
        if not isinstance(issue, dict):
            raise TypeError(""issue must be a dict"")

        # Keep only known fields and coerce to str, drop empty values
        sanitized = {}
        for k in self._EXTERNAL_ISSUE_FIELDS:
            if k in issue and issue[k] is not None:
                val = str(issue[k]).strip()
                if val:
                    sanitized[k] = val

        if not sanitized:
            raise ValueError(""External issue must contain at least one supported non-empty field"")

        self._external_system_issues.append(sanitized)

    @property
    def payload(self):
        '''Form the correct dictionary for the issue.'''
        data = {
            ""issueType"": self._issue_type,
            ""autoAnalyzed"": self._auto_analyzed,
            ""ignoreAnalyzer"": self._ignore_analyzer,
        }
        if self._comment is not None:
            data[""comment""] = self._comment
        if self._external_system_issues:
            data[""externalSystemIssues""] = list(self._external_system_issues)
        return data"
312030,SuperCowPowers/workbench,aws_setup/aws_identity_check.py,aws_identity_check.AWSIdentityCheck,"from workbench.core.cloud_platform.aws.aws_account_clamp import AWSAccountClamp
from workbench.utils.config_manager import ConfigManager
import sys
import logging

class AWSIdentityCheck:
    """"""Just a Utility Script that allows people to check which AWS Identity is active""""""

    def __init__(self):
        """"""AWSIdentityCheck Initialization""""""
        self.log = logging.getLogger('workbench')
        self.aws_clamp = AWSAccountClamp()

    def check(self):
        """"""Check the AWS Identity""""""
        cm = ConfigManager()
        active_profile = cm.get_config('AWS_PROFILE')
        if active_profile:
            self.log.info(f'Workbench AWS_PROFILE: {active_profile}')
        else:
            self.log.info('No AWS_PROFILE set')
            sys.exit(0)
        self.log.info('\n\n*** Caller/Base Identity Check ***')
        self.aws_clamp.check_aws_identity()
        self.log.info('Caller/Base Identity Check Success...')
        self.log.info('\n\n*** AWS Assumed Role Check ***')
        self.aws_clamp.check_assumed_role()
        self.log.info('Assumed Role Check Success...')","
class AWSIdentityCheck:
    '''Just a Utility Script that allows people to check which AWS Identity is active'''

    def __init__(self):
        '''AWSIdentityCheck Initialization'''
        pass

    def check(self):
        '''Check the AWS Identity'''
        pass",snippet_242,"class AWSIdentityCheck:
    '''Just a Utility Script that allows people to check which AWS Identity is active'''

    def __init__(self):
        '''AWSIdentityCheck Initialization'''
        # Lazy import to avoid hard dependency at import time
        try:
            import boto3  # noqa: F401
            import botocore  # noqa: F401
        except Exception as e:
            raise RuntimeError(""boto3 and botocore are required to use AWSIdentityCheck"") from e

        self._boto3 = boto3
        self._botocore = botocore

        # Capture environment hints
        import os
        self._env = {
            ""AWS_PROFILE"": os.environ.get(""AWS_PROFILE""),
            ""AWS_DEFAULT_REGION"": os.environ.get(""AWS_DEFAULT_REGION""),
            ""AWS_REGION"": os.environ.get(""AWS_REGION""),
            ""AWS_ROLE_ARN"": os.environ.get(""AWS_ROLE_ARN""),
            ""AWS_ACCESS_KEY_ID"": os.environ.get(""AWS_ACCESS_KEY_ID""),
        }

        # Create a session using the environment or default configuration
        profile = self._env[""AWS_PROFILE""]
        try:
            if profile:
                self._session = self._boto3.Session(profile_name=profile)
            else:
                self._session = self._boto3.Session()
        except Exception as e:
            # If profile is set but invalid, surface a clear error
            raise RuntimeError(f""Failed to create boto3 session (profile={profile!r}): {e}"") from e

    def _parse_arn(self, arn):
        # arn:partition:service:region:account-id:resource
        out = {
            ""partition"": None,
            ""service"": None,
            ""region"": None,
            ""account"": None,
            ""resource"": None,
            ""principal_type"": None,
            ""principal_name"": None,
        }
        try:
            parts = arn.split("":"", 5)
            if len(parts) >= 6:
                _, partition, service, region, account, resource = parts
                out[""partition""] = partition or None
                out[""service""] = service or None
                out[""region""] = region or None
                out[""account""] = account or None
                out[""resource""] = resource or None

                # resource often looks like: user/NAME or role/NAME or root
                res = resource or """"
                if res == ""root"":
                    out[""principal_type""] = ""root""
                    out[""principal_name""] = ""root""
                else:
                    segs = res.split(""/"", 1)
                    if segs:
                        out[""principal_type""] = segs[0]
                        out[""principal_name""] = segs[1] if len(segs) > 1 else None
        except Exception:
            pass
        return out

    def _get_account_aliases(self):
        try:
            iam = self._session.client(""iam"")
            aliases = []
            marker = None
            while True:
                if marker:
                    resp = iam.list_account_aliases(Marker=marker)
                else:
                    resp = iam.list_account_aliases()
                aliases.extend(resp.get(""AccountAliases"", []))
                if resp.get(""IsTruncated""):
                    marker = resp.get(""Marker"")
                else:
                    break
            return aliases
        except self._botocore.exceptions.ClientError as e:
            # Access denied or iam disabled is fine; return empty
            return []
        except Exception:
            return []

    def check(self):
        '''Check the AWS Identity'''
        sts = self._session.client(""sts"")
        region = (
            self._env[""AWS_REGION""]
            or self._env[""AWS_DEFAULT_REGION""]
            or self._session.region_name
        )

        # Determine credential source/method if possible
        cred_method = None
        try:
            creds = self._session.get_credentials()
            if creds is not None:
                # botocore credentials expose .method (e.g., env, shared-credentials-file, assume-role, sso, iam-role, container-role)
                cred_method = getattr(creds, ""method"", None)
        except Exception:
            pass

        result = {
            ""ok"": False,
            ""error"": None,
            ""account"": None,
            ""arn"": None,
            ""user_id"": None,
            ""principal_type"": None,
            ""principal_name"": None,
            ""partition"": None,
            ""profile"": self._env[""AWS_PROFILE""] or self._session.profile_name,
            ""region"": region,
            ""credential_method"": cred_method,
            ""account_aliases"": [],
            ""environment"": {
                ""AWS_PROFILE"": self._env[""AWS_PROFILE""],
                ""AWS_REGION"": self._env[""AWS_REGION""],
                ""AWS_DEFAULT_REGION"": self._env[""AWS_DEFAULT_REGION""],
                ""AWS_ROLE_ARN"": self._env[""AWS_ROLE_ARN""],
                ""AWS_ACCESS_KEY_ID_present"": bool(self._env[""AWS_ACCESS_KEY_ID""]),
            },
        }

        try:
            identity = sts.get_caller_identity()
            result[""ok""] = True
            result[""account""] = identity.get(""Account"")
            result[""arn""] = identity.get(""Arn"")
            result[""user_id""] = identity.get(""UserId"")

            arn_meta = self._parse_arn(result[""arn""] or """")
            result[""principal_type""] = arn_meta.get(""principal_type"")
            result[""principal_name""] = arn_meta.get(""principal_name"")
            result[""partition""] = arn_meta.get(""partition"")

            result[""account_aliases""] = self._get_account_aliases()
            return result
        except self._botocore.exceptions.NoCredentialsError as e:
            result[""error""] = ""No AWS credentials found""
            return result
        except self._botocore.exceptions.ClientError as e:
            result[""error""] = f""ClientError: {e.response.get('Error', {}).get('Code')}: {e.response.get('Error', {}).get('Message')}""
            return result
        except self._botocore.exceptions.BotoCoreError as e:
            result[""error""] = f""BotoCoreError: {str(e)}""
            return result
        except Exception as e:
            result[""error""] = f""UnexpectedError: {str(e)}""
            return result"
312949,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/factory/source/element.py,apidoc.factory.source.element.Element,"from apidoc.lib.util.cast import to_boolean
import collections
from apidoc.object.source_raw import Sampleable, Displayable

class Element:
    """""" Populate Helper Factory
    """"""

    def set_common_datas(self, element, name, datas):
        """"""Populated common data for an element from dictionnary datas
        """"""
        element.name = str(name)
        if 'description' in datas:
            element.description = str(datas['description']).strip()
        if isinstance(element, Sampleable) and element.sample is None and ('sample' in datas):
            element.sample = str(datas['sample']).strip()
        if isinstance(element, Displayable):
            if 'display' in datas:
                element.display = to_boolean(datas['display'])
            if 'label' in datas:
                element.label = datas['label']
            else:
                element.label = element.name

    def create_dictionary_of_element_from_dictionary(self, property_name, datas):
        """"""Populate a dictionary of elements
        """"""
        response = {}
        if property_name in datas and datas[property_name] is not None and isinstance(datas[property_name], collections.Iterable):
            for key, value in datas[property_name].items():
                response[key] = self.create_from_name_and_dictionary(key, value)
        return response

    def create_list_of_element_from_dictionary(self, property_name, datas):
        """"""Populate a list of elements
        """"""
        response = []
        if property_name in datas and datas[property_name] is not None and isinstance(datas[property_name], list):
            for value in datas[property_name]:
                response.append(self.create_from_dictionary(value))
        return response

    def get_enum(self, property, enum, datas):
        """"""Factory enum type
        """"""
        str_property = str(datas[property]).lower()
        if str_property not in enum:
            raise ValueError('Unknow enum ""%s"" for ""%s"".' % (str_property, property))
        return enum(str_property)","
class Element:
    ''' Populate Helper Factory
    '''

    def set_common_datas(self, element, name, datas):
        '''Populated common data for an element from dictionnary datas
        '''
        pass

    def create_dictionary_of_element_from_dictionary(self, property_name, datas):
        '''Populate a dictionary of elements
        '''
        pass

    def create_list_of_element_from_dictionary(self, property_name, datas):
        '''Populate a list of elements
        '''
        pass

    def get_enum(self, property, enum, datas):
        '''Factory enum type
        '''
        pass",snippet_244,"class Element:
    ''' Populate Helper Factory
    '''

    def set_common_datas(self, element, name, datas):
        '''Populated common data for an element from dictionnary datas
        '''
        setattr(element, ""name"", name)
        if isinstance(datas, dict):
            for k, v in datas.items():
                try:
                    setattr(element, k, v)
                except Exception:
                    # Ignore attributes that cannot be set
                    pass
        return element

    def create_dictionary_of_element_from_dictionary(self, property_name, datas):
        '''Populate a dictionary of elements
        '''
        if not isinstance(datas, dict):
            return {}
        value = datas.get(property_name)
        if value is None:
            return {}
        if isinstance(value, dict):
            return dict(value)
        if isinstance(value, (list, tuple)):
            result = {}
            for item in value:
                if isinstance(item, dict) and ""key"" in item and ""value"" in item:
                    result[item[""key""]] = item[""value""]
                elif isinstance(item, (list, tuple)) and len(item) == 2:
                    k, v = item
                    result[k] = v
            return result
        return {}

    def create_list_of_element_from_dictionary(self, property_name, datas):
        '''Populate a list of elements
        '''
        if not isinstance(datas, dict):
            return []
        value = datas.get(property_name)
        if value is None:
            return []
        if isinstance(value, list):
            return list(value)
        if isinstance(value, (tuple, set)):
            return list(value)
        if isinstance(value, dict):
            return list(value.values())
        return [value]

    def get_enum(self, property, enum, datas):
        '''Factory enum type
        '''
        if not isinstance(datas, dict):
            return None
        val = datas.get(property)
        if val is None:
            return None

        try:
            from enum import Enum
        except Exception:
            Enum = None  # Fallback if enum is unavailable

        # If enum is already the correct instance
        if Enum is not None and isinstance(val, enum if isinstance(enum, type) else type(val)):
            return val

        # Enum class handling
        if Enum is not None and isinstance(enum, type) and issubclass(enum, Enum):
            # Direct match
            if isinstance(val, enum):
                return val
            # Try by name
            if isinstance(val, str):
                try:
                    return enum[val]
                except KeyError:
                    try:
                        return enum[val.upper()]
                    except Exception:
                        pass
                    # Case-insensitive name match
                    name_map = {m.name.lower(): m for m in enum}
                    found = name_map.get(val.lower())
                    if found is not None:
                        return found
                # Try by value when values are strings
                for member in enum:
                    if str(member.value) == str(val):
                        return member
            # Try by value (int/str)
            try:
                return enum(val)
            except Exception:
                pass
            return None

        # Mapping/dict fallback
        if isinstance(enum, dict):
            if val in enum:
                return enum[val]
            if isinstance(val, str):
                # case-insensitive key lookup
                for k in enum:
                    if isinstance(k, str) and k.lower() == val.lower():
                        return enum[k]
            return None

        # Callable factory fallback
        if callable(enum):
            try:
                return enum(val)
            except Exception:
                return None

        return None"
313013,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/object/source_raw.py,apidoc.object.source_raw.Sampleable,"class Sampleable:
    """"""Element who can provide samples
    """"""

    def __init__(self):
        """"""Class instantiation
        """"""
        super().__init__()
        self.sample = None

    def get_sample(self):
        """"""Return the a sample for the element
        """"""
        if self.sample is None:
            return self.get_default_sample()
        return self.sample

    def get_default_sample(self):
        """"""Return default value for the element
        """"""
        return 'my_%s' % self.name","class Sampleable:
    '''Element who can provide samples
    '''

    def __init__(self):
        '''Class instantiation
        '''
        pass

    def get_sample(self):
        '''Return the a sample for the element
        '''
        pass

    def get_default_sample(self):
        '''Return default value for the element
        '''
        pass",snippet_245,"class Sampleable:
    '''Element who can provide samples
    '''

    def __init__(self, sample=None, default=None, sampler=None):
        '''Class instantiation
        '''
        self._sample = sample
        self._default_sample = default
        self._sampler = sampler

    def get_sample(self):
        '''Return the a sample for the element
        '''
        if callable(self._sampler):
            return self._sampler()
        if self._sample is not None:
            return self._sample
        return self.get_default_sample()

    def get_default_sample(self):
        '''Return default value for the element
        '''
        return self._default_sample"
313027,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/service/config.py,apidoc.service.config.Config,"import os
from apidoc.object.config import Config as ConfigObject

class Config:
    """"""Provide tool to managed config
    """"""

    def validate(self, config):
        """"""Validate that the source file is ok
        """"""
        if not isinstance(config, ConfigObject):
            raise Exception('Config object expected')
        if config['output']['componants'] not in ('local', 'remote', 'embedded', 'without'):
            raise ValueError('Unknown componant ""%s"".' % config['output']['componants'])
        if config['output']['layout'] not in ('default', 'content-only'):
            raise ValueError('Unknown layout ""%s"".' % config['output']['layout'])
        if config['input']['locations'] is not None:
            unknown_locations = [x for x in config['input']['locations'] if not os.path.exists(x)]
            if len(unknown_locations) > 0:
                raise ValueError('Location%s ""%s"" does not exists' % ('s' if len(unknown_locations) > 1 else '', '"" and ""'.join(unknown_locations)))
            config['input']['locations'] = [os.path.realpath(x) for x in config['input']['locations']]
        if config['input']['arguments'] is not None:
            if not isinstance(config['input']['arguments'], dict):
                raise ValueError('Sources arguments ""%s"" are not a dict' % config['input']['arguments'])

    def get_template_from_config(self, config):
        """"""Retrieve a template path from the config object
        """"""
        if config['output']['template'] == 'default':
            return os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'template', 'default.html')
        else:
            return os.path.abspath(config['output']['template'])","
class Config:
    '''Provide tool to managed config
    '''

    def validate(self, config):
        '''Validate that the source file is ok
        '''
        pass

    def get_template_from_config(self, config):
        '''Retrieve a template path from the config object
        '''
        pass",snippet_246,"from pathlib import Path
from collections.abc import Mapping

class Config:
    '''Provide tool to managed config
    '''

    _SOURCE_KEYS = (""source"", ""src"", ""source_file"", ""sourceFile"")
    _TEMPLATE_KEYS = (""template"", ""template_path"", ""templateFile"", ""template_file"")

    def validate(self, config):
        '''Validate that the source file is ok
        '''
        cfg = self._ensure_mapping_like(config)
        value = self._get_any(cfg, self._SOURCE_KEYS)
        if value is None:
            raise KeyError(f""Missing source key. Expected one of: {', '.join(self._SOURCE_KEYS)}"")

        path = self._coerce_path(value)
        if not path.exists():
            raise FileNotFoundError(f""Source file not found: {path}"")
        if not path.is_file():
            raise IsADirectoryError(f""Source path is not a file: {path}"")
        return True

    def get_template_from_config(self, config):
        '''Retrieve a template path from the config object
        '''
        cfg = self._ensure_mapping_like(config)
        value = self._get_any(cfg, self._TEMPLATE_KEYS)
        if value is None:
            raise KeyError(f""Missing template key. Expected one of: {', '.join(self._TEMPLATE_KEYS)}"")

        path = self._coerce_path(value)
        if not path.exists():
            raise FileNotFoundError(f""Template file not found: {path}"")
        if not path.is_file():
            raise IsADirectoryError(f""Template path is not a file: {path}"")
        return str(path)

    def _ensure_mapping_like(self, config):
        if isinstance(config, Mapping):
            return config
        # Allow simple objects with attributes
        class AttrMapping(dict):
            def __init__(self, obj):
                super().__init__()
                for k in dir(obj):
                    if k.startswith(""_""):
                        continue
                    try:
                        v = getattr(obj, k)
                    except Exception:
                        continue
                    if not callable(v):
                        self[k] = v
        try:
            return AttrMapping(config)
        except Exception as e:
            raise TypeError(""Config must be a mapping or an object with attributes."") from e

    def _get_any(self, cfg, keys):
        for k in keys:
            if k in cfg and cfg[k] not in (None, """"):
                return cfg[k]
        return None

    def _coerce_path(self, value):
        if isinstance(value, Path):
            return value.expanduser().resolve(strict=False)
        if isinstance(value, (str, bytes)):
            return Path(value).expanduser().resolve(strict=False)
        raise TypeError(f""Path value must be str, bytes, or Path, got {type(value).__name__}"")"
313029,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/service/merger.py,apidoc.service.merger.Merger,"from apidoc.lib.util.cast import to_boolean

class Merger:
    """"""Provide tool to merge elements
    """"""

    def merge_extends(self, target, extends, inherit_key='inherit', inherit=False):
        """"""Merge extended dicts
        """"""
        if isinstance(target, dict):
            if inherit and inherit_key in target and (not to_boolean(target[inherit_key])):
                return
            if not isinstance(extends, dict):
                raise ValueError('Unable to merge: Dictionnary expected')
            for key in extends:
                if key not in target:
                    target[str(key)] = extends[key]
                else:
                    self.merge_extends(target[key], extends[key], inherit_key, True)
        elif isinstance(target, list):
            if not isinstance(extends, list):
                raise ValueError('Unable to merge: List expected')
            target += extends

    def merge_sources(self, datas):
        """"""Merge sources files
        """"""
        datas = [data for data in datas if data is not None]
        if len(datas) == 0:
            raise ValueError('Data missing')
        if len(datas) == 1:
            return datas[0]
        if isinstance(datas[0], list):
            if len([x for x in datas if not isinstance(x, list)]) > 0:
                raise TypeError('Unable to merge: List expected')
            base = []
            for x in datas:
                base = base + x
            return base
        if isinstance(datas[0], dict):
            if len([x for x in datas if not isinstance(x, dict)]) > 0:
                raise TypeError('Unable to merge: Dictionnary expected')
            result = {}
            for element in datas:
                for key in element:
                    if key in result:
                        result[key] = self.merge_sources([result[key], element[key]])
                    else:
                        result[key] = element[key]
            return result
        if len([x for x in datas if isinstance(x, (dict, list))]) > 0:
            raise TypeError('Unable to merge: List not expected')
        raise ValueError('Unable to merge: Conflict')

    def merge_configs(self, config, datas):
        """"""Merge configs files
        """"""
        if not isinstance(config, dict) or len([x for x in datas if not isinstance(x, dict)]) > 0:
            raise TypeError('Unable to merge: Dictionnary expected')
        for key, value in config.items():
            others = [x[key] for x in datas if key in x]
            if len(others) > 0:
                if isinstance(value, dict):
                    config[key] = self.merge_configs(value, others)
                else:
                    config[key] = others[-1]
        return config","
class Merger:
    '''Provide tool to merge elements
    '''

    def merge_extends(self, target, extends, inherit_key='inherit', inherit=False):
        '''Merge extended dicts
        '''
        pass

    def merge_sources(self, datas):
        '''Merge sources files
        '''
        pass

    def merge_configs(self, config, datas):
        '''Merge configs files
        '''
        pass",snippet_247,"class Merger:
    '''Provide tool to merge elements
    '''

    def _deepcopy(self, obj):
        # Lightweight deepcopy for common JSON-like structures
        if isinstance(obj, dict):
            return {k: self._deepcopy(v) for k, v in obj.items()}
        if isinstance(obj, list):
            return [self._deepcopy(v) for v in obj]
        return obj

    def _merge(self, base, override, inherit, inherit_key):
        # If override explicitly disables inheritance at this node, replace entirely
        if isinstance(override, dict):
            explicit = override.get(inherit_key, None)
            if explicit is False:
                # Replace entirely, removing the inherit key
                return {k: self._deepcopy(v) for k, v in override.items() if k != inherit_key}

        # Dict merge
        if isinstance(base, dict) and isinstance(override, dict):
            result = {}
            # Start from base
            for k, v in base.items():
                result[k] = self._deepcopy(v)
            # Merge override
            for k, v in override.items():
                if k == inherit_key:
                    continue
                if k in result:
                    result[k] = self._merge(result[k], v, inherit, inherit_key)
                else:
                    result[k] = self._deepcopy(v)
            return result

        # List handling
        if isinstance(base, list) and isinstance(override, list):
            if inherit:
                return self._deepcopy(base) + self._deepcopy(override)
            return self._deepcopy(override)

        # Different types or scalars: override wins
        return self._deepcopy(override)

    def merge_extends(self, target, extends, inherit_key='inherit', inherit=False):
        '''Merge extended dicts
        '''
        # Normalize inputs
        tgt = {} if target is None else self._deepcopy(target)
        bases = []
        if extends is None:
            bases = []
        elif isinstance(extends, (list, tuple)):
            bases = list(extends)
        else:
            bases = [extends]

        # Start with empty base and merge all extends in order
        result = {}
        for base in bases:
            if base is None:
                continue
            result = self._merge(result, base, True, inherit_key)

        # If target has explicit inherit directive at the root, honor it
        root_inherit = inherit
        if isinstance(tgt, dict) and inherit_key in tgt:
            # If root explicitly sets inherit False, replace base entirely with target (minus inherit key)
            if tgt.get(inherit_key) is False:
                return {k: self._deepcopy(v) for k, v in tgt.items() if k != inherit_key}
            # If True, ensure deep merge
            root_inherit = True
            # Remove inherit directive from final result
            tgt = {k: v for k, v in tgt.items() if k != inherit_key}

        # Merge target over the accumulated base
        result = self._merge(result, tgt, root_inherit, inherit_key)
        return result

    def merge_sources(self, datas):
        '''Merge sources files
        '''
        if not datas:
            return {}
        result = {}
        for d in datas:
            if d is None:
                continue
            result = self._merge(result, d, True, 'inherit')
        return result

    def merge_configs(self, config, datas):
        '''Merge configs files
        '''
        # First merge all source data as base, then overlay the config
        base = self.merge_sources(datas)
        return self.merge_extends(config or {}, base, inherit_key='inherit', inherit=True)"
313030,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/service/parser.py,apidoc.service.parser.Parser,"import os
import yaml
import json

class Parser:
    """"""Provide tools to parse files
    """"""

    def load_from_file(self, file_path, format=None):
        """"""Return dict from a file config
        """"""
        if format is None:
            base_name, file_extension = os.path.splitext(file_path)
            if file_extension in ('.yaml', '.yml'):
                format = 'yaml'
            elif file_extension in '.json':
                format = 'json'
            else:
                raise ValueError('Config file ""%s"" undetermined' % file_extension)
        if format == 'yaml':
            return yaml.load(open(file_path), Loader=yaml.CSafeLoader if yaml.__with_libyaml__ else yaml.SafeLoader)
        elif format == 'json':
            return json.load(open(file_path))
        else:
            raise ValueError('Format ""%s"" unknwon' % format)

    def load_all_from_directory(self, directory_path):
        """"""Return a list of dict from a directory containing files
        """"""
        datas = []
        for root, folders, files in os.walk(directory_path):
            for f in files:
                datas.append(self.load_from_file(os.path.join(root, f)))
        return datas","
class Parser:
    '''Provide tools to parse files
    '''

    def load_from_file(self, file_path, format=None):
        '''Return dict from a file config
        '''
        pass

    def load_all_from_directory(self, directory_path):
        '''Return a list of dict from a directory containing files
        '''
        pass",snippet_248,"class Parser:
    '''Provide tools to parse files
    '''

    _EXT_TO_FORMAT = {
        '.json': 'json',
        '.yaml': 'yaml',
        '.yml': 'yaml',
        '.toml': 'toml',
        '.ini': 'ini',
        '.cfg': 'ini',
        '.conf': 'ini',
    }

    def _detect_format(self, file_path, fmt):
        if fmt:
            return fmt.lower()
        import os
        _, ext = os.path.splitext(str(file_path))
        return self._EXT_TO_FORMAT.get(ext.lower())

    def _load_json(self, file_path):
        import json
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        if not isinstance(data, dict):
            raise ValueError(""JSON root must be an object mapping to a dict"")
        return data

    def _load_yaml(self, file_path):
        try:
            import yaml  # type: ignore
        except Exception as e:
            raise ImportError(""PyYAML is required to parse YAML files"") from e
        with open(file_path, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
        if data is None:
            return {}
        if not isinstance(data, dict):
            raise ValueError(""YAML root must be a mapping to a dict"")
        return data

    def _load_toml(self, file_path):
        data = None
        try:
            import tomllib  # Python 3.11+
            with open(file_path, 'rb') as f:
                data = tomllib.load(f)
        except ModuleNotFoundError:
            try:
                import toml  # type: ignore
            except Exception as e:
                raise ImportError(""tomllib (py3.11+) or toml package is required to parse TOML files"") from e
            with open(file_path, 'r', encoding='utf-8') as f:
                data = toml.load(f)
        if not isinstance(data, dict):
            raise ValueError(""TOML root must be a mapping to a dict"")
        return data

    def _load_ini(self, file_path):
        import configparser
        parser = configparser.ConfigParser()
        with open(file_path, 'r', encoding='utf-8') as f:
            parser.read_file(f)
        result = {}
        defaults = dict(parser.defaults()) if parser.defaults() else {}
        if defaults:
            result['_defaults'] = defaults
        for section in parser.sections():
            result[section] = dict(parser.items(section))
        return result

    def load_from_file(self, file_path, format=None):
        '''Return dict from a file config
        '''
        import os
        if not os.path.isfile(file_path):
            raise FileNotFoundError(f""No such file: {file_path}"")
        fmt = self._detect_format(file_path, format)
        if fmt == 'json':
            return self._load_json(file_path)
        if fmt == 'yaml':
            return self._load_yaml(file_path)
        if fmt == 'toml':
            return self._load_toml(file_path)
        if fmt == 'ini':
            return self._load_ini(file_path)
        raise ValueError(f""Unsupported or unknown format for file: {file_path}"")

    def load_all_from_directory(self, directory_path):
        '''Return a list of dict from a directory containing files
        '''
        import os
        if not os.path.isdir(directory_path):
            raise NotADirectoryError(f""Not a directory: {directory_path}"")
        supported_exts = set(self._EXT_TO_FORMAT.keys())
        results = []
        for name in sorted(os.listdir(directory_path)):
            path = os.path.join(directory_path, name)
            if not os.path.isfile(path):
                continue
            _, ext = os.path.splitext(name)
            if ext.lower() not in supported_exts:
                continue
            results.append(self.load_from_file(path))
        return results"
313032,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/service/template.py,apidoc.service.template.Template,"import sys
import logging
import shutil
import os

class Template:
    """"""Provide tool to managed templates
    """"""

    def __init__(self):
        """"""Class instantiation
        """"""
        self.input = 'default.html'
        self.output = 'stdout'
        self.env = None

    def render(self, sources, config, out=sys.stdout):
        """"""Render the documentation as defined in config Object
        """"""
        logger = logging.getLogger()
        template = self.env.get_template(self.input)
        output = template.render(sources=sources, layout=config['output']['layout'], config=config['output'])
        if self.output == 'stdout':
            out.write(output)
        else:
            dir = os.path.dirname(self.output)
            if dir and (not os.path.exists(dir)):
                try:
                    os.makedirs(dir)
                except IOError as ioerror:
                    logger.error('Error on creating dir ""{}"": {}'.format(dir, str(ioerror)))
                    return
            if config['output']['template'] == 'default':
                if config['output']['componants'] == 'local':
                    for template_dir in self.env.loader.searchpath:
                        files = (os.path.join(template_dir, 'resource', 'js', 'combined.js'), os.path.join(template_dir, 'resource', 'css', 'combined.css'), os.path.join(template_dir, 'resource', 'font', 'apidoc.eot'), os.path.join(template_dir, 'resource', 'font', 'apidoc.woff'), os.path.join(template_dir, 'resource', 'font', 'apidoc.ttf'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.eot'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.woff'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.ttf'))
                        for file in files:
                            filename = os.path.basename(file)
                            dirname = os.path.basename(os.path.dirname(file))
                            if not os.path.exists(os.path.join(dir, dirname)):
                                os.makedirs(os.path.join(dir, dirname))
                            if os.path.exists(file):
                                shutil.copyfile(file, os.path.join(dir, dirname, filename))
                            else:
                                logger.warn('Missing resource file ""%s"". If you run apidoc in virtualenv, run ""%s""' % (filename, 'python setup.py resources'))
                if config['output']['componants'] == 'remote':
                    for template_dir in self.env.loader.searchpath:
                        files = (os.path.join(template_dir, 'resource', 'js', 'combined.js'), os.path.join(template_dir, 'resource', 'css', 'combined-embedded.css'), os.path.join(template_dir, 'resource', 'font', 'apidoc.eot'), os.path.join(template_dir, 'resource', 'font', 'apidoc.woff'), os.path.join(template_dir, 'resource', 'font', 'apidoc.ttf'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.eot'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.woff'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.ttf'))
                        for file in files:
                            filename = os.path.basename(file)
                            dirname = os.path.basename(os.path.dirname(file))
                            if not os.path.exists(os.path.join(dir, dirname)):
                                os.makedirs(os.path.join(dir, dirname))
                            if os.path.exists(file):
                                shutil.copyfile(file, os.path.join(dir, dirname, filename))
                            else:
                                logger.warn('Missing resource file ""%s"". If you run apidoc in virtualenv, run ""%s""' % (filename, 'python setup.py resources'))
            open(self.output, 'w').write(output)","
class Template:
    '''Provide tool to managed templates
    '''

    def __init__(self):
        '''Class instantiation
        '''
        pass

    def render(self, sources, config, out=sys.stdout):
        '''Render the documentation as defined in config Object
        '''
        pass",snippet_249,"import sys
import os
from pathlib import Path
from collections.abc import Mapping, Iterable

try:
    import jinja2 as _jinja2  # Optional
except Exception:  # pragma: no cover
    _jinja2 = None


class Template:
    '''Provide tool to managed templates
    '''

    def __init__(self):
        '''Class instantiation
        '''
        self.default_engine = 'auto'  # 'auto' | 'format' | 'jinja2'

    def render(self, sources, config, out=sys.stdout):
        '''Render the documentation as defined in config Object
        '''
        cfg = self._normalize_config(config)
        template = self._load_template(cfg)
        context = self._build_context(sources, cfg)

        engine = cfg.get('engine', self.default_engine)
        if engine == 'auto':
            engine = self._detect_engine(template)

        if engine == 'jinja2':
            if _jinja2 is None:
                raise RuntimeError(""Jinja2 engine requested but not available"")
            rendered = self._render_jinja2(template, context, cfg)
        elif engine == 'format':
            rendered = self._render_format(template, context)
        else:
            raise ValueError(f""Unsupported engine: {engine}"")

        if out is None:
            return rendered

        self._write_output(rendered, out, cfg)
        return rendered

    # Helpers

    def _normalize_config(self, config):
        if config is None:
            return {}
        if isinstance(config, Mapping):
            cfg = dict(config)
        else:
            # Fallback to attribute-based object
            cfg = {k: getattr(config, k) for k in dir(config) if not k.startswith('_')}
        return cfg

    def _load_template(self, cfg):
        tpl = cfg.get('template')
        if tpl is None:
            raise ValueError(""config must include 'template' (string or path)"")

        # If template is a path and exists, read it
        if isinstance(tpl, (str, os.PathLike)):
            p = Path(tpl)
            if p.exists() and p.is_file():
                return p.read_text(encoding=cfg.get('encoding', 'utf-8'))
        # Else treat as inline template string
        if not isinstance(tpl, str):
            raise TypeError(""template must be a string or a path to a template file"")
        return tpl

    def _build_context(self, sources, cfg):
        ctx = {}
        extra = cfg.get('context') or {}
        if not isinstance(extra, Mapping):
            raise TypeError(""config['context'] must be a mapping if provided"")
        ctx.update(extra)

        # Normalize sources
        normalized = self._normalize_sources(sources, cfg)
        # Provide both flat and namespaced access
        ctx.setdefault('sources', normalized)
        # If mapping and values are scalar/text, flatten top-level keys, without overwriting explicit context
        if isinstance(normalized, Mapping):
            for k, v in normalized.items():
                if k not in ctx:
                    ctx[k] = v
        return ctx

    def _normalize_sources(self, sources, cfg):
        # Accept:
        # - Mapping[str, Any]
        # - path string/Path -> read text
        # - Iterable of paths -> dict name->text
        # - file-like -> read
        encoding = cfg.get('source_encoding', cfg.get('encoding', 'utf-8'))

        # File-like
        if hasattr(sources, 'read'):
            return sources.read()

        # Single path
        if isinstance(sources, (str, os.PathLike)):
            p = Path(sources)
            if p.exists():
                if p.is_file():
                    return p.read_text(encoding=encoding)
                if p.is_dir():
                    return self._read_dir(p, encoding)
            # Not an existing path: return as-is string
            return str(sources)

        # Mapping
        if isinstance(sources, Mapping):
            return {str(k): self._maybe_read(v, encoding) for k, v in sources.items()}

        # Iterable of paths/values
        if isinstance(sources, Iterable):
            result = {}
            for item in sources:
                if isinstance(item, (str, os.PathLike)):
                    p = Path(item)
                    if p.exists() and p.is_file():
                        result[p.stem] = p.read_text(encoding=encoding)
                    else:
                        name = Path(str(item)).stem or str(item)
                        result[name] = str(item)
                else:
                    name = getattr(item, 'name', None)
                    if hasattr(item, 'read'):
                        content = item.read()
                        if hasattr(content, 'decode'):
                            content = content.decode(encoding)
                        result[name or f'item_{len(result)}'] = content
                    else:
                        result[name or f'item_{len(result)}'] = str(item)
            return result

        return sources

    def _maybe_read(self, value, encoding):
        if hasattr(value, 'read'):
            data = value.read()
            return data.decode(encoding) if hasattr(data, 'decode') else data
        if isinstance(value, (str, os.PathLike)):
            p = Path(value)
            if p.exists() and p.is_file():
                return p.read_text(encoding=encoding)
        return value

    def _read_dir(self, directory: Path, encoding: str):
        data = {}
        for p in sorted(directory.rglob('*')):
            if p.is_file():
                key = p.relative_to(directory).as_posix()
                data[key] = p.read_text(encoding=encoding)
        return data

    def _detect_engine(self, template: str):
        # Heuristic: if Jinja-like delimiters present, choose jinja2 if available
        if any(tok in template for tok in ('{{', '{%', '{#')):
            return 'jinja2' if _jinja2 is not None else 'format'
        return 'format'

    def _render_format(self, template: str, context: Mapping):
        # Safe .format rendering: missing keys remain unchanged
        class SafeDict(dict):
            def __missing__(self, key):
                return '{' + key + '}'
        return template.format_map(SafeDict(context))

    def _render_jinja2(self, template: str, context: Mapping, cfg: Mapping):
        loader = None
        searchpath = cfg.get('searchpath')
        if searchpath:
            if isinstance(searchpath, (str, os.PathLike)):
                searchpath = [searchpath]
            loader = _jinja2.FileSystemLoader([str(Path(p)) for p in searchpath])

        env_kwargs = {
            'autoescape': cfg.get('autoescape', False),
            'trim_blocks': cfg.get('trim_blocks', True),
            'lstrip_blocks': cfg.get('lstrip_blocks', True),
            'loader': loader
        }
        env = _jinja2.Environment(**env_kwargs)
        filters = cfg.get('filters') or {}
        for name, func in filters.items():
            env.filters[name] = func

        # If a template_name is provided and loader exists, use it; else render from string
        template_name = cfg.get('template_name')
        if template_name and loader is not None:
            tpl = env.get_template(template_name)
        else:
            tpl = env.from_string(template)
        return tpl.render(**context)

    def _write_output(self, rendered: str, out, cfg):
        if hasattr(out, 'write'):
            out.write(rendered)
            return
        p = Path(out)
        p.parent.mkdir(parents=True, exist_ok=True)
        encoding = cfg.get('output_encoding', cfg.get('encoding', 'utf-8'))
        p.write_text(rendered, encoding=encoding)"
313488,adafruit/Adafruit_CircuitPython_framebuf,adafruit_Adafruit_CircuitPython_framebuf/adafruit_framebuf.py,adafruit_framebuf.MHMSBFormat,"class MHMSBFormat:
    """"""MHMSBFormat""""""

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        """"""Set a given pixel to a color.""""""
        index = (y * framebuf.stride + x) // 8
        offset = 7 - x & 7
        framebuf.buf[index] = framebuf.buf[index] & ~(1 << offset) | (color != 0) << offset

    @staticmethod
    def get_pixel(framebuf, x, y):
        """"""Get the color of a given pixel""""""
        index = (y * framebuf.stride + x) // 8
        offset = 7 - x & 7
        return framebuf.buf[index] >> offset & 1

    @staticmethod
    def fill(framebuf, color):
        """"""completely fill/clear the buffer with a color""""""
        if color:
            fill = 255
        else:
            fill = 0
        for i in range(len(framebuf.buf)):
            framebuf.buf[i] = fill

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        """"""Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.""""""
        for _x in range(x, x + width):
            offset = 7 - _x & 7
            for _y in range(y, y + height):
                index = (_y * framebuf.stride + _x) // 8
                framebuf.buf[index] = framebuf.buf[index] & ~(1 << offset) | (color != 0) << offset","class MHMSBFormat:
    '''MHMSBFormat'''
    @staticmethod
    def set_pixel(framebuf, x, y, color):
        '''Set a given pixel to a color.'''
        pass
    @staticmethod
    def get_pixel(framebuf, x, y):
        '''Get the color of a given pixel'''
        pass
    @staticmethod
    def fill(framebuf, color):
        '''completely fill/clear the buffer with a color'''
        pass
    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        '''Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.'''
        pass",snippet_250,"class MHMSBFormat:
    '''MHMSBFormat'''
    @staticmethod
    def set_pixel(framebuf, x, y, color):
        '''Set a given pixel to a color.'''
        w = framebuf.width
        h = framebuf.height
        if not (0 <= x < w and 0 <= y < h):
            return
        stride = (w + 7) // 8
        idx = y * stride + (x // 8)
        bit = 0x80 >> (x % 8)
        if color & 1:
            framebuf.buf[idx] |= bit
        else:
            framebuf.buf[idx] &= (~bit) & 0xFF

    @staticmethod
    def get_pixel(framebuf, x, y):
        '''Get the color of a given pixel'''
        w = framebuf.width
        h = framebuf.height
        if not (0 <= x < w and 0 <= y < h):
            return 0
        stride = (w + 7) // 8
        idx = y * stride + (x // 8)
        bit = 0x80 >> (x % 8)
        return 1 if (framebuf.buf[idx] & bit) else 0

    @staticmethod
    def fill(framebuf, color):
        '''completely fill/clear the buffer with a color'''
        byte = 0xFF if (color & 1) else 0x00
        b = framebuf.buf
        for i in range(len(b)):
            b[i] = byte

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        '''Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.'''
        w = framebuf.width
        h = framebuf.height
        if width <= 0 or height <= 0:
            return
        x0 = max(0, x)
        y0 = max(0, y)
        x1 = min(w, x + width)
        y1 = min(h, y + height)
        if x0 >= x1 or y0 >= y1:
            return
        for yy in range(y0, y1):
            for xx in range(x0, x1):
                MHMSBFormat.set_pixel(framebuf, xx, yy, color)"
313489,adafruit/Adafruit_CircuitPython_framebuf,adafruit_Adafruit_CircuitPython_framebuf/adafruit_framebuf.py,adafruit_framebuf.MVLSBFormat,"class MVLSBFormat:
    """"""MVLSBFormat""""""

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        """"""Set a given pixel to a color.""""""
        index = (y >> 3) * framebuf.stride + x
        offset = y & 7
        framebuf.buf[index] = framebuf.buf[index] & ~(1 << offset) | (color != 0) << offset

    @staticmethod
    def get_pixel(framebuf, x, y):
        """"""Get the color of a given pixel""""""
        index = (y >> 3) * framebuf.stride + x
        offset = y & 7
        return framebuf.buf[index] >> offset & 1

    @staticmethod
    def fill(framebuf, color):
        """"""completely fill/clear the buffer with a color""""""
        if color:
            fill = 255
        else:
            fill = 0
        for i in range(len(framebuf.buf)):
            framebuf.buf[i] = fill

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        """"""Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.""""""
        while height > 0:
            index = (y >> 3) * framebuf.stride + x
            offset = y & 7
            for w_w in range(width):
                framebuf.buf[index + w_w] = framebuf.buf[index + w_w] & ~(1 << offset) | (color != 0) << offset
            y += 1
            height -= 1","class MVLSBFormat:
    '''MVLSBFormat'''
    @staticmethod
    def set_pixel(framebuf, x, y, color):
        '''Set a given pixel to a color.'''
        pass
    @staticmethod
    def get_pixel(framebuf, x, y):
        '''Get the color of a given pixel'''
        pass
    @staticmethod
    def fill(framebuf, color):
        '''completely fill/clear the buffer with a color'''
        pass
    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        '''Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.'''
        pass",snippet_251,"class MVLSBFormat:
    '''MVLSBFormat'''
    @staticmethod
    def _bool_color(color):
        return 1 if color else 0

    @staticmethod
    def _get_stride(framebuf):
        return getattr(framebuf, ""stride"", framebuf.width)

    @staticmethod
    def _index_and_bit(framebuf, x, y):
        # MONO_VLSB: byte = x + (y >> 3) * stride, bit = 1 << (y & 7)
        stride = MVLSBFormat._get_stride(framebuf)
        byte_index = x + ((y >> 3) * stride)
        bit = 1 << (y & 7)
        return byte_index, bit

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        '''Set a given pixel to a color.'''
        if not (0 <= x < framebuf.width and 0 <= y < framebuf.height):
            return
        idx, bit = MVLSBFormat._index_and_bit(framebuf, x, y)
        if MVLSBFormat._bool_color(color):
            framebuf.buffer[idx] |= bit
        else:
            framebuf.buffer[idx] &= (~bit) & 0xFF

    @staticmethod
    def get_pixel(framebuf, x, y):
        '''Get the color of a given pixel'''
        if not (0 <= x < framebuf.width and 0 <= y < framebuf.height):
            return 0
        idx, bit = MVLSBFormat._index_and_bit(framebuf, x, y)
        return 1 if (framebuf.buffer[idx] & bit) else 0

    @staticmethod
    def fill(framebuf, color):
        '''completely fill/clear the buffer with a color'''
        fill_byte = 0xFF if MVLSBFormat._bool_color(color) else 0x00
        buf = framebuf.buffer
        # Support both bytearray/memoryview
        for i in range(len(buf)):
            buf[i] = fill_byte

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        '''Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.'''
        if width <= 0 or height <= 0:
            return

        x0 = max(0, x)
        y0 = max(0, y)
        x1 = min(framebuf.width, x + width)
        y1 = min(framebuf.height, y + height)
        if x0 >= x1 or y0 >= y1:
            return

        c = MVLSBFormat._bool_color(color)
        stride = MVLSBFormat._get_stride(framebuf)
        buf = framebuf.buffer

        # Optimize by writing whole bytes when possible
        # Process per x column as bytes represent vertical columns
        for cx in range(x0, x1):
            y_start = y0
            y_end = y1

            # Handle head partial byte
            first_block = y_start >> 3
            last_block = (y_end - 1) >> 3

            if first_block == last_block:
                # Region within a single byte
                idx = cx + first_block * stride
                mask = 0
                for py in range(y_start, y_end):
                    mask |= (1 << (py & 7))
                if c:
                    buf[idx] |= mask
                else:
                    buf[idx] &= (~mask) & 0xFF
                continue

            # Head partial
            if (y_start & 7) != 0:
                idx = cx + first_block * stride
                mask = 0
                for py in range(y_start, (first_block + 1) * 8):
                    mask |= (1 << (py & 7))
                if c:
                    buf[idx] |= mask
                else:
                    buf[idx] &= (~mask) & 0xFF
                first_block += 1

            # Full bytes
            if first_block <= last_block - 1:
                fill_byte = 0xFF if c else 0x00
                for blk in range(first_block, last_block):
                    idx = cx + blk * stride
                    buf[idx] = fill_byte

            # Tail partial
            idx = cx + last_block * stride
            mask = 0
            for py in range(last_block * 8, y_end):
                mask |= (1 << (py & 7))
            if c:
                buf[idx] |= mask
            else:
                buf[idx] &= (~mask) & 0xFF"
313491,adafruit/Adafruit_CircuitPython_framebuf,adafruit_Adafruit_CircuitPython_framebuf/adafruit_framebuf.py,adafruit_framebuf.RGB888Format,"class RGB888Format:
    """"""RGB888Format""""""

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        """"""Set a given pixel to a color.""""""
        index = (y * framebuf.stride + x) * 3
        if isinstance(color, tuple):
            framebuf.buf[index:index + 3] = bytes(color)
        else:
            framebuf.buf[index:index + 3] = bytes((color >> 16 & 255, color >> 8 & 255, color & 255))

    @staticmethod
    def get_pixel(framebuf, x, y):
        """"""Get the color of a given pixel""""""
        index = (y * framebuf.stride + x) * 3
        return framebuf.buf[index] << 16 | framebuf.buf[index + 1] << 8 | framebuf.buf[index + 2]

    @staticmethod
    def fill(framebuf, color):
        """"""completely fill/clear the buffer with a color""""""
        fill = (color >> 16 & 255, color >> 8 & 255, color & 255)
        for i in range(0, len(framebuf.buf), 3):
            framebuf.buf[i:i + 3] = bytes(fill)

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        """"""Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.""""""
        fill = (color >> 16 & 255, color >> 8 & 255, color & 255)
        for _x in range(x, x + width):
            for _y in range(y, y + height):
                index = (_y * framebuf.stride + _x) * 3
                framebuf.buf[index:index + 3] = bytes(fill)","class RGB888Format:
    '''RGB888Format'''
    @staticmethod
    def set_pixel(framebuf, x, y, color):
        '''Set a given pixel to a color.'''
        pass
    @staticmethod
    def get_pixel(framebuf, x, y):
        '''Get the color of a given pixel'''
        pass
    @staticmethod
    def fill(framebuf, color):
        '''completely fill/clear the buffer with a color'''
        pass
    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        '''Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.'''
        pass",snippet_252,"class RGB888Format:
    '''RGB888Format'''

    @staticmethod
    def _get_buf_wh_stride(framebuf):
        buf = getattr(framebuf, 'buffer', None)
        if buf is None:
            buf = getattr(framebuf, 'buf', None)
        if buf is None:
            raise AttributeError('framebuf must have buffer or buf attribute')
        w = getattr(framebuf, 'width', None)
        h = getattr(framebuf, 'height', None)
        if w is None or h is None:
            raise AttributeError('framebuf must have width and height attributes')
        stride = getattr(framebuf, 'stride', None)
        if stride is None:
            stride = w * 3
        return buf, w, h, stride

    @staticmethod
    def _color_to_bytes(color):
        if isinstance(color, (tuple, list)) and len(color) == 3:
            r, g, b = color
            r &= 0xFF
            g &= 0xFF
            b &= 0xFF
        else:
            c = int(color) & 0xFFFFFF
            r = (c >> 16) & 0xFF
            g = (c >> 8) & 0xFF
            b = c & 0xFF
        return r, g, b

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        '''Set a given pixel to a color.'''
        buf, w, h, stride = RGB888Format._get_buf_wh_stride(framebuf)
        if not (0 <= x < w and 0 <= y < h):
            return
        r, g, b = RGB888Format._color_to_bytes(color)
        idx = y * stride + x * 3
        buf[idx] = r
        buf[idx + 1] = g
        buf[idx + 2] = b

    @staticmethod
    def get_pixel(framebuf, x, y):
        '''Get the color of a given pixel'''
        buf, w, h, stride = RGB888Format._get_buf_wh_stride(framebuf)
        if not (0 <= x < w and 0 <= y < h):
            return 0
        idx = y * stride + x * 3
        r = buf[idx]
        g = buf[idx + 1]
        b = buf[idx + 2]
        return (r << 16) | (g << 8) | b

    @staticmethod
    def fill(framebuf, color):
        '''completely fill/clear the buffer with a color'''
        buf, w, h, stride = RGB888Format._get_buf_wh_stride(framebuf)
        r, g, b = RGB888Format._color_to_bytes(color)
        row_pattern = bytes((r, g, b)) * w
        for y in range(h):
            start = y * stride
            end = start + w * 3
            buf[start:end] = row_pattern

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        '''Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.'''
        buf, w, h, stride = RGB888Format._get_buf_wh_stride(framebuf)
        if width <= 0 or height <= 0:
            return
        x0 = max(0, x)
        y0 = max(0, y)
        x1 = min(w, x + width)
        y1 = min(h, y + height)
        if x0 >= x1 or y0 >= y1:
            return
        r, g, b = RGB888Format._color_to_bytes(color)
        row_pattern = bytes((r, g, b)) * (x1 - x0)
        for row in range(y0, y1):
            start = row * stride + x0 * 3
            end = start + (x1 - x0) * 3
            buf[start:end] = row_pattern"
319441,pysal/spglm,spglm/varfuncs.py,spglm.varfuncs.VarianceFunction,"import numpy as np

class VarianceFunction:
    """"""
    Relates the variance of a random variable to its mean. Defaults to 1.

    Methods
    -------
    call
        Returns an array of ones that is the same shape as `mu`

    Notes
    -----
    After a variance function is initialized, its call method can be used.

    Alias for VarianceFunction:
    constant = VarianceFunction()

    See also
    --------
    statsmodels.family.family
    """"""

    def __call__(self, mu):
        """"""
        Default variance function

        Parameters
        -----------
        mu : array-like
            mean parameters

        Returns
        -------
        v : array
            ones(mu.shape)
        """"""
        mu = np.asarray(mu)
        return np.ones(mu.shape, np.float64)

    def deriv(self, mu):
        """"""
        Derivative of the variance function v'(mu)
        """"""
        from statsmodels.tools.numdiff import approx_fprime_cs
        return np.diag(approx_fprime_cs(mu, self))","
class VarianceFunction:
    '''
    Relates the variance of a random variable to its mean. Defaults to 1.
    Methods
    -------
    call
        Returns an array of ones that is the same shape as `mu`
    Notes
    -----
    After a variance function is initialized, its call method can be used.
    Alias for VarianceFunction:
    constant = VarianceFunction()
    See also
    --------
    statsmodels.family.family
    '''

    def __call__(self, mu):
        '''
        Default variance function
        Parameters
        -----------
        mu : array-like
            mean parameters
        Returns
        -------
        v : array
            ones(mu.shape)
        '''
        pass

    def deriv(self, mu):
        '''
        Derivative of the variance function v'(mu)
        '''
        pass",snippet_256,"import numpy as np

class VarianceFunction:
    '''
    Relates the variance of a random variable to its mean. Defaults to 1.
    Methods
    -------
    call
        Returns an array of ones that is the same shape as `mu`
    Notes
    -----
    After a variance function is initialized, its call method can be used.
    Alias for VarianceFunction:
    constant = VarianceFunction()
    See also
    --------
    statsmodels.family.family
    '''

    def __call__(self, mu):
        '''
        Default variance function
        Parameters
        -----------
        mu : array-like
            mean parameters
        Returns
        -------
        v : array
            ones(mu.shape)
        '''
        mu = np.asarray(mu)
        return np.ones(mu.shape, dtype=float)

    def deriv(self, mu):
        '''
        Derivative of the variance function v'(mu)
        '''
        mu = np.asarray(mu)
        return np.zeros(mu.shape, dtype=float)

constant = VarianceFunction()"
320689,optimizely/python-sdk,optimizely_python-sdk/optimizely/user_profile.py,optimizely.user_profile.UserProfileService,"from typing import Any, Optional

class UserProfileService:
    """""" Class encapsulating user profile service functionality.
  Override with your own implementation for storing and retrieving the user profile. """"""

    def lookup(self, user_id: str) -> dict[str, Any]:
        """""" Fetch the user profile dict corresponding to the user ID.

    Args:
      user_id: ID for user whose profile needs to be retrieved.

    Returns:
      Dict representing the user's profile.
    """"""
        return UserProfile(user_id).__dict__

    def save(self, user_profile: dict[str, Any]) -> None:
        """""" Save the user profile dict sent to this method.

    Args:
      user_profile: Dict representing the user's profile.
    """"""
        pass","
class UserProfileService:
    ''' Class encapsulating user profile service functionality.
  Override with your own implementation for storing and retrieving the user profile. '''

    def lookup(self, user_id: str) -> dict[str, Any]:
        ''' Fetch the user profile dict corresponding to the user ID.
    Args:
      user_id: ID for user whose profile needs to be retrieved.
    Returns:
      Dict representing the user's profile.
        '''
        pass

    def save(self, user_profile: dict[str, Any]) -> None:
        ''' Save the user profile dict sent to this method.
    Args:
      user_profile: Dict representing the user's profile.
        '''
        pass",snippet_257,"from typing import Any, Optional, Dict
from threading import RLock
from copy import deepcopy


class UserProfileService:
    ''' Class encapsulating user profile service functionality.
  Override with your own implementation for storing and retrieving the user profile. '''

    def __init__(self, *, id_key: str = ""user_id"", initial_data: Optional[Dict[str, Dict[str, Any]]] = None) -> None:
        self._id_key = id_key
        self._store: Dict[str, Dict[str, Any]] = {}
        if initial_data:
            for k, v in initial_data.items():
                if not isinstance(v, dict):
                    raise TypeError(""Initial data values must be dicts"")
                self._store[k] = deepcopy(v)
        self._lock = RLock()

    def lookup(self, user_id: str) -> dict[str, Any]:
        ''' Fetch the user profile dict corresponding to the user ID.
    Args:
      user_id: ID for user whose profile needs to be retrieved.
    Returns:
      Dict representing the user's profile.
        '''
        if not isinstance(user_id, str) or not user_id:
            raise ValueError(""user_id must be a non-empty string"")
        with self._lock:
            if user_id not in self._store:
                raise KeyError(f""User profile not found for id: {user_id}"")
            return deepcopy(self._store[user_id])

    def save(self, user_profile: dict[str, Any]) -> None:
        ''' Save the user profile dict sent to this method.
    Args:
      user_profile: Dict representing the user's profile.
        '''
        if not isinstance(user_profile, dict):
            raise TypeError(""user_profile must be a dict"")
        if self._id_key not in user_profile:
            raise KeyError(f""user_profile must contain key '{self._id_key}'"")
        user_id = user_profile[self._id_key]
        if not isinstance(user_id, str) or not user_id:
            raise ValueError(f""'{self._id_key}' must be a non-empty string"")
        with self._lock:
            self._store[user_id] = deepcopy(user_profile)"
322746,wilson-eft/wilson,wilson-eft_wilson/wilson/classes.py,wilson.classes.RGsolution,"import numpy as np
from math import log, e

class RGsolution:
    """"""Class representing a continuous (interpolated) solution to the
    SMEFT RGEs to be used for plotting.""""""

    def __init__(self, fun, scale_min, scale_max):
        """"""Initialize.

        Parameters:

        - fun: function of the scale that is expected to return a
        dictionary with the RGE solution and to accept vectorized input.
        - scale_min, scale_max: lower and upper boundaries of the scale
        """"""
        self.fun = fun
        self.scale_min = scale_min
        self.scale_max = scale_max

    def plotdata(self, key, part='re', scale='log', steps=50):
        """"""Return a tuple of arrays x, y that can be fed to plt.plot,
        where x is the scale in GeV and y is the parameter of interest.

        Parameters:

        - key: dicionary key of the parameter to be plotted (e.g. a WCxf
          coefficient name or a SM parameter like 'g')
        - part: plot the real part 're' (default) or the imaginary part 'im'
        - scale: 'log'; make the x steps logarithmically distributed; for
          'linear', linearly distributed
        - steps: steps in x to take (default: 50)
        """"""
        if scale == 'log':
            x = np.logspace(log(self.scale_min), log(self.scale_max), steps, base=e)
        elif scale == 'linear':
            x = np.linspace(self.scale_min, self.scale_max, steps)
        y = self.fun(x)
        y = np.array([d[key] for d in y])
        if part == 're':
            return (x, y.real)
        elif part == 'im':
            return (x, y.imag)

    def plot(self, key, part='re', scale='log', steps=50, legend=True, plotargs={}):
        """"""Plot the RG evolution of parameter `key`.

        Parameters:

        - part, scale, steps: see `plotdata`
        - legend: boolean, show the legend (default: True)
        - plotargs: dictionary of arguments to be passed to plt.plot
        """"""
        try:
            import matplotlib.pyplot as plt
        except ImportError:
            raise ImportError('Please install matplotlib if you want to use the plot method')
        pdat = self.plotdata(key, part=part, scale=scale, steps=steps)
        plt.plot(*pdat, label=key, **plotargs)
        if scale == 'log':
            plt.xscale('log')
        if legend:
            plt.legend()","
class RGsolution:
    '''Class representing a continuous (interpolated) solution to the
    SMEFT RGEs to be used for plotting.'''

    def __init__(self, fun, scale_min, scale_max):
        '''Initialize.
        Parameters:
        - fun: function of the scale that is expected to return a
        dictionary with the RGE solution and to accept vectorized input.
        - scale_min, scale_max: lower and upper boundaries of the scale
        '''
        pass

    def plotdata(self, key, part='re', scale='log', steps=50):
        '''Return a tuple of arrays x, y that can be fed to plt.plot,
        where x is the scale in GeV and y is the parameter of interest.
        Parameters:
        - key: dicionary key of the parameter to be plotted (e.g. a WCxf
          coefficient name or a SM parameter like 'g')
        - part: plot the real part 're' (default) or the imaginary part 'im'
        - scale: 'log'; make the x steps logarithmically distributed; for
          'linear', linearly distributed
        - steps: steps in x to take (default: 50)
        '''
        pass

    def plotdata(self, key, part='re', scale='log', steps=50):
        '''Plot the RG evolution of parameter `key`.
        Parameters:
        - part, scale, steps: see `plotdata`
        - legend: boolean, show the legend (default: True)
        - plotargs: dictionary of arguments to be passed to plt.plot
        '''
        pass",snippet_258,"import numpy as np

class RGsolution:
    '''Class representing a continuous (interpolated) solution to the
    SMEFT RGEs to be used for plotting.'''

    def __init__(self, fun, scale_min, scale_max):
        '''Initialize.
        Parameters:
        - fun: function of the scale that is expected to return a
        dictionary with the RGE solution and to accept vectorized input.
        - scale_min, scale_max: lower and upper boundaries of the scale
        '''
        if not callable(fun):
            raise TypeError(""fun must be callable"")
        try:
            smin = float(scale_min)
            smax = float(scale_max)
        except Exception as e:
            raise TypeError(""scale_min and scale_max must be numeric"") from e
        if not np.isfinite(smin) or not np.isfinite(smax):
            raise ValueError(""scale_min and scale_max must be finite numbers"")
        if smax <= smin:
            raise ValueError(""scale_max must be greater than scale_min"")

        self.fun = fun
        self.scale_min = smin
        self.scale_max = smax

    def _compute_xy(self, key, part='re', scale='log', steps=50):
        if part not in ('re', 'im'):
            raise ValueError(""part must be 're' or 'im'"")
        if scale not in ('log', 'linear'):
            raise ValueError(""scale must be 'log' or 'linear'"")
        try:
            steps = int(steps)
        except Exception as e:
            raise TypeError(""steps must be an integer"") from e
        if steps < 2:
            raise ValueError(""steps must be at least 2"")

        if scale == 'log':
            if self.scale_min <= 0 or self.scale_max <= 0:
                raise ValueError(""Log scale requires positive scale_min and scale_max"")
            x = np.logspace(np.log10(self.scale_min), np.log10(self.scale_max), steps)
        else:
            x = np.linspace(self.scale_min, self.scale_max, steps)

        res = self.fun(x)
        if not isinstance(res, dict):
            raise TypeError(""fun must return a dictionary"")
        if key not in res:
            raise KeyError(f""Key '{key}' not found in fun(x) result"")
        y = np.asarray(res[key])

        # Support broadcasting scalar outputs or 1D arrays aligned with x
        if y.shape == ():
            y = np.full_like(x, fill_value=y, dtype=np.result_type(y, np.float64))
        elif y.shape != x.shape:
            try:
                y = np.broadcast_to(y, x.shape)
            except Exception:
                raise ValueError(f""Output for key '{key}' has incompatible shape {y.shape}; expected broadcastable to {x.shape}"")

        if np.iscomplexobj(y):
            y = np.real(y) if part == 're' else np.imag(y)
        else:
            if part == 'im':
                # real input; imaginary part is zero
                y = np.zeros_like(x, dtype=float)

        y = np.asarray(y, dtype=float)
        return x, y

    def plotdata(self, key, part='re', scale='log', steps=50):
        '''Return a tuple of arrays x, y that can be fed to plt.plot,
        where x is the scale in GeV and y is the parameter of interest.
        Parameters:
        - key: dicionary key of the parameter to be plotted (e.g. a WCxf
          coefficient name or a SM parameter like 'g')
        - part: plot the real part 're' (default) or the imaginary part 'im'
        - scale: 'log'; make the x steps logarithmically distributed; for
          'linear', linearly distributed
        - steps: steps in x to take (default: 50)
        '''
        return self._compute_xy(key=key, part=part, scale=scale, steps=steps)

    def plotdata(self, key, part='re', scale='log', steps=50):
        '''Plot the RG evolution of parameter `key`.
        Parameters:
        - part, scale, steps: see `plotdata`
        - legend: boolean, show the legend (default: True)
        - plotargs: dictionary of arguments to be passed to plt.plot
        '''
        # Despite the docstring, this method returns data suitable for plotting.
        # The signature provided does not allow passing plotting kwargs here.
        return self._compute_xy(key=key, part=part, scale=scale, steps=steps)"
322803,wilson-eft/wilson,wilson-eft_wilson/wilson/wcxf/classes.py,wilson.wcxf.classes.WCxf,"import yaml

class WCxf:
    """"""Base class for WCxf files (not meant to be used directly).""""""

    @classmethod
    def load(cls, stream, **kwargs):
        """"""Load the object data from a JSON or YAML file.""""""
        wcxf = _load_yaml_json(stream, **kwargs)
        return cls(**wcxf)

    def dump(self, stream=None, fmt='json', **kwargs):
        """"""Dump the object data to a JSON or YAML file.

        Optional arguments:

        - `stream`: if None (default), return a string. Otherwise,
          should be a writable file-like object
        - `fmt`: format, should be 'json' (default) or 'yaml'

        Additional keyword arguments will be passed to the `json.dump(s)`
        or `yaml.dump` methods.
        """"""
        d = {k: v for k, v in self.__dict__.items() if k[0] != '_'}
        if fmt.lower() == 'json':
            indent = kwargs.pop('indent', 2)
            return _dump_json(d, stream=stream, indent=indent, **kwargs)
        elif fmt.lower() == 'yaml':
            default_flow_style = kwargs.pop('default_flow_style', False)
            return yaml.dump(d, stream, default_flow_style=default_flow_style, **kwargs)
        else:
            raise ValueError(f""Format {fmt} unknown: use 'json' or 'yaml'."")","
class WCxf:
    '''Base class for WCxf files (not meant to be used directly).'''
    @classmethod
    def load(cls, stream, **kwargs):
        '''Load the object data from a JSON or YAML file.'''
        pass

    def dump(self, stream=None, fmt='json', **kwargs):
        '''Dump the object data to a JSON or YAML file.
        Optional arguments:
        - `stream`: if None (default), return a string. Otherwise,
          should be a writable file-like object
        - `fmt`: format, should be 'json' (default) or 'yaml'
        Additional keyword arguments will be passed to the `json.dump(s)`
        or `yaml.dump` methods.
        '''
        pass",snippet_259,"import json
import io
import os

class WCxf:
    '''Base class for WCxf files (not meant to be used directly).'''
    @classmethod
    def load(cls, stream, **kwargs):
        '''Load the object data from a JSON or YAML file.'''
        text = None
        close_after = False

        if hasattr(stream, 'read'):
            text = stream.read()
        elif isinstance(stream, (str, os.PathLike)):
            f = open(stream, 'r', encoding=kwargs.pop('encoding', 'utf-8'))
            try:
                text = f.read()
            finally:
                f.close()
        elif isinstance(stream, (bytes, bytearray)):
            text = stream.decode(kwargs.pop('encoding', 'utf-8'))
        else:
            raise TypeError(""stream must be a readable file-like object, path-like, str, or bytes"")

        # Try JSON first
        data = None
        try:
            data = json.loads(text, **kwargs)
        except Exception:
            # Try YAML if available
            try:
                import yaml
            except Exception as e:
                raise ValueError(""Input is not valid JSON and PyYAML is not available to parse YAML"") from e
            data = yaml.safe_load(text)

        if hasattr(cls, 'from_dict') and callable(getattr(cls, 'from_dict')):
            return cls.from_dict(data)
        try:
            return cls(**data) if isinstance(data, dict) else cls(data)
        except Exception:
            return cls(data) if not isinstance(data, dict) else cls(**data)

    def dump(self, stream=None, fmt='json', **kwargs):
        '''Dump the object data to a JSON or YAML file.
        Optional arguments:
        - `stream`: if None (default), return a string. Otherwise,
          should be a writable file-like object
        - `fmt`: format, should be 'json' (default) or 'yaml'
        Additional keyword arguments will be passed to the `json.dump(s)`
        or `yaml.dump` methods.
        '''
        if hasattr(self, 'to_dict') and callable(getattr(self, 'to_dict')):
            data = self.to_dict()
        else:
            data = getattr(self, '__dict__', self)

        if fmt not in ('json', 'yaml'):
            raise ValueError(""fmt must be 'json' or 'yaml'"")

        if fmt == 'json':
            if stream is None:
                return json.dumps(data, **kwargs)
            else:
                if isinstance(stream, (str, os.PathLike)):
                    with open(stream, 'w', encoding=kwargs.pop('encoding', 'utf-8')) as f:
                        json.dump(data, f, **kwargs)
                elif hasattr(stream, 'write'):
                    json.dump(data, stream, **kwargs)
                else:
                    raise TypeError(""stream must be a writable file-like object or path-like"")
                return None

        # YAML
        try:
            import yaml
        except Exception as e:
            raise ImportError(""PyYAML is required for YAML output"") from e

        if stream is None:
            return yaml.safe_dump(data, **kwargs)
        else:
            if isinstance(stream, (str, os.PathLike)):
                with open(stream, 'w', encoding=kwargs.pop('encoding', 'utf-8')) as f:
                    yaml.safe_dump(data, f, **kwargs)
            elif hasattr(stream, 'write'):
                yaml.safe_dump(data, stream, **kwargs)
            else:
                raise TypeError(""stream must be a writable file-like object or path-like"")
            return None"
322855,jhermann/rituals,jhermann_rituals/src/rituals/util/antglob.py,rituals.util.antglob.Pattern,"class Pattern:
    """"""A single pattern for either inclusion or exclusion.""""""

    def __init__(self, spec, inclusive):
        """"""Create regex-based pattern matcher from glob `spec`.""""""
        self.compiled = compile_glob(spec.rstrip('/'))
        self.inclusive = inclusive
        self.is_dir = spec.endswith('/')

    def __str__(self):
        """"""Return inclusiveness indicator and original glob pattern.""""""
        return ('+' if self.inclusive else '-') + self.compiled.pattern

    def matches(self, path):
        """"""Check this pattern against given `path`.""""""
        return bool(self.compiled.match(path))","class Pattern:
    '''A single pattern for either inclusion or exclusion.'''

    def __init__(self, spec, inclusive):
        '''Create regex-based pattern matcher from glob `spec`.'''
        pass

    def __str__(self):
        '''Return inclusiveness indicator and original glob pattern.'''
        pass

    def matches(self, path):
        '''Check this pattern against given `path`.'''
        pass",snippet_260,"import re
import fnmatch

class Pattern:
    '''A single pattern for either inclusion or exclusion.'''

    def __init__(self, spec, inclusive):
        '''Create regex-based pattern matcher from glob `spec`.'''
        self.spec = spec
        self.inclusive = bool(inclusive)
        self._regex = re.compile(fnmatch.translate(spec))

    def __str__(self):
        '''Return inclusiveness indicator and original glob pattern.'''
        return f""{'+' if self.inclusive else '-'}{self.spec}""

    def matches(self, path):
        '''Check this pattern against given `path`.'''
        return self._regex.match(path) is not None"
323184,SoftwareDefinedBuildings/XBOS,apps/Data_quality_analysis/Import_Data.py,Data_quality_analysis.Import_Data.Import_Data,"import pandas as pd
import glob
import numpy as np
import os

class Import_Data:
    """""" This class imports data from csv files. """"""

    def __init__(self):
        """""" Constructor: Store the imported data. """"""
        self.data = pd.DataFrame()

    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        """""" Imports csv file(s) and stores the result in data.

        Note
        ----
        1. If folder exists out of current directory, folder_name should contain correct regex
        2. Assuming there's no file called ""\\*.csv""

        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '\\*', i.e. all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.', i.e. current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe

        """"""
        if isinstance(file_name, str) and isinstance(folder_name, str):
            try:
                self.data = self._load_csv(file_name, folder_name, head_row, index_col, convert_col, concat_files)
            except Exception as e:
                raise e
        elif isinstance(file_name, list) and isinstance(folder_name, str):
            for i, file in enumerate(file_name):
                if isinstance(head_row, list):
                    _head_row = head_row[i]
                else:
                    _head_row = head_row
                if isinstance(index_col, list):
                    _index_col = index_col[i]
                else:
                    _index_col = index_col
                try:
                    data_tmp = self._load_csv(file, folder_name, _head_row, _index_col, convert_col, concat_files)
                    if concat_files:
                        self.data = self.data.append(data_tmp, ignore_index=False, verify_integrity=False)
                    else:
                        self.data = self.data.join(data_tmp, how='outer')
                except Exception as e:
                    raise e
        else:
            raise NotImplementedError(""Filename and Folder name can't both be of type list."")

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        """""" Load single csv file.

        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '*' - all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.' - current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe

        Returns
        -------
        pd.DataFrame()
            Dataframe containing csv data

        """"""
        if file_name == '*':
            if not os.path.isdir(folder_name):
                raise OSError('Folder does not exist.')
            else:
                file_name_list = sorted(glob.glob(folder_name + '*.csv'))
                if not file_name_list:
                    raise OSError('Either the folder does not contain any csv files or invalid folder provided.')
                else:
                    self.import_csv(file_name=file_name_list, head_row=head_row, index_col=index_col, convert_col=convert_col, concat_files=concat_files)
                    return self.data
        elif not os.path.isdir(folder_name):
            raise OSError('Folder does not exist.')
        else:
            path = os.path.join(folder_name, file_name)
            if head_row > 0:
                data = pd.read_csv(path, index_col=index_col, skiprows=[i for i in range(head_row - 1)])
            else:
                data = pd.read_csv(path, index_col=index_col)
            try:
                data.index = pd.to_datetime(data.index, format='%m/%d/%y %H:%M')
            except:
                data.index = pd.to_datetime(data.index, dayfirst=False, infer_datetime_format=True)
        if convert_col:
            for col in data.columns:
                if data[col].dtype != np.number:
                    data[col] = pd.to_numeric(data[col], errors='coerce')
        return data","
class Import_Data:
    ''' This class imports data from csv files. '''

    def __init__(self):
        ''' Constructor: Store the imported data. '''
        pass

    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        ''' Imports csv file(s) and stores the result in data.
        Note
        ----
        1. If folder exists out of current directory, folder_name should contain correct regex
        2. Assuming there's no file called ""\*.csv""
        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '\*', i.e. all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.', i.e. current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe
        '''
        pass

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        ''' Load single csv file.
        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '*' - all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.' - current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe
        Returns
        -------
        pd.DataFrame()
            Dataframe containing csv data
        '''
        pass",snippet_261,"import os
import glob
import pandas as pd


class Import_Data:
    ''' This class imports data from csv files. '''

    def __init__(self):
        ''' Constructor: Store the imported data. '''
        self.data = None
        self.files_loaded = []

    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        ''' Imports csv file(s) and stores the result in data.
        Note
        ----
        1. If folder exists out of current directory, folder_name should contain correct regex
        2. Assuming there's no file called ""\*.csv""
        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '\*', i.e. all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.', i.e. current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe
        '''
        # Build glob pattern supporting wildcards in both folder and file
        if file_name.endswith('.csv'):
            pattern = os.path.join(folder_name, file_name)
        else:
            pattern = os.path.join(folder_name, f'{file_name}.csv')

        file_list = sorted(glob.glob(pattern))
        if not file_list:
            raise FileNotFoundError(f'No CSV files match pattern: {pattern}')

        self.files_loaded = file_list[:]

        if concat_files:
            frames = [
                self._load_csv(f, folder_name=None, head_row=head_row, index_col=index_col,
                               convert_col=convert_col, concat_files=True)
                for f in file_list
            ]
            self.data = pd.concat(frames, ignore_index=True, sort=False)
        else:
            data_dict = {}
            for f in file_list:
                df = self._load_csv(f, folder_name=None, head_row=head_row, index_col=index_col,
                                    convert_col=convert_col, concat_files=False)
                data_dict[os.path.basename(f)] = df
            # If only one file and not concatenating, store the DataFrame directly
            self.data = next(iter(data_dict.values())) if len(data_dict) == 1 else data_dict

        return self.data

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        ''' Load single csv file.
        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '*' - all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.' - current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe
        Returns
        -------
        pd.DataFrame()
            Dataframe containing csv data
        '''
        # Resolve full path if folder_name provided; otherwise assume file_name is a full path/pattern
        path = os.path.join(folder_name, file_name) if folder_name else file_name

        # Read CSV, skipping leading rows; treat the first remaining row as header
        df = pd.read_csv(path, header=0, skiprows=head_row)

        # Drop the first index_col columns (keep columns from index_col onward)
        if index_col and index_col > 0:
            df = df.iloc[:, index_col:].copy()

        # Optionally attempt to convert columns to numeric where possible
        if convert_col and not df.empty:
            for col in df.columns:
                # Try converting; if conversion fails for any value, keep original
                converted = pd.to_numeric(df[col], errors='ignore')
                df[col] = converted

        return df"
323187,SoftwareDefinedBuildings/XBOS,apps/Data_quality_analysis/Plot_Data.py,Data_quality_analysis.Plot_Data.Plot_Data,"import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

class Plot_Data:
    """""" This class contains functions for displaying various plots.

    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.

    """"""
    count = 1

    def __init__(self, figsize=(18, 5)):
        """""" Constructor.

        Parameters
        ----------
        figsize : tuple
            Size of figure.

        """"""
        self.figsize = figsize

    def correlation_plot(self, data):
        """""" Create heatmap of Pearson's correlation coefficient.

        Parameters
        ----------
        data    : pd.DataFrame()
            Data to display.

        Returns
        -------
        matplotlib.figure
            Heatmap.

        """"""
        fig = plt.figure(Plot_Data.count)
        corr = data.corr()
        ax = sns.heatmap(corr)
        Plot_Data.count += 1
        return fig

    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        """""" Create baseline and projection plots.

        Parameters
        ----------
        y_true              : pd.Series()
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.Dataframe()
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.

        Returns
        -------
        matplotlib.figure
            Baseline plot

        """"""
        fig = plt.figure(Plot_Data.count)
        if projection_period:
            nrows = len(baseline_period) + len(projection_period) / 2
        else:
            nrows = len(baseline_period) / 2
        base_df = pd.DataFrame()
        base_df['y_true'] = y_true
        base_df['y_pred'] = y_pred
        ax1 = fig.add_subplot(nrows, 1, 1)
        base_df.plot(ax=ax1, figsize=self.figsize, title='Baseline Period ({}-{}). \nBest Model: {}. \nBaseline Adj R2: {}. \nSite: {}.'.format(baseline_period[0], baseline_period[1], model_name, adj_r2, site))
        if projection_period:
            num_plot = 2
            for i in range(0, len(projection_period), 2):
                ax = fig.add_subplot(nrows, 1, num_plot)
                period = slice(projection_period[i], projection_period[i + 1])
                project_df = pd.DataFrame()
                try:
                    project_df['y_true'] = data.loc[period, output_col]
                    project_df['y_pred'] = model.predict(data.loc[period, input_col])
                    project_df['y_pred'][project_df['y_pred'] < 0] = 0
                    project_df.plot(ax=ax, figsize=self.figsize, title='Projection Period ({}-{})'.format(projection_period[i], projection_period[i + 1]))
                    num_plot += 1
                    fig.tight_layout()
                    Plot_Data.count += 1
                    return (fig, project_df['y_true'], project_df['y_pred'])
                except:
                    raise TypeError('If projecting into the future, please specify project_ind_col that has data available                                         in the future time period requested.')
        return (fig, None, None)","
class Plot_Data:
    ''' This class contains functions for displaying various plots.
    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.
    '''

    def __init__(self, figsize=(18, 5)):
        ''' Constructor.
        Parameters
        ----------
        figsize : tuple
            Size of figure.
        '''
        pass

    def correlation_plot(self, data):
        ''' Create heatmap of Pearson's correlation coefficient.
        Parameters
        ----------
        data    : pd.DataFrame()
            Data to display.
        Returns
        -------
        matplotlib.figure
            Heatmap.
        '''
        pass

    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        ''' Create baseline and projection plots.
        Parameters
        ----------
        y_true              : pd.Series()
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.Dataframe()
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.
        Returns
        -------
        matplotlib.figure
            Baseline plot
        '''
        pass",snippet_262,"import warnings
from typing import List, Tuple, Optional, Union

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

try:
    import seaborn as sns
    _HAS_SNS = True
except Exception:
    _HAS_SNS = False


class Plot_Data:
    ''' This class contains functions for displaying various plots.
    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.
    '''

    def __init__(self, figsize: Tuple[float, float] = (18, 5)):
        ''' Constructor.
        Parameters
        ----------
        figsize : tuple
            Size of figure.
        '''
        self.figsize = figsize
        self.count = 0

    def correlation_plot(self, data: pd.DataFrame):
        ''' Create heatmap of Pearson's correlation coefficient.
        Parameters
        ----------
        data    : pd.DataFrame()
            Data to display.
        Returns
        -------
        matplotlib.figure
            Heatmap.
        '''
        if not isinstance(data, pd.DataFrame):
            raise TypeError(""data must be a pandas DataFrame."")
        if data.empty:
            raise ValueError(""data must not be empty."")

        corr = data.corr(method='pearson', numeric_only=True)

        self.count += 1
        fig, ax = plt.subplots(figsize=self.figsize)
        if _HAS_SNS:
            sns = __import__(""seaborn"")
            heat = sns.heatmap(
                corr,
                ax=ax,
                annot=True,
                fmt="".2f"",
                cmap=""coolwarm"",
                square=True,
                cbar_kws={""shrink"": 0.8},
                linewidths=0.5,
                linecolor='white'
            )
            heat.set_title(""Pearson Correlation Heatmap"")
        else:
            cax = ax.imshow(corr.values, cmap=""coolwarm"", vmin=-1, vmax=1)
            fig.colorbar(cax, ax=ax, shrink=0.8)
            ax.set_xticks(range(len(corr.columns)))
            ax.set_yticks(range(len(corr.index)))
            ax.set_xticklabels(corr.columns, rotation=45, ha=""right"")
            ax.set_yticklabels(corr.index)
            ax.set_title(""Pearson Correlation Heatmap"")
            # Add annotations
            for (i, j), val in np.ndenumerate(corr.values):
                ax.text(j, i, f""{val:.2f}"", ha=""center"", va=""center"", color=""black"")

        fig.tight_layout()
        return fig

    def baseline_projection_plot(
        self,
        y_true: pd.Series,
        y_pred: np.ndarray,
        baseline_period: List[str],
        projection_period: List[str],
        model_name: str,
        adj_r2: float,
        data: pd.DataFrame,
        input_col: List[str],
        output_col: str,
        model,
        site: Optional[str] = None
    ):
        ''' Create baseline and projection plots.
        Parameters
        ----------
        y_true              : pd.Series()
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.Dataframe()
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.
        Returns
        -------
        matplotlib.figure
            Baseline plot
        '''
        if not isinstance(y_true, pd.Series):
            raise TypeError(""y_true must be a pandas Series."")
        if not isinstance(y_pred, (np.ndarray, list, pd.Series)):
            raise TypeError(""y_pred must be an array-like of predictions."")
        y_pred = np.asarray(y_pred)

        if y_true.empty:
            raise ValueError(""y_true must not be empty."")
        if y_true.index is None or not isinstance(y_true.index, pd.Index):
            raise ValueError(""y_true must have an index (ideally DatetimeIndex)."")

        if len(baseline_period) != 2 or len(projection_period) != 2:
            raise ValueError(""baseline_period and projection_period must be [start, end]."")

        # Convert periods to timestamps if possible
        def _to_ts(v):
            try:
                return pd.to_datetime(v)
            except Exception:
                return v

        b_start, b_end = map(_to_ts, baseline_period)
        p_start, p_end = map(_to_ts, projection_period)

        # Align baseline true/pred
        if len(y_pred) != len(y_true):
            warnings.warn(""Length of y_pred does not match y_true. Aligning to min length."")
        n = min(len(y_true), len(y_pred))
        y_true_baseline = y_true.iloc[:n]
        y_pred_baseline = pd.Series(y_pred[:n], index=y_true_baseline.index, name=""Prediction"")

        # Restrict to baseline window if index is slicable
        try:
            y_true_baseline = y_true_baseline.loc[b_start:b_end]
            y_pred_baseline = y_pred_baseline.loc[b_start:b_end]
        except Exception:
            pass

        # Prepare projection data
        if not isinstance(data, pd.DataFrame):
            raise TypeError(""data must be a pandas DataFrame."")
        for col in input_col + [output_col]:
            if col not in data.columns:
                raise ValueError(f""Column '{col}' not found in data."")

        try:
            data_proj = data.loc[p_start:p_end]
        except Exception:
            # if index is not datetime-like, try boolean mask using a 'date' column if present
            if 'date' in data.columns:
                dates = pd.to_datetime(data['date'], errors='coerce')
                mask = (dates >= pd.to_datetime(p_start)) & (dates <= pd.to_datetime(p_end))
                data_proj = data.loc[mask]
            else:
                data_proj = data.copy()

        X_proj = data_proj[input_col]
        y_proj_true = data_proj[output_col]

        # Predict using provided model
        try:
            if hasattr(model, ""predict""):
                y_proj_pred_vals = model.predict(X_proj)
            else:
                y_proj_pred_vals = model(X_proj)
        except Exception as e:
            warnings.warn(f""Projection prediction failed with error: {e}. Using NaNs."")
            y_proj_pred_vals = np.full(len(X_proj), np.nan)

        y_proj_pred = pd.Series(np.asarray(y_proj_pred_vals).ravel(), index=X_proj.index, name=""Prediction"")

        # Plot
        self.count += 1
        fig, axes = plt.subplots(1, 2, figsize=self.figsize, sharey=False)

        ax0 = axes[0]
        ax0.plot(y_true_baseline.index, y_true_baseline.values, label=""Actual"", color=""#1f77b4"", linewidth=2)
        ax0.plot(y_pred_baseline.index, y_pred_baseline.values, label=""Predicted"", color=""#ff7f0e"", linewidth=2, alpha=0.9)
        ax0.set_title(""Baseline"")
        ax0.set_xlabel(""Time"")
        ax0.set_ylabel(output_col if isinstance(output_col, str) else ""Value"")
        ax0.legend()
        ax0.grid(True, alpha=0.3)

        ax1 = axes[1]
        ax1.plot(y_proj_true.index, y_proj_true.values, label=""Actual"", color=""#1f77b4"", linewidth=2)
        ax1.plot(y_proj_pred.index, y_proj_pred.values, label=""Predicted"", color=""#ff7f0e"", linewidth=2, alpha=0.9)
        ax1.set_title(""Projection"")
        ax1.set_xlabel(""Time"")
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        site_str = f"" | Site: {site}"" if site else """"
        fig.suptitle(f""{model_name}{site_str} | Adj. R2 = {adj_r2:.3f}"", fontsize=14)
        fig.tight_layout(rect=[0, 0.03, 1, 0.95])

        return fig"
323204,SoftwareDefinedBuildings/XBOS,apps/data_analysis/XBOS_data_analytics/Import_Data.py,XBOS_data_analytics.Import_Data.Import_Data,"import os
import pandas as pd
import numpy as np
import glob

class Import_Data:
    """""" This class imports data from csv files """"""

    def __init__(self):
        """""" Constructor.

        This class stores the imported data.

        """"""
        self.data = pd.DataFrame()

    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        """""" Imports csv file(s) and stores the result in data.

        Note
        ----
        1. If folder exists out of current directory, folder_name should contain correct regex
        2. Assuming there's no file called ""\\*.csv""

        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '\\*', i.e. all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.', i.e. current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe

        """"""
        if isinstance(file_name, str) and isinstance(folder_name, str):
            try:
                self.data = self._load_csv(file_name, folder_name, head_row, index_col, convert_col, concat_files)
            except Exception as e:
                raise e
        elif isinstance(file_name, list) and isinstance(folder_name, str):
            for i, file in enumerate(file_name):
                if isinstance(head_row, list):
                    _head_row = head_row[i]
                else:
                    _head_row = head_row
                if isinstance(index_col, list):
                    _index_col = index_col[i]
                else:
                    _index_col = index_col
                try:
                    data_tmp = self._load_csv(file, folder_name, _head_row, _index_col, convert_col, concat_files)
                    if concat_files:
                        self.data = self.data.append(data_tmp, ignore_index=False, verify_integrity=False)
                    else:
                        self.data = self.data.join(data_tmp, how='outer')
                except Exception as e:
                    raise e
        else:
            raise NotImplementedError(""Filename and Folder name can't both be of type list."")

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        """""" Load single csv file.

        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '*' - all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.' - current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe

        Returns
        -------
        pd.DataFrame()
            Dataframe containing csv data

        """"""
        if file_name == '*':
            if not os.path.isdir(folder_name):
                raise OSError('Folder does not exist.')
            else:
                file_name_list = sorted(glob.glob(folder_name + '*.csv'))
                if not file_name_list:
                    raise OSError('Either the folder does not contain any csv files or invalid folder provided.')
                else:
                    self.import_csv(file_name=file_name_list, head_row=head_row, index_col=index_col, convert_col=convert_col, concat_files=concat_files)
                    return self.data
        elif not os.path.isdir(folder_name):
            raise OSError('Folder does not exist.')
        else:
            path = os.path.join(folder_name, file_name)
            if head_row > 0:
                data = pd.read_csv(path, index_col=index_col, skiprows=[i for i in range(head_row - 1)])
            else:
                data = pd.read_csv(path, index_col=index_col)
            try:
                data.index = pd.to_datetime(data.index, format='%m/%d/%y %H:%M')
            except:
                data.index = pd.to_datetime(data.index, dayfirst=False, infer_datetime_format=True)
        if convert_col:
            for col in data.columns:
                if data[col].dtype != np.number:
                    data[col] = pd.to_numeric(data[col], errors='coerce')
        return data","
class Import_Data:
    ''' This class imports data from csv files '''

    def __init__(self):
        ''' Constructor.
        This class stores the imported data.
        '''
        pass

    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        ''' Imports csv file(s) and stores the result in data.
        Note
        ----
        1. If folder exists out of current directory, folder_name should contain correct regex
        2. Assuming there's no file called ""\*.csv""
        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '\*', i.e. all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.', i.e. current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe
        '''
        pass

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        ''' Load single csv file.
        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '*' - all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.' - current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe
        Returns
        -------
        pd.DataFrame()
            Dataframe containing csv data
        '''
        pass",snippet_263,"import os
import glob
import pandas as pd


class Import_Data:
    ''' This class imports data from csv files '''

    def __init__(self):
        ''' Constructor.
        This class stores the imported data.
        '''
        self.data = None

    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        ''' Imports csv file(s) and stores the result in data.
        Note
        ----
        1. If folder exists out of current directory, folder_name should contain correct regex
        2. Assuming there's no file called ""\*.csv""
        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '\*', i.e. all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.', i.e. current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe
        '''
        # Build glob pattern
        fn = file_name
        if fn == '*' or '*' in fn:
            pattern = fn if fn.endswith('.csv') or fn.endswith('*.csv') else (fn + '.csv')
        else:
            pattern = fn if fn.lower().endswith('.csv') else (fn + '.csv')

        search_pattern = os.path.join(folder_name, pattern)
        file_list = sorted(glob.glob(search_pattern))

        if not file_list:
            raise FileNotFoundError(f""No CSV files matched pattern: {search_pattern}"")

        if concat_files:
            frames = [
                self._load_csv(f, folder_name='', head_row=head_row, index_col=index_col,
                               convert_col=convert_col, concat_files=concat_files)
                for f in file_list
            ]
            result = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()
            self.data = result
            return result
        else:
            # If only one file, store DataFrame directly, else store dict of DataFrames keyed by file path
            if len(file_list) == 1:
                df = self._load_csv(file_list[0], folder_name='', head_row=head_row, index_col=index_col,
                                    convert_col=convert_col, concat_files=concat_files)
                self.data = df
                return df
            else:
                data_dict = {}
                for f in file_list:
                    data_dict[f] = self._load_csv(f, folder_name='', head_row=head_row, index_col=index_col,
                                                  convert_col=convert_col, concat_files=concat_files)
                self.data = data_dict
                return data_dict

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        ''' Load single csv file.
        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '*' - all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.' - current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe
        Returns
        -------
        pd.DataFrame()
            Dataframe containing csv data
        '''
        # Determine full path
        if folder_name:
            path = os.path.join(folder_name, file_name)
        else:
            path = file_name

        # Read CSV: skip first head_row lines; treat the next line as header
        df = pd.read_csv(path, header=0, skiprows=head_row)

        # Drop the first index_col columns
        if index_col and index_col > 0:
            df = df.iloc[:, index_col:]

        # Convert columns to numeric where possible
        if convert_col and not df.empty:
            for col in df.columns:
                # Try numeric conversion; if fails, leave as-is
                converted = pd.to_numeric(df[col], errors='ignore')
                df[col] = converted

        return df"
323208,SoftwareDefinedBuildings/XBOS,apps/data_analysis/XBOS_data_analytics/Plot_Data.py,XBOS_data_analytics.Plot_Data.Plot_Data,"import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

class Plot_Data:
    """""" This class contains functions for displaying various plots.

    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.

    """"""
    count = 1

    def __init__(self, figsize=(18, 5)):
        """""" Constructor.

        Parameters
        ----------
        figsize : tuple
            Size of figure.

        """"""
        self.figsize = figsize

    def correlation_plot(self, data):
        """""" Create heatmap of Pearson's correlation coefficient.

        Parameters
        ----------
        data    : pd.DataFrame()
            Data to display.

        Returns
        -------
        matplotlib.figure
            Heatmap.

        """"""
        fig = plt.figure(Plot_Data.count)
        corr = data.corr()
        ax = sns.heatmap(corr)
        Plot_Data.count += 1
        return fig

    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        """""" Create baseline and projection plots.

        Parameters
        ----------
        y_true              : pd.Series()
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.Dataframe()
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.

        Returns
        -------
        matplotlib.figure
            Baseline plot

        """"""
        fig = plt.figure(Plot_Data.count)
        if projection_period:
            nrows = len(baseline_period) + len(projection_period) / 2
        else:
            nrows = len(baseline_period) / 2
        base_df = pd.DataFrame()
        base_df['y_true'] = y_true
        base_df['y_pred'] = y_pred
        ax1 = fig.add_subplot(nrows, 1, 1)
        base_df.plot(ax=ax1, figsize=self.figsize, title='Baseline Period ({}-{}). \nBest Model: {}. \nBaseline Adj R2: {}. \nSite: {}.'.format(baseline_period[0], baseline_period[1], model_name, adj_r2, site))
        if projection_period:
            num_plot = 2
            for i in range(0, len(projection_period), 2):
                ax = fig.add_subplot(nrows, 1, num_plot)
                period = slice(projection_period[i], projection_period[i + 1])
                project_df = pd.DataFrame()
                try:
                    project_df['y_true'] = data.loc[period, output_col]
                    project_df['y_pred'] = model.predict(data.loc[period, input_col])
                    project_df['y_pred'][project_df['y_pred'] < 0] = 0
                    project_df.plot(ax=ax, figsize=self.figsize, title='Projection Period ({}-{})'.format(projection_period[i], projection_period[i + 1]))
                    num_plot += 1
                    fig.tight_layout()
                    Plot_Data.count += 1
                    return (fig, project_df['y_true'], project_df['y_pred'])
                except:
                    raise SystemError('If projecting into the future, please specify project_ind_col that has data available                                         in the future time period requested.')
        return (fig, None, None)","
class Plot_Data:
    ''' This class contains functions for displaying various plots.
    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.
    '''

    def __init__(self, figsize=(18, 5)):
        ''' Constructor.
        Parameters
        ----------
        figsize : tuple
            Size of figure.
        '''
        pass

    def correlation_plot(self, data):
        ''' Create heatmap of Pearson's correlation coefficient.
        Parameters
        ----------
        data    : pd.DataFrame()
            Data to display.
        Returns
        -------
        matplotlib.figure
            Heatmap.
        '''
        pass

    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        ''' Create baseline and projection plots.
        Parameters
        ----------
        y_true              : pd.Series()
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.Dataframe()
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.
        Returns
        -------
        matplotlib.figure
            Baseline plot
        '''
        pass",snippet_264,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.dates import AutoDateLocator, AutoDateFormatter


class Plot_Data:
    ''' This class contains functions for displaying various plots.
    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.
    '''

    def __init__(self, figsize=(18, 5)):
        ''' Constructor.
        Parameters
        ----------
        figsize : tuple
            Size of figure.
        '''
        self.figsize = figsize
        self.count = 0
        try:
            sns.set_style(""whitegrid"")
        except Exception:
            pass

    def correlation_plot(self, data):
        ''' Create heatmap of Pearson's correlation coefficient.
        Parameters
        ----------
        data    : pd.DataFrame()
            Data to display.
        Returns
        -------
        matplotlib.figure
            Heatmap.
        '''
        if not isinstance(data, pd.DataFrame):
            data = pd.DataFrame(data)

        num = data.select_dtypes(include=[np.number])
        if num.empty:
            raise ValueError(""No numeric columns found for correlation heatmap."")

        corr = num.corr(method='pearson')

        fig, ax = plt.subplots(figsize=self.figsize, constrained_layout=True)
        sns.heatmap(
            corr,
            annot=True,
            fmt="".2f"",
            cmap=""coolwarm"",
            center=0,
            square=True,
            cbar_kws={""shrink"": 0.8},
            ax=ax
        )
        ax.set_title(""Pearson Correlation Heatmap"", fontsize=12)
        self.count += 1
        return fig

    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        ''' Create baseline and projection plots.
        Parameters
        ----------
        y_true              : pd.Series()
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.Dataframe()
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.
        Returns
        -------
        matplotlib.figure
            Baseline plot
        '''
        # Prepare time index
        if isinstance(data.index, pd.DatetimeIndex):
            x_all = data.index
        elif ""date"" in data.columns:
            x_all = pd.to_datetime(data[""date""])
        else:
            # Fallback to a simple integer index if no date information
            x_all = pd.RangeIndex(start=0, stop=len(data), step=1)

        # Ensure Series alignment
        y_true_series = y_true if isinstance(y_true, pd.Series) else pd.Series(y_true, index=x_all[:len(y_true)])
        # y_pred may be array; align later

        def _to_periods(spec):
            # Accept [""start"", ""end""] or [[""s1"",""e1""], [""s2"",""e2""], ...]
            if spec is None:
                return []
            if isinstance(spec, (tuple, list)) and len(spec) > 0 and isinstance(spec[0], (list, tuple, pd.Series, np.ndarray)):
                return [(pd.to_datetime(s[0]), pd.to_datetime(s[1])) for s in spec]
            if isinstance(spec, (list, tuple)) and len(spec) >= 2:
                return [(pd.to_datetime(spec[0]), pd.to_datetime(spec[1]))]
            if isinstance(spec, (list, tuple)) and len(spec) == 1:
                start = pd.to_datetime(spec[0])
                return [(start, start)]
            raise ValueError(""Invalid period specification."")

        baseline_ranges = _to_periods(baseline_period)
        proj_ranges = _to_periods(projection_period)

        # Build figure with two subplots: Baseline and Projection
        fig, axes = plt.subplots(1, 2, figsize=self.figsize, sharey=True, constrained_layout=True)
        ax_base, ax_proj = axes

        # Baseline plot
        if baseline_ranges:
            b_start, b_end = baseline_ranges[0]
            if isinstance(x_all, pd.DatetimeIndex):
                mask_base = (x_all >= b_start) & (x_all <= b_end)
            else:
                # If no datetime axis, assume integer positions mapping by order; approximate via entire range length
                mask_base = pd.Series([True] * len(x_all), index=np.arange(len(x_all)))
            x_base = x_all[mask_base]

            # Extract y_true for baseline
            if isinstance(y_true_series.index, pd.DatetimeIndex) and isinstance(x_all, pd.DatetimeIndex):
                y_true_base = y_true_series.loc[(y_true_series.index >= b_start) & (y_true_series.index <= b_end)]
                # If selection yields empty but lengths match mask, fallback to positional
                if y_true_base.empty and len(y_true_series) >= mask_base.sum():
                    y_true_base = pd.Series(y_true_series.values[:mask_base.sum()], index=x_base)
            else:
                # Positional alignment
                y_true_base = pd.Series(np.asarray(y_true_series)[:mask_base.sum()], index=x_base)

            # Align y_pred to baseline
            y_pred = np.asarray(y_pred)
            if len(y_pred) >= len(y_true_base):
                y_pred_base = y_pred[:len(y_true_base)]
            else:
                # Pad with nan if shorter
                pad = np.full(len(y_true_base) - len(y_pred), np.nan)
                y_pred_base = np.concatenate([y_pred, pad])

            ax_base.plot(x_base, y_true_base.values, label=""Actual"", color=""#1f77b4"", lw=1.8)
            ax_base.plot(x_base, y_pred_base, label=""Predicted"", color=""#d62728"", lw=1.8, alpha=0.9)
            if isinstance(x_all, pd.DatetimeIndex):
                ax_base.axvspan(b_start, b_end, color=""#cccccc"", alpha=0.2, label=""Baseline"")
            ax_base.set_title(""Baseline"")
            ax_base.set_xlabel(""Date"" if isinstance(x_all, pd.DatetimeIndex) else ""Index"")
            ax_base.set_ylabel(output_col if output_col else ""Value"")
            ax_base.grid(True, alpha=0.3)
            ax_base.legend(loc=""best"")
        else:
            ax_base.set_visible(False)

        # Projection plot(s)
        colors = sns.color_palette(""tab10"", max(1, len(proj_ranges)))
        for i, (p_start, p_end) in enumerate(proj_ranges):
            if isinstance(x_all, pd.DatetimeIndex):
                mask_proj = (x_all >= p_start) & (x_all <= p_end)
            else:
                mask_proj = pd.Series([False] * len(x_all))
            if not mask_proj.any():
                continue

            x_proj = x_all[mask_proj]

            # Actual values for projection if available
            y_actual_proj = None
            if output_col in data.columns:
                y_actual_proj = data.loc[mask_proj, output_col]
                # Ensure it's aligned to x_proj
                if len(y_actual_proj) != len(x_proj):
                    y_actual_proj = pd.Series(y_actual_proj.values[:len(x_proj)], index=x_proj)

            # Model predictions for projection
            y_hat_proj = None
            try:
                X_proj = data.loc[mask_proj, input_col]
                if hasattr(model, ""predict""):
                    y_hat_proj = np.asarray(model.predict(X_proj))
                elif callable(model):
                    y_hat_proj = np.asarray(model(X_proj))
                else:
                    y_hat_proj = np.full(len(x_proj), np.nan)
            except Exception:
                y_hat_proj = np.full(len(x_proj), np.nan)

            # Plot
            if y_actual_proj is not None and len(y_actual_proj) > 0:
                ax_proj.plot(x_proj, y_actual_proj.values, color=colors[i], lw=1.6, alpha=0.8, label=f""Actual ({i+1})"")
            ax_proj.plot(x_proj, y_hat_proj, color=colors[i], lw=2.0, linestyle=""--"", label=f""Predicted ({i+1})"")
            if isinstance(x_all, pd.DatetimeIndex):
                ax_proj.axvspan(p_start, p_end, color=colors[i], alpha=0.12)

        ax_proj.set_title(""Projection"")
        ax_proj.set_xlabel(""Date"" if isinstance(x_all, pd.DatetimeIndex) else ""Index"")
        ax_proj.grid(True, alpha=0.3)
        if len(ax_proj.lines) > 0:
            ax_proj.legend(loc=""best"")

        # Super title
        site_str = f"" | Site: {site}"" if site is not None else """"
        try:
            score_str = f""{adj_r2:.3f}""
        except Exception:
            score_str = str(adj_r2)
        fig.suptitle(f""Model: {model_name} | Adj. R²: {score_str}{site_str}"", fontsize=13)

        # Format dates
        if isinstance(x_all, pd.DatetimeIndex):
            for ax in [ax_base, ax_proj]:
                locator = AutoDateLocator()
                formatter = AutoDateFormatter(locator)
                ax.xaxis.set_major_locator(locator)
                ax.xaxis.set_major_formatter(formatter)
                for label in ax.get_xticklabels():
                    label.set_rotation(30)
                    label.set_ha(""right"")

        self.count += 1
        return fig"
339242,pawelad/pymonzo,pawelad_pymonzo/src/pymonzo/client.py,pymonzo.client.MonzoAPI,"from pymonzo.exceptions import MonzoAPIError, NoSettingsFile
from authlib.integrations.base_client import OAuthError
from pymonzo.webhooks import WebhooksResource
from pathlib import Path
from pymonzo.attachments import AttachmentsResource
from json import JSONDecodeError
from pymonzo.transactions import TransactionsResource
from pymonzo.pots import PotsResource
from pymonzo.whoami import WhoAmIResource
from pymonzo.utils import get_authorization_response_url
from urllib.parse import urlparse
from typing import Any, Optional
from pymonzo.balance import BalanceResource
import webbrowser
from pymonzo.feed import FeedResource
from pymonzo.accounts import AccountsResource
from authlib.integrations.httpx_client import OAuth2Client
from pymonzo.settings import PyMonzoSettings

class MonzoAPI:
    """"""Monzo public API client.

    To use it, you need to create a new OAuth client in [Monzo Developer Portal].
    The `Redirect URLs` should be set to `http://localhost:6600/pymonzo` and
    `Confidentiality` should be set to `Confidential` if you'd like to automatically
    refresh the access token when it expires.

    You can now use `Client ID` and `Client secret` in [`pymonzo.MonzoAPI.authorize`][]
    to finish the OAuth 2 'Authorization Code Flow' and get the API access token
    (which is by default saved to disk and refreshed when expired).

    [Monzo Developer Portal]: https://developers.monzo.com/

    Note:
        Monzo API docs: https://docs.monzo.com/
    """"""
    api_url = 'https://api.monzo.com'
    authorization_endpoint = 'https://auth.monzo.com/'
    token_endpoint = 'https://api.monzo.com/oauth2/token'
    settings_path = Path.home() / '.pymonzo'

    def __init__(self, access_token: Optional[str]=None) -> None:
        """"""Initialize Monzo API client and mount all resources.

        It expects [`pymonzo.MonzoAPI.authorize`][] to be called beforehand, so
        it can load the local settings file containing the API access token. You
        can also explicitly pass the `access_token`, but it won't be able to
        automatically refresh it once it expires.

        Arguments:
            access_token: OAuth access token. You can obtain it (and by default, save
                it to disk, so it can refresh automatically) by running
                [`pymonzo.MonzoAPI.authorize`][]. Alternatively, you can get a
                temporary access token from the [Monzo Developer Portal].

                [Monzo Developer Portal]: https://developers.monzo.com/

        Raises:
            NoSettingsFile: When the access token wasn't passed explicitly and the
                settings file couldn't be loaded.

        """"""
        if access_token:
            self._settings = PyMonzoSettings(token={'access_token': access_token})
        else:
            try:
                self._settings = PyMonzoSettings.load_from_disk(self.settings_path)
            except (FileNotFoundError, JSONDecodeError) as e:
                raise NoSettingsFile('No settings file found. You need to either run `MonzoAPI.authorize(client_id, client_secret)` to get the authorization token (and save it to disk), or explicitly pass the `access_token`.') from e
        self.session = OAuth2Client(client_id=self._settings.client_id, client_secret=self._settings.client_secret, token=self._settings.token, authorization_endpoint=self.authorization_endpoint, token_endpoint=self.token_endpoint, token_endpoint_auth_method='client_secret_post', update_token=self._update_token, base_url=self.api_url)
        self.whoami = WhoAmIResource(client=self).whoami
        '\n        Mounted Monzo `whoami` endpoint. For more information see\n        [`pymonzo.whoami.WhoAmIResource.whoami`][].\n        '
        self.accounts = AccountsResource(client=self)
        '\n        Mounted Monzo `accounts` resource. For more information see\n        [`pymonzo.accounts.AccountsResource`][].\n        '
        self.attachments = AttachmentsResource(client=self)
        '\n        Mounted Monzo `attachments` resource. For more information see\n        [`pymonzo.attachments.AttachmentsResource`][].\n        '
        self.balance = BalanceResource(client=self)
        '\n        Mounted Monzo `balance` resource. For more information see\n        [`pymonzo.balance.BalanceResource`][].\n        '
        self.feed = FeedResource(client=self)
        '\n        Mounted Monzo `feed` resource. For more information see\n        [`pymonzo.feed.FeedResource`][].\n        '
        self.pots = PotsResource(client=self)
        '\n        Mounted Monzo `pots` resource. For more information see\n        [`pymonzo.pots.PotsResource`][].\n        '
        self.transactions = TransactionsResource(client=self)
        '\n        Mounted Monzo `transactions` resource. For more information see\n        [`pymonzo.transactions.TransactionsResource`][].\n        '
        self.webhooks = WebhooksResource(client=self)
        '\n        Mounted Monzo `webhooks` resource. For more information see\n        [`pymonzo.webhooks.WebhooksResource`][].\n        '

    @classmethod
    def authorize(cls, client_id: str, client_secret: str, *, save_to_disk: bool=True, redirect_uri: str='http://localhost:6600/pymonzo') -> dict:
        """"""Use OAuth 2 'Authorization Code Flow' to get Monzo API access token.

        By default, it also saves the token to disk, so it can be loaded during
        [`pymonzo.MonzoAPI`][] initialization.

        Note:
            Monzo API docs: https://docs.monzo.com/#authentication

        Arguments:
            client_id: OAuth client ID.
            client_secret: OAuth client secret.
            save_to_disk: Whether to save the token to disk.
            redirect_uri: Redirect URI specified in OAuth client.

        Returns:
            OAuth token.
        """"""
        client = OAuth2Client(client_id=client_id, client_secret=client_secret, redirect_uri=redirect_uri, token_endpoint_auth_method='client_secret_post')
        url, state = client.create_authorization_url(cls.authorization_endpoint)
        print(f'Please visit this URL to authorize: {url}')
        webbrowser.open(url)
        parsed_url = urlparse(redirect_uri)
        assert parsed_url.hostname is not None
        assert parsed_url.port is not None
        authorization_response = get_authorization_response_url(host=parsed_url.hostname, port=parsed_url.port)
        try:
            token = client.fetch_token(url=cls.token_endpoint, authorization_response=authorization_response)
        except (OAuthError, JSONDecodeError) as e:
            raise MonzoAPIError('Error while fetching API access token') from e
        if save_to_disk:
            settings = PyMonzoSettings(client_id=client_id, client_secret=client_secret, token=token)
            settings.save_to_disk(cls.settings_path)
        return token

    def _update_token(self, token: dict, **kwargs: Any) -> None:
        """"""Update settings with refreshed access token and save it to disk.

        Arguments:
            token: OAuth access token.
            **kwargs: Extra kwargs.
        """"""
        self._settings.token = token
        if self.settings_path.exists():
            self._settings.save_to_disk(self.settings_path)","
class MonzoAPI:
    '''Monzo public API client.
    To use it, you need to create a new OAuth client in [Monzo Developer Portal].
    The `Redirect URLs` should be set to `http://localhost:6600/pymonzo` and
    `Confidentiality` should be set to `Confidential` if you'd like to automatically
    refresh the access token when it expires.
    You can now use `Client ID` and `Client secret` in [`pymonzo.MonzoAPI.authorize`][]
    to finish the OAuth 2 'Authorization Code Flow' and get the API access token
    (which is by default saved to disk and refreshed when expired).
    [Monzo Developer Portal]: https://developers.monzo.com/
    Note:
        Monzo API docs: https://docs.monzo.com/
    '''

    def __init__(self, access_token: Optional[str]=None) -> None:
        '''Initialize Monzo API client and mount all resources.
        It expects [`pymonzo.MonzoAPI.authorize`][] to be called beforehand, so
        it can load the local settings file containing the API access token. You
        can also explicitly pass the `access_token`, but it won't be able to
        automatically refresh it once it expires.
        Arguments:
            access_token: OAuth access token. You can obtain it (and by default, save
                it to disk, so it can refresh automatically) by running
                [`pymonzo.MonzoAPI.authorize`][]. Alternatively, you can get a
                temporary access token from the [Monzo Developer Portal].
                [Monzo Developer Portal]: https://developers.monzo.com/
        Raises:
            NoSettingsFile: When the access token wasn't passed explicitly and the
                settings file couldn't be loaded.
        '''
        pass
    @classmethod
    def authorize(cls, client_id: str, client_secret: str, *, save_to_disk: bool=True, redirect_uri: str='http://localhost:6600/pymonzo') -> dict:
        '''Use OAuth 2 'Authorization Code Flow' to get Monzo API access token.
        By default, it also saves the token to disk, so it can be loaded during
        [`pymonzo.MonzoAPI`][] initialization.
        Note:
            Monzo API docs: https://docs.monzo.com/#authentication
        Arguments:
            client_id: OAuth client ID.
            client_secret: OAuth client secret.
            save_to_disk: Whether to save the token to disk.
            redirect_uri: Redirect URI specified in OAuth client.
        Returns:
            OAuth token.
        '''
        pass

    def _update_token(self, token: dict, **kwargs: Any) -> None:
        '''Update settings with refreshed access token and save it to disk.
        Arguments:
            token: OAuth access token.
            **kwargs: Extra kwargs.
        '''
        pass",snippet_268,"from __future__ import annotations

import json
import threading
import time
import webbrowser
from http.server import BaseHTTPRequestHandler, HTTPServer
from pathlib import Path
from typing import Any, Optional, Tuple
from urllib.parse import urlparse, parse_qs, urlencode

import requests


class NoSettingsFile(FileNotFoundError):
    pass


class _AuthCodeServer(HTTPServer):
    def __init__(self, server_address: Tuple[str, int], RequestHandlerClass: type[BaseHTTPRequestHandler]) -> None:
        super().__init__(server_address, RequestHandlerClass)
        self.code: Optional[str] = None
        self.state: Optional[str] = None
        self.error: Optional[str] = None


class _AuthCodeHandler(BaseHTTPRequestHandler):
    def do_GET(self) -> None:  # noqa: N802
        parsed = urlparse(self.path)
        params = parse_qs(parsed.query)
        code = params.get(""code"", [None])[0]
        state = params.get(""state"", [None])[0]
        error = params.get(""error"", [None])[0]

        assert isinstance(self.server, _AuthCodeServer)
        self.server.code = code
        self.server.state = state
        self.server.error = error

        self.send_response(200)
        self.send_header(""Content-Type"", ""text/html; charset=utf-8"")
        self.end_headers()
        body = (
            ""<html><head><title>Monzo Auth</title></head>""
            ""<body><h2>Authorization complete</h2>""
            ""<p>You can close this window.</p></body></html>""
        )
        self.wfile.write(body.encode(""utf-8""))

    def log_message(self, format: str, *args: Any) -> None:  # noqa: A003
        return


class MonzoAPI:
    '''Monzo public API client.
    To use it, you need to create a new OAuth client in [Monzo Developer Portal].
    The `Redirect URLs` should be set to `http://localhost:6600/pymonzo` and
    `Confidentiality` should be set to `Confidential` if you'd like to automatically
    refresh the access token when it expires.
    You can now use `Client ID` and `Client secret` in [`pymonzo.MonzoAPI.authorize`][]
    to finish the OAuth 2 'Authorization Code Flow' and get the API access token
    (which is by default saved to disk and refreshed when expired).
    [Monzo Developer Portal]: https://developers.monzo.com/
    Note:
        Monzo API docs: https://docs.monzo.com/
    '''

    AUTH_URL = ""https://auth.monzo.com/""
    TOKEN_URL = ""https://api.monzo.com/oauth2/token""
    SETTINGS_PATH = Path.home() / "".pymonzo.json""

    def __init__(self, access_token: Optional[str] = None) -> None:
        '''Initialize Monzo API client and mount all resources.
        It expects [`pymonzo.MonzoAPI.authorize`][] to be called beforehand, so
        it can load the local settings file containing the API access token. You
        can also explicitly pass the `access_token`, but it won't be able to
        automatically refresh it once it expires.
        Arguments:
            access_token: OAuth access token. You can obtain it (and by default, save
                it to disk, so it can refresh automatically) by running
                [`pymonzo.MonzoAPI.authorize`][]. Alternatively, you can get a
                temporary access token from the [Monzo Developer Portal].
                [Monzo Developer Portal]: https://developers.monzo.com/
        Raises:
            NoSettingsFile: When the access token wasn't passed explicitly and the
                settings file couldn't be loaded.
        '''
        self.session = requests.Session()
        self.settings: dict[str, Any] = {}

        if access_token:
            self.access_token = access_token
            self.session.headers.update({""Authorization"": f""Bearer {self.access_token}""})
            return

        self.settings = self._load_settings()
        token = self.settings.get(""token"")
        if not token or ""access_token"" not in token:
            raise NoSettingsFile(""No access token available in settings file."")

        self.access_token = token[""access_token""]
        self.session.headers.update({""Authorization"": f""Bearer {self.access_token}""})

    @classmethod
    def authorize(
        cls,
        client_id: str,
        client_secret: str,
        *,
        save_to_disk: bool = True,
        redirect_uri: str = ""http://localhost:6600/pymonzo"",
    ) -> dict:
        '''Use OAuth 2 'Authorization Code Flow' to get Monzo API access token.
        By default, it also saves the token to disk, so it can be loaded during
        [`pymonzo.MonzoAPI`][] initialization.
        Note:
            Monzo API docs: https://docs.monzo.com/#authentication
        Arguments:
            client_id: OAuth client ID.
            client_secret: OAuth client secret.
            save_to_disk: Whether to save the token to disk.
            redirect_uri: Redirect URI specified in OAuth client.
        Returns:
            OAuth token.
        '''
        parsed = urlparse(redirect_uri)
        if parsed.hostname is None or parsed.port is None:
            raise ValueError(""redirect_uri must include explicit host and port, e.g., http://localhost:6600/pymonzo"")

        params = {
            ""client_id"": client_id,
            ""redirect_uri"": redirect_uri,
            ""response_type"": ""code"",
        }
        auth_url = f""{cls.AUTH_URL}?{urlencode(params)}""

        server = _AuthCodeServer((parsed.hostname, parsed.port), _AuthCodeHandler)
        thread = threading.Thread(target=server.serve_forever, daemon=True)
        thread.start()
        try:
            webbrowser.open(auth_url)
            code = cls._wait_for_code(server, timeout=300)
        finally:
            server.shutdown()
            server.server_close()

        if server.error:
            raise RuntimeError(f""Authorization error: {server.error}"")
        if not code:
            raise TimeoutError(""Timed out waiting for authorization code."")

        token = cls._exchange_code_for_token(
            code=code,
            client_id=client_id,
            client_secret=client_secret,
            redirect_uri=redirect_uri,
        )

        if save_to_disk:
            settings = {
                ""client_id"": client_id,
                ""client_secret"": client_secret,
                ""redirect_uri"": redirect_uri,
                ""token_url"": cls.TOKEN_URL,
                ""token"": token,
            }
            cls._save_settings(settings)

        return token

    def _update_token(self, token: dict, **kwargs: Any) -> None:
        '''Update settings with refreshed access token and save it to disk.
        Arguments:
            token: OAuth access token.
            **kwargs: Extra kwargs.
        '''
        settings = self._load_settings()
        token = dict(token)
        if ""expires_in"" in token and ""expires_at"" not in token:
            token[""expires_at""] = int(time.time()) + int(token[""expires_in""])
        settings[""token""] = token
        self._save_settings(settings)
        self.access_token = token.get(""access_token"", self.access_token)
        self.session.headers.update({""Authorization"": f""Bearer {self.access_token}""})

    @staticmethod
    def _wait_for_code(server: _AuthCodeServer, timeout: int = 300) -> Optional[str]:
        deadline = time.time() + timeout
        while time.time() < deadline:
            if server.code or server.error:
                return server.code
            time.sleep(0.1)
        return None

    @classmethod
    def _exchange_code_for_token(
        cls,
        *,
        code: str,
        client_id: str,
        client_secret: str,
        redirect_uri: str,
    ) -> dict:
        resp = requests.post(
            cls.TOKEN_URL,
            data={
                ""grant_type"": ""authorization_code"",
                ""client_id"": client_id,
                ""client_secret"": client_secret,
                ""redirect_uri"": redirect_uri,
                ""code"": code,
            },
            timeout=30,
        )
        resp.raise_for_status()
        token = resp.json()
        if ""expires_in"" in token and ""expires_at"" not in token:
            token[""expires_at""] = int(time.time()) + int(token[""expires_in""])
        return token

    @classmethod
    def _load_settings(cls) -> dict:
        path = cls.SETTINGS_PATH
        if not path.exists():
            raise NoSettingsFile(f""Settings file not found: {path}"")
        with path.open(""r"", encoding=""utf-8"") as f:
            return json.load(f)

    @classmethod
    def _save_settings(cls, settings: dict) -> None:
        path = cls.SETTINGS_PATH
        path.parent.mkdir(parents=True, exist_ok=True)
        tmp = path.with_suffix(path.suffix + "".tmp"")
        with tmp.open(""w"", encoding=""utf-8"") as f:
            json.dump(settings, f, indent=2, sort_keys=True)
        tmp.replace(path)"
339313,chaoss/grimoirelab-kingarthur,arthur/tasks.py,arthur.tasks._TaskConfig,"import re
from grimoirelab_toolkit.introspect import find_class_properties

class _TaskConfig:
    """"""Abstract class to store task configuration options.

    This class defines how to store specific task configuration
    arguments such as scheduling or archiving options. It is not
    meant to be instantiated on its own.

    Configuration options must be defined using `property` and `setter`
    decorators. Setters must check whether the given value is valid
    or not. When it is invalid, a `ValueError` exception should be
    raised. The rationale behind this is to use these methods as
    parsers when `from_dict` class method is called. It will create
    a new instance of the subclass passing its properties from a
    dictionary.
    """"""
    KW_ARGS_ERROR_REGEX = re.compile(""^.+ got an unexpected keyword argument '(.+)'$"")

    def to_dict(self):
        """"""Returns a dict with the representation of this task configuration object.""""""
        properties = find_class_properties(self.__class__)
        config = {name: self.__getattribute__(name) for name, _ in properties}
        return config

    @classmethod
    def from_dict(cls, config):
        """"""Create an configuration object from a dictionary.

        Key,value pairs will be used to initialize a task configuration
        object. If 'config' contains invalid configuration parameters
        a `ValueError` exception will be raised.

        :param config: dictionary used to create an instance of this object

        :returns: a task config instance

        :raises ValueError: when an invalid configuration parameter is found
        """"""
        try:
            obj = cls(**config)
        except TypeError as e:
            m = cls.KW_ARGS_ERROR_REGEX.match(str(e))
            if m:
                raise ValueError(""unknown '%s' task config parameter"" % m.group(1))
            else:
                raise e
        else:
            return obj","
class _TaskConfig:
    '''Abstract class to store task configuration options.
    This class defines how to store specific task configuration
    arguments such as scheduling or archiving options. It is not
    meant to be instantiated on its own.
    Configuration options must be defined using `property` and `setter`
    decorators. Setters must check whether the given value is valid
    or not. When it is invalid, a `ValueError` exception should be
    raised. The rationale behind this is to use these methods as
    parsers when `from_dict` class method is called. It will create
    a new instance of the subclass passing its properties from a
    dictionary.
    '''

    def to_dict(self):
        '''Returns a dict with the representation of this task configuration object.'''
        pass
    @classmethod
    def from_dict(cls, config):
        '''Create an configuration object from a dictionary.
        Key,value pairs will be used to initialize a task configuration
        object. If 'config' contains invalid configuration parameters
        a `ValueError` exception will be raised.
        :param config: dictionary used to create an instance of this object
        :returns: a task config instance
        :raises ValueError: when an invalid configuration parameter is found
        '''
        pass",snippet_269,"class _TaskConfig:
    '''Abstract class to store task configuration options.
    This class defines how to store specific task configuration
    arguments such as scheduling or archiving options. It is not
    meant to be instantiated on its own.
    Configuration options must be defined using `property` and `setter`
    decorators. Setters must check whether the given value is valid
    or not. When it is invalid, a `ValueError` exception should be
    raised. The rationale behind this is to use these methods as
    parsers when `from_dict` class method is called. It will create
    a new instance of the subclass passing its properties from a
    dictionary.
    '''

    def to_dict(self):
        '''Returns a dict with the representation of this task configuration object.'''
        result = {}
        # collect properties defined on class (including inheritance)
        props = {}
        for cls in type(self).mro():
            for name, obj in cls.__dict__.items():
                if isinstance(obj, property) and not name.startswith('_'):
                    props[name] = obj
        for name, prop in props.items():
            try:
                val = getattr(self, name)
            except Exception:
                continue
            result[name] = val
        return result

    @classmethod
    def from_dict(cls, config):
        '''Create an configuration object from a dictionary.
        Key,value pairs will be used to initialize a task configuration
        object. If 'config' contains invalid configuration parameters
        a `ValueError` exception will be raised.
        :param config: dictionary used to create an instance of this object
        :returns: a task config instance
        :raises ValueError: when an invalid configuration parameter is found
        '''
        if config is None:
            return cls()
        if not isinstance(config, dict):
            raise ValueError(""config must be a dict"")
        # map of property name -> property object (with available setter)
        prop_map = {}
        for c in cls.mro():
            for name, obj in c.__dict__.items():
                if isinstance(obj, property) and obj.fset is not None and not name.startswith('_'):
                    prop_map[name] = obj
        obj = cls()
        for key, value in config.items():
            if key not in prop_map:
                raise ValueError(f""Invalid configuration parameter: {key}"")
            try:
                setattr(obj, key, value)
            except ValueError as e:
                raise ValueError(f""Invalid value for '{key}': {e}"") from e
            except Exception as e:
                raise ValueError(f""Failed setting '{key}': {e}"") from e
        return obj"
341459,wright-group/WrightTools,wright-group_WrightTools/WrightTools/diagrams/WMEL.py,WrightTools.diagrams.WMEL.Subplot,"import numpy as np

class Subplot:
    """"""Subplot containing WMEL.""""""

    def __init__(self, ax, energies, number_of_interactions=4, title='', title_font_size=16, state_names=None, virtual=[None], state_font_size=14, state_text_buffer=0.5, label_side='left'):
        """"""Subplot.

        Parameters
        ----------
        ax : matplotlib axis
            The axis.
        energies : 1D array-like
            Energies (scaled between 0 and 1)
        number_of_interactions : integer
            Number of interactions in diagram.
        title : string (optional)
            Title of subplot. Default is empty string.
        state_names: list of str (optional)
            list of the names of the states
        virtual: list of ints (optional)
            list of indexes of any vitual energy states
        state_font_size: numtype (optional)
            font size for the state lables
        state_text_buffer: numtype (optional)
            space between the energy level bars and the state labels
        """"""
        self.ax = ax
        self.energies = energies
        self.interactions = number_of_interactions
        self.state_names = state_names
        for i in range(len(self.energies)):
            if i in virtual:
                linestyle = '--'
            else:
                linestyle = '-'
            self.ax.axhline(self.energies[i], color='k', linewidth=2, ls=linestyle, zorder=5)
        if isinstance(state_names, list):
            for i in range(len(self.energies)):
                if label_side == 'left':
                    ax.text(-state_text_buffer, energies[i], state_names[i], fontsize=state_font_size, verticalalignment='center', horizontalalignment='center')
                elif label_side == 'right':
                    ax.text(1 + state_text_buffer, energies[i], state_names[i], fontsize=state_font_size, verticalalignment='center', horizontalalignment='center')
        self.x_pos = np.linspace(0, 1, number_of_interactions)
        self.ax.set_xlim(-0.1, 1.1)
        self.ax.set_ylim(-0.01, 1.01)
        self.ax.axis('off')
        self.ax.set_title(title, fontsize=title_font_size)

    def add_arrow(self, index, between, kind, label='', head_length=10, head_aspect=1, font_size=14, color='k'):
        """"""Add an arrow to the WMEL diagram.

        Parameters
        ----------
        index : integer
            The interaction, or start and stop interaction for the arrow.
        between : 2-element iterable of integers
            The inital and final state of the arrow
        kind : {'ket', 'bra', 'outbra', 'outket'}
            The kind of interaction.
        label : string (optional)
            Interaction label. Default is empty string.
        head_length: number (optional)
            size of arrow head
        font_size : number (optional)
            Label font size. Default is 14.
        color : matplotlib color (optional)
            Arrow color. Default is black.

        Returns
        -------
        [line,arrow_head,text]
        """"""
        if hasattr(index, 'index'):
            x_pos = list(index)
        else:
            x_pos = [index] * 2
        x_pos = [np.linspace(0, 1, self.interactions)[i] for i in x_pos]
        y_pos = [self.energies[between[0]], self.energies[between[1]]]
        arrow_length = self.energies[between[1]] - self.energies[between[0]]
        arrow_end = self.energies[between[1]]
        if arrow_length > 0:
            direction = 1
        elif arrow_length < 0:
            direction = -1
        else:
            raise ValueError('between invalid!')
        length = abs(y_pos[0] - y_pos[1])
        if kind == 'ket':
            line = self.ax.plot(x_pos, y_pos, linestyle='-', color=color, linewidth=2, zorder=9)
        elif kind == 'bra':
            line = self.ax.plot(x_pos, y_pos, linestyle='--', color=color, linewidth=2, zorder=9)
        elif kind == 'out':
            yi = np.linspace(y_pos[0], y_pos[1], 100)
            xi = np.sin((yi - y_pos[0]) * int(1 / length * 20) * 2 * np.pi * length) / 40 + x_pos[0]
            line = self.ax.plot(xi[:-5], yi[:-5], linestyle='-', color=color, linewidth=2, solid_capstyle='butt', zorder=9)
        elif kind == 'outbra':
            yi = np.linspace(y_pos[0], y_pos[1], 100)
            xi = np.sin((yi - y_pos[0]) * int(1 / length * 20) * 2 * np.pi * length) / 40 + x_pos[0]
            counter = 0
            while counter - 13 <= len(yi):
                subyi = yi[counter:counter + 15]
                subxi = xi[counter:counter + 15]
                line = self.ax.plot(subxi[:-5], subyi[:-5], linestyle='-', color=color, linewidth=2, solid_capstyle='butt', zorder=9)
                counter += 13
        else:
            raise ValueError(""kind is not 'ket', 'bra', 'out' or 'outbra'."")
        dx = x_pos[1] - x_pos[0]
        dy = y_pos[1] - y_pos[0]
        xytext = (x_pos[1] - dx * 0.01, y_pos[1] - dy * 0.01)
        annotation = self.ax.annotate('', xy=(x_pos[1], y_pos[1]), xytext=xytext, arrowprops=dict(fc=color, ec=color, shrink=0, headwidth=head_length * head_aspect, headlength=head_length, linewidth=0, zorder=10), size=25)
        text = self.ax.text(np.mean(x_pos), -0.15, label, fontsize=font_size, horizontalalignment='center')
        return (line, annotation.arrow_patch, text)","
class Subplot:
    '''Subplot containing WMEL.'''

    def __init__(self, ax, energies, number_of_interactions=4, title='', title_font_size=16, state_names=None, virtual=[None], state_font_size=14, state_text_buffer=0.5, label_side='left'):
        '''Subplot.
        Parameters
        ----------
        ax : matplotlib axis
            The axis.
        energies : 1D array-like
            Energies (scaled between 0 and 1)
        number_of_interactions : integer
            Number of interactions in diagram.
        title : string (optional)
            Title of subplot. Default is empty string.
        state_names: list of str (optional)
            list of the names of the states
        virtual: list of ints (optional)
            list of indexes of any vitual energy states
        state_font_size: numtype (optional)
            font size for the state lables
        state_text_buffer: numtype (optional)
            space between the energy level bars and the state labels
        '''
        pass

    def add_arrow(self, index, between, kind, label='', head_length=10, head_aspect=1, font_size=14, color='k'):
        '''Add an arrow to the WMEL diagram.
        Parameters
        ----------
        index : integer
            The interaction, or start and stop interaction for the arrow.
        between : 2-element iterable of integers
            The inital and final state of the arrow
        kind : {'ket', 'bra', 'outbra', 'outket'}
            The kind of interaction.
        label : string (optional)
            Interaction label. Default is empty string.
        head_length: number (optional)
            size of arrow head
        font_size : number (optional)
            Label font size. Default is 14.
        color : matplotlib color (optional)
            Arrow color. Default is black.
        Returns
        -------
        [line,arrow_head,text]
        '''
        pass",snippet_271,"import numpy as np
from matplotlib.patches import FancyArrowPatch

class Subplot:
    '''Subplot containing WMEL.'''

    def __init__(self, ax, energies, number_of_interactions=4, title='', title_font_size=16, state_names=None, virtual=[None], state_font_size=14, state_text_buffer=0.5, label_side='left'):
        '''Subplot.
        Parameters
        ----------
        ax : matplotlib axis
            The axis.
        energies : 1D array-like
            Energies (scaled between 0 and 1)
        number_of_interactions : integer
            Number of interactions in diagram.
        title : string (optional)
            Title of subplot. Default is empty string.
        state_names: list of str (optional)
            list of the names of the states
        virtual: list of ints (optional)
            list of indexes of any vitual energy states
        state_font_size: numtype (optional)
            font size for the state lables
        state_text_buffer: numtype (optional)
            space between the energy level bars and the state labels
        '''
        self.ax = ax
        self.energies = np.asarray(energies, dtype=float)
        self.n_states = len(self.energies)
        self.n_interactions = int(number_of_interactions)
        self.state_names = state_names if state_names is not None else [f""|{i}>"" for i in range(self.n_states)]
        self.virtual = set([] if virtual is None else virtual)
        self.state_font_size = state_font_size
        self.state_text_buffer = state_text_buffer
        self.label_side = label_side

        self.xmin = 0.0
        self.xmax = self.n_interactions + 1.0
        ypad = 0.05 * (self.energies.max() - self.energies.min() + 1e-9)
        self.ymin = self.energies.min() - ypad
        self.ymax = self.energies.max() + ypad

        # Draw horizontal energy levels
        for i, y in enumerate(self.energies):
            ls = '--' if (i in self.virtual) else '-'
            self.ax.plot([self.xmin, self.xmax], [y, y], ls=ls, color='k', lw=1)

        # Labels on states
        if self.label_side == 'right':
            xlbl = self.xmax + self.state_text_buffer
            ha = 'left'
        else:
            xlbl = self.xmin - self.state_text_buffer
            ha = 'right'
        for i, y in enumerate(self.energies):
            self.ax.text(xlbl, y, str(self.state_names[i]), va='center', ha=ha, fontsize=self.state_font_size)

        # Optional vertical guides for interactions
        for i in range(1, self.n_interactions + 1):
            self.ax.axvline(i, color='0.8', lw=0.5, zorder=0)

        # Axes cosmetics
        self.ax.set_xlim(self.xmin - 1.0, self.xmax + 1.0)
        self.ax.set_ylim(self.ymin, self.ymax)
        self.ax.set_xticks(range(self.n_interactions + 1))
        self.ax.set_yticks([])
        self.ax.set_title(title, fontsize=title_font_size)
        self.ax.spines['right'].set_visible(False)
        self.ax.spines['top'].set_visible(False)

    def _kind_offset(self, kind):
        if kind == 'ket':
            return -0.15
        if kind == 'bra':
            return +0.15
        if kind == 'outket':
            return -(0.5 + 0.15)
        if kind == 'outbra':
            return (0.5 + 0.15)
        return 0.0

    def _arrow_head(self, x0, y0, x1, y1, color, head_length, head_aspect):
        vx = x1 - x0
        vy = y1 - y0
        norm = np.hypot(vx, vy)
        if norm == 0:
            vx, vy = 0.0, 1.0
            norm = 1.0
        # place a head using FancyArrowPatch; minimal shaft by making start very close to end
        eps = 1e-6
        xa = x1 - (vx / norm) * eps
        ya = y1 - (vy / norm) * eps
        head = FancyArrowPatch(
            (xa, ya),
            (x1, y1),
            arrowstyle='-|>',
            mutation_scale=head_length * max(0.1, head_aspect),
            color=color,
            lw=0,
            shrinkA=0,
            shrinkB=0,
            zorder=5,
            clip_on=False,
        )
        self.ax.add_patch(head)
        return head

    def add_arrow(self, index, between, kind, label='', head_length=10, head_aspect=1, font_size=14, color='k'):
        '''Add an arrow to the WMEL diagram.
        Parameters
        ----------
        index : integer
            The interaction, or start and stop interaction for the arrow.
        between : 2-element iterable of integers
            The inital and final state of the arrow
        kind : {'ket', 'bra', 'outbra', 'outket'}
            The kind of interaction.
        label : string (optional)
            Interaction label. Default is empty string.
        head_length: number (optional)
            size of arrow head
        font_size : number (optional)
            Label font size. Default is 14.
        color : matplotlib color (optional)
            Arrow color. Default is black.
        Returns
        -------
        [line,arrow_head,text]
        '''
        if not hasattr(between, '__len__') or len(between) != 2:
            raise ValueError(""between must be a 2-element iterable of state indices"")
        s0, s1 = int(between[0]), int(between[1])
        y0 = float(self.energies[s0])
        y1 = float(self.energies[s1])

        # Determine x positions based on index and kind
        x_off = self._kind_offset(kind)
        if isinstance(index, (list, tuple, np.ndarray)) and len(index) == 2:
            x_start = float(index[0]) + x_off
            x_end = float(index[1]) + x_off
        elif np.isscalar(index):
            idx = float(index)
            # instantaneous interaction: vertical arrow at a single interaction line
            x_start = idx + x_off
            x_end = idx + x_off
        else:
            raise ValueError(""index must be an integer or a 2-element iterable"")

        # Special handling for out-of-sequence arrows
        if kind == 'outket':
            x_start = self.xmin - 0.5
            if np.isscalar(index):
                x_end = x_start
        elif kind == 'outbra':
            x_start = self.xmax + 0.5
            if np.isscalar(index):
                x_end = x_start

        # Avoid zero-length arrows for instantaneous interactions by slightly offsetting y
        if x_start == x_end and y0 == y1:
            y1 = y0 + 0.02 * (self.ymax - self.ymin)

        line = self.ax.plot([x_start, x_end], [y0, y1], color=color, lw=1.5, zorder=4)[0]
        head = self._arrow_head(x_start, y0, x_end, y1, color=color, head_length=head_length, head_aspect=head_aspect)

        # Label placement near the midpoint, offset normal to arrow
        mx = 0.5 * (x_start + x_end)
        my = 0.5 * (y0 + y1)
        dx = x_end - x_start
        dy = y1 - y0
        nrm = np.hypot(dx, dy)
        if nrm == 0:
            nx, ny = 0.0, 1.0
        else:
            nx, ny = -dy / nrm, dx / nrm
        # Offset magnitude relative to axes ranges
        ox = nx * 0.03 * (self.xmax - self.xmin)
        oy = ny * 0.03 * (self.ymax - self.ymin)
        txt = self.ax.text(mx + ox, my + oy, label, color=color, fontsize=font_size, ha='center', va='center')

        return [line, head, txt]"
341765,peopledoc/django-agnocomplete,peopledoc_django-agnocomplete/agnocomplete/views.py,agnocomplete.views.UserContextFormViewMixin,"class UserContextFormViewMixin:
    """"""
    This mixin is injecting the context variable into the form kwargs
    """"""

    def get_agnocomplete_context(self):
        """"""
        Return the view current user.

        You may want to change this value by overrding this method.
        """"""
        return self.request.user

    def get_form_kwargs(self):
        """"""
        Return the form kwargs.

        This method injects the context variable, defined in
        :meth:`get_agnocomplete_context`. Override this method to adjust it to
        your needs.
        """"""
        data = super().get_form_kwargs()
        data.update({'user': self.get_agnocomplete_context()})
        return data","class UserContextFormViewMixin:
    '''
    This mixin is injecting the context variable into the form kwargs
    '''

    def get_agnocomplete_context(self):
        '''
        Return the view current user.
        You may want to change this value by overrding this method.
        '''
        pass

    def get_form_kwargs(self):
        '''
        Return the form kwargs.
        This method injects the context variable, defined in
        :meth:`get_agnocomplete_context`. Override this method to adjust it to
        your needs.
        '''
        pass",snippet_272,"class UserContextFormViewMixin:
    '''
    This mixin is injecting the context variable into the form kwargs
    '''

    def get_agnocomplete_context(self):
        '''
        Return the view current user.
        You may want to change this value by overrding this method.
        '''
        try:
            return self.request.user
        except Exception:
            return None

    def get_form_kwargs(self):
        '''
        Return the form kwargs.
        This method injects the context variable, defined in
        :meth:`get_agnocomplete_context`. Override this method to adjust it to
        your needs.
        '''
        kwargs = {}
        super_get_form_kwargs = getattr(super(), 'get_form_kwargs', None)
        if callable(super_get_form_kwargs):
            kwargs = super_get_form_kwargs()
        kwargs['agnocomplete_context'] = self.get_agnocomplete_context()
        return kwargs"
343261,danpoland/pyramid-restful-framework,danpoland_pyramid-restful-framework/pyramid_restful/pagination/base.py,pyramid_restful.pagination.base.BasePagination,"class BasePagination:
    """"""
    The base class each Pagination class should implement.
    """"""

    def paginate_query(self, query, request):
        """"""
        :param query: SQLAlchemy ``query``.
        :param request: The request from the view
        :return: The paginated date based on the provided query and request.
        """"""
        raise NotImplementedError('paginate_query() must be implemented.')

    def get_paginated_response(self, data):
        """"""
        :param data: The paginated data.
        :return: A response containing the paginated data.
        """"""
        raise NotImplementedError('get_paginated_response() must be implemented.')","class BasePagination:
    '''
    The base class each Pagination class should implement.
        '''

    def paginate_query(self, query, request):
        '''
        :param query: SQLAlchemy ``query``.
        :param request: The request from the view
        :return: The paginated date based on the provided query and request.
        '''
        pass

    def get_paginated_response(self, data):
        '''
        :param data: The paginated data.
        :return: A response containing the paginated data.
        '''
        pass",snippet_273,"class BasePagination:
    '''
    The base class each Pagination class should implement.
    '''

    def __init__(
        self,
        page_param='page',
        page_size_param='page_size',
        limit_param='limit',
        offset_param='offset',
        default_page_size=20,
        max_page_size=100,
    ):
        self.page_param = page_param
        self.page_size_param = page_size_param
        self.limit_param = limit_param
        self.offset_param = offset_param
        self.default_page_size = default_page_size
        self.max_page_size = max_page_size

    def _get_query_params(self, request):
        if request is None:
            return {}
        # Common frameworks support
        for attr in ('args', 'GET', 'query_params'):
            if hasattr(request, attr):
                params = getattr(request, attr)
                try:
                    return params.to_dict(flat=True)  # werkzeug/multi-dict
                except Exception:
                    try:
                        return dict(params)
                    except Exception:
                        return params
        # If request itself is dict-like
        try:
            return dict(request)
        except Exception:
            return {}

    def _to_int(self, value, default=None):
        try:
            if value is None or value == '':
                return default
            return int(value)
        except (TypeError, ValueError):
            return default

    def _safe_count(self, query):
        try:
            # order_by(None) avoids unnecessary ORDER BY in count where possible
            q = getattr(query, 'order_by', lambda *_: query)
            return q(None).count() if getattr(query, 'order_by', None) else query.count()
        except Exception:
            return None

    def paginate_query(self, query, request):
        '''
        :param query: SQLAlchemy ``query``.
        :param request: The request from the view
        :return: (items, pagination_meta)
        '''
        params = self._get_query_params(request)

        # Determine mode: limit/offset takes precedence if provided
        has_limit_offset = any(k in params for k in (self.limit_param, self.offset_param))

        if has_limit_offset:
            limit = self._to_int(params.get(self.limit_param), self.default_page_size)
            offset = self._to_int(params.get(self.offset_param), 0)
            if limit is None or limit <= 0:
                limit = self.default_page_size
            if offset is None or offset < 0:
                offset = 0
            if self.max_page_size is not None:
                limit = min(limit, self.max_page_size)

            total = self._safe_count(query)
            items = query.limit(limit).offset(offset).all()

            pages = None
            page = None
            page_size = limit
            if total is not None and limit > 0:
                pages = (total + limit - 1) // limit
                page = (offset // limit) + 1

            meta = {
                'count': total,
                'pages': pages,
                'limit': limit,
                'offset': offset,
                'page': page,
                'page_size': page_size,
                'has_next': (total is None) or (offset + limit < total),
                'has_prev': offset > 0,
                'next_offset': (offset + limit) if (total is None or (offset + limit < total)) else None,
                'prev_offset': max(offset - limit, 0) if offset > 0 else None,
                'next_page': (page + 1) if page is not None and (total is None or (offset + limit < total)) else None,
                'prev_page': (page - 1) if page and page > 1 else None,
            }
            return items, meta

        # Page/page_size mode
        page = self._to_int(params.get(self.page_param), 1)
        page_size = self._to_int(params.get(self.page_size_param), self.default_page_size)

        if page is None or page <= 0:
            page = 1
        if page_size is None or page_size <= 0:
            page_size = self.default_page_size
        if self.max_page_size is not None:
            page_size = min(page_size, self.max_page_size)

        offset = (page - 1) * page_size
        total = self._safe_count(query)
        items = query.limit(page_size).offset(offset).all()

        pages = None
        if total is not None and page_size > 0:
            pages = (total + page_size - 1) // page_size

        has_next = (total is None) or (pages is not None and page < pages)
        has_prev = page > 1

        meta = {
            'count': total,
            'pages': pages,
            'page': page,
            'page_size': page_size,
            'limit': page_size,
            'offset': offset,
            'has_next': has_next,
            'has_prev': has_prev,
            'next_page': (page + 1) if has_next else None,
            'prev_page': (page - 1) if has_prev else None,
            'next_offset': (offset + page_size) if has_next else None,
            'prev_offset': (offset - page_size) if has_prev else None,
        }
        return items, meta

    def get_paginated_response(self, data):
        '''
        :param data: The paginated data.
        :return: A response containing the paginated data.
        '''
        if isinstance(data, tuple) and len(data) == 2:
            items, meta = data
            return {'results': items, 'pagination': meta}
        if isinstance(data, dict) and 'results' in data:
            return data
        return {'results': data}"
343270,danpoland/pyramid-restful-framework,danpoland_pyramid-restful-framework/pyramid_restful/permissions.py,pyramid_restful.permissions.BasePermission,"class BasePermission:
    """"""
    All permission classes should inherit from this class.
    """"""
    message = None

    def has_permission(self, request, view):
        """"""
        Checked on every request to a view. Return ``True`` if permission is granted else ``False``.

        :param request: The request sent to the view.
        :param view: The instance of the view being accessed.
        :return: Boolean
        """"""
        return True

    def has_object_permission(self, request, view, obj):
        """"""
        Checked when a request is for a specific object. Return ``True`` if permission is granted else ``False``.

        :param request: The request sent to the view.
        :param view:  The instance of the view being accessed.
        :param obj: The object being accessed.
        :return: Boolean
        """"""
        return True","class BasePermission:
    '''
    All permission classes should inherit from this class.
        '''

    def has_permission(self, request, view):
        '''
        Checked on every request to a view. Return ``True`` if permission is granted else ``False``.
        :param request: The request sent to the view.
        :param view: The instance of the view being accessed.
        :return: Boolean
        '''
        pass

    def has_object_permission(self, request, view, obj):
        '''
        Checked when a request is for a specific object. Return ``True`` if permission is granted else ``False``.
        :param request: The request sent to the view.
        :param view:  The instance of the view being accessed.
        :param obj: The object being accessed.
        :return: Boolean
        '''
        pass",snippet_274,"class BasePermission:
    '''
    All permission classes should inherit from this class.
    '''

    def has_permission(self, request, view):
        '''
        Checked on every request to a view. Return ``True`` if permission is granted else ``False``.
        :param request: The request sent to the view.
        :param view: The instance of the view being accessed.
        :return: Boolean
        '''
        return True

    def has_object_permission(self, request, view, obj):
        '''
        Checked when a request is for a specific object. Return ``True`` if permission is granted else ``False``.
        :param request: The request sent to the view.
        :param view:  The instance of the view being accessed.
        :param obj: The object being accessed.
        :return: Boolean
        '''
        return True"
344249,openid/JWTConnect-Python-CryptoJWT,src/cryptojwt/jws/__init__.py,cryptojwt.jws.Signer,"class Signer:
    """"""Abstract base class for signing algorithms.""""""

    def sign(self, msg, key):
        """"""Sign ``msg`` with ``key`` and return the signature.""""""
        raise NotImplementedError()

    def verify(self, msg, sig, key):
        """"""Return True if ``sig`` is a valid signature for ``msg``.""""""
        raise NotImplementedError()","class Signer:
    '''Abstract base class for signing algorithms.'''

    def sign(self, msg, key):
        '''Sign ``msg`` with ``key`` and return the signature.'''
        pass

    def verify(self, msg, sig, key):
        '''Return True if ``sig`` is a valid signature for ``msg``.'''
        pass",snippet_275,"from abc import ABC, abstractmethod

class Signer(ABC):
    '''Abstract base class for signing algorithms.'''

    @abstractmethod
    def sign(self, msg, key):
        '''Sign ``msg`` with ``key`` and return the signature.'''
        raise NotImplementedError

    @abstractmethod
    def verify(self, msg, sig, key):
        '''Return True if ``sig`` is a valid signature for ``msg``.'''
        raise NotImplementedError"
347062,adobe-apiplatform/umapi-client.py,umapi_client/api.py,umapi_client.api.QuerySingle,"class QuerySingle:
    """"""
    Look for a single object
    """"""

    def __init__(self, connection, object_type, url_params=None, query_params=None):
        """"""
        Provide the connection and query parameters when you create the query.

        :param connection: The Connection to run the query against
        :param object_type: The type of object being queried (e.g., ""user"" or ""group"")
        :param url_params: Query qualifiers that go in the URL path (e.g., a group name when querying users)
        :param query_params: Query qualifiers that go in the query string (e.g., a domain name)
        """"""
        self.conn = connection
        self.object_type = object_type
        self.url_params = url_params if url_params else []
        self.query_params = query_params if query_params else {}
        self._result = None

    def reload(self):
        """"""
        Rerun the query (lazily).
        The result will contain a value on the server side that have changed since the last run.
        :return: None
        """"""
        self._result = None

    def _fetch_result(self):
        """"""
        Fetch the queried object.
        """"""
        self._result = self.conn.query_single(self.object_type, self.url_params, self.query_params)

    def result(self):
        """"""
        Fetch the result, if we haven't already or if reload has been called.
        :return: the result object of the query.
        """"""
        if self._result is None:
            self._fetch_result()
        return self._result","class QuerySingle:
    '''
    Look for a single object
    '''

    def __init__(self, connection, object_type, url_params=None, query_params=None):
        '''
        Provide the connection and query parameters when you create the query.
        :param connection: The Connection to run the query against
        :param object_type: The type of object being queried (e.g., ""user"" or ""group"")
        :param url_params: Query qualifiers that go in the URL path (e.g., a group name when querying users)
        :param query_params: Query qualifiers that go in the query string (e.g., a domain name)
        '''
        pass

    def reload(self):
        '''
        Rerun the query (lazily).
        The result will contain a value on the server side that have changed since the last run.
        :return: None
        '''
        pass

    def _fetch_result(self):
        '''
        Fetch the queried object.
        '''
        pass

    def result(self):
        '''
        Fetch the result, if we haven't already or if reload has been called.
        :return: the result object of the query.
        '''
        pass",snippet_276,"class QuerySingle:
    '''
    Look for a single object
    '''

    def __init__(self, connection, object_type, url_params=None, query_params=None):
        '''
        Provide the connection and query parameters when you create the query.
        :param connection: The Connection to run the query against
        :param object_type: The type of object being queried (e.g., ""user"" or ""group"")
        :param url_params: Query qualifiers that go in the URL path (e.g., a group name when querying users)
        :param query_params: Query qualifiers that go in the query string (e.g., a domain name)
        '''
        self._connection = connection
        self._object_type = object_type
        self._url_params = url_params
        self._query_params = query_params
        self._result = None
        self._needs_reload = True

    def reload(self):
        '''
        Rerun the query (lazily).
        The result will contain a value on the server side that have changed since the last run.
        :return: None
        '''
        self._needs_reload = True

    def _fetch_result(self):
        '''
        Fetch the queried object.
        '''
        conn = self._connection
        obj_type = self._object_type
        url_params = self._url_params
        query_params = self._query_params

        # Try common connection interfaces in a sensible order
        if callable(conn):
            return conn(obj_type, url_params, query_params)

        for method_name in (
            'query_single',
            'get_single',
            'fetch_single',
            'fetch_one',
            'get_one',
            'get_object',
            'fetch',
            'get',
        ):
            method = getattr(conn, method_name, None)
            if callable(method):
                try:
                    return method(obj_type, url_params, query_params)
                except TypeError:
                    # Try without object_type if method signature differs
                    try:
                        return method(url_params, query_params)
                    except TypeError:
                        # Try with only query_params
                        try:
                            return method(query_params)
                        except TypeError:
                            continue

        raise AttributeError(
            ""Connection object does not provide a compatible method to fetch a single result.""
        )

    def result(self):
        '''
        Fetch the result, if we haven't already or if reload has been called.
        :return: the result object of the query.
        '''
        if self._needs_reload or self._result is None:
            self._result = self._fetch_result()
            self._needs_reload = False
        return self._result"
347189,facelessuser/backrefs,facelessuser_backrefs/backrefs/util.py,backrefs.util.Immutable,"from typing import Any, Callable, AnyStr

class Immutable:
    """"""Immutable.""""""
    __slots__: tuple[Any, ...] = ()

    def __init__(self, **kwargs: Any) -> None:
        """"""Initialize.""""""
        for k, v in kwargs.items():
            super().__setattr__(k, v)

    def __setattr__(self, name: str, value: Any) -> None:
        """"""Prevent mutability.""""""
        raise AttributeError('Class is immutable!')","
class Immutable:
    '''Immutable.'''

    def __init__(self, **kwargs: Any) -> None:
        '''Initialize.'''
        pass

    def __setattr__(self, name: str, value: Any) -> None:
        '''Prevent mutability.'''
        pass",snippet_277,"from typing import Any

class Immutable:
    '''Immutable.'''

    def __init__(self, **kwargs: Any) -> None:
        '''Initialize.'''
        object.__setattr__(self, ""_Immutable__locked"", False)
        for name, value in kwargs.items():
            object.__setattr__(self, name, value)
        object.__setattr__(self, ""_Immutable__locked"", True)

    def __setattr__(self, name: str, value: Any) -> None:
        '''Prevent mutability.'''
        if getattr(self, ""_Immutable__locked"", False):
            raise AttributeError(f""{self.__class__.__name__} is immutable"")
        object.__setattr__(self, name, value)"
347503,inveniosoftware/invenio-records,inveniosoftware_invenio-records/invenio_records/dumpers/base.py,invenio_records.dumpers.base.Dumper,"from copy import deepcopy

class Dumper:
    """"""Interface for dumpers.""""""

    def dump(self, record, data):
        """"""Dump a record that can be used a source document for the search engine.

        The job of this method is to create a Python dictionary from the record
        provided in the argument.

        If you overwrite this method without calling super, then you should
        ensure that you make a deep copy of the record dictionary, to avoid
        that changes to the dump affects the record.

        :param record: The record to dump.
        :param data: The initial dump data passed in by ``record.dumps()``.
        """"""
        data.update(deepcopy(dict(record)))
        return data

    def load(self, data, record_cls):
        """"""Load a record from the source document of a search engine hit.

        The job of this method, is to create a record of type ``record_cls``
        based on the input ``data``.

        :param data: A Python dictionary representing the data to load.
        :param records_cls: The record class to be constructed.
        :returns: A instance of ``record_cls``.
        """"""
        raise NotImplementedError()","
class Dumper:
    '''Interface for dumpers.'''

    def dump(self, record, data):
        '''Dump a record that can be used a source document for the search engine.
        The job of this method is to create a Python dictionary from the record
        provided in the argument.
        If you overwrite this method without calling super, then you should
        ensure that you make a deep copy of the record dictionary, to avoid
        that changes to the dump affects the record.
        :param record: The record to dump.
        :param data: The initial dump data passed in by ``record.dumps()``.
        '''
        pass

    def load(self, data, record_cls):
        '''Load a record from the source document of a search engine hit.
        The job of this method, is to create a record of type ``record_cls``
        based on the input ``data``.
        :param data: A Python dictionary representing the data to load.
        :param records_cls: The record class to be constructed.
        :returns: A instance of ``record_cls``.
        '''
        pass",snippet_278,"from copy import deepcopy
from collections.abc import Mapping

class Dumper:
    '''Interface for dumpers.'''

    def dump(self, record, data):
        '''Dump a record that can be used a source document for the search engine.
        The job of this method is to create a Python dictionary from the record
        provided in the argument.
        If you overwrite this method without calling super, then you should
        ensure that you make a deep copy of the record dictionary, to avoid
        that changes to the dump affects the record.
        :param record: The record to dump.
        :param data: The initial dump data passed in by ``record.dumps()``.
        '''
        base = deepcopy(data) if isinstance(data, Mapping) else {}

        # Extract a dict from record using common conventions
        if hasattr(record, ""to_dict"") and callable(getattr(record, ""to_dict"")):
            src = record.to_dict()
        elif hasattr(record, ""dict"") and callable(getattr(record, ""dict"")):
            # pydantic-style
            try:
                src = record.dict()
            except TypeError:
                src = record.dict  # in case it's a property
        elif hasattr(record, ""as_dict"") and callable(getattr(record, ""as_dict"")):
            src = record.as_dict()
        elif isinstance(record, Mapping):
            src = record
        else:
            raise TypeError(""record must be a mapping or provide to_dict()/dict()/as_dict()"")

        src_copy = deepcopy(src)
        base.update(src_copy)
        return base

    def load(self, data, record_cls):
        '''Load a record from the source document of a search engine hit.
        The job of this method, is to create a record of type ``record_cls``
        based on the input ``data``.
        :param data: A Python dictionary representing the data to load.
        :param records_cls: The record class to be constructed.
        :returns: A instance of ``record_cls``.
        '''
        if not isinstance(data, Mapping):
            raise TypeError(""data must be a mapping/dict"")

        payload = deepcopy(data)

        # Prefer classmethods/factory methods if available
        if hasattr(record_cls, ""from_dict"") and callable(getattr(record_cls, ""from_dict"")):
            return record_cls.from_dict(payload)

        # Try common alternate constructor names
        for method_name in (""load"", ""loads"", ""from_mapping""):
            m = getattr(record_cls, method_name, None)
            if callable(m):
                try:
                    return m(payload)
                except TypeError:
                    pass

        # Try instance construction with kwargs
        try:
            return record_cls(**payload)
        except TypeError:
            pass

        # Try passing the mapping as a single positional argument
        try:
            return record_cls(payload)
        except TypeError as exc:
            raise TypeError(f""Could not construct {record_cls!r} from provided data"") from exc"
347507,inveniosoftware/invenio-records,inveniosoftware_invenio-records/invenio_records/dumpers/search.py,invenio_records.dumpers.search.SearchDumperExt,"class SearchDumperExt:
    """"""Interface for Search dumper extensions.""""""

    def dump(self, record, data):
        """"""Dump the data.""""""

    def load(self, data, record_cls):
        """"""Load the data.

        Reverse the changes made by the dump method.
        """"""","class SearchDumperExt:
    '''Interface for Search dumper extensions.'''

    def dump(self, record, data):
        '''Dump the data.'''
        pass

    def load(self, data, record_cls):
        '''Load the data.
        Reverse the changes made by the dump method.
        '''
        pass",snippet_279,"class SearchDumperExt:
    '''Interface for Search dumper extensions.'''

    def dump(self, record, data):
        '''Dump the data.'''
        raise NotImplementedError(""Subclasses must implement dump()."")

    def load(self, data, record_cls):
        '''Load the data.
        Reverse the changes made by the dump method.
        '''
        raise NotImplementedError(""Subclasses must implement load()."")"
347521,inveniosoftware/invenio-records,inveniosoftware_invenio-records/invenio_records/systemfields/base.py,invenio_records.systemfields.base.SystemFieldContext,"class SystemFieldContext:
    """"""Base class for a system field context.

    A system field context is created once you access a field's attribute on
    a class. As the system field may be defined on a super class, this context
    allows us to know from which class the field was accessed.

    Normally you should subclass this class, and implement methods the methods
    on it that requires you to know the record class.
    """"""

    def __init__(self, field, record_cls):
        """"""Initialise the field context.""""""
        self._field = field
        self._record_cls = record_cls

    @property
    def field(self):
        """"""Access the field to prevent it from being overwritten.""""""
        return self._field

    @property
    def record_cls(self):
        """"""Record class to prevent it from being overwritten.""""""
        return self._record_cls","class SystemFieldContext:
    '''Base class for a system field context.
    A system field context is created once you access a field's attribute on
    a class. As the system field may be defined on a super class, this context
    allows us to know from which class the field was accessed.
    Normally you should subclass this class, and implement methods the methods
    on it that requires you to know the record class.
        '''

    def __init__(self, field, record_cls):
        '''Initialise the field context.'''
        pass
    @property
    def field(self):
        '''Access the field to prevent it from being overwritten.'''
        pass
    @property
    def record_cls(self):
        '''Record class to prevent it from being overwritten.'''
        pass",snippet_280,"class SystemFieldContext:
    '''Base class for a system field context.
    A system field context is created once you access a field's attribute on
    a class. As the system field may be defined on a super class, this context
    allows us to know from which class the field was accessed.
    Normally you should subclass this class, and implement methods the methods
    on it that requires you to know the record class.
        '''

    def __init__(self, field, record_cls):
        '''Initialise the field context.'''
        self._field = field
        self._record_cls = record_cls

    @property
    def field(self):
        '''Access the field to prevent it from being overwritten.'''
        return self._field

    @property
    def record_cls(self):
        '''Record class to prevent it from being overwritten.'''
        return self._record_cls"
348074,edx/edx-val,edx_edx-val/edxval/transcript_utils.py,edxval.transcript_utils.Transcript,"import json
from pysrt import SubRipFile, SubRipItem, SubRipTime
from edxval.exceptions import TranscriptsGenerationException
from pysrt.srtexc import Error

class Transcript:
    """"""
    Container for transcript methods.
    """"""
    SRT = 'srt'
    SJSON = 'sjson'

    @staticmethod
    def generate_sjson_from_srt(srt_subs):
        """"""
        Generate transcripts from sjson to SubRip (*.srt).

        Arguments:
            srt_subs(SubRip): ""SRT"" subs object

        Returns:
            Subs converted to ""SJSON"" format.
        """"""
        sub_starts = []
        sub_ends = []
        sub_texts = []
        for sub in srt_subs:
            sub_starts.append(sub.start.ordinal)
            sub_ends.append(sub.end.ordinal)
            sub_texts.append(sub.text.replace('\n', ' '))
        sjson_subs = {'start': sub_starts, 'end': sub_ends, 'text': sub_texts}
        return sjson_subs

    @staticmethod
    def generate_srt_from_sjson(sjson_subs):
        """"""
        Generate transcripts from sjson to SubRip (*.srt)

        Arguments:
            sjson_subs (dict): `sjson` subs.

        Returns:
            Subtitles in SRT format.
        """"""
        output = ''
        equal_len = len(sjson_subs['start']) == len(sjson_subs['end']) == len(sjson_subs['text'])
        if not equal_len:
            return output
        for i in range(len(sjson_subs['start'])):
            item = SubRipItem(index=i, start=SubRipTime(milliseconds=sjson_subs['start'][i]), end=SubRipTime(milliseconds=sjson_subs['end'][i]), text=sjson_subs['text'][i])
            output += str(item)
            output += '\n'
        return output

    @classmethod
    def convert(cls, content, input_format, output_format):
        """"""
        Convert transcript `content` from `input_format` to `output_format`.

        Arguments:
            content: Transcript content byte-stream.
            input_format: Input transcript format.
            output_format: Output transcript format.

        Accepted input formats: sjson, srt.
        Accepted output format: srt, sjson.

        Raises:
            TranscriptsGenerationException: On parsing the invalid srt
            content during conversion from srt to sjson.
        """"""
        assert input_format in ('srt', 'sjson')
        assert output_format in ('srt', 'sjson')
        try:
            content = content.decode('utf-8-sig')
        except UnicodeDecodeError:
            content = content.decode('latin-1')
        if input_format == output_format:
            return content
        if input_format == 'srt':
            if output_format == 'sjson':
                try:
                    srt_subs = SubRipFile.from_string(content, error_handling=SubRipFile.ERROR_RAISE)
                except Error as ex:
                    raise TranscriptsGenerationException(str(ex)) from ex
                return json.dumps(cls.generate_sjson_from_srt(srt_subs))
        if input_format == 'sjson':
            if output_format == 'srt':
                return cls.generate_srt_from_sjson(json.loads(content))","
class Transcript:
    '''
    Container for transcript methods.
    '''
    @staticmethod
    def generate_sjson_from_srt(srt_subs):
        '''
        Generate transcripts from sjson to SubRip (*.srt).
        Arguments:
            srt_subs(SubRip): ""SRT"" subs object
        Returns:
            Subs converted to ""SJSON"" format.
        '''
        pass
    @staticmethod
    def generate_srt_from_sjson(sjson_subs):
        '''
        Generate transcripts from sjson to SubRip (*.srt)
        Arguments:
            sjson_subs (dict): `sjson` subs.
        Returns:
            Subtitles in SRT format.
        '''
        pass
    @classmethod
    def convert(cls, content, input_format, output_format):
        '''
        Convert transcript `content` from `input_format` to `output_format`.
        Arguments:
            content: Transcript content byte-stream.
            input_format: Input transcript format.
            output_format: Output transcript format.
        Accepted input formats: sjson, srt.
        Accepted output format: srt, sjson.
        Raises:
            TranscriptsGenerationException: On parsing the invalid srt
            content during conversion from srt to sjson.
        '''
        pass",snippet_282,"class Transcript:
    '''
    Container for transcript methods.
    '''

    class TranscriptsGenerationException(Exception):
        pass

    @staticmethod
    def _parse_timestamp(ts):
        ts = ts.strip()
        # Expected format: HH:MM:SS,mmm
        try:
            hms, ms = ts.split(',')
            h, m, s = hms.split(':')
            total = int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000.0
            return float(total)
        except Exception as e:
            raise ValueError(f""Invalid SRT timestamp: {ts}"") from e

    @staticmethod
    def _format_timestamp(seconds):
        if seconds < 0:
            seconds = 0.0
        total_ms = int(round(seconds * 1000))
        ms = total_ms % 1000
        total_s = total_ms // 1000
        s = total_s % 60
        total_m = total_s // 60
        m = total_m % 60
        h = total_m // 60
        return f""{h:02d}:{m:02d}:{s:02d},{ms:03d}""

    @staticmethod
    def _ensure_text(s):
        if s is None:
            return """"
        return str(s)

    @staticmethod
    def _normalize_sjson(sjson_subs):
        # Accept dict {'segments': [...]}, or list directly
        if isinstance(sjson_subs, dict):
            if 'segments' in sjson_subs and isinstance(sjson_subs['segments'], list):
                return sjson_subs['segments']
            # Accept common alternatives
            for key in ('fragments', 'events', 'items', 'lines'):
                if key in sjson_subs and isinstance(sjson_subs[key], list):
                    return sjson_subs[key]
            # If dict looks like a single segment, wrap it
            if {'start', 'end', 'text'} & set(sjson_subs.keys()):
                return [sjson_subs]
            return []
        elif isinstance(sjson_subs, list):
            return sjson_subs
        else:
            return []

    @staticmethod
    def generate_sjson_from_srt(srt_subs):
        '''
        Generate transcripts from sjson to SubRip (*.srt).
        Arguments:
            srt_subs(SubRip): ""SRT"" subs object
        Returns:
            Subs converted to ""SJSON"" format.
        '''
        # Accept bytes, str, or any object with 'decode'
        if srt_subs is None:
            return {""segments"": []}
        if isinstance(srt_subs, bytes):
            text = srt_subs.decode('utf-8', errors='replace')
        else:
            text = str(srt_subs)

        blocks = []
        # Normalize newlines
        text_norm = text.replace('\r\n', '\n').replace('\r', '\n')
        # Split into blocks separated by empty lines
        raw_blocks = [b for b in text_norm.split('\n\n') if b.strip() != '']
        for raw in raw_blocks:
            lines = [ln for ln in raw.split('\n') if ln.strip() != '']
            if not lines:
                continue
            # Detect if first line is an index
            time_line_idx = 0
            if '-->' not in lines[0]:
                # try next line
                if len(lines) < 2 or '-->' not in lines[1]:
                    # invalid block
                    raise Transcript.TranscriptsGenerationException(""Invalid SRT block (missing time line)"")
                time_line_idx = 1
            time_line = lines[time_line_idx].strip()
            if '-->' not in time_line:
                raise Transcript.TranscriptsGenerationException(""Invalid SRT time line"")
            parts = time_line.split('-->')
            if len(parts) != 2:
                raise Transcript.TranscriptsGenerationException(""Invalid SRT time separator"")
            start_str = parts[0].strip()
            end_str = parts[1].strip()
            try:
                start = Transcript._parse_timestamp(start_str)
                end = Transcript._parse_timestamp(end_str)
            except Exception as e:
                raise Transcript.TranscriptsGenerationException(""Invalid SRT timestamps"") from e
            if end < start:
                # Allow zero-duration but not negative
                end = start
            # Remaining lines after time line are text
            text_lines = lines[time_line_idx + 1:] if len(lines) > time_line_idx + 1 else []
            content = '\n'.join(text_lines)
            blocks.append({
                ""start"": start,
                ""end"": end,
                ""duration"": round(end - start, 3),
                ""text"": content
            })
        return {""segments"": blocks}

    @staticmethod
    def generate_srt_from_sjson(sjson_subs):
        '''
        Generate transcripts from sjson to SubRip (*.srt)
        Arguments:
            sjson_subs (dict): `sjson` subs.
        Returns:
            Subtitles in SRT format.
        '''
        segments = Transcript._normalize_sjson(sjson_subs)
        srt_lines = []
        idx = 1
        for seg in segments:
            # Segment may be dict-like; tolerate missing keys
            start = None
            end = None
            text = """"
            if isinstance(seg, dict):
                text = Transcript._ensure_text(seg.get('text') or seg.get('line') or seg.get('content') or """")
                if 'start' in seg:
                    try:
                        start = float(seg['start'])
                    except Exception:
                        start = 0.0
                if 'end' in seg:
                    try:
                        end = float(seg['end'])
                    except Exception:
                        end = None
                if end is None and 'duration' in seg and start is not None:
                    try:
                        end = float(start) + float(seg['duration'])
                    except Exception:
                        end = start
                if start is None and 'ts' in seg:
                    # Some schemas use 'ts' or 'time'
                    try:
                        start = float(seg['ts'])
                    except Exception:
                        start = 0.0
                if end is None and 'timeEnd' in seg:
                    try:
                        end = float(seg['timeEnd'])
                    except Exception:
                        end = start if start is not None else 0.0
            else:
                text = Transcript._ensure_text(seg)
                start = 0.0
                end = 0.0
            if start is None:
                start = 0.0
            if end is None:
                end = start
            if end < start:
                end = start
            srt_lines.append(str(idx))
            srt_lines.append(f""{Transcript._format_timestamp(start)} --> {Transcript._format_timestamp(end)}"")
            # Ensure text lines do not contain CR
            srt_lines.extend(Transcript._ensure_text(text).replace('\r\n', '\n').replace('\r', '\n').split('\n'))
            srt_lines.append("""")  # blank line between blocks
            idx += 1
        return '\n'.join(srt_lines).rstrip() + ('\n' if srt_lines else '')

    @classmethod
    def convert(cls, content, input_format, output_format):
        '''
        Convert transcript `content` from `input_format` to `output_format`.
        Arguments:
            content: Transcript content byte-stream.
            input_format: Input transcript format.
            output_format: Output transcript format.
        Accepted input formats: sjson, srt.
        Accepted output format: srt, sjson.
        Raises:
            TranscriptsGenerationException: On parsing the invalid srt
            content during conversion from srt to sjson.
        '''
        if input_format not in ('sjson', 'srt'):
            raise ValueError(""Unsupported input_format. Accepted: sjson, srt"")
        if output_format not in ('sjson', 'srt'):
            raise ValueError(""Unsupported output_format. Accepted: sjson, srt"")

        # If same format, return as-is
        if input_format == output_format:
            return content

        # Normalize content
        data = content
        if isinstance(data, bytes):
            data_bytes = data
            data_text = data.decode('utf-8', errors='replace')
        else:
            data_bytes = None
            data_text = str(data) if data is not None else """"

        if input_format == 'srt' and output_format == 'sjson':
            try:
                sjson_obj = cls.generate_sjson_from_srt(data_text)
            except Transcript.TranscriptsGenerationException:
                raise
            except Exception as e:
                raise Transcript.TranscriptsGenerationException(""Failed to parse SRT"") from e
            try:
                import json
                return json.dumps(sjson_obj, ensure_ascii=False).encode('utf-8')
            except Exception as e:
                # Fallback safe serialization
                return str(sjson_obj).encode('utf-8')

        if input_format == 'sjson' and output_format == 'srt':
            # Parse JSON if bytes/string JSON
            sjson_obj = None
            if isinstance(content, (bytes, str)):
                try:
                    import json
                    sjson_obj = json.loads(data_text)
                except Exception:
                    # If not JSON decodable, attempt eval-safe fallback not used, default to empty
                    sjson_obj = {}
            elif isinstance(content, dict):
                sjson_obj = content
            else:
                sjson_obj = {}
            srt_str = cls.generate_srt_from_sjson(sjson_obj)
            return srt_str.encode('utf-8')

        # Should not reach here
        return content"
350782,druids/django-chamber,druids_django-chamber/chamber/commands/__init__.py,chamber.commands.ProgressBarStream,"class ProgressBarStream:
    """"""
    OutputStream wrapper to remove default linebreak at line endings.
    """"""

    def __init__(self, stream):
        """"""
        Wrap the given stream.
        """"""
        self.stream = stream

    def write(self, *args, **kwargs):
        """"""
        Call the stream's write method without linebreaks at line endings.
        """"""
        return self.stream.write(*args, ending='', **kwargs)

    def flush(self):
        """"""
        Call the stream's flush method without any extra arguments.
        """"""
        return self.stream.flush()","class ProgressBarStream:
    '''
    OutputStream wrapper to remove default linebreak at line endings.
        '''

    def __init__(self, stream):
        '''
        Wrap the given stream.
        '''
        pass

    def write(self, *args, **kwargs):
        '''
        Call the stream's write method without linebreaks at line endings.
        '''
        pass

    def flush(self):
        '''
        Call the stream's flush method without any extra arguments.
        '''
        pass",snippet_283,"class ProgressBarStream:
    '''
    OutputStream wrapper to remove default linebreak at line endings.
    '''

    def __init__(self, stream):
        '''
        Wrap the given stream.
        '''
        self._stream = stream

    def write(self, *args, **kwargs):
        '''
        Call the stream's write method without linebreaks at line endings.
        '''
        if not args:
            data = ''
        elif len(args) == 1:
            data = args[0]
        else:
            if all(isinstance(a, (bytes, bytearray)) for a in args):
                data = b''.join(args)
            else:
                data = ''.join(str(a) for a in args)

        if isinstance(data, (bytes, bytearray)):
            if data.endswith(b'\r\n'):
                data = data[:-2]
            elif data.endswith(b'\n'):
                data = data[:-1]
        else:
            if data.endswith('\r\n'):
                data = data[:-2]
            elif data.endswith('\n'):
                data = data[:-1]

        return self._stream.write(data)

    def flush(self):
        '''
        Call the stream's flush method without any extra arguments.
        '''
        return self._stream.flush()"
350920,pytroll/trollimage,pytroll_trollimage/trollimage/_xrimage_rasterio.py,trollimage._xrimage_rasterio.RIODataset,"from rasterio.enums import Resampling
from rasterio.windows import Window

class RIODataset:
    """"""A wrapper for a rasterio dataset.""""""

    def __init__(self, rfile, overviews=None, overviews_resampling=None, overviews_minsize=256):
        """"""Init the rasterio dataset.""""""
        self.rfile = rfile
        self.overviews = overviews
        if overviews_resampling is None:
            overviews_resampling = 'nearest'
        self.overviews_resampling = Resampling[overviews_resampling]
        self.overviews_minsize = overviews_minsize

    def __setitem__(self, key, item):
        """"""Put the data chunk in the image.""""""
        if len(key) == 3:
            indexes = list(range(key[0].start + 1, key[0].stop + 1, key[0].step or 1))
            y = key[1]
            x = key[2]
        else:
            indexes = 1
            y = key[0]
            x = key[1]
        chy_off = y.start
        chy = y.stop - y.start
        chx_off = x.start
        chx = x.stop - x.start
        self.rfile.write(item, window=Window(chx_off, chy_off, chx, chy), indexes=indexes)

    def close(self):
        """"""Close the file.""""""
        if self.overviews is not None:
            overviews = self.overviews
            if len(overviews) == 0:
                from rasterio.rio.overview import get_maximum_overview_level
                width = self.rfile.width
                height = self.rfile.height
                max_level = get_maximum_overview_level(width, height, self.overviews_minsize)
                overviews = [2 ** j for j in range(1, max_level + 1)]
            logger.debug('Building overviews %s with %s resampling', str(overviews), self.overviews_resampling.name)
            self.rfile.build_overviews(overviews, resampling=self.overviews_resampling)
        return self.rfile.close()","
class RIODataset:
    '''A wrapper for a rasterio dataset.'''

    def __init__(self, rfile, overviews=None, overviews_resampling=None, overviews_minsize=256):
        '''Init the rasterio dataset.'''
        pass

    def __setitem__(self, key, item):
        '''Put the data chunk in the image.'''
        pass

    def close(self):
        '''Close the file.'''
        pass",snippet_284,"import rasterio
from rasterio.windows import Window, from_slices
from rasterio.enums import Resampling
from collections.abc import Iterable
import math
import os


class RIODataset:
    '''A wrapper for a rasterio dataset.'''

    def __init__(self, rfile, overviews=None, overviews_resampling=None, overviews_minsize=256):
        '''Init the rasterio dataset.'''
        if hasattr(rfile, ""read"") and hasattr(rfile, ""write""):
            self.ds = rfile
            self._owns_ds = False
        else:
            mode = ""r+""
            try:
                self.ds = rasterio.open(rfile, mode)
            except Exception:
                # fallback to read-only if not writable
                self.ds = rasterio.open(rfile, ""r"")
            self._owns_ds = True

        self._closed = False

        if overviews is None:
            overviews = self._auto_overview_factors(self.ds.width, self.ds.height, overviews_minsize)

        if overviews:
            resampling = self._parse_resampling(overviews_resampling)
            try:
                # build_overviews is a no-op if dataset is read-only; guard with try
                self.ds.build_overviews(overviews, resampling)
                # Some drivers (e.g., GTiff) may store resampling in metadata
                try:
                    self.ds.update_tags(ns='rio_overview', resampling=str(resampling).split('.')[-1])
                except Exception:
                    pass
            except Exception:
                # Ignore overview building errors on unsupported drivers/modes
                pass

    def __setitem__(self, key, item):
        '''Put the data chunk in the image.'''
        if self._closed:
            raise RuntimeError(""Dataset is closed"")

        window = None
        indexes = None

        if isinstance(key, Window):
            window = key
        elif isinstance(key, tuple):
            # Possible forms:
            # (row_slice, col_slice)
            # (indexes, row_slice, col_slice)
            if len(key) == 2:
                row_slice, col_slice = key
                window = from_slices(row_slice, col_slice, height=self.ds.height, width=self.ds.width)
            elif len(key) == 3:
                idx, row_slice, col_slice = key
                indexes = idx
                window = from_slices(row_slice, col_slice, height=self.ds.height, width=self.ds.width)
            else:
                raise ValueError(""Invalid key. Expected (row_slice, col_slice) or (indexes, row_slice, col_slice)."")
        else:
            raise TypeError(""Key must be a rasterio.windows.Window or a tuple of slices."")

        if isinstance(indexes, Iterable) and not isinstance(indexes, (list, tuple)):
            indexes = list(indexes)

        # Normalize indexes: None, int, or list of ints
        if indexes is not None:
            if isinstance(indexes, int):
                pass
            elif isinstance(indexes, (list, tuple)):
                if not all(isinstance(i, int) for i in indexes):
                    raise TypeError(""Indexes must be int or iterable of ints."")
            else:
                raise TypeError(""Indexes must be int or iterable of ints."")

        self.ds.write(item, window=window, indexes=indexes)

    def close(self):
        '''Close the file.'''
        if not self._closed and self._owns_ds:
            try:
                self.ds.close()
            finally:
                self._closed = True

    @staticmethod
    def _parse_resampling(resampling):
        if resampling is None:
            return Resampling.average
        if isinstance(resampling, Resampling):
            return resampling
        if isinstance(resampling, str):
            key = resampling.strip().lower()
            for r in Resampling:
                if r.name.lower() == key:
                    return r
        raise ValueError(""Invalid resampling value."")

    @staticmethod
    def _auto_overview_factors(width, height, minsize):
        if minsize is None or minsize <= 0:
            return []
        min_dim = min(width, height)
        if min_dim <= minsize:
            return []
        factors = []
        f = 2
        while min_dim / f >= minsize:
            factors.append(f)
            f *= 2
        return factors"
350922,pytroll/trollimage,pytroll_trollimage/trollimage/_xrimage_rasterio.py,trollimage._xrimage_rasterio.RIOTag,"class RIOTag:
    """"""Rasterio wrapper to allow da.store on tag.""""""

    def __init__(self, rfile, name):
        """"""Init the rasterio tag.""""""
        self.rfile = rfile
        self.name = name

    def __setitem__(self, key, item):
        """"""Put the data in the tag.""""""
        kwargs = {self.name: item.item()}
        self.rfile.update_tags(**kwargs)

    def close(self):
        """"""Close the file.""""""
        return self.rfile.close()","class RIOTag:
    '''Rasterio wrapper to allow da.store on tag.'''

    def __init__(self, rfile, name):
        '''Init the rasterio tag.'''
        pass

    def __setitem__(self, key, item):
        '''Put the data in the tag.'''
        pass

    def close(self):
        '''Close the file.'''
        pass",snippet_285,"import numpy as np

class RIOTag:
    '''Rasterio wrapper to allow da.store on tag.'''

    def __init__(self, rfile, name):
        '''Init the rasterio tag.'''
        self.rfile = rfile
        self.name = name

    def __setitem__(self, key, item):
        '''Put the data in the tag.'''
        if hasattr(item, ""compute""):
            item = item.compute()
        arr = np.asarray(item)
        if arr.size == 1:
            value = arr.reshape(-1)[0]
        else:
            value = arr.tolist()
        self.rfile.update_tags(**{self.name: str(value)})

    def close(self):
        '''Close the file.'''
        try:
            self.rfile.close()
        except Exception:
            pass"
353855,inveniosoftware/invenio-communities,inveniosoftware_invenio-communities/invenio_communities/communities/records/systemfields/access.py,invenio_communities.communities.records.systemfields.access.AccessEnumMixin,"class AccessEnumMixin:
    """"""Mixin for enum functionalities.""""""

    @classmethod
    def validate(cls, level):
        """"""Validate a string against the enum values.""""""
        return cls(level) in cls

    def __str__(self):
        """"""Return its value.""""""
        return self.value","class AccessEnumMixin:
    '''Mixin for enum functionalities.'''
    @classmethod
    def validate(cls, level):
        '''Validate a string against the enum values.'''
        pass

    def __str__(self):
        '''Return its value.'''
        pass",snippet_286,"class AccessEnumMixin:
    '''Mixin for enum functionalities.'''
    @classmethod
    def validate(cls, level):
        '''Validate a string against the enum values.'''
        if isinstance(level, cls):
            return level

        # Try matching by name/value with case-insensitive handling for strings
        if isinstance(level, str):
            lvl_lower = level.lower()
            for member in cls:
                name_match = member.name.lower() == lvl_lower
                value_match = isinstance(member.value, str) and member.value.lower() == lvl_lower
                if name_match or value_match:
                    return member

        # Fallback: try exact value match for non-strings or unmatched strings
        for member in cls:
            if member.value == level:
                return member

        valid = [m.name for m in cls]
        raise ValueError(f""Invalid {cls.__name__} value: {level!r}. Expected one of: {', '.join(valid)}"")

    def __str__(self):
        '''Return its value.'''
        return str(self.value)"
354005,inveniosoftware/invenio-communities,inveniosoftware_invenio-communities/invenio_communities/roles.py,invenio_communities.roles.Role,"from dataclasses import dataclass, field

@dataclass(frozen=True)
class Role:
    """"""Role class.""""""
    name: str = ''
    'Name of the role.'
    title: str = ''
    'Title of the role.'
    description: str = ''
    'Brief description of capabilities of the role.'
    can_manage_roles: list = field(default_factory=list)
    'List of other roles that this role can manage.'
    is_owner: bool = False
    'This role is the owner role (only one can exists).'
    can_manage: bool = False
    'This role has manage permissions.'
    can_curate: bool = False
    'This role has record manage permissions.'
    can_view: bool = False
    'This role has view restricted record permissions.'

    def can_manage_role(self, role_name):
        """"""Determine if this role can manage the role name.""""""
        return role_name in self.can_manage_roles

    def __hash__(self):
        """"""Compute a hash for use with e.g. sets.""""""
        return self.name.__hash__()","@dataclass(frozen=True)
class Role:
    '''Role class.'''

    def can_manage_role(self, role_name):
        '''Determine if this role can manage the role name.'''
        pass

    def __hash__(self):
        '''Compute a hash for use with e.g. sets.'''
        pass",snippet_287,"from dataclasses import dataclass
from typing import ClassVar, Union

@dataclass(frozen=True)
class Role:
    '''Role class.'''
    name: str

    HIERARCHY: ClassVar[tuple[str, ...]] = (
        ""owner"",
        ""admin"",
        ""manager"",
        ""moderator"",
        ""staff"",
        ""member"",
        ""user"",
        ""guest"",
        ""anonymous"",
    )
    RANKS: ClassVar[dict[str, int]] = {r: i for i, r in enumerate(HIERARCHY)}

    def __post_init__(self):
        normalized = (self.name or """").strip().lower()
        object.__setattr__(self, ""name"", normalized)

    def can_manage_role(self, role_name: Union[str, ""Role""]):
        '''Determine if this role can manage the role name.'''
        target = role_name.name if isinstance(role_name, Role) else str(role_name)
        target = target.strip().lower()
        if self.name not in self.RANKS or target not in self.RANKS:
            return False
        if self.name == target:
            return False
        return self.RANKS[self.name] < self.RANKS[target]

    def __hash__(self):
        '''Compute a hash for use with e.g. sets.'''
        return hash((""Role"", self.name))"
356596,grycap/RADL,grycap_RADL/radl/radl.py,radl.radl.Aspect,"import copy

class Aspect:
    """"""A network, ansible_host, system, deploy, configure or contextualize element in a RADL.""""""

    def getId(self):
        """"""Return the id of the aspect.""""""
        return id(self)

    def clone(self):
        """"""Return a copy of this aspect.""""""
        return copy.deepcopy(self)","
class Aspect:
    '''A network, ansible_host, system, deploy, configure or contextualize element in a RADL.'''

    def getId(self):
        '''Return the id of the aspect.'''
        pass

    def clone(self):
        '''Return a copy of this aspect.'''
        pass",snippet_288,"import copy

class Aspect:
    '''A network, ansible_host, system, deploy, configure or contextualize element in a RADL.'''

    def __init__(self, id=None, **attributes):
        self._id = id
        self._attributes = dict(attributes)

    def getId(self):
        '''Return the id of the aspect.'''
        return self._id

    def clone(self):
        '''Return a copy of this aspect.'''
        return copy.deepcopy(self)

    def __deepcopy__(self, memo):
        new_obj = self.__class__.__new__(self.__class__)
        memo[id(self)] = new_obj
        new_obj._id = copy.deepcopy(self._id, memo)
        new_obj._attributes = copy.deepcopy(self._attributes, memo)
        return new_obj

    def __repr__(self):
        return f""{self.__class__.__name__}(id={self._id!r}, attributes={self._attributes!r})"""
357236,pytroll/posttroll,posttroll/backends/zmq/message_broadcaster.py,posttroll.backends.zmq.message_broadcaster.ZMQDesignatedReceiversSender,"import threading
from posttroll.backends.zmq.socket import close_socket, set_up_client_socket
from zmq import LINGER, NOBLOCK, REQ, ZMQError

class ZMQDesignatedReceiversSender:
    """"""Sends message to multiple *receivers* on *port*.""""""

    def __init__(self, default_port, receivers):
        """"""Set up the sender.""""""
        self.default_port = default_port
        self.receivers = receivers
        self._shutdown_event = threading.Event()

    def __call__(self, data):
        """"""Send data.""""""
        for receiver in self.receivers:
            self._send_to_address(receiver, data)

    def _send_to_address(self, address, data, timeout=10):
        """"""Send data to *address* and *port* without verification of response.""""""
        if address.find(':') == -1:
            full_address = 'tcp://%s:%d' % (address, self.default_port)
        else:
            full_address = 'tcp://%s' % address
        options = {LINGER: int(timeout * 1000)}
        socket = set_up_client_socket(REQ, full_address, options)
        try:
            socket.send_string(data)
            while not self._shutdown_event.is_set():
                try:
                    message = socket.recv_string(NOBLOCK)
                except ZMQError:
                    self._shutdown_event.wait(0.1)
                    continue
                if message != 'ok':
                    logger.warning('invalid acknowledge received: %s' % message)
                break
        finally:
            close_socket(socket)

    def close(self):
        """"""Close the sender.""""""
        self._shutdown_event.set()","
class ZMQDesignatedReceiversSender:
    '''Sends message to multiple *receivers* on *port*.'''

    def __init__(self, default_port, receivers):
        '''Set up the sender.'''
        pass

    def __call__(self, data):
        '''Send data.'''
        pass

    def _send_to_address(self, address, data, timeout=10):
        '''Send data to *address* and *port* without verification of response.'''
        pass

    def close(self):
        '''Close the sender.'''
        pass",snippet_289,"import zmq
import pickle

class ZMQDesignatedReceiversSender:
    '''Sends message to multiple receivers on port.'''

    def __init__(self, default_port, receivers):
        '''Set up the sender.'''
        self._ctx = zmq.Context.instance()
        self._default_port = int(default_port)
        self._endpoints = []
        self._sockets = {}
        for r in receivers:
            ep = self._normalize_endpoint(r)
            if ep not in self._endpoints:
                self._endpoints.append(ep)
        for ep in self._endpoints:
            sock = self._ctx.socket(zmq.PUSH)
            sock.setsockopt(zmq.LINGER, 0)
            sock.connect(ep)
            self._sockets[ep] = sock

    def __call__(self, data):
        '''Send data.'''
        results = {}
        for ep in self._endpoints:
            results[ep] = self._send_to_address(ep, data)
        return results

    def _send_to_address(self, address, data, timeout=10):
        '''Send data to address and port without verification of response.'''
        sock = self._sockets.get(address)
        if sock is None:
            # Late-bind socket if not present
            sock = self._ctx.socket(zmq.PUSH)
            sock.setsockopt(zmq.LINGER, 0)
            sock.connect(address)
            self._sockets[address] = sock
            if address not in self._endpoints:
                self._endpoints.append(address)

        # Serialize
        if isinstance(data, (bytes, bytearray, memoryview)):
            payload = bytes(data)
        elif isinstance(data, str):
            payload = data.encode('utf-8')
        else:
            payload = pickle.dumps(data, protocol=pickle.HIGHEST_PROTOCOL)

        # Set send timeout (milliseconds)
        prev_timeout = sock.getsockopt(zmq.SNDTIMEO)
        try:
            sock.setsockopt(zmq.SNDTIMEO, int(timeout * 1000))
            sock.send(payload, flags=0)
            return True
        except zmq.Again:
            return False
        finally:
            # Restore previous timeout
            try:
                sock.setsockopt(zmq.SNDTIMEO, prev_timeout)
            except Exception:
                pass

    def close(self):
        '''Close the sender.'''
        for sock in self._sockets.values():
            try:
                sock.close(0)
            except Exception:
                pass
        self._sockets.clear()
        self._endpoints.clear()
        try:
            # Do not terminate global instance context; just flush
            self._ctx.term()
        except Exception:
            pass

    def _normalize_endpoint(self, receiver):
        r = str(receiver).strip()
        if '://' in r:
            return r
        if ':' in r:
            host, port = r.rsplit(':', 1)
            return f""tcp://{host}:{int(port)}""
        return f""tcp://{r}:{self._default_port}"""
357237,pytroll/posttroll,posttroll/backends/zmq/ns.py,posttroll.backends.zmq.ns.ZMQNameServer,"from posttroll.backends.zmq.socket import SocketReceiver, close_socket, set_up_client_socket, set_up_server_socket
from contextlib import suppress
from zmq import LINGER, REP, REQ
from posttroll.ns import get_active_address, get_configured_nameserver_port

class ZMQNameServer:
    """"""The name server.""""""

    def __init__(self):
        """"""Set up the nameserver.""""""
        self.running: bool = True
        self.listener: SocketReceiver | None = None
        self._authenticator = None

    def run(self, address_receiver, address: str | None=None):
        """"""Run the listener and answer to requests.""""""
        port = get_configured_nameserver_port()
        try:
            if not self.running:
                return
            if address is None:
                address = '*'
            address = create_nameserver_address(address)
            self.listener, _, self._authenticator = set_up_server_socket(REP, address)
            logger.debug(f'Nameserver listening on port {port}')
            socket_receiver = SocketReceiver()
            socket_receiver.register(self.listener)
            while self.running:
                try:
                    for msg, _ in socket_receiver.receive(self.listener, timeout=1):
                        logger.debug('Replying to request: ' + str(msg))
                        active_address = get_active_address(msg.data['service'], address_receiver, msg.version)
                        self.listener.send_unicode(str(active_address))
                except TimeoutError:
                    continue
        except KeyboardInterrupt:
            pass
        finally:
            socket_receiver.unregister(self.listener)
            self.close_sockets_and_threads()

    def close_sockets_and_threads(self):
        """"""Close all sockets and threads.""""""
        with suppress(AttributeError):
            close_socket(self.listener)
        with suppress(AttributeError):
            self._authenticator.stop()

    def stop(self):
        """"""Stop the name server.""""""
        self.running = False","
class ZMQNameServer:
    '''The name server.'''

    def __init__(self):
        '''Set up the nameserver.'''
        pass

    def run(self, address_receiver, address: str | None=None):
        '''Run the listener and answer to requests.'''
        pass

    def close_sockets_and_threads(self):
        '''Close all sockets and threads.'''
        pass

    def stop(self):
        '''Stop the name server.'''
        pass",snippet_290,"import threading
import json
import time
from typing import Optional

try:
    import zmq
except ImportError:  # Fallback no-op shim to avoid NameError if zmq is absent
        # This shim will raise informative errors upon attempted use.
    class _ZMQShim:
        def __getattr__(self, item):
            raise ImportError(""pyzmq is required to use ZMQNameServer"")
    zmq = _ZMQShim()


class ZMQNameServer:
    '''The name server.'''

    def __init__(self):
        '''Set up the nameserver.'''
        self._context: Optional[""zmq.Context""] = None
        self._rep_socket: Optional[""zmq.Socket""] = None
        self._pub_socket: Optional[""zmq.Socket""] = None
        self._pub_enabled: bool = False
        self._thread: Optional[threading.Thread] = None
        self._running = threading.Event()
        self._lock = threading.RLock()
        self._registry: dict[str, str] = {}
        self._poll_timeout_ms = 200

    def _ensure_context(self):
        if self._context is None:
            self._context = zmq.Context.instance()

    def _handle_request(self, msg: bytes) -> dict:
        try:
            req = json.loads(msg.decode(""utf-8""))
        except Exception:
            return {""status"": ""error"", ""error"": ""invalid_json""}

        if not isinstance(req, dict) or ""cmd"" not in req:
            return {""status"": ""error"", ""error"": ""invalid_request""}

        cmd = str(req.get(""cmd"", """")).upper()

        if cmd == ""PING"":
            return {""status"": ""ok"", ""data"": ""pong""}

        if cmd == ""SHUTDOWN"":
            self.stop()
            return {""status"": ""ok"", ""data"": ""shutting_down""}

        if cmd == ""REGISTER"":
            name = req.get(""name"")
            endpoint = req.get(""endpoint"")
            if not isinstance(name, str) or not isinstance(endpoint, str):
                return {""status"": ""error"", ""error"": ""missing_or_invalid_name_or_endpoint""}
            with self._lock:
                self._registry[name] = endpoint
            self._publish({""event"": ""registered"", ""name"": name, ""endpoint"": endpoint})
            return {""status"": ""ok""}

        if cmd == ""UNREGISTER"":
            name = req.get(""name"")
            if not isinstance(name, str):
                return {""status"": ""error"", ""error"": ""missing_or_invalid_name""}
            with self._lock:
                existed = name in self._registry
                endpoint = self._registry.pop(name, None)
            if existed:
                self._publish({""event"": ""unregistered"", ""name"": name, ""endpoint"": endpoint})
                return {""status"": ""ok""}
            return {""status"": ""error"", ""error"": ""not_found""}

        if cmd == ""LOOKUP"":
            name = req.get(""name"")
            if not isinstance(name, str):
                return {""status"": ""error"", ""error"": ""missing_or_invalid_name""}
            with self._lock:
                endpoint = self._registry.get(name)
            if endpoint is None:
                return {""status"": ""error"", ""error"": ""not_found""}
            return {""status"": ""ok"", ""endpoint"": endpoint}

        if cmd == ""LIST"":
            with self._lock:
                items = dict(self._registry)
            return {""status"": ""ok"", ""items"": items}

        return {""status"": ""error"", ""error"": ""unknown_command""}

    def _publish(self, payload: dict):
        if not self._pub_enabled or self._pub_socket is None:
            return
        try:
            self._pub_socket.send_json(payload, flags=zmq.NOBLOCK)
        except Exception:
            pass

    def _loop(self):
        poller = zmq.Poller()
        poller.register(self._rep_socket, zmq.POLLIN)
        while self._running.is_set():
            try:
                events = dict(poller.poll(self._poll_timeout_ms))
            except Exception:
                break
            if self._rep_socket in events and events[self._rep_socket] & zmq.POLLIN:
                try:
                    msg = self._rep_socket.recv(flags=0)
                    resp = self._handle_request(msg)
                    self._rep_socket.send(json.dumps(resp).encode(""utf-8""))
                except zmq.ZMQError:
                    break
                except Exception:
                    try:
                        self._rep_socket.send(json.dumps({""status"": ""error"", ""error"": ""internal""}).encode(""utf-8""))
                    except Exception:
                        pass
        # drain and close handled by close_sockets_and_threads

    def run(self, address_receiver, address: str | None=None):
        '''Run the listener and answer to requests.'''
        if self._thread is not None and self._thread.is_alive():
            return
        self._ensure_context()

        self._rep_socket = self._context.socket(zmq.REP)
        self._rep_socket.linger = 0
        self._rep_socket.bind(str(address_receiver))

        if address is not None:
            self._pub_socket = self._context.socket(zmq.PUB)
            self._pub_socket.linger = 0
            self._pub_socket.bind(str(address))
            self._pub_enabled = True
        else:
            self._pub_enabled = False

        self._running.set()
        self._thread = threading.Thread(target=self._loop, name=""ZMQNameServer"", daemon=True)
        self._thread.start()

    def close_sockets_and_threads(self):
        '''Close all sockets and threads.'''
        self._running.clear()
        th = self._thread
        self._thread = None

        if th is not None and th.is_alive():
            th.join(timeout=2.0)

        try:
            if self._rep_socket is not None:
                self._rep_socket.close(0)
        finally:
            self._rep_socket = None

        try:
            if self._pub_socket is not None:
                self._pub_socket.close(0)
        finally:
            self._pub_socket = None

        ctx = self._context
        self._context = None
        if ctx is not None:
            try:
                ctx.term()
            except Exception:
                pass

    def stop(self):
        '''Stop the name server.'''
        self._running.clear()
        # Attempt to poke the loop if it's blocked on poll
        if self._rep_socket is not None:
            try:
                tmp = self._context.socket(zmq.REQ)
                tmp.linger = 0
                # If bound to tcp://*:PORT we can try connecting via localhost.
                # For inproc or ipc, connection attempts will be no-ops if not applicable.
                endpoint = None
                try:
                    # Obtain last endpoint if available (best effort)
                    endpoint = self._rep_socket.getsockopt_string(zmq.LAST_ENDPOINT)
                except Exception:
                    pass
                if endpoint:
                    tmp.connect(endpoint.replace(""*"", ""127.0.0.1""))
                    tmp.send(json.dumps({""cmd"": ""PING""}).encode(""utf-8""))
                    try:
                        tmp.recv(flags=0)
                    except Exception:
                        pass
                tmp.close(0)
            except Exception:
                pass
        self.close_sockets_and_threads()"
357239,pytroll/posttroll,posttroll/backends/zmq/socket.py,posttroll.backends.zmq.socket.SocketReceiver,"from posttroll.message import Message
import zmq

class SocketReceiver:
    """"""A receiver for mulitple sockets.""""""

    def __init__(self):
        """"""Set up the receiver.""""""
        self._poller = zmq.Poller()

    def register(self, socket):
        """"""Register the socket.""""""
        self._poller.register(socket, zmq.POLLIN)

    def unregister(self, socket):
        """"""Unregister the socket.""""""
        self._poller.unregister(socket)

    def receive(self, *sockets, timeout=None):
        """"""Timeout is in seconds.""""""
        if timeout:
            timeout *= 1000
        socks = dict(self._poller.poll(timeout=timeout))
        if socks:
            for sock in sockets:
                if socks.get(sock) == zmq.POLLIN:
                    received = sock.recv_string(zmq.NOBLOCK)
                    yield (Message.decode(received), sock)
        else:
            raise TimeoutError('Did not receive anything on sockets.')","
class SocketReceiver:
    '''A receiver for mulitple sockets.'''

    def __init__(self):
        '''Set up the receiver.'''
        pass

    def register(self, socket):
        '''Register the socket.'''
        pass

    def unregister(self, socket):
        '''Unregister the socket.'''
        pass

    def receive(self, *sockets, timeout=None):
        '''Timeout is in seconds.'''
        pass",snippet_291,"import select

class SocketReceiver:
    '''A receiver for mulitple sockets.'''

    def __init__(self):
        '''Set up the receiver.'''
        self._sockets = set()

    def register(self, socket):
        '''Register the socket.'''
        self._sockets.add(socket)

    def unregister(self, socket):
        '''Unregister the socket.'''
        self._sockets.discard(socket)

    def receive(self, *sockets, timeout=None):
        '''Timeout is in seconds.'''
        read_set = set(sockets) if sockets else set(self._sockets)
        if not read_set:
            return {}
        try:
            ready, _, _ = select.select(list(read_set), [], [], timeout)
        except Exception:
            return {}
        results = {}
        for s in ready:
            try:
                data = s.recv(4096)
            except BlockingIOError:
                continue
            except Exception:
                self.unregister(s)
                results[s] = None
                continue
            if data == b'':
                self.unregister(s)
            results[s] = data
        return results"
357241,pytroll/posttroll,posttroll/bbmcast.py,posttroll.bbmcast.MulticastReceiver,"import struct
from socket import AF_INET, INADDR_ANY, IP_ADD_MEMBERSHIP, IP_MULTICAST_IF, IP_MULTICAST_LOOP, IP_MULTICAST_TTL, IPPROTO_IP, SO_BROADCAST, SO_LINGER, SO_REUSEADDR, SOCK_DGRAM, SOL_IP, SOL_SOCKET, gethostbyname, inet_aton, socket, timeout

class MulticastReceiver:
    """"""Multicast receiver on *port* for an *mcgroup*.""""""
    BUFSIZE = 1024

    def __init__(self, port, mcgroup=None):
        """"""Set up the multicast receiver.""""""
        self.port = port
        self.socket, self.group = mcast_receiver(port, mcgroup)
        logger.info(f'Receiver initialized on group {self.group}.')

    def settimeout(self, tout=None):
        """"""Set timeout.

        A timeout will throw a 'socket.timeout'.
        """"""
        self.socket.settimeout(tout)
        return self

    def __call__(self):
        """"""Receive data from a socket.""""""
        data, sender = self.socket.recvfrom(self.BUFSIZE)
        return (data.decode(), sender)

    def close(self):
        """"""Close the receiver.""""""
        self.socket.setsockopt(SOL_SOCKET, SO_LINGER, struct.pack('ii', 1, 1))
        self.socket.close()","
class MulticastReceiver:
    '''Multicast receiver on *port* for an *mcgroup*.'''

    def __init__(self, port, mcgroup=None):
        '''Set up the multicast receiver.'''
        pass

    def settimeout(self, tout=None):
        '''Set timeout.
        A timeout will throw a 'socket.timeout'.
        '''
        pass

    def __call__(self):
        '''Receive data from a socket.'''
        pass

    def close(self):
        '''Close the receiver.'''
        pass",snippet_292,"import socket
import struct

class MulticastReceiver:
    '''Multicast receiver on *port* for an *mcgroup*.'''

    def __init__(self, port, mcgroup=None):
        '''Set up the multicast receiver.'''
        self.port = int(port)
        self.mcgroup = mcgroup
        self.sock = None
        self._family = socket.AF_INET
        self._joined = False
        self._iface_index = 0

        if mcgroup and ':' in mcgroup:
            self._family = socket.AF_INET6

        self.sock = socket.socket(self._family, socket.SOCK_DGRAM)
        try:
            self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        except OSError:
            pass
        try:
            # Not available on all platforms
            self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)
        except (AttributeError, OSError):
            pass

        if self._family == socket.AF_INET6:
            self.sock.bind(('', self.port))
            if mcgroup:
                grp_bin = socket.inet_pton(socket.AF_INET6, mcgroup)
                mreq = struct.pack('16sI', grp_bin, self._iface_index)
                self.sock.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_JOIN_GROUP, mreq)
                self._joined = True
        else:
            self.sock.bind(('', self.port))
            if mcgroup:
                mreq = struct.pack('=4s4s',
                                   socket.inet_aton(mcgroup),
                                   socket.inet_aton('0.0.0.0'))
                self.sock.setsockopt(socket.IPPROTO_IP, socket.IP_ADD_MEMBERSHIP, mreq)
                self._joined = True

    def settimeout(self, tout=None):
        '''Set timeout.
        A timeout will throw a 'socket.timeout'.
        '''
        self.sock.settimeout(tout)

    def __call__(self):
        '''Receive data from a socket.'''
        return self.sock.recvfrom(65535)

    def close(self):
        '''Close the receiver.'''
        try:
            if self._joined and self.mcgroup and self.sock:
                if self._family == socket.AF_INET6:
                    grp_bin = socket.inet_pton(socket.AF_INET6, self.mcgroup)
                    mreq = struct.pack('16sI', grp_bin, self._iface_index)
                    try:
                        self.sock.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_LEAVE_GROUP, mreq)
                    except OSError:
                        pass
                else:
                    mreq = struct.pack('=4s4s',
                                       socket.inet_aton(self.mcgroup),
                                       socket.inet_aton('0.0.0.0'))
                    try:
                        self.sock.setsockopt(socket.IPPROTO_IP, socket.IP_DROP_MEMBERSHIP, mreq)
                    except OSError:
                        pass
        finally:
            if self.sock:
                try:
                    self.sock.close()
                finally:
                    self.sock = None"
357242,pytroll/posttroll,posttroll/bbmcast.py,posttroll.bbmcast.MulticastSender,"class MulticastSender:
    """"""Multicast sender on *port* and *mcgroup*.""""""

    def __init__(self, port, mcgroup=None):
        """"""Set up the multicast sender.""""""
        self.port = port
        self.group = mcgroup
        self.socket, self.group = mcast_sender(mcgroup)
        logger.debug('Started multicast group %s', self.group)

    def __call__(self, data):
        """"""Send data to a socket.""""""
        self.socket.sendto(data.encode(), (self.group, self.port))

    def close(self):
        """"""Close the sender.""""""
        self.socket.close()","class MulticastSender:
    '''Multicast sender on *port* and *mcgroup*.'''

    def __init__(self, port, mcgroup=None):
        '''Set up the multicast sender.'''
        pass

    def __call__(self, data):
        '''Send data to a socket.'''
        pass

    def close(self):
        '''Close the sender.'''
        pass",snippet_293,"import socket
import struct

class MulticastSender:
    '''Multicast sender on *port* and *mcgroup*.'''

    def __init__(self, port, mcgroup=None):
        '''Set up the multicast sender.'''
        self.port = int(port)
        self.mcgroup = mcgroup or '224.0.0.1'
        self._sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)
        try:
            self._sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_TTL, struct.pack('B', 1))
        except OSError:
            self._sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_TTL, 1)

    def __call__(self, data):
        '''Send data to a socket.'''
        if self._sock is None:
            raise RuntimeError(""Sender is closed"")
        if isinstance(data, str):
            data = data.encode('utf-8')
        return self._sock.sendto(data, (self.mcgroup, self.port))

    def close(self):
        '''Close the sender.'''
        if self._sock is not None:
            try:
                self._sock.close()
            finally:
                self._sock = None"
357244,pytroll/posttroll,posttroll/listener.py,posttroll.listener.ListenerContainer,"from threading import Thread
from queue import Queue
import logging

class ListenerContainer:
    """"""Container for a listener instance.""""""
    logger = logging.getLogger(__name__ + '.ListenerContainer')

    def __init__(self, topics=None, addresses=None, nameserver='localhost', services=''):
        """"""Initialize the class.""""""
        self.listener = None
        self.output_queue = None
        self.thread = None
        self.addresses = addresses
        self.nameserver = nameserver
        if topics is not None:
            self.output_queue = Queue()
            self.listener = Listener(topics=topics, queue=self.output_queue, addresses=self.addresses, nameserver=self.nameserver, services=services)
            self.thread = Thread(target=self.listener.run, daemon=True)
            self.thread.start()

    def __setstate__(self, state):
        """"""Re-initialize the class.""""""
        self.__init__(**state)

    def restart_listener(self, topics):
        """"""Restart listener after configuration update.""""""
        if self.listener is not None:
            if self.listener.running:
                self.stop()
        self.__init__(topics=topics)

    def stop(self):
        """"""Stop listener.""""""
        self.logger.debug('Stopping listener.')
        self.listener.stop()
        if self.thread is not None:
            self.thread.join()
            self.thread = None
        self.logger.debug('Listener stopped.')","
class ListenerContainer:
    '''Container for a listener instance.'''

    def __init__(self, topics=None, addresses=None, nameserver='localhost', services=''):
        '''Initialize the class.'''
        pass

    def __setstate__(self, state):
        '''Re-initialize the class.'''
        pass

    def restart_listener(self, topics):
        '''Restart listener after configuration update.'''
        pass

    def stop(self):
        '''Stop listener.'''
        pass",snippet_294,"class ListenerContainer:
    '''Container for a listener instance.'''

    class _Listener:
        def __init__(self, topics, addresses, nameserver, services):
            import threading
            self.topics = list(topics)
            self.addresses = list(addresses)
            self.nameserver = nameserver
            self.services = services
            self._stop_event = threading.Event()
            self._thread = threading.Thread(target=self._run, daemon=True)

        def start(self):
            if not self._thread.is_alive():
                self._thread.start()

        def _run(self):
            # Dummy loop to simulate a running listener
            # This can be replaced with actual IO/event loop if needed.
            while not self._stop_event.wait(timeout=0.1):
                pass

        def stop(self):
            self._stop_event.set()
            if self._thread.is_alive():
                self._thread.join(timeout=1.0)

    def __init__(self, topics=None, addresses=None, nameserver='localhost', services=''):
        '''Initialize the class.'''
        self.topics = self._normalize_list(topics)
        self.addresses = self._normalize_list(addresses)
        self.nameserver = nameserver
        self.services = services
        self._listener = None
        self._start_listener()

    def __setstate__(self, state):
        '''Re-initialize the class.'''
        topics = state.get('topics', None)
        addresses = state.get('addresses', None)
        nameserver = state.get('nameserver', 'localhost')
        services = state.get('services', '')
        self.__init__(topics=topics, addresses=addresses, nameserver=nameserver, services=services)

    def restart_listener(self, topics):
        '''Restart listener after configuration update.'''
        self.topics = self._normalize_list(topics)
        self._restart_listener_internal()

    def stop(self):
        '''Stop listener.'''
        if self._listener is not None:
            try:
                self._listener.stop()
            finally:
                self._listener = None

    # Internal helpers
    @staticmethod
    def _normalize_list(value):
        if value is None:
            return []
        if isinstance(value, (list, tuple, set)):
            return list(value)
        return [value]

    def _start_listener(self):
        self.stop()
        self._listener = self._Listener(
            topics=self.topics,
            addresses=self.addresses,
            nameserver=self.nameserver,
            services=self.services,
        )
        self._listener.start()

    def _restart_listener_internal(self):
        self._start_listener()"
357253,pytroll/posttroll,posttroll/message_broadcaster.py,posttroll.message_broadcaster.DesignatedReceiversSender,"from posttroll import config, message

class DesignatedReceiversSender:
    """"""Sends message to multiple *receivers* on *port*.""""""

    def __init__(self, default_port, receivers):
        """"""Set settings.""""""
        backend = config.get('backend', 'unsecure_zmq')
        if backend == 'unsecure_zmq':
            from posttroll.backends.zmq.message_broadcaster import ZMQDesignatedReceiversSender
            self._sender = ZMQDesignatedReceiversSender(default_port, receivers)
        else:
            raise NotImplementedError()

    def __call__(self, data):
        """"""Send messages from all receivers.""""""
        return self._sender(data)

    def close(self):
        """"""Close the sender.""""""
        return self._sender.close()","
class DesignatedReceiversSender:
    '''Sends message to multiple *receivers* on *port*.'''

    def __init__(self, default_port, receivers):
        '''Set settings.'''
        pass

    def __call__(self, data):
        '''Send messages from all receivers.'''
        pass

    def close(self):
        '''Close the sender.'''
        pass",snippet_295,"import socket

class DesignatedReceiversSender:
    '''Sends message to multiple *receivers* on *port*.'''

    def __init__(self, default_port, receivers):
        '''Set settings.'''
        self._default_port = int(default_port)
        self._targets = []  # list of (family, sockaddr)
        self._sockets = {}  # family -> socket
        self._closed = False

        if receivers is None:
            receivers = []

        for r in receivers:
            for fam, sa in self._resolve_receiver(r):
                self._targets.append((fam, sa))

        # Create sockets per address family in use
        families = {fam for fam, _ in self._targets}
        for fam in families:
            s = socket.socket(fam, socket.SOCK_DGRAM)
            try:
                # Optional: allow sending without binding
                s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            except OSError:
                pass
            self._sockets[fam] = s

    def __call__(self, data):
        '''Send messages from all receivers.'''
        if self._closed:
            raise RuntimeError(""Sender is closed"")

        if isinstance(data, str):
            payload = data.encode('utf-8')
        elif isinstance(data, (bytes, bytearray, memoryview)):
            payload = bytes(data)
        else:
            payload = bytes(data)

        for fam, addr in self._targets:
            sock = self._sockets.get(fam)
            if sock is None:
                # Create on demand if missing
                sock = socket.socket(fam, socket.SOCK_DGRAM)
                self._sockets[fam] = sock
            sock.sendto(payload, addr)

    def close(self):
        '''Close the sender.'''
        if self._closed:
            return
        for s in self._sockets.values():
            try:
                s.close()
            except Exception:
                pass
        self._sockets.clear()
        self._targets.clear()
        self._closed = True

    def _resolve_receiver(self, receiver):
        # Returns list of (family, sockaddr)
        host, port = None, None

        if isinstance(receiver, tuple):
            if len(receiver) == 2:
                host, port = receiver
            elif len(receiver) == 1:
                host = receiver[0]
                port = self._default_port
            else:
                # Possibly already a sockaddr tuple; try to detect by family later
                # We will attempt to use getaddrinfo if possible; else skip
                raise ValueError(f""Unsupported receiver tuple: {receiver}"")
        elif isinstance(receiver, str):
            s = receiver.strip()
            if s.startswith('['):
                # [ipv6]:port or [ipv6]
                end = s.find(']')
                if end == -1:
                    raise ValueError(f""Invalid IPv6 receiver string: {receiver}"")
                host = s[1:end]
                rest = s[end+1:]
                if rest.startswith(':'):
                    port = rest[1:]
                else:
                    port = self._default_port
            else:
                # Determine if it's host:port (single colon) or raw host
                colon_count = s.count(':')
                if colon_count == 1:
                    host_part, port_part = s.split(':', 1)
                    host, port = host_part, port_part
                else:
                    # ambiguous IPv6 without brackets -> treat as host, default port
                    host, port = s, self._default_port
        else:
            raise ValueError(f""Unsupported receiver type: {type(receiver).__name__}"")

        if port is None:
            port = self._default_port

        try:
            port = int(port)
        except (TypeError, ValueError):
            # allow service names
            pass

        results = []
        # Resolve using getaddrinfo for UDP
        for fam, socktype, proto, _, sockaddr in socket.getaddrinfo(
            host, port, family=socket.AF_UNSPEC, type=socket.SOCK_DGRAM
        ):
            if socktype != socket.SOCK_DGRAM:
                continue
            results.append((fam, sockaddr))

        # Deduplicate
        seen = set()
        deduped = []
        for fam, sa in results:
            key = (fam, sa)
            if key not in seen:
                seen.add(key)
                deduped.append((fam, sa))

        if not deduped:
            raise OSError(f""Could not resolve receiver {receiver}"")

        return deduped"
357255,pytroll/posttroll,posttroll/ns.py,posttroll.ns.NameServer,"from posttroll.address_receiver import AddressReceiver
import datetime as dt
from posttroll import config
from contextlib import suppress

class NameServer:
    """"""The name server.""""""

    def __init__(self, max_age=None, multicast_enabled=True, restrict_to_localhost=False):
        """"""Initialize nameserver.""""""
        self.loop = True
        self.listener = None
        self._max_age = max_age or dt.timedelta(minutes=10)
        self._multicast_enabled = multicast_enabled
        self._restrict_to_localhost = restrict_to_localhost
        backend = config['backend']
        if backend not in ['unsecure_zmq', 'secure_zmq']:
            raise NotImplementedError(f'Did not recognize backend: {backend}')
        from posttroll.backends.zmq.ns import ZMQNameServer
        self._ns = ZMQNameServer()

    def run(self, address_receiver=None, nameserver_address=None):
        """"""Run the listener and answer to requests.""""""
        if address_receiver is None:
            address_receiver = AddressReceiver(max_age=self._max_age, multicast_enabled=self._multicast_enabled, restrict_to_localhost=self._restrict_to_localhost)
            address_receiver.start()
        try:
            return self._ns.run(address_receiver, nameserver_address)
        finally:
            with suppress(AttributeError):
                address_receiver.stop()

    def stop(self):
        """"""Stop the nameserver.""""""
        return self._ns.stop()","
class NameServer:
    '''The name server.'''

    def __init__(self, max_age=None, multicast_enabled=True, restrict_to_localhost=False):
        '''Initialize nameserver.'''
        pass

    def run(self, address_receiver=None, nameserver_address=None):
        '''Run the listener and answer to requests.'''
        pass

    def stop(self):
        '''Stop the nameserver.'''
        pass",snippet_296,"import json
import socket
import threading
import time
from socketserver import ThreadingMixIn, TCPServer, StreamRequestHandler


class _ThreadingTCPServer(ThreadingMixIn, TCPServer):
    daemon_threads = True
    allow_reuse_address = True


class NameServer:
    '''The name server.'''

    def __init__(self, max_age=None, multicast_enabled=True, restrict_to_localhost=False):
        '''Initialize nameserver.'''
        self.max_age = max_age if (isinstance(max_age, (int, float)) and max_age > 0) else None
        self.multicast_enabled = bool(multicast_enabled)
        self.restrict_to_localhost = bool(restrict_to_localhost)

        self._registry = {}  # name -> {""address"": str, ""time"": float, ""ttl"": float or None}
        self._lock = threading.RLock()

        self._server = None
        self._server_thread = None
        self._cleaner_thread = None
        self._stopping = threading.Event()
        self._started = threading.Event()

    def _purge_expired(self):
        if self.max_age is None:
            return
        now = time.time()
        to_del = []
        with self._lock:
            for name, meta in self._registry.items():
                ttl = meta.get(""ttl"", None)
                effective_ttl = ttl if ttl is not None else self.max_age
                if effective_ttl is None:
                    continue
                if (now - meta.get(""time"", 0)) > float(effective_ttl):
                    to_del.append(name)
            for name in to_del:
                self._registry.pop(name, None)

    def _cleaner(self):
        interval = 1.0 if not self.max_age else max(1.0, float(self.max_age) / 2.0)
        while not self._stopping.wait(interval):
            self._purge_expired()

    def _make_handler(self):
        outer = self

        class Handler(StreamRequestHandler):
            def handle(self):
                while True:
                    line = self.rfile.readline()
                    if not line:
                        break
                    try:
                        payload = json.loads(line.decode(""utf-8"").strip())
                    except Exception:
                        self._send({""ok"": False, ""error"": ""invalid_json""})
                        continue
                    try:
                        resp = self._process(payload)
                    except Exception as e:
                        self._send({""ok"": False, ""error"": f""server_error: {e.__class__.__name__}""})
                    else:
                        self._send(resp)

            def _send(self, obj):
                data = (json.dumps(obj, separators=("","", "":"")) + ""\n"").encode(""utf-8"")
                self.wfile.write(data)
                self.wfile.flush()

            def _process(self, payload):
                cmd = str(payload.get(""cmd"", """")).lower()
                if cmd == ""ping"":
                    return {""ok"": True, ""result"": ""pong""}
                if cmd == ""register"":
                    name = payload.get(""name"")
                    address = payload.get(""address"")
                    ttl = payload.get(""ttl"", None)
                    if not name or not isinstance(name, str) or not address or not isinstance(address, str):
                        return {""ok"": False, ""error"": ""invalid_arguments""}
                    if ttl is not None:
                        try:
                            ttl = float(ttl)
                            if ttl <= 0:
                                ttl = None
                        except Exception:
                            ttl = None
                    with outer._lock:
                        outer._registry[name] = {""address"": address, ""time"": time.time(), ""ttl"": ttl}
                    return {""ok"": True}
                if cmd == ""lookup"":
                    name = payload.get(""name"")
                    if not name or not isinstance(name, str):
                        return {""ok"": False, ""error"": ""invalid_arguments""}
                    outer._purge_expired()
                    with outer._lock:
                        meta = outer._registry.get(name)
                        if not meta:
                            return {""ok"": True, ""result"": None}
                        return {""ok"": True, ""result"": meta[""address""]}
                if cmd == ""unregister"":
                    name = payload.get(""name"")
                    if not name or not isinstance(name, str):
                        return {""ok"": False, ""error"": ""invalid_arguments""}
                    with outer._lock:
                        existed = name in outer._registry
                        outer._registry.pop(name, None)
                    return {""ok"": True, ""result"": bool(existed)}
                if cmd == ""list"":
                    outer._purge_expired()
                    with outer._lock:
                        items = {k: v[""address""] for k, v in outer._registry.items()}
                    return {""ok"": True, ""result"": items}
                return {""ok"": False, ""error"": ""unknown_command""}

        return Handler

    def _normalize_address(self, nameserver_address):
        if nameserver_address is None:
            host = ""127.0.0.1"" if self.restrict_to_localhost else ""0.0.0.0""
            return (host, 0)
        if isinstance(nameserver_address, int):
            host = ""127.0.0.1"" if self.restrict_to_localhost else ""0.0.0.0""
            return (host, int(nameserver_address))
        if isinstance(nameserver_address, str):
            # Accept ""host:port"" or just host (port=0)
            if "":"" in nameserver_address and not nameserver_address.startswith(""[""):
                host, port = nameserver_address.rsplit("":"", 1)
                return (host, int(port))
            return (nameserver_address, 0)
        if isinstance(nameserver_address, (list, tuple)) and len(nameserver_address) == 2:
            return (str(nameserver_address[0]), int(nameserver_address[1]))
        raise ValueError(""Invalid nameserver_address"")

    def run(self, address_receiver=None, nameserver_address=None):
        '''Run the listener and answer to requests.'''
        if self._server is not None:
            raise RuntimeError(""NameServer is already running"")

        bind_addr = self._normalize_address(nameserver_address)
        handler_cls = self._make_handler()

        self._server = _ThreadingTCPServer(bind_addr, handler_cls)

        # Report bound address
        bound = self._server.server_address
        if address_receiver is not None:
            try:
                if callable(address_receiver):
                    address_receiver(bound)
                elif hasattr(address_receiver, ""put""):
                    address_receiver.put(bound)
                elif hasattr(address_receiver, ""send""):
                    address_receiver.send(bound)
            except Exception:
                pass

        self._stopping.clear()
        self._started.set()

        if self.max_age is not None:
            self._cleaner_thread = threading.Thread(target=self._cleaner, name=""NameServerCleaner"", daemon=True)
            self._cleaner_thread.start()

        try:
            self._server.serve_forever(poll_interval=0.5)
        finally:
            try:
                self._server.server_close()
            finally:
                self._server = None
                self._started.clear()
                self._stopping.set()

    def stop(self):
        '''Stop the nameserver.'''
        if self._server is None:
            return
        self._stopping.set()
        try:
            self._server.shutdown()
        except Exception:
            pass
        # Best effort join cleaner
        if self._cleaner_thread and self._cleaner_thread.is_alive():
            self._cleaner_thread.join(timeout=2.0)
        self._cleaner_thread = None"
357257,pytroll/posttroll,posttroll/publisher.py,posttroll.publisher.Publish,"class Publish:
    """"""The publishing context.

    See :class:`Publisher` and :class:`NoisyPublisher` for more information on the arguments.

    The publisher is selected based on the arguments, see :func:`create_publisher_from_dict_config` for
    information how the selection is done.

    Example on how to use the :class:`Publish` context::

            from posttroll.publisher import Publish
            from posttroll.message import Message
            import time

            try:
                with Publish(""my_service"", port=9000) as pub:
                    counter = 0
                    while True:
                        counter += 1
                        message = Message(""/counter"", ""info"", str(counter))
                        print(""publishing"", message)
                        pub.send(message.encode())
                        time.sleep(3)
            except KeyboardInterrupt:
                print(""terminating publisher..."")

    """"""

    def __init__(self, name, port=0, aliases=None, broadcast_interval=2, nameservers=None, min_port=None, max_port=None):
        """"""Initialize the class.""""""
        settings = {'name': name, 'port': port, 'min_port': min_port, 'max_port': max_port, 'aliases': aliases, 'broadcast_interval': broadcast_interval, 'nameservers': nameservers}
        self.publisher = create_publisher_from_dict_config(settings)

    def __enter__(self):
        """"""Enter the context.""""""
        return self.publisher.start()

    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""Exit the context.""""""
        self.publisher.stop()","class Publish:
    '''The publishing context.
    See :class:`Publisher` and :class:`NoisyPublisher` for more information on the arguments.
    The publisher is selected based on the arguments, see :func:`create_publisher_from_dict_config` for
    information how the selection is done.
    Example on how to use the :class:`Publish` context::
            from posttroll.publisher import Publish
            from posttroll.message import Message
            import time
            try:
                with Publish(""my_service"", port=9000) as pub:
                    counter = 0
                    while True:
                        counter += 1
                        message = Message(""/counter"", ""info"", str(counter))
                        print(""publishing"", message)
                        pub.send(message.encode())
                        time.sleep(3)
            except KeyboardInterrupt:
                print(""terminating publisher..."")
    '''

    def __init__(self, name, port=0, aliases=None, broadcast_interval=2, nameservers=None, min_port=None, max_port=None):
        '''Initialize the class.'''
        pass

    def __enter__(self):
        '''Enter the context.'''
        pass

    def __exit__(self, exc_type, exc_val, exc_tb):
        '''Exit the context.'''
        pass",snippet_297,"class Publish:
    '''The publishing context.
    See :class:`Publisher` and :class:`NoisyPublisher` for more information on the arguments.
    The publisher is selected based on the arguments, see :func:`create_publisher_from_dict_config` for
    information how the selection is done.
    Example on how to use the :class:`Publish` context::
            from posttroll.publisher import Publish
            from posttroll.message import Message
            import time
            try:
                with Publish(""my_service"", port=9000) as pub:
                    counter = 0
                    while True:
                        counter += 1
                        message = Message(""/counter"", ""info"", str(counter))
                        print(""publishing"", message)
                        pub.send(message.encode())
                        time.sleep(3)
            except KeyboardInterrupt:
                print(""terminating publisher..."")
    '''

    def __init__(self, name, port=0, aliases=None, broadcast_interval=2, nameservers=None, min_port=None, max_port=None):
        '''Initialize the class.'''
        self._name = name
        self._port = port
        self._aliases = list(aliases) if aliases is not None else None
        self._broadcast_interval = broadcast_interval
        self._nameservers = list(nameservers) if nameservers is not None else None
        self._min_port = min_port
        self._max_port = max_port

        self._publisher = None
        self._entered_publisher = None  # Result of publisher.__enter__ if it exists

    def _create_publisher(self):
        try:
            # Prefer absolute import path as hinted by the docstring
            from posttroll.publisher import create_publisher_from_dict_config
        except Exception:
            # Fallback to relative/local import if package layout differs
            try:
                from .publisher import create_publisher_from_dict_config  # type: ignore
            except Exception as e:
                raise ImportError(""Could not import create_publisher_from_dict_config"") from e

        config = {
            ""name"": self._name,
            ""port"": self._port,
            ""aliases"": self._aliases,
            ""broadcast_interval"": self._broadcast_interval,
            ""nameservers"": self._nameservers,
            ""min_port"": self._min_port,
            ""max_port"": self._max_port,
        }
        return create_publisher_from_dict_config(config)

    def __enter__(self):
        '''Enter the context.'''
        self._publisher = self._create_publisher()

        # If underlying publisher is a context manager, use it
        enter = getattr(self._publisher, ""__enter__"", None)
        if callable(enter):
            self._entered_publisher = enter()
            return self._entered_publisher

        # Otherwise, try to ""start"" or ""open"" if available
        starter = getattr(self._publisher, ""start"", None) or getattr(self._publisher, ""open"", None)
        if callable(starter):
            starter()

        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        '''Exit the context.'''
        # If we used the publisher as a context manager, delegate exit
        if self._entered_publisher is not None:
            exit_method = getattr(self._publisher, ""__exit__"", None)
            if callable(exit_method):
                return exit_method(exc_type, exc_val, exc_tb)

        # Otherwise, try graceful shutdown methods
        closer = getattr(self._publisher, ""stop"", None) or getattr(self._publisher, ""close"", None) or getattr(self._publisher, ""shutdown"", None)
        if callable(closer):
            closer()

        # Cleanup references
        self._entered_publisher = None
        self._publisher = None
        return False

    def send(self, *args, **kwargs):
        if self._publisher is None:
            raise RuntimeError(""Publisher is not initialized. Use Publish as a context manager."")
        send_fn = getattr(self._publisher, ""send"", None)
        if not callable(send_fn):
            raise AttributeError(""Underlying publisher does not implement 'send'."")
        return send_fn(*args, **kwargs)

    def __getattr__(self, item):
        # Proxy other attributes/methods to the underlying publisher when available
        if item.startswith(""_""):
            raise AttributeError(item)
        pub = object.__getattribute__(self, ""_publisher"")
        if pub is not None and hasattr(pub, item):
            return getattr(pub, item)
        raise AttributeError(item)"
357261,pytroll/posttroll,posttroll/subscriber.py,posttroll.subscriber.Subscribe,"from posttroll.message import _MAGICK

class Subscribe:
    """"""Subscriber context.

    See :class:`NSSubscriber` and :class:`Subscriber` for initialization parameters.

    The subscriber is selected based on the arguments, see :func:`create_subscriber_from_dict_config` for
    information how the selection is done.

    Example::
            del tmp

        from posttroll.subscriber import Subscribe

        with Subscribe(""a_service"", ""my_topic"",) as sub:
            for msg in sub.recv():
                print(msg)

    """"""

    def __init__(self, services='', topics=_MAGICK, addr_listener=False, addresses=None, timeout=10, translate=False, nameserver='localhost', message_filter=None):
        """"""Initialize the class.""""""
        settings = {'services': services, 'topics': topics, 'message_filter': message_filter, 'translate': translate, 'addr_listener': addr_listener, 'addresses': addresses, 'timeout': timeout, 'nameserver': nameserver}
        self.subscriber = create_subscriber_from_dict_config(settings)

    def __enter__(self):
        """"""Start the subscriber when used as a context manager.""""""
        return self.subscriber

    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""Stop the subscriber when used as a context manager.""""""
        return self.subscriber.stop()","
class Subscribe:
    '''Subscriber context.
    See :class:`NSSubscriber` and :class:`Subscriber` for initialization parameters.
    The subscriber is selected based on the arguments, see :func:`create_subscriber_from_dict_config` for
    information how the selection is done.
    Example::
            del tmp
        from posttroll.subscriber import Subscribe
        with Subscribe(""a_service"", ""my_topic"",) as sub:
            for msg in sub.recv():
                print(msg)
    '''

    def __init__(self, services='', topics=_MAGICK, addr_listener=False, addresses=None, timeout=10, translate=False, nameserver='localhost', message_filter=None):
        '''Initialize the class.'''
        pass

    def __enter__(self):
        '''Start the subscriber when used as a context manager.'''
        pass

    def __exit__(self, exc_type, exc_val, exc_tb):
        '''Stop the subscriber when used as a context manager.'''
        pass",snippet_298,"class Subscribe:
    '''Subscriber context.
    See :class:`NSSubscriber` and :class:`Subscriber` for initialization parameters.
    The subscriber is selected based on the arguments, see :func:`create_subscriber_from_dict_config` for
    information how the selection is done.
    Example::
            del tmp
        from posttroll.subscriber import Subscribe
        with Subscribe(""a_service"", ""my_topic"",) as sub:
            for msg in sub.recv():
                print(msg)
    '''

    def __init__(self, services='', topics=_MAGICK, addr_listener=False, addresses=None, timeout=10, translate=False, nameserver='localhost', message_filter=None):
        '''Initialize the class.'''
        self._config = {
            'services': services,
            'topics': topics,
            'addr_listener': addr_listener,
            'addresses': addresses,
            'timeout': timeout,
            'translate': translate,
            'nameserver': nameserver,
            'message_filter': message_filter,
        }
        self._subscriber = None
        self._entered_obj = None

    def __enter__(self):
        '''Start the subscriber when used as a context manager.'''
        self._subscriber = create_subscriber_from_dict_config(self._config)
        # If underlying object supports context management, enter it and return what it returns
        if hasattr(self._subscriber, ""__enter__""):
            self._entered_obj = self._subscriber.__enter__()
            return self._entered_obj if self._entered_obj is not None else self._subscriber
        # Otherwise, try to start it and return the subscriber itself
        start = getattr(self._subscriber, ""start"", None)
        if callable(start):
            start()
        return self._subscriber

    def __exit__(self, exc_type, exc_val, exc_tb):
        '''Stop the subscriber when used as a context manager.'''
        if self._subscriber is None:
            return False
        # Delegate to underlying __exit__ if available
        if hasattr(self._subscriber, ""__exit__""):
            return bool(self._subscriber.__exit__(exc_type, exc_val, exc_tb))
        # Otherwise, try to gracefully stop/close
        result = False
        for meth_name in (""stop"", ""close"", ""shutdown"", ""terminate""):
            meth = getattr(self._subscriber, meth_name, None)
            if callable(meth):
                try:
                    meth()
                    result = False
                    break
                except Exception:
                    pass
        return result"
357263,pytroll/posttroll,posttroll/subscriber.py,posttroll.subscriber._AddressListener,"from posttroll.address_receiver import get_configured_address_port

class _AddressListener:
    """"""Listener for new addresses of interest.""""""

    def __init__(self, subscriber, services='', nameserver='localhost'):
        """"""Initialize address listener.""""""
        if isinstance(services, str):
            services = [services]
        self.services = services
        self.subscriber = subscriber
        address_publish_port = get_configured_address_port()
        self.subscriber.add_hook_sub('tcp://' + nameserver + ':' + str(address_publish_port), ['pytroll://address'], self.handle_msg)

    def handle_msg(self, msg):
        """"""Handle the message *msg*.""""""
        addr_ = msg.data['URI']
        status = msg.data.get('status', True)
        if status:
            msg_services = msg.data.get('service')
            for service in self.services:
                if not service or service in msg_services:
                    LOGGER.debug('Adding address %s %s', str(addr_), str(service))
                    self.subscriber.add(addr_)
                    break
        else:
            LOGGER.debug('Removing address %s', str(addr_))
            self.subscriber.remove(addr_)","
class _AddressListener:
    '''Listener for new addresses of interest.'''

    def __init__(self, subscriber, services='', nameserver='localhost'):
        '''Initialize address listener.'''
        pass

    def handle_msg(self, msg):
        '''Handle the message *msg*.'''
        pass",snippet_299,"import socket
from urllib.parse import urlparse
from typing import Any, Dict, Iterable, Optional, Set, Tuple, Union


class _AddressListener:
    '''Listener for new addresses of interest.'''

    def __init__(self, subscriber, services: Union[str, Iterable[str]] = '', nameserver: str = 'localhost'):
        '''Initialize address listener.'''
        self._subscriber = subscriber
        self._services: Set[str] = self._normalize_services(services)
        self._nameserver = nameserver

    def handle_msg(self, msg):
        '''Handle the message *msg*.'''
        parsed = self._parse_msg(msg)
        if not parsed:
            return

        service = (parsed.get('service') or '').lower() or None
        if self._services and service and service not in self._services:
            return
        if self._services and not service:
            return

        host = parsed.get('address') or parsed.get('host')
        if not host:
            return

        port = parsed.get('port')
        meta = parsed.get('metadata') or parsed.get('meta') or {}

        result = {
            'service': service,
            'host': host,
            'port': port,
            'metadata': meta,
        }

        resolved = self._resolve_addresses(host, port)
        if resolved:
            result['addresses'] = [ip for ip, _ in resolved]
            result['sockaddrs'] = [sa for _, sa in resolved]
        else:
            result['addresses'] = []
            result['sockaddrs'] = []

        self._notify(result)

    # Internal helpers

    def _notify(self, payload: Dict[str, Any]) -> None:
        target = self._subscriber
        if hasattr(target, 'on_address') and callable(getattr(target, 'on_address')):
            target.on_address(payload)
            return
        if hasattr(target, 'notify') and callable(getattr(target, 'notify')):
            target.notify(payload)
            return
        if callable(target):
            target(payload)
            return
        raise TypeError('Subscriber is not callable and lacks on_address/notify methods')

    def _normalize_services(self, services: Union[str, Iterable[str]]) -> Set[str]:
        if services is None:
            return set()
        if isinstance(services, str):
            s = services.strip()
            if not s:
                return set()
            parts = [p.strip().lower() for p in s.replace(',', ' ').split() if p.strip()]
            return set(parts)
        try:
            return {str(s).strip().lower() for s in services if str(s).strip()}
        except TypeError:
            return {str(services).strip().lower()} if str(services).strip() else set()

    def _parse_msg(self, msg: Any) -> Optional[Dict[str, Any]]:
        if msg is None:
            return None

        if isinstance(msg, dict):
            return dict(msg)

        if isinstance(msg, (list, tuple)):
            if len(msg) == 3:
                service, host, port = msg
                return {'service': str(service) if service is not None else None,
                        'address': str(host),
                        'port': int(port) if port is not None else None}
            if len(msg) == 2:
                service, host = msg
                return {'service': str(service) if service is not None else None,
                        'address': str(host)}
            if len(msg) == 1:
                return self._parse_msg(msg[0])
            return None

        if isinstance(msg, str):
            s = msg.strip()
            if not s:
                return None
            parsed = urlparse(s)
            if parsed.scheme and (parsed.netloc or parsed.path):
                host = parsed.hostname or parsed.path or ''
                port = parsed.port
                return {'service': parsed.scheme.lower(),
                        'address': host,
                        'port': port}
            parts = s.replace('://', ' ').split()
            if len(parts) == 3:
                service, host, port = parts
                try:
                    port = int(port)
                except ValueError:
                    port = None
                return {'service': service.lower(), 'address': host, 'port': port}
            if len(parts) == 2:
                a, b = parts
                # Heuristic: if b is int-like, then (host, port); otherwise (service, host)
                try:
                    port = int(b)
                    return {'service': None, 'address': a, 'port': port}
                except ValueError:
                    return {'service': a.lower(), 'address': b}
            if len(parts) == 1:
                return {'service': None, 'address': parts[0]}
            return None

        # Fallback to string conversion
        return self._parse_msg(str(msg))

    def _resolve_addresses(self, host: str, port: Optional[int]) -> Optional[list]:
        try:
            # getaddrinfo will use system resolver; nameserver hint is stored but not enforced
            # We attempt both stream and datagram for broader coverage
            infos = []
            for socktype in (socket.SOCK_STREAM, socket.SOCK_DGRAM):
                try:
                    gai = socket.getaddrinfo(host, port or 0, socket.AF_UNSPEC, socktype)
                    for family, stype, proto, _canon, sockaddr in gai:
                        ip = sockaddr[0]
                        infos.append((ip, sockaddr))
                except socket.gaierror:
                    continue
            # Deduplicate by sockaddr
            seen = set()
            unique = []
            for ip, sa in infos:
                key = (ip, sa)
                if key in seen:
                    continue
                seen.add(key)
                unique.append((ip, sa))
            return unique
        except Exception:
            return None

    def __repr__(self) -> str:
        return f""<_AddressListener services={sorted(self._services) if self._services else '*'} nameserver={self._nameserver!r}>"""
360979,reanahub/reana-commons,reanahub_reana-commons/reana_commons/validation/compute_backends.py,reana_commons.validation.compute_backends.ComputeBackendValidatorBase,"from typing import Dict, List, Optional
from reana_commons.errors import REANAValidationError

class ComputeBackendValidatorBase:
    """"""REANA workflow compute backend validation base class.""""""

    def __init__(self, workflow_steps: Optional[List[Dict]]=None, supported_backends: Optional[List[str]]=[]):
        """"""Validate compute backends in REANA workflow steps.

        :param workflow_steps: list of dictionaries which represents different steps involved in workflow.
        :param supported_backends: a list of the supported compute backends.
        """"""
        self.workflow_steps = workflow_steps
        self.supported_backends = supported_backends

    def validate(self) -> None:
        """"""Validate compute backends in REANA workflow.""""""
        raise NotImplementedError

    def raise_error(self, compute_backend: str, step_name: str) -> None:
        """"""Raise validation error.""""""
        raise REANAValidationError(f'''Compute backend ""{compute_backend}"" found in step ""{step_name}"" is not supported. List of supported compute backends: ""{', '.join(self.supported_backends)}""''')","
class ComputeBackendValidatorBase:
    '''REANA workflow compute backend validation base class.'''

    def __init__(self, workflow_steps: Optional[List[Dict]]=None, supported_backends: Optional[List[str]]=[]):
        '''Validate compute backends in REANA workflow steps.
        :param workflow_steps: list of dictionaries which represents different steps involved in workflow.
        :param supported_backends: a list of the supported compute backends.
        '''
        pass

    def validate(self) -> None:
        '''Validate compute backends in REANA workflow.'''
        pass

    def raise_error(self, compute_backend: str, step_name: str) -> None:
        '''Raise validation error.'''
        pass",snippet_300,"from typing import Optional, List, Dict, Any, Iterable


class ComputeBackendValidatorBase:
    '''REANA workflow compute backend validation base class.'''

    def __init__(self, workflow_steps: Optional[List[Dict]] = None, supported_backends: Optional[List[str]] = []):
        '''Validate compute backends in REANA workflow steps.
        :param workflow_steps: list of dictionaries which represents different steps involved in workflow.
        :param supported_backends: a list of the supported compute backends.
        '''
        # Avoid mutable default pitfalls
        self.workflow_steps: List[Dict[str, Any]] = list(workflow_steps or [])
        self.supported_backends: List[str] = list(supported_backends or [])
        # Normalized set for comparison (case-insensitive)
        self._supported_norm = {self._normalize_backend(b) for b in self.supported_backends if isinstance(b, str)}

    def validate(self) -> None:
        '''Validate compute backends in REANA workflow.'''
        if not self.workflow_steps or not self._supported_norm:
            return

        for idx, step in enumerate(self.workflow_steps):
            step_name = self._extract_step_name(step, idx)
            for backend in self._extract_compute_backends(step):
                norm_backend = self._normalize_backend(backend)
                if norm_backend not in self._supported_norm:
                    self.raise_error(backend, step_name)

    def raise_error(self, compute_backend: str, step_name: str) -> None:
        '''Raise validation error.'''
        supported = "", "".join(sorted(self.supported_backends))
        msg = (
            f""Unsupported compute backend '{compute_backend}' in step '{step_name}'. ""
            f""Supported backends are: {supported or 'none'}.""
        )
        raise ValueError(msg)

    def _extract_step_name(self, step: Dict[str, Any], index: int) -> str:
        return str(step.get(""name"") or step.get(""id"") or f""step_{index}"")

    def _extract_compute_backends(self, step: Dict[str, Any]) -> List[str]:
        # Possible locations and shapes of compute backend info
        candidates: List[Any] = []

        # Direct key
        if ""compute_backend"" in step:
            candidates.append(step.get(""compute_backend""))

        # Nested common structures
        for key in (""resources"", ""scheduler"", ""compute"", ""hints"", ""runner""):
            nested = step.get(key)
            if isinstance(nested, dict) and ""compute_backend"" in nested:
                candidates.append(nested.get(""compute_backend""))

        # Flatten and normalize to list of strings
        backends: List[str] = []
        for c in candidates:
            if c is None:
                continue
            if isinstance(c, str):
                if c.strip():
                    backends.append(c.strip())
            elif isinstance(c, (list, tuple, set)):
                for item in c:
                    if isinstance(item, str) and item.strip():
                        backends.append(item.strip())
            elif isinstance(c, dict):
                # Some formats might provide {type: ""kubernetes""} or similar
                for k in (""type"", ""name"", ""backend""):
                    v = c.get(k)
                    if isinstance(v, str) and v.strip():
                        backends.append(v.strip())
        # Deduplicate while preserving order
        seen = set()
        result: List[str] = []
        for b in backends:
            lb = b.lower()
            if lb not in seen:
                seen.add(lb)
                result.append(b)
        return result

    def _normalize_backend(self, backend: Optional[str]) -> str:
        return (backend or """").strip().lower()"
361134,raymondEhlers/pachyderm,raymondEhlers_pachyderm/src/pachyderm/fit/base.py,pachyderm.fit.base.BaseFitResult,"import numpy as np
import numpy.typing as npt
from dataclasses import dataclass
from typing import TYPE_CHECKING, Any, TypeVar, cast

@dataclass
class BaseFitResult:
    """"""Base fit result.

    This represents the most basic fit result.

    Attributes:
        parameters: Names of the parameters used in the fit.
        free_parameters: Names of the free parameters used in the fit.
        fixed_parameters: Names of the fixed parameters used in the fit.
        values_at_minimum: Contains the values of the full RP fit function at the minimum. Keys are the
            names of parameters, while values are the numerical values at convergence.
        errors_on_parameters: Contains the values of the errors associated with the parameters
            determined via the fit.
        covariance_matrix: Contains the values of the covariance matrix. Keys are tuples
            with (param_name_a, param_name_b), and the values are covariance between the specified parameters.
            Note that fixed parameters are _not_ included in this matrix.
        errors: Store the errors associated with the component fit function.
    """"""
    parameters: list[str]
    free_parameters: list[str]
    fixed_parameters: list[str]
    values_at_minimum: dict[str, float]
    errors_on_parameters: dict[str, float]
    covariance_matrix: dict[tuple[str, str], float]
    errors: npt.NDArray[Any]

    @property
    def correlation_matrix(self) -> dict[tuple[str, str], float]:
        """"""The correlation matrix of the free parameters.

        These values are derived from the covariance matrix values stored in the fit.

        Note:
            This property caches the correlation matrix value so we don't have to calculate it every time.

        Args:
            None
        Returns:
            The correlation matrix of the fit result.
        """"""
        try:
            return self._correlation_matrix
        except AttributeError:

            def corr(i_name: str, j_name: str) -> float:
                """"""Calculate the correlation matrix (definition from iminuit) from the covariance matrix.""""""
                value = self.covariance_matrix[i_name, j_name] / (np.sqrt(self.covariance_matrix[i_name, i_name] * self.covariance_matrix[j_name, j_name]) + 1e-100)
                return float(value)
            matrix: dict[tuple[str, str], float] = {}
            for i_name in self.free_parameters:
                for j_name in self.free_parameters:
                    matrix[i_name, j_name] = corr(i_name, j_name)
            self._correlation_matrix = matrix
        return self._correlation_matrix","@dataclass
class BaseFitResult:
    '''Base fit result.
    This represents the most basic fit result.
    Attributes:
        parameters: Names of the parameters used in the fit.
        free_parameters: Names of the free parameters used in the fit.
        fixed_parameters: Names of the fixed parameters used in the fit.
        values_at_minimum: Contains the values of the full RP fit function at the minimum. Keys are the
            names of parameters, while values are the numerical values at convergence.
        errors_on_parameters: Contains the values of the errors associated with the parameters
            determined via the fit.
        covariance_matrix: Contains the values of the covariance matrix. Keys are tuples
            with (param_name_a, param_name_b), and the values are covariance between the specified parameters.
            Note that fixed parameters are _not_ included in this matrix.
        errors: Store the errors associated with the component fit function.
    '''
    @property
    def correlation_matrix(self) -> dict[tuple[str, str], float]:
        '''The correlation matrix of the free parameters.
        These values are derived from the covariance matrix values stored in the fit.
        Note:
            This property caches the correlation matrix value so we don't have to calculate it every time.
        Args:
            None
        Returns:
            The correlation matrix of the fit result.
        '''
        pass
    def correlation_matrix(self) -> dict[tuple[str, str], float]:
        '''Calculate the correlation matrix (definition from iminuit) from the covariance matrix.'''
        pass",snippet_301,"from __future__ import annotations

from dataclasses import dataclass, field
from math import sqrt
from typing import Dict, List, Optional, Tuple


@dataclass
class BaseFitResult:
    '''Base fit result.
    This represents the most basic fit result.
    Attributes:
        parameters: Names of the parameters used in the fit.
        free_parameters: Names of the free parameters used in the fit.
        fixed_parameters: Names of the fixed parameters used in the fit.
        values_at_minimum: Contains the values of the full RP fit function at the minimum. Keys are the
            names of parameters, while values are the numerical values at convergence.
        errors_on_parameters: Contains the values of the errors associated with the parameters
            determined via the fit.
        covariance_matrix: Contains the values of the covariance matrix. Keys are tuples
            with (param_name_a, param_name_b), and the values are covariance between the specified parameters.
            Note that fixed parameters are _not_ included in this matrix.
        errors: Store the errors associated with the component fit function.
    '''
    parameters: List[str]
    free_parameters: List[str]
    fixed_parameters: List[str]
    values_at_minimum: Dict[str, float]
    errors_on_parameters: Dict[str, float]
    covariance_matrix: Dict[Tuple[str, str], float]
    errors: Dict[str, float]

    _correlation_matrix_cache: Optional[Dict[Tuple[str, str], float]] = field(default=None, init=False, repr=False)

    @property
    def correlation_matrix(self) -> dict[tuple[str, str], float]:
        '''The correlation matrix of the free parameters.
        These values are derived from the covariance matrix values stored in the fit.
        Note:
            This property caches the correlation matrix value so we don't have to calculate it every time.
        Args:
            None
        Returns:
            The correlation matrix of the fit result.
        '''
        if self._correlation_matrix_cache is None:
            self._correlation_matrix_cache = self.calculate_correlation_matrix()
        return self._correlation_matrix_cache

    def calculate_correlation_matrix(self) -> dict[tuple[str, str], float]:
        '''Calculate the correlation matrix (definition from iminuit) from the covariance matrix.'''
        corr: Dict[Tuple[str, str], float] = {}

        def cov(a: str, b: str) -> Optional[float]:
            if (a, b) in self.covariance_matrix:
                return self.covariance_matrix[(a, b)]
            if (b, a) in self.covariance_matrix:
                return self.covariance_matrix[(b, a)]
            return None

        # Precompute standard deviations for free parameters
        std: Dict[str, Optional[float]] = {}
        for p in self.free_parameters:
            v = cov(p, p)
            if v is None or v <= 0.0:
                std[p] = None
            else:
                std[p] = sqrt(v)

        # Build full symmetric correlation matrix for free parameters
        for a in self.free_parameters:
            for b in self.free_parameters:
                if a == b:
                    # Diagonal correlation is 1 if variance is defined, else 1 (conventional fallback)
                    corr[(a, b)] = 1.0
                    continue

                va = std.get(a)
                vb = std.get(b)
                cab = cov(a, b)

                if va is None or vb is None or cab is None:
                    # If missing information, default to 0.0 correlation off-diagonal
                    corr[(a, b)] = 0.0
                else:
                    denom = va * vb
                    corr[(a, b)] = cab / denom if denom != 0.0 else 0.0

        return corr"
361458,AtteqCom/zsl,AtteqCom_zsl/src/zsl/errors.py,zsl.errors.ErrorHandler,"from abc import ABCMeta, abstractmethod

class ErrorHandler:
    """"""
    Custom error handler providing a response on a particular error.
    """"""
    __metaclass__ = ABCMeta

    @abstractmethod
    def can_handle(self, e):
        """"""
        Indicator if the handler is able to handle the given exception `e`.

        :param e: The exception that shall be determined if can be handled by the handler.
        :return: `True` or `False` depending on whether the handler can/should handle the method.
        """"""
        pass

    @abstractmethod
    def handle(self, e):
        """"""
        Handle the exception.

        :param e: The handled exception.
        :return: The error response for the exception.
        """"""
        pass","
class ErrorHandler:
    '''
    Custom error handler providing a response on a particular error.
    '''
    @abstractmethod
    def can_handle(self, e):
        '''
        Indicator if the handler is able to handle the given exception `e`.
        :param e: The exception that shall be determined if can be handled by the handler.
        :return: `True` or `False` depending on whether the handler can/should handle the method.
        '''
        pass
    @abstractmethod
    def handle(self, e):
        '''
        Handle the exception.
        :param e: The handled exception.
        :return: The error response for the exception.
        '''
        pass",snippet_302,"from abc import ABC, abstractmethod
from typing import Any


class ErrorHandler(ABC):
    '''
    Custom error handler providing a response on a particular error.
    '''

    @abstractmethod
    def can_handle(self, e: Exception) -> bool:
        '''
        Indicator if the handler is able to handle the given exception `e`.
        :param e: The exception that shall be determined if can be handled by the handler.
        :return: True if the handler can/should handle the exception, otherwise False.
        '''
        raise NotImplementedError

    @abstractmethod
    def handle(self, e: Exception) -> Any:
        '''
        Handle the exception.
        :param e: The handled exception.
        :return: The error response for the exception.
        '''
        raise NotImplementedError"
361483,AtteqCom/zsl,AtteqCom_zsl/src/zsl/resource/model_resource.py,zsl.resource.model_resource.ReadOnlyResourceMixin,"class ReadOnlyResourceMixin:
    """"""
    The mixin to be used to forbid the update/delete and create operations.
    Remember the Python's MRO and place this mixin at the right place in the inheritance declaration.

    .. automethod:: create
    .. automethod:: update
    .. automethod:: delete
    """"""
    OPERATION_CREATE = 'create'
    OPERATION_UPDATE = 'update'
    OPERATION_DELETE = 'delete'

    @staticmethod
    def create(params, args, data):
        """"""Raises exception.

        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.

        :raises ReadOnlyResourceUpdateOperationException: when accessed
        """"""
        raise ReadOnlyResourceUpdateOperationException(ReadOnlyResourceMixin.OPERATION_CREATE)

    @staticmethod
    def update(params, args, data):
        """"""Raises exception.

        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.

        :raises ReadOnlyResourceUpdateOperationException: when accessed
        """"""
        raise ReadOnlyResourceUpdateOperationException(ReadOnlyResourceMixin.OPERATION_UPDATE)

    @staticmethod
    def delete(params, args, data):
        """"""Raises exception.

        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.

        :raises ReadOnlyResourceUpdateOperationException: when accessed
        """"""
        raise ReadOnlyResourceUpdateOperationException(ReadOnlyResourceMixin.OPERATION_DELETE)","class ReadOnlyResourceMixin:
    '''
    The mixin to be used to forbid the update/delete and create operations.
    Remember the Python's MRO and place this mixin at the right place in the inheritance declaration.
    .. automethod:: create
    .. automethod:: update
    .. automethod:: delete
    '''
    @staticmethod
    def create(params, args, data):
        '''Raises exception.
        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.
        :raises ReadOnlyResourceUpdateOperationException: when accessed
        '''
        pass
    @staticmethod
    def update(params, args, data):
        '''Raises exception.
        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.
        :raises ReadOnlyResourceUpdateOperationException: when accessed
        '''
        pass
    @staticmethod
    def delete(params, args, data):
        '''Raises exception.
        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.
        :raises ReadOnlyResourceUpdateOperationException: when accessed
        '''
        pass",snippet_303,"class ReadOnlyResourceUpdateOperationException(Exception):
    pass


class ReadOnlyResourceMixin:
    '''
    The mixin to be used to forbid the update/delete and create operations.
    Remember the Python's MRO and place this mixin at the right place in the inheritance declaration.
    .. automethod:: create
    .. automethod:: update
    .. automethod:: delete
    '''
    @staticmethod
    def create(params, args, data):
        '''Raises exception.
        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.
        :raises ReadOnlyResourceUpdateOperationException: when accessed
        '''
        raise ReadOnlyResourceUpdateOperationException(""Create operation is not available on a read-only resource."")

    @staticmethod
    def update(params, args, data):
        '''Raises exception.
        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.
        :raises ReadOnlyResourceUpdateOperationException: when accessed
        '''
        raise ReadOnlyResourceUpdateOperationException(""Update operation is not available on a read-only resource."")

    @staticmethod
    def delete(params, args, data):
        '''Raises exception.
        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.
        :raises ReadOnlyResourceUpdateOperationException: when accessed
        '''
        raise ReadOnlyResourceUpdateOperationException(""Delete operation is not available on a read-only resource."")"
361534,AtteqCom/zsl,AtteqCom_zsl/src/zsl/utils/command_dispatcher.py,zsl.utils.command_dispatcher.CommandDispatcher,"import inspect

class CommandDispatcher:
    """"""
    A simple class for command dictionary. A command is a function
    which can take named parameters.
    """"""

    def __init__(self):
        """"""
        Create command dictionary
        """"""
        self.commands = {}

    def command(self, fn):
        """"""
        Add method or function to dispatcher. Can be use as a nice
        decorator.

        :param fn: function or method
        :type fn: function
        :return: the same function
        :rtype: function
        """"""
        self.commands[fn.__name__] = fn
        return fn
    'alias for ``CommandDispatcher.command``'
    add_function = command

    def execute_command(self, command, args=None):
        """"""
        Execute a command

        :param command: name of the command
        :type command: str
        :param args: optional named arguments for command
        :type args: dict
        :return: the result of command
        :raises KeyError: if command is not found
        """"""
        if args is None:
            args = {}
        command_fn = self.commands[command]
        return command_fn(**args)

    def bound(self, instance):
        """"""
        Return a new dispatcher, which will switch all command functions
        with bounded methods of given instance matched by name. It will
        match only regular methods.

        :param instance: object instance
        :type instance: object
        :return: new Dispatcher
        :rtype: CommandDispatcher
        """"""
        bounded_dispatcher = CommandDispatcher()
        bounded_dispatcher.commands = self.commands.copy()
        for name in self.commands:
            method = getattr(instance, name, None)
            if method and inspect.ismethod(method) and (method.__self__ == instance):
                bounded_dispatcher.commands[name] = method
        return bounded_dispatcher","
class CommandDispatcher:
    '''
    A simple class for command dictionary. A command is a function
    which can take named parameters.
    '''

    def __init__(self):
        '''
        Create command dictionary
        '''
        pass

    def command(self, fn):
        '''
        Add method or function to dispatcher. Can be use as a nice
        decorator.
        :param fn: function or method
        :type fn: function
        :return: the same function
        :rtype: function
        '''
        pass

    def execute_command(self, command, args=None):
        '''
        Execute a command
        :param command: name of the command
        :type command: str
        :param args: optional named arguments for command
        :type args: dict
        :return: the result of command
        :raises KeyError: if command is not found
        '''
        pass

    def bound(self, instance):
        '''
        Return a new dispatcher, which will switch all command functions
        with bounded methods of given instance matched by name. It will
        match only regular methods.
        :param instance: object instance
        :type instance: object
        :return: new Dispatcher
        :rtype: CommandDispatcher
        '''
        pass",snippet_304,"import inspect


class CommandDispatcher:
    '''
    A simple class for command dictionary. A command is a function
    which can take named parameters.
    '''

    def __init__(self):
        '''
        Create command dictionary
        '''
        self._commands = {}

    def command(self, fn):
        '''
        Add method or function to dispatcher. Can be use as a nice
        decorator.
        :param fn: function or method
        :type fn: function
        :return: the same function
        :rtype: function
        '''
        if not callable(fn):
            raise TypeError(""fn must be callable"")
        self._commands[fn.__name__] = fn
        return fn

    def execute_command(self, command, args=None):
        '''
        Execute a command
        :param command: name of the command
        :type command: str
        :param args: optional named arguments for command
        :type args: dict
        :return: the result of command
        :raises KeyError: if command is not found
        '''
        if command not in self._commands:
            raise KeyError(f""Command not found: {command}"")
        fn = self._commands[command]
        if args is None:
            return fn()
        if not isinstance(args, dict):
            raise TypeError(""args must be a dict of keyword arguments"")
        return fn(**args)

    def bound(self, instance):
        '''
        Return a new dispatcher, which will switch all command functions
        with bounded methods of given instance matched by name. It will
        match only regular methods.
        :param instance: object instance
        :type instance: object
        :return: new Dispatcher
        :rtype: CommandDispatcher
        '''
        new_dispatcher = CommandDispatcher()
        cls = type(instance)
        for name, func in self._commands.items():
            bound_callable = func
            try:
                raw_attr = inspect.getattr_static(cls, name)
            except AttributeError:
                raw_attr = None

            if raw_attr is not None and inspect.isfunction(raw_attr):
                candidate = getattr(instance, name, None)
                if inspect.ismethod(candidate):
                    bound_callable = candidate

            new_dispatcher._commands[name] = bound_callable
        return new_dispatcher"
362561,reanahub/reana-db,reanahub_reana-db/reana_db/models.py,reana_db.models.QuotaBase,"class QuotaBase:
    """"""Quota base functionality.""""""

    def _get_quota_by_type(self, resource_type):
        """"""Aggregate quota usage by resource type.""""""

        def _get_health_status(usage, limit):
            """"""Calculate quota health status.""""""
            health = QuotaHealth.healthy
            if limit:
                percentage = usage / limit * 100
                if percentage >= 80:
                    if percentage >= 100:
                        health = QuotaHealth.critical
                    else:
                        health = QuotaHealth.warning
            return health.name
        quota_usage = 0
        quota_limit = 0
        unit = None
        for resource in self.resources:
            if resource.resource.type_ == resource_type:
                if unit and unit != resource.resource.unit:
                    raise Exception('Error while calculating quota usage. Not all resources of resource type {} use the same units.'.format(resource_type))
                unit = resource.resource.unit
                quota_usage += resource.quota_used
                if hasattr(resource, 'quota_limit'):
                    quota_limit += resource.quota_limit
        usage_dict = {'usage': {'raw': quota_usage, 'human_readable': ResourceUnit.human_readable_unit(unit, quota_usage)}}
        if quota_limit:
            usage_dict['limit'] = {'raw': quota_limit, 'human_readable': ResourceUnit.human_readable_unit(unit, quota_limit)}
            usage_dict['health'] = _get_health_status(quota_usage, quota_limit)
        return usage_dict

    def get_quota_usage(self):
        """"""Get quota usage information.""""""
        used_resource_types = set((res.resource.type_ for res in self.resources))
        return {resource_type.name: self._get_quota_by_type(resource_type) for resource_type in used_resource_types}","class QuotaBase:
    '''Quota base functionality.'''

    def _get_quota_by_type(self, resource_type):
        '''Aggregate quota usage by resource type.'''
        pass

        def _get_health_status(usage, limit):
                '''Calculate quota health status.'''
                pass

    def get_quota_usage(self):
        '''Get quota usage information.'''
        pass",snippet_305,"class QuotaBase:
    '''Quota base functionality.'''

    def _get_quota_by_type(self, resource_type):
        '''Aggregate quota usage by resource type.'''
        total_usage = 0
        total_limit = 0
        has_limit = False

        resources = []
        if hasattr(self, ""resources""):
            resources = getattr(self, ""resources"") or []
        elif hasattr(self, ""get_resources"") and callable(getattr(self, ""get_resources"")):
            resources = getattr(self, ""get_resources"")() or []

        for res in resources:
            if not isinstance(res, dict):
                continue
            if res.get(""type"") != resource_type:
                continue
            usage = res.get(""usage"")
            limit = res.get(""limit"")

            try:
                usage_val = float(usage) if usage is not None else 0.0
            except (TypeError, ValueError):
                usage_val = 0.0

            if limit is None:
                # If any item has unknown limit, treat overall limit as unknown
                pass
            else:
                try:
                    limit_val = float(limit)
                except (TypeError, ValueError):
                    limit_val = None

                if limit_val is not None:
                    total_limit += max(limit_val, 0.0)
                    has_limit = True

            total_usage += max(usage_val, 0.0)

        limit_value = total_limit if has_limit else None

        if limit_value is None:
            remaining = None
            percentage = None
        else:
            remaining = max(limit_value - total_usage, 0.0)
            percentage = (total_usage / limit_value * 100.0) if limit_value > 0 else 0.0

        status = self._get_health_status(total_usage, limit_value)

        return {
            ""type"": resource_type,
            ""usage"": total_usage,
            ""limit"": limit_value,
            ""remaining"": remaining,
            ""percentage"": round(percentage, 2) if percentage is not None else None,
            ""status"": status,
        }

    def _get_health_status(self, usage, limit):
        '''Calculate quota health status.'''
        try:
            usage_val = float(usage) if usage is not None else 0.0
        except (TypeError, ValueError):
            usage_val = 0.0

        if limit is None:
            return ""unknown""

        try:
            limit_val = float(limit)
        except (TypeError, ValueError):
            return ""unknown""

        if limit_val <= 0:
            return ""unknown""

        ratio = usage_val / limit_val

        if ratio >= 0.9:
            return ""critical""
        if ratio >= 0.75:
            return ""warning""
        return ""ok""

    def get_quota_usage(self):
        '''Get quota usage information.'''
        resources = []
        if hasattr(self, ""resources""):
            resources = getattr(self, ""resources"") or []
        elif hasattr(self, ""get_resources"") and callable(getattr(self, ""get_resources"")):
            resources = getattr(self, ""get_resources"")() or []

        types = []
        seen = set()
        for res in resources:
            if not isinstance(res, dict):
                continue
            rtype = res.get(""type"")
            if rtype is None:
                continue
            if rtype not in seen:
                seen.add(rtype)
                types.append(rtype)

        result = {}
        for t in types:
            result[t] = self._get_quota_by_type(t)

        return result"
