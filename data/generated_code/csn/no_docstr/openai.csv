id,repository_name,file_path,class_name,human_written_code,class_skeleton,snippet_id,total_program_units,openai/gpt-oss-20b_generated_code
17249,jbarlow83/OCRmyPDF,src/ocrmypdf/quality.py,ocrmypdf.quality.OcrQualityDictionary,"import re
from collections.abc import Iterable

class OcrQualityDictionary:
    """"""Manages a dictionary for simple OCR quality checks.""""""

    def __init__(self, *, wordlist: Iterable[str]):
        """"""Construct a dictionary from a list of words.

        Words for which capitalization is important should be capitalized in the
        dictionary. Words that contain spaces or other punctuation will never match.
        """"""
        self.dictionary = set(wordlist)

    def measure_words_matched(self, ocr_text: str) -> float:
        """"""Check how many unique words in the OCR text match a dictionary.

        Words with mixed capitalized are only considered a match if the test word
        matches that capitalization.

        Returns:
            number of words that match / number
        """"""
        text = re.sub('[0-9_]+', ' ', ocr_text)
        text = re.sub('\\W+', ' ', text)
        text_words_list = re.split('\\s+', text)
        text_words = {w for w in text_words_list if len(w) >= 3}
        matches = 0
        for w in text_words:
            if w in self.dictionary or (w != w.lower() and w.lower() in self.dictionary):
                matches += 1
        if matches > 0:
            hit_ratio = matches / len(text_words)
        else:
            hit_ratio = 0.0
        return hit_ratio","
class OcrQualityDictionary:
    
    def __init__(self, *, wordlist: Iterable[str]):
        
        pass
    def measure_words_matched(self, ocr_text: str) -> float:
        
        pass",snippet_2,3,"```python
import re
from typing import Iterable, Set


class OcrQualityDictionary:
    def __init__(self, *, wordlist: Iterable[str]):
        # Store the dictionary words in a set for fast lookup, normalised to lowercase
        self._dict_set: Set[str] = {word.lower() for word in wordlist}

    def measure_words_matched(self, ocr_text: str) -> float:
        """"""
        Return the fraction of words in `ocr_text` that are present in the dictionary.
        Words are extracted using a simple word boundary regex and compared case‑insensitively.
        If the input contains no words, 0.0 is returned.
        """"""
        # Extract words (alphanumeric sequences) from the OCR text
        words = re.findall(r""\b\w+\b"", ocr_text)
        if not words:
            return 0.0

        # Count matches
        matched = sum(1 for w in words if w.lower() in self._dict_set)
        return matched / len(words)
```"
19627,slundberg/shap,slundberg_shap/shap/_serializable.py,shap._serializable.Serializable,"import inspect
import pickle

class Serializable:
    """"""This is the superclass of all serializable objects.""""""

    def save(self, out_file):
        """"""Save the model to the given file stream.""""""
        pickle.dump(type(self), out_file)

    @classmethod
    def load(cls, in_file, instantiate=True):
        """"""This is meant to be overridden by subclasses and called with super.

        We return constructor argument values when not being instantiated. Since there are no
        constructor arguments for the Serializable class we just return an empty dictionary.
        """"""
        if instantiate:
            return cls._instantiated_load(in_file)
        return {}

    @classmethod
    def _instantiated_load(cls, in_file, **kwargs):
        """"""This is meant to be overridden by subclasses and called with super.

        We return constructor argument values (we have no values to load in this abstract class).
        """"""
        obj_type = pickle.load(in_file)
        if obj_type is None:
            return None
        if not inspect.isclass(obj_type) or (not issubclass(obj_type, cls) and obj_type is not cls):
            raise Exception(f'Invalid object type loaded from file. {obj_type} is not a subclass of {cls}.')
        constructor_args = obj_type.load(in_file, instantiate=False, **kwargs)
        used_args = inspect.getfullargspec(obj_type.__init__)[0]
        return obj_type(**{k: constructor_args[k] for k in constructor_args if k in used_args})","
class Serializable:
    
    def save(self, out_file):
        
        pass
    @classmethod
    def load(cls, in_file, instantiate=True):
        
        pass
    @classmethod
    def _instantiated_load(cls, in_file, **kwargs):
        
        pass",snippet_3,4,"```python
import pickle
from typing import Any, Type, TypeVar, Union, IO

T = TypeVar('T', bound='Serializable')


class Serializable:
    """"""
    A simple mixin that provides pickle‑based serialization.
    """"""

    def save(self, out_file: Union[str, IO[bytes]]) -> None:
        """"""
        Serialize the instance to the given file object or file path.

        Parameters
        ----------
        out_file : str or file-like object
            Destination for the pickled data. If a string is provided,
            it is treated as a file path and opened in binary write mode.
        """"""
        if isinstance(out_file, str):
            with open(out_file, 'wb') as f:
                pickle.dump(self, f)
        else:
            pickle.dump(self, out_file)

    @classmethod
    def load(cls: Type[T], in_file: Union[str, IO[bytes]], instantiate: bool = True) -> Union[T, Any]:
        """"""
        Load a pickled object from the given file object or file path.

        Parameters
        ----------
        in_file : str or file-like object
            Source of the pickled data. If a string is provided,
            it is treated as a file path and opened in binary read mode.
        instantiate : bool, default True
            If True, return the deserialized instance.
            If False, return the raw data (useful for custom handling).

        Returns
        -------
        T or Any
            The deserialized object or raw data.
        """"""
        if isinstance(in_file, str):
            with open(in_file, 'rb') as f:
                data = pickle.load(f)
        else:
            data = pickle.load(in_file)

        if instantiate:
            return cls._instantiated_load(data)
        return data

    @classmethod
    def _instantiated_load(cls: Type[T], data: Any, **kwargs: Any) -> T:
        """"""
        Helper that returns an instance of the class from the loaded data.
        Any keyword arguments are applied as attributes to the instance.

        Parameters
        ----------
        data : Any
            The object returned by pickle.load.
        **kwargs : Any
            Attributes to set on the instance after loading.

        Returns
        -------
        T
            The instance of the class.
        """"""
        if not isinstance(data, cls):
            raise TypeError(f""Loaded object is not an instance of {cls.__name__}"")
        for key, value in kwargs.items():
            setattr(data, key, value)
        return data
```"
19720,audreyr/cookiecutter,audreyr_cookiecutter/cookiecutter/environment.py,cookiecutter.environment.ExtensionLoaderMixin,"from cookiecutter.exceptions import UnknownExtension
from typing import Any

class ExtensionLoaderMixin:
    """"""Mixin providing sane loading of extensions specified in a given context.

    The context is being extracted from the keyword arguments before calling
    the next parent class in line of the child.
    """"""

    def __init__(self, *, context: dict[str, Any] | None=None, **kwargs: Any) -> None:
        """"""Initialize the Jinja2 Environment object while loading extensions.

        Does the following:

        1. Establishes default_extensions (currently just a Time feature)
        2. Reads extensions set in the cookiecutter.json _extensions key.
        3. Attempts to load the extensions. Provides useful error if fails.
        """"""
        context = context or {}
        default_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']
        extensions = default_extensions + self._read_extensions(context)
        try:
            super().__init__(extensions=extensions, **kwargs)
        except ImportError as err:
            msg = f'Unable to load extension: {err}'
            raise UnknownExtension(msg) from err

    def _read_extensions(self, context: dict[str, Any]) -> list[str]:
        """"""Return list of extensions as str to be passed on to the Jinja2 env.

        If context does not contain the relevant info, return an empty
        list instead.
        """"""
        try:
            extensions = context['cookiecutter']['_extensions']
        except KeyError:
            return []
        else:
            return [str(ext) for ext in extensions]","
class ExtensionLoaderMixin:
    
    def __init__(self, *, context: dict[str, Any] | None=None, **kwargs: Any) -> None:
        
        pass
    def _read_extensions(self, context: dict[str, Any]) -> list[str]:
        
        pass",snippet_5,3,"```python
from __future__ import annotations

from typing import Any, Dict, List, Optional


class ExtensionLoaderMixin:
    """"""
    A mixin that provides a simple mechanism for loading extensions from a
    context dictionary.  The context can be supplied either via the ``context``
    keyword argument or via arbitrary keyword arguments passed to ``__init__``.
    The extensions are expected to be stored under the key ``""extensions""``
    (or ``""extension""`` for backward compatibility) and must be an iterable
    of strings.
    """"""

    def __init__(
        self,
        *,
        context: Optional[Dict[str, Any]] = None,
        **kwargs: Any,
    ) -> None:
        """"""
        Initialise the mixin.

        Parameters
        ----------
        context:
            Optional dictionary containing configuration values.  If omitted,
            an empty dictionary is used.
        **kwargs:
            Additional keyword arguments are merged into the context.  This
            allows callers to provide configuration values directly without
            constructing a dictionary.
        """"""
        # Start with a copy of the supplied context or an empty dict
        self._context: Dict[str, Any] = dict(context or {})
        # Merge any additional keyword arguments into the context
        self._context.update(kwargs)

    def _read_extensions(self, context: Dict[str, Any]) -> List[str]:
        """"""
        Extract a list of extensions from the provided context.

        The method looks for the key ``""extensions""`` first.  If it is not
        present, it falls back to ``""extension""`` for backward compatibility.
        The value must be an iterable of strings; otherwise a ``ValueError`` is
        raised.

        Parameters
        ----------
        context:
            Dictionary from which to read the extensions.

        Returns
        -------
        List[str]
            A list of extension names.
        """"""
        # Prefer the plural key, fall back to the singular key
        raw = context.get(""extensions"", context.get(""extension"", []))

        # Ensure we have an iterable of strings
        if isinstance(raw, str):
            # Treat a single string as a single extension
            return [raw]
        try:
            # Try to iterate over the value
            extensions = list(raw)
        except TypeError:
            raise ValueError(
                f""Extensions value must be an iterable of strings, got {type(raw)!r}""
            )

        # Validate that each element is a string
        for ext in extensions:
            if not isinstance(ext, str):
                raise ValueError(
                    f""Each extension must be a string, got {type(ext)!r} in {extensions!r}""
                )

        return extensions
```"
21340,Delgan/loguru,loguru/_recattrs.py,loguru._recattrs.RecordFile,"class RecordFile:
    """"""A class representing a file record with name and path.

    Attributes
    ----------
    name : str
        The name of the file
    path : str
        The path to the file
    """"""
    __slots__ = ('name', 'path')

    def __init__(self, name, path):
        """"""Initialize a RecordFile instance.

        Parameters
        ----------
        name : str
            The name of the file
        path : str
            The path to the file
        """"""
        self.name = name
        self.path = path

    def __repr__(self):
        """"""Return string representation of RecordFile.

        Returns
        -------
        str
            Formatted string with name and path
        """"""
        return '(name=%r, path=%r)' % (self.name, self.path)

    def __format__(self, spec):
        """"""Format the RecordFile instance.

        Parameters
        ----------
        spec : str
            Format specification

        Returns
        -------
        str
            Formatted name according to specification
        """"""
        return self.name.__format__(spec)","class RecordFile:
    
    def __init__(self, name, path):
        
        pass
    def __repr__(self):
        
        pass
    def __format__(self, spec):
        
        pass",snippet_6,4,"```python
class RecordFile:
    def __init__(self, name, path):
        self.name = name
        self.path = path

    def __repr__(self):
        return f""RecordFile(name={self.name!r}, path={self.path!r})""

    def __format__(self, spec):
        """"""
        Format specifiers:
            ''  -> repr(self)
            'n' -> self.name
            'p' -> self.path
        Any other spec falls back to repr.
        """"""
        if spec == ""n"":
            return self.name
        if spec == ""p"":
            return self.path
        return repr(self)
```"
21341,Delgan/loguru,loguru/_recattrs.py,loguru._recattrs.RecordLevel,"class RecordLevel:
    """"""A class representing the logging level record with name, number and icon.

    Attributes
    ----------
    icon : str
        The icon representing the log level
    name : str
        The name of the log level
    no : int
        The numeric value of the log level
    """"""
    __slots__ = ('icon', 'name', 'no')

    def __init__(self, name, no, icon):
        """"""Initialize a RecordLevel instance.

        Parameters
        ----------
        name : str
            The name of the log level
        no : int
            The numeric value of the log level
        icon : str
            The icon representing the log level
        """"""
        self.name = name
        self.no = no
        self.icon = icon

    def __repr__(self):
        """"""Return string representation of RecordLevel.

        Returns
        -------
        str
            Formatted string with name, number and icon
        """"""
        return '(name=%r, no=%r, icon=%r)' % (self.name, self.no, self.icon)

    def __format__(self, spec):
        """"""Format the RecordLevel instance.

        Parameters
        ----------
        spec : str
            Format specification

        Returns
        -------
        str
            Formatted name according to specification
        """"""
        return self.name.__format__(spec)","class RecordLevel:
    
    def __init__(self, name, no, icon):
        
        pass
    def __repr__(self):
        
        pass
    def __format__(self, spec):
        
        pass",snippet_7,4,"```python
class RecordLevel:
    def __init__(self, name, no, icon):
        self.name = name
        self.no = no
        self.icon = icon

    def __repr__(self):
        return f""RecordLevel(name={self.name!r}, no={self.no!r}, icon={self.icon!r})""

    def __format__(self, spec):
        if not spec:
            return repr(self)

        spec = spec.lower()
        if spec in (""n"", ""name""):
            return str(self.name)
        if spec in (""i"", ""icon""):
            return str(self.icon)
        if spec in (""o"", ""no""):
            return str(self.no)
        if spec == ""repr"":
            return repr(self)

        raise ValueError(f""Unknown format specifier {spec!r}"")
```"
21342,Delgan/loguru,loguru/_recattrs.py,loguru._recattrs.RecordProcess,"class RecordProcess:
    """"""A class representing a process record with ID and name.

    Attributes
    ----------
    id : int
        The process ID
    name : str
        The process name
    """"""
    __slots__ = ('id', 'name')

    def __init__(self, id_, name):
        """"""Initialize a RecordProcess instance.

        Parameters
        ----------
        id_ : int
            The process ID
        name : str
            The process name
        """"""
        self.id = id_
        self.name = name

    def __repr__(self):
        """"""Return string representation of RecordProcess.

        Returns
        -------
        str
            Formatted string with id and name
        """"""
        return '(id=%r, name=%r)' % (self.id, self.name)

    def __format__(self, spec):
        """"""Format the RecordProcess instance.

        Parameters
        ----------
        spec : str
            Format specification

        Returns
        -------
        str
            Formatted ID according to specification
        """"""
        return self.id.__format__(spec)","class RecordProcess:
    
    def __init__(self, id_, name):
        
        pass
    def __repr__(self):
        
        pass
    def __format__(self, spec):
        
        pass",snippet_8,4,"```python
class RecordProcess:
    def __init__(self, id_, name):
        self.id_ = id_
        self.name = name

    def __repr__(self):
        return f""RecordProcess(id={self.id_!r}, name={self.name!r})""

    def __format__(self, spec):
        if spec == ""id"":
            return str(self.id_)
        if spec == ""name"":
            return str(self.name)
        if spec == ""full"":
            return f""{self.id_}:{self.name}""
        # Default formatting falls back to the repr
        return self.__repr__()
```"
21343,Delgan/loguru,loguru/_recattrs.py,loguru._recattrs.RecordThread,"class RecordThread:
    """"""A class representing a thread record with ID and name.

    Attributes
    ----------
    id : int
        The thread ID
    name : str
        The thread name
    """"""
    __slots__ = ('id', 'name')

    def __init__(self, id_, name):
        """"""Initialize a RecordThread instance.

        Parameters
        ----------
        id_ : int
            The thread ID
        name : str
            The thread name
        """"""
        self.id = id_
        self.name = name

    def __repr__(self):
        """"""Return string representation of RecordThread.

        Returns
        -------
        str
            Formatted string with id and name
        """"""
        return '(id=%r, name=%r)' % (self.id, self.name)

    def __format__(self, spec):
        """"""Format the RecordThread instance.

        Parameters
        ----------
        spec : str
            Format specification

        Returns
        -------
        str
            Formatted ID according to specification
        """"""
        return self.id.__format__(spec)","class RecordThread:
    
    def __init__(self, id_, name):
        
        pass
    def __repr__(self):
        
        pass
    def __format__(self, spec):
        
        pass",snippet_9,4,"```python
class RecordThread:
    def __init__(self, id_, name):
        self.id = id_
        self.name = name

    def __repr__(self):
        return f""RecordThread(id={self.id!r}, name={self.name!r})""

    def __format__(self, spec):
        # Default formatting: id followed by name
        if not spec:
            return f""{self.id} {self.name}""
        # If the spec is ""name"", return only the name
        if spec == ""name"":
            return f""{self.name}""
        # If the spec is ""id"", return only the id (formatted with the spec)
        if spec == ""id"":
            return f""{self.id}""
        # Try to format the id with the given spec (e.g., numeric formatting)
        try:
            return format(self.id, spec)
        except Exception:
            # Fallback to the default representation
            return f""{self.id} {self.name}""
```"
33971,Miserlou/Zappa,Miserlou_Zappa/zappa/asynchronous.py,zappa.asynchronous.LambdaAsyncResponse,"import json
import uuid

class LambdaAsyncResponse:
    """"""
    Base Response Dispatcher class
    Can be used directly or subclassed if the method to send the message is changed.
    """"""

    def __init__(self, lambda_function_name=None, aws_region=None, capture_response=False, **kwargs):
        """""" """"""
        if kwargs.get('boto_session'):
            self.client = kwargs.get('boto_session').client('lambda')
        else:
            self.client = LAMBDA_CLIENT
        self.lambda_function_name = lambda_function_name
        self.aws_region = aws_region
        if capture_response:
            if ASYNC_RESPONSE_TABLE is None:
                print(""Warning! Attempted to capture a response without async_response_table configured in settings (you won't capture async responses)."")
                capture_response = False
                self.response_id = 'MISCONFIGURED'
            else:
                self.response_id = str(uuid.uuid4())
        else:
            self.response_id = None
        self.capture_response = capture_response

    def send(self, task_path, args, kwargs):
        """"""
        Create the message object and pass it to the actual sender.
        """"""
        message = {'task_path': task_path, 'capture_response': self.capture_response, 'response_id': self.response_id, 'args': args, 'kwargs': kwargs}
        self._send(message)
        return self

    def _send(self, message):
        """"""
        Given a message, directly invoke the lamdba function for this task.
        """"""
        message['command'] = 'zappa.asynchronous.route_lambda_task'
        payload = json.dumps(message).encode('utf-8')
        if len(payload) > LAMBDA_ASYNC_PAYLOAD_LIMIT:
            raise AsyncException('Payload too large for async Lambda call')
        self.response = self.client.invoke(FunctionName=self.lambda_function_name, InvocationType='Event', Payload=payload)
        self.sent = self.response.get('StatusCode', 0) == 202","
class LambdaAsyncResponse:
    
    def __init__(self, lambda_function_name=None, aws_region=None, capture_response=False, **kwargs):
        
        pass
    def send(self, task_path, args, kwargs):
        
        pass

    def _send(self, message):
        
        pass",snippet_10,4,"```python
import json
import boto3
from botocore.exceptions import BotoCoreError, ClientError


class LambdaAsyncResponse:
    """"""
    Helper to invoke an AWS Lambda function asynchronously (or synchronously if requested).
    """"""

    def __init__(self, lambda_function_name=None, aws_region=None, capture_response=False, **kwargs):
        """"""
        Parameters
        ----------
        lambda_function_name : str, optional
            The name or ARN of the Lambda function to invoke.
        aws_region : str, optional
            AWS region where the Lambda function is deployed. If not provided, the default
            region configured in the environment or AWS SDK will be used.
        capture_response : bool, default False
            If True, the response payload from the Lambda invocation will be returned.
            If False, the invocation will be fire-and-forget (InvocationType='Event').
        **kwargs
            Additional keyword arguments passed to boto3.client() for Lambda.
        """"""
        if not lambda_function_name:
            raise ValueError(""lambda_function_name must be provided"")

        self.lambda_function_name = lambda_function_name
        self.aws_region = aws_region
        self.capture_response = capture_response

        # Create a Lambda client
        client_kwargs = {}
        if aws_region:
            client_kwargs['region_name'] = aws_region
        client_kwargs.update(kwargs)
        self.client = boto3.client('lambda', **client_kwargs)

    def send(self, task_path, args, kwargs):
        """"""
        Send a message to the Lambda function.

        Parameters
        ----------
        task_path : str
            Identifier for the task to be performed by the Lambda.
        args : list or tuple
            Positional arguments for the task.
        kwargs : dict
            Keyword arguments for the task.

        Returns
        -------
        str or None
            If capture_response is True, returns the decoded response payload.
            Otherwise, returns None.
        """"""
        message = {
            'task_path': task_path,
            'args': args,
            'kwargs': kwargs,
        }
        return self._send(message)

    def _send(self, message):
        """"""
        Internal method to invoke the Lambda function.

        Parameters
        ----------
        message : dict
            The payload to send to the Lambda function.

        Returns
        -------
        str or None
            If capture_response is True, returns the decoded response payload.
            Otherwise, returns None.
        """"""
        payload = json.dumps(message).encode('utf-8')
        invocation_type = 'RequestResponse' if self.capture_response else 'Event'

        try:
            response = self.client.invoke(
                FunctionName=self.lambda_function_name,
                InvocationType=invocation_type,
                Payload=payload,
            )
        except (BotoCoreError, ClientError) as exc:
            raise RuntimeError(f""Failed to invoke Lambda '{self.lambda_function_name}': {exc}"") from exc

        if self.capture_response:
            # The payload is a streaming body; read and decode it.
            try:
                response_payload = response['Payload'].read()
                if isinstance(response_payload, bytes):
                    response_payload = response_payload.decode('utf-8')
                return response_payload
            except Exception as exc:
                raise RuntimeError(f""Failed to read Lambda response payload: {exc}"") from exc

        return None
```"
44118,LonamiWebs/Telethon,LonamiWebs_Telethon/telethon/crypto/aesctr.py,telethon.crypto.aesctr.AESModeCTR,"import pyaes

class AESModeCTR:
    """"""Wrapper around pyaes.AESModeOfOperationCTR mode with custom IV""""""

    def __init__(self, key, iv):
        """"""
        Initializes the AES CTR mode with the given key/iv pair.

        :param key: the key to be used as bytes.
        :param iv: the bytes initialization vector. Must have a length of 16.
        """"""
        assert isinstance(key, bytes)
        self._aes = pyaes.AESModeOfOperationCTR(key)
        assert isinstance(iv, bytes)
        assert len(iv) == 16
        self._aes._counter._counter = list(iv)

    def encrypt(self, data):
        """"""
        Encrypts the given plain text through AES CTR.

        :param data: the plain text to be encrypted.
        :return: the encrypted cipher text.
        """"""
        return self._aes.encrypt(data)

    def decrypt(self, data):
        """"""
        Decrypts the given cipher text through AES CTR

        :param data: the cipher text to be decrypted.
        :return: the decrypted plain text.
        """"""
        return self._aes.decrypt(data)","
class AESModeCTR:
    
    def __init__(self, key, iv):
        
        pass
    def encrypt(self, data):
        
        pass

    def decrypt(self, data):
        
        pass",snippet_11,4,"```python
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
from cryptography.hazmat.backends import default_backend


class AESModeCTR:
    """"""
    AES encryption/decryption in CTR mode.

    Parameters
    ----------
    key : bytes
        The AES key. Must be 16, 24, or 32 bytes long.
    iv : bytes
        The initial counter value (nonce). Must be 16 bytes long.
    """"""

    def __init__(self, key: bytes, iv: bytes):
        if not isinstance(key, (bytes, bytearray)):
            raise TypeError(""key must be bytes"")
        if not isinstance(iv, (bytes, bytearray)):
            raise TypeError(""iv must be bytes"")
        if len(key) not in (16, 24, 32):
            raise ValueError(""key must be 16, 24, or 32 bytes long"")
        if len(iv) != 16:
            raise ValueError(""iv must be 16 bytes long"")

        self._key = bytes(key)
        self._iv = bytes(iv)
        self._backend = default_backend()
        self._cipher = Cipher(algorithms.AES(self._key), modes.CTR(self._iv), backend=self._backend)

    def encrypt(self, data: bytes) -> bytes:
        """"""
        Encrypt data using AES-CTR.

        Parameters
        ----------
        data : bytes
            Plaintext data to encrypt.

        Returns
        -------
        bytes
            Ciphertext.
        """"""
        if not isinstance(data, (bytes, bytearray)):
            raise TypeError(""data must be bytes"")
        encryptor = self._cipher.encryptor()
        return encryptor.update(data) + encryptor.finalize()

    def decrypt(self, data: bytes) -> bytes:
        """"""
        Decrypt data using AES-CTR.

        Parameters
        ----------
        data : bytes
            Ciphertext data to decrypt.

        Returns
        -------
        bytes
            Plaintext.
        """"""
        if not isinstance(data, (bytes, bytearray)):
            raise TypeError(""data must be bytes"")
        decryptor = self._cipher.decryptor()
        return decryptor.update(data) + decryptor.finalize()
```"
44121,LonamiWebs/Telethon,LonamiWebs_Telethon/telethon/crypto/factorization.py,telethon.crypto.factorization.Factorization,"from random import randint

class Factorization:
    """"""
    Simple module to factorize large numbers really quickly.
    """"""

    @classmethod
    def factorize(cls, pq):
        """"""
        Factorizes the given large integer.

        Implementation from https://comeoncodeon.wordpress.com/2010/09/18/pollard-rho-brent-integer-factorization/.

        :param pq: the prime pair pq.
        :return: a tuple containing the two factors p and q.
        """"""
        if pq % 2 == 0:
            return (2, pq // 2)
        y, c, m = (randint(1, pq - 1), randint(1, pq - 1), randint(1, pq - 1))
        g = r = q = 1
        x = ys = 0
        while g == 1:
            x = y
            for i in range(r):
                y = (pow(y, 2, pq) + c) % pq
            k = 0
            while k < r and g == 1:
                ys = y
                for i in range(min(m, r - k)):
                    y = (pow(y, 2, pq) + c) % pq
                    q = q * abs(x - y) % pq
                g = cls.gcd(q, pq)
                k += m
            r *= 2
        if g == pq:
            while True:
                ys = (pow(ys, 2, pq) + c) % pq
                g = cls.gcd(abs(x - ys), pq)
                if g > 1:
                    break
        p, q = (g, pq // g)
        return (p, q) if p < q else (q, p)

    @staticmethod
    def gcd(a, b):
        """"""
        Calculates the Greatest Common Divisor.

        :param a: the first number.
        :param b: the second number.
        :return: GCD(a, b)
        """"""
        while b:
            a, b = (b, a % b)
        return a","
class Factorization:
    
    @classmethod
    def factorize(cls, pq):
        
        pass
    @staticmethod
    def gcd(a, b):
        
        pass",snippet_12,3,"```python
class Factorization:
    @classmethod
    def factorize(cls, pq):
        """"""Return the prime factorization of an integer as a list of primes.""""""
        n = pq
        if n <= 1:
            return []

        factors = []
        # Handle factor 2 separately to allow step of 2 later
        while n % 2 == 0:
            factors.append(2)
            n //= 2

        # Check odd factors up to sqrt(n)
        i = 3
        max_factor = int(n**0.5) + 1
        while i <= max_factor and n > 1:
            while n % i == 0:
                factors.append(i)
                n //= i
                max_factor = int(n**0.5) + 1
            i += 2

        # If remaining n is a prime > 1
        if n > 1:
            factors.append(n)

        return factors

    @staticmethod
    def gcd(a, b):
        """"""Return the greatest common divisor of a and b using Euclid's algorithm.""""""
        while b:
            a, b = b, a % b
        return abs(a)
```"
49235,blue-yonder/tsfresh,blue-yonder_tsfresh/tsfresh/examples/driftbif_simulation.py,tsfresh.examples.driftbif_simulation.velocity,"import numpy as np

class velocity:
    """"""
    Simulates the velocity of a dissipative soliton (kind of self organized particle) [6]_.
    The equilibrium velocity without noise R=0 for
    $	au>1.0/\\kappa_3$ is $\\kappa_3 \\sqrt{(tau - 1.0/\\kappa_3)/Q}.
    Before the drift-bifurcation $	au \\le 1.0/\\kappa_3$ the velocity is zero.

    References
    ----------

    .. [6] Andreas Kempa-Liehr (2013, p. 159-170)
        Dynamics of Dissipative Soliton
        Dissipative Solitons in Reaction Diffusion Systems.
        Springer: Berlin


    >>> ds = velocity(tau=3.5) # Dissipative soliton with equilibrium velocity 1.5e-3
    >>> print(ds.label) # Discriminating before or beyond Drift-Bifurcation
    1

    # Equilibrium velocity
    >>> print(ds.deterministic)
    0.0015191090506254991

    # Simulated velocity as a time series with 20000 time steps being disturbed by Gaussian white noise
    >>> v = ds.simulate(20000)
    """"""

    def __init__(self, tau=3.8, kappa_3=0.3, Q=1950.0, R=0.0003, delta_t=0.05, seed=None):
        """"""
        :param tau: Bifurcation parameter determining the intrinsic velocity of the dissipative soliton,
                    which is zero for tau<=1.0/kappa_3 and np.sqrt(kappa_3**3/Q * (tau - 1.0/kappa_3)) otherwise
        :type tau: float
        :param kappa_3: Inverse bifurcation point.
        :type kappa_3:
        :param Q: Shape parameter of dissipative soliton
        :type Q: float
        :param R: Noise amplitude
        :type R: float
        :param delta_t: temporal discretization
        :type delta_t: float
        """"""
        self.delta_t = delta_t
        self.kappa_3 = kappa_3
        self.Q = Q
        self.tau = tau
        self.a = self.delta_t * kappa_3 ** 2 * (tau - 1.0 / kappa_3)
        self.b = self.delta_t * Q / kappa_3
        self.label = int(tau > 1.0 / kappa_3)
        self.c = np.sqrt(self.delta_t) * R
        self.delta_t = self.delta_t
        if seed is not None:
            np.random.seed(seed)
        if tau <= 1.0 / kappa_3:
            self.deterministic = 0.0
        else:
            self.deterministic = kappa_3 ** 1.5 * np.sqrt((tau - 1.0 / kappa_3) / Q)

    def __call__(self, v):
        """"""
        returns deterministic dynamic = acceleration (without noise)

        :param v: initial velocity vector
        :rtype v: ndarray
        :return: velocity vector of next time step
        :return type: ndarray
        """"""
        return v * (1.0 + self.a - self.b * np.dot(v, v))

    def simulate(self, N, v0=np.zeros(2)):
        """"""

        :param N: number of time steps
        :type N: int
        :param v0: initial velocity vector
        :type v0: ndarray
        :return: time series of velocity vectors with shape (N, v0.shape[0])
        :rtype: ndarray
        """"""
        v = [v0]
        n = N - 1
        gamma = np.random.randn(n, v0.size)
        for i in range(n):
            next_v = self.__call__(v[i]) + self.c * gamma[i]
            v.append(next_v)
        v_vec = np.array(v)
        return v_vec","
class velocity:
    '''
    Simulates the velocity of a dissipative soliton (kind of self organized particle) [6]_.
    The equilibrium velocity without noise R=0 for
    $    au>1.0/\kappa_3$ is $\kappa_3 \sqrt{(tau - 1.0/\kappa_3)/Q}.
    Before the drift-bifurcation $    au \le 1.0/\kappa_3$ the velocity is zero.
    References
    ----------
    .. [6] Andreas Kempa-Liehr (2013, p. 159-170)
        Dynamics of Dissipative Soliton
        Dissipative Solitons in Reaction Diffusion Systems.
        Springer: Berlin
    >>> ds = velocity(tau=3.5) # Dissipative soliton with equilibrium velocity 1.5e-3
    >>> print(ds.label) # Discriminating before or beyond Drift-Bifurcation
    1
    # Equilibrium velocity
    >>> print(ds.deterministic)
    0.0015191090506254991
    # Simulated velocity as a time series with 20000 time steps being disturbed by Gaussian white noise
    >>> v = ds.simulate(20000)
    '''
    def __init__(self, tau=3.8, kappa_3=0.3, Q=1950.0, R=0.0003, delta_t=0.05, seed=None):
        
        pass

    def __call__(self, v):
        '''
        returns deterministic dynamic = acceleration (without noise)
        :param v: initial velocity vector
        :rtype v: ndarray
        :return: velocity vector of next time step
        :return type: ndarray
        '''
        pass

    def simulate(self, N, v0=np.zeros(2)):
        
        pass",snippet_13,4,"```python
import numpy as np

class velocity:
    '''
    Simulates the velocity of a dissipative soliton (kind of self organized particle) [6]_.
    The equilibrium velocity without noise R=0 for
    $    au>1.0/\kappa_3$ is $\kappa_3 \sqrt{(tau - 1.0/\kappa_"
98891,peterbrittain/asciimatics,peterbrittain_asciimatics/asciimatics/effects.py,asciimatics.effects._Flake,"from random import randint, random, choice
from asciimatics.screen import Screen

class _Flake:
    """"""
    Track a single snow flake.
    """"""
    _snow_chars = '.+*'
    _drift_chars = ' ,;#@'

    def __init__(self, screen: Screen):
        """"""
        :param screen: The Screen being used for the Scene.
        """"""
        self._screen = screen
        self._x = 0
        self._y = 0
        self._rate = 0
        self._char = ''
        self._reseed()

    def _reseed(self):
        """"""
        Randomly create a new snowflake once this one is finished.
        """"""
        self._char = choice(self._snow_chars)
        self._rate = randint(1, 3)
        self._x = randint(0, self._screen.width - 1)
        self._y = self._screen.start_line + randint(0, self._rate)

    def update(self, reseed: bool):
        """"""
        Update that snowflake!

        :param reseed: Whether we are in the normal reseed cycle or not.
        """"""
        self._screen.print_at(' ', self._x, self._y)
        cell = None
        for _ in range(self._rate):
            self._y += 1
            cell = self._screen.get_from(self._x, self._y)
            if cell is None or cell[0] != 32:
                break
        if (cell is not None and cell[0] in [ord(x) for x in self._snow_chars + ' ']) and self._y < self._screen.start_line + self._screen.height:
            self._screen.print_at(self._char, self._x, self._y)
        else:
            self._y = min(self._y, self._screen.start_line + self._screen.height)
            drift_index = -1
            if cell:
                drift_index = self._drift_chars.find(chr(cell[0]))
            if 0 <= drift_index < len(self._drift_chars) - 1:
                drift_char = self._drift_chars[drift_index + 1]
                self._screen.print_at(drift_char, self._x, self._y)
            else:
                self._screen.print_at(',', self._x, self._y - 1)
            if reseed:
                self._reseed()","
class _Flake:
    
    def __init__(self, screen: Screen):
        
        pass
    def _reseed(self):
        
        pass

    def update(self, reseed: bool):
        
        pass",snippet_14,4,"```python
import random
from typing import Any

class _Flake:
    """"""
    A simple snowflake simulation object.

    The class expects the `screen` argument to expose `width` and `height`
    attributes that define the drawing area.  The flake is represented by
    a position, a falling speed, a size, and an opacity.  The flake
    automatically re‑spawns when it moves past the bottom of the screen.
    """"""

    def __init__(self, screen: Any):
        """"""
        Initialise a new flake.

        Parameters
        ----------
        screen : Any
            An object that provides `width` and `height` attributes.
        """"""
        self.screen = screen
        self._reseed()

    def _reseed(self):
        """"""
        Randomise the flake's starting position and visual properties.
        """"""
        # Position: start somewhere above the visible area
        self.x = random.uniform(0, self.screen.width)
        self.y = random.uniform(-self.screen.height, 0)

        # Falling speed (pixels per update)
        self.speed = random.uniform(1.0, 3.0)

        # Size (used for drawing and wrap‑around)
        self.size = random.randint(1, 3)

        # Opacity (0.0 to 1.0)
        self.opacity = random.uniform(0.5, 1.0)

        # Optional horizontal drift
        self.drift = random.uniform(-0.5, 0.5)

    def update(self, reseed: bool):
        """"""
        Move the flake downwards and optionally reseed it when it leaves
        the screen.

        Parameters
        ----------
        reseed : bool
            If True, the flake will be reseeded when it moves past the
            bottom of the screen.  If False, it will simply wrap to the
            top.
        """"""
        # Apply vertical movement
        self.y += self.speed

        # Apply horizontal drift
        self.x += self.drift

        # Keep the flake within horizontal bounds
        if self.x < 0:
            self.x += self.screen.width
        elif self.x > self.screen.width:
            self.x -= self.screen.width

        # If the flake has fallen below the screen
        if self.y > self.screen.height:
            if reseed:
                self._reseed()
            else:
                # Wrap to the top
                self.y = -self.size
```"
98892,peterbrittain/asciimatics,peterbrittain_asciimatics/asciimatics/effects.py,asciimatics.effects._Star,"from random import randint, random, choice
from asciimatics.screen import Screen

class _Star:
    """"""
    Simple class to represent a single star for the Stars special effect.
    """"""

    def __init__(self, screen: Screen, pattern: str):
        """"""
        :param screen: The Screen being used for the Scene.
        :param pattern: The pattern to loop through
        """"""
        self._screen = screen
        self._star_chars = pattern
        self._cycle = 0
        self._old_char = ''
        self._respawn()

    def _respawn(self):
        """"""
        Pick a random location for the star making sure it does
        not overwrite an existing piece of text.
        """"""
        self._cycle = randint(0, len(self._star_chars))
        height, width = self._screen.dimensions
        while True:
            self._x = randint(0, width - 1)
            self._y = self._screen.start_line + randint(0, height - 1)
            c = self._screen.get_from(self._x, self._y)
            if c is not None and c[0] == 32:
                break
        self._old_char = ' '

    def update(self):
        """"""
        Draw the star.
        """"""
        if not self._screen.is_visible(self._x, self._y):
            self._respawn()
        c = self._screen.get_from(self._x, self._y)
        if c is not None and c[0] not in (ord(self._old_char), 32):
            self._respawn()
        self._cycle += 1
        if self._cycle >= len(self._star_chars):
            self._cycle = 0
        new_char = self._star_chars[self._cycle]
        if new_char == self._old_char:
            return
        self._screen.print_at(new_char, self._x, self._y)
        self._old_char = new_char","
class _Star:
    
    def __init__(self, screen: Screen, pattern: str):
        
        pass
    def _respawn(self):
        
        pass

    def update(self):
        
        pass",snippet_15,4,"```python
import random

class _Star:
    """"""
    A simple star that moves leftwards across a screen and respawns on the right edge.
    The star cycles through the characters in `pattern` each frame.
    """"""

    def __init__(self, screen: ""Screen"", pattern: str):
        """"""
        Parameters
        ----------
        screen : Screen
            The screen object that provides width, height and a method to draw characters.
        pattern : str
            A string of characters that the star cycles through.
        """"""
        self.screen = screen
        self.pattern = pattern
        self.index = 0  # current character index in the pattern
        self.x = 0
        self.y = 0
        self._respawn()

    def _respawn(self):
        """"""
        Place the star at a random vertical position on the rightmost column of the screen.
        """"""
        # Use the screen's width and height attributes if available
        width = getattr(self.screen, ""width"", None)
        height = getattr(self.screen, ""height"", None)

        if width is None or height is None:
            raise AttributeError(""Screen object must have 'width' and 'height' attributes"")

        self.x = width - 1
        self.y = random.randint(0, height - 1)

    def update(self):
        """"""
        Draw the star at its current position, advance its character, and move it left.
        If it moves off the screen, respawn it on the right.
        """"""
        # Determine the drawing method
        draw_method = getattr(self.screen, ""draw"", None)
        if draw_method is None:
            draw_method = getattr(self.screen, ""put"", None)
        if draw_method is None:
            raise AttributeError(""Screen object must have a 'draw' or 'put' method"")

        # Draw the current character
        char = self.pattern[self.index % len(self.pattern)]
        draw_method(self.x, self.y, char)

        # Advance to the next character in the pattern
        self.index += 1

        # Move left
        self.x -= 1

        # If the star has moved off the left edge, respawn it
        if self.x < 0:
            self._respawn()
```"
98893,peterbrittain/asciimatics,peterbrittain_asciimatics/asciimatics/effects.py,asciimatics.effects._Trail,"from random import randint, random, choice
from asciimatics.screen import Screen

class _Trail:
    """"""
    Track a single trail  for a falling character effect (a la Matrix).
    """"""

    def __init__(self, screen: Screen, x: int):
        """"""
        :param screen: The Screen being used for the Scene.
        :param x: The column (y coordinate) for this trail to use.
        """"""
        self._screen = screen
        self._x = x
        self._y = 0
        self._life = 0
        self._rate = 0
        self._clear = True
        self._maybe_reseed(True)

    def _maybe_reseed(self, normal: bool):
        """"""
        Randomly create a new column once this one is finished.
        """"""
        self._y += self._rate
        self._life -= 1
        if self._life <= 0:
            self._clear = not self._clear if normal else True
            self._rate = randint(1, 2)
            if self._clear:
                self._y = 0
                self._life = self._screen.height // self._rate
            else:
                self._y = randint(0, self._screen.height // 2) - self._screen.height // 4
                self._life = randint(1, self._screen.height - self._y) // self._rate

    def update(self, reseed: bool):
        """"""
        Update that trail!

        :param reseed: Whether we are in the normal reseed cycle or not.
        """"""
        if self._clear:
            for i in range(0, 3):
                self._screen.print_at(' ', self._x, self._screen.start_line + self._y + i)
            self._maybe_reseed(reseed)
        else:
            for i in range(0, 3):
                self._screen.print_at(chr(randint(32, 126)), self._x, self._screen.start_line + self._y + i, Screen.COLOUR_GREEN)
            for i in range(4, 6):
                self._screen.print_at(chr(randint(32, 126)), self._x, self._screen.start_line + self._y + i, Screen.COLOUR_GREEN, Screen.A_BOLD)
            self._maybe_reseed(reseed)","
class _Trail:
    
    def __init__(self, screen: Screen, x: int):
        
        pass
    def _maybe_reseed(self, normal: bool):
        
        pass

    def update(self, reseed: bool):
        
        pass",snippet_16,4,"```python
import random
from typing import List, Tuple

class _Trail:
    """"""
    A simple trail that keeps a list of characters at a fixed x position on a screen.
    Each update step moves the trail down by one row and optionally adds a new
    character at the top. The new character's vertical position can be chosen
    either from a normal distribution (if normal=True) or uniformly across the
    screen height.
    """"""

    def __init__(self, screen, x: int):
        """"""
        Parameters
        ----------
        screen : object
            The screen object. It must expose a `height` attribute and a
            `write(x, y, char)` method.
        x : int
            The fixed horizontal position of the trail.
        """"""
        self.screen = screen
        self.x = x
        self.segments: List[Tuple[int, str]] = []  # list of (y, char)
        self.max_length = getattr(screen, ""width"", 80)  # fallback if width not present

    def _maybe_reseed(self, normal: bool):
        """"""
        Add a new segment at the top of the trail.

        Parameters
        ----------
        normal : bool
            If True, the y position is drawn from a normal distribution
            centered at half the screen height. If False, the y position
            is drawn uniformly from the screen height.
        """"""
        height = getattr(self.screen, ""height"", 24)
        if normal:
            # Normal distribution centered at middle with std dev of 1/4 height
            y = int(random.gauss(height / 2, height / 8))
        else:
            y = random.randint(0, height - 1)

        # Clamp y to screen bounds
        y = max(0, min(height - 1, y))

        # Choose a random character for the trail
        char = random.choice("".*+"")
        self.segments.append((y, char))

        # Keep the trail length bounded
        if len(self.segments) > self.max_length:
            self.segments.pop(0)

    def update(self, reseed: bool):
        """"""
        Advance the trail by one step.

        Parameters
        ----------
        reseed : bool
            If True, a new segment is added at the top of the trail.
        """"""
        if reseed:
            self._maybe_reseed(normal=True)

        # Move all segments down by one row
        new_segments: List[Tuple[int, str]] = []
        for y, char in self.segments:
            new_y = y + 1
            if new_y < getattr(self.screen, ""height"", 24):
                new_segments.append((new_y, char))

        self.segments = new_segments

        # Draw the trail on the screen
        for y, char in self.segments:
            try:
                self.screen.write(self.x, y, char)
            except Exception:
                # If the screen object does not support write, ignore
                pass
```"
99108,ConsenSys/mythril-classic,ConsenSys_mythril-classic/mythril/analysis/ops.py,mythril.analysis.ops.Variable,"class Variable:
    """"""The representation of a variable with value and type.""""""

    def __init__(self, val, _type):
        """"""

        :param val:
        :param _type:
        """"""
        self.val = val
        self.type = _type

    def __str__(self):
        """"""

        :return:
        """"""
        return str(self.val)","class Variable:
    
    def __init__(self, val, _type):
        
        pass
    def __str__(self):
        
        pass",snippet_17,3,"```python
class Variable:
    def __init__(self, val, _type):
        """"""
        Initialize a Variable instance.

        Parameters
        ----------
        val : any
            The value of the variable.
        _type : type or str
            The type of the variable. It can be a Python type object or a string
            representation of the type.
        """"""
        self.val = val
        self.type = _type

    def __str__(self):
        """"""
        Return a string representation of the variable's value.

        Returns
        -------
        str
            The string representation of `self.val`.
        """"""
        return str(self.val)
```"
100863,bigchaindb/bigchaindb,bigchaindb_bigchaindb/bigchaindb/validation.py,bigchaindb.validation.BaseValidationRules,"class BaseValidationRules:
    """"""Base validation rules for BigchainDB.

    A validation plugin must expose a class inheriting from this one via an entry_point.

    All methods listed below must be implemented.
    """"""

    @staticmethod
    def validate_transaction(bigchaindb, transaction):
        """"""See :meth:`bigchaindb.models.Transaction.validate`
        for documentation.
        """"""
        return transaction.validate(bigchaindb)

    @staticmethod
    def validate_block(bigchaindb, block):
        """"""See :meth:`bigchaindb.models.Block.validate` for documentation.""""""
        return block.validate(bigchaindb)","class BaseValidationRules:
    
    @staticmethod
    def validate_transaction(bigchaindb, transaction):
        
        pass
    @staticmethod
    def validate_block(bigchaindb, block):
        
        pass",snippet_22,3,"```python
import json
import hashlib
from typing import Any, Dict, List, Optional


class BaseValidationRules:
    @staticmethod
    def _hash(data: Any) -> str:
        """"""Return a SHA-256 hex digest of the JSON representation of *data*.""""""
        return hashlib.sha256(json.dumps(data, sort_keys=True).encode()).hexdigest()

    @staticmethod
    def _hash_pair(a: str, b: str) -> str:
        """"""Return a SHA-256 hex digest of the concatenation of two hex strings.""""""
        return hashlib.sha256((a + b).encode()).hexdigest()

    @staticmethod
    def _merkle_root(hashes: List[str]) -> str:
        """"""Compute the merkle root of a list of hex string hashes.""""""
        if not hashes:
            return """"
        while len(hashes) > 1:
            if len(hashes) % 2 == 1:
                hashes.append(hashes[-1])
            new_hashes = []
            for i in range(0, len(hashes), 2):
                new_hashes.append(BaseValidationRules._hash_pair(hashes[i], hashes[i + 1]))
            hashes = new_hashes
        return hashes[0]

    @staticmethod
    def validate_transaction(bigchaindb, transaction: Dict[str, Any]) -> bool:
        """"""
        Validate a BigchainDB transaction.

        Raises:
            ValueError: If the transaction is invalid.
        """"""
        if not isinstance(transaction, dict):
            raise ValueError(""Transaction must be a dictionary"")

        required_keys = {""id"", ""operation"", ""data"", ""metadata"", ""signatures"", ""outputs"", ""inputs""}
        missing = required_keys - transaction.keys()
        if missing:
            raise ValueError(f""Missing transaction keys: {missing}"")

        tx_id = transaction[""id""]
        if not isinstance(tx_id, str):
            raise ValueError(""Transaction 'id' must be a string"")

        operation = transaction[""operation""]
        if operation not in {""CREATE"", ""TRANSFER"", ""REVOKE""}:
            raise ValueError(f""Unsupported operation: {operation}"")

        if not isinstance(transaction[""data""], dict):
            raise ValueError(""Transaction 'data' must be a dictionary"")

        if not isinstance(transaction[""metadata""], dict):
            raise ValueError(""Transaction 'metadata' must be a dictionary"")

        signatures = transaction[""signatures""]
        if not isinstance(signatures, list):
            raise ValueError(""Transaction 'signatures' must be a list"")

        outputs = transaction[""outputs""]
        if not isinstance(outputs, list):
            raise ValueError(""Transaction 'outputs' must be a list"")

        inputs = transaction[""inputs""]
        if not isinstance(inputs, list):
            raise ValueError(""Transaction 'inputs' must be a list"")

        # Validate signatures
        for sig in signatures:
            if not isinstance(sig, dict):
                raise ValueError(""Each signature must be a dictionary"")
            for key in (""public_key"", ""signature"", ""type"", ""output_index"", ""input_index"", ""output""):
                if key not in sig:
                    raise ValueError(f""Signature missing key: {key}"")
            if not isinstance(sig[""output_index""], int) or sig[""output_index""] >= len(outputs):
                raise ValueError(""Signature 'output_index' out of range"")

        # Validate inputs
        for inp in inputs:
            if not isinstance(inp, dict):
                raise ValueError(""Each input must be a dictionary"")
            for key in (""output"", ""fulfillment"", ""fulfills"", ""owners_before""):
                if key not in inp:
                    raise ValueError(f""Input missing key: {key}"")

            fulfills = inp[""fulfills""]
            if not isinstance(fulfills, dict):
                raise ValueError(""Input 'fulfills' must be a dictionary"")
            if ""transaction_id"" not in fulfills or ""output_index"" not in fulfills:
                raise ValueError(""Input 'fulfills' missing required keys"")

            prev_tx_id = fulfills[""transaction_id""]
            prev_output_index = fulfills[""output_index""]

            # Verify referenced transaction exists
            try"
100865,bigchaindb/bigchaindb,bigchaindb_bigchaindb/bigchaindb/web/strip_content_type_middleware.py,bigchaindb.web.strip_content_type_middleware.StripContentTypeMiddleware,"class StripContentTypeMiddleware:
    """"""WSGI middleware to strip Content-Type header for GETs.""""""

    def __init__(self, app):
        """"""Create the new middleware.

        Args:
            app: a flask application
        """"""
        self.app = app

    def __call__(self, environ, start_response):
        """"""Run the middleware and then call the original WSGI application.""""""
        if environ['REQUEST_METHOD'] == 'GET':
            try:
                del environ['CONTENT_TYPE']
            except KeyError:
                pass
            else:
                logger.debug('Remove header ""Content-Type"" from GET request')
        return self.app(environ, start_response)","class StripContentTypeMiddleware:
    
    def __init__(self, app):
        
        pass
    def __call__(self, environ, start_response):
        
        pass",snippet_23,3,"```python
class StripContentTypeMiddleware:
    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        def custom_start_response(status, headers, exc_info=None):
            # Remove any 'Content-Type' header (case-insensitive)
            filtered_headers = [(k, v) for k, v in headers if k.lower() != 'content-type']
            return start_response(status, filtered_headers, exc_info)

        return self.app(environ, custom_start_response)
```"
100895,ansible/molecule,src/molecule/app.py,molecule.app.App,"from pathlib import Path
from subprocess import CalledProcessError, CompletedProcess
from molecule.console import original_stderr
from ansible_compat.runtime import Runtime
from molecule.ansi_output import CommandBorders
from molecule.util import print_environment_vars

class App:
    """"""App class that keep runtime status.""""""

    def __init__(self, path: Path) -> None:
        """"""Create a new app instance.

        Args:
            path: The path to the project.
        """"""
        self.runtime = Runtime(project_dir=path, isolated=False)

    def run_command(self, cmd: str | list[str], env: dict[str, str] | None=None, cwd: Path | None=None, *, debug: bool=False, echo: bool=False, quiet: bool=False, check: bool=False, command_borders: bool=False) -> CompletedProcess[str]:
        """"""Execute the given command and returns None.

        Args:
            cmd: A list of strings containing the command to run.
            env: A dict containing the shell's environment.
            cwd: An optional Path to the working directory.
            debug: An optional bool to toggle debug output.
            echo: An optional bool to toggle command echo.
            quiet: An optional bool to toggle command output.
            check: An optional bool to toggle command error checking.
            command_borders: An optional bool to enable borders around command output.

        Returns:
            A completed process object.

        Raises:
            CalledProcessError: If return code is nonzero and check is True.
        """"""
        if debug:
            print_environment_vars(env)
        borders = None
        if command_borders:
            borders = CommandBorders(cmd=cmd, original_stderr=original_stderr)
        result = self.runtime.run(args=cmd, env=env, cwd=cwd, tee=True, set_acp=False)
        if borders:
            borders.finalize(result.returncode)
        if result.returncode != 0 and check:
            raise CalledProcessError(returncode=result.returncode, cmd=result.args, output=result.stdout, stderr=result.stderr)
        return result","
class App:
    
    def __init__(self, path: Path) -> None:
        
        pass
    def run_command(self, cmd: str | list[str], env: dict[str, str] | None=None, cwd: Path | None=None, *, debug: bool=False, echo: bool=False, quiet: bool=False, check: bool=False, command_borders: bool=False) -> CompletedProcess[str]:
        
        pass",snippet_24,3,"```python
from __future__ import annotations

import os
import shlex
import subprocess
from pathlib import Path
from subprocess import CompletedProcess
from typing import Any, Dict, Iterable, List, Optional, Union


class App:
    """"""
    Simple wrapper around subprocess.run that provides a few convenience options.
    """"""

    def __init__(self, path: Path) -> None:
        """"""
        Store the base path for the application.

        Parameters
        ----------
        path : Path
            The working directory for commands that do not specify a cwd.
        """"""
        self.path = path

    def run_command(
        self,
        cmd: Union[str, List[str]],
        env: Optional[Dict[str, str]] = None,
        cwd: Optional[Path] = None,
        *,
        debug: bool = False,
        echo: bool = False,
        quiet: bool = False,
        check: bool = False,
        command_borders: bool = False,
    ) -> CompletedProcess[str]:
        """"""
        Execute a command with optional debugging, echoing, and output suppression.

        Parameters
        ----------
        cmd : str | list[str]
            The command to run. If a string, it will be executed through the shell.
        env : dict[str, str] | None
            Environment variables for the subprocess. If None, the current environment is used.
        cwd : Path | None
            Working directory for the subprocess. If None, ``self.path`` is used.
        debug : bool
            If True, print the command and environment before execution.
        echo : bool
            If True, print the command string before execution.
        quiet : bool
            If True, suppress stdout and stderr from the subprocess.
        check : bool
            If True, raise ``subprocess.CalledProcessError`` on non‑zero exit status.
        command_borders : bool
            If True, print start/end borders around the command execution.

        Returns
        -------
        CompletedProcess[str]
            The result of the subprocess execution.
        """"""
        # Resolve working directory
        run_cwd = cwd if cwd is not None else self.path

        # Resolve environment
        run_env = env if env is not None else os.environ.copy()

        # Prepare command
        if isinstance(cmd, str):
            # Use shell for string commands
            shell = True
            # For shell=True, we need a string
            cmd_str = cmd
        else:
            shell = False
            cmd_str = cmd

        # Debug output
        if debug:
            print(""DEBUG: Running command:"", cmd_str)
            print(""DEBUG: Environment:"", run_env)
            print(""DEBUG: Working directory:"", run_cwd)

        # Echo command
        if echo:
            print(""Executing:"", cmd_str)

        # Command borders
        if command_borders:
            print(""=== COMMAND START ==="")

        # Configure subprocess.run
        result = subprocess.run(
            cmd_str,
            shell=shell,
            cwd=run_cwd,
            env=run_env,
            capture_output=not quiet,
            text=True,
            check=check,
        )

        # Command borders end
        if command_borders:
            print(""=== COMMAND END ==="")

        return result
```"
104788,autokey/autokey,autokey_autokey/lib/autokey/scripting/system.py,autokey.scripting.system.System,"import subprocess

class System:
    """"""
    Simplified access to some system commands.
    """"""

    @staticmethod
    def exec_command(command, getOutput=True):
        """"""
        Execute a shell command

        Usage: C{system.exec_command(command, getOutput=True)}

        Set getOutput to False if the command does not exit and return immediately. Otherwise
        AutoKey will not respond to any hotkeys/abbreviations etc until the process started
        by the command exits.

        @param command: command to be executed (including any arguments) - e.g. ""ls -l""
        @param getOutput: whether to capture the (stdout) output of the command
        @raise subprocess.CalledProcessError: if the command returns a non-zero exit code
        """"""
        if getOutput:
            with subprocess.Popen(command, shell=True, bufsize=-1, stdout=subprocess.PIPE, universal_newlines=True) as p:
                output = p.communicate()[0]
                output = output.rstrip('\n')
                if p.returncode:
                    raise subprocess.CalledProcessError(p.returncode, output)
                return output
        else:
            subprocess.Popen(command, shell=True, bufsize=-1)

    @staticmethod
    def create_file(file_name, contents=''):
        """"""
        Create a file with contents

        Usage: C{system.create_file(fileName, contents="""")}

        @param fileName: full path to the file to be created
        @param contents: contents to insert into the file
        """"""
        with open(file_name, 'w') as written_file:
            written_file.write(contents)","
class System:
    
    @staticmethod
    def exec_command(command, getOutput=True):
        
        pass
    @staticmethod
    def create_file(file_name, contents=''):
        
        pass",snippet_26,3,"```python
import os
import subprocess
from typing import Union


class System:
    @staticmethod
    def exec_command(command: str, getOutput: bool = True) -> Union[str, int]:
        """"""
        Execute a shell command.

        :param command: The command to run.
        :param getOutput: If True, return the command's stdout as a string.
                          If False, return the command's exit code.
        :return: stdout string if getOutput is True, otherwise the exit code.
        """"""
        try:
            result = subprocess.run(
                command,
                shell=True,
                capture_output=getOutput,
                text=True,
                check=False,
            )
            if getOutput:
                # Combine stdout and stderr for easier debugging
                output = result.stdout
                if result.stderr:
                    output += ""\n"" + result.stderr
                return output.strip()
            else:
                return result.returncode
        except Exception as exc:
            # If an exception occurs, return the exception message
            return str(exc)

    @staticmethod
    def create_file(file_name: str, contents: str = """") -> None:
        """"""
        Create a file with the given contents. If the directory path does not exist,
        it will be created.

        :param file_name: Path to the file to create.
        :param contents: Text to write into the file.
        """"""
        dir_path = os.path.dirname(file_name)
        if dir_path:
            os.makedirs(dir_path, exist_ok=True)
        with open(file_name, ""w"", encoding=""utf-8"") as f:
            f.write(contents)
```"
106092,mikedh/trimesh,trimesh/collision.py,trimesh.collision.DistanceData,"class DistanceData:
    """"""
    Data structure for holding information about a distance query.
    """"""

    def __init__(self, names, result):
        """"""
        Initialize a DistanceData.

        Parameters
        ----------
        names : list of str
          The names of the two objects in order.
        contact : fcl.DistanceResult
          The distance query result.
        """"""
        self.names = set(names)
        self._inds = {names[0]: result.b1, names[1]: result.b2}
        self._points = {names[0]: result.nearest_points[0], names[1]: result.nearest_points[1]}
        self._distance = result.min_distance

    @property
    def distance(self):
        """"""
        Returns the distance between the two objects.

        Returns
        -------
        distance : float
          The euclidean distance between the objects.
        """"""
        return self._distance

    def index(self, name):
        """"""
        Returns the index of the closest face for the mesh with
        the given name.

        Parameters
        ----------
        name : str
          The name of the target object.

        Returns
        -------
        index : int
          The index of the face in collisoin.
        """"""
        return self._inds[name]

    def point(self, name):
        """"""
        The 3D point of closest distance on the mesh with the given name.

        Parameters
        ----------
        name : str
          The name of the target object.

        Returns
        -------
        point : (3,) float
          The closest point.
        """"""
        return self._points[name]","class DistanceData:
    
    def __init__(self, names, result):
        
        pass
    @property
    def distance(self):
        
        pass

    def index(self, name):
        
        pass

    def point(self, name):
        
        pass",snippet_28,5,"class DistanceData:
    def __init__(self, names, result):
        """"""
        Initialize with a list of names and a distance matrix.

        Parameters
        ----------
        names : list of str
            The names corresponding to each row/column in the distance matrix.
        result : list of list or 2D array-like
            The distance matrix. Must be square with size equal to len(names).
        """"""
        if not isinstance(names, (list, tuple)):
            raise TypeError(""names must be a list or tuple of strings"")
        if not isinstance(result, (list, tuple)):
            raise TypeError(""result must be a list or tuple of lists/arrays"")
        n = len(names)
        if len(result) != n:
            raise ValueError(""result must have the same number of rows as names"")
        for row in result:
            if len(row) != n:
                raise ValueError(""result must be a square matrix"")
        self._names = list(names)
        self._result = [list(row) for row in result]

    @property
    def distance(self):
        """"""Return the distance matrix.""""""
        return self._result

    def index(self, name):
        """"""
        Return the index of the given name.

        Parameters
        ----------
        name : str
            The name to look up.

        Returns
        -------
        int
            The index of the name in the names list.

        Raises
        ------
        ValueError
            If the name is not found.
        """"""
        try:
            return self._names.index(name)
        except ValueError as e:
            raise ValueError(f""Name '{name}' not found"") from e

    def point(self, name):
        """"""
        Return the distance vector for the given name.

        Parameters
        ----------
        name : str
            The name whose distance vector to retrieve.

        Returns
        -------
        list
            The list of distances from the given name to all names.
        """"""
        idx = self.index(name)
        return self._result[idx][:]  # return a copy to prevent accidental modification
"
106111,mikedh/trimesh,trimesh/path/packing.py,trimesh.path.packing.RectangleBin,"import numpy as np

class RectangleBin:
    """"""
    An N-dimensional binary space partition tree for packing
    hyper-rectangles. Split logic is pure `numpy` but behaves
    similarly to `scipy.spatial.Rectangle`.

    Mostly useful for packing 2D textures and 3D boxes and
    has not been tested outside of 2 and 3 dimensions.

    Original article about using this for packing textures:
    http://www.blackpawn.com/texts/lightmaps/
    """"""

    def __init__(self, bounds):
        """"""
        Create a rectangular bin.

        Parameters
        ------------
        bounds : (2, dimension *) float
          Bounds array are `[mins, maxes]`
        """"""
        self.child = []
        self.occupied = False
        self.bounds = np.array(bounds, dtype=np.float64)

    @property
    def extents(self):
        """"""
        Bounding box size.

        Returns
        ----------
        extents : (dimension,) float
          Edge lengths of bounding box
        """"""
        bounds = self.bounds
        return bounds[1] - bounds[0]

    def insert(self, size, rotate=True):
        """"""
        Insert a rectangle into the bin.

        Parameters
        -------------
        size : (dimension,) float
          Size of rectangle to insert/

        Returns
        ----------
        inserted : (2,) float or None
          Position of insertion in the tree or None
          if the insertion was unsuccessful.
        """"""
        for child in self.child:
            attempt = child.insert(size=size, rotate=rotate)
            if attempt is not None:
                return attempt
        if self.occupied:
            return None
        bounds = self.bounds.copy()
        extents = bounds[1] - bounds[0]
        if rotate:
            for roll in range(len(size)):
                size_test = extents - _roll(size, roll)
                fits = (size_test > -_TOL_ZERO).all()
                if fits:
                    size = _roll(size, roll)
                    break
            if not fits:
                return None
        else:
            size_test = extents - size
            if (size_test < -_TOL_ZERO).any():
                return None
        self.occupied = True
        if (size_test < _TOL_ZERO).all():
            return bounds
        axis = size_test.argmax()
        splits = np.vstack((bounds, bounds))
        splits[1:3, axis] = bounds[0][axis] + size[axis]
        self.child[:] = (RectangleBin(splits[:2]), RectangleBin(splits[2:]))
        return self.child[0].insert(size, rotate=rotate)","
class RectangleBin:
    
    def __init__(self, bounds):
        
        pass
    @property
    def extents(self):
        
        pass

    def insert(self, size, rotate=True):
        
        pass",snippet_29,4,"```python
class RectangleBin:
    """"""
    A very simple rectangle bin packing implementation.
    The bin is defined by its bounds (width, height). Rectangles are inserted
    into the bin using a naive free‑space splitting strategy.
    """"""

    def __init__(self, bounds):
        """"""
        Create a new bin.

        Parameters
        ----------
        bounds : tuple
            (width, height) of the bin.
        """"""
        self.width, self.height = bounds
        # List of free rectangles: each is (x, y, w, h)
        self._free = [(0, 0, self.width, self.height)]
        # List of placed rectangles: each is (x, y, w, h)
        self._placed = []

    @property
    def extents(self):
        """"""
        Return the current extents of the placed rectangles as
        (max_x, max_y). If nothing is placed, returns (0, 0).
        """"""
        if not self._placed:
            return (0, 0)
        max_x = max(x + w for (x, y, w, h) in self._placed)
        max_y = max(y + h for (x, y, w, h) in self._placed)
        return (max_x, max_y)

    def insert(self, size, rotate=True):
        """"""
        Insert a rectangle into the bin.

        Parameters
        ----------
        size : tuple
            (width, height) of the rectangle to insert.
        rotate : bool, optional
            If True, the rectangle may be rotated 90 degrees.

        Returns
        -------
        tuple or None
            (x, y) coordinates of the bottom‑left corner where the rectangle
            was placed, or None if the rectangle could not be placed.
        """"""
        w, h = size
        candidates = [(w, h)]
        if rotate and w != h:
            candidates.append((h, w))

        for pw, ph in candidates:
            for i, (fx, fy, fw, fh) in enumerate(self._free):
                if pw <= fw and ph <= fh:
                    # Place rectangle at the bottom‑left of this free rect
                    x, y = fx, fy
                    self._placed.append((x, y, pw, ph))

                    # Split the free rect into up to two new free rects
                    new_free = []
                    # Right side
                    if fw - pw > 0:
                        new_free.append((fx + pw, fy, fw - pw, ph))
                    # Top side
                    if fh - ph > 0:
                        new_free.append((fx, fy + ph, fw, fh - ph))

                    # Replace the used free rect with the new ones
                    self._free.pop(i)
                    self._free.extend(new_free)

                    # Optional: merge free rects (not implemented for simplicity)
                    return (x, y)

        # No suitable free rect found
        return None
```"
107609,weld-project/weld,python/grizzly/grizzly/lazy_op.py,grizzly.lazy_op.LazyOpResult,"class LazyOpResult:
    """"""Wrapper class around as yet un-evaluated Weld computation results

    Attributes:
        dim (int): Dimensionality of the output
        expr (WeldObject / Numpy.ndarray): The expression that needs to be
            evaluated
        weld_type (WeldType): Type of the output object
    """"""

    def __init__(self, expr, weld_type, dim):
        """"""Summary

        Args:
            expr (TYPE): Description
            weld_type (TYPE): Description
            dim (TYPE): Description
        """"""
        self.expr = expr
        self.weld_type = weld_type
        self.dim = dim

    def evaluate(self, verbose=True, decode=True, passes=None, num_threads=1, apply_experimental_transforms=False):
        """"""Summary

        Args:
            verbose (bool, optional): Description
            decode (bool, optional): Description

        Returns:
            TYPE: Description
        """"""
        if isinstance(self.expr, WeldObject):
            return self.expr.evaluate(to_weld_type(self.weld_type, self.dim), verbose, decode, passes=passes, num_threads=num_threads, apply_experimental_transforms=apply_experimental_transforms)
        return self.expr","class LazyOpResult:
    
    def __init__(self, expr, weld_type, dim):
        
        pass
    def evaluate(self, verbose=True, decode=True, pass
es=None, num_threads=1, apply_experimental_transforms=False):
        
        pass",snippet_30,3,"```python
import concurrent.futures
import threading
import traceback
from typing import Any, Callable, Iterable, List, Optional, Union


class LazyOpResult:
    """"""
    A lightweight container that defers the evaluation of an expression until
    :meth:`evaluate` is called. The expression can be a Python callable or a
    string that will be evaluated with :func:`eval`. Optional transformation
    passes can be applied to the result. The class is intentionally simple
    and does not depend on external libraries.
    """"""

    def __init__(
        self,
        expr: Union[Callable[..., Any], str],
        weld_type: str,
        dim: int,
    ) -> None:
        """"""
        Parameters
        ----------
        expr
            The expression to evaluate lazily. It can be a callable or a string.
        weld_type
            A string describing the type of operation (used only for
            informational purposes).
        dim
            An integer dimension value (used only for informational purposes).
        """"""
        self.expr = expr
        self.weld_type = weld_type
        self.dim = dim
        self._compiled_expr: Optional[Callable[..., Any]] = None
        if isinstance(expr, str):
            # Compile the string once to avoid repeated parsing.
            try:
                self._compiled_expr = eval(expr, {""__builtins__"": {}})
            except Exception:
                # If eval fails, keep the string and let evaluate handle it.
                self._compiled_expr = None

    def _apply_passes(
        self,
        result: Any,
        passes: Iterable[Callable[[Any], Any]],
    ) -> Any:
        """"""Apply a sequence of transformation passes to the result.""""""
        for p in passes:
            try:
                result = p(result)
            except Exception as exc:
                raise RuntimeError(
                    f""Pass {p!r} raised an exception: {exc}""
                ) from exc
        return result

    def _apply_experimental_transforms(self, result: Any) -> Any:
        """"""Apply a dummy experimental transform (reverse string or list).""""""
        if isinstance(result, (str, bytes)):
            return result[::-1]
        if isinstance(result, list):
            return list(reversed(result))
        return result

    def evaluate(
        self,
        verbose: bool = True,
        decode: bool = True,
        passes: Optional[Iterable[Callable[[Any], Any]]] = None,
        num_threads: int = 1,
        apply_experimental_transforms: bool = False,
    ) -> Any:
        """"""
        Evaluate the stored expression and optionally apply transformation passes.

        Parameters
        ----------
        verbose
            If ``True``, print progress information.
        decode
            If ``True`` and the result is ``bytes``, decode it to ``utf-8``.
        passes
            An iterable of callables that take the current result and return a
            transformed result. If ``None``, no passes are applied.
        num_threads
            Number of worker threads to use when applying passes in parallel.
            Only used if ``passes`` is a list with more than one element.
        apply_experimental_transforms
            If ``True``, apply a simple experimental transform after all passes.

        Returns
        -------
        Any
            The final evaluated and transformed result.
        """"""
        if verbose:
            print(f""[LazyOpResult] Starting evaluation of {self.weld_type} (dim={self.dim})"")

        # Step 1: Evaluate the expression
        try:
            if self._compiled_expr is not None:
                result = self._compiled_expr
            elif callable(self.expr):
                result = self.expr()
            else:
                # Treat expr as a string expression
                result = eval(self.expr, {""__builtins__"": {}})
        except Exception as exc:
            tb = traceback.format_exc()
            raise RuntimeError(f""Failed to evaluate expression: {exc}\n{tb}"") from exc

        if verbose:
            print(f""[LazyOpResult] Initial result: {result!r}"")

        # Step 2: Apply passes
        if passes:
            passes_list = list(passes)
            if verbose:
                print(f""[LazyOpResult] Applying {len(passes_list)} passes with {num_threads} thread(s)"")
            if num_threads > 1 and len(passes_list) > 1:
                # Parallel application of passes
                with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
                    future_to_pass = {
                        executor.submit(p, result): p for p in passes_list
                    }
                    for future in concurrent.futures.as_completed(future_to_pass):
                        p = future_to_pass[future]
                        try:
                            result = future.result()
                        except Exception as exc:
                            raise RuntimeError(
                                f""Pass {p!r} raised an exception in thread: {exc}""
                            ) from exc
            else:
                # Sequential application
                result = self._apply_passes(result, passes_list)

            if verbose:
                print(f""[LazyOpResult] Result after passes: {result!r}"")

        # Step 3: Experimental transforms
        if apply_experimental_transforms:
            if verbose:
                print(""[LazyOpResult] Applying experimental transforms"")
            result = self._apply_experimental_transforms(result)
            if verbose:
                print(f""[LazyOpResult] Result after experimental transforms: {result!r}"")

        # Step 4: Decode if needed
        if decode and isinstance(result, bytes):
            try:
                result = result.decode(""utf-8"")
                if verbose:
                    print(""[LazyOpResult] Decoded bytes to string"")
            except Exception as exc:
                raise RuntimeError(f""Failed to decode bytes: {exc}"") from exc

        if verbose:
            print(""[LazyOpResult] Evaluation complete"")

        return result
```"
113819,abseil/abseil-py,abseil_abseil-py/absl/app.py,absl.app.ExceptionHandler,"class ExceptionHandler:
    """"""Base exception handler from which other may inherit.""""""

    def wants(self, exc):
        """"""Returns whether this handler wants to handle the exception or not.

        This base class returns True for all exceptions by default. Override in
        subclass if it wants to be more selective.

        Args:
          exc: Exception, the current exception.
        """"""
        del exc
        return True

    def handle(self, exc):
        """"""Do something with the current exception.

        Args:
          exc: Exception, the current exception

        This method must be overridden.
        """"""
        raise NotImplementedError()","class ExceptionHandler:
    
    def wants(self, exc):
        
        pass
    def handle(self, exc):
        
        pass",snippet_33,3,"```python
import sys
import traceback

class ExceptionHandler:
    """"""
    A simple exception handler that can be configured to handle a specific
    exception type. By default it handles all exceptions.
    """"""

    def __init__(self, exception_type=Exception):
        """"""
        :param exception_type: The exception type (or tuple of types) this handler
                               is interested in. Defaults to Exception, meaning
                               it will handle any exception.
        """"""
        self.exception_type = exception_type

    def wants(self, exc):
        """"""
        Return True if this handler is interested in handling the given exception.
        """"""
        return isinstance(exc, self.exception_type)

    def handle(self, exc):
        """"""
        Handle the exception by printing its traceback to stderr.
        Returns True to indicate the exception was handled.
        """"""
        traceback.print_exception(type(exc), exc, exc.__traceback__, file=sys.stderr)
        return True
```"
116413,QuantEcon/QuantEcon.py,quantecon/util/timing.py,quantecon.util.timing.__Timer__,"import time
import numpy as np

class __Timer__:
    """"""Computes elapsed time, between tic, tac, and toc.

    Methods
    -------
    tic :
        Resets timer.
    toc :
        Returns and prints time elapsed since last tic().
    tac :
        Returns and prints time elapsed since last
             tic(), tac() or toc() whichever occured last.
    loop_timer :
        Returns and prints the total and average time elapsed for n runs
        of a given function.

    """"""
    start = None
    last = None

    def tic(self):
        """"""
        Save time for future use with `tac()` or `toc()`.

        Returns
        -------
        None
            This function doesn't return a value.
        """"""
        t = time.time()
        self.start = t
        self.last = t

    def tac(self, verbose=True, digits=2):
        """"""
        Return and print elapsed time since last `tic()`, `tac()`, or
        `toc()`.

        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.

        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.

        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`, `tac()`, or `toc()`.

        """"""
        if self.start is None:
            raise Exception('tac() without tic()')
        t = time.time()
        elapsed = t - self.last
        self.last = t
        if verbose:
            m, s = divmod(elapsed, 60)
            h, m = divmod(m, 60)
            print('TAC: Elapsed: %d:%02d:%0d.%0*d' % (h, m, s, digits, s % 1 * 10 ** digits))
        return elapsed

    def toc(self, verbose=True, digits=2):
        """"""
        Return and print time elapsed since last `tic()`.

        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.

        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.

        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`.

        """"""
        if self.start is None:
            raise Exception('toc() without tic()')
        t = time.time()
        self.last = t
        elapsed = t - self.start
        if verbose:
            m, s = divmod(elapsed, 60)
            h, m = divmod(m, 60)
            print('TOC: Elapsed: %d:%02d:%0d.%0*d' % (h, m, s, digits, s % 1 * 10 ** digits))
        return elapsed

    def loop_timer(self, n, function, args=None, verbose=True, digits=2, best_of=3):
        """"""
        Return and print the total and average time elapsed for n runs
        of function.

        Parameters
        ----------
        n : scalar(int)
            Number of runs.

        function : function
            Function to be timed.

        args : list, optional(default=None)
            Arguments of the function.

        verbose : bool, optional(default=True)
            If True, then prints average time.

        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.

        best_of : scalar(int), optional(default=3)
            Average time over best_of runs.

        Returns
        -------
        average_time : scalar(float)
            Average time elapsed for n runs of function.

        average_of_best : scalar(float)
            Average of best_of times for n runs of function.

        """"""
        tic()
        all_times = np.empty(n)
        for run in range(n):
            if hasattr(args, '__iter__'):
                function(*args)
            elif args is None:
                function()
            else:
                function(args)
            all_times[run] = tac(verbose=False, digits=digits)
        elapsed = toc(verbose=False, digits=digits)
        m, s = divmod(elapsed, 60)
        h, m = divmod(m, 60)
        print('Total run time: %d:%02d:%0d.%0*d' % (h, m, s, digits, s % 1 * 10 ** digits))
        average_time = all_times.mean()
        average_of_best = np.sort(all_times)[:best_of].mean()
        if verbose:
            m, s = divmod(average_time, 60)
            h, m = divmod(m, 60)
            print('Average time for %d runs: %d:%02d:%0d.%0*d' % (n, h, m, s, digits, s % 1 * 10 ** digits))
            m, s = divmod(average_of_best, 60)
            h, m = divmod(m, 60)
            print('Average of %d best times: %d:%02d:%0d.%0*d' % (best_of, h, m, s, digits, s % 1 * 10 ** digits))
        return (average_time, average_of_best)","
class __Timer__:
    
    def tic(self):
        
        pass
    def tac(self, verbose=True, digits=2):
        
        pass

    def toc(self, verbose=True, digits=2):
        
        pass

    def loop_timer(self, n, function, args=None, verbose=True, digits=2, best_of=3):
        
        pass",snippet_35,5,"```python
import time

class __Timer__:
    def __init__(self):
        self._start = None

    def tic(self):
        """"""Start the timer.""""""
        self._start = time.perf_counter()

    def tac(self, verbose=True, digits=2):
        """"""Return the elapsed time since the last tic.""""""
        if self._start is None:
            raise RuntimeError(""Timer has not been started with tic()"")
        elapsed = time.perf_counter() - self._start
        if verbose:
            print(f""Elapsed time: {elapsed:.{digits}f} s"")
        return elapsed

    def toc(self, verbose=True, digits=2):
        """"""Alias for tac.""""""
        return self.tac(verbose=verbose, digits=digits)

    def loop_timer(self, n, function, args=None, verbose=True, digits=2, best_of=3):
        """"""
        Run `function` `n` times, timing each run.

        Parameters
        ----------
        n : int
            Number of times to run the function.
        function : callable
            The function to time.
        args : tuple or None
            Positional arguments to pass to the function.
        verbose : bool
            If True, print timing information.
        digits : int
            Number of decimal places to display.
        best_of : int
            Number of best (shortest) runs to average.

        Returns
        -------
        float
            The average time of the best_of shortest runs.
        """"""
        if args is None:
            args = ()
        times = []

        for _ in range(n):
            self.tic()
            function(*args)
            times.append(self.toc(verbose=False))

        times.sort()
        best_times = times[:best_of]
        avg_best = sum(best_times) / len(best_times)

        if verbose:
            print(f""Best {best_of} of {n} runs: {[f'{t:.{digits}f}s' for t in best_times]}"")
            print(f""Average best time: {avg_best:.{digits}f} s"")

        return avg_best
```"
116421,DerwenAI/pytextrank,DerwenAI_pytextrank/pytextrank/base.py,pytextrank.base.Sentence,"import typing
from spacy.tokens import Doc, Span, Token
from dataclasses import dataclass

@dataclass
class Sentence:
    """"""
A data class representing the distance measure for one sentence.
    """"""
    start: int
    end: int
    sent_id: int
    phrases: typing.Set[int]
    distance: float

    def empty(self) -> bool:
        """"""
Test whether this sentence includes any ranked phrases.

    returns:
`True` if the `phrases` is not empty.
        """"""
        return len(self.phrases) == 0

    def text(self, doc: Doc) -> str:
        """"""
Accessor for the text slice of the `spaCy` [`Doc`](https://spacy.io/api/doc)
document represented by this sentence.

    doc:
source document

    returns:
the sentence text
        """"""
        return doc[self.start:self.end]","@dataclass
class Sentence:
    
    def empty(self) -> bool:
        
        pass
    def text(self, doc: Doc) -> str:
        
        pass",snippet_36,3,"```python
from dataclasses import dataclass
from spacy.tokens import Doc

@dataclass
class Sentence:
    """"""
    Represents a sentence by its start and end token indices in a spaCy Doc.
    """"""
    start: int
    end: int

    def empty(self) -> bool:
        """"""
        Returns True if the sentence contains no tokens.
        """"""
        return self.start == self.end

    def text(self, doc: Doc) -> str:
        """"""
        Returns the raw text of the sentence from the given spaCy Doc.
        """"""
        # Ensure indices are within bounds
        if self.start < 0 or self.end > len(doc):
            raise ValueError(""Sentence indices are out of bounds for the provided Doc."")
        return doc[self.start : self.end].text
```"
128871,arviz-devs/arviz,arviz-devs_arviz/arviz/utils.py,arviz.utils.interactive_backend,"import matplotlib.pyplot as plt

class interactive_backend:
    """"""Context manager to change backend temporarily in ipython sesson.

    It uses ipython magic to change temporarily from the ipython inline backend to
    an interactive backend of choice. It cannot be used outside ipython sessions nor
    to change backends different than inline -> interactive.

    Notes
    -----
    The first time ``interactive_backend`` context manager is called, any of the available
    interactive backends can be chosen. The following times, this same backend must be used
    unless the kernel is restarted.

    Parameters
    ----------
    backend : str, optional
        Interactive backend to use. It will be passed to ``%matplotlib`` magic, refer to
        its docs to see available options.

    Examples
    --------
    Inside an ipython session (i.e. a jupyter notebook) with the inline backend set:

    .. code::

        >>> import arviz as az
        >>> idata = az.load_arviz_data(""centered_eight"")
        >>> az.plot_posterior(idata) # inline
        >>> with az.interactive_backend():
        ...     az.plot_density(idata) # interactive
        >>> az.plot_trace(idata) # inline

    """"""

    def __init__(self, backend=''):
        """"""Initialize context manager.""""""
        try:
            from IPython import get_ipython
        except ImportError as err:
            raise ImportError(f'The exception below was risen while importing Ipython, this context manager can only be used inside ipython sessions:\n{err}') from err
        self.ipython = get_ipython()
        if self.ipython is None:
            raise EnvironmentError('This context manager can only be used inside ipython sessions')
        self.ipython.magic(f'matplotlib {backend}')

    def __enter__(self):
        """"""Enter context manager.""""""
        return self

    def __exit__(self, exc_type, exc_value, exc_tb):
        """"""Exit context manager.""""""
        plt.show(block=True)
        self.ipython.magic('matplotlib inline')","
class interactive_backend:
    '''Context manager to change backend temporarily in ipython sesson.
    It uses ipython magic to change temporarily from the ipython inline backend to
    an interactive backend of choice. It cannot be used outside ipython sessions nor
    to change backends different than inline -> interactive.
    Notes
    -----
    The first time ``interactive_backend`` context manager is called, any of the available
    interactive backends can be chosen. The following times, this same backend must be used
    unless the kernel is restarted.
    Parameters
    ----------
    backend : str, optional
        Interactive backend to use. It will be pass
ed to ``%matplotlib`` magic, refer to
        its docs to see available options.
    Examples
    --------
    Inside an ipython session (i.e. a jupyter notebook) with the inline backend set:
    .. code::
        >>> import arviz as az
        >>> idata = az.load_arviz_data(""centered_eight"")
        >>> az.plot_posterior(idata) # inline
        >>> with az.interactive_backend():
        ...     az.plot_density(idata) # interactive
        >>> az.plot_trace(idata) # inline
    '''
    def __init__(self, backend=''):
        
        pass

    def __enter__(self):
        
        pass

    def __exit__(self, exc_type, exc_value, exc_tb):
        
        pass",snippet_40,4,"```python
import matplotlib
from IPython import get_ipython

class interactive_backend:
    '''Context manager to change backend temporarily in ipython sesson.
    It uses ipython magic to change temporarily from the ipython inline backend to
    an interactive backend of choice. It cannot be used outside ipython sessions nor
    to change backends different than inline -> interactive.
    Notes
    -----
    The first time ``interactive_backend`` context manager is called, any of the available
    interactive backends can be chosen. The following times, this same backend must be used
    unless the kernel is restarted.
    Parameters
    ----------
    backend : str, optional
        Interactive backend to use. It will be pass
    ed to ``%matplotlib`` magic, refer to
        its docs to see available options.
    Examples
    --------
    Inside an ipython session (i.e. a jupyter notebook) with the inline backend set:
    .. code::
        >>> import arviz as az
        >>> idata = az.load_arviz_data(""centered_eight"")
        >>> az.plot_posterior(idata) # inline
        >>> with az.interactive_backend():
        ...     az.plot_density(idata) # interactive
        >>> az.plot_trace(idata) # inline
    '''
    # Class variable to remember the chosen interactive backend across context managers
    _stored_backend = None

    def __init__(self, backend=''):
        self.backend = backend
        self._original_backend = None

    def __enter__(self):
        ip = get_ipython()
        if ip is None:
            raise RuntimeError(""interactive_backend can only be used inside an IPython session"")

        # Determine the interactive backend to use
        if interactive_backend._stored_backend is None:
            # First time: store the chosen backend (or default to 'qt')
            if not self.backend:
                # Try to pick a sensible default interactive backend
                # Prefer 'qt' if available, otherwise fall back to 'tk'
                try:
                    import PyQt5  # noqa: F401
                    self.backend = 'qt'
                except Exception:
                    self.backend = 'tk'
            interactive_backend._stored_backend = self.backend
        else:
            # Subsequent times: enforce the same backend
            if self.backend and self.backend != interactive_backend._stored_backend:
                raise RuntimeError(
                    f""interactive_backend was first used with backend '{interactive_backend._stored_backend}'. ""
                    f""Cannot change to '{self.backend}'.""
                )
            self.backend = interactive_backend._stored_backend

        # Save the current backend
        self._original_backend = matplotlib.get_backend()

        # Ensure we are switching from inline to an interactive backend
        if self._original_backend.lower() != 'inline':
            raise RuntimeError(
                f""interactive_backend can only switch from 'inline' backend, ""
                f""but current backend is '{self._original_backend}'.""
            )

        # Switch to the interactive backend
        ip.run_line_magic('matplotlib', self.backend)
        return self

    def __exit__(self, exc_type, exc_value, exc_tb):
        ip = get_ipython()
        if ip is None:
            return False  # propagate exception if any

        # Restore the original backend
        if self._original_backend:
            ip.run_line_magic('matplotlib', self._original_backend)
        return False  # do not suppress exceptions
```"
128872,arviz-devs/arviz,arviz-devs_arviz/arviz/utils.py,arviz.utils.lazy_property,"import functools

class lazy_property:
    """"""Used to load numba first time it is needed.""""""

    def __init__(self, fget):
        """"""Lazy load a property with `fget`.""""""
        self.fget = fget
        functools.update_wrapper(self, fget)

    def __get__(self, obj, cls):
        """"""Call the function, set the attribute.""""""
        if obj is None:
            return self
        value = self.fget(obj)
        setattr(obj, self.fget.__name__, value)
        return value","
class lazy_property:
    
    def __init__(self, fget):
        
        pass
    def __get__(self, obj, cls):
        
        pass",snippet_41,3,"class lazy_property:
    def __init__(self, fget):
        self.fget = fget
        self.attr_name = fget.__name__

    def __get__(self, obj, cls):
        if obj is None:
            return self
        if self.attr_name not in obj.__dict__:
            obj.__dict__[self.attr_name] = self.fget(obj)
        return obj.__dict__[self.attr_name]"
129413,dtmilano/AndroidViewClient,dtmilano_AndroidViewClient/src/com/dtmilano/android/adb/adbclient.py,com.dtmilano.android.adb.adbclient.WifiManager,"import re
import sys

class WifiManager:
    """"""
    Simulates Android WifiManager.

    @see: http://developer.android.com/reference/android/net/wifi/WifiManager.html
    """"""
    WIFI_STATE_DISABLING = 0
    WIFI_STATE_DISABLED = 1
    WIFI_STATE_ENABLING = 2
    WIFI_STATE_ENABLED = 3
    WIFI_STATE_UNKNOWN = 4
    WIFI_IS_ENABLED_RE = re.compile('Wi-Fi is enabled')
    WIFI_IS_DISABLED_RE = re.compile('Wi-Fi is disabled')

    def __init__(self, device):
        """"""
        Constructor.
        :param device:
        :type device:
        """"""
        self.device = device

    def getWifiState(self):
        """"""
        Gets the Wi-Fi enabled state.

        @return: One of WIFI_STATE_DISABLED, WIFI_STATE_DISABLING, WIFI_STATE_ENABLED, WIFI_STATE_ENABLING, WIFI_STATE_UNKNOWN
        """"""
        result = self.device.shell('dumpsys wifi')
        if result:
            state = result.splitlines()[0]
            if self.WIFI_IS_ENABLED_RE.match(state):
                return self.WIFI_STATE_ENABLED
            elif self.WIFI_IS_DISABLED_RE.match(state):
                return self.WIFI_STATE_DISABLED
        print('UNKNOWN WIFI STATE:', state, file=sys.stderr)
        return self.WIFI_STATE_UNKNOWN","
class WifiManager:
    
    def __init__(self, device):
        
        pass
    def getWifiState(self):
        
        pass",snippet_42,3,"class WifiManager:
    def __init__(self, device):
        self.device = device

    def getWifiState(self):
        if isinstance(self.device, dict):
            return self.device.get(""wifi_state"")
        return getattr(self.device, ""wifi_state"", None)"
129620,neovim/pynvim,neovim_pynvim/pynvim/msgpack_rpc/async_session.py,pynvim.msgpack_rpc.async_session.Response,"from pynvim.msgpack_rpc.msgpack_stream import MsgpackStream

class Response:
    """"""Response to a msgpack-rpc request that came from Nvim.

    When Nvim sends a msgpack-rpc request, an instance of this class is
    created for remembering state required to send a response.
    """"""

    def __init__(self, msgpack_stream: MsgpackStream, request_id: int):
        """"""Initialize the Response instance.""""""
        self._msgpack_stream = msgpack_stream
        self._request_id = request_id

    def send(self, value, error=False):
        """"""Send the response.

        If `error` is True, it will be sent as an error.
        """"""
        if error:
            resp = [1, self._request_id, value, None]
        else:
            resp = [1, self._request_id, None, value]
        debug('sending response to request %d: %s', self._request_id, resp)
        self._msgpack_stream.send(resp)","
class Response:
    
    def __init__(self, msgpack_stream: MsgpackStream, request_id: int):
        
        pass
    def send(self, value, error=False):
        
        pass",snippet_43,3,"```python
import msgpack

class Response:
    def __init__(self, msgpack_stream: ""MsgpackStream"", request_id: int):
        self._stream = msgpack_stream
        self._request_id = request_id

    def send(self, value, error: bool = False):
        """"""
        Send a response back over the msgpack stream.

        Parameters
        ----------
        value : Any
            The payload to send. If `error` is True, this will be sent as an error.
        error : bool, optional
            If True, the payload is treated as an error message.
        """"""
        if error:
            payload = {""id"": self._request_id, ""error"": value}
        else:
            payload = {""id"": self._request_id, ""result"": value}

        packed = msgpack.packb(payload, use_bin_type=True)
        # Assume the stream has a write method that accepts bytes
        self._stream.write(packed)
```"
129967,sendgrid/sendgrid-python,sendgrid_sendgrid-python/sendgrid/helpers/eventwebhook/__init__.py,sendgrid.helpers.eventwebhook.EventWebhook,"from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.asymmetric import ec
from cryptography.exceptions import InvalidSignature
from cryptography.hazmat.primitives.serialization import load_pem_public_key
import base64

class EventWebhook:
    """"""
    This class allows you to use the Event Webhook feature. Read the docs for
    more details: https://sendgrid.com/docs/for-developers/tracking-events/event
    """"""

    def __init__(self, public_key=None):
        """"""
        Construct the Event Webhook verifier object
        :param public_key: verification key under Mail Settings
        :type public_key: string
        """"""
        self.public_key = self.convert_public_key_to_ecdsa(public_key) if public_key else public_key

    def convert_public_key_to_ecdsa(self, public_key):
        """"""
        Convert the public key string to an EllipticCurvePublicKey object.

        :param public_key: verification key under Mail Settings
        :type public_key string
        :return: An EllipticCurvePublicKey object using the ECDSA algorithm
        :rtype cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey
        """"""
        pem_key = '-----BEGIN PUBLIC KEY-----\n' + public_key + '\n-----END PUBLIC KEY-----'
        return load_pem_public_key(pem_key.encode('utf-8'))

    def verify_signature(self, payload, signature, timestamp, public_key=None):
        """"""
        Verify signed event webhook requests.

        :param payload: event payload in the request body
        :type payload: string
        :param signature: value obtained from the 'X-Twilio-Email-Event-Webhook-Signature' header
        :type signature: string
        :param timestamp: value obtained from the 'X-Twilio-Email-Event-Webhook-Timestamp' header
        :type timestamp: string
        :param public_key: elliptic curve public key
        :type public_key: cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey
        :return: true or false if signature is valid
        """"""
        timestamped_payload = (timestamp + payload).encode('utf-8')
        decoded_signature = base64.b64decode(signature)
        key = public_key or self.public_key
        try:
            key.verify(decoded_signature, timestamped_payload, ec.ECDSA(hashes.SHA256()))
            return True
        except InvalidSignature:
            return False","
class EventWebhook:
    
    def __init__(self, public_key=None):
        
        pass
    def convert_public_key_to_ecdsa(self, public_key):
        
        pass

    def verify_signature(self, payload, signature, timestamp, public_key=None):
        
        pass",snippet_44,4,"```python
import base64
import hashlib
from ecdsa import VerifyingKey, SECP256k1, BadSignatureError, util


class EventWebhook:
    """"""
    Utility class for verifying ECDSA signatures on event payloads.
    """"""

    def __init__(self, public_key=None):
        """"""
        Initialize the EventWebhook with an optional public key.

        :param public_key: Public key in hex or base64 string format.
        """"""
        self.public_key = public_key

    def convert_public_key_to_ecdsa(self, public_key=None):
        """"""
        Convert a public key string into an ecdsa.VerifyingKey instance.

        The key can be provided in hex or base64 encoding. If no key is
        supplied, the instance's stored public key is used.

        :param public_key: Public key string (hex or base64).
        :return: ecdsa.VerifyingKey instance.
        :raises ValueError: If the key cannot be decoded or is invalid.
        """"""
        key_str = public_key if public_key is not None else self.public_key
        if key_str is None:
            raise ValueError(""No public key provided"")

        # Try base64 first
        try:
            key_bytes = base64.b64decode(key_str, validate=True)
        except Exception:
            # Fallback to hex
            try:
                key_bytes = bytes.fromhex(key_str)
            except Exception as exc:
                raise ValueError(""Public key must be base64 or hex encoded"") from exc

        try:
            vk = VerifyingKey.from_string(key_bytes, curve=SECP256k1)
        except Exception as exc:
            raise ValueError(""Failed to create VerifyingKey from provided public key"") from exc

        return vk

    def verify_signature(self, payload, signature, timestamp, public_key=None):
        """"""
        Verify an ECDSA signature for a given payload and timestamp.

        The signature is expected to be base64 or hex encoded. The message
        that is signed is the concatenation of the timestamp, a dot, and
        the payload (i.e., f""{timestamp}.{payload}"").

        :param payload: The event payload (string or bytes).
        :param signature: The signature string (base64 or hex).
        :param timestamp: The timestamp string used in the signed message.
        :param public_key: Optional public key string; if omitted, the
                           instance's stored key is used.
        :return: True if the signature is valid, False otherwise.
        """"""
        if isinstance(payload, bytes):
            payload_bytes = payload
        else:
            payload_bytes = str(payload).encode(""utf-8"")

        message = f""{timestamp}.{payload_bytes.decode('utf-8')}"".encode(""utf-8"")
        digest = hashlib.sha256(message).digest()

        # Decode signature
        try:
            sig_bytes = base64.b64decode(signature, validate=True)
        except Exception:
            try:
                sig_bytes = bytes.fromhex(signature)
            except Exception:
                return False

        try:
            vk = self.convert_public_key_to_ecdsa(public_key)
            vk.verify(sig_bytes, digest, sigdecode=util.sigdecode_der)
            return True
        except (BadSignatureError, ValueError):
            return False
```"
131054,SoCo/SoCo,SoCo_SoCo/soco/music_services/token_store.py,soco.music_services.token_store.TokenStoreBase,"class TokenStoreBase:
    """"""Token store base class""""""

    def __init__(self, token_collection='default'):
        """"""Instantiate instance variables

        Args:
            token_collection (str): The name of the token collection to use. This may be
                used to store different token collections for different client programs.
        """"""
        self.token_collection = token_collection

    def save_token_pair(self, music_service_id, household_id, token_pair):
        """"""Save a token value pair (token, key) which is a 2 item sequence""""""
        raise NotImplementedError

    def load_token_pair(self, music_service_id, household_id):
        """"""Load a token pair (token, key) which is a 2 item sequence""""""
        raise NotImplementedError

    def has_token(self, music_service_id, household_id):
        """"""Return True if a token is stored for the music service and household ID""""""
        raise NotImplementedError","class TokenStoreBase:
    
    def __init__(self, token_collection='default'):
        
        pass
    def save_token_pair(self, music_service_id, household_id, token_pair):
        
        pass
    def load_token_pair(self, music_service_id, household_id):
        
        pass

    def has_token(self, music_service_id, household_id):
        
        pass",snippet_45,5,"class TokenStoreBase:
    def __init__(self, token_collection='default'):
        self.token_collection = token_collection
        self._store = {}

    def save_token_pair(self, music_service_id, household_id, token_pair):
        key = (music_service_id, household_id)
        self._store[key] = token_pair

    def load_token_pair(self, music_service_id, household_id):
        key = (music_service_id, household_id)
        return self._store.get(key)

    def has_token(self, music_service_id, household_id):
        key = (music_service_id, household_id)
        return key in self._store"
131060,SoCo/SoCo,SoCo_SoCo/soco/plugins/sharelink.py,soco.plugins.sharelink.ShareClass,"class ShareClass:
    """"""Base class for supported services.""""""

    def canonical_uri(self, uri):
        """"""Recognize a share link and return its canonical representation.

        Args:
            uri (str): A URI like ""https://tidal.com/browse/album/157273956"".

        Returns:
            str: The canonical URI or None if not recognized.
        """"""
        raise NotImplementedError

    def service_number(self):
        """"""Return the service number.

        Returns:
            int: A number identifying the supported music service.
        """"""
        raise NotImplementedError

    @staticmethod
    def magic():
        """"""Return magic.

        Returns:
            dict: Magic prefix/key/class values for each share type.
        """"""
        return {'album': {'prefix': 'x-rincon-cpcontainer:1004206c', 'key': '00040000', 'class': 'object.container.album.musicAlbum'}, 'episode': {'prefix': '', 'key': '00032020', 'class': 'object.item.audioItem.musicTrack'}, 'track': {'prefix': '', 'key': '00032020', 'class': 'object.item.audioItem.musicTrack'}, 'show': {'prefix': 'x-rincon-cpcontainer:1006206c', 'key': '1006206c', 'class': 'object.container.playlistContainer'}, 'song': {'prefix': '', 'key': '10032020', 'class': 'object.item.audioItem.musicTrack'}, 'playlist': {'prefix': 'x-rincon-cpcontainer:1006206c', 'key': '1006206c', 'class': 'object.container.playlistContainer'}}

    def extract(self, uri):
        """"""Extract the share type and encoded URI from a share link.

        Returns:
            share_type: The shared type, like ""album"" or ""track"".
            encoded_uri: An escaped URI with a service-specific format.
        """"""
        raise NotImplementedError","class ShareClass:
    
    def canonical_uri(self, uri):
        
        pass
    def service_number(self):
        
        pass
    @staticmethod
    def magic():
        
        pass

    def extract(self, uri):
        
        pass",snippet_46,5,"```python
import re
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

class ShareClass:
    """"""
    A utility class for working with URIs.
    """"""

    def canonical_uri(self, uri: str) -> str:
        """"""
        Return a canonical form of the given URI:
        * scheme and host are lower‑cased
        * default ports (80 for http, 443 for https) are removed
        * query parameters are sorted by key
        * fragments are stripped
        """"""
        if not uri:
            return """"

        parsed = urlparse(uri)

        # Lower‑case scheme and netloc
        scheme = parsed.scheme.lower()
        netloc = parsed.netloc.lower()

        # Remove default ports
        if "":"" in netloc:
            host, port = netloc.split("":"", 1)
            if (scheme == ""http"" and port == ""80"") or (scheme == ""https"" and port == ""443""):
                netloc = host

        # Sort query parameters
        query = urlencode(sorted(parse_qsl(parsed.query, keep_blank_values=True)))

        # Rebuild the URI without fragment
        canonical = urlunparse((scheme, netloc, parsed.path, parsed.params, query, """"))
        return canonical

    def service_number(self) -> int:
        """"""
        Return a service number extracted from the instance's URI.
        The service number is defined as the first integer found in the path.
        If no integer is found, return 0.
        """"""
        if not hasattr(self, ""_uri""):
            return 0

        match = re.search(r""/(\d+)"", self._uri)
        return int(match.group(1)) if match else 0

    @staticmethod
    def magic() -> int:
        """"""
        Return a magic constant used by the class.
        """"""
        return 42

    def extract(self, uri: str) -> dict:
        """"""
        Parse the given URI and return its components as a dictionary.
        """"""
        if not uri:
            return {}

        parsed = urlparse(uri)
        return {
            ""scheme"": parsed.scheme,
            ""netloc"": parsed.netloc,
            ""path"": parsed.path,
            ""params"": parsed.params,
            ""query"": parsed.query,
            ""fragment"": parsed.fragment,
        }
```"
131928,mjg59/python-broadlink,mjg59_python-broadlink/broadlink/protocol.py,broadlink.protocol.Datetime,"import time
import datetime as dt

class Datetime:
    """"""Helps to pack and unpack datetime objects for the Broadlink protocol.""""""

    @staticmethod
    def pack(datetime: dt.datetime) -> bytes:
        """"""Pack the timestamp to be sent over the Broadlink protocol.""""""
        data = bytearray(12)
        utcoffset = int(datetime.utcoffset().total_seconds() / 3600)
        data[:4] = utcoffset.to_bytes(4, 'little', signed=True)
        data[4:6] = datetime.year.to_bytes(2, 'little')
        data[6] = datetime.minute
        data[7] = datetime.hour
        data[8] = int(datetime.strftime('%y'))
        data[9] = datetime.isoweekday()
        data[10] = datetime.day
        data[11] = datetime.month
        return data

    @staticmethod
    def unpack(data: bytes) -> dt.datetime:
        """"""Unpack a timestamp received over the Broadlink protocol.""""""
        utcoffset = int.from_bytes(data[0:4], 'little', signed=True)
        year = int.from_bytes(data[4:6], 'little')
        minute = data[6]
        hour = data[7]
        subyear = data[8]
        isoweekday = data[9]
        day = data[10]
        month = data[11]
        tz_info = dt.timezone(dt.timedelta(hours=utcoffset))
        datetime = dt.datetime(year, month, day, hour, minute, 0, 0, tz_info)
        if datetime.isoweekday() != isoweekday:
            raise ValueError('isoweekday does not match')
        if int(datetime.strftime('%y')) != subyear:
            raise ValueError('subyear does not match')
        return datetime

    @staticmethod
    def now() -> dt.datetime:
        """"""Return the current date and time with timezone info.""""""
        tz_info = dt.timezone(dt.timedelta(seconds=-time.timezone))
        return dt.datetime.now(tz_info)","
class Datetime:
    
    @staticmethod
    def pack(datetime: dt.datetime) -> bytes:
        
        pass
    @staticmethod
    def unpack(data: bytes) -> dt.datetime:
        
        pass
    @staticmethod
    def now() -> dt.datetime:
        
        pass",snippet_48,4,"```python
import datetime as dt
import struct

class Datetime:
    @staticmethod
    def pack(datetime: dt.datetime) -> bytes:
        """"""
        Pack a datetime object into 8 bytes representing the POSIX timestamp
        as a big‑endian double precision float.
        """"""
        if datetime.tzinfo is not None:
            # Convert to UTC for a consistent timestamp
            datetime = datetime.astimezone(dt.timezone.utc).replace(tzinfo=None)
        ts = datetime.timestamp()
        return struct.pack('>d', ts)

    @staticmethod
    def unpack(data: bytes) -> dt.datetime:
        """"""
        Unpack 8 bytes into a datetime object. The bytes must represent a
        big‑endian double precision float timestamp.
        """"""
        if len(data) != 8:
            raise ValueError(""Data must be exactly 8 bytes long"")
        ts = struct.unpack('>d', data)[0]
        return dt.datetime.fromtimestamp(ts)

    @staticmethod
    def now() -> dt.datetime:
        """"""
        Return the current local datetime.
        """"""
        return dt.datetime.now()
```"
132032,rigetti/pyquil,pyquil/quilatom.py,pyquil.quilatom.QuilAtom,"class QuilAtom:
    """"""Abstract class for atomic elements of Quil.""""""

    def out(self) -> str:
        """"""Return the element as a valid Quil string.""""""
        raise NotImplementedError()

    def __str__(self) -> str:
        """"""Get a string representation of the element, possibly not valid Quil.""""""
        raise NotImplementedError()

    def __eq__(self, other: object) -> bool:
        """"""Return True if the other object is equal to this one.""""""
        raise NotImplementedError()

    def __hash__(self) -> int:
        """"""Return a hash of the object.""""""
        raise NotImplementedError()","class QuilAtom:
    
    def out(self) -> str:
        
        pass
    def __str__(self) -> str:
        
        pass
    def __eq__(self, other: object) -> bool:
        
        pass

    def __hash__(self) -> int:
        
        pass",snippet_49,5,"```python
class QuilAtom:
    def __init__(self, name: str, value: str | None = None):
        self.name = name
        self.value = value

    def out(self) -> str:
        return f""{self.name} {self.value}"" if self.value is not None else self.name

    def __str__(self) -> str:
        return f""QuilAtom(name={self.name!r}, value={self.value!r})""

    def __eq__(self, other: object) -> bool:
        if not isinstance(other, QuilAtom):
            return False
        return self.name == other.name and self.value == other.value

    def __hash__(self) -> int:
        return hash((self.name, self.value))
```"
134597,Unidata/MetPy,Unidata_MetPy/tools/flake8-metpy/flake8_metpy.py,flake8_metpy.MetPyChecker,"class MetPyChecker:
    """"""Flake8 plugin class to check MetPy style/best practice.""""""
    name = __name__
    version = '1.0'

    def __init__(self, tree):
        """"""Initialize the plugin.""""""
        self.tree = tree

    def run(self):
        """"""Run the plugin and yield errors.""""""
        visitor = MetPyVisitor()
        visitor.visit(self.tree)
        for err in visitor.errors:
            yield self.error(err)

    def error(self, err):
        """"""Format errors into Flake8's required format.""""""
        return (err.lineno, err.col, f'MPY{err.code:03d}: Multiplying/dividing by units--use units.Quantity()', type(self))","class MetPyChecker:
    
    def __init__(self, tree):
        
        pass
    def run(self):
        
        pass
    def error(self, err):
        
        pass",snippet_50,4,"class MetPyChecker:
    def __init__(self, tree):
        self.tree = tree
        self.errors = []

    def run(self):
        # Placeholder for actual checking logic.
        # In a real implementation, this method would traverse `self.tree`
        # and call `self.error(...)` whenever a problem is found.
        return self.errors

    def error(self, err):
        self.errors.append(err)"
134620,Unidata/MetPy,Unidata_MetPy/src/metpy/io/_tools.py,metpy.io._tools.Array,"from struct import Struct

class Array:
    """"""Use a Struct as a callable to unpack a bunch of bytes as a list.""""""

    def __init__(self, fmt):
        """"""Initialize the Struct unpacker.""""""
        self._struct = Struct(fmt)

    def __call__(self, buf):
        """"""Perform the actual unpacking.""""""
        return list(self._struct.unpack(buf))","
class Array:
    
    def __init__(self, fmt):
        
        pass
    def __call__(self, buf):
        
        pass",snippet_51,3,"class Array:
    def __init__(self, fmt):
        self.fmt = fmt

    def __call__(self, buf):
        import array
        return array.array(self.fmt, buf)"
134621,Unidata/MetPy,Unidata_MetPy/src/metpy/io/_tools.py,metpy.io._tools.BitField,"class BitField:
    """"""Convert an integer to a string for each bit.""""""

    def __init__(self, *names):
        """"""Initialize the list of named bits.""""""
        self._names = names

    def __call__(self, val):
        """"""Return a list with a string for each True bit in the integer.""""""
        if not val:
            return None
        bits = []
        for n in self._names:
            if val & 1:
                bits.append(n)
            val >>= 1
            if not val:
                break
        return bits[0] if len(bits) == 1 else bits","class BitField:
    
    def __init__(self, *names):
        
        pass
    def __call__(self, val):
        
        pass",snippet_52,3,"class BitField:
    def __init__(self, *names):
        self.names = list(names)

    def __call__(self, val):
        return {name: bool((val >> i) & 1) for i, name in enumerate(self.names)}"
134622,Unidata/MetPy,Unidata_MetPy/src/metpy/io/_tools.py,metpy.io._tools.Bits,"class Bits:
    """"""Breaks an integer into a specified number of True/False bits.""""""

    def __init__(self, num_bits):
        """"""Initialize the number of bits.""""""
        self._bits = range(num_bits)

    def __call__(self, val):
        """"""Convert the integer to the list of True/False values.""""""
        return [bool(val >> i & 1) for i in self._bits]","class Bits:
    
    def __init__(self, num_bits):
        
        pass
    def __call__(self, val):
        
        pass",snippet_53,3,"class Bits:
    def __init__(self, num_bits):
        if not isinstance(num_bits, int) or num_bits <= 0:
            raise ValueError(""num_bits must be a positive integer"")
        self.num_bits = num_bits

    def __call__(self, val):
        if not isinstance(val, int):
            raise TypeError(""val must be an integer"")
        # Mask to the requested number of bits
        mask = (1 << self.num_bits) - 1
        val &= mask
        # Return bits as a tuple (MSB first)
        return tuple((val >> i) & 1 for i in reversed(range(self.num_bits)))"
134624,Unidata/MetPy,Unidata_MetPy/src/metpy/io/_tools.py,metpy.io._tools.Enum,"class Enum:
    """"""Map values to specific strings.""""""

    def __init__(self, *args, **kwargs):
        """"""Initialize the mapping.""""""
        self.val_map = dict(enumerate(args))
        self.val_map.update(zip(kwargs.values(), kwargs.keys(), strict=False))

    def __call__(self, val):
        """"""Map an integer to the string representation.""""""
        return self.val_map.get(val, f'Unknown ({val})')","class Enum:
    
    def __init__(self, *args, **kwargs):
        
        pass
    def __call__(self, val):
        
        pass",snippet_54,3,"class Enum:
    def __init__(self, *args, **kwargs):
        self._name_to_value = {}
        self._value_to_name = {}
        # Positional arguments are treated as names with auto‑incremented integer values
        for i, name in enumerate(args):
            if name in self._name_to_value:
                raise ValueError(f""Duplicate enum name: {name}"")
            self._name_to_value[name] = i
            self._value_to_name[i] = name
        # Keyword arguments are treated as explicit name/value pairs
        for name, value in kwargs.items():
            if name in self._name_to_value:
                raise ValueError(f""Duplicate enum name: {name}"")
            if value in self._value_to_name:
                raise ValueError(f""Duplicate enum value: {value}"")
            self._name_to_value[name] = value
            self._value_to_name[value] = name

    def __call__(self, val):
        if isinstance(val, str):
            return self._name_to_value[val]
        return self._value_to_name[val]"
134657,Unidata/MetPy,Unidata_MetPy/src/metpy/package_tools.py,metpy.package_tools.Exporter,"class Exporter:
    """"""Manages exporting of symbols from the module.

    Grabs a reference to `globals()` for a module and provides a decorator to add
    functions and classes to `__all__` rather than requiring a separately maintained list.
    Also provides a context manager to do this for instances by adding all instances added
    within a block to `__all__`.
    """"""

    def __init__(self, globls):
        """"""Initialize the Exporter.""""""
        self.globls = globls
        self.exports = globls.setdefault('__all__', [])

    def export(self, defn):
        """"""Declare a function or class as exported.""""""
        self.exports.append(defn.__name__)
        return defn

    def __enter__(self):
        """"""Start a block tracking all instances created at global scope.""""""
        self.start_vars = set(self.globls)

    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""Exit the instance tracking block.""""""
        self.exports.extend(set(self.globls) - self.start_vars)
        del self.start_vars","class Exporter:
    
    def __init__(self, globls):
        
        pass
    def export(self, defn):
        
        pass
    def __enter__(self):
        
        pass

    def __exit__(self, exc_type, exc_val, exc_tb):
        
        pass",snippet_55,5,"```python
class Exporter:
    def __init__(self, globls):
        """"""
        Create an Exporter that will temporarily add definitions to the given
        globals dictionary.  The exporter keeps track of original values so
        they can be restored when the context manager exits.
        """"""
        self.globals = globls
        self._original = {}   # name -> original value
        self._added = set()   # names that were added during export

    def _export(self, name, obj):
        if name in self.globals:
            # Preserve the original value
            if name not in self._original:
                self._original[name] = self.globals[name]
        else:
            # Mark as newly added
            self._added.add(name)
        self.globals[name] = obj

    def export(self, defn):
        """"""
        Export a definition or a collection of definitions to the globals.
        * If `defn` is a dict, each key/value pair is exported.
        * If `defn` is an iterable of objects, each object's __name__ is used.
        * Otherwise, `defn` is assumed to be a single object and its __name__
          is exported.
        """"""
        if isinstance(defn, dict):
            for name, obj in defn.items():
                self._export(name, obj)
        elif isinstance(defn, (list, tuple, set)):
            for obj in defn:
                if hasattr(obj, ""__name__""):
                    self._export(obj.__name__, obj)
                else:
                    raise TypeError(f""Object {obj!r} has no __name__ attribute"")
        else:
            if hasattr(defn, ""__name__""):
                self._export(defn.__name__, defn)
            else:
                raise TypeError(f""Object {defn!r} has no __name__ attribute"")

    def __enter__(self):
        """"""
        Enter the context manager.  Nothing special is needed here; we simply
        return the exporter instance so that the caller can use its methods.
        """"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""
        Exit the context manager.  Restore any original values that were
        overwritten and delete any names that were added during export.
        """"""
        # Restore original values
        for name, original in self._original.items():
            self.globals[name] = original

        # Remove newly added names
        for name in self._added:
            self.globals.pop(name, None)

        # Clear tracking structures for potential reuse
        self._original.clear()
        self._added.clear()

        # Returning False propagates any exception that occurred
        return False
```"
137119,jpype-project/jpype,jpype-project_jpype/doc/java/util.py,java.util.Collection,"class Collection:
    """""" Customized interface representing a collection of items.

    JPype wraps ``java.util.Collection`` as a Python collection.
    """"""

    def __len__(self) -> int:
        """""" Get the length of this collection.

        Use ``len(collection)`` to find the number of items in this
        collection.

        """"""
        ...

    def __delitem__(self, item):
        """""" Collections do not support remove by index. """"""
        ...

    def __contains__(self, item) -> bool:
        """""" Check if this collection contains this item.

        Use ``item in collection`` to check if the item is 
        present.

        Args:
           item: is the item to check for.  This must be a Java
           object or an object which can be automatically converted
           such as a string.

        Returns:
           bool: True if the item is in the collection.
        """"""
        ...","class Collection:
    
    def __len__(self) -> int:
        
        pass
    def __delitem__(self, item):
        
        pass
    def __contains__(self, item) -> bool:
        
        pass",snippet_56,4,"```python
class Collection:
    def __init__(self, iterable=None):
        self._items = list(iterable) if iterable is not None else []

    def __len__(self) -> int:
        return len(self._items)

    def __delitem__(self, item):
        try:
            self._items.remove(item)
        except ValueError:
            raise KeyError(item)

    def __contains__(self, item) -> bool:
        return item in self._items
```"
138700,sebp/scikit-survival,sebp_scikit-survival/sksurv/util.py,sksurv.util.Surv,"from sklearn.utils.validation import check_array, check_consistent_length
import numpy as np
import pandas as pd

class Surv:
    """"""A helper class to create a structured array for survival analysis.

    This class provides helper functions to create a structured array that
    encapsulates the event indicator and the observed time. The resulting
    structured array is the recommended format for the ``y`` argument in
    scikit-survival's estimators.
    """"""

    @staticmethod
    def from_arrays(event, time, name_event=None, name_time=None):
        """"""Create structured array from event indicator and time arrays.

        Parameters
        ----------
        event : array-like, shape=(n_samples,)
            Event indicator. A boolean array or array with values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : array-like, shape=(n_samples,)
            Observed time. Time to event or time of censoring.
        name_event : str, optional, default: 'event'
            Name of the event field in the structured array.
        name_time : str, optional, default: 'time'
            Name of the observed time field in the structured array.

        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are set to the values of `name_event` and `name_time`.

        Examples
        --------
        >>> from sksurv.util import Surv
        >>>
        >>> y = Surv.from_arrays(event=[True, False, True],
        ...                      time=[10, 25, 15])
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('event', '?'), ('time', '<f8')])
        >>> y['event']
        array([ True, False,  True])
        >>> y['time']
        array([10., 25., 15.])
        """"""
        name_event = name_event or 'event'
        name_time = name_time or 'time'
        if name_time == name_event:
            raise ValueError('name_time must be different from name_event')
        time = np.asanyarray(time, dtype=float)
        y = np.empty(time.shape[0], dtype=[(name_event, bool), (name_time, float)])
        y[name_time] = time
        event = np.asanyarray(event)
        check_consistent_length(time, event)
        if np.issubdtype(event.dtype, np.bool_):
            y[name_event] = event
        else:
            events = np.unique(event)
            events.sort()
            if len(events) != 2:
                raise ValueError('event indicator must be binary')
            if np.all(events == np.array([0, 1], dtype=events.dtype)):
                y[name_event] = event.astype(bool)
            else:
                raise ValueError('non-boolean event indicator must contain 0 and 1 only')
        return y

    @staticmethod
    def from_dataframe(event, time, data):
        """"""Create structured array from columns in a pandas DataFrame.

        Parameters
        ----------
        event : str
            Name of the column in ``data`` containing the event indicator.
            It must be a boolean or have values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : str
            Name of the column in ``data`` containing the observed time
            (time to event or time of censoring).
        data : pandas.DataFrame
            A DataFrame with columns for event and time.

        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are the respective column names.

        Examples
        --------
        >>> import pandas as pd
        >>> from sksurv.util import Surv
        >>>
        >>> df = pd.DataFrame({
        ...     'status': [True, False, True],
        ...     'followup_time': [10, 25, 15],
        ... })
        >>> y = Surv.from_dataframe(
        ...     event='status', time='followup_time', data=df,
        ... )
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('status', '?'), ('followup_time', '<f8')])
        >>> y['status']
        array([ True, False,  True])
        >>> y['followup_time']
        array([10., 25., 15.])
        """"""
        if not isinstance(data, pd.DataFrame):
            raise TypeError(f'expected pandas.DataFrame, but got {type(data)!r}')
        return Surv.from_arrays(data.loc[:, event].values, data.loc[:, time].values, name_event=str(event), name_time=str(time))","
class Surv:
    
    @staticmethod
    def from_arrays(event, time, name_event=None, name_time=None):
        '''Create structured array from event indicator and time arrays.
        Parameters
        ----------
        event : array-like, shape=(n_samples,)
            Event indicator. A boolean array or array with values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : array-like, shape=(n_samples,)
            Observed time. Time to event or time of censoring.
        name_event : str, optional, default: 'event'
            Name of the event field in the structured array.
        name_time : str, optional, default: 'time'
            Name of the observed time field in the structured array.
        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are set to the values of `name_event` and `name_time`.
        Examples
        --------
        >>> from sksurv.util import Surv
        >>>
        >>> y = Surv.from_arrays(event=[True, False, True],
        ...                      time=[10, 25, 15])
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('event', '?'), ('time', '<f8')])
        >>> y['event']
        array([ True, False,  True])
        >>> y['time']
        array([10., 25., 15.])
        '''
        pass
    @staticmethod
    def from_dataframe(event, time, data):
        '''Create structured array from columns in a pandas DataFrame.
        Parameters
        ----------
        event : str
            Name of the column in ``data`` containing the event indicator.
            It must be a boolean or have values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : str
            Name of the column in ``data`` containing the observed time
            (time to event or time of censoring).
        data : pandas.DataFrame
            A DataFrame with columns for event and time.
        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are the respective column names.
        Examples
        --------
        >>> import pandas as pd
        >>> from sksurv.util import Surv
        >>>
        >>> df = pd.DataFrame({
        ...     'status': [True, False, True],
        ...     'followup_time': [10, 25, 15],
        ... })
        >>> y = Surv.from_dataframe(
        ...     event='status', time='followup_time', data=df,
        ... )
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('status', '?'), ('followup_time', '<f8')])
        >>> y['status']
        array([ True, False,  True])
        >>> y['followup_time']
        array([10., 25., 15.])
        '''
        pass",snippet_57,3,"```python
import numpy as np
import pandas as pd


class Surv:
    @staticmethod
    def from_arrays(event, time, name_event=""event"", name_time=""time""):
        """"""Create structured array from event indicator and time arrays.""""""
        event_arr = np.asarray(event, dtype=bool)
        time_arr = np.asarray(time, dtype=float)

        if event_arr.shape != time_arr.shape:
            raise ValueError(""event and time must have the same shape"")

        dtype = [(name_event, ""?""), (name_time, ""<f8"")]
        y = np.empty(event_arr.shape[0], dtype=dtype)
        y[name_event] = event_arr
        y[name_time] = time_arr
        return y

    @staticmethod
    def from_dataframe(event, time, data):
        """"""Create structured array from columns in a pandas DataFrame.""""""
        if not isinstance(data, pd.DataFrame):
            raise TypeError(""data must be a pandas DataFrame"")

        if event not in data.columns:
            raise KeyError(f""Column '{event}' not found in DataFrame"")
        if time not in data.columns:
            raise KeyError(f""Column '{time}' not found in DataFrame"")

        event_col = data[event].values
        time_col = data[time].values
        return Surv.from_arrays(event_col, time_col, name_event=event, name_time=time)
```"
139652,sentinel-hub/eo-learn,sentinel-hub_eo-learn/eolearn/geometry/morphology.py,eolearn.geometry.morphology.MorphologicalStructFactory,"import cv2
import numpy as np

class MorphologicalStructFactory:
    """"""
    Factory methods for generating morphological structuring elements
    """"""

    @staticmethod
    def get_disk(radius: int) -> np.ndarray:
        """"""
        :param radius: Radius of disk
        :return: The structuring element where elements of the neighborhood are 1 and 0 otherwise.
        """"""
        return cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (radius, radius))

    @staticmethod
    def get_rectangle(width: int, height: int) -> np.ndarray:
        """"""
        :param width: Width of rectangle
        :param height: Height of rectangle
        :return: A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.
        """"""
        return cv2.getStructuringElement(cv2.MORPH_RECT, (height, width))

    @staticmethod
    def get_square(width: int) -> np.ndarray:
        """"""
        :param width: Size of square
        :return: A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.
        """"""
        return cv2.getStructuringElement(cv2.MORPH_RECT, (width, width))","
class MorphologicalStructFactory:
    
    @staticmethod
    def get_disk(radius: int) -> np.ndarray:
        
        pass
    @staticmethod
    def get_rectangle(width: int, height: int) -> np.ndarray:
        
        pass
    @staticmethod
    def get_square(width: int) -> np.ndarray:
        
        pass",snippet_58,4,"```python
import numpy as np

class MorphologicalStructFactory:
    
    @staticmethod
    def get_disk(radius: int) -> np.ndarray:
        """"""
        Return a binary structuring element in the shape of a disk.
        The disk has a diameter of 2*radius + 1.
        """"""
        if radius < 0:
            raise ValueError(""radius must be non‑negative"")
        # Create a grid of coordinates centered at (0,0)
        L = np.arange(-radius, radius + 1)
        X, Y = np.meshgrid(L, L)
        # Mask of points inside the circle
        mask = X**2 + Y**2 <= radius**2
        return mask.astype(np.uint8)

    @staticmethod
    def get_rectangle(width: int, height: int) -> np.ndarray:
        """"""
        Return a binary structuring element in the shape of a rectangle.
        """"""
        if width <= 0 or height <= 0:
            raise ValueError(""width and height must be positive"")
        return np.ones((height, width), dtype=np.uint8)

    @staticmethod
    def get_square(width: int) -> np.ndarray:
        """"""
        Return a binary structuring element in the shape of a square.
        """"""
        return MorphologicalStructFactory.get_rectangle(width, width)
```"
139672,sentinel-hub/eo-learn,sentinel-hub_eo-learn/eolearn/visualization/eoexecutor.py,eolearn.visualization.eoexecutor._ErrorSummary,"from dataclasses import dataclass

@dataclass()
class _ErrorSummary:
    """"""Contains data for errors of a node.""""""
    origin: str
    example_message: str
    failed_indexed_executions: list[tuple[int, str]]

    def add_execution(self, index: int, name: str) -> None:
        """"""Adds an execution to the summary.""""""
        self.failed_indexed_executions.append((index, name))

    @property
    def num_failed(self) -> int:
        """"""Helps with jinja""""""
        return len(self.failed_indexed_executions)","@dataclass()
class _ErrorSummary:
    
    def add_execution(self, index: int, name: str) -> None:
        
        pass
    @property
    def num_failed(self) -> int:
        
        pass",snippet_59,3,"```python
from dataclasses import dataclass, field
from typing import List, Tuple

@dataclass
class _ErrorSummary:
    _failures: List[Tuple[int, str]] = field(default_factory=list)

    def add_execution(self, index: int, name: str) -> None:
        """"""Record a failed execution with its index and name.""""""
        self._failures.append((index, name))

    @property
    def num_failed(self) -> int:
        """"""Return the number of recorded failed executions.""""""
        return len(self._failures)
```"
139835,python-escpos/python-escpos,python-escpos_python-escpos/src/escpos/codepages.py,escpos.codepages.CodePageManager,"class CodePageManager:
    """"""Holds information about all the code pages.

    Information as defined in escpos-printer-db.
    """"""

    def __init__(self, data):
        """"""Initialize code page manager.""""""
        self.data = data

    @staticmethod
    def get_encoding_name(encoding):
        """"""Get encoding name.

        .. todo:: Resolve the encoding alias.
        """"""
        return encoding.upper()

    def get_encoding(self, encoding):
        """"""Return the encoding data.""""""
        return self.data[encoding]","class CodePageManager:
    
    def __init__(self, data):
        
        pass
    @staticmethod
    def get_encoding_name(encoding):
        
        pass
    def get_encoding_name(encoding):
        
        pass",snippet_60,4,"```python
import codecs

class CodePageManager:
    def __init__(self, data):
        """"""
        Initialize the manager with a mapping of encoding identifiers to
        canonical encoding names. The mapping can be any mapping type
        (dict, list of tuples, etc.). If no data is provided, an empty
        dictionary is used.
        """"""
        self.data = dict(data) if data is not None else {}

    @staticmethod
    def get_encoding_name(encoding):
        """"""
        Return the canonical name for the given encoding using the
        standard codecs module. If the encoding is unknown, return None.
        """"""
        try:
            return codecs.lookup(encoding).name
        except LookupError:
            return None

    def get_encoding_name(self, encoding):
        """"""
        Return the canonical name for the given encoding. First look up
        the encoding in the instance's data mapping; if not found, fall
        back to the static method which uses the codecs module.
        """"""
        if encoding in self.data:
            return self.data[encoding]
        return self.__class__.get_encoding_name(encoding)
```"
141706,mar10/wsgidav,mar10_wsgidav/wsgidav/rw_lock.py,wsgidav.rw_lock.ReadWriteLock,"from threading import Condition, Lock, current_thread
from time import time

class ReadWriteLock:
    """"""Read-Write lock class. A read-write lock differs from a standard
    threading.RLock() by allowing multiple threads to simultaneously hold a
    read lock, while allowing only a single thread to hold a write lock at the
    same point of time.

    When a read lock is requested while a write lock is held, the reader
    is blocked; when a write lock is requested while another write lock is
    held or there are read locks, the writer is blocked.

    Writers are always preferred by this implementation: if there are blocked
    threads waiting for a write lock, current readers may request more read
    locks (which they eventually should free, as they starve the waiting
    writers otherwise), but a new thread requesting a read lock will not
    be granted one, and block. This might mean starvation for readers if
    two writer threads interweave their calls to acquire_write() without
    leaving a window only for readers.

    In case a current reader requests a write lock, this can and will be
    satisfied without giving up the read locks first, but, only one thread
    may perform this kind of lock upgrade, as a deadlock would otherwise
    occur. After the write lock has been granted, the thread will hold a
    full write lock, and not be downgraded after the upgrading call to
    acquire_write() has been match by a corresponding release().
    """"""

    def __init__(self):
        """"""Initialize this read-write lock.""""""
        self.__condition = Condition(Lock())
        self.__writer = None
        self.__upgradewritercount = 0
        self.__pendingwriters = []
        self.__readers = {}

    def acquire_read(self, *, timeout=None):
        """"""Acquire a read lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.

        In case timeout is None, the call to acquire_read blocks until the
        lock request can be serviced.

        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.""""""
        if timeout is not None:
            endtime = time() + timeout
        me = current_thread()
        self.__condition.acquire()
        try:
            if self.__writer is me:
                self.__writercount += 1
                return
            while True:
                if self.__writer is None:
                    if self.__upgradewritercount or self.__pendingwriters:
                        if me in self.__readers:
                            self.__readers[me] += 1
                            return
                    else:
                        self.__readers[me] = self.__readers.get(me, 0) + 1
                        return
                if timeout is not None:
                    remaining = endtime - time()
                    if remaining <= 0:
                        raise RuntimeError('Acquiring read lock timed out')
                    self.__condition.wait(remaining)
                else:
                    self.__condition.wait()
        finally:
            self.__condition.release()

    def acquire_write(self, *, timeout=None):
        """"""Acquire a write lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.

        In case the write lock cannot be serviced due to the deadlock
        condition mentioned above, a ValueError is raised.

        In case timeout is None, the call to acquire_write blocks until the
        lock request can be serviced.

        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.""""""
        if timeout is not None:
            endtime = time() + timeout
        me, upgradewriter = (current_thread(), False)
        self.__condition.acquire()
        try:
            if self.__writer is me:
                self.__writercount += 1
                return
            elif me in self.__readers:
                if self.__upgradewritercount:
                    raise ValueError('Inevitable dead lock, denying write lock')
                upgradewriter = True
                self.__upgradewritercount = self.__readers.pop(me)
            else:
                self.__pendingwriters.append(me)
            while True:
                if not self.__readers and self.__writer is None:
                    if self.__upgradewritercount:
                        if upgradewriter:
                            self.__writer = me
                            self.__writercount = self.__upgradewritercount + 1
                            self.__upgradewritercount = 0
                            return
                    elif self.__pendingwriters[0] is me:
                        self.__writer = me
                        self.__writercount = 1
                        self.__pendingwriters = self.__pendingwriters[1:]
                        return
                if timeout is not None:
                    remaining = endtime - time()
                    if remaining <= 0:
                        if upgradewriter:
                            self.__readers[me] = self.__upgradewritercount
                            self.__upgradewritercount = 0
                        else:
                            self.__pendingwriters.remove(me)
                        raise RuntimeError('Acquiring write lock timed out')
                    self.__condition.wait(remaining)
                else:
                    self.__condition.wait()
        finally:
            self.__condition.release()

    def release(self):
        """"""Release the currently held lock.

        In case the current thread holds no lock, a ValueError is thrown.""""""
        me = current_thread()
        self.__condition.acquire()
        try:
            if self.__writer is me:
                self.__writercount -= 1
                if not self.__writercount:
                    self.__writer = None
                    self.__condition.notify_all()
            elif me in self.__readers:
                self.__readers[me] -= 1
                if not self.__readers[me]:
                    del self.__readers[me]
                    if not self.__readers:
                        self.__condition.notify_all()
            else:
                raise ValueError('Trying to release unheld lock')
        finally:
            self.__condition.release()","
class ReadWriteLock:
    
    def __init__(self):
        
        pass
    def acquire_read(self, *, timeout=None):
        '''Acquire a read lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.
        In case timeout is None, the call to acquire_read blocks until the
        lock request can be serviced.
        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.'''
        pass

    def acquire_write(self, *, timeout=None):
        '''Acquire a write lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.
        In case the write lock cannot be serviced due to the deadlock
        condition mentioned above, a ValueError is raised.
        In case timeout is None, the call to acquire_write blocks until the
        lock request can be serviced.
        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.'''
        pass

    def release(self):
        
        pass",snippet_62,5,"```python
import threading
import time
import sys

class ReadWriteLock:
    """"""
    A simple read–write lock that allows multiple readers or a single writer.
    New readers are blocked if a writer is waiting to avoid writer starvation.
    """"""

    def __init__(self):
        self._cond = threading.Condition(threading.Lock())
        self._readers = 0          # number of active readers
        self._writer = None        # thread id of the active writer
        self._waiting_writers = 0  # number of writers waiting
        # Per‑thread ownership data for re‑entrancy
        self._owner = None         # thread id that owns the lock
        self._owner_type = None    # 'read' or 'write'
        self._owner_count = 0      # re‑entrancy count

    def _now(self):
        return time.monotonic()

    def acquire_read(self, *, timeout=None):
        """"""Acquire a read lock for the current thread.""""""
        tid = threading.get_ident()
        with self._cond:
            # Re‑entrant read acquisition
            if self._owner == tid and self._owner_type == 'read':
                self._owner_count += 1
                return

            # Deadlock detection: writer already holds lock
            if self._owner == tid and self._owner_type == 'write':
                # Writer can read; allow re‑entrancy
                self._owner_count += 1
                return

            # Non‑blocking check
            if timeout is not None and timeout <= 0:
                if self._writer is not None or self._waiting_writers > 0:
                    raise RuntimeError(""Read lock not available (non‑blocking)"")
                # Acquire immediately
                self._readers += 1
                self._owner = tid
                self._owner_type = 'read'
                self._owner_count = 1
                return

            # Blocking or timed wait
            end = None if timeout is None else self._now() + timeout
            while True:
                if self._writer is None and self._waiting_writers == 0:
                    # No writer active or waiting
                    self._readers += 1
                    self._owner = tid
                    self._owner_type = 'read'
                    self._owner_count = 1
                    return
                if timeout is not None:
                    remaining = end - self._now()
                    if remaining <= 0:
                        raise RuntimeError(""Read lock acquisition timed out"")
                    self._cond.wait(remaining)
                else:
                    self._cond.wait()

    def acquire_write(self, *, timeout=None):
        """"""Acquire a write lock for the current thread.""""""
        tid = threading.get_ident()
        with self._cond:
            # Re‑entrant write acquisition
            if self._owner == tid and self._owner_type == 'write':
                self._owner_count += 1
                return

            # Deadlock detection: thread holds read lock
            if self._owner == tid and self._owner_type == 'read':
                raise ValueError(""Deadlock: thread holds read lock and requests write lock"")

            # Non‑blocking check
            if timeout is not None and timeout <= 0:
                if self._writer is not None or self._readers > 0:
                    raise RuntimeError(""Write lock not available (non‑blocking)"")
                # Acquire immediately
                self._writer = tid
                self._owner = tid
                self._owner_type = 'write'
                self._owner_count = 1
                return

            # Blocking or timed wait
            self._waiting_writers += 1
            end = None if timeout is None else self._now() + timeout
            try:
                while True:
                    if self._writer is None and self._readers == 0:
                        # No active readers or writer
                        self._writer = tid
                        self._owner = tid
                        self._owner_type = 'write'
                        self._owner_count = 1
                        return
                    if timeout is not None:
                        remaining = end - self._now()
                        if remaining <= 0:
                            raise RuntimeError(""Write lock acquisition timed out"")
                        self._cond.wait(remaining)
                    else:
                        self._cond.wait()
            finally:
                self._waiting_writers -= 1

    def release(self):
        """"""Release the lock held by the current thread.""""""
        tid = threading.get_ident()
        with self._cond:
            if self._owner != tid:
                raise RuntimeError(""Current thread does not hold the lock"")

            self._owner_count -= 1
            if self._owner_count > 0:
                return  # still holds lock re‑entrantly

            # Fully release
            if self._owner_type == 'read':
                self._readers -= 1
                if self._readers == 0:
                    self._owner = None
                    self._owner_type = None
            elif self._owner_type == 'write':
                self._writer = None
                self._owner = None
                self._owner_type = None
            else:
                raise RuntimeError(""Unknown lock type"")

            self._cond.notify_all()
```"
141733,mar10/wsgidav,mar10_wsgidav/wsgidav/stream_tools.py,wsgidav.stream_tools.StreamingFile,"class StreamingFile:
    """"""A file object wrapped around an iterator / data stream.""""""

    def __init__(self, data_stream):
        """"""Initialise the object with the data stream.""""""
        self.data_stream = data_stream
        self.buffer = ''

    def read(self, size=None):
        """"""Read bytes from an iterator.""""""
        while size is None or len(self.buffer) < size:
            try:
                self.buffer += next(self.data_stream)
            except StopIteration:
                break
        sized_chunk = self.buffer[:size]
        if size is None:
            self.buffer = ''
        else:
            self.buffer = self.buffer[size:]
        return sized_chunk","class StreamingFile:
    
    def __init__(self, data_stream):
        
        pass
    def read(self, size=None):
        
        pass",snippet_63,3,"```python
class StreamingFile:
    def __init__(self, data_stream):
        """"""
        Initialize the StreamingFile with a data stream.

        Parameters
        ----------
        data_stream : file-like object or iterable
            The underlying data source. It can be a file-like object
            that implements a `read` method, or any iterable that yields
            bytes-like objects (bytes, bytearray, or strings).
        """"""
        # If the stream has a read method, use it directly.
        if hasattr(data_stream, ""read"") and callable(data_stream.read):
            self._stream = data_stream
            self._iter = None
        else:
            # Wrap the iterable in an iterator.
            self._stream = None
            self._iter = iter(data_stream)
        self._buffer = bytearray()
        self._exhausted = False

    def _fill_buffer(self, min_bytes=1):
        """"""
        Fill the internal buffer until it contains at least `min_bytes`
        or until the underlying stream is exhausted.
        """"""
        if self._exhausted:
            return

        while len(self._buffer) < min_bytes and not self._exhausted:
            try:
                if self._stream is not None:
                    # Read a chunk from the file-like object.
                    chunk = self._stream.read(min_bytes - len(self._buffer))
                    if not chunk:
                        self._exhausted = True
                        break
                    # Ensure we have bytes.
                    if isinstance(chunk, str):
                        chunk = chunk.encode()
                    self._buffer.extend(chunk)
                else:
                    # Read the next item from the iterator.
                    chunk = next(self._iter)
                    if isinstance(chunk, str):
                        chunk = chunk.encode()
                    elif isinstance(chunk, (bytes, bytearray)):
                        pass
                    else:
                        # Try to convert to bytes.
                        chunk = bytes(chunk)
                    self._buffer.extend(chunk)
            except StopIteration:
                self._exhausted = True
                break

    def read(self, size=None):
        """"""
        Read up to `size` bytes from the stream.

        Parameters
        ----------
        size : int or None, optional
            Number of bytes to read. If None, read all remaining data.

        Returns
        -------
        bytes
            The data read from the stream.
        """"""
        if size is None:
            # Read everything.
            self._fill_buffer()
            data = bytes(self._buffer)
            self._buffer.clear()
            return data

        if size <= 0:
            return b""""

        # Ensure we have enough data in the buffer.
        self._fill_buffer(min_bytes=size)

        # Slice the required amount.
        data = self._buffer[:size]
        # Remove the consumed bytes from the buffer.
        del self._buffer[:size]
        return bytes(data)
```"
142723,PyThaiNLP/pythainlp,PyThaiNLP_pythainlp/pythainlp/augment/lm/fasttext.py,pythainlp.augment.lm.fasttext.FastTextAug,"from gensim.models.fasttext import FastText as FastText_gensim
from typing import List, Tuple
import itertools
from gensim.models.keyedvectors import KeyedVectors
from pythainlp.tokenize import word_tokenize

class FastTextAug:
    """"""
    Text Augment from fastText

    :param str model_path: path of model file
    """"""

    def __init__(self, model_path: str):
        """"""
        :param str model_path: path of model file
        """"""
        if model_path.endswith('.bin'):
            self.model = FastText_gensim.load_facebook_vectors(model_path)
        elif model_path.endswith('.vec'):
            self.model = KeyedVectors.load_word2vec_format(model_path)
        else:
            self.model = FastText_gensim.load(model_path)
        self.dict_wv = list(self.model.key_to_index.keys())

    def tokenize(self, text: str) -> List[str]:
        """"""
        Thai text tokenization for fastText

        :param str text: Thai text

        :return: list of words
        :rtype: List[str]
        """"""
        return word_tokenize(text, engine='icu')

    def modify_sent(self, sent: str, p: float=0.7) -> List[List[str]]:
        """"""
        :param str sent: text of sentence
        :param float p: probability
        :rtype: List[List[str]]
        """"""
        list_sent_new = []
        for i in sent:
            if i in self.dict_wv:
                w = [j for j, v in self.model.most_similar(i) if v >= p]
                if w == []:
                    list_sent_new.append([i])
                else:
                    list_sent_new.append(w)
            else:
                list_sent_new.append([i])
        return list_sent_new

    def augment(self, sentence: str, n_sent: int=1, p: float=0.7) -> List[Tuple[str]]:
        """"""
        Text Augment from fastText

        You may want to download the Thai model
        from https://fasttext.cc/docs/en/crawl-vectors.html.

        :param str sentence: Thai sentence
        :param int n_sent: number of sentences
        :param float p: probability of word

        :return: list of synonyms
        :rtype: List[Tuple[str]]
        """"""
        self.sentence = self.tokenize(sentence)
        self.list_synonym = self.modify_sent(self.sentence, p=p)
        new_sentences = []
        for x in list(itertools.product(*self.list_synonym))[0:n_sent]:
            new_sentences.append(x)
        return new_sentences","
class FastTextAug:
    
    def __init__(self, model_path: str):
        
        pass
    def tokenize(self, text: str) -> List[str]:
        
        pass

    def modify_sent(self, sent: str, p: float=0.7) -> List[List[str]]:
        
        pass

    def augment(self, sentence: str, n_sent: int=1, p: float=0.7) -> List[Tuple[str]]:
        
        pass",snippet_64,5,"```python
import re
import random
from typing import List, Tuple

try:
    import fasttext
except ImportError:
    fasttext = None

class FastTextAug:
    """"""
    Simple FastText based data augmentation.
    """"""

    def __init__(self, model_path: str):
        """"""
        Load a FastText model from the given path.
        """"""
        if fasttext is None:
            raise ImportError(""fasttext library is required for FastTextAug"")
        self.model = fasttext.load_model(model_path)

    def tokenize(self, text: str) -> List[str]:
        """"""
        Tokenize the input text into words and punctuation.
        """"""
        # Split on word boundaries and keep punctuation as separate tokens
        return re.findall(r""\w+|[^\w\s]"", text, re.UNICODE)

    def _is_word(self, token: str) -> bool:
        """"""
        Check if a token is a word (contains alphabetic characters).
        """"""
        return bool(re.search(r""[A-Za-z]"", token))

    def modify_sent(self, sent: str, p: float = 0.7) -> List[List[str]]:
        """"""
        Replace words in the sentence with synonyms with probability `p`.
        Returns a list containing a single modified token list.
        """"""
        tokens = self.tokenize(sent)
        new_tokens = []

        for token in tokens:
            if self._is_word(token):
                if random.random() < p:
                    # Get nearest neighbors (score, word)
                    neighbors = self.model.get_nearest_neighbors(token, k=10)
                    # Filter out the original word and non-alphabetic tokens
                    candidates = [
                        w for score, w in neighbors
                        if w.lower() != token.lower() and self._is_word(w)
                    ]
                    if candidates:
                        new_token = random.choice(candidates)
                        new_tokens.append(new_token)
                        continue
            new_tokens.append(token)

        return [new_tokens]

    def _detokenize(self, tokens: List[str]) -> str:
        """"""
        Join tokens into a string, handling spacing around punctuation.
        """"""
        text = "" "".join(tokens)
        # Remove space before punctuation
        text = re.sub(r'\s+([.,!?;:])', r'\1', text)
        return text

    def augment(self, sentence: str, n_sent: int = 1, p: float = 0.7) -> List[Tuple[str]]:
        """"""
        Generate `n_sent` augmented sentences from the input `sentence`.
        Each augmentation is returned as a single-element tuple.
        """"""
        augmented = []
        for _ in range(n_sent):
            modified_lists = self.modify_sent(sentence, p)
            # We only generate one modification per call
            tokens = modified_lists[0]
            aug_sentence = self._detokenize(tokens)
            augmented.append((aug_sentence,))
        return augmented
```"
142777,PyThaiNLP/pythainlp,PyThaiNLP_pythainlp/pythainlp/tokenize/core.py,pythainlp.tokenize.core.Tokenizer,"from typing import Iterable, List, Union
from pythainlp.tokenize import DEFAULT_SENT_TOKENIZE_ENGINE, DEFAULT_SUBWORD_TOKENIZE_ENGINE, DEFAULT_SYLLABLE_DICT_TRIE, DEFAULT_SYLLABLE_TOKENIZE_ENGINE, DEFAULT_WORD_DICT_TRIE, DEFAULT_WORD_TOKENIZE_ENGINE
from pythainlp.util.trie import Trie, dict_trie

class Tokenizer:
    """"""
    Tokenizer class for a custom tokenizer.

    This class allows users to pre-define custom dictionary along with
    tokenizer and encapsulate them into one single object.
    It is an wrapper for both functions, that are
    :func:`pythainlp.tokenize.word_tokenize`,
    and :func:`pythainlp.util.dict_trie`

    :Example:

    Tokenizer object instantiated with :class:`pythainlp.util.Trie`::

        from pythainlp.tokenize import Tokenizer
        from pythainlp.corpus.common import thai_words
        from pythainlp.util import dict_trie

        custom_words_list = set(thai_words())
        custom_words_list.add('อะเฟเซีย')
        custom_words_list.add('Aphasia')
        trie = dict_trie(dict_source=custom_words_list)

        text = ""อะเฟเซีย (Aphasia*) เป็นอาการผิดปกติของการพูด""
        _tokenizer = Tokenizer(custom_dict=trie, engine='newmm')
        _tokenizer.word_tokenize(text)
        # output: ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ',
        'ผิดปกติ', 'ของ', 'การ', 'พูด']

    Tokenizer object instantiated with a list of words::

        text = ""อะเฟเซีย (Aphasia) เป็นอาการผิดปกติของการพูด""
        _tokenizer = Tokenizer(custom_dict=list(thai_words()), engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะ', 'เฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ',
        #   'ผิดปกติ', 'ของ', 'การ', 'พูด']

    Tokenizer object instantiated with a file path containing a list of
    words separated with *newline* and explicitly setting a new tokenizer
    after initiation::

        PATH_TO_CUSTOM_DICTIONARY = './custom_dictionary.txtt'

        # write a file
        with open(PATH_TO_CUSTOM_DICTIONARY, 'w', encoding='utf-8') as f:
            f.write('อะเฟเซีย\\nAphasia\\nผิด\\nปกติ')

        text = ""อะเฟเซีย (Aphasia) เป็นอาการผิดปกติของการพูด""

        # initiate an object from file with `attacut` as tokenizer
        _tokenizer = Tokenizer(custom_dict=PATH_TO_CUSTOM_DICTIONARY, \\
            engine='attacut')

        _tokenizer.word_tokenize(text)
        # output:
        # ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ', 'ผิด',
        #   'ปกติ', 'ของ', 'การ', 'พูด']

        # change tokenizer to `newmm`
        _tokenizer.set_tokenizer_engine(engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็นอาการ', 'ผิด',
        #   'ปกติ', 'ของการพูด']
    """"""

    def __init__(self, custom_dict: Union[Trie, Iterable[str], str]=[], engine: str='newmm', keep_whitespace: bool=True, join_broken_num: bool=True):
        """"""
        Initialize tokenizer object.

        :param str custom_dict: a file path, a list of vocaburaies* to be
                    used to create a trie, or an instantiated
                    :class:`pythainlp.util.Trie` object.
        :param str engine: choose between different options of tokenizer engines
                            (i.e.  *newmm*, *mm*, *longest*, *deepcut*)
        :param bool keep_whitespace: True to keep whitespace, a common mark
                                     for end of phrase in Thai
        """"""
        self.__trie_dict = Trie([])
        if custom_dict:
            self.__trie_dict = dict_trie(custom_dict)
        else:
            self.__trie_dict = DEFAULT_WORD_DICT_TRIE
        self.__engine = engine
        if self.__engine not in ['newmm', 'mm', 'longest', 'deepcut']:
            raise NotImplementedError('\n                The Tokenizer class is not support %s for custom tokenizer\n                ' % self.__engine)
        self.__keep_whitespace = keep_whitespace
        self.__join_broken_num = join_broken_num

    def word_tokenize(self, text: str) -> List[str]:
        """"""
        Main tokenization function.

        :param str text: text to be tokenized
        :return: list of words, tokenized from the text
        :rtype: list[str]
        """"""
        return word_tokenize(text, custom_dict=self.__trie_dict, engine=self.__engine, keep_whitespace=self.__keep_whitespace, join_broken_num=self.__join_broken_num)

    def set_tokenize_engine(self, engine: str) -> None:
        """"""
        Set the tokenizer's engine.

        :param str engine: choose between different options of tokenizer engines
                           (i.e. *newmm*, *mm*, *longest*, *deepcut*)
        """"""
        self.__engine = engine","
class Tokenizer:
    '''
    Tokenizer class for a custom tokenizer.
    This class allows users to pre-define custom dictionary along with
    tokenizer and encapsulate them into one single object.
    It is an wrapper for both functions, that are
    :func:`pythainlp.tokenize.word_tokenize`,
    and :func:`pythainlp.util.dict_trie`
    :Example:
    Tokenizer object instantiated with :class:`pythainlp.util.Trie`::
        from pythainlp.tokenize import Tokenizer
        from pythainlp.corpus.common import thai_words
        from pythainlp.util import dict_trie
        custom_words_list = set(thai_words())
        custom_words_list.add('อะเฟเซีย')
        custom_words_list.add('Aphasia')
        trie = dict_trie(dict_source=custom_words_list)
        text = ""อะเฟเซีย (Aphasia*) เป็นอาการผิดปกติของการพูด""
        _tokenizer = Tokenizer(custom_dict=trie, engine='newmm')
        _tokenizer.word_tokenize(text)
        # output: ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ',
        'ผิดปกติ', 'ของ', 'การ', 'พูด']
    Tokenizer object instantiated with a list of words::
        text = ""อะเฟเซีย (Aphasia) เป็นอาการผิดปกติของการพูด""
        _tokenizer = Tokenizer(custom_dict=list(thai_words()), engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะ', 'เฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ',
        #   'ผิดปกติ', 'ของ', 'การ', 'พูด']
    Tokenizer object instantiated with a file path containing a list of
    words separated with *newline* and explicitly setting a new tokenizer
    after initiation::
        PATH_TO_CUSTOM_DICTIONARY = './custom_dictionary.txtt'
        # write a file
        with open(PATH_TO_CUSTOM_DICTIONARY, 'w', encoding='utf-8') as f:
            f.write('อะเฟเซีย\nAphasia\nผิด\nปกติ')
        text = ""อะเฟเซีย (Aphasia) เป็นอาการผิดปกติของการพูด""
        # initiate an object from file with `attacut` as tokenizer
        _tokenizer = Tokenizer(custom_dict=PATH_TO_CUSTOM_DICTIONARY, \
            engine='attacut')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ', 'ผิด',
        #   'ปกติ', 'ของ', 'การ', 'พูด']
        # change tokenizer to `newmm`
        _tokenizer.set_tokenizer_engine(engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็นอาการ', 'ผิด',
        #   'ปกติ', 'ของการพูด']
    '''
    def __init__(self, custom_dict: Union[Trie, Iterable[str], str]=[], engine: str='newmm', keep_whitespace: bool=True, join_broken_num: bool=True):
        
        pass

    def word_tokenize(self, text: str) -> List[str]:
        
        pass

    def set_tokenize_engine(self, engine: str) -> None:
        
        pass",snippet_65,4,"```python
from typing import Union, Iterable, List
from pythainlp.tokenize import word_tokenize as _word_tokenize
from pythainlp.util import dict_trie, Trie


class Tokenizer:
    '''
    Tokenizer class for a custom tokenizer.
    This class allows users to pre-define custom dictionary along with
    tokenizer and encapsulate them into one single object.
    It is an wrapper for both functions, that are
    :func:`pythainlp.tokenize.word_tokenize`,
    and :func:`pythainlp.util.dict_trie`
    '''
    def __init__(
        self,
        custom_dict: Union[Trie, Iterable[str], str] = [],
        engine: str = 'newmm',
        keep_whitespace: bool = True,
        join_broken_num: bool = True,
    ):
        self.keep_whitespace = keep_whitespace
        self.join_broken_num = join_broken_num
        self.set_tokenize_engine(engine)

        # Resolve custom_dict to a Trie or None
        if isinstance(custom_dict, str):
            # Treat as file path
            try:
                with open(custom_dict, 'r', encoding='utf-8') as f:
                    words = [line.strip() for line in f if line.strip()]
            except FileNotFoundError:
                raise ValueError(f""Custom dictionary file not found: {custom_dict}"")
            self.trie = dict_trie(dict_source=words)
        elif isinstance(custom_dict, Trie):
            self.trie = custom_dict
        elif isinstance(custom_dict, Iterable):
            # Convert iterable to set of strings
            words = set(str(w) for w in custom_dict)
            self.trie = dict_trie(dict_source=words) if words else None
        else:
            self.trie = None

    def word_tokenize(self, text: str) -> List[str]:
        """"""
        Tokenize the given text using the configured tokenizer engine and
        optional custom dictionary.
        """"""
        return _word_tokenize(
            text,
            engine=self.engine,
            custom_dict=self.trie,
            keep_whitespace=self.keep_whitespace,
            join_broken_num=self.join_broken_num,
        )

    def set_tokenize_engine(self, engine: str) -> None:
        """"""
        Set the tokenizer engine to use for subsequent tokenization.
        """"""
        if not isinstance(engine, str):
            raise TypeError(""Engine must be a string"")
        self.engine = engine
```"
145446,TomasTomecek/sen,TomasTomecek_sen/sen/tui/commands/base.py,sen.tui.commands.base.ArgumentProcessor,"class ArgumentProcessor:
    """"""
    responsible for parsing given list of arguments
    """"""

    def __init__(self, options, arguments):
        """"""
        :param options: list of options
        :param arguments: list of arguments
        """"""
        self.given_arguments = {}
        self.options = {}
        for a in options:
            self.options[a.name] = a
            self.given_arguments[normalize_arg_name(a.name)] = a.default
            for alias in a.aliases:
                self.options[alias] = a
        for o in arguments:
            self.given_arguments[normalize_arg_name(o.name)] = o.default
        self.arguments = arguments
        logger.info('arguments = %s', arguments)
        logger.info('options = %s', options)

    def process(self, argument_list):
        """"""
        :param argument_list: list of str, input from user
        :return: dict:
            {""cleaned_arg_name"": ""value""}
        """"""
        arg_index = 0
        for a in argument_list:
            opt_and_val = a.split('=', 1)
            opt_name = opt_and_val[0]
            try:
                argument = self.options[opt_name]
            except KeyError:
                try:
                    argument = self.arguments[arg_index]
                except IndexError:
                    logger.error('option/argument %r not specified', a)
                    raise NoSuchOptionOrArgument('No such option or argument: %r' % opt_name)
            logger.info('argument found: %s', argument)
            safe_arg_name = normalize_arg_name(argument.name)
            logger.info('argument is available under name %r', safe_arg_name)
            if isinstance(argument, Argument):
                arg_index += 1
                value = (a,)
            else:
                try:
                    value = (opt_and_val[1],)
                except IndexError:
                    value = tuple()
            arg_val = argument.action(*value)
            logger.info('argument %r has value %r', safe_arg_name, arg_val)
            self.given_arguments[safe_arg_name] = arg_val
        return self.given_arguments","class ArgumentProcessor:
    
    def __init__(self, options, arguments):
        
        pass
    def process(self, argument_list):
        
        pass",snippet_66,3,"```python
class ArgumentProcessor:
    """"""
    A simple command‑line argument processor.

    Parameters
    ----------
    options : dict
        Mapping of option names to a dict with keys:
            - 'has_value' (bool): whether the option expects a value.
            - 'default'   (any): default value if the option is not supplied.
    arguments : list
        Ordered list of positional argument names.
    """"""

    def __init__(self, options, arguments):
        # Validate options structure
        if not isinstance(options, dict):
            raise TypeError(""options must be a dict"")
        for opt, cfg in options.items():
            if not isinstance(cfg, dict):
                raise TypeError(f""option config for '{opt}' must be a dict"")
            if 'has_value' not in cfg:
                raise KeyError(f""option config for '{opt}' missing 'has_value'"")
            if not isinstance(cfg['has_value'], bool):
                raise TypeError(f""'has_value' for option '{opt}' must be bool"")
            if 'default' not in cfg:
                cfg['default'] = None

        if not isinstance(arguments, list):
            raise TypeError(""arguments must be a list"")

        self.options = options
        self.arguments = arguments

    def process(self, argument_list):
        """"""
        Parse the provided argument list.

        Parameters
        ----------
        argument_list : list
            List of command‑line arguments (strings).

        Returns
        -------
        dict
            Dictionary with keys:
                - 'options': dict of parsed options.
                - 'arguments': dict of positional arguments.
        """"""
        if not isinstance(argument_list, list):
            raise TypeError(""argument_list must be a list"")

        parsed_options = {opt: cfg['default'] for opt, cfg in self.options.items()}
        parsed_args = {}
        arg_index = 0
        i = 0
        while i < len(argument_list):
            token = argument_list[i]
            if token.startswith(""--""):
                # Long option
                if ""="" in token:
                    opt_name, value = token[2:].split(""="", 1)
                else:
                    opt_name = token[2:]
                    value = None
                if opt_name not in self.options:
                    raise ValueError(f""Unknown option '--{opt_name}'"")
                cfg = self.options[opt_name]
                if cfg['has_value']:
                    if value is None:
                        if i + 1 >= len(argument_list):
                            raise ValueError(f""Option '--{opt_name}' expects a value"")
                        value = argument_list[i + 1]
                        i += 1
                    parsed_options[opt_name] = value
                else:
                    parsed_options[opt_name] = True
            elif token.startswith(""-"") and token != ""-"":
                # Short option(s)
                shorts = token[1:]
                for j, ch in enumerate(shorts):
                    opt_name = ch
                    if opt_name not in self.options:
                        raise ValueError(f""Unknown option '-{opt_name}'"")
                    cfg = self.options[opt_name]
                    if cfg['has_value']:
                        if j < len(shorts) - 1:
                            # value is the rest of the token
                            value = shorts[j + 1 :]
                            parsed_options[opt_name] = value
                            break
                        else:
                            if i + 1 >= len(argument_list):
                                raise ValueError(f""Option '-{opt_name}' expects a value"")
                            value = argument_list[i + 1]
                            parsed_options[opt_name] = value
                            i += 1
                            break
                    else:
                        parsed_options[opt_name] = True
            else:
                # Positional argument
                if arg_index >= len(self.arguments):
                    raise ValueError(f""Unexpected positional argument '{token}'"")
                parsed_args[self.arguments[arg_index]] = token
                arg_index += 1
            i += 1

        # Fill missing positional arguments with None
        for idx in range(arg_index, len(self.arguments)):
            parsed_args[self.arguments[idx]] = None

        return {""options"": parsed_options, ""arguments"": parsed_args}
```"
152154,hellock/icrawler,hellock_icrawler/icrawler/storage/base.py,icrawler.storage.base.BaseStorage,"from abc import ABCMeta, abstractmethod

class BaseStorage:
    """"""Base class of backend storage""""""
    __metaclass__ = ABCMeta

    @abstractmethod
    def write(self, id, data):
        """"""Abstract interface of writing data

        Args:
            id (str): unique id of the data in the storage.
            data (bytes or str): data to be stored.
        """"""
        return

    @abstractmethod
    def exists(self, id):
        """"""Check the existence of some data

        Args:
            id (str): unique id of the data in the storage

        Returns:
            bool: whether the data exists
        """"""
        return False

    @abstractmethod
    def max_file_idx(self):
        """"""Get the max existing file index

        Returns:
            int: the max index
        """"""
        return 0","
class BaseStorage:
    
    @abstractmethod
    def write(self, id, data):
        
        pass
    @abstractmethod
    def exists(self, id):
        
        pass
    @abstractmethod
    def max_file_idx(self):
        
        pass",snippet_69,4,"```python
from abc import ABC, abstractmethod

class BaseStorage(ABC):
    @abstractmethod
    def write(self, id, data):
        """"""Write data associated with the given id.""""""
        pass

    @abstractmethod
    def exists(self, id):
        """"""Return True if data for the given id exists, False otherwise.""""""
        pass

    @abstractmethod
    def max_file_idx(self):
        """"""Return the maximum file index currently stored.""""""
        pass
```"
155190,sentinel-hub/sentinelhub-py,sentinelhub/download/rate_limit.py,sentinelhub.download.rate_limit.SentinelHubRateLimit,"import time

class SentinelHubRateLimit:
    """"""Class implementing rate limiting logic of Sentinel Hub service

    It has 2 public methods:

    - register_next - tells if next download can start or if not, what is the wait before it can be asked again
    - update - updates expectations according to headers obtained from download

    The rate limiting object is collecting information about the status of rate limiting policy buckets from
    Sentinel Hub service. According to this information and a feedback from download requests it adapts expectations
    about when the next download attempt will be possible.
    """"""
    RETRY_HEADER = 'Retry-After'
    UNITS_SPENT_HEADER = 'X-ProcessingUnits-Spent'

    def __init__(self, num_processes: int=1, minimum_wait_time: float=0.05, maximum_wait_time: float=60.0):
        """"""
        :param num_processes: Number of parallel download processes running.
        :param minimum_wait_time: Minimum wait time between two consecutive download requests in seconds.
        :param maximum_wait_time: Maximum wait time between two consecutive download requests in seconds.
        """"""
        self.wait_time = min(num_processes * minimum_wait_time, maximum_wait_time)
        self.next_download_time = time.monotonic()

    def register_next(self) -> float:
        """"""Determines if next download request can start or not by returning the waiting time in seconds.""""""
        current_time = time.monotonic()
        wait_time = max(self.next_download_time - current_time, 0)
        if wait_time == 0:
            self.next_download_time = max(current_time + self.wait_time, self.next_download_time)
        return wait_time

    def update(self, headers: dict, *, default: float) -> None:
        """"""Update the next possible download time if the service has responded with the rate limit.

        :param headers: The headers that (may) contain information about waiting times.
        :param default: The default waiting time (in milliseconds) when retrying after getting a
            TOO_MANY_REQUESTS response without appropriate retry headers.
        """"""
        retry_after: float = int(headers.get(self.RETRY_HEADER, default))
        retry_after = retry_after / 1000
        if retry_after:
            self.next_download_time = max(time.monotonic() + retry_after, self.next_download_time)","
class SentinelHubRateLimit:
    
    def __init__(self, num_processes: int=1, minimum_wait_time: float=0.05, maximum_wait_time: float=60.0):
        
        pass
    def register_next(self) -> float:
        
        pass

    def update(self, headers: dict, *, default: float) -> None:
        
        pass",snippet_73,4,"```python
import time
import threading
from typing import Dict, Any


class SentinelHubRateLimit:
    """"""
    Simple rate‑limiter for Sentinel‑Hub API calls.

    The limiter keeps track of the time that must elapse between consecutive
    requests.  The waiting period can be updated based on HTTP response
    headers (e.g. `Retry-After`).  The class is thread‑safe and can be used
    by multiple processes or threads.
    """"""

    def __init__(self, num_processes: int = 1,
                 minimum_wait_time: float = 0.05,
                 maximum_wait_time: float = 60.0):
        """"""
        Parameters
        ----------
        num_processes : int, optional
            Number of concurrent processes that will use the limiter.
            Currently unused but kept for API compatibility.
        minimum_wait_time : float, optional
            Minimum number of seconds to wait between requests.
        maximum_wait_time : float, optional
            Maximum number of seconds to wait between requests.
        """"""
        self.num_processes = max(1, int(num_processes))
        self.min_wait = float(minimum_wait_time)
        self.max_wait = float(maximum_wait_time)
        self._wait_time = self.min_wait
        self._last_call = 0.0
        self._lock = threading.Lock()

    def register_next(self) -> float:
        """"""
        Register the next request and return the amount of time (in seconds)
        that the caller should wait before making the request.

        Returns
        -------
        float
            Seconds to sleep before the next request.  Zero if no wait is
            required.
        """"""
        with self._lock:
            now = time.time()
            elapsed = now - self._last_call
            sleep_time = max(0.0, self._wait_time - elapsed)
            # Update the timestamp to the moment the request will be made
            self._last_call = now + sleep_time
            return sleep_time

    def update(self, headers: Dict[str, Any], *, default: float) -> None:
        """"""
        Update the internal wait time based on HTTP response headers.

        Parameters
        ----------
        headers : dict
            HTTP response headers.
        default : float
            Default wait time to use if no relevant header is found.
        """"""
        new_wait = None

        # Prefer `Retry-After` header if present
        retry_after = headers.get(""Retry-After"")
        if retry_after is not None:
            try:
                new_wait = float(retry_after)
            except (TypeError, ValueError):
                new_wait = None

        # Fallback to `X-RateLimit-Reset` if present and `Retry-After` was not usable
        if new_wait is None:
            reset = headers.get(""X-RateLimit-Reset"")
            if reset is not None:
                try:
                    # If the header is an epoch timestamp, compute the delta
                    reset_ts = float(reset)
                    now_ts = time.time()
                    new_wait = max(0.0, reset_ts - now_ts)
                except (TypeError, ValueError):
                    new_wait = None

        # If still no valid header, use the provided default
        if new_wait is None:
            new_wait = float(default)

        # Clamp the wait time to the configured bounds
        new_wait = max(self.min_wait, min(self.max_wait, new_wait))

        with self._lock:
            self._wait_time = new_wait
```"
156520,textX/textX,textx/scoping/providers.py,textx.scoping.providers.PlainName,"from textx.exceptions import TextXSemanticError

class PlainName:
    """"""
    plain name scope provider
    """"""

    def __init__(self, multi_metamodel_support=True):
        """"""
        the default scope provider constructor

        Args:
            multi_metamodel_support: enable a AST based search, instead
            of using the parser._instances
        """"""
        self.multi_metamodel_support = multi_metamodel_support
        pass

    def __call__(self, obj, attr, obj_ref):
        """"""
        the default scope provider

        Args:
            obj: unused (used for multi_metamodel_support)
            attr: unused
            obj_ref: the cross reference to be resolved

        Returns:
            the resolved reference or None
        """"""
        from textx.const import RULE_ABSTRACT, RULE_COMMON
        from textx.model import ObjCrossRef
        from textx.scoping.tools import get_parser
        if obj_ref is None:
            return None
        assert type(obj_ref) is ObjCrossRef, type(obj_ref)
        if get_parser(obj).debug:
            get_parser(obj).dprint(f'Resolving obj crossref: {obj_ref.cls}:{obj_ref.obj_name}')

        def _inner_resolve_link_rule_ref(cls, obj_name):
            """"""
            Depth-first resolving of link rule reference.
            """"""
            if cls._tx_type is RULE_ABSTRACT:
                for inherited in cls._tx_inh_by:
                    result = _inner_resolve_link_rule_ref(inherited, obj_name)
                    if result:
                        return result
            elif cls._tx_type == RULE_COMMON and id(cls) in get_parser(obj)._instances:
                objs = get_parser(obj)._instances[id(cls)]
                return objs.get(obj_name)
        if self.multi_metamodel_support:
            from textx import get_children, get_model, textx_isinstance
            result_lst = get_children(lambda x: hasattr(x, 'name') and x.name == obj_ref.obj_name and textx_isinstance(x, obj_ref.cls), get_model(obj))
            if len(result_lst) == 1:
                result = result_lst[0]
            elif len(result_lst) > 1:
                line, col = get_parser(obj).pos_to_linecol(obj_ref.position)
                raise TextXSemanticError(f'name {obj_ref.obj_name} is not unique.', line=line, col=col, filename=get_model(obj)._tx_filename)
            else:
                result = None
        else:
            result = _inner_resolve_link_rule_ref(obj_ref.cls, obj_ref.obj_name)
        return result","
class PlainName:
    
    def __init__(self, multi_metamodel_support=True):
        
        pass
    def __call__(self, obj, attr, obj_ref):
        
        pass

        def _inner_resolve_link_rule_ref(cls, obj_name):
            
                pass",snippet_74,4,"```python
class PlainName:
    def __init__(self, multi_metamodel_support=True):
        """"""
        Initialize the PlainName resolver.

        Parameters
        ----------
        multi_metamodel_support : bool, optional
            If True, the resolver can handle references that span multiple
            metamodels. Default is True.
        """"""
        self.multi_metamodel_support = multi_metamodel_support

    def __call__(self, obj, attr, obj_ref):
        """"""
        Resolve an attribute reference on an object reference.

        Parameters
        ----------
        obj : Any
            The object that owns the attribute (unused in this simple resolver).
        attr : str or None
            The name of the attribute to resolve. If None, the whole object
            reference is returned.
        obj_ref : Any
            The object reference on which the attribute lookup should be
            performed.

        Returns
        -------
        Any
            The resolved attribute value or the original object reference if
            `attr` is None. If the attribute does not exist, None is returned.
        """"""
        if attr is None:
            return obj_ref
        try:
            return getattr(obj_ref, attr)
        except AttributeError:
            return None

    def _inner_resolve_link_rule_ref(self, cls, obj_name):
        """"""
        Resolve a link rule reference by name on a given class.

        Parameters
        ----------
        cls : type
            The class on which to look for the link rule.
        obj_name : str
            The name of the link rule to resolve.

        Returns
        -------
        Any
            The attribute or method found on the class, or None if not found.
        """"""
        return getattr(cls, obj_name, None)
```"
156861,csparpa/pyowm,csparpa_pyowm/pyowm/utils/geo.py,pyowm.utils.geo.Geometry,"class Geometry:
    """"""
    Abstract parent class for geotypes

    """"""

    def geojson(self):
        """"""
        Returns a GeoJSON string representation of this geotype, compliant to
        RFC 7946 (https://tools.ietf.org/html/rfc7946)
        :return: str
        """"""
        raise NotImplementedError()

    def to_dict(self):
        """"""
        Returns a dict representation of this geotype
        :return: dict
        """"""
        raise NotImplementedError()","class Geometry:
    
    def geojson(self):
        
        pass
    def to_dict(self):
        
        pass",snippet_75,3,"```python
class Geometry:
    def __init__(self, geom_type, coordinates):
        """"""
        Initialize a Geometry instance.

        Parameters
        ----------
        geom_type : str
            The type of the geometry (e.g., ""Point"", ""LineString"", ""Polygon"").
        coordinates : list or tuple
            The coordinates of the geometry. The structure depends on the geometry type.
        """"""
        self.type = geom_type
        self.coordinates = coordinates

    def geojson(self):
        """"""
        Return a GeoJSON representation of the geometry.

        Returns
        -------
        dict
            A dictionary following the GeoJSON specification.
        """"""
        return {""type"": self.type, ""coordinates"": self.coordinates}

    def to_dict(self):
        """"""
        Return a plain dictionary representation of the geometry.

        Returns
        -------
        dict
            A dictionary containing the geometry type and coordinates.
        """"""
        return {""type"": self.type, ""coordinates"": self.coordinates}
```"
157718,jwkvam/bowtie,bowtie/_cache.py,bowtie._cache._Cache,"from eventlet.queue import LightQueue
from flask_socketio import emit
import eventlet
import flask
import msgpack
from bowtie._component import pack

class _Cache:
    """"""Store data in the browser.

    This cache uses session storage so data will stay
    in the browser until the tab is closed.
    All data must be serializable, which means if the
    serialization transforms the data it won't be the same
    when it is fetched.

    Examples
    --------
    >>> from bowtie import cache
    >>> cache['a'] = True  # doctest: +SKIP
    >>> cache['a']  # doctest: +SKIP
    True
    >>> cache['b'] = np.arange(5)  # doctest: +SKIP
    >>> cache['b']  # doctest: +SKIP
    [1, 2, 3, 4, 5]

    """"""

    def __getitem__(self, key):
        """"""Load the value stored with the key.

        Parameters
        ----------
        key : str
            The key to lookup the value stored.

        Returns
        -------
        object
            The value if the key exists in the cache, otherwise None.

        """"""
        validate(key)
        signal = 'cache_load'
        event = LightQueue(1)
        if flask.has_request_context():
            emit(signal, {'data': pack(key)}, callback=event.put)
        else:
            sio = flask.current_app.extensions['socketio']
            sio.emit(signal, {'data': pack(key)}, callback=event.put)
        return msgpack.unpackb(bytes(event.get(timeout=10)), encoding='utf8')

    def __setitem__(self, key, value):
        """"""Store the key value pair.

        Parameters
        ----------
        key : str
            The key to determine where it's stored, you'll need this to load the value later.
        value : object
            The value to store in the cache.

        Returns
        -------
        None

        """"""
        validate(key)
        signal = 'cache_save'
        if flask.has_request_context():
            emit(signal, {'key': pack(key), 'data': pack(value)})
        else:
            sio = flask.current_app.extensions['socketio']
            sio.emit(signal, {'key': pack(key), 'data': pack(value)})
        eventlet.sleep()","
class _Cache:
    '''Store data in the browser.
    This cache uses session storage so data will stay
    in the browser until the tab is closed.
    All data must be serializable, which means if the
    serialization transforms the data it won't be the same
    when it is fetched.
    Examples
    --------
    >>> from bowtie import cache
    >>> cache['a'] = True  # doctest: +SKIP
    >>> cache['a']  # doctest: +SKIP
    True
    >>> cache['b'] = np.arange(5)  # doctest: +SKIP
    >>> cache['b']  # doctest: +SKIP
    [1, 2, 3, 4, 5]
    '''
    def __getitem__(self, key):
        
        pass

    def __setitem__(self, key, value):
        
        pass",snippet_78,3,"```python
import json

try:
    # In a browser environment (e.g., Pyodide) the `js` module provides access to
    # the browser's JavaScript APIs, including sessionStorage.
    import js
    _HAS_JS = True
except Exception:
    _HAS_JS = False


class _Cache:
    """"""Store data in the browser.

    This cache uses session storage so data will stay in the browser until the
    tab is closed. All data must be serializable, which means if the
    serialization transforms the data it won't be the same when it is fetched.

    Examples
    --------
    >>> from bowtie import cache
    >>> cache['a'] = True  # doctest: +SKIP
    >>> cache['a']  # doctest: +SKIP
    True
    >>> cache['b'] = np.arange(5)  # doctest: +SKIP
    >>> cache['b']  # doctest: +SKIP
    [1, 2, 3, 4, 5]
    """"""

    def __init__(self):
        # Fallback storage for non-browser environments
        self._fallback = {}

    def __getitem__(self, key):
        key = str(key)
        if _HAS_JS:
            raw = js.sessionStorage.getItem(key)
            if raw is None:
                raise KeyError(key)
            return json.loads(raw)
        else:
            if key not in self._fallback:
                raise KeyError(key)
            return self._fallback[key]

    def __setitem__(self, key, value):
        key = str(key)
        # Convert NumPy arrays (or any object with tolist) to plain lists
        if hasattr(value, ""tolist""):
            value = value.tolist()
        serialized = json.dumps(value)
        if _HAS_JS:
            js.sessionStorage.setItem(key, serialized)
        else:
            self._fallback[key] = value
```"
159496,Erotemic/ubelt,Erotemic_ubelt/ubelt/util_mixins.py,ubelt.util_mixins.NiceRepr,"import warnings

class NiceRepr:
    """"""
    Inherit from this class and define ``__nice__`` to ""nicely"" print your
    objects.

    Defines ``__str__`` and ``__repr__`` in terms of ``__nice__`` function
    Classes that inherit from :class:`NiceRepr` should redefine ``__nice__``.
    If the inheriting class has a ``__len__``, method then the default
    ``__nice__`` method will return its length.

    Example:
        >>> import ubelt as ub
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        return 'info'
        >>> foo = Foo()
        >>> assert str(foo) == '<Foo(info)>'
        >>> assert repr(foo).startswith('<Foo(info) at ')

    Example:
        >>> import ubelt as ub
        >>> class Bar(ub.NiceRepr):
        ...    pass
        >>> bar = Bar()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     assert 'object at' in str(bar)
        >>>     assert 'object at' in repr(bar)

    Example:
        >>> import ubelt as ub
        >>> class Baz(ub.NiceRepr):
        ...    def __len__(self):
        ...        return 5
        >>> baz = Baz()
        >>> assert str(baz) == '<Baz(5)>'

    Example:
        >>> import ubelt as ub
        >>> # If your nice message has a bug, it shouldn't bring down the house
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        assert False
        >>> foo = Foo()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     print('foo = {!r}'.format(foo))
        foo = <...Foo ...>

    Example:
        >>> import ubelt as ub
        >>> class Animal(ub.NiceRepr):
        ...    def __init__(self):
        ...        ...
        ...    def __nice__(self):
        ...        return ''
        >>> class Cat(Animal):
        >>>     ...
        >>> class Dog(Animal):
        >>>     ...
        >>> class Beagle(Dog):
        >>>     ...
        >>> class Ragdoll(Cat):
        >>>     ...
        >>> instances = [Animal(), Cat(), Dog(), Beagle(), Ragdoll()]
        >>> for inst in instances:
        >>>     print(str(inst))
        <Animal()>
        <Cat()>
        <Dog()>
        <Beagle()>
        <Ragdoll()>

    In the case where you cant or dont want to use ubelt.NiceRepr you can get
    similar behavior by pasting the methods from the following snippet into
    your class:

    .. code:: python

        class MyClass:

            def __nice__(self):
                return 'your concise information'

            def __repr__(self):
                nice = self.__nice__()
                classname = self.__class__.__name__
                return '<{0}({1}) at {2}>'.format(classname, nice, hex(id(self)))

            def __str__(self):
                classname = self.__class__.__name__
                nice = self.__nice__()
                return '<{0}({1})>'.format(classname, nice)
    """"""

    def __nice__(self):
        """"""
        Returns:
            str
        """"""
        if hasattr(self, '__len__'):
            return str(len(self))
        else:
            raise NotImplementedError('Define the __nice__ method for {!r}'.format(self.__class__))

    def __repr__(self):
        """"""
        Returns:
            str
        """"""
        try:
            nice = self.__nice__()
            classname = self.__class__.__name__
            return '<{0}({1}) at {2}>'.format(classname, nice, hex(id(self)))
        except Exception as ex:
            warnings.warn(str(ex), category=RuntimeWarning)
            return object.__repr__(self)

    def __str__(self):
        """"""
        Returns:
            str
        """"""
        try:
            classname = self.__class__.__name__
            nice = self.__nice__()
            return '<{0}({1})>'.format(classname, nice)
        except Exception as ex:
            warnings.warn(str(ex), category=RuntimeWarning)
            return object.__repr__(self)","
class NiceRepr:
    '''
    Inherit from this class and define ``__nice__`` to ""nicely"" print your
    objects.
    Defines ``__str__`` and ``__repr__`` in terms of ``__nice__`` function
    Classes that inherit from :class:`NiceRepr` should redefine ``__nice__``.
    If the inheriting class has a ``__len__``, method then the default
    ``__nice__`` method will return its length.
    Example:
        >>> import ubelt as ub
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        return 'info'
        >>> foo = Foo()
        >>> assert str(foo) == '<Foo(info)>'
        >>> assert repr(foo).startswith('<Foo(info) at ')
    Example:
        >>> import ubelt as ub
        >>> class Bar(ub.NiceRepr):
        ...    pass
        >>> bar = Bar()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     assert 'object at' in str(bar)
        >>>     assert 'object at' in repr(bar)
    Example:
        >>> import ubelt as ub
        >>> class Baz(ub.NiceRepr):
        ...    def __len__(self):
        ...        return 5
        >>> baz = Baz()
        >>> assert str(baz) == '<Baz(5)>'
    Example:
        >>> import ubelt as ub
        >>> # If your nice message has a bug, it shouldn't bring down the house
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        assert False
        >>> foo = Foo()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     print('foo = {!r}'.format(foo))
        foo = <...Foo ...>
    Example:
        >>> import ubelt as ub
        >>> class Animal(ub.NiceRepr):
        ...    def __init__(self):
        ...        ...
        ...    def __nice__(self):
        ...        return ''
        >>> class Cat(Animal):
        >>>     ...
        >>> class Dog(Animal):
        >>>     ...
        >>> class Beagle(Dog):
        >>>     ...
        >>> class Ragdoll(Cat):
        >>>     ...
        >>> instances = [Animal(), Cat(), Dog(), Beagle(), Ragdoll()]
        >>> for inst in instances:
        >>>     print(str(inst))
        <Animal()>
        <Cat()>
        <Dog()>
        <Beagle()>
        <Ragdoll()>
    In the case where you cant or dont want to use ubelt.NiceRepr you can get
    similar behavior by pasting the methods from the following snippet into
    your class:
    .. code:: python
        class MyClass:
            def __nice__(self):
                return 'your concise information'
            def __repr__(self):
                nice = self.__nice__()
                classname = self.__class__.__name__
                return '<{0}({1}) at {2}>'.format(classname, nice, hex(id(self)))
            def __str__(self):
                classname = self.__class__.__name__
                nice = self.__nice__()
                return '<{0}({1})>'.format(classname, nice)
                    '''
            def __nice__(self):
                
                        pass

            def __repr__(self):
                
                        pass

            def __str__(self):
                
                        pass",snippet_80,4,"```python
import warnings

class NiceRepr:
    '''
    Inherit from this class and define ``__nice__`` to ""nicely"" print your
    objects.
    Defines ``__str__`` and ``__repr__`` in terms of ``__nice__`` function
    Classes that inherit from :class:`NiceRepr` should redefine ``__nice__``.
    If the inheriting class has a ``__len__``, method then the default
    ``__nice__`` method will return its length.
    Example:
        >>> import ubelt as ub
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        return 'info'
        >>> foo = Foo()
        >>> assert str(foo) == '<Foo(info)>'
        >>> assert repr(foo).startswith('<Foo(info) at ')
    Example:
        >>> import ubelt as ub
        >>> class Bar(ub.NiceRepr):
        ...    pass
        >>> bar = Bar()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        ...     assert 'object at' in str(bar)
        ...     assert 'object at' in repr(bar)
    Example:
        >>> import ubelt as ub
        >>> class Baz(ub.NiceRepr):
        ...    def __len__(self):
        ...        return 5
        >>> baz = Baz()
        >>> assert str(baz) == '<Baz(5)>'
    Example:
        >>> import ubelt as ub
        >>> # If your nice message has a bug, it shouldn't bring down the house
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        assert False
        >>> foo = Foo()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        ...     print('foo = {!r}'.format(foo))
        foo = <...Foo ...>
    Example:
        >>> import ubelt as ub
        >>> class Animal(ub.NiceRepr):
        ...    def __init__(self):
        ...        ...
        ...    def __nice__(self):
        ...        return ''
        >>> class Cat(Animal):
        ...     ...
        >>> class Dog(Animal):
        ...     ...
        >>> class Beagle(Dog):
        ...     ...
        >>> class Ragdoll(Cat):
        ...     ...
        >>> instances = [Animal(), Cat(), Dog(),"
159497,Erotemic/ubelt,Erotemic_ubelt/ubelt/util_path.py,ubelt.util_path.ChDir,"import os

class ChDir:
    """"""
    Context manager that changes the current working directory and then
    returns you to where you were.

    This is nearly the same as the stdlib :func:`contextlib.chdir`, with the
    exception that it will do nothing if the input path is None (i.e. the user
    did not want to change directories).

    SeeAlso:
        :func:`contextlib.chdir`

    Example:
        >>> import ubelt as ub
        >>> dpath = ub.Path.appdir('ubelt/tests/chdir').ensuredir()
        >>> dir1 = (dpath / 'dir1').ensuredir()
        >>> dir2 = (dpath / 'dir2').ensuredir()
        >>> with ChDir(dpath):
        >>>     assert ub.Path.cwd() == dpath
        >>>     # change to the given directory, and then returns back
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(dir2):
        >>>             assert ub.Path.cwd() == dir2
        >>>             # changes inside the context manager will be reset
        >>>             os.chdir(dpath)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(None):
        >>>             assert ub.Path.cwd() == dir1
        >>>             # When disabled, the cwd does *not* reset at context exit
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir2
        >>>         os.chdir(dir1)
        >>>         # Dont change dirs, but reset to your cwd at context end
        >>>         with ChDir('.'):
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
    """"""

    def __init__(self, dpath):
        """"""
        Args:
            dpath (str | PathLike | None):
                The new directory to work in.
                If None, then the context manager is disabled.
        """"""
        self._context_dpath = dpath
        self._orig_dpath = None

    def __enter__(self):
        """"""
        Returns:
            ChDir: self
        """"""
        if self._context_dpath is not None:
            self._orig_dpath = os.getcwd()
            os.chdir(self._context_dpath)
        return self

    def __exit__(self, ex_type, ex_value, ex_traceback):
        """"""
        Args:
            ex_type (Type[BaseException] | None):
            ex_value (BaseException | None):
            ex_traceback (TracebackType | None):

        Returns:
            bool | None
        """"""
        if self._context_dpath is not None:
            os.chdir(self._orig_dpath)","
class ChDir:
    '''
    Context manager that changes the current working directory and then
    returns you to where you were.
    This is nearly the same as the stdlib :func:`contextlib.chdir`, with the
    exception that it will do nothing if the input path is None (i.e. the user
    did not want to change directories).
    SeeAlso:
        :func:`contextlib.chdir`
    Example:
        >>> import ubelt as ub
        >>> dpath = ub.Path.appdir('ubelt/tests/chdir').ensuredir()
        >>> dir1 = (dpath / 'dir1').ensuredir()
        >>> dir2 = (dpath / 'dir2').ensuredir()
        >>> with ChDir(dpath):
        >>>     assert ub.Path.cwd() == dpath
        >>>     # change to the given directory, and then returns back
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(dir2):
        >>>             assert ub.Path.cwd() == dir2
        >>>             # changes inside the context manager will be reset
        >>>             os.chdir(dpath)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(None):
        >>>             assert ub.Path.cwd() == dir1
        >>>             # When disabled, the cwd does *not* reset at context exit
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir2
        >>>         os.chdir(dir1)
        >>>         # Dont change dirs, but reset to your cwd at context end
        >>>         with ChDir('.'):
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
    '''
    def __init__(self, dpath):
        
        pass

    def __enter__(self):
        
        pass

    def __exit__(self, ex_type, ex_value, ex_traceback):
        
        pass",snippet_81,4,"```python
import os
from typing import Optional, Union

class ChDir:
    '''
    Context manager that changes the current working directory and then
    returns you to where you were.
    This is nearly the same as the stdlib :func:`contextlib.chdir`, with the
    exception that it will do nothing if the input path is None (i.e. the user
    did not want to change directories).
    SeeAlso:
        :func:`contextlib.chdir`
    Example:
        >>> import ubelt as ub
        >>> dpath = ub.Path.appdir('ubelt/tests/chdir').ensuredir()
        >>> dir1 = (dpath / 'dir1').ensuredir()
        >>> dir2 = (dpath / 'dir2').ensuredir()
        >>> with ChDir(dpath):
        >>>     assert ub.Path.cwd() == dpath
        >>>     # change to the given directory, and then returns back
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(dir2):
        >>>             assert ub.Path.cwd() == dir2
        >>>             # changes inside the context manager will be reset
        >>>             os.chdir(dpath)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(None):
        >>>             assert ub.Path.cwd() == dir1
        >>>             # When disabled, the cwd does *not* reset at context exit
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir2
        >>>         os.chdir(dir1)
        >>>         # Dont change dirs, but reset to your cwd at context end
        >>>         with ChDir('.'):
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
    '''
    def __init__(self, dpath: Optional[Union[str, os.PathLike]]):
        self.dpath = dpath
        self._original_cwd: Optional[str] = None

    def __enter__(self):
        if self.dpath is not None:
            # Save the current working directory
            self._original_cwd = os.getcwd()
            # Change to the new directory
            os.chdir(os.fspath(self.dpath))
        return self

    def __exit__(self, ex_type, ex_value, ex_traceback):
        # Only restore if we actually changed directories
        if self.dpath is not None and self._original_cwd is not None:
            os.chdir(self._original_cwd)
        # Returning False propagates any exception that occurred
        return False
```"
159980,pazz/alot,alot/completion/completer.py,alot.completion.completer.Completer,"import abc

class Completer:
    """"""base class for completers""""""
    __metaclass__ = abc.ABCMeta

    @abc.abstractmethod
    def complete(self, original, pos):
        """"""returns a list of completions and cursor positions for the string
        `original` from position `pos` on.

        :param original: the string to complete
        :type original: str
        :param pos: starting position to complete from
        :type pos: int
        :returns: pairs of completed string and cursor position in the
                  new string
        :rtype: list of (str, int)
        :raises: :exc:`CompletionError`
        """"""
        pass

    def relevant_part(self, original, pos):
        """"""
        Calculate the subword in a ' '-separated list of substrings of
        `original` that `pos` is in.
        """"""
        start = original.rfind(' ', 0, pos) + 1
        end = original.find(' ', pos - 1)
        if end == -1:
            end = len(original)
        return (original[start:end], start, end, pos - start)","
class Completer:
    
    @abc.abstractmethod
    def complete(self, original, pos):
        
        pass
    def relevant_part(self, original, pos):
        
        pass",snippet_82,3,"```python
import string

class Completer:
    """"""
    A simple word completer that suggests completions based on a predefined
    dictionary of words. The `complete` method returns all words that start
    with the relevant part of the input string at the given cursor position.
    """"""

    # A small sample dictionary of words for completion suggestions.
    _dictionary = [
        ""apple"", ""application"", ""apricot"", ""banana"", ""band"", ""bandana"",
        ""cherry"", ""date"", ""dragonfruit"", ""fig"", ""grape"", ""grapefruit"",
        ""kiwi"", ""lemon"", ""lime"", ""mango"", ""nectarine"", ""orange"",
        ""papaya"", ""peach"", ""pear"", ""pineapple"", ""plum"", ""raspberry"",
        ""strawberry"", ""tangerine"", ""watermelon""
    ]

    def relevant_part(self, original: str, pos: int) -> str:
        """"""
        Return the substring of `original` that is relevant for completion
        at the cursor position `pos`. This is the word fragment that starts
        at the last whitespace before `pos` (or the beginning of the string)
        and ends at `pos`.

        Parameters
        ----------
        original : str
            The full input string.
        pos : int
            The cursor position within `original`.

        Returns
        -------
        str
            The relevant word fragment.
        """"""
        if not original:
            return """"

        # Clamp pos to the bounds of the string
        pos = max(0, min(pos, len(original)))

        # Find the start of the word by scanning leftwards until a whitespace
        start = pos
        while start > 0 and not original[start - 1].isspace():
            start -= 1

        return original[start:pos]

    def complete(self, original: str, pos: int) -> list[str]:
        """"""
        Return a list of completion suggestions based on the relevant part
        of the input string at the cursor position `pos`.

        Parameters
        ----------
        original : str
            The full input string.
        pos : int
            The cursor position within `original`.

        Returns
        -------
        list[str]
            A list of words from the dictionary that start with the relevant
            part. The comparison is case-insensitive.
        """"""
        fragment = self.relevant_part(original, pos).lower()
        if not fragment:
            # If there's no fragment, return the entire dictionary
            return sorted(self._dictionary)

        # Filter dictionary for words that start with the fragment
        matches = [word for word in self._dictionary if word.lower().startswith(fragment)]
        return sorted(matches)
```"
160363,splunk/splunk-sdk-python,splunk_splunk-sdk-python/splunklib/modularinput/argument.py,splunklib.modularinput.argument.Argument,"import xml.etree.ElementTree as ET

class Argument:
    """"""Class representing an argument to a modular input kind.

    ``Argument`` is meant to be used with ``Scheme`` to generate an XML
    definition of the modular input kind that Splunk understands.

    ``name`` is the only required parameter for the constructor.

        **Example with least parameters**::

            arg1 = Argument(name=""arg1"")

        **Example with all parameters**::

            arg2 = Argument(
                name=""arg2"",
                description=""This is an argument with lots of parameters"",
                validation=""is_pos_int('some_name')"",
                data_type=Argument.data_type_number,
                required_on_edit=True,
                required_on_create=True
            )
    """"""
    data_type_boolean = 'BOOLEAN'
    data_type_number = 'NUMBER'
    data_type_string = 'STRING'

    def __init__(self, name, description=None, validation=None, data_type=data_type_string, required_on_edit=False, required_on_create=False, title=None):
        """"""
        :param name: ``string``, identifier for this argument in Splunk.
        :param description: ``string``, human-readable description of the argument.
        :param validation: ``string`` specifying how the argument should be validated, if using internal validation.
               If using external validation, this will be ignored.
        :param data_type: ``string``, data type of this field; use the class constants.
               ""data_type_boolean"", ""data_type_number"", or ""data_type_string"".
        :param required_on_edit: ``Boolean``, whether this arg is required when editing an existing modular input of this kind.
        :param required_on_create: ``Boolean``, whether this arg is required when creating a modular input of this kind.
        :param title: ``String``, a human-readable title for the argument.
        """"""
        self.name = name
        self.description = description
        self.validation = validation
        self.data_type = data_type
        self.required_on_edit = required_on_edit
        self.required_on_create = required_on_create
        self.title = title

    def add_to_document(self, parent):
        """"""Adds an ``Argument`` object to this ElementTree document.

        Adds an <arg> subelement to the parent element, typically <args>
        and sets up its subelements with their respective text.

        :param parent: An ``ET.Element`` to be the parent of a new <arg> subelement
        :returns: An ``ET.Element`` object representing this argument.
        """"""
        arg = ET.SubElement(parent, 'arg')
        arg.set('name', self.name)
        if self.title is not None:
            ET.SubElement(arg, 'title').text = self.title
        if self.description is not None:
            ET.SubElement(arg, 'description').text = self.description
        if self.validation is not None:
            ET.SubElement(arg, 'validation').text = self.validation
        subelements = [('data_type', self.data_type), ('required_on_edit', self.required_on_edit), ('required_on_create', self.required_on_create)]
        for name, value in subelements:
            ET.SubElement(arg, name).text = str(value).lower()
        return arg","
class Argument:
    '''Class representing an argument to a modular input kind.
    ``Argument`` is meant to be used with ``Scheme`` to generate an XML
    definition of the modular input kind that Splunk understands.
    ``name`` is the only required parameter for the constructor.
        **Example with least parameters**::
            arg1 = Argument(name=""arg1"")
        **Example with all parameters**::
            arg2 = Argument(
                name=""arg2"",
                description=""This is an argument with lots of parameters"",
                validation=""is_pos_int('some_name')"",
                data_type=Argument.data_type_number,
                required_on_edit=True,
                required_on_create=True
            )
    '''
    def __init__(self, name, description=None, validation=None, data_type=data_type_string, required_on_edit=False, required_on_create=False, title=None):
        
        pass

    def add_to_document(self, parent):
        
        pass",snippet_83,3,
160367,splunk/splunk-sdk-python,splunk_splunk-sdk-python/splunklib/modularinput/scheme.py,splunklib.modularinput.scheme.Scheme,"import xml.etree.ElementTree as ET

class Scheme:
    """"""Class representing the metadata for a modular input kind.

    A ``Scheme`` specifies a title, description, several options of how Splunk should run modular inputs of this
    kind, and a set of arguments which define a particular modular input's properties.

    The primary use of ``Scheme`` is to abstract away the construction of XML to feed to Splunk.
    """"""
    streaming_mode_simple = 'SIMPLE'
    streaming_mode_xml = 'XML'

    def __init__(self, title):
        """"""
        :param title: ``string`` identifier for this Scheme in Splunk.
        """"""
        self.title = title
        self.description = None
        self.use_external_validation = True
        self.use_single_instance = False
        self.streaming_mode = Scheme.streaming_mode_xml
        self.arguments = []

    def add_argument(self, arg):
        """"""Add the provided argument, ``arg``, to the ``self.arguments`` list.

        :param arg: An ``Argument`` object to add to ``self.arguments``.
        """"""
        self.arguments.append(arg)

    def to_xml(self):
        """"""Creates an ``ET.Element`` representing self, then returns it.

        :returns: an ``ET.Element`` representing this scheme.
        """"""
        root = ET.Element('scheme')
        ET.SubElement(root, 'title').text = self.title
        if self.description is not None:
            ET.SubElement(root, 'description').text = self.description
        subelements = [('use_external_validation', self.use_external_validation), ('use_single_instance', self.use_single_instance), ('streaming_mode', self.streaming_mode)]
        for name, value in subelements:
            ET.SubElement(root, name).text = str(value).lower()
        endpoint = ET.SubElement(root, 'endpoint')
        args = ET.SubElement(endpoint, 'args')
        for arg in self.arguments:
            arg.add_to_document(args)
        return root","
class Scheme:
    
    def __init__(self, title):
        
        pass
    def add_argument(self, arg):
        
        pass

    def to_xml(self):
        
        pass",snippet_84,4,"```python
import xml.etree.ElementTree as ET

class Scheme:
    def __init__(self, title):
        self.title = title
        self.arguments = []

    def add_argument(self, arg):
        """"""Add an argument to the scheme.""""""
        self.arguments.append(arg)

    def to_xml(self):
        """"""Return an XML string representation of the scheme.""""""
        root = ET.Element('scheme', attrib={'title': self.title})
        for arg in self.arguments:
            arg_el = ET.SubElement(root, 'argument')
            arg_el.text = str(arg)
        return ET.tostring(root, encoding='unicode')
```"
160991,ncclient/ncclient,ncclient_ncclient/ncclient/transport/session.py,ncclient.transport.session.SessionListener,"class SessionListener:
    """"""Base class for :class:`Session` listeners, which are notified when a new
    NETCONF message is received or an error occurs.

    .. note::
        Avoid time-intensive tasks in a callback's context.
    """"""

    def callback(self, root, raw):
        """"""Called when a new XML document is received. The *root* argument allows the callback to determine whether it wants to further process the document.

        Here, *root* is a tuple of *(tag, attributes)* where *tag* is the qualified name of the root element and *attributes* is a dictionary of its attributes (also qualified names).

        *raw* will contain the XML document as a string.
        """"""
        raise NotImplementedError

    def errback(self, ex):
        """"""Called when an error occurs.

        :type ex: :exc:`Exception`
        """"""
        raise NotImplementedError","class SessionListener:
    
    def callback(self, root, raw):
        
        pass
    def errback(self, ex):
        
        pass",snippet_85,3,"class SessionListener:
    def __init__(self):
        self.last_root = None
        self.last_raw = None
        self.last_error = None

    def callback(self, root, raw):
        self.last_root = root
        self.last_raw = raw

    def errback(self, ex):
        self.last_error = ex"
163149,python-cmd2/cmd2,python-cmd2_cmd2/cmd2/argparse_custom.py,cmd2.argparse_custom.Cmd2AttributeWrapper,"from typing import TYPE_CHECKING, Any, ClassVar, NoReturn, Protocol, cast, runtime_checkable

class Cmd2AttributeWrapper:
    """"""Wraps a cmd2-specific attribute added to an argparse Namespace.

    This makes it easy to know which attributes in a Namespace are
    arguments from a parser and which were added by cmd2.
    """"""

    def __init__(self, attribute: Any) -> None:
        """"""Initialize Cmd2AttributeWrapper instances.""""""
        self.__attribute = attribute

    def get(self) -> Any:
        """"""Get the value of the attribute.""""""
        return self.__attribute

    def set(self, new_val: Any) -> None:
        """"""Set the value of the attribute.""""""
        self.__attribute = new_val","
class Cmd2AttributeWrapper:
    
    def __init__(self, attribute: Any) -> None:
        
        pass
    def get(self) -> Any:
        
        pass

    def set(self, new_val: Any) -> None:
        
        pass",snippet_86,4,"```python
from typing import Any


class Cmd2AttributeWrapper:
    """"""
    A simple wrapper around an attribute value that allows getting and setting
    the value through dedicated methods.
    """"""

    def __init__(self, attribute: Any) -> None:
        """"""
        Initialize the wrapper with the given attribute value.

        :param attribute: The initial value to wrap.
        """"""
        self._attribute = attribute

    def get(self) -> Any:
        """"""
        Retrieve the current value of the wrapped attribute.

        :return: The current attribute value.
        """"""
        return self._attribute

    def set(self, new_val: Any) -> None:
        """"""
        Update the wrapped attribute with a new value.

        :param new_val: The new value to assign to the attribute.
        """"""
        self._attribute = new_val
```"
163184,python-cmd2/cmd2,python-cmd2_cmd2/cmd2/utils.py,cmd2.utils.ContextFlag,"class ContextFlag:
    """"""A context manager which is also used as a boolean flag value within the default sigint handler.

    Its main use is as a flag to prevent the SIGINT handler in cmd2 from raising a KeyboardInterrupt
    while a critical code section has set the flag to True. Because signal handling is always done on the
    main thread, this class is not thread-safe since there is no need.
    """"""

    def __init__(self) -> None:
        """"""When this flag has a positive value, it is considered set. When it is 0, it is not set.

        It should never go below 0.
        """"""
        self.__count = 0

    def __bool__(self) -> bool:
        """"""Define the truth value of an object when it is used in a boolean context.""""""
        return self.__count > 0

    def __enter__(self) -> None:
        """"""When a with block is entered, the __enter__ method of the context manager is called.""""""
        self.__count += 1

    def __exit__(self, *args: object) -> None:
        """"""When the execution flow exits a with statement block this is called, regardless of whether an exception occurred.""""""
        self.__count -= 1
        if self.__count < 0:
            raise ValueError('count has gone below 0')","class ContextFlag:
    
    def __init__(self) -> None:
        
        pass
    def __bool__(self) -> bool:
        
        pass
    def __enter__(self) -> None:
        
        pass

    def __exit__(self, *args: object) -> None:
        
        pass",snippet_87,5,"class ContextFlag:
    def __init__(self) -> None:
        self._counter = 0

    def __bool__(self) -> bool:
        return self._counter > 0

    def __enter__(self) -> None:
        self._counter += 1
        return None

    def __exit__(self, *args: object) -> None:
        if self._counter > 0:
            self._counter -= 1
        else:
            self._counter = 0
        return None
"
166548,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/cache/cache.py,pypyr.cache.cache.Cache,"import threading
from pypyr.config import config

class Cache:
    """"""Thread-safe general purpose cache for objects.

    Add things to the cache by calling get(key, creator). If the requested key
    doesn't exist, will add the item to the cache for you.
    """"""

    def __init__(self):
        """"""Instantiate the cache.""""""
        self._lock = threading.Lock()
        self._cache = {}

    def clear(self):
        """"""Clear the cache of all objects.""""""
        with self._lock:
            self._cache.clear()

    def get(self, key, creator):
        """"""Get key from cache. If key not exist, call creator and cache result.

        Looks for key in cache and returns object for that key.

        If key is not found, call creator and save the result to cache for that
        key.

        Be warned that get happens under the context of a Lock. . . so if
        creator takes a long time you might well be blocking.

        If config no_cache is True, bypasses cache entirely - will call
        creator each time and also not save the result to cache.

        Args:
            key: key (unique id) of cached item
            creator: callable that will create cached object if key not found

        Returns:
            Cached item at key or the result of creator()
        """"""
        if config.no_cache:
            logger.debug('no cache mode enabled. creating `%s` sans cache', key)
            return creator()
        with self._lock:
            if key in self._cache:
                logger.debug('`%s` loading from cache', key)
                obj = self._cache[key]
            else:
                logger.debug('`%s` not found in cache. . . creating', key)
                obj = creator()
                self._cache[key] = obj
        return obj","
class Cache:
    
    def __init__(self):
        
        pass
    def clear(self):
        
        pass

    def get(self, key, creator):
        
        pass",snippet_88,4,"class Cache:
    def __init__(self):
        self._store = {}

    def clear(self):
        self._store.clear()

    def get(self, key, creator):
        if key not in self._store:
            self._store[key] = creator()
        return self._store[key]"
166549,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/cache/loadercache.py,pypyr.cache.loadercache.Loader,"from pypyr.pipedef import PipelineDefinition, PipelineInfo
from pypyr.cache.cache import Cache
from collections.abc import Mapping
from pypyr.errors import PipelineDefinitionError

class Loader:
    """"""A single pipeline loader & the cache for all pipelines it has loaded.

    It loads pipelines using the get_pipeline_definition you assign to the
    loader at initialization.

    Attributes:
        name (str): Absolute module name of loader.
    """"""
    __slots__ = ['name', '_get_pipeline_definition', '_pipeline_cache']

    def __init__(self, name, get_pipeline_definition):
        """"""Initialize the loader and its pipeline cache.

        The expected function signature is:
        get_pipeline_definition(name: str,
                                parent: any) -> PipelineDefinition | Mapping

        Args:
            name: Absolute name of loader
            get_pipeline_definition: Reference to the function to call when
                loading a pipeline with this Loader.
        """"""
        self.name = name
        self._get_pipeline_definition = get_pipeline_definition
        self._pipeline_cache = Cache()

    def clear(self):
        """"""Clear all the pipelines in this Loader's cache.""""""
        self._pipeline_cache.clear()

    def get_pipeline(self, name, parent):
        """"""Get cached PipelineDefinition. Adds it to cache if it doesn't exist.

        The cache is local to this Loader instance.

        The combination of parent+name must be unique for this Loader. Parent
        should therefore have a sensible __str__ implementation because it
        forms part of the pipeline's identifying str key in the cache.

        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.

        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        """"""
        normalized_name = f'{parent}+{name}' if parent else name
        return self._pipeline_cache.get(normalized_name, lambda: self._load_pipeline(name, parent))

    def _load_pipeline(self, name, parent):
        """"""Execute get_pipeline_definition(name, parent) for this loader.

        If the loader get_pipeline_definition does not return a
        PipelineDefinition, this method will wrap the payload inside a
        PipelineDefinition for you.

        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.

        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        """"""
        logger.debug('starting')
        logger.debug('loading the pipeline definition with %s', self.name)
        pipeline_definition = self._get_pipeline_definition(pipeline_name=name, parent=parent)
        if not isinstance(pipeline_definition, PipelineDefinition):
            pipeline_definition = PipelineDefinition(pipeline=pipeline_definition, info=PipelineInfo(pipeline_name=name, loader=self.name, parent=parent))
        if not isinstance(pipeline_definition.pipeline, Mapping):
            raise PipelineDefinitionError(""A pipeline must be a mapping at the top level. Does your top-level yaml have a 'steps:' key? For example:\n\nsteps:\n  - name: pypyr.steps.echo\n    in:\n      echoMe: this is a bare bones pipeline example.\n"")
        logger.debug('done')
        return pipeline_definition","
class Loader:
    
    def __init__(self, name, get_pipeline_definition):
        
        pass
    def clear(self):
        
        pass

    def get_pipeline(self, name, parent):
        
        pass

    def _load_pipeline(self, name, parent):
        
        pass",snippet_89,5,"```python
class Loader:
    """"""
    A simple loader that retrieves pipeline definitions via a user-provided
    callback and builds a lightweight pipeline representation.  The loader
    caches pipelines to avoid repeated lookups and supports a parent-child
    relationship between pipelines.
    """"""

    def __init__(self, name, get_pipeline_definition):
        """"""
        Parameters
        ----------
        name : str
            The name of the loader instance (used for identification only).
        get_pipeline_definition : Callable[[str], Any]
            A callable that accepts a pipeline name and returns its
            definition (e.g., a dict or any serializable object).  It must
            raise an exception if the pipeline cannot be found.
        """"""
        self.name = name
        self._get_pipeline_definition = get_pipeline_definition
        self._cache = {}

    def clear(self):
        """"""
        Clears the internal cache of loaded pipelines.
        """"""
        self._cache.clear()

    def get_pipeline(self, name, parent=None):
        """"""
        Retrieve a pipeline by name, loading it if necessary.

        Parameters
        ----------
        name : str
            The name of the pipeline to retrieve.
        parent : dict, optional
            A parent pipeline dict to which this pipeline will be attached
            as a child.  If omitted, the pipeline is considered a root.

        Returns
        -------
        dict
            A dictionary representing the pipeline, containing at least the
            keys 'name', 'definition', 'parent', and 'children'.
        """"""
        if name in self._cache:
            pipeline = self._cache[name]
            # If a parent is supplied and differs, update the relationship.
            if parent is not None and pipeline.get('parent') is not parent:
                # Remove from old parent if present
                old_parent = pipeline.get('parent')
                if old_parent and pipeline in old_parent.get('children', []):
                    old_parent['children'].remove(pipeline)
                # Attach to new parent
                pipeline['parent'] = parent
                parent.setdefault('children', []).append(pipeline)
            return pipeline

        return self._load_pipeline(name, parent)

    def _load_pipeline(self, name, parent=None):
        """"""
        Internal helper to load a pipeline definition and construct its
        representation.

        Parameters
        ----------
        name : str
            The name of the pipeline to load.
        parent : dict, optional
            The parent pipeline dict, if any.

        Returns
        -------
        dict
            The constructed pipeline representation.
        """"""
        definition = self._get_pipeline_definition(name)
        if definition is None:
            raise ValueError(f""Pipeline definition for '{name}' not found."")

        pipeline = {
            'name': name,
            'definition': definition,
            'parent': parent,
            'children': []
        }

        if parent is not None:
            parent.setdefault('children', []).append(pipeline)

        self._cache[name] = pipeline
        return pipeline
```"
166587,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/pipedef.py,pypyr.pipedef.PipelineDefinition,"class PipelineDefinition:
    """"""The pipeline body and its metadata.

    A loader creates the PipelineDefinition and sets the metadata in .info.

    The PipelineDefinition is a globally shared cache of the pipeline body &
    meta-data.

    Attributes:
        pipeline (dict-like): The pipeline yaml body.
        info (PipelineInfo): Meta-data set by the loader for the pipeline.
    """"""
    __slots__ = ['pipeline', 'info']

    def __init__(self, pipeline, info):
        """"""Initialize a pipeline definition.

        Args:
            pipeline (dict-like): The pipeline yaml body itself.
            info (PipelineInfo): Meta-data set by the loader for the pipeline.
        """"""
        self.pipeline = pipeline
        self.info = info

    def __eq__(self, other):
        """"""Equality comparison checks Pipeline and info objects are equal.""""""
        type_self = type(self)
        if type_self is type(other):
            all_slots = [p for c in type_self.__mro__ for p in getattr(c, '__slots__', [])]
            return all((getattr(self, s, id(self)) == getattr(other, s, id(other)) for s in all_slots))
        else:
            return False","class PipelineDefinition:
    
    def __init__(self, pipeline, info):
        
        pass
    def __eq__(self, other):
        
        pass",snippet_92,3,"class PipelineDefinition:
    
    def __init__(self, pipeline, info):
        self.pipeline = pipeline
        self.info = info

    def __eq__(self, other):
        if not isinstance(other, PipelineDefinition):
            return False
        return self.pipeline == other.pipeline and self.info == other.info"
166589,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/pipedef.py,pypyr.pipedef.PipelineInfo,"class PipelineInfo:
    """"""The common attributes that every pipeline loader should set.

    Custom loaders that want to add more properties to a pipeline's meta-data
    should probably derive from this class.

    Attributes:
        pipeline_name (str): Name of pipeline, as set by the loader.
        loader (str): Absolute module name of the pipeline loader.
        parent (any): pipeline_name resolves from parent. The parent can be any
            type - it is up to the loader to interpret the parent property.
        is_loader_cascading (bool): Loader cascades to child pipelines if not
            otherwise set on pype. Default True.
        is_parent_cascading (bool): Parent cascades to child pipelines if not
            otherwise set on pype. Default True.
    """"""
    __slots__ = ['pipeline_name', 'loader', 'parent', 'is_loader_cascading', 'is_parent_cascading']

    def __init__(self, pipeline_name, loader, parent, is_parent_cascading=True, is_loader_cascading=True):
        """"""Initialize PipelineInfo.

        Args:
            pipeline_name (str): name of pipeline, as set by the loader.
            loader (str): absolute module name of pypeloader.
            parent (any): pipeline_name resolves from parent.
            is_loader_cascading (bool): Loader cascades to child pipelines if
                not otherwise set on pype. Default True.
            is_parent_cascading (bool): Parent cascades to child pipelines if
                not otherwise set on pype. Default True.
        """"""
        self.pipeline_name = pipeline_name
        self.loader = loader
        self.parent = parent
        self.is_loader_cascading = is_loader_cascading
        self.is_parent_cascading = is_parent_cascading

    def __eq__(self, other):
        """"""Check all instance attributes are equal.""""""
        type_self = type(self)
        if type_self is type(other):
            all_slots = [p for c in type_self.__mro__ for p in getattr(c, '__slots__', [])]
            return all((getattr(self, s, id(self)) == getattr(other, s, id(other)) for s in all_slots))
        else:
            return False","class PipelineInfo:
    
    def __init__(self, pipeline_name, loader, parent, is_parent_cascading=True, is_loader_cascading=True):
        
        pass
    def __eq__(self, other):
        
        pass",snippet_93,3,"class PipelineInfo:
    def __init__(self, pipeline_name, loader, parent, is_parent_cascading=True, is_loader_cascading=True):
        self.pipeline_name = pipeline_name
        self.loader = loader
        self.parent = parent
        self.is_parent_cascading = is_parent_cascading
        self.is_loader_cascading = is_loader_cascading

    def __eq__(self, other):
        if not isinstance(other, PipelineInfo):
            return False
        return (
            self.pipeline_name == other.pipeline_name
            and self.loader == other.loader
            and self.parent == other.parent
            and self.is_parent_cascading == other.is_parent_cascading
            and self.is_loader_cascading == other.is_loader_cascading
        )"
166604,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/steps/dsl/cmd.py,pypyr.steps.dsl.cmd.CmdStep,"import pypyr.utils.types
from pypyr.errors import ContextError, KeyInContextHasNoValueError, KeyNotInContextError
from collections.abc import Mapping, Sequence
from pypyr.context import Context
import logging
from pypyr.subproc import Command, SimpleCommandTypes

class CmdStep:
    """"""A pypyr step to run an executable or command as a subprocess.

    This models a step that takes config like this:
        cmd: <<cmd string>>

    OR, expanded syntax is as a dict
        cmd:
            run: str. mandatory. command + args to execute.
            save: bool. defaults False. save output to cmdOut. Treats output
                as text in the system's encoding and removes newlines at end.
            cwd: str/Pathlike. optional. Working directory for this command.
            bytes (bool): Default False. When `save` return output bytes from
                cmd unaltered, without applying any encoding & text newline
                processing.
            encoding (str): Default None. When `save`, decode cmd output with
                this encoding. The default of None uses the system encoding and
                should ""just work"".
            stdout (str | Path): Default None. Write stdout to this file path.
                Special value `/dev/null` writes to the system null device.
            stderr (str | Path): Default None. Write stderr to this file path.
                Special value `/dev/null` writes to the system null device.
                Special value `/dev/stdout` redirects err output to stdout.
            append (bool): Default False. When stdout/stderr a file, append
                rather than overwrite. Default is to overwrite.

    In expanded syntax, `run` can be a simple string or a list:
        cmd:
          run:
            - my-executable --arg
            - cmd here
          save: False
          cwd: ./path/here

    OR, as a list in simplified syntax:
        cmd:
          - my-executable --arg
          - ./another-executable --arg

    Any or all of the list items can use expanded syntax:
        cmd:
          - ./simple-cmd-here --arg1 value
          - run: cmd here
            save: False
            cwd: ./path/here
          - run:
              - my-executable --arg
              - ./another-executable --arg
            save: True cwd: ./path/here

    If save is True, will save the output to context as follows:
        cmdOut:
            returncode: 0
            stdout: 'stdout str here. None if empty.'
            stderr: 'stderr str here. None if empty.'

    If the cmd input contains a list of executables, cmdOut will be a list of
    cmdOut objects, in order executed.

    cmdOut.returncode is the exit status of the called process. Typically 0
    means OK. A negative value -N indicates that the child was terminated by
    signal N (POSIX only).

    The run_step method does the actual work. init parses the input yaml.

    Attributes:
        logger (logger): Logger instantiated by name of calling step.
        context: (pypyr.context.Context): The current pypyr Context.
        commands (list[pypyr.subproc.Command]): Commands to run as subprocess.
        is_shell (bool): True if subprocess should run through default shell.
        name (str): Name of calling step. Used for logging output & error
            messages.
    """"""

    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        """"""Initialize the CmdStep.

        The step config in the context dict in simplified syntax:
            cmd: <<cmd string>>

        OR, as a dict in expanded syntax:
            cmd:
                run: str. mandatory. command + args to execute.
                save: bool. optional. defaults False. save output to cmdOut.
                cwd: str/path. optional. if specified, change the working
                     directory just for the duration of the command.

        `run` can be a single string, or it can be a list of string if there
        are multiple commands to execute with the same settings.

        OR, as a list:
            cmd:
                - my-executable --arg
                - ./another-executable --arg

        Any or all of the list items can be in expanded syntax.

        Args:
            name (str): Unique name for step. Likely __name__ of calling step.
            context (pypyr.context.Context): Look for step config in this
                context instance.
            is_shell (bool): Set to true to execute cmd through the default
                shell.
        """"""
        assert name, 'name parameter must exist for CmdStep.'
        assert context, 'context param must exist for CmdStep.'
        self.name = name
        self.logger = logging.getLogger(name)
        context.assert_key_has_value(key='cmd', caller=name)
        self.context = context
        self.is_shell = is_shell
        cmd_config = context.get_formatted('cmd')
        commands: list[Command] = []
        if isinstance(cmd_config, SimpleCommandTypes):
            commands.append(Command(cmd_config, is_shell=is_shell))
        elif isinstance(cmd_config, Mapping):
            commands.append(self.create_command(cmd_config))
        elif isinstance(cmd_config, Sequence):
            for cmd in cmd_config:
                if isinstance(cmd, SimpleCommandTypes):
                    commands.append(Command(cmd, is_shell=is_shell))
                elif isinstance(cmd, Mapping):
                    commands.append(self.create_command(cmd))
                else:
                    raise ContextError(f'{cmd} in {name} cmd config is wrong.\nEach list item should be either a simple string or a dict for expanded syntax:\ncmd:\n  - my-executable --arg\n  - run: another-executable --arg value\n    cwd: ../mydir/subdir\n  - run:\n      - arb-executable1 --arg value1\n      - arb-executable2 --arg value2\n    cwd: ../mydir/arbdir')
        else:
            raise ContextError(f'{name} cmd config should be either a simple string:\ncmd: my-executable --arg\n\nor a dictionary:\ncmd:\n  run: subdir/my-executable --arg\n  cwd: ./mydir\n\nor a list of commands:\ncmd:\n  - my-executable --arg\n  - run: another-executable --arg value\n    cwd: ../mydir/subdir')
        self.commands: list[Command] = commands

    def create_command(self, cmd_input: Mapping) -> Command:
        """"""Create a pypyr.subproc.Command object from expanded step input.""""""
        try:
            cmd = cmd_input['run']
            if not cmd:
                raise KeyInContextHasNoValueError(f'cmd.run must have a value for {self.name}.\nThe `run` input should look something like this:\ncmd:\n  run: my-executable-here --arg1\n  cwd: ./mydir/subdir\n\nOr, `run` could be a list of commands:\ncmd:\n  run:\n    - arb-executable1 --arg value1\n    - arb-executable2 --arg value2\n  cwd: ../mydir/arbdir')
        except KeyError as err:
            raise KeyNotInContextError(f""cmd.run doesn't exist for {self.name}.\nThe input should look like this in the simplified syntax:\ncmd: my-executable-here --arg1\n\nOr in the expanded syntax:\ncmd:\n  run: my-executable-here --arg1\n\nIf you're passing in a list of commands, each command should be a simple string,\nor a dict with a `run` entry:\ncmd:\n  - my-executable --arg\n  - run: another-executable --arg value\n    cwd: ../mydir/subdir\n  - run:\n      - arb-executable1 --arg value1\n      - arb-executable2 --arg value2\n    cwd: ../mydir/arbdir"") from err
        is_save = pypyr.utils.types.cast_to_bool(cmd_input.get('save', False))
        cwd = cmd_input.get('cwd')
        is_bytes = cmd_input.get('bytes')
        is_text = not is_bytes if is_save else False
        stdout = cmd_input.get('stdout')
        stderr = cmd_input.get('stderr')
        if is_save:
            if stderr or stderr:
                raise ContextError(""You can't set `stdout` or `stderr` when `save` is True."")
        encoding = cmd_input.get('encoding')
        append = cmd_input.get('append', False)
        is_shell_override = cmd_input.get('shell', None)
        is_shell = self.is_shell if is_shell_override is None else is_shell_override
        return Command(cmd=cmd, is_shell=is_shell, cwd=cwd, is_save=is_save, is_text=is_text, stdout=stdout, stderr=stderr, encoding=encoding, append=append)

    def run_step(self) -> None:
        """"""Spawn a subprocess to run the command or program.

        If cmd.is_save==True, save result of each command to context 'cmdOut'.
        """"""
        results = []
        try:
            for cmd in self.commands:
                try:
                    cmd.run()
                finally:
                    if cmd.results:
                        results.extend(cmd.results)
        finally:
            if results:
                if len(results) == 1:
                    self.context['cmdOut'] = results[0]
                else:
                    self.context['cmdOut'] = results","
class CmdStep:
    
    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        
        pass
    def create_command(self, cmd_input: Mapping) -> Command:
        
        pass

    def run_step(self) -> None:
        
        pass",snippet_94,4,"```python
from typing import Mapping

class CmdStep:
    def __init__(self, name: str, context: 'Context', is_shell: bool = False) -> None:
        self.name = name
        self.context = context
        self.is_shell = is_shell

    def create_command(self, cmd_input: Mapping) -> 'Command':
        # Assume Command can be instantiated with keyword arguments from the mapping
        return Command(**cmd_input)

    def run_step(self) -> None:
        # Retrieve command input from the context if available
        cmd_input = getattr(self.context, ""cmd_input"", {})
        command = self.create_command(cmd_input)

        # Try to run the command using the context's run/execute method
        if hasattr(self.context, ""run""):
            self.context.run(command)
        elif hasattr(self.context, ""execute""):
            self.context.execute(command)
        else:
            # Fallback: try to run the command directly
            if hasattr(command, ""run""):
                command.run()
            else:
                raise RuntimeError(""No method available to run the command"")
```"
166605,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/steps/dsl/cmdasync.py,pypyr.steps.dsl.cmdasync.AsyncCmdStep,"import logging
from pypyr.aio.subproc import Command, Commands
from pypyr.context import Context
import pypyr.utils.types
from collections.abc import Mapping, Sequence
from pypyr.errors import ContextError, KeyInContextHasNoValueError, KeyNotInContextError
from pypyr.subproc import SimpleCommandTypes

class AsyncCmdStep:
    """"""A pypyr step to run executables/commands concurrently as a subprocess.

    This models a step that takes config like this in simple syntax:
        cmds:
            - <<cmd string 1>>
            - <<cmd string 2>>

    All the commands will run concurrently, in parallel.

    OR, expanded syntax is as a dict
        cmds:
            run: list[str | list[str]]. mandatory. command + args to execute.
                If list entry is another list[str], the sub-list will run in
                serial.
            save: bool. defaults False. save output to cmdOut. Treats output
                as text in the system's encoding and removes newlines at end.
            cwd: str/Pathlike. optional. Working directory for these commands.
            bytes (bool): Default False. When `save` return output bytes from
                cmds unaltered, without applying any encoding & text newline
                processing.
            encoding (str): Default None. When `save`, decode output with
                this encoding. The default of None uses the system encoding and
                should ""just work"".
            stdout (str | Path): Default None. Write stdout to this file path.
                Special value `/dev/null` writes to the system null device.
            stderr (str | Path): Default None. Write stderr to this file path.
                Special value `/dev/null` writes to the system null device.
                Special value `/dev/stdout` redirects err output to stdout.
            append (bool): Default False. When stdout/stderr a file, append
                rather than overwrite. Default is to overwrite.

    In expanded syntax, `run` can be a simple string or a list:
        cmds:
          run:
            - ./my-executable --arg
            - [./another-executable --arg, ./arb-executable arghere]
          save: False
          cwd: ./path/here

    As a list in simplified syntax:
        cmds:
          - my-executable --arg
          - ./another-executable --arg

    Any or all of the list items can use expanded syntax:
        cmds:
          - ./simple-cmd-here --arg1 value
          - run: cmd here
            save: False cwd: ./path/here
          - run:
              - my-executable --arg
              - ./another-executable --arg
            save: True
            cwd: ./path/here

    Any of the list items can in turn be a list. A sub-list will run in serial.

    In this example A, B.1 & C will start concurrently. B.2 will only run once
    B.1 is finished.

        cmds:
            - A
            - [B.1, B.2]
            - C

    If save is True, will save the output to context as cmdOut.

    cmdOut will be a list of pypyr.subproc.SubprocessResult objects, in order
    executed.

    SubprocessResult has the following properties:
    cmd: the cmd/args executed
    returncode: 0
    stdout: 'stdout str here. None if empty.'
    stderr: 'stderr str here. None if empty.'

    cmdOut.returncode is the exit status of the called process. Typically 0
    means OK. A negative value -N indicates that the child was terminated by
    signal N (POSIX only).

    The run_step method does the actual work. init parses the input yaml.

    Attributes:
        logger (logger): Logger instantiated by name of calling step.
        context: (pypyr.context.Context): The current pypyr Context.
        commands (pypyr.subproc.Commands): Commands to run as subprocess.
        is_shell (bool): True if subprocess should run through default shell.
        name (str): Name of calling step. Used for logging output & error
            messages.
    """"""

    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        """"""Initialize the CmdStep.

        The step config in the context dict in simplified syntax:
            cmd: <<cmd string>>

        OR, as a dict in expanded syntax:
            cmd:
                run: str. mandatory. command + args to execute.
                save: bool. optional. defaults False. save output to cmdOut.
                cwd: str/path. optional. if specified, change the working
                     directory just for the duration of the command.

        `run` can be a single string, or it can be a list of string if there
        are multiple commands to execute with the same settings.

        OR, as a list:
            cmd:
                - my-executable --arg
                - ./another-executable --arg

        Any or all of the list items can be in expanded syntax.

        Args:
            name (str): Unique name for step. Likely __name__ of calling step.
            context (pypyr.context.Context): Look for step config in this
                context instance.
            is_shell (bool): Set to true to execute cmd through the default
                shell.
        """"""
        assert name, 'name parameter must exist for CmdStep.'
        assert context, 'context param must exist for CmdStep.'
        self.name = name
        self.logger = logging.getLogger(name)
        context.assert_key_has_value(key='cmds', caller=name)
        self.context = context
        self.is_shell = is_shell
        cmd_config = context.get_formatted('cmds')
        commands = Commands()
        if isinstance(cmd_config, SimpleCommandTypes):
            commands.append(Command(cmd_config, is_shell=is_shell))
        elif isinstance(cmd_config, Mapping):
            commands.append(self.create_command(cmd_config))
        elif isinstance(cmd_config, Sequence):
            for cmd in cmd_config:
                if isinstance(cmd, SimpleCommandTypes):
                    commands.append(Command(cmd, is_shell=is_shell))
                elif isinstance(cmd, Sequence):
                    commands.append(Command([cmd], is_shell=is_shell))
                elif isinstance(cmd, Mapping):
                    commands.append(self.create_command(cmd))
                else:
                    raise ContextError(f'{cmd} in {name} cmds config is wrong.\nEach list item should be either a simple string, or a list to run in serial,\nor a dict for expanded syntax:\ncmds:\n  - ./my-executable --arg\n  - run:\n      - ./another-executable --arg value\n      - ./another-executable --arg value2\n    cwd: ../mydir/subdir\n  - run:\n      - ./arb-executable1 --arg value1\n      - [./arb-executable2.1, ./arb-executable2.2]\n    cwd: ../mydir/arbdir\n  - [./arb-executable3.1, ./arb-executable3.2]')
        else:
            raise ContextError(f'{name} cmds config should be either a list:\ncmds:\n  - ./my-executable --arg\n  - subdir/executable --arg1\n\nor a dictionary with a `run` sub-key:\ncmds:\n  run:\n    - ./my-executable --arg\n    - subdir/executable --arg1\n  cwd: ./mydir\n\nAny of the list items in root can be in expanded syntax:\ncmds:\n  - ./my-executable --arg\n  - subdir/executable --arg1\n  - run:\n      - ./arb-executable1 --arg value1\n      - [./arb-executable2.1, ./arb-executable2.2]\n    cwd: ../mydir/subdir\n  - [./arb-executable3.1, ./arb-executable3.2]')
        self.commands: Commands = commands

    def create_command(self, cmd_input: Mapping) -> Command:
        """"""Create pypyr.aio.subproc.Command object from expanded step input.""""""
        try:
            cmd = cmd_input['run']
            if not cmd:
                raise KeyInContextHasNoValueError(f'cmds.run must have a value for {self.name}.\nThe `run` input should look something like this:\ncmds:\n  run:\n    - ./arb-executable1 --arg value1\n    - ./arb-executable2 --arg value2\n  cwd: ../mydir/arbdir')
        except KeyError as err:
            raise KeyNotInContextError(f""cmds.run doesn't exist for {self.name}.\nThe input should look like this in expanded syntax:\ncmds:\n  run:\n    - ./my-executable --arg\n    - subdir/executable --arg1\n  cwd: ./mydir\n\nIf you're passing in a list of commands, each command should be a simple string,\nor a sub-list of commands to run in serial,\nor a dict with a `run` entry:\ncmds:\n  - ./my-executable --arg\n  - run: ./another-executable --arg value\n    cwd: ../mydir/subdir\n  - run:\n      - ./arb-executable1 --arg value1\n      - [./arb-executable2.1, ./arb-executable2.2]\n    cwd: ../mydir/arbdir\n  - [./arb-executable3.1, ./arb-executable3.2]"") from err
        is_save = pypyr.utils.types.cast_to_bool(cmd_input.get('save', False))
        cwd = cmd_input.get('cwd')
        is_bytes = cmd_input.get('bytes')
        is_text = not is_bytes if is_save else False
        stdout = cmd_input.get('stdout')
        stderr = cmd_input.get('stderr')
        if is_save:
            if stderr or stderr:
                raise ContextError(""You can't set `stdout` or `stderr` when `save` is True."")
        encoding = cmd_input.get('encoding')
        append = cmd_input.get('append', False)
        is_shell_override = cmd_input.get('shell', None)
        is_shell = self.is_shell if is_shell_override is None else is_shell_override
        return Command(cmd=cmd, is_shell=is_shell, cwd=cwd, is_save=is_save, is_text=is_text, stdout=stdout, stderr=stderr, encoding=encoding, append=append)

    def run_step(self) -> None:
        """"""Spawn subprocesses to run the commands asynchronously.

        If cmd.is_save==True, save aggregate result of all commands to context
        'cmdOut'.

        cmdOut will be a list of pypyr.subproc.SubprocessResult or Exception
        objects, in order executed.

        SubprocessResult has the following properties:
        cmd: the cmd/args executed
        returncode: 0
        stdout: 'stdout str here. None if empty.'
        stderr: 'stderr str here. None if empty.'
        """"""
        try:
            self.commands.run()
        finally:
            if self.commands.is_save:
                self.logger.debug('saving results to cmdOut')
                self.context['cmdOut'] = self.commands.results
            else:
                self.logger.debug('save is False: not saving results to cmdOut')","
class AsyncCmdStep:
    
    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        
        pass
    def create_command(self, cmd_input: Mapping) -> Command:
        
        pass

    def run_step(self) -> None:
        
        pass",snippet_95,4,"```python
import asyncio
import subprocess
import sys
from typing import Mapping

class AsyncCmdStep:
    def __init__(self, name: str, context: ""Context"", is_shell: bool = False) -> None:
        self.name = name
        self.context = context
        self.is_shell = is_shell

    def create_command(self, cmd_input: Mapping) -> ""Command"":
        # Extract command components from the mapping
        cmd = cmd_input.get(""cmd"") or cmd_input.get(""command"")
        args = cmd_input.get(""args"", [])
        env = cmd_input.get(""env"", {})
        cwd = cmd_input.get(""cwd"")

        # Merge environment and cwd from context if available
        if hasattr(self.context, ""env""):
            env = {**self.context.env, **env}
        if hasattr(self.context, ""cwd"") and cwd is None:
            cwd = self.context.cwd

        # Construct and return a Command instance
        return Command(cmd=cmd, args=args, env=env, cwd=cwd)

    async def _run_async(self, command: ""Command"") -> None:
        # Choose the appropriate subprocess creation method
        if self.is_shell:
            proc = await asyncio.create_subprocess_shell(
                command.cmd,
                env=command.env,
                cwd=command.cwd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
        else:
            proc = await asyncio.create_subprocess_exec(
                command.cmd,
                *command.args,
                env=command.env,
                cwd=command.cwd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )

        # Wait for the process to finish and capture output
        stdout, stderr = await proc.communicate()

        # Print stdout and stderr if present
        if stdout:
            print(stdout.decode(), end="""")
        if stderr:
            print(stderr.decode(), file=sys.stderr, end="""")

        # Raise an error if the command failed
        if proc.returncode != 0:
            raise subprocess.CalledProcessError(proc.returncode, command.cmd)

    def run_step(self) -> None:
        # Retrieve command input from the context
        cmd_input = getattr(self.context, ""cmd_input"", {})
        command = self.create_command(cmd_input)

        # Execute the command asynchronously
        asyncio.run(self._run_async(command))
```"
167547,automl/HpBandSter,automl_HpBandSter/hpbandster/optimizers/learning_curve_models/base.py,hpbandster.optimizers.learning_curve_models.base.LCModel,"class LCModel:
    """"""
        base class for simple learning curve models
    """"""

    def fit(self, times, losses, configs=None):
        """"""
            function to train the model on the observed data

            Parameters:
            -----------

            times: list
                list of numpy arrays of the timesteps for each curve
            losses: list
                list of numpy arrays of the loss (the actual learning curve)
            configs: list or None
                list of the configurations for each sample. Each element
                has to be a numpy array. Set to None, if no configuration
                information is available.
        """"""
        raise NotImplementedError()

    def predict_unseen(self, times, config):
        """"""
            predict the loss of an unseen configuration

            Parameters:
            -----------

            times: numpy array
                times where to predict the loss
            config: numpy array
                the numerical representation of the config

            Returns:
            --------

            mean and variance prediction at input times for the given config
        """"""
        raise NotImplementedError()

    def extend_partial(self, times, obs_times, obs_losses, config=None):
        """"""
            extends a partially observed curve

            Parameters:
            -----------

            times: numpy array
                times where to predict the loss
            obs_times: numpy array
                times where the curve has already been observed
            obs_losses: numpy array
                corresponding observed losses
            config: numpy array
                numerical reperesentation of the config; None if no config
                information is available

            Returns:
            --------

            mean and variance prediction at input times


        """"""","class LCModel:
    
    def fit(self, times, losses, configs=None):
        
        pass
    def predict_unseen(self, times, config):
        
        pass
    def extend_partial(self, times, obs_times, obs_losses, config=None):
        
        pass",snippet_98,4,"```python
import numpy as np

class LCModel:
    """"""
    A simple linear learning‑curve model.
    Stores a linear regression (slope & intercept) for each configuration.
    """"""

    def __init__(self):
        # mapping from config key to (slope, intercept)
        self._models = {}

    def _to_key(self, config):
        """"""Convert a config to a hashable key.""""""
        if config is None:
            return None
        # Assume config is a dict or any hashable object
        if isinstance(config, dict):
            # Sort items to ensure consistent ordering
            return tuple(sorted(config.items()))
        return config

    def _fit_linear(self, times, losses):
        """"""Fit a simple linear regression: loss = slope * time + intercept.""""""
        times = np.asarray(times, dtype=float)
        losses = np.asarray(losses, dtype=float)
        if times.ndim != 1 or losses.ndim != 1:
            raise ValueError(""times and losses must be 1‑D arrays"")
        if times.size != losses.size:
            raise ValueError(""times and losses must have the same length"")
        if times.size < 2:
            # Not enough points to fit a line; use constant model
            slope = 0.0
            intercept = losses.mean()
        else:
            mean_t = times.mean()
            mean_l = losses.mean()
            cov = ((times - mean_t) * (losses - mean_l)).sum()
            var = ((times - mean_t) ** 2).sum()
            slope = cov / var if var != 0 else 0.0
            intercept = mean_l - slope * mean_t
        return slope, intercept

    def fit(self, times, losses, configs=None):
        """"""
        Fit the model(s) to the provided data.

        Parameters
        ----------
        times : array‑like
            Training times (or epochs) for each observation.
        losses : array‑like
            Corresponding loss values.
        configs : array‑like or None
            Optional configuration identifiers for each observation.
            If None, a single global model is fitted.
        """"""
        times = np.asarray(times, dtype=float)
        losses = np.asarray(losses, dtype=float)

        if configs is None:
            # Single global model
            slope, intercept = self._fit_linear(times, losses)
            self._models[None] = (slope, intercept)
        else:
            configs = np.asarray(configs)
            if configs.size != times.size:
                raise ValueError(""configs must have the same length as times"")
            # Group by config
            unique_configs = np.unique(configs)
            for cfg in unique_configs:
                mask = configs == cfg
                slope, intercept = self._fit_linear(times[mask], losses[mask])
                self._models[self._to_key(cfg)] = (slope, intercept)

    def predict_unseen(self, times, config=None):
        """"""
        Predict loss values for unseen times given a configuration.

        Parameters
        ----------
        times : array‑like
            Times at which to predict loss.
        config : hashable or None
            Configuration identifier. If None, uses the global model.

        Returns
        -------
        preds : np.ndarray
            Predicted loss values.
        """"""
        key = self._to_key(config)
        if key not in self._models:
            raise KeyError(f""No model found for configuration {config!r}"")
        slope, intercept = self._models[key]
        times = np.asarray(times, dtype=float)
        return slope * times + intercept

    def extend_partial(self, times, obs_times, obs_losses, config=None):
        """"""
        Extend a partial training curve to the full set of times.

        Parameters
        ----------
        times : array‑like
            Full set of times (e.g., all epochs).
        obs_times : array‑like
            Observed times (subset of `times`).
        obs_losses : array‑like
            Observed loss values at `obs_times`.
        config : hashable or None
            Configuration identifier.

        Returns
        -------
        full_losses : np.ndarray
            Predicted loss values for all `times`.
        """"""
        # Fit a model to the partial data
        slope, intercept = self._fit_linear(obs_times, obs_losses)
        # Store or update the model for this config
        self._models[self._to_key(config)] = (slope, intercept)
        # Predict for all times
        times = np.asarray(times, dtype=float)
        return slope * times + intercept
```"
168574,romanz/trezor-agent,romanz_trezor-agent/libagent/age/client.py,libagent.age.client.Client,"from cryptography.hazmat.primitives.kdf.hkdf import HKDF
from cryptography.hazmat.primitives import hashes

class Client:
    """"""Sign messages and get public keys from a hardware device.""""""

    def __init__(self, device):
        """"""C-tor.""""""
        self.device = device

    def pubkey(self, identity, ecdh=False):
        """"""Return public key as VerifyingKey object.""""""
        with self.device:
            pubkey = bytes(self.device.pubkey(ecdh=ecdh, identity=identity))
            assert len(pubkey) == 32
            return pubkey

    def ecdh(self, identity, peer_pubkey):
        """"""Derive shared secret using ECDH from peer public key.""""""
        log.info('please confirm AGE decryption on %s for ""%s""...', self.device, identity.to_string())
        with self.device:
            assert len(peer_pubkey) == 32
            result, self_pubkey = self.device.ecdh_with_pubkey(pubkey=b'@' + peer_pubkey, identity=identity)
            assert result[:1] == b'\x04'
            hkdf = HKDF(algorithm=hashes.SHA256(), length=32, salt=peer_pubkey + self_pubkey, info=b'age-encryption.org/v1/X25519')
            return hkdf.derive(result[1:])","
class Client:
    
    def __init__(self, device):
        
        pass
    def pubkey(self, identity, ecdh=False):
        
        pass

    def ecdh(self, identity, peer_pubkey):
        
        pass",snippet_100,4,"```python
import os
from cryptography.hazmat.primitives.asymmetric import ec
from cryptography.hazmat.primitives import serialization, hashes
from cryptography.hazmat.primitives.kdf.hkdf import HKDF


class Client:
    """"""
    A simple client that manages a set of elliptic‑curve key pairs
    and can perform ECDH key exchange with a peer.
    """"""

    def __init__(self, device=None):
        """"""
        Initialise the client with an optional device.

        Parameters
        ----------
        device : dict, optional
            A mapping from identity strings to private keys. If not
            provided, an empty dictionary is created.
        """"""
        self._device = device if device is not None else {}

    def pubkey(self, identity, ecdh=False):
        """"""
        Return the public key for the given identity. If the identity
        does not yet have a key pair, one is generated.

        Parameters
        ----------
        identity : str
            The identifier for the key pair.
        ecdh : bool, optional
            Ignored in this implementation but kept for API compatibility.

        Returns
        -------
        bytes
            The public key encoded in PEM format.
        """"""
        if identity not in self._device:
            # Generate a new private key for this identity
            private_key = ec.generate_private_key(ec.SECP256R1())
            self._device[identity] = private_key
        else:
            private_key = self._device[identity]

        public_key = private_key.public_key()
        pem = public_key.public_bytes(
            encoding=serialization.Encoding.PEM,
            format=serialization.PublicFormat.SubjectPublicKeyInfo,
        )
        return pem

    def ecdh(self, identity, peer_pubkey):
        """"""
        Perform an ECDH key exchange with a peer public key.

        Parameters
        ----------
        identity : str
            The identifier for the local key pair.
        peer_pubkey : bytes
            The peer's public key in PEM format.

        Returns
        -------
        bytes
            A derived shared secret (32 bytes) using HKDF.
        """"""
        if identity not in self._device:
            raise ValueError(f""Identity '{identity}' not found in device."")

        private_key = self._device[identity]

        # Load the peer's public key
        peer_public_key = serialization.load_pem_public_key(peer_pubkey)

        # Perform raw ECDH to get the shared secret
        shared_secret = private_key.exchange(ec.ECDH(), peer_public_key)

        # Derive a fixed‑size key from the shared secret
        derived_key = HKDF(
            algorithm=hashes.SHA256(),
            length=32,
            salt=None,
            info=b""ecdh shared secret"",
        ).derive(shared_secret)

        return derived_key
```"
168596,romanz/trezor-agent,romanz_trezor-agent/libagent/ssh/__init__.py,libagent.ssh.ClosableNamedTemporaryFile,"import tempfile
import os

class ClosableNamedTemporaryFile:
    """"""Creates a temporary file that is not deleted when the file is closed.

    This allows the file to be opened with an exclusive lock, but used by other programs before
    it is deleted
    """"""

    def __init__(self):
        """"""Create a temporary file.""""""
        self.file = tempfile.NamedTemporaryFile(prefix='trezor-ssh-pubkey-', mode='w', delete=False)
        self.name = self.file.name

    def write(self, buf):
        """"""Write `buf` to the file.""""""
        self.file.write(buf)

    def close(self):
        """"""Closes the file, allowing it to be opened by other programs. Does not delete the file.""""""
        self.file.close()

    def __del__(self):
        """"""Deletes the temporary file.""""""
        try:
            os.unlink(self.file.name)
        except OSError:
            log.warning('Failed to delete temporary file: %s', self.file.name)","
class ClosableNamedTemporaryFile:
    
    def __init__(self):
        
        pass
    def write(self, buf):
        
        pass

    def close(self):
        
        pass

    def __del__(self):
        
        pass",snippet_101,5,"```python
import os
import tempfile

class ClosableNamedTemporaryFile:
    def __init__(self):
        # Create a temporary file that is not deleted automatically
        self._tmp = tempfile.NamedTemporaryFile(delete=False, mode='w+b')
        self.name = self._tmp.name
        self._closed = False

    def write(self, buf):
        if self._closed:
            raise ValueError(""I/O operation on closed file."")
        if isinstance(buf, str):
            buf = buf.encode()
        self._tmp.write(buf)

    def close(self):
        if not self._closed:
            try:
                self._tmp.close()
            finally:
                try:
                    os.remove(self.name)
                except FileNotFoundError:
                    pass
                self._closed = True

    def __del__(self):
        try:
            self.close()
        except Exception:
            pass
```"
168603,romanz/trezor-agent,romanz_trezor-agent/libagent/util.py,libagent.util.ExpiringCache,"import time

class ExpiringCache:
    """"""Simple cache with a deadline.""""""

    def __init__(self, seconds, timer=time.time):
        """"""C-tor.""""""
        self.duration = seconds
        self.timer = timer
        self.value = None
        self.set(None)

    def get(self):
        """"""Returns existing value, or None if deadline has expired.""""""
        if self.timer() > self.deadline:
            self.value = None
        return self.value

    def set(self, value):
        """"""Set new value and reset the deadline for expiration.""""""
        self.deadline = self.timer() + self.duration
        self.value = value","
class ExpiringCache:
    
    def __init__(self, seconds, timer=time.time):
        
        pass
    def get(self):
        
        pass

    def set(self, value):
        
        pass",snippet_102,4,"```python
import time

class ExpiringCache:
    def __init__(self, seconds, timer=time.time):
        self.seconds = seconds
        self.timer = timer
        self._value = None
        self._expires_at = None

    def set(self, value):
        """"""Store a value with an expiration time.""""""
        self._value = value
        self._expires_at = self.timer() + self.seconds

    def get(self):
        """"""Return the stored value if it hasn't expired, otherwise None.""""""
        if self._expires_at is None:
            return None
        if self.timer() < self._expires_at:
            return self._value
        # expired
        self._value = None
        self._expires_at = None
        return None
```"
168604,romanz/trezor-agent,romanz_trezor-agent/libagent/util.py,libagent.util.Reader,"import struct
import contextlib

class Reader:
    """"""Read basic type objects out of given stream.""""""

    def __init__(self, stream):
        """"""Create a non-capturing reader.""""""
        self.s = stream
        self._captured = None

    def readfmt(self, fmt):
        """"""Read a specified object, using a struct format string.""""""
        size = struct.calcsize(fmt)
        blob = self.read(size)
        obj, = struct.unpack(fmt, blob)
        return obj

    def read(self, size=None):
        """"""Read `size` bytes from stream.""""""
        blob = self.s.read(size)
        if size is not None and len(blob) < size:
            raise EOFError
        if self._captured:
            self._captured.write(blob)
        return blob

    @contextlib.contextmanager
    def capture(self, stream):
        """"""Capture all data read during this context.""""""
        self._captured = stream
        try:
            yield
        finally:
            self._captured = None","
class Reader:
    
    def __init__(self, stream):
        
        pass
    def readfmt(self, fmt):
        
        pass

    def readfmt(self, fmt):
        
        pass
    @contextlib.contextmanager
    def capture(self, stream):
        
        pass",snippet_103,5,"```python
import contextlib
import struct

class Reader:
    def __init__(self, stream):
        """"""
        Initialize the Reader with a binary stream.
        """"""
        self.stream = stream

    def readfmt(self, fmt):
        """"""
        Read data from the stream according to the given struct format string.
        Returns a tuple of unpacked values.
        """"""
        size = struct.calcsize(fmt)
        data = self.stream.read(size)
        if len(data) != size:
            raise EOFError(f""Expected {size} bytes, got {len(data)}"")
        return struct.unpack(fmt, data)

    @contextlib.contextmanager
    def capture(self, stream):
        """"""
        Temporarily replace the Reader's stream with the given stream.
        Restores the original stream after the context exits.
        """"""
        original = self.stream
        self.stream = stream
        try:
            yield
        finally:
            self.stream = original
```"
171012,indygreg/python-zstandard,indygreg_python-zstandard/zstandard/backend_cffi.py,zstandard.backend_cffi.BufferSegment,"class BufferSegment:
    """"""Represents a segment within a ``BufferWithSegments``.

    This type is essentially a reference to N bytes within a
    ``BufferWithSegments``.

    The object conforms to the buffer protocol.
    """"""

    @property
    def offset(self):
        """"""The byte offset of this segment within its parent buffer.""""""
        raise NotImplementedError()

    def __len__(self):
        """"""Obtain the length of the segment, in bytes.""""""
        raise NotImplementedError()

    def tobytes(self):
        """"""Obtain bytes copy of this segment.""""""
        raise NotImplementedError()","class BufferSegment:
    
    @property
    def offset(self):
        
        pass
    def __len__(self):
        
        pass

    def tobytes(self):
        
        pass",snippet_104,4,"```python
class BufferSegment:
    """"""
    Represents a contiguous segment of a buffer (bytes, bytearray, or memoryview).
    """"""

    def __init__(self, buffer, offset=0, length=None):
        """"""
        Parameters
        ----------
        buffer : bytes | bytearray | memoryview
            The underlying buffer.
        offset : int, optional
            The starting index of the segment within the buffer.
        length : int | None, optional
            The length of the segment. If None, the segment extends to the end of the buffer.
        """"""
        if not isinstance(buffer, (bytes, bytearray, memoryview)):
            raise TypeError(""buffer must be bytes, bytearray, or memoryview"")

        self._buffer = memoryview(buffer)
        if not isinstance(offset, int):
            raise TypeError(""offset must be an integer"")
        if offset < 0 or offset > len(self._buffer):
            raise ValueError(""offset out of range"")

        if length is None:
            length = len(self._buffer) - offset
        if not isinstance(length, int):
            raise TypeError(""length must be an integer or None"")
        if length < 0 or offset + length > len(self._buffer):
            raise ValueError(""length out of range"")

        self._offset = offset
        self._length = length

    @property
    def offset(self):
        """"""The starting index of the segment within the underlying buffer.""""""
        return self._offset

    def __len__(self):
        """"""The length of the segment.""""""
        return self._length

    def tobytes(self):
        """"""Return the segment as an immutable bytes object.""""""
        return self._buffer[self._offset : self._offset + self._length].tobytes()
```"
171015,indygreg/python-zstandard,indygreg_python-zstandard/zstandard/backend_cffi.py,zstandard.backend_cffi.BufferWithSegmentsCollection,"class BufferWithSegmentsCollection:
    """"""A virtual spanning view over multiple BufferWithSegments.

    Instances are constructed from 1 or more :py:class:`BufferWithSegments`
    instances. The resulting object behaves like an ordered sequence whose
    members are the segments within each ``BufferWithSegments``.

    If the object is composed of 2 ``BufferWithSegments`` instances with the
    first having 2 segments and the second have 3 segments, then ``b[0]``
    and ``b[1]`` access segments in the first object and ``b[2]``, ``b[3]``,
    and ``b[4]`` access segments from the second.
    """"""

    def __len__(self):
        """"""The number of segments within all ``BufferWithSegments``.""""""
        raise NotImplementedError()

    def __getitem__(self, i):
        """"""Obtain the ``BufferSegment`` at an offset.""""""
        raise NotImplementedError()","class BufferWithSegmentsCollection:
    
    def __len__(self):
        
        pass
    def __getitem__(self, i):
        
        pass",snippet_105,3,"```python
class BufferWithSegmentsCollection:
    def __init__(self, buffer: bytes, segment_size: int):
        if segment_size <= 0:
            raise ValueError(""segment_size must be a positive integer"")
        self._segments = [
            buffer[i:i + segment_size] for i in range(0, len(buffer), segment_size)
        ]

    def __len__(self):
        return len(self._segments)

    def __getitem__(self, i):
        return self._segments[i]
```"
171185,ssalentin/plip,ssalentin_plip/plip/exchange/xml.py,plip.exchange.xml.XMLStorage,"class XMLStorage:
    """"""Generic class for storing XML data from PLIP XML files.""""""

    @staticmethod
    def getdata(tree, location, force_string=False):
        """"""Gets XML data from a specific element and handles types.""""""
        found = tree.xpath('%s/text()' % location)
        if not found:
            return None
        else:
            data = found[0]
        if force_string:
            return data
        if data == 'True':
            return True
        elif data == 'False':
            return False
        else:
            try:
                return int(data)
            except ValueError:
                try:
                    return float(data)
                except ValueError:
                    return data

    @staticmethod
    def getcoordinates(tree, location):
        """"""Gets coordinates from a specific element in PLIP XML""""""
        return tuple((float(x) for x in tree.xpath('.//%s/*/text()' % location)))","class XMLStorage:
    
    @staticmethod
    def getdata(tree, location, force_string=False):
        
        pass
    @staticmethod
    def getcoordinates(tree, location):
        
        pass",snippet_106,3,"```python
import xml.etree.ElementTree as ET
from typing import Any, Tuple, Optional


class XMLStorage:
    @staticmethod
    def _find_element(tree: Any, location: str) -> Optional[ET.Element]:
        """"""
        Helper to locate an element in the XML tree.
        Accepts either an ElementTree or an Element.
        """"""
        if isinstance(tree, ET.ElementTree):
            root = tree.getroot()
        elif isinstance(tree, ET.Element):
            root = tree
        else:
            raise TypeError(""tree must be an ElementTree or Element"")

        # Use the standard find method which supports simple XPath
        return root.find(location)

    @staticmethod
    def getdata(tree: Any, location: str, force_string: bool = False) -> Any:
        """"""
        Retrieve data from the XML tree at the specified location.

        Parameters
        ----------
        tree : ElementTree or Element
            The XML tree to search.
        location : str
            XPath-like string to locate the element.
        force_string : bool, optional
            If True, always return a string representation of the data.
            Default is False.

        Returns
        -------
        Any
            The text content of the element, its attributes as a dict,
            or a string representation depending on `force_string`.
            Returns None if the element is not found.
        """"""
        elem = XMLStorage._find_element(tree, location)
        if elem is None:
            return None

        # Prefer text content if available
        if elem.text and elem.text.strip():
            data = elem.text.strip()
        else:
            # Fallback to attributes dictionary
            data = dict(elem.attrib)

        if force_string:
            return str(data)
        return data

    @staticmethod
    def getcoordinates(tree: Any, location: str) -> Optional[Tuple[float, float]]:
        """"""
        Retrieve (x, y) coordinates from an element's attributes.

        Parameters
        ----------
        tree : ElementTree or Element
            The XML tree to search.
        location : str
            XPath-like string to locate the element.

        Returns
        -------
        tuple[float, float] or None
            A tuple of (x, y) coordinates if both attributes are present
            and convertible to float. Returns None otherwise.
        """"""
        elem = XMLStorage._find_element(tree, location)
        if elem is None:
            return None

        try:
            x = float(elem.attrib.get(""x""))
            y = float(elem.attrib.get(""y""))
            return (x, y)
        except (TypeError, ValueError):
            return None
```"
172857,cogeotiff/rio-tiler,rio_tiler/colormap.py,rio_tiler.colormap.ColorMaps,"import attr
from rio_tiler.errors import ColorMapAlreadyRegistered, InvalidColorFormat, InvalidColorMapName, InvalidFormat
import json
import numpy
from typing import Dict, List, Sequence, Tuple, Union
import pathlib
from rio_tiler.types import ColorMapType, DataMaskType, DiscreteColorMapType, GDALColorMapType, IntervalColorMapType

@attr.s(frozen=True)
class ColorMaps:
    """"""Default Colormaps holder.

    Attributes:
        data (dict): colormaps. Defaults to `rio_tiler.colormap.DEFAULTS_CMAPS`.

    """"""
    data: Dict[str, Union[str, pathlib.Path, ColorMapType]] = attr.ib(default=attr.Factory(lambda: DEFAULT_CMAPS_FILES))

    def get(self, name: str) -> ColorMapType:
        """"""Fetch a colormap.

        Args:
            name (str): colormap name.

        Returns
            dict: colormap dictionary.

        """"""
        cmap = self.data.get(name, None)
        if cmap is None:
            raise InvalidColorMapName(f'Invalid colormap name: {name}')
        if isinstance(cmap, (pathlib.Path, str)):
            if isinstance(cmap, str):
                cmap = pathlib.Path(cmap)
            if cmap.suffix == '.npy':
                colormap = numpy.load(cmap)
                assert colormap.shape == (256, 4)
                assert colormap.dtype == numpy.uint8
                cmap_data = {idx: tuple(value) for idx, value in enumerate(colormap)}
            elif cmap.suffix == '.json':
                with cmap.open() as f:
                    cmap_data = json.load(f, object_hook=lambda x: {int(k): parse_color(v) for k, v in x.items()})
                if isinstance(cmap_data, Sequence):
                    cmap_data = [(tuple(inter), parse_color(v)) for inter, v in cmap_data]
            else:
                raise ValueError(f'Not supported {cmap.suffix} extension for ColorMap')
            self.data[name] = cmap_data
            return cmap_data
        return cmap

    def list(self) -> List[str]:
        """"""List registered Colormaps.

        Returns
            list: list of colormap names.

        """"""
        return list(self.data)

    def register(self, custom_cmap: Dict[str, Union[str, pathlib.Path, ColorMapType]], overwrite: bool=False) -> 'ColorMaps':
        """"""Register a custom colormap.

        Args:
            custom_cmap (dict): custom colormap(s) to register.
            overwrite (bool): Overwrite existing colormap with same key. Defaults to False.

        Examples:
            >>> cmap = cmap.register({""acmap"": {0: (0, 0, 0, 0), ...}})

            >>> cmap = cmap.register({""acmap"": ""acmap.npy""})

        """"""
        for name, _ in custom_cmap.items():
            if not overwrite and name in self.data:
                raise ColorMapAlreadyRegistered(f'{name} is already registered. Use force=True to overwrite.')
        return ColorMaps({**self.data, **custom_cmap})","@attr.s(frozen=True)
class ColorMaps:
    
    def get(self, name: str) -> ColorMapType:
        
        pass
    def list(self) -> List[str]:
        
        pass

    def register(self, custom_cmap: Dict[str, Union[str, pathlib.Path, ColorMapType]], overwrite: bool=False) -> 'ColorMaps':
        '''Register a custom colormap.
        Args:
            custom_cmap (dict): custom colormap(s) to register.
            overwrite (bool): Overwrite existing colormap with same key. Defaults to False.
        Examples:
            >>> cmap = cmap.register({""acmap"": {0: (0, 0, 0, 0), ...}})
            >>> cmap = cmap.register({""acmap"": ""acmap.npy""})
        '''
        pass",snippet_107,4,"```python
import pathlib
from typing import Any, Dict, List, Union

import attr


# A very loose type alias for a colormap.  The actual type can be a
# dictionary mapping floats to RGBA tuples, a NumPy array, or any
# other representation that the user supplies.
ColorMapType = Any


@attr.s(frozen=True, slots=True)
class ColorMaps:
    """"""
    A registry for colormaps.  The registry is immutable; calling
    :meth:`register` returns a new instance with the updated mapping.
    """"""

    _cmap: Dict[str, ColorMapType] = attr.ib(default=attr.Factory(dict), init=False)

    def get(self, name: str) -> ColorMapType:
        """"""Return the colormap registered under *name*.""""""
        try:
            return self._cmap[name]
        except KeyError as exc:
            raise KeyError(f""Colormap '{name}' not found."") from exc

    def list(self) -> List[str]:
        """"""Return a sorted list of registered colormap names.""""""
        return sorted(self._cmap.keys())

    def register(
        self,
        custom_cmap: Dict[str, Union[str, pathlib.Path, ColorMapType]],
        overwrite: bool = False,
    ) -> ""ColorMaps"":
        """"""
        Register a custom colormap.

        Parameters
        ----------
        custom_cmap : dict
            Mapping of colormap names to colormap objects or file paths.
        overwrite : bool, optional
            If ``True`` an existing colormap with the same name will be
            replaced.  Defaults to ``False``.

        Returns
        -------
        ColorMaps
            A new instance with the updated registry.
        """"""
        new_cmap = dict(self._cmap)

        for name, cmap in custom_cmap.items():
            if not overwrite and name in new_cmap:
                raise ValueError(f""Colormap '{name}' already exists. Use overwrite=True to replace it."")
            new_cmap[name] = cmap

        return attr.evolve(self, _cmap=new_cmap)
```"
174678,ipinfo/python,ipinfo_python/ipinfo/details.py,ipinfo.details.Details,"class Details:
    """"""Encapsulates data for single IP address.""""""

    def __init__(self, details):
        """"""Initialize by settings `details` attribute.""""""
        self.details = details

    def __getattr__(self, attr):
        """"""Return attribute if it exists in details array, else return error.""""""
        if attr not in self.details:
            raise AttributeError(f'{attr} is not a valid attribute of Details')
        return self.details[attr]

    @property
    def all(self):
        """"""Return all details as dict.""""""
        return self.details","class Details:
    
    def __init__(self, details):
        
        pass
    def __getattr__(self, attr):
        
        pass
    @property
    def all(self):
        
        pass",snippet_108,4,"```python
class Details:
    def __init__(self, details):
        """"""
        Initialize with a mapping of detail names to values.
        """"""
        if not isinstance(details, dict):
            raise TypeError(""details must be a dict"")
        self._details = dict(details)

    def __getattr__(self, attr):
        """"""
        Allow attribute access to the underlying details dictionary.
        """"""
        try:
            return self._details[attr]
        except KeyError:
            raise AttributeError(f""'Details' object has no attribute '{attr}'"")

    @property
    def all(self):
        """"""
        Return a copy of all stored details.
        """"""
        return dict(self._details)
```"
176091,opencobra/cobrapy,opencobra_cobrapy/src/cobra/util/context.py,cobra.util.context.HistoryManager,"from typing import TYPE_CHECKING, Any, Callable, Optional

class HistoryManager:
    """"""
    Define a base context manager.

    It records a list of actions to be taken at a later time.
    This is used to implement context managers that allow temporary
    changes to a `cobra.core.Model`.

    """"""

    def __init__(self, **kwargs) -> None:
        """"""Initialize the class.""""""
        super().__init__(**kwargs)
        self._history = []

    def __call__(self, operation: Callable[[Any], Any]) -> None:
        """"""Add the corresponding operation to the history stack.

        Parameters
        ----------
        operation : callable
            A function to be called at a later time.

        """"""
        self._history.append(operation)

    def reset(self) -> None:
        """"""Trigger executions for all items in the stack in reverse order.""""""
        while self._history:
            entry = self._history.pop()
            entry()

    def size(self) -> int:
        """"""Calculate number of operations on the stack.""""""
        return len(self._history)","
class HistoryManager:
    
    def __init__(self, **kwargs) -> None:
        
        pass
    def __call__(self, operation: Callable[[Any], Any]) -> None:
        
        pass

    def reset(self) -> None:
        
        pass

    def size(self) -> int:
        
        pass",snippet_109,5,"```python
from typing import Any, Callable, List, Tuple


class HistoryManager:
    """"""
    A simple history manager that records the operations executed via the
    instance callable interface. Each operation is stored along with its
    result. The manager can be reset and its size queried.
    """"""

    def __init__(self, max_size: int | None = None, **kwargs) -> None:
        """"""
        Parameters
        ----------
        max_size : int | None, optional
            Maximum number of history entries to keep. If None, history
            grows indefinitely.
        **kwargs
            Any additional attributes to set on the instance.
        """"""
        self.max_size: int | None = max_size
        self._history: List[Tuple[Callable[[Any], Any], Any]] = []

        # Set any extra keyword arguments as attributes
        for key, value in kwargs.items():
            setattr(self, key, value)

    def __call__(self, operation: Callable[[Any], Any]) -> None:
        """"""
        Execute the given operation and record it in the history.

        Parameters
        ----------
        operation : Callable[[Any], Any]
            A callable that takes no arguments and returns a value.
        """"""
        # Execute the operation
        result = operation()

        # Store the operation and its result
        self._history.append((operation, result))

        # Enforce maximum history size if specified
        if self.max_size is not None and len(self._history) > self.max_size:
            # Remove the oldest entry
            self._history.pop(0)

    def reset(self) -> None:
        """"""
        Clear the history.
        """"""
        self._history.clear()

    def size(self) -> int:
        """"""
        Return the number of recorded operations.

        Returns
        -------
        int
            The size of the history.
        """"""
        return len(self._history)
```"
176713,adafruit/Adafruit_Blinka,adafruit_Adafruit_Blinka/src/adafruit_blinka/__init__.py,adafruit_blinka.Enum,"class Enum:
    """"""
    Object supporting CircuitPython-style of static symbols
    as seen with Direction.OUTPUT, Pull.UP
    """"""

    def __repr__(self):
        """"""
        Assumes instance will be found as attribute of own class.
        Returns dot-subscripted path to instance
        (assuming absolute import of containing package)
        """"""
        cls = type(self)
        for key in dir(cls):
            if getattr(cls, key) is self:
                return '{}.{}.{}'.format(cls.__module__, cls.__qualname__, key)
        return repr(self)

    @classmethod
    def iteritems(cls):
        """"""
        Inspects attributes of the class for instances of the class
        and returns as key,value pairs mirroring dict#iteritems
        """"""
        for key in dir(cls):
            val = getattr(cls, key)
            if isinstance(cls, val):
                yield (key, val)","class Enum:
    
    def __repr__(self):
        
        pass
    @classmethod
    def iteritems(cls):
        
        pass",snippet_110,3,"```python
class Enum:
    """"""
    A simple enumeration base class.

    Example usage:

        class Color(Enum):
            RED = 1
            GREEN = 2
            BLUE = 3

        print(Color.iteritems())   # [('RED', 1), ('GREEN', 2), ('BLUE', 3)]
        print(Color.RED)           # <Color RED>
    """"""

    def __repr__(self):
        """"""
        Return a readable representation of the enum value.
        If the instance has `_name` and `_value` attributes (created by
        accessing an enum member), show them; otherwise fall back to
        the default string conversion.
        """"""
        if hasattr(self, ""_name"") and hasattr(self, ""_value""):
            return f""<{self.__class__.__name__} {self._name}>""
        return f""<{self.__class__.__name__} {self!s}>""

    @classmethod
    def iteritems(cls):
        """"""
        Return a list of (name, value) tuples for all public
        attributes of the enum class that are not callables.
        """"""
        return [
            (name, value)
            for name, value in vars(cls).items()
            if not name.startswith(""_"") and not callable(value)
        ]
```"
176756,adafruit/Adafruit_Blinka,adafruit_Adafruit_Blinka/src/adafruit_blinka/microcontroller/nova/__init__.py,adafruit_blinka.microcontroller.nova.Connection,"class Connection:
    """"""Connection class""""""
    __instance = None

    @staticmethod
    def getInstance():
        """"""Static access method.""""""
        if Connection.__instance is None:
            Connection()
        return Connection.__instance

    def __init__(self):
        """"""Virtually private constructor.""""""
        if Connection.__instance is not None:
            raise Exception('This class is a singleton!')
        from binhoHostAdapter import binhoHostAdapter
        from binhoHostAdapter import binhoUtilities
        devices = binhoUtilities.listAvailableDevices()
        if len(devices) > 0:
            Connection.__instance = binhoHostAdapter.binhoHostAdapter(devices[0])
        else:
            raise RuntimeError('No Binho Nova found!')","class Connection:
    
    @staticmethod
    def getInstance():
        
        pass
    def __init__(self):
        
        pass",snippet_111,3,"class Connection:
    _instance = None

    @staticmethod
    def getInstance():
        if Connection._instance is None:
            Connection._instance = Connection()
        return Connection._instance

    def __init__(self):
        pass"
177578,ottogroup/palladium,ottogroup_palladium/palladium/persistence.py,palladium.persistence.FileLikeIO,"from abc import abstractmethod

class FileLikeIO:
    """"""Used by :class:`FileLike` to access low level file handle
    operations.
    """"""

    @abstractmethod
    def open(self, path, mode='r'):
        """"""Return a file handle

        For normal files, the implementation is:

        ```python
        return open(path, mode)
        ```
        """"""

    @abstractmethod
    def exists(self, path):
        """"""Test whether a path exists

        For normal files, the implementation is:

        ```python
        return os.path.exists(path)
        ```
        """"""

    @abstractmethod
    def remove(self, path):
        """"""Remove a file

        For normal files, the implementation is:

        ```python
        os.remove(path)
        ```
        """"""","
class FileLikeIO:
    
    @abstractmethod
    def open(self, path, mode='r'):
        
        pass
    @abstractmethod
    def exists(self, path):
        
        pass
    @abstractmethod
    def remove(self, path):
        
        pass",snippet_113,4,"import os

class FileLikeIO:
    def open(self, path, mode='r'):
        return open(path, mode)

    def exists(self, path):
        return os.path.exists(path)

    def remove(self, path):
        os.remove(path)"
179419,python-constraint/python-constraint,python-constraint_python-constraint/constraint/domain.py,constraint.domain.Variable,"class Variable:
    """"""Helper class for variable definition.

    Using this class is optional, since any hashable object,
    including plain strings and integers, may be used as variables.
    """"""

    def __init__(self, name):
        """"""Initialization method.

        Args:
            name (string): Generic variable name for problem-specific
                purposes
        """"""
        self.name = name

    def __repr__(self):
        """"""Represents itself with the name attribute.""""""
        return self.name","class Variable:
    
    def __init__(self, name):
        
        pass
    def __repr__(self):
        
        pass",snippet_115,3,"class Variable:
    def __init__(self, name):
        self.name = name

    def __repr__(self):
        return f""Variable({self.name!r})"""
179425,python-constraint/python-constraint,python-constraint_python-constraint/constraint/solvers.py,constraint.solvers.Solver,"class Solver:
    """"""Abstract base class for solvers.""""""
    requires_pickling = False

    def getSolution(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Return one solution for the given problem.

        Args:
            domains (dict): Dictionary mapping variables to their domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        """"""
        msg = f'{self.__class__.__name__} is an abstract class'
        raise NotImplementedError(msg)

    def getSolutions(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Return all solutions for the given problem.

        Args:
            domains (dict): Dictionary mapping variables to domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        """"""
        msg = f'{self.__class__.__name__} provides only a single solution'
        raise NotImplementedError(msg)

    def getSolutionIter(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Return an iterator for the solutions of the given problem.

        Args:
            domains (dict): Dictionary mapping variables to domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        """"""
        msg = f""{self.__class__.__name__} doesn't provide iteration""
        raise NotImplementedError(msg)","class Solver:
    
    def getSolution(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        
        pass
    def getSolutions(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        
        pass
    def getSolutionIter(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        
        pass",snippet_116,4,"```python
class Solver:
    def getSolution(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Return the first solution found or None.""""""
        gen = self.getSolutionIter(domains, constraints, vconstraints)
        try:
            return next(gen)
        except StopIteration:
            return None

    def getSolutions(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Return a list of all solutions.""""""
        return list(self.getSolutionIter(domains, constraints, vconstraints))

    def getSolutionIter(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Yield all solutions one by one.""""""
        # Prepare list of variables in a deterministic order
        variables = list(domains.keys())

        # Helper to check all constraints given current assignment
        def constraints_satisfied(assignment):
            # Unary variable constraints
            for var, val in assignment.items():
                if var in vconstraints and not vconstraints[var](val):
                    return False
            # General constraints
            for constraint in constraints:
                # If constraint is a function, call it
                if callable(constraint):
                    try:
                        if not constraint(assignment):
                            return False
                    except Exception:
                        return False
                # If constraint is a tuple of variable names, check consistency
                elif isinstance(constraint, tuple):
                    # Only check if all vars in constraint are assigned
                    if all(v in assignment for v in constraint):
                        values = tuple(assignment[v] for v in constraint)
                        # If values are not all equal, fail
                        if len(set(values)) > 1:
                            return False
                else:
                    # Unsupported constraint type
                    return False
            return True

        def backtrack(assignment, idx):
            if idx == len(variables):
                # All variables assigned
                yield dict(assignment)
                return
            var = variables[idx]
            for val in domains.get(var, []):
                assignment[var] = val
                if constraints_satisfied(assignment):
                    yield from backtrack(assignment, idx + 1)
                # Backtrack
                del assignment[var]

        yield from backtrack({}, 0)
```"
179445,maroba/findiff,findiff/pde.py,findiff.pde.PDE,"import numpy as np
from scipy.sparse.linalg import spsolve
import scipy.sparse as sparse

class PDE:
    """"""
    Representation of a partial differential equation.
    """"""

    def __init__(self, lhs, rhs, bcs):
        """"""
        Initializes the PDE.

        You need to specify the left hand side (lhs) in terms of derivatives
        as well as the right hand side in terms of an array.

        Parameters
        ----------
        lhs: FinDiff object or combination of FinDiff objects
            the left hand side of the PDE
        rhs: numpy.ndarray
            the right hand side of the PDE
        bcs: BoundaryConditions
            the boundary conditions for the PDE

        """"""
        self.lhs = lhs
        self.rhs = rhs
        self.bcs = bcs
        self._L = None

    def solve(self):
        """"""
        Solves the PDE.

        Returns
        -------
        out: numpy.ndarray
            Array with the solution of the PDE.
        """"""
        shape = self.bcs.shape
        if self._L is None:
            self._L = self.lhs.matrix(shape)
        L = sparse.lil_matrix(self._L)
        f = self.rhs.reshape(-1, 1)
        nz = list(self.bcs.row_inds())
        L[nz, :] = self.bcs.lhs[nz, :]
        f[nz] = np.array(self.bcs.rhs[nz].toarray()).reshape(-1, 1)
        L = sparse.csr_matrix(L)
        return spsolve(L, f).reshape(shape)","
class PDE:
    
    def __init__(self, lhs, rhs, bcs):
        
        pass
    def solve(self):
        
        pass",snippet_117,3,"```python
import numpy as np

class PDE:
    """"""
    A very simple 1‑D heat‑equation solver using an explicit finite‑difference scheme.
    The constructor accepts the left‑hand side (lhs) and right‑hand side (rhs) functions
    of the PDE, boundary conditions (bcs), and optional discretisation parameters.
    """"""

    def __init__(self, lhs, rhs, bcs,
                 domain=(0.0, 1.0), nx=51, dt=1e-4, t_final=0.01):
        """"""
        Parameters
        ----------
        lhs : callable
            Function representing the left‑hand side of the PDE. For the heat
            equation this is typically ``u_t``. It is not used directly in the
            solver but kept for compatibility with the interface.
        rhs : callable
            Function representing the spatial part of the PDE. For the heat
            equation this is typically ``u_xx``. It is expected to accept a
            1‑D numpy array of spatial points and return the initial condition
            for the temperature field.
        bcs : dict
            Boundary conditions. Expected keys are ``'left'`` and ``'right'``.
            The values are the Dirichlet boundary values at the left and right
            ends of the domain.
        domain : tuple of float, optional
            Spatial domain as (x_start, x_end).
        nx : int, optional
            Number of spatial grid points.
        dt : float, optional
            Time step size.
        t_final : float, optional
            Final time to integrate to.
        """"""
        self.lhs = lhs
        self.rhs = rhs
        self.bcs = bcs
        self.domain = domain
        self.nx = nx
        self.dt = dt
        self.t_final = t_final

    def solve(self):
        """"""
        Solve the PDE using an explicit finite‑difference scheme.

        Returns
        -------
        x : numpy.ndarray
            Spatial grid points.
        u : numpy.ndarray
            Solution at the final time.
        """"""
        x_start, x_end = self.domain
        L = x_end - x_start
        dx = L / (self.nx - 1)

        # Initial condition from rhs (interpreted as u(x,0))
        x = np.linspace(x_start, x_end, self.nx)
        u = self.rhs(x)

        # Apply Dirichlet boundary conditions
        u[0] = self.bcs.get('left', 0.0)
        u[-1] = self.bcs.get('right', 0.0)

        # Stability condition for explicit scheme (alpha = 1)
        alpha = 1.0
        if alpha * self.dt / dx**2 > 0.5:
            raise ValueError(""Time step too large for stability (dt > 0.5*dx^2)."")

        nt = int(np.ceil(self.t_final / self.dt))
        for _ in range(nt):
            u_new = u.copy()
            for i in range(1, self.nx - 1):
                u_new[i] = u[i] + alpha * self.dt / dx**2 * (
                    u[i + 1] - 2 * u[i] + u[i - 1]
                )
            # Re‑apply boundary conditions
            u_new[0] = self.bcs.get('left', 0.0)
            u_new[-1] = self.bcs.get('right', 0.0)
            u = u_new

        return x, u
```"
186773,sphinx-gallery/sphinx-gallery,sphinx_gallery/_dummy/__init__.py,sphinx_gallery._dummy.DummyClass,"class DummyClass:
    """"""Dummy class for testing method resolution.""""""

    def run(self):
        """"""Do nothing.""""""
        pass

    @property
    def prop(self):
        """"""Property.""""""
        return 'Property'","class DummyClass:
    
    def run(self):
        
        pass
    @property
    def prop(self):
        
        pass",snippet_119,3,"class DummyClass:
    def run(self):
        return ""run executed""

    @property
    def prop(self):
        return ""property value"""
186774,sphinx-gallery/sphinx-gallery,sphinx_gallery/_dummy/nested.py,sphinx_gallery._dummy.nested.NestedDummyClass,"class NestedDummyClass:
    """"""Nested dummy class for testing method resolution.""""""

    def run(self):
        """"""Do nothing.""""""
        pass

    @property
    def prop(self):
        """"""Property.""""""
        return 'Property'","class NestedDummyClass:
    
    def run(self):
        
        pass
    @property
    def prop(self):
        
        pass",snippet_120,3,"class NestedDummyClass:
    def run(self):
        return ""NestedDummyClass run""

    @property
    def prop(self):
        return ""NestedDummyClass prop"""
190420,JamesPHoughton/pysd,JamesPHoughton_pysd/pysd/py_backend/external.py,pysd.py_backend.external.Excels,"import numpy as np
from openpyxl import load_workbook
import pandas as pd

class Excels:
    """"""
    Class to save the read Excel files and thus avoid double reading
    """"""
    _Excels, _Excels_opyxl = ({}, {})

    @classmethod
    def read(cls, file_name, tab):
        """"""
        Read the Excel file or return the previously read one
        """"""
        if file_name.joinpath(tab) in cls._Excels:
            return cls._Excels[file_name.joinpath(tab)]
        else:
            read_kwargs = {}
            ext = file_name.suffix.lower()
            if ext in _SPREADSHEET_EXTS:
                read_func = pd.read_excel
                read_kwargs['sheet_name'] = tab
            elif ext == '.csv':
                read_func = pd.read_csv
                if tab and (not tab[0].isalnum()):
                    read_kwargs['sep'] = tab
            else:
                read_func = pd.read_table
                if tab and (not tab[0].isalnum()):
                    read_kwargs['sep'] = tab
            excel = np.array([pd.to_numeric(ex, errors='coerce') for ex in read_func(file_name, header=None, **read_kwargs).values])
            cls._Excels[file_name.joinpath(tab)] = excel
            return excel

    @classmethod
    def read_opyxl(cls, file_name):
        """"""
        Read the Excel file using OpenPyXL or return the previously read one
        """"""
        if file_name in cls._Excels_opyxl:
            return cls._Excels_opyxl[file_name]
        else:
            excel = load_workbook(file_name, read_only=True, data_only=True)
            cls._Excels_opyxl[file_name] = excel
            return excel

    @classmethod
    def clean(cls):
        """"""
        Clean the dictionary of read files
        """"""
        for file in cls._Excels_opyxl.values():
            file.close()
        cls._Excels, cls._Excels_opyxl = ({}, {})","
class Excels:
    
    @classmethod
    def read(cls, file_name, tab):
        
        pass
    @classmethod
    def read_opyxl(cls, file_name):
        
        pass
    @classmethod
    def clean(cls):
        
        pass",snippet_121,4,"```python
import os
import pandas as pd
import openpyxl


class Excels:
    """"""
    Simple helper for reading Excel files with optional caching.
    """"""

    _cache = {}

    @classmethod
    def read(cls, file_name: str, tab: str):
        """"""
        Read a specific sheet from an Excel file and cache the result.

        Parameters
        ----------
        file_name : str
            Path to the Excel file.
        tab : str
            Sheet name to read.

        Returns
        -------
        pandas.DataFrame
            The contents of the requested sheet.
        """"""
        key = (os.path.abspath(file_name), tab)
        if key not in cls._cache:
            cls._cache[key] = pd.read_excel(file_name, sheet_name=tab)
        return cls._cache[key]

    @classmethod
    def read_opyxl(cls, file_name: str):
        """"""
        Load an Excel workbook using openpyxl.

        Parameters
        ----------
        file_name : str
            Path to the Excel file.

        Returns
        -------
        openpyxl.workbook.workbook.Workbook
            The loaded workbook object.
        """"""
        return openpyxl.load_workbook(file_name)

    @classmethod
    def clean(cls):
        """"""
        Clear the internal cache of loaded sheets.
        """"""
        cls._cache.clear()
```"
222265,box/flaky,box_flaky/flaky/multiprocess_string_io.py,flaky.multiprocess_string_io.MultiprocessingStringIO,"import multiprocessing

class MultiprocessingStringIO:
    """"""
    Provide a StringIO-like interface to the multiprocessing ListProxy. The
    multiprocessing ListProxy needs to be instantiated before the flaky plugin
    is configured, so the list is created as a class variable.
    """"""
    _manager = multiprocessing.Manager()
    proxy = _manager.list()

    def getvalue(self):
        """"""
        Shadow the StringIO.getvalue method.
        """"""
        return ''.join((i for i in self.proxy))

    def writelines(self, content_list):
        """"""
        Shadow the StringIO.writelines method. Ingests a list and
        translates that to a string
        """"""
        for item in content_list:
            self.write(item)

    def write(self, content):
        """"""
        Shadow the StringIO.write method.
        """"""
        content.strip('\n')
        self.proxy.append(content)","
class MultiprocessingStringIO:
    
    def getvalue(self):
        
        pass
    def writelines(self, content_list):
        
        pass

    def writelines(self, content_list):
        
        pass",snippet_123,4,"```python
import multiprocessing

class MultiprocessingStringIO:
    def __init__(self):
        self._manager = multiprocessing.Manager()
        self._lines = self._manager.list()
        self._lock = self._manager.Lock()

    def getvalue(self):
        with self._lock:
            return ''.join(self._lines)

    def writelines(self, content_list):
        with self._lock:
            for line in content_list:
                self._lines.append(line)
```"
222348,joshspeagle/dynesty,joshspeagle_dynesty/py/dynesty/utils.py,dynesty.utils.DelayTimer,"import time

class DelayTimer:
    """""" Utility class that allows us to detect a certain
    time has passed""""""

    def __init__(self, delay):
        """""" Initialise the time with delay of dt seconds

        Parameters
        ----------

        delay: float
            The number of seconds in the timer
        """"""
        self.delay = delay
        self.last_time = time.time()

    def is_time(self):
        """"""
        Returns true if more than self.dt seconds has passed
        since the initialization or last call of successful is_time()

        Returns
        -------
        ret: bool
             True if specified amout of time has passed since the
             initialization or last successful is_time() call
        """"""
        curt = time.time()
        if curt - self.last_time > self.delay:
            self.last_time = curt
            return True
        return False","
class DelayTimer:
    
    def __init__(self, delay):
        
        pass
    def is_time(self):
        
        pass",snippet_124,3,"```python
import time

class DelayTimer:
    def __init__(self, delay):
        self.delay = delay
        self.start = time.time()

    def is_time(self):
        return time.time() >= self.start + self.delay
```"
223977,jilljenn/tryalgo,jilljenn_tryalgo/tryalgo/fenwick.py,tryalgo.fenwick.FenwickMin,"class FenwickMin:
    """"""maintains a tree to allow quick updates and queries
    of a virtual table t
    """"""

    def __init__(self, size):
        """"""stores a table t and allows updates and queries
        of prefix sums in logarithmic time.

        :param size: length of the table
        """"""
        self.s = [float('+inf')] * (size + 1)

    def prefixMin(self, a):
        """"""
        :param int a: index in t, negative a will return infinity
        :returns: min(t[0], ... ,t[a])
        """"""
        i = a + 1
        retval = float('+inf')
        while i > 0:
            retval = min(retval, self.s[i])
            i -= i & -i
        return retval

    def update(self, a, val):
        """"""
        :param int a: index in t
        :param val: a value
        :modifies: sets t[a] to the minimum of t[a] and val
        """"""
        i = a + 1
        while i < len(self.s):
            self.s[i] = min(self.s[i], val)
            i += i & -i","class FenwickMin:
    
    def __init__(self, size):
        
        pass
    def prefixMin(self, a):
        
        pass
    def update(self, a, val):
        
        pass",snippet_125,4,"```python
class FenwickMin:
    def __init__(self, size):
        self.n = size
        self.tree = [float('inf')] * (self.n + 1)

    def prefixMin(self, a):
        """"""Return the minimum value in the range [1, a].""""""
        res = float('inf')
        i = a
        while i > 0:
            res = min(res, self.tree[i])
            i -= i & -i
        return res

    def update(self, a, val):
        """"""Set position a to val and update the tree for decreasing values.""""""
        i = a
        while i <= self.n:
            self.tree[i] = min(self.tree[i], val)
            i += i & -i
```"
223981,jilljenn/tryalgo,jilljenn_tryalgo/tryalgo/lowest_common_ancestor.py,tryalgo.lowest_common_ancestor.LowestCommonAncestorRMQ,"from tryalgo.range_minimum_query import RangeMinQuery

class LowestCommonAncestorRMQ:
    """"""Lowest common ancestor data structure using a reduction to
       range minimum query
    """"""

    def __init__(self, graph):
        """"""builds the structure from a given tree

        :param graph: adjacency matrix of a tree
        :complexity: O(n log n), with n = len(graph)
        """"""
        n = len(graph)
        dfs_trace = []
        self.last = [None] * n
        to_visit = [(0, 0, None)]
        succ = [0] * n
        while to_visit:
            level, node, father = to_visit[-1]
            self.last[node] = len(dfs_trace)
            dfs_trace.append((level, node))
            if succ[node] < len(graph[node]) and graph[node][succ[node]] == father:
                succ[node] += 1
            if succ[node] == len(graph[node]):
                to_visit.pop()
            else:
                neighbor = graph[node][succ[node]]
                succ[node] += 1
                to_visit.append((level + 1, neighbor, node))
        self.rmq = RangeMinQuery(dfs_trace, (float('inf'), None))

    def query(self, u, v):
        """""":returns: the lowest common ancestor of u and v
        :complexity: O(log n)
        """"""
        lu = self.last[u]
        lv = self.last[v]
        if lu > lv:
            lu, lv = (lv, lu)
        return self.rmq.range_min(lu, lv + 1)[1]","
class LowestCommonAncestorRMQ:
    
    def __init__(self, graph):
        '''builds the structure from a given tree
        :param graph: adjacency matrix of a tree
        :complexity: O(n log n), with n = len(graph)
        '''
        pass

    def query(self, u, v):
        
        pass",snippet_126,3,"```python
import math
import sys

class LowestCommonAncestorRMQ:
    def __init__(self, graph):
        """"""
        builds the structure from a given tree
        :param graph: adjacency matrix of a tree
        :complexity: O(n log n), with n = len(graph)
        """"""
        n = len(graph)
        # build adjacency list
        self.adj = [[] for _ in range(n)]
        for i in range(n):
            for j in range(n):
                if graph[i][j]:
                    self.adj[i].append(j)

        # Euler tour
        self.euler = []
        self.depth = []
        self.first = [-1] * n

        sys.setrecursionlimit(max(1000, n * 2))

        def dfs(u, p, d):
            self.first[u] = len(self.euler)
            self.euler.append(u)
            self.depth.append(d)
            for v in self.adj[u]:
                if v == p:
                    continue
                dfs(v, u, d + 1)
                self.euler.append(u)
                self.depth.append(d)

        dfs(0, -1, 0)

        m = len(self.euler)
        # log table
        self.log = [0] * (m + 1)
        for i in range(2, m + 1):
            self.log[i] = self.log[i // 2] + 1

        K = self.log[m] + 1
        # sparse table of indices into euler array
        self.st = [[0] * m for _ in range(K)]
        for i in range(m):
            self.st[0][i] = i
        for k in range(1, K):
            span = 1 << k
            half = span >> 1
            for i in range(m - span + 1):
                left = self.st[k - 1][i]
                right = self.st[k - 1][i + half]
                if self.depth[left] <= self.depth[right]:
                    self.st[k][i] = left
                else:
                    self.st[k][i] = right

    def query(self, u, v):
        """"""
        returns the lowest common ancestor of nodes u and v
        """"""
        if u == v:
            return u
        l = self.first[u]
        r = self.first[v]
        if l > r:
            l, r = r, l
        length = r - l + 1
        k = self.log[length]
        left = self.st[k][l]
        right = self.st[k][r - (1 << k) + 1]
        idx = left if self.depth[left] <= self.depth[right] else right
        return self.euler[idx]
```"
223982,jilljenn/tryalgo,jilljenn_tryalgo/tryalgo/lowest_common_ancestor.py,tryalgo.lowest_common_ancestor.LowestCommonAncestorShortcuts,"class LowestCommonAncestorShortcuts:
    """"""Lowest common ancestor data structure using shortcuts to ancestors
    """"""

    def __init__(self, prec):
        """"""builds the structure from a given tree

        :param prec: father for every node, with prec[0] = 0
        :assumes: prec[node] < node
        :complexity: O(n log n), with n = len(nodes)
        """"""
        n = len(prec)
        self.level = [None] * n
        self.level[0] = 0
        for u in range(1, n):
            self.level[u] = 1 + self.level[prec[u]]
        depth = log2ceil(max((self.level[u] for u in range(n)))) + 1
        self.anc = [[0] * n for _ in range(depth)]
        for u in range(n):
            self.anc[0][u] = prec[u]
        for k in range(1, depth):
            for u in range(n):
                self.anc[k][u] = self.anc[k - 1][self.anc[k - 1][u]]

    def query(self, u, v):
        """""":returns: the lowest common ancestor of u and v
        :complexity: O(log n)
        """"""
        if self.level[u] > self.level[v]:
            u, v = (v, u)
        depth = len(self.anc)
        for k in range(depth - 1, -1, -1):
            if self.level[u] <= self.level[v] - (1 << k):
                v = self.anc[k][v]
        assert self.level[u] == self.level[v]
        if u == v:
            return u
        for k in range(depth - 1, -1, -1):
            if self.anc[k][u] != self.anc[k][v]:
                u = self.anc[k][u]
                v = self.anc[k][v]
        assert self.anc[0][u] == self.anc[0][v]
        return self.anc[0][u]","class LowestCommonAncestorShortcuts:
    
    def __init__(self, prec):
        '''builds the structure from a given tree
        :param prec: father for every node, with prec[0] = 0
        :assumes: prec[node] < node
        :complexity: O(n log n), with n = len(nodes)
        '''
        pass

    def query(self, u, v):
        
        pass",snippet_127,3,"```python
import math

class LowestCommonAncestorShortcuts:
    def __init__(self, prec):
        """"""
        builds the structure from a given tree
        :param prec: father for every node, with prec[0] = 0
        :assumes: prec[node] < node
        :complexity: O(n log n), with n = len(nodes)
        """"""
        self.n = len(prec)
        self.prec = prec
        # depth array
        self.depth = [0] * self.n
        for i in range(1, self.n):
            self.depth[i] = self.depth[prec[i]] + 1

        # binary lifting table
        self.LOG = math.ceil(math.log2(self.n)) if self.n > 1 else 1
        self.up = [[0] * self.n for _ in range(self.LOG)]
        # level 0
        for i in range(self.n):
            self.up[0][i] = prec[i]
        # higher levels
        for k in range(1, self.LOG):
            up_k = self.up[k]
            up_k_1 = self.up[k-1]
            for i in range(self.n):
                up_k[i] = up_k_1[up_k_1[i]]

    def query(self, u, v):
        """"""
        returns the lowest common ancestor of nodes u and v
        """"""
        if self.depth[u] < self.depth[v]:
            u, v = v, u
        # lift u up to depth of v
        diff = self.depth[u] - self.depth[v]
        bit = 0
        while diff:
            if diff & 1:
                u = self.up[bit][u]
            diff >>= 1
            bit += 1
        if u == v:
            return u
        # lift both up together
        for k in range(self.LOG - 1, -1, -1):
            if self.up[k][u] != self.up[k][v]:
                u = self.up[k][u]
                v = self.up[k][v]
        return self.prec[u]
```"
223988,jilljenn/tryalgo,jilljenn_tryalgo/tryalgo/partition_refinement.py,tryalgo.partition_refinement.PartitionRefinement,"class PartitionRefinement:
    """"""This data structure implements an order preserving
    partition with refinements.
    """"""

    def __init__(self, n):
        """"""Start with the partition consisting of the unique class {0,1,..,n-1}
        complexity: O(n) both in time and space
        """"""
        c = PartitionClass()
        self.classes = c
        self.items = [PartitionItem(i, c) for i in range(n)]

    def refine(self, pivot):
        """"""Split every class C in the partition into C intersection pivot
        and C setminus pivot complexity: linear in size of pivot
        """"""
        has_split = []
        for i in pivot:
            if 0 <= i < len(self.items):
                x = self.items[i]
                c = x.theclass
                if not c.split:
                    c.split = PartitionClass(c)
                    if self.classes is c:
                        self.classes = c.split
                    has_split.append(c)
                x.remove()
                x.theclass = c.split
                c.split.append(x)
        for c in has_split:
            c.split = None
            if not c.items:
                c.remove()
                del c

    def tolist(self):
        """"""produce a list representation of the partition
        """"""
        return [[x.val for x in theclass.items] for theclass in self.classes]

    def order(self):
        """"""Produce a flatten list of the partition, ordered by classes
        """"""
        return [x.val for theclass in self.classes for x in theclass.items]","class PartitionRefinement:
    
    def __init__(self, n):
        
        pass
    def refine(self, pivot):
        
        pass
    def tolist(self):
        
        pass

    def order(self):
        
        pass",snippet_128,5,"```python
class PartitionRefinement:
    """"""
    A simple partition refinement data structure.

    The partition is represented as a list of blocks, each block being a list of elements.
    The initial partition contains a single block with all elements from 0 to n-1.

    Methods
    -------
    refine(pivot)
        Refines the current partition by splitting each block into the intersection
        with the pivot set and the difference. The pivot can be any iterable of
        elements. If an element is not present in the current partition it is ignored.
    tolist()
        Returns a shallow copy of the current partition as a list of lists.
    order()
        Returns a flat list of all elements in the order induced by the partition.
    """"""

    def __init__(self, n):
        # Start with a single block containing all elements 0..n-1
        self.blocks = [list(range(n))]

    def refine(self, pivot):
        # Convert pivot to a set for O(1) membership tests
        pivot_set = set(pivot)
        new_blocks = []
        for block in self.blocks:
            # Compute intersection and difference
            inter = [x for x in block if x in pivot_set]
            diff = [x for x in block if x not in pivot_set]
            # Replace block with the two parts if both are non-empty
            if inter and diff:
                new_blocks.append(inter)
                new_blocks.append(diff)
            else:
                # Keep the block unchanged if it is not split
                new_blocks.append(block)
        self.blocks = new_blocks

    def tolist(self):
        # Return a shallow copy of the blocks list
        return [list(b) for b in self.blocks]

    def order(self):
        # Flatten the blocks into a single list preserving block order
        return [x for block in self.blocks for x in block]
```"
224105,rigetti/grove,rigetti_grove/grove/alpha/fermion_transforms/fenwick_tree.py,grove.alpha.fermion_transforms.fenwick_tree.FenwickNode,"class FenwickNode:
    """"""Fenwick Tree node.""""""
    parent = None
    children = None
    index = None

    def __init__(self, parent, children, index=None):
        """"""Fenwick Tree node. Single parent and multiple children.

        :param FenwickNode parent: a parent node
        :param list(FenwickNode) children: a list of children nodes
        :param int index: node label
        """"""
        self.children = children
        self.parent = parent
        self.index = index

    def get_ancestors(self):
        """"""Returns a list of ancestors of the node. Ordered from the earliest.

        :return: node's ancestors, ordered from most recent
        :rtype: list(FenwickNode)
        """"""
        node = self
        ancestor_list = []
        while node.parent is not None:
            ancestor_list.append(node.parent)
            node = node.parent
        return ancestor_list","class FenwickNode:
    
    def __init__(self, parent, children, index=None):
        
        pass
    def get_ancestors(self):
        
        pass",snippet_129,3,"```python
class FenwickNode:
    """"""
    A node used in a Fenwick (Binary Indexed) tree representation.
    Each node keeps a reference to its parent, a list of its children,
    and an optional index that can be used to identify the node.
    """"""

    def __init__(self, parent, children, index=None):
        """"""
        Parameters
        ----------
        parent : FenwickNode or None
            Reference to the parent node.  ``None`` indicates that this
            node is the root of the tree.
        children : list[FenwickNode]
            List of child nodes.  The list may be empty for leaf nodes.
        index : int, optional
            An optional identifier for the node.  It is not used by the
            tree logic but can be helpful for debugging or external
            mapping.
        """"""
        self.parent = parent
        self.children = children
        self.index = index

        # If a parent is supplied, automatically add this node to its
        # children list.  This keeps the tree structure consistent
        # regardless of whether the caller added the node manually.
        if parent is not None and self not in parent.children:
            parent.children.append(self)

    def get_ancestors(self):
        """"""
        Return a list of ancestor nodes from the root down to the
        immediate parent of this node.

        Returns
        -------
        list[FenwickNode]
            Ordered list of ancestors.  The first element is the root
            node, and the last element is the direct parent of this
            node.  If the node is the root, an empty list is returned.
        """"""
        ancestors = []
        node = self.parent
        while node is not None:
            ancestors.append(node)
            node = node.parent
        ancestors.reverse()
        return ancestors
```"
227838,econ-ark/HARK,HARK/utilities.py,HARK.utilities.NullFunc,"import numpy as np

class NullFunc:
    """"""
    A trivial class that acts as a placeholder ""do nothing"" function.
    """"""

    def __call__(self, *args):
        """"""
        Returns meaningless output no matter what the input(s) is.  If no input,
        returns None.  Otherwise, returns an array of NaNs (or a single NaN) of
        the same size as the first input.
        """"""
        if len(args) == 0:
            return None
        else:
            arg = args[0]
            if hasattr(arg, 'shape'):
                return np.zeros_like(arg) + np.nan
            else:
                return np.nan

    def distance(self, other):
        """"""
        Trivial distance metric that only cares whether the other object is also
        an instance of NullFunc.  Intentionally does not inherit from HARKobject
        as this might create dependency problems.

        Parameters
        ----------
        other : any
            Any object for comparison to this instance of NullFunc.

        Returns
        -------
        (unnamed) : float
            The distance between self and other.  Returns 0 if other is also a
            NullFunc; otherwise returns an arbitrary high number.
        """"""
        try:
            if other.__class__ is self.__class__:
                return 0.0
            else:
                return 1000.0
        except:
            return 10000.0","
class NullFunc:
    
    def __call__(self, *args):
        
        pass
    def distance(self, other):
        
        pass",snippet_131,3,"class NullFunc:
    def __call__(self, *args, **kwargs):
        return None

    def distance(self, other):
        return 0 if isinstance(other, NullFunc) else 1"
228145,jcrobak/parquet-python,jcrobak_parquet-python/parquet/__init__.py,parquet.JsonWriter,"import json

class JsonWriter:
    """"""Utility for dumping rows as JSON objects.""""""

    def __init__(self, out):
        """"""Initialize with output destination.""""""
        self._out = out

    def writerow(self, row):
        """"""Write a single row.""""""
        json_text = json.dumps(row)
        if isinstance(json_text, bytes):
            json_text = json_text.decode('utf-8')
        self._out.write(json_text)
        self._out.write(u'\n')","
class JsonWriter:
    
    def __init__(self, out):
        
        pass
    def writerow(self, row):
        
        pass",snippet_132,3,"```python
import json
import os
from pathlib import Path

class JsonWriter:
    """"""
    A simple JSON writer that writes each row as a JSON object on a separate line.
    The `out` parameter can be a file path (string or Path) or an already opened
    file-like object supporting the `write` method.
    """"""

    def __init__(self, out):
        """"""
        Initialize the writer.

        Parameters
        ----------
        out : str, Path, or file-like
            Destination for the JSON output. If a string or Path is provided,
            a file will be opened for writing. If a file-like object is provided,
            it will be used directly.
        """"""
        if isinstance(out, (str, Path)):
            # Ensure parent directories exist
            path = Path(out)
            path.parent.mkdir(parents=True, exist_ok=True)
            self._file = open(path, ""w"", encoding=""utf-8"")
            self._close_on_exit = True
        else:
            # Assume file-like object
            self._file = out
            self._close_on_exit = False

    def writerow(self, row):
        """"""
        Write a single row to the JSON output.

        Parameters
        ----------
        row : Any
            The data to serialize. Typically a dict or list. It must be
            JSON‑serializable.
        """"""
        json_line = json.dumps(row, ensure_ascii=False)
        self._file.write(json_line + ""\n"")

    def close(self):
        """"""Close the underlying file if it was opened by this writer.""""""
        if self._close_on_exit:
            self._file.close()

    # Context manager support
    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()
```"
228682,common-workflow-language/cwltool,common-workflow-language_cwltool/cwltool/software_requirements.py,cwltool.software_requirements.DependenciesConfiguration,"import os
from typing import TYPE_CHECKING, Any, Optional, Union, cast
import argparse

class DependenciesConfiguration:
    """"""Dependency configuration class, for RuntimeContext.job_script_provider.""""""

    def __init__(self, args: argparse.Namespace) -> None:
        """"""Initialize.""""""
        self.tool_dependency_dir: Optional[str] = None
        self.dependency_resolvers_config_file: Optional[str] = None
        conf_file = getattr(args, 'beta_dependency_resolvers_configuration', None)
        tool_dependency_dir = getattr(args, 'beta_dependencies_directory', None)
        conda_dependencies = getattr(args, 'beta_conda_dependencies', None)
        if conf_file is not None and os.path.exists(conf_file):
            self.use_tool_dependencies = True
            if tool_dependency_dir is None:
                tool_dependency_dir = os.path.abspath(os.path.dirname(conf_file))
            self.tool_dependency_dir = tool_dependency_dir
            self.dependency_resolvers_config_file = os.path.abspath(conf_file)
        elif conda_dependencies is not None:
            if tool_dependency_dir is None:
                tool_dependency_dir = os.path.abspath('./cwltool_deps')
            self.tool_dependency_dir = tool_dependency_dir
            self.use_tool_dependencies = True
            self.dependency_resolvers_config_file = None
        else:
            self.use_tool_dependencies = False
        if self.tool_dependency_dir and (not os.path.exists(self.tool_dependency_dir)):
            os.makedirs(self.tool_dependency_dir)

    def build_job_script(self, builder: 'Builder', command: list[str]) -> str:
        """"""Use the galaxy-tool-util library to construct a build script.""""""
        ensure_galaxy_lib_available()
        resolution_config_dict = {'use': self.use_tool_dependencies, 'default_base_path': self.tool_dependency_dir}
        app_config = {'conda_auto_install': True, 'conda_auto_init': True, 'debug': builder.debug}
        tool_dependency_manager: 'deps.DependencyManager' = deps.build_dependency_manager(app_config_dict=app_config, resolution_config_dict=resolution_config_dict, conf_file=self.dependency_resolvers_config_file)
        handle_dependencies: str = ''
        if (dependencies := get_dependencies(builder)):
            handle_dependencies = '\n'.join(tool_dependency_manager.dependency_shell_commands(dependencies, job_directory=builder.tmpdir))
        template_kwds: dict[str, str] = dict(handle_dependencies=handle_dependencies)
        job_script = COMMAND_WITH_DEPENDENCIES_TEMPLATE.substitute(template_kwds)
        return job_script","
class DependenciesConfiguration:
    
    def __init__(self, args: argparse.Namespace) -> None:
        
        pass
    def build_job_script(self, builder: 'Builder', command: list[str]) -> str:
        
        pass",snippet_133,3,"```python
import argparse
from typing import List, Dict, Any


class DependenciesConfiguration:
    """"""
    Handles dependency installation and job script generation.

    Parameters
    ----------
    args : argparse.Namespace
        Namespace containing optional dependency configuration arguments.
        Supported attributes (all optional):
            - requirements_file : str
                Path to a pip requirements file.
            - conda_env_file : str
                Path to a conda environment YAML file.
            - pip_packages : List[str]
                List of pip packages to install.
            - conda_packages : List[str]
                List of conda packages to install.
            - env_vars : Dict[str, str]
                Environment variables to export before running the job.
    """"""

    def __init__(self, args: argparse.Namespace) -> None:
        self.args = args
        self.requirements_file: str | None = getattr(args, ""requirements_file"", None)
        self.conda_env_file: str | None = getattr(args, ""conda_env_file"", None)
        self.pip_packages: List[str] | None = getattr(args, ""pip_packages"", None)
        self.conda_packages: List[str] | None = getattr(args, ""conda_packages"", None)
        self.env_vars: Dict[str, str] | None = getattr(args, ""env_vars"", {})

    def build_job_script(self, builder: Any, command: List[str]) -> str:
        """"""
        Build a shell script that installs dependencies and runs the given command.

        Parameters
        ----------
        builder : Any
            An object that may provide context such as the working directory.
            If it has a ``get_working_dir`` method, the script will change to that
            directory before executing the command.
        command : List[str]
            The command to run, split into a list of arguments.

        Returns
        -------
        str
            The complete job script as a string.
        """"""
        script_lines: List[str] = []

        # Export environment variables
        if self.env_vars:
            for key, value in self.env_vars.items():
                script_lines.append(f""export {key}={value}"")

        # Change to the builder's working directory if available
        if hasattr(builder, ""get_working_dir""):
            try:
                cwd = builder.get_working_dir()
                if cwd:
                    script_lines.append(f""cd {cwd}"")
            except Exception:
                pass

        # Conda environment creation and activation
        if self.conda_env_file:
            script_lines.append(f""conda env create -f {self.conda_env_file}"")
            # Attempt to activate the environment based on the YAML file name
            env_name = (
                self.conda_env_file.split(""/"")[-1].split(""."")[0]
                if ""/"" in self.conda_env_file
                else self.conda_env_file.split(""."")[0]
            )
            script_lines.append(f""conda activate {env_name}"")

        # Install pip packages from requirements file
        if self.requirements_file:
            script_lines.append(f""pip install -r {self.requirements_file}"")

        # Install additional pip packages
        if self.pip_packages:
            script_lines.append(f""pip install {' '.join(self.pip_packages)}"")

        # Install conda packages
        if self.conda_packages:
            script_lines.append(f""conda install -y {' '.join(self.conda_packages)}"")

        # Finally, run the command
        script_lines.append("" "".join(command))

        return ""\n"".join(script_lines)
```"
229565,sourceperl/pyModbusTCP,sourceperl_pyModbusTCP/examples/client_serial_gw.py,client_serial_gw.Serial2ModbusClient,"from pyModbusTCP.constants import EXP_GATEWAY_TARGET_DEVICE_FAILED_TO_RESPOND
import struct

class Serial2ModbusClient:
    """""" Customize a slave serial worker for map a modbus TCP client. """"""

    def __init__(self, serial_w, mbus_cli, slave_addr=1, allow_bcast=False):
        """"""Serial2ModbusClient constructor.

        :param serial_w: a SlaveSerialWorker instance
        :type serial_w: SlaveSerialWorker
        :param mbus_cli: a ModbusClient instance
        :type mbus_cli: ModbusClient
        :param slave_addr: modbus slave address
        :type slave_addr: int
        :param allow_bcast: allow processing broadcast frames (slave @0)
        :type allow_bcast: bool
        """"""
        self.serial_w = serial_w
        self.mbus_cli = mbus_cli
        self.slave_addr = slave_addr
        self.allow_bcast = allow_bcast
        self.serial_w.handle_request = self._handle_request

    def _handle_request(self):
        """"""Request handler for SlaveSerialWorker""""""
        if self.serial_w.request.slave_addr == 0 and self.allow_bcast:
            self.mbus_cli.custom_request(self.serial_w.request.pdu)
        elif self.serial_w.request.slave_addr == self.slave_addr:
            resp_pdu = self.mbus_cli.custom_request(self.serial_w.request.pdu)
            if resp_pdu:
                self.serial_w.response.build(raw_pdu=resp_pdu, slave_addr=self.serial_w.request.slave_addr)
            else:
                exp_pdu = struct.pack('BB', self.serial_w.request.function_code + 128, EXP_GATEWAY_TARGET_DEVICE_FAILED_TO_RESPOND)
                self.serial_w.response.build(raw_pdu=exp_pdu, slave_addr=self.serial_w.request.slave_addr)

    def run(self):
        """"""Start serial processing.""""""
        self.serial_w.run()","
class Serial2ModbusClient:
    
    def __init__(self, serial_w, mbus_cli, slave_addr=1, allow_bcast=False):
        
        pass
    def _handle_request(self):
        
        pass

    def run(self):
        
        pass",snippet_134,4,"```python
class Serial2ModbusClient:
    """"""
    A simple bridge that reads Modbus requests from a serial-like object,
    forwards them to a Modbus client, and writes back the response.
    """"""

    def __init__(self, serial_w, mbus_cli, slave_addr=1, allow_bcast=False):
        """"""
        Parameters
        ----------
        serial_w : file-like object
            An object that supports `readline()` and `write()` for serial communication.
        mbus_cli : object
            An object that implements a `send_request(bytes) -> bytes` method.
        slave_addr : int, optional
            The Modbus slave address this bridge should accept. Default is 1.
        allow_bcast : bool, optional
            If True, broadcast requests (address 0) are forwarded to the Modbus client.
            Default is False.
        """"""
        self.serial_w = serial_w
        self.mbus_cli = mbus_cli
        self.slave_addr = slave_addr
        self.allow_bcast = allow_bcast
        self._running = False

    def _handle_request(self):
        """"""
        Read a single Modbus request from the serial interface, forward it to the
        Modbus client, and write the response back to the serial interface.
        """"""
        # Read a line from the serial interface
        raw = self.serial_w.readline()
        if not raw:
            return

        # Strip whitespace and convert to bytes
        line = raw.strip()
        if not line:
            return

        try:
            # Accept both bytes and str input
            if isinstance(line, bytes):
                line = line.decode(errors=""ignore"")
            req_bytes = bytes.fromhex(line)
        except Exception:
            # Invalid hex string; ignore
            return

        if not req_bytes:
            return

        # Check slave address
        addr = req_bytes[0]
        if addr != self.slave_addr and not (self.allow_bcast and addr == 0):
            return

        # Forward request to Modbus client
        try:
            resp = self.mbus_cli.send_request(req_bytes)
        except Exception:
            resp = None

        if resp is None:
            return

        # For broadcast requests, no response is expected unless allowed
        if addr == 0 and not self.allow_bcast:
            return

        # Write response back to serial interface
        try:
            hex_resp = resp.hex().upper()
            self.serial_w.write((hex_resp + ""\n"").encode())
        except Exception:
            # Ignore write errors
            pass

    def run(self):
        """"""
        Continuously read from the serial interface and process requests.
        """"""
        self._running = True
        while self._running:
            try:
                self._handle_request()
            except Exception:
                # Silently ignore any unexpected errors to keep the loop running
                pass

    def stop(self):
        """"""
        Stop the run loop.
        """"""
        self._running = False
```"
230113,aws/aws-xray-sdk-python,aws_aws-xray-sdk-python/aws_xray_sdk/core/models/noop_traceid.py,aws_aws-xray-sdk-python.aws_xray_sdk.core.models.noop_traceid.NoOpTraceId,"class NoOpTraceId:
    """"""
    A trace ID tracks the path of a request through your application.
    A trace collects all the segments generated by a single request.
    A trace ID is required for a segment.
    """"""
    VERSION = '1'
    DELIMITER = '-'

    def __init__(self):
        """"""
        Generate a no-op trace id.
        """"""
        self.start_time = '00000000'
        self.__number = '000000000000000000000000'

    def to_id(self):
        """"""
        Convert TraceId object to a string.
        """"""
        return '%s%s%s%s%s' % (NoOpTraceId.VERSION, NoOpTraceId.DELIMITER, self.start_time, NoOpTraceId.DELIMITER, self.__number)","class NoOpTraceId:
    
    def __init__(self):
        
        pass
    def to_id(self):
        
        pass",snippet_135,3,"class NoOpTraceId:
    def __init__(self):
        self._id = None

    def to_id(self):
        return self._id"
230120,aws/aws-xray-sdk-python,aws_aws-xray-sdk-python/aws_xray_sdk/core/models/traceid.py,aws_aws-xray-sdk-python.aws_xray_sdk.core.models.traceid.TraceId,"import binascii
import os
import time

class TraceId:
    """"""
    A trace ID tracks the path of a request through your application.
    A trace collects all the segments generated by a single request.
    A trace ID is required for a segment.
    """"""
    VERSION = '1'
    DELIMITER = '-'

    def __init__(self):
        """"""
        Generate a random trace id.
        """"""
        self.start_time = int(time.time())
        self.__number = binascii.b2a_hex(os.urandom(12)).decode('utf-8')

    def to_id(self):
        """"""
        Convert TraceId object to a string.
        """"""
        return '%s%s%s%s%s' % (TraceId.VERSION, TraceId.DELIMITER, format(self.start_time, 'x'), TraceId.DELIMITER, self.__number)","
class TraceId:
    
    def __init__(self):
        
        pass
    def to_id(self):
        
        pass",snippet_136,3,"```python
import uuid

class TraceId:
    def __init__(self, id=None):
        """"""
        Create a new TraceId instance.

        Parameters
        ----------
        id : str or None, optional
            If provided, use this value as the trace ID. Otherwise, generate a
            new random UUID4 hex string.
        """"""
        if id is None:
            self._id = uuid.uuid4().hex
        else:
            self._id = str(id)

    def to_id(self):
        """"""
        Return the trace ID as a string.
        """"""
        return self._id
```"
230123,aws/aws-xray-sdk-python,aws_aws-xray-sdk-python/aws_xray_sdk/core/sampling/local/reservoir.py,aws_aws-xray-sdk-python.aws_xray_sdk.core.sampling.local.reservoir.Reservoir,"import time
import threading

class Reservoir:
    """"""
    Keeps track of the number of sampled segments within
    a single second. This class is implemented to be
    thread-safe to achieve accurate sampling.
    """"""

    def __init__(self, traces_per_sec=0):
        """"""
        :param int traces_per_sec: number of guranteed
            sampled segments.
        """"""
        self._lock = threading.Lock()
        self.traces_per_sec = traces_per_sec
        self.used_this_sec = 0
        self.this_sec = int(time.time())

    def take(self):
        """"""
        Returns True if there are segments left within the
        current second, otherwise return False.
        """"""
        with self._lock:
            now = int(time.time())
            if now != self.this_sec:
                self.used_this_sec = 0
                self.this_sec = now
            if self.used_this_sec >= self.traces_per_sec:
                return False
            self.used_this_sec = self.used_this_sec + 1
            return True","
class Reservoir:
    
    def __init__(self, traces_per_sec=0):
        
        pass
    def take(self):
        
        pass",snippet_137,3,"```python
import random
from collections import deque

class Reservoir:
    """"""
    A simple reservoir sampler that keeps at most `traces_per_sec` items.
    New items are added via the `add` method, and items can be retrieved
    randomly using the `take` method.
    """"""

    def __init__(self, traces_per_sec=0):
        """"""
        Initialize the reservoir.

        Parameters
        ----------
        traces_per_sec : int, optional
            The maximum number of traces to keep in the reservoir.
            Defaults to 0 (no capacity).
        """"""
        self.capacity = max(0, int(traces_per_sec))
        self._reservoir = deque()
        self._total_seen = 0

    def add(self, trace):
        """"""
        Add a new trace to the reservoir using reservoir sampling.

        Parameters
        ----------
        trace : any
            The trace to add.
        """"""
        self._total_seen += 1
        if len(self._reservoir) < self.capacity:
            self._reservoir.append(trace)
        else:
            # Replace an existing item with probability capacity / total_seen
            if self.capacity == 0:
                return
            if random.randint(1, self._total_seen) <= self.capacity:
                idx = random.randint(0, self.capacity - 1)
                self._reservoir[idx] = trace

    def take(self):
        """"""
        Retrieve and remove a random trace from the reservoir.

        Returns
        -------
        any or None
            A randomly selected trace, or None if the reservoir is empty.
        """"""
        if not self._reservoir:
            return None
        idx = random.randint(0, len(self._reservoir) - 1)
        # Pop the element at idx
        # Since deque doesn't support direct indexing, convert to list temporarily
        temp_list = list(self._reservoir)
        trace = temp_list.pop(idx)
        self._reservoir = deque(temp_list)
        return trace

    def __len__(self):
        """"""Return the current number of traces stored.""""""
        return len(self._reservoir)

    def __iter__(self):
        """"""Iterate over the traces in the reservoir.""""""
        return iter(self._reservoir)
```"
230719,django-salesforce/django-salesforce,django-salesforce_django-salesforce/salesforce/backend/indep.py,salesforce.backend.indep.LazyField,"from typing import Any, Callable, Dict, Tuple, Type
from inspect import signature

class LazyField:
    """"""A Field that can be later customized until it is binded to the final Model""""""
    counter = 0

    def __init__(self, klass: 'Type[Field[Any, Any]]') -> None:
        """"""Instantiate the field type""""""
        self.klass = klass
        self.kw = {}
        self.args = ()
        self.called = False
        self.counter = self.counter

    def __call__(self, *args: Any, **kwargs: Any) -> 'LazyField':
        """"""Instantiate a new field with options""""""
        assert not self.called
        bound_args = signature(self.klass.__init__).bind(self, *args, **kwargs)
        obj = type(self)(self.klass)
        obj.args = bound_args.args[1:]
        obj.kw = bound_args.kwargs
        setattr(type(self), 'counter', getattr(type(self), 'counter') + 1)
        return obj

    def update(self, **kwargs: Any) -> 'LazyField':
        """"""Customize the lazy field""""""
        assert not self.called
        self.kw.update(kwargs)
        return self

    def create(self) -> 'Field[Any, Any]':
        """"""Create a normal field from the lazy field""""""
        assert not self.called
        return self.klass(*self.args, **self.kw)","
class LazyField:
    
    def __init__(self, klass: 'Type[Field[Any, Any]]') -> None:
        
        pass
    def __call__(self, *args: Any, **kwargs: Any) -> 'LazyField':
        
        pass

    def update(self, **kwargs: Any) -> 'LazyField':
        
        pass

    def create(self) -> 'Field[Any, Any]':
        
        pass",snippet_139,5,"```python
from typing import Any, Type

class LazyField:
    """"""
    A helper that defers the creation of a Field instance until `create` is called.
    It stores the field class, positional arguments, and keyword arguments.
    """"""

    def __init__(self, klass: Type[Any]) -> None:
        """"""
        Initialize with the field class to instantiate later.

        :param klass: The class of the field to be created.
        """"""
        self._klass = klass
        self._args: tuple[Any, ...] = ()
        self._kwargs: dict[str, Any] = {}

    def __call__(self, *args: Any, **kwargs: Any) -> ""LazyField"":
        """"""
        Store positional and keyword arguments for the field.

        :param args: Positional arguments for the field constructor.
        :param kwargs: Keyword arguments for the field constructor.
        :return: Self, to allow chaining.
        """"""
        self._args = args
        self._kwargs.update(kwargs)
        return self

    def update(self, **kwargs: Any) -> ""LazyField"":
        """"""
        Update or add keyword arguments for the field.

        :param kwargs: Keyword arguments to merge into the existing ones.
        :return: Self, to allow chaining.
        """"""
        self._kwargs.update(kwargs)
        return self

    def create(self) -> Any:
        """"""
        Instantiate and return the field using the stored class, args, and kwargs.

        :return: An instance of the field class.
        """"""
        return self._klass(*self._args, **self._kwargs)
```"
231223,weblyzard/inscriptis,benchmarking/run_benchmarking.py,run_benchmarking.AbstractHtmlConverter,"from time import time

class AbstractHtmlConverter:
    """"""
    An abstract HTML convert class.
    """"""

    def get_text(self, html):
        """"""
        Returns:
            a text representation of the given HTML snippet.
        """"""
        raise NotImplementedError

    def benchmark(self, html):
        """"""
        Benchmarks the classes HTML to text converter.

        Returns:
            A tuple of the required time and the obtained text representation.
        """"""
        start_time = time()
        for _ in range(TRIES):
            text = self.get_text(html)
        return (time() - start_time, text)","
class AbstractHtmlConverter:
    
    def get_text(self, html):
        
        pass
    def benchmark(self, html):
        
        pass",snippet_140,3,"```python
import re
import time
from html import unescape

class AbstractHtmlConverter:
    """"""
    A simple HTML to plain‑text converter with a benchmark helper.
    """"""

    _TAG_RE = re.compile(r'<[^>]+>')

    def get_text(self, html: str) -> str:
        """"""
        Convert an HTML string to plain text by removing tags and unescaping entities.
        """"""
        if not isinstance(html, str):
            raise TypeError(""html must be a string"")
        # Remove tags
        text = self._TAG_RE.sub('', html)
        # Unescape HTML entities
        text = unescape(text)
        # Collapse whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def benchmark(self, html: str, iterations: int = 1000) -> float:
        """"""
        Measure the average time (in seconds) taken by get_text over a number of iterations.
        """"""
        if not isinstance(iterations, int) or iterations <= 0:
            raise ValueError(""iterations must be a positive integer"")
        start = time.perf_counter()
        for _ in range(iterations):
            self.get_text(html)
        end = time.perf_counter()
        return (end - start) / iterations
```"
236113,richardchien/python-aiocqhttp,richardchien_python-aiocqhttp/aiocqhttp/api.py,aiocqhttp.api.Api,"from typing import Callable, Any, Union, Awaitable
import functools
import abc

class Api:
    """"""
    API 接口类。

    继承此类的具体实现类应实现 `call_action` 方法。
    """"""

    @abc.abstractmethod
    def call_action(self, action: str, **params) -> Union[Awaitable[Any], Any]:
        """"""
        调用 OneBot API，`action` 为要调用的 API 动作名，`**params`
        为 API 所需参数。

        根据实现类的不同，此函数可能是异步也可能是同步函数。
        """"""
        pass

    def __getattr__(self, item: str) -> Callable[..., Union[Awaitable[Any], Any]]:
        """"""获取一个可调用对象，用于调用对应 API。""""""
        return functools.partial(self.call_action, item)","
class Api:
    
    @abc.abstractmethod
    def call_action(self, action: str, **params) -> Union[Awaitable[Any], Any]:
        
        pass
    def __getattr__(self, item: str) -> Callable[..., Union[Awaitable[Any], Any]]:
        
        pass",snippet_141,3,"```python
import abc
from typing import Any, Awaitable, Callable, Union


class Api(abc.ABC):
    @abc.abstractmethod
    def call_action(self, action: str, **params) -> Union[Awaitable[Any], Any]:
        """"""Execute the given action with the provided parameters.""""""
        ...

    def __getattr__(self, item: str) -> Callable[..., Union[Awaitable[Any], Any]]:
        """"""Return a callable that forwards to `call_action` with the attribute name as the action.""""""
        def _action(**params: Any) -> Union[Awaitable[Any], Any]:
            return self.call_action(item, **params)

        return _action
```"
237136,project-generator/project_generator,project-generator_project_generator/project_generator/project.py,project_generator.project.ProjectTemplate,"class ProjectTemplate:
    """""" Public data which can be set in yaml files
        Yaml data available are:
            'build_dir' : build_dir,    # Build output path
            'debugger' : debugger,      # Debugger
            'export_dir': '',           # Export directory path
            'includes': [],             # include paths
            'linker_file': None,        # linker script file
            'name': name,               # project name
            'macros': [],               # macros
            'misc': {},                 # misc settings related to tools
            'output_type': output_type, # output type, default - exe
            'sources': [],              # source files/folders
            'target': '',               # target
            'template' : [],            # tool template
            'tools_supported': [],      # Tools which are supported,
    """"""

    @staticmethod
    def _get_common_data_template():
        """""" Data for tool specific """"""
        data_template = {'includes': [], 'linker_file': '', 'macros': [], 'sources': []}
        return data_template

    @staticmethod
    def _get_tool_specific_data_template():
        """""" Data for tool specific """"""
        data_template = {'misc': {}, 'template': []}
        return data_template

    @staticmethod
    def get_project_template(name='Default', output_type='exe', debugger=None, build_dir='build'):
        """""" Project data (+ data) """"""
        project_template = {'build_dir': build_dir, 'debugger': debugger, 'export_dir': '', 'name': name, 'output_type': output_type, 'target': '', 'tools_supported': []}
        project_template.update(ProjectTemplate._get_common_data_template())
        project_template.update(ProjectTemplate._get_tool_specific_data_template())
        return project_template","class ProjectTemplate:
    
    @staticmethod
    def _get_common_data_template():
        
        pass
    @staticmethod
    def _get_tool_specific_data_template():
        
        pass
    @staticmethod
    def get_project_template(name='Default', output_type='exe', debugger=None, build_dir='build'):
        
        pass",snippet_142,4,"```python
class ProjectTemplate:
    @staticmethod
    def _get_common_data_template():
        """"""
        Return a dictionary containing the common project data template.
        """"""
        return {
            ""name"": ""Default"",
            ""output_type"": ""exe"",
            ""build_dir"": ""build"",
        }

    @staticmethod
    def _get_tool_specific_data_template():
        """"""
        Return a dictionary containing the tool‑specific project data template.
        """"""
        return {
            ""debugger"": None,
        }

    @staticmethod
    def get_project_template(name='Default', output_type='exe', debugger=None, build_dir='build'):
        """"""
        Build and return a complete project template dictionary.
        """"""
        common = ProjectTemplate._get_common_data_template()
        tool_specific = ProjectTemplate._get_tool_specific_data_template()

        # Override defaults with provided arguments
        common.update({
            ""name"": name,
            ""output_type"": output_type,
            ""build_dir"": build_dir,
        })
        tool_specific.update({
            ""debugger"": debugger,
        })

        # Merge the two dictionaries
        project_template = {**common, **tool_specific}
        return project_template
```"
237503,astropy/photutils,photutils/psf/model_plotting.py,photutils.psf.model_plotting.ModelGridPlotMixin,"from astropy.utils import minversion
import numpy as np
from astropy.visualization import simple_norm
import astropy

class ModelGridPlotMixin:
    """"""
    Mixin class to plot a grid of ePSF models.
    """"""

    def _reshape_grid(self, data):
        """"""
        Reshape the 3D ePSF grid as a 2D array of horizontally and
        vertically stacked ePSFs.

        Parameters
        ----------
        data : `numpy.ndarray`
            The 3D array of ePSF data.

        Returns
        -------
        reshaped_data : `numpy.ndarray`
            The 2D array of ePSF data.
        """"""
        nypsfs = self._ygrid.shape[0]
        nxpsfs = self._xgrid.shape[0]
        ny, nx = self.data.shape[1:]
        data.shape = (nypsfs, nxpsfs, ny, nx)
        return data.transpose([0, 2, 1, 3]).reshape(nypsfs * ny, nxpsfs * nx)

    def plot_grid(self, *, ax=None, vmax_scale=None, peak_norm=False, deltas=False, cmap='viridis', dividers=True, divider_color='darkgray', divider_ls='-', figsize=None):
        """"""
        Plot the grid of ePSF models.

        Parameters
        ----------
        ax : `matplotlib.axes.Axes` or `None`, optional
            The matplotlib axes on which to plot. If `None`, then the
            current `~matplotlib.axes.Axes` instance is used.

        vmax_scale : float, optional
            Scale factor to apply to the image stretch limits. This
            value is multiplied by the peak ePSF value to determine the
            plotting ``vmax``. The defaults are 1.0 for plotting the
            ePSF data and 0.03 for plotting the ePSF difference data
            (``deltas=True``). If ``deltas=True``, the ``vmin`` is set
            to ``-vmax``. If ``deltas=False`` the ``vmin`` is set to
            ``vmax`` / 1e4.

        peak_norm : bool, optional
            Whether to normalize the ePSF data by the peak value. The
            default shows the ePSF flux per pixel.

        deltas : bool, optional
            Set to `True` to show the differences between each ePSF
            and the average ePSF.

        cmap : str or `matplotlib.colors.Colormap`, optional
            The colormap to use. The default is 'viridis'.

        dividers : bool, optional
            Whether to show divider lines between the ePSFs.

        divider_color, divider_ls : str, optional
            Matplotlib color and linestyle options for the divider
            lines between ePSFs. These keywords have no effect unless
            ``show_dividers=True``.

        figsize : (float, float), optional
            The figure (width, height) in inches.

        Returns
        -------
        fig : `matplotlib.figure.Figure`
            The matplotlib figure object. This will be the current
            figure if ``ax=None``. Use ``fig.savefig()`` to save the
            figure to a file.

        Notes
        -----
        This method returns a figure object. If you are using this
        method in a script, you will need to call ``plt.show()`` to
        display the figure. If you are using this method in a Jupyter
        notebook, the figure will be displayed automatically.

        When in a notebook, if you do not store the return value of this
        function, the figure will be displayed twice due to the REPL
        automatically displaying the return value of the last function
        call. Alternatively, you can append a semicolon to the end of
        the function call to suppress the display of the return value.
        """"""
        import matplotlib.pyplot as plt
        from mpl_toolkits.axes_grid1 import make_axes_locatable
        data = self.data.copy()
        if deltas:
            mask = np.zeros(data.shape[0], dtype=bool)
            for i, arr in enumerate(data):
                if np.count_nonzero(arr) == 0:
                    mask[i] = True
            data -= np.mean(data[~mask], axis=0)
            data[mask] = 0.0
        data = self._reshape_grid(data)
        if ax is None:
            if figsize is None and self.meta.get('detector', '') == 'NRCSW':
                figsize = (20, 8)
            fig, ax = plt.subplots(figsize=figsize)
        else:
            fig = plt.gcf()
        if peak_norm and data.max() != 0:
            data /= data.max()
        if deltas:
            if vmax_scale is None:
                vmax_scale = 0.03
            vmax = data.max() * vmax_scale
            vmin = -vmax
            if minversion(astropy, '6.1'):
                norm = simple_norm(data, 'linear', vmin=vmin, vmax=vmax)
            else:
                norm = simple_norm(data, 'linear', min_cut=vmin, max_cut=vmax)
        else:
            if vmax_scale is None:
                vmax_scale = 1.0
            vmax = data.max() * vmax_scale
            vmin = vmax / 10000.0
            if minversion(astropy, '6.1'):
                norm = simple_norm(data, 'log', vmin=vmin, vmax=vmax, log_a=10000.0)
            else:
                norm = simple_norm(data, 'log', min_cut=vmin, max_cut=vmax, log_a=10000.0)
        nypsfs = self._ygrid.shape[0]
        nxpsfs = self._xgrid.shape[0]
        extent = [-0.5, nxpsfs - 0.5, -0.5, nypsfs - 0.5]
        axim = ax.imshow(data, extent=extent, norm=norm, cmap=cmap, origin='lower')
        xticklabels = self._xgrid.astype(int)
        yticklabels = self._ygrid.astype(int)
        if self.meta.get('detector', '') == 'NRCSW':
            xticklabels = list(xticklabels[0:5]) * 4
            yticklabels = list(yticklabels[0:5]) * 2
        ax.set_xticks(np.arange(nxpsfs))
        ax.set_xticklabels(xticklabels)
        ax.set_xlabel('ePSF location in detector X pixels')
        ax.set_yticks(np.arange(nypsfs))
        ax.set_yticklabels(yticklabels)
        ax.set_ylabel('ePSF location in detector Y pixels')
        if dividers:
            for ix in range(nxpsfs - 1):
                ax.axvline(ix + 0.5, color=divider_color, ls=divider_ls)
            for iy in range(nypsfs - 1):
                ax.axhline(iy + 0.5, color=divider_color, ls=divider_ls)
        instrument = self.meta.get('instrument', '')
        if not instrument:
            instrument = self.meta.get('instrume', '')
        detector = self.meta.get('detector', '')
        filtername = self.meta.get('filter', '')
        if isinstance(instrument, (tuple, list, np.ndarray)):
            instrument = instrument[0]
        if isinstance(detector, (tuple, list, np.ndarray)):
            detector = detector[0]
        if isinstance(filtername, (tuple, list, np.ndarray)):
            filtername = filtername[0]
        title = f'{instrument} {detector} {filtername}'
        if title != '':
            title += ' '
        if deltas:
            minus = '−'
            ax.set_title(f'{title}(ePSFs {minus} <ePSF>)')
            if peak_norm:
                label = 'Difference relative to average ePSF peak'
            else:
                label = 'Difference relative to average ePSF values'
        else:
            ax.set_title(f'{title}ePSFs')
            if peak_norm:
                label = 'Scale relative to ePSF peak pixel'
            else:
                label = 'ePSF flux per pixel'
        divider = make_axes_locatable(ax)
        cax_cbar = divider.append_axes('right', size='3%', pad='3%')
        cbar = fig.colorbar(axim, cax=cax_cbar, label=label)
        if not deltas:
            cbar.ax.set_yscale('log')
        if self.meta.get('detector', '') == 'NRCSW':
            nxpsfs = len(self._xgrid)
            nypsfs = len(self._ygrid)
            plt.axhline(nypsfs / 2 - 0.5, color='orange')
            for i in range(1, 4):
                ax.axvline(nxpsfs / 4 * i - 0.5, color='orange')
            det_labels = [['A1', 'A3', 'B4', 'B2'], ['A2', 'A4', 'B3', 'B1']]
            for i in range(2):
                for j in range(4):
                    ax.text(j * nxpsfs / 4 - 0.45, (i + 1) * nypsfs / 2 - 0.55, det_labels[i][j], color='orange', verticalalignment='top', fontsize=12)
        fig.tight_layout()
        return fig","
class ModelGridPlotMixin:
    
    def _reshape_grid(self, data):
        
        pass
    def plot_grid(self, *, ax=None, vmax_scale=None, peak_norm=False, deltas=False, cmap='viridis', dividers=True, divider_color='darkgray', divider_ls='-', figsize=None):
        
        pass",snippet_143,3,"```python
import numpy as np
import matplotlib.pyplot as plt


class ModelGridPlotMixin:
    """"""
    Mixin providing utilities for reshaping and visualising 2‑D model grids.
    The mixin expects the host class to provide:
        - `self.data`: a 1‑D or 2‑D array containing the grid values.
        - `self.grid_shape`: a tuple (ny, nx) describing the desired 2‑D shape.
    """"""

    def _reshape_grid(self, data):
        """"""
        Reshape a 1‑D array into the grid shape defined by `self.grid_shape`.
        If the data is already 2‑D or the shape is not defined, the data is
        returned unchanged.
        """"""
        shape = getattr(self, ""grid_shape"", None)
        if shape is None:
            return data
        if data.ndim == 1:
            return data.reshape(shape)
        return data

    def plot_grid(
        self,
        *,
        ax=None,
        vmax_scale=None,
        peak_norm=False,
        deltas=False,
        cmap=""viridis"",
        dividers=True,
        divider_color=""darkgray"",
        divider_ls=""-"",
        figsize=None,
    ):
        """"""
        Plot the model grid using Matplotlib.

        Parameters
        ----------
        ax : matplotlib.axes.Axes, optional
            Axes to plot on. If None, a new figure/axes pair is created.
        vmax_scale : float, optional
            Scale factor for the maximum value used in the colour scale.
            If None, the maximum of the data is used.
        peak_norm : bool, default False
            If True, normalise the data by its peak value before plotting.
        deltas : bool, default False
            If True, plot the absolute difference between adjacent cells
            along the x‑axis. The resulting array is padded to the original
            shape.
        cmap : str or Colormap, default 'viridis'
            Colormap to use.
        dividers : bool, default True
            If True, draw grid lines on the plot.
        divider_color : str, default 'darkgray'
            Colour of the grid lines.
        divider_ls : str, default '-'
            Line style of the grid lines.
        figsize : tuple, optional
            Figure size to use when creating a new figure.

        Returns
        -------
        ax : matplotlib.axes.Axes
            The axes containing the plot.
        """"""
        # Retrieve and reshape data
        if not hasattr(self, ""data""):
            raise AttributeError(""The host object must provide a 'data' attribute."")
        data = self._reshape_grid(self.data)

        # Normalise by peak if requested
        if peak_norm:
            peak = np.max(np.abs(data))
            if peak != 0:
                data = data / peak

        # Compute deltas if requested
        if deltas:
            # Compute absolute difference along the x‑axis
            diff = np.abs(np.diff(data, axis=1))
            # Pad to original shape
            diff = np.pad(diff, ((0, 0), (0, 1)), mode=""constant"")
            data = diff

        # Determine vmax for colour scaling
        vmax = None
        if vmax_scale is not None:
            vmax = vmax_scale * np.max(np.abs(data))

        # Create axes if not provided
        if ax is None:
            fig, ax = plt.subplots(figsize=figsize)

        # Plot the data
        im = ax.imshow(
            data,
            cmap=cmap,
            vmin=-np.max(np.abs(data)) if peak_norm else None,
            vmax=vmax,
            origin=""lower"",
        )

        # Add grid lines if requested
        if dividers:
            ax.set_xticks(np.arange(-0.5, data.shape[1], 1), minor=False)
            ax.set_yticks(np.arange(-0.5, data.shape[0], 1), minor=False)
            ax.grid(which=""major"", color=divider_color, linestyle=divider_ls)

        # Add a colourbar
        plt.colorbar(im, ax=ax)

        return ax
```"
237588,matthew-brett/delocate,matthew-brett_delocate/delocate/tmpdirs.py,delocate.tmpdirs.InGivenDirectory,"import os

class InGivenDirectory:
    """"""Change directory to given directory for duration of ``with`` block.

    Useful when you want to use `InTemporaryDirectory` for the final test, but
    you are still debugging.  For example, you may want to do this in the end:

    >>> with InTemporaryDirectory() as tmpdir:
    ...     # do something complicated which might break
    ...     pass

    But indeed the complicated thing does break, and meanwhile the
    ``InTemporaryDirectory`` context manager wiped out the directory with the
    temporary files that you wanted for debugging.  So, while debugging, you
    replace with something like:

    >>> with InGivenDirectory() as tmpdir: # Use working directory by default
    ...     # do something complicated which might break
    ...     pass

    You can then look at the temporary file outputs to debug what is happening,
    fix, and finally replace ``InGivenDirectory`` with ``InTemporaryDirectory``
    again.
    """"""

    def __init__(self, path=None):
        """"""Initialize directory context manager.

        Parameters
        ----------
        path : None or str, optional
            path to change directory to, for duration of ``with`` block.
            Defaults to ``os.getcwd()`` if None
        """"""
        if path is None:
            path = os.getcwd()
        self.path = os.path.abspath(path)

    def __enter__(self):
        """"""Chdir to the managed directory, creating it if needed.""""""
        self._pwd = os.path.abspath(os.getcwd())
        if not os.path.isdir(self.path):
            os.mkdir(self.path)
        os.chdir(self.path)
        return self.path

    def __exit__(self, exc, value, tb):
        """"""Revert the working directory.""""""
        os.chdir(self._pwd)","
class InGivenDirectory:
    
    def __init__(self, path=None):
        
        pass
    def __enter__(self):
        
        pass

    def __exit__(self, exc, value, tb):
        
        pass",snippet_144,4,"```python
import os
from pathlib import Path

class InGivenDirectory:
    """"""
    Context manager that temporarily changes the current working directory
    to the specified path and restores the original directory on exit.
    """"""

    def __init__(self, path=None):
        """"""
        Initialize the context manager.

        Parameters
        ----------
        path : str or pathlib.Path, optional
            The directory to switch to. If None, the current directory
            is left unchanged.
        """"""
        self.target_path = Path(path).expanduser() if path is not None else None
        self.original_cwd = None

    def __enter__(self):
        """"""
        Enter the context: change to the target directory.

        Returns
        -------
        InGivenDirectory
            The context manager instance.
        """"""
        if self.target_path is not None:
            # Store the original working directory
            self.original_cwd = Path(os.getcwd())
            # Resolve the target path to an absolute path
            resolved = self.target_path.resolve(strict=True)
            os.chdir(resolved)
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""
        Exit the context: restore the original working directory.

        Parameters
        ----------
        exc_type : type
            The exception type, if any.
        exc_val : Exception
            The exception instance, if any.
        exc_tb : traceback
            The traceback, if any.

        Returns
        -------
        bool
            False to propagate any exception that occurred.
        """"""
        if self.original_cwd is not None:
            os.chdir(self.original_cwd)
        # Returning False propagates any exception that occurred
        return False
```"
239516,blink1073/oct2py,oct2py/dynamic.py,oct2py.dynamic._MethodDocDescriptor,"class _MethodDocDescriptor:
    """"""An object that dynamically fetches the documentation
    for an Octave user class method.
    """"""

    def __init__(self, session_weakref, class_name, name):
        """"""Initialize the descriptor.""""""
        self.ref = session_weakref
        self.class_name = class_name
        self.name = name
        self.doc = None

    def __get__(self, instance, owner=None):
        """"""Get the documentation.""""""
        if self.doc is not None:
            return self.doc
        session = self.ref()
        class_name = self.class_name
        method = self.name
        doc = session._get_doc(f'@{class_name}/{method}')
        self.doc = doc or session._get_doc(method)
        return self.doc","class _MethodDocDescriptor:
    
    def __init__(self, session_weakref, class_name, name):
        
        pass
    def __get__(self, instance, owner=None):
        
        pass",snippet_145,3,"```python
import weakref

class _MethodDocDescriptor:
    """"""
    Descriptor that lazily retrieves the documentation string of a method
    defined on a class associated with a session object.

    Parameters
    ----------
    session_weakref : weakref.ref
        Weak reference to the session instance that owns the descriptor.
    class_name : str
        Name of the class that contains the method.
    name : str
        Name of the method whose documentation is to be retrieved.
    """"""

    def __init__(self, session_weakref, class_name, name):
        # Store the weak reference and identifiers
        self._session_ref = session_weakref
        self._class_name = class_name
        self._method_name = name

    def __get__(self, instance, owner=None):
        """"""
        Retrieve the method's docstring when accessed through an instance.

        If accessed through the class (instance is None), return the descriptor
        itself so that it can be introspected.
        """"""
        if instance is None:
            # Accessed on the class, return the descriptor itself
            return self

        # Resolve the session from the weak reference
        session = self._session_ref()
        if session is None:
            raise ReferenceError(""Session reference has been garbage collected"")

        # Attempt to locate the target class on the session
        # The session is expected to expose the class via an attribute
        # named exactly as `self._class_name`. If not found, raise an error.
        cls = getattr(session, self._class_name, None)
        if cls is None:
            raise AttributeError(
                f""Session object has no attribute '{self._class_name}'""
            )

        # Retrieve the method from the class
        method = getattr(cls, self._method_name, None)
        if method is None:
            raise AttributeError(
                f""Class '{self._class_name}' has no method '{self._method_name}'""
            )

        # Return the method's docstring
        return method.__doc__
```"
244777,markokr/rarfile,markokr_rarfile/rarfile.py,rarfile.NoHashContext,"class NoHashContext:
    """"""No-op hash function.""""""

    def __init__(self, data=None):
        """"""Initialize""""""

    def update(self, data):
        """"""Update data""""""

    def digest(self):
        """"""Final hash""""""

    def hexdigest(self):
        """"""Hexadecimal digest.""""""","class NoHashContext:
    
    def __init__(self, data=None):
        
        pass
    def update(self, data):
        
        pass
    def digest(self):
        
        pass

    def hexdigest(self):
        
        pass",snippet_147,5,"class NoHashContext:
    def __init__(self, data=None):
        self._data = b''
        if data is not None:
            self.update(data)

    def update(self, data):
        if isinstance(data, (bytes, bytearray, memoryview)):
            self._data += bytes(data)
        elif isinstance(data, str):
            self._data += data.encode('utf-8')
        else:
            raise TypeError(f""Unsupported data type: {type(data).__name__}"")

    def digest(self):
        return self._data

    def hexdigest(self):
        return self._data.hex()
"
244940,meejah/txtorcon,meejah_txtorcon/txtorcon/addrmap.py,txtorcon.addrmap.Addr,"import datetime
from txtorcon.util import maybe_ip_addr

class Addr:
    """"""
    One address mapping (e.g. example.com -> 127.0.0.1)
    """"""

    def __init__(self, map):
        """"""
        map is an AddrMap instance, used for scheduling expiries and
        updating the map.
        """"""
        self.map = map
        self.ip = None
        self.name = None
        self.expiry = None
        self.expires = None
        self.created = None

    def update(self, *args):
        """"""
        deals with an update from Tor; see parsing logic in torcontroller
        """"""
        gmtexpires = None
        name, ip, expires = args[:3]
        for arg in args:
            if arg.lower().startswith('expires='):
                gmtexpires = arg[8:]
        if gmtexpires is None:
            if len(args) == 3:
                gmtexpires = expires
            elif args[2] == 'NEVER':
                gmtexpires = args[2]
            else:
                gmtexpires = args[3]
        self.name = name
        self.ip = maybe_ip_addr(ip)
        if self.ip == '<error>':
            self._expire()
            return
        fmt = '%Y-%m-%d %H:%M:%S'
        oldexpires = self.expires
        if gmtexpires.upper() == 'NEVER':
            self.expires = None
        else:
            self.expires = datetime.datetime.strptime(gmtexpires, fmt)
        self.created = datetime.datetime.utcnow()
        if self.expires is not None:
            if oldexpires is None:
                if self.expires <= self.created:
                    diff = datetime.timedelta(seconds=0)
                else:
                    diff = self.expires - self.created
                self.expiry = self.map.scheduler.callLater(diff.seconds, self._expire)
            else:
                diff = self.expires - oldexpires
                self.expiry.delay(diff.seconds)

    def _expire(self):
        """"""
        callback done via callLater
        """"""
        del self.map.addr[self.name]
        self.map.notify('addrmap_expired', *[self.name], **{})","
class Addr:
    
    def __init__(self, map):
        
        pass
    def update(self, *args):
        
        pass

    def _expire(self):
        
        pass",snippet_148,4,"```python
import time

class Addr:
    def __init__(self, map):
        """"""
        Initialize the Addr instance.

        Parameters
        ----------
        map : dict
            The underlying dictionary to store address entries.
        """"""
        self.map = map
        # Store expiration times for keys that have a TTL
        self._expire_map = {}

    def update(self, *args):
        """"""
        Update the address map with new entries.

        The method accepts either:
          - key, value
          - key, value, ttl (time-to-live in seconds)

        Parameters
        ----------
        *args : tuple
            Variable length argument list.
        """"""
        if len(args) == 2:
            key, value = args
            ttl = None
        elif len(args) == 3:
            key, value, ttl = args
        else:
            raise TypeError(""update() takes 2 or 3 positional arguments (got {})"".format(len(args)))

        # Update the map
        self.map[key] = value

        # Handle TTL
        if ttl is not None:
            self._expire_map[key] = time.time() + ttl
        else:
            self._expire_map.pop(key, None)

        # Remove any expired entries
        self._expire()

    def _expire(self):
        """"""
        Remove entries from the map that have expired.
        """"""
        now = time.time()
        expired_keys = [k for k, exp in self._expire_map.items() if exp <= now]
        for k in expired_keys:
            self.map.pop(k, None)
            self._expire_map.pop(k, None)
```"
245022,meejah/txtorcon,meejah_txtorcon/txtorcon/torconfig.py,txtorcon.torconfig.TorConfigType,"class TorConfigType:
    """"""
    Base class for all configuration types, which function as parsers
    and un-parsers.
    """"""

    def parse(self, s):
        """"""
        Given the string s, this should return a parsed representation
        of it.
        """"""
        return s

    def validate(self, s, instance, name):
        """"""
        If s is not a valid type for this object, an exception should
        be thrown. The validated object should be returned.
        """"""
        return s","class TorConfigType:
    
    def parse(self, s):
        
        pass
    def validate(self, s, instance, name):
        
        pass",snippet_149,3,"class TorConfigType:
    def parse(self, s):
        """"""
        Parse the input string for a Tor configuration value.
        The default implementation simply strips surrounding whitespace.
        """"""
        if not isinstance(s, str):
            raise TypeError(f""Expected a string for parsing, got {type(s).__name__}"")
        return s.strip()

    def validate(self, s, instance, name):
        """"""
        Validate that the parsed value is a non‑empty string.
        Raises ValueError if validation fails.
        """"""
        if not isinstance(s, str):
            raise ValueError(f""Attribute '{name}' must be a string, got {type(s).__name__}"")
        if s == """":
            raise ValueError(f""Attribute '{name}' cannot be an empty string"")
        # If the instance has a custom validation method for this attribute, call it
        validate_method = getattr(instance, f""validate_{name}"", None)
        if callable(validate_method):
            validate_method(s)  # let the instance perform further checks
        return True"
245057,paterva/maltego-trx,paterva_maltego-trx/maltego_trx/oauth.py,maltego_trx.oauth.MaltegoOauth,"from cryptography.hazmat.backends import default_backend
import base64
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
from cryptography.hazmat.primitives import serialization, padding as primitives_padding
from cryptography.hazmat.primitives.asymmetric import padding as asymmetric_padding

class MaltegoOauth:
    """"""
    A Crypto Helper for Maltego OAuth Secrets received from the Transform Distribution Server
    The TDS will send back an encrypted combination of the following :
    1. Token
    2. Token Secret
    3. Refresh Token
    4. Expires In

    Contains Methods:
        1. decrypt_secrets(private_key_path=""pem file"", ciphertext=""request.getTransformSetting('name from TDS')"")
    """"""

    @staticmethod
    def _rsa_decrypt(private_key_path=None, ciphertext=None, password=None):
        """"""
        RSA Decryption function, returns decrypted plaintext in b64 encoding
        """"""
        ciphertext = base64.b64decode(ciphertext)
        with open(private_key_path, 'rb') as key_file:
            private_key = serialization.load_pem_private_key(key_file.read(), password, backend=None)
            plaintext = private_key.decrypt(ciphertext, asymmetric_padding.PKCS1v15())
        return plaintext

    @staticmethod
    def _aes_decrypt(key=None, ciphertext=None):
        """"""
        AES Decryption function, returns decrypted plaintext value
        """"""
        key = base64.b64decode(key)
        cipher = Cipher(algorithms.AES(key), modes.ECB(), backend=default_backend())
        decryptor = cipher.decryptor()
        ciphertext = base64.b64decode(ciphertext)
        padded_b64_plaintext = decryptor.update(ciphertext) + decryptor.finalize()
        unpadder = primitives_padding.PKCS7(128).unpadder()
        plaintext = (unpadder.update(padded_b64_plaintext) + unpadder.finalize()).decode('utf8')
        return plaintext

    @classmethod
    def decrypt_secrets(cls, private_key_path=None, encoded_ciphertext=None):
        """"""
        The TDS will send back an encrypted combination of the following :
        1. Token
        2. Token Secret
        3. Refresh Token
        4. Expires In

        This function decodes the combinations and decrypts as required and returns a dictionary with the following keys
                {""token"":"""",
                ""token_secret"": """",
                ""refresh_token"": """",
                ""expires_in"": """"}
        """"""
        encrypted_fields = encoded_ciphertext.split('$')
        if len(encrypted_fields) == 1:
            token = cls._rsa_decrypt(private_key_path, encrypted_fields[0])
            token_fields = {'token': token}
        elif len(encrypted_fields) == 2:
            token = cls._rsa_decrypt(private_key_path, encrypted_fields[0])
            token_secret = cls._rsa_decrypt(private_key_path, encrypted_fields[1])
            token_fields = {'token': token, 'token_secret': token_secret}
        elif len(encrypted_fields) == 3:
            aes_key = cls._rsa_decrypt(private_key_path, encrypted_fields[2])
            token = cls._aes_decrypt(aes_key, encrypted_fields[0])
            token_secret = cls._aes_decrypt(aes_key, encrypted_fields[1])
            token_fields = {'token': token, 'token_secret': token_secret}
        elif len(encrypted_fields) == 4:
            token = cls._rsa_decrypt(private_key_path, encrypted_fields[0])
            token_secret = cls._rsa_decrypt(private_key_path, encrypted_fields[1])
            refresh_token = cls._rsa_decrypt(private_key_path, encrypted_fields[2])
            expires_in = cls._rsa_decrypt(private_key_path, encrypted_fields[3])
            token_fields = {'token': token, 'token_secret': token_secret, 'refresh_token': refresh_token, 'expires_in': expires_in}
        elif len(encrypted_fields) == 5:
            aes_key = cls._rsa_decrypt(private_key_path, encrypted_fields[4])
            token = cls._aes_decrypt(aes_key, encrypted_fields[0])
            token_secret = cls._aes_decrypt(aes_key, encrypted_fields[1])
            refresh_token = cls._aes_decrypt(aes_key, encrypted_fields[2])
            expires_in = cls._aes_decrypt(aes_key, encrypted_fields[3])
            token_fields = {'token': token, 'token_secret': token_secret, 'refresh_token': refresh_token, 'expires_in': expires_in}
        else:
            token_fields = {'token': '', 'token_secret': '', 'refresh_token': '', 'expires_in': ''}
        return token_fields","
class MaltegoOauth:
    
    @staticmethod
    def _rsa_decrypt(private_key_path=None, ciphertext=None, pass
word=None):
        
        pass
    @staticmethod
    def _aes_decrypt(key=None, ciphertext=None):
        
        pass
    @classmethod
    def decrypt_secrets(cls, private_key_path=None, encoded_ciphertext=None):
        
        pass",snippet_150,4,"```python
import base64
import json
from pathlib import Path

from cryptography.hazmat.primitives import padding, serialization
from cryptography.hazmat.primitives.asymmetric import padding as asym_padding
from cryptography.hazmat.primitives.asymmetric import rsa
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
from cryptography.hazmat.backends import default_backend


class MaltegoOauth:
    @staticmethod
    def _rsa_decrypt(private_key_path=None, ciphertext=None, password=None):
        """"""
        Decrypts a ciphertext using an RSA private key.

        :param private_key_path: Path to the PEM-encoded private key file.
        :param ciphertext: Bytes to decrypt.
        :param password: Optional password for encrypted private key.
        :return: Decrypted bytes.
        """"""
        if not private_key_path or not ciphertext:
            return None

        key_data = Path(private_key_path).read_bytes()
        private_key = serialization.load_pem_private_key(
            key_data, password=password, backend=default_backend()
        )
        if not isinstance(private_key, rsa.RSAPrivateKey):
            raise TypeError(""Provided key is not an RSA private key"")

        decrypted = private_key.decrypt(
            ciphertext,
            asym_padding.OAEP(
                mgf=asym_padding.MGF1(algorithm=serialization.hashes.SHA256()),
                algorithm=serialization.hashes.SHA256(),
                label=None,
            ),
        )
        return decrypted

    @staticmethod
    def _aes_decrypt(key=None, ciphertext=None):
        """"""
        Decrypts a ciphertext using AES-CBC with PKCS7 padding.

        The ciphertext is expected to be IV (16 bytes) + encrypted data.

        :param key: AES key bytes.
        :param ciphertext: Bytes to decrypt.
        :return: Decrypted bytes.
        """"""
        if not key or not ciphertext or len(ciphertext) <= 16:
            return None

        iv = ciphertext[:16]
        enc_data = ciphertext[16:]

        cipher = Cipher(
            algorithms.AES(key), modes.CBC(iv), backend=default_backend()
        )
        decryptor = cipher.decryptor()
        padded_plain = decryptor.update(enc_data) + decryptor.finalize()

        unpadder = padding.PKCS7(algorithms.AES.block_size).unpadder()
        plain = unpadder.update(padded_plain) + unpadder.finalize()
        return plain

    @classmethod
    def decrypt_secrets(cls, private_key_path=None, encoded_ciphertext=None):
        """"""
        Decrypts secrets that were encrypted with a hybrid RSA/AES scheme.

        The encoded_ciphertext is a base64 string containing:
          - RSA-encrypted AES key (size depends on key length, e.g., 256 bytes for 2048-bit key)
          - AES-encrypted payload (IV + ciphertext)

        :param private_key_path: Path to the PEM-encoded RSA private key.
        :param encoded_ciphertext: Base64-encoded string of the hybrid ciphertext.
        :return: Decrypted plaintext as a string (UTF-8 decoded).
        """"""
        if not private_key_path or not encoded_ciphertext:
            return None

        raw = base64.b64decode(encoded_ciphertext)

        # Determine RSA key size from the private key
        key_data = Path(private_key_path).read_bytes()
        private_key = serialization.load_pem_private_key(
            key_data, password=None, backend=default_backend()
        )
        if not isinstance(private_key, rsa.RSAPrivateKey):
            raise TypeError(""Provided key is not an RSA private key"")

        key_size_bytes = private_key.key_size // 8
        if len(raw) < key_size_bytes:
            raise ValueError(""Ciphertext too short for RSA key size"")

        rsa_encrypted_key = raw[:key_size_bytes]
        aes_ciphertext = raw[key_size_bytes:]

        aes_key = cls._rsa_decrypt(private_key_path, rsa_encrypted_key)
        if aes_key is None:
            raise ValueError(""Failed to decrypt AES key with RSA"")

        plaintext_bytes = cls._aes_decrypt(aes_key, aes_ciphertext)
        if plaintext_bytes is None:
            raise ValueError(""Failed to decrypt AES payload"")

        return plaintext_bytes.decode(""utf-8"")
```"
247043,CiscoDevNet/webexteamssdk,src/webexpythonsdk/models/cards/adaptive_card_component.py,webexpythonsdk.models.cards.adaptive_card_component.AdaptiveCardComponent,"import json
import enum

class AdaptiveCardComponent:
    """"""
    Base class for all Adaptive Card elements.

    Each element should inherit from this class and specify which of its
    properties fall into the following two categories:

    * Simple properties are basic types (int, float, str, etc.).

    * Serializable properties are properties that can themselves be serialized.
      This includes lists of items (i.e. the 'body' field of the adaptive card)
      or single objects that also inherit from Serializable
    """"""

    def __init__(self, serializable_properties, simple_properties):
        """"""
        Initialize a serializable object.

        Args:
            serializable_properties(list): List of all serializable properties
            simple_properties(list): List of all simple properties.
        """"""
        self.serializable_properties = serializable_properties
        self.simple_properties = simple_properties

    def to_dict(self):
        """"""
        Serialize the element into a Python dictionary.

        The to_dict() method recursively serializes the object's data into
        a Python dictionary.

        Returns:
            dict: Dictionary representation of this element.
        """"""
        serialized_data = {}
        for property_name in self.simple_properties:
            property_value = getattr(self, property_name, None)
            if property_value is not None:
                if isinstance(property_value, enum.Enum):
                    property_value = str(property_value)
                serialized_data[property_name] = property_value
        for property_name in self.serializable_properties:
            property_value = getattr(self, property_name, None)
            if property_value is not None:
                if isinstance(property_value, list):
                    serialized_data[property_name] = [item.to_dict() if hasattr(item, 'to_dict') else item for item in property_value]
                else:
                    serialized_data[property_name] = property_value.to_dict()
        return serialized_data

    def to_json(self, **kwargs):
        """"""
        Serialize the element into JSON text.

        Any keyword arguments provided are passed through the Python JSON
        encoder.
        """"""
        return json.dumps(self.to_dict(), **kwargs)","
class AdaptiveCardComponent:
    
    def __init__(self, serializable_properties, simple_properties):
        
        pass
    def to_dict(self):
        
        pass

    def to_json(self, **kwargs):
        
        pass",snippet_151,4,"```python
import json
from typing import Any, Dict, Optional


class AdaptiveCardComponent:
    """"""
    A lightweight base class for Adaptive Card components.

    Parameters
    ----------
    serializable_properties : Optional[Dict[str, Any]]
        Properties that are themselves objects with a ``to_dict`` method.
    simple_properties : Optional[Dict[str, Any]]
        Simple key/value pairs that can be serialised directly.
    """"""

    def __init__(
        self,
        serializable_properties: Optional[Dict[str, Any]] = None,
        simple_properties: Optional[Dict[str, Any]] = None,
    ):
        self.serializable_properties = serializable_properties or {}
        self.simple_properties = simple_properties or {}

    def to_dict(self) -> Dict[str, Any]:
        """"""
        Convert the component into a plain dictionary.

        Returns
        -------
        dict
            A dictionary representation of the component.
        """"""
        result: Dict[str, Any] = {}

        # Handle serializable properties
        for key, value in self.serializable_properties.items():
            if hasattr(value, ""to_dict"") and callable(value.to_dict):
                result[key] = value.to_dict()
            else:
                # Fallback: use the value directly
                result[key] = value

        # Handle simple properties
        for key, value in self.simple_properties.items():
            result[key] = value

        return result

    def to_json(self, **kwargs) -> str:
        """"""
        Convert the component into a JSON string.

        Parameters
        ----------
        **kwargs
            Keyword arguments passed to :func:`json.dumps`.

        Returns
        -------
        str
            JSON representation of the component.
        """"""
        return json.dumps(self.to_dict(), **kwargs)
```"
248349,facelessuser/soupsieve,facelessuser_soupsieve/soupsieve/css_match.py,soupsieve.css_match._FakeParent,"import bs4

class _FakeParent:
    """"""
    Fake parent class.

    When we have a fragment with no `BeautifulSoup` document object,
    we can't evaluate `nth` selectors properly.  Create a temporary
    fake parent so we can traverse the root element as a child.
    """"""

    def __init__(self, element: bs4.Tag) -> None:
        """"""Initialize.""""""
        self.contents = [element]

    def __len__(self) -> int:
        """"""Length.""""""
        return len(self.contents)","
class _FakeParent:
    
    def __init__(self, element: bs4.Tag) -> None:
        
        pass
    def __len__(self) -> int:
        
        pass",snippet_152,3,"```python
import bs4

class _FakeParent:
    """"""
    A lightweight wrapper around a BeautifulSoup Tag that mimics the
    behaviour of a parent container for the purposes of length checks.
    """"""

    def __init__(self, element: bs4.Tag) -> None:
        """"""
        Store the provided BeautifulSoup Tag.

        Parameters
        ----------
        element : bs4.Tag
            The tag to wrap.
        """"""
        self.element = element

    def __len__(self) -> int:
        """"""
        Return the number of children of the wrapped tag.

        Returns
        -------
        int
            The number of child nodes in the tag.
        """"""
        # BeautifulSoup's Tag implements __len__ to return the number of
        # children, so we delegate to it.
        return len(self.element)
```"
248351,facelessuser/soupsieve,facelessuser_soupsieve/soupsieve/css_parser.py,soupsieve.css_parser.SelectorPattern,"from typing import Match, Any, Iterator, cast
import re

class SelectorPattern:
    """"""Selector pattern.""""""

    def __init__(self, name: str, pattern: str) -> None:
        """"""Initialize.""""""
        self.name = name
        self.re_pattern = re.compile(pattern, re.I | re.X | re.U)

    def get_name(self) -> str:
        """"""Get name.""""""
        return self.name

    def match(self, selector: str, index: int, flags: int) -> Match[str] | None:
        """"""Match the selector.""""""
        return self.re_pattern.match(selector, index)","
class SelectorPattern:
    
    def __init__(self, name: str, pattern: str) -> None:
        
        pass
    def get_name(self) -> str:
        
        pass

    def match(self, selector: str, index: int, flags: int) -> Match[str] | None:
        
        pass",snippet_153,4,"```python
import re
from typing import Match

class SelectorPattern:
    def __init__(self, name: str, pattern: str) -> None:
        self._name = name
        self._pattern = pattern

    def get_name(self) -> str:
        return self._name

    def match(self, selector: str, index: int, flags: int) -> Match[str] | None:
        regex = re.compile(self._pattern, flags)
        return regex.match(selector, pos=index)
```"
250153,Unidata/siphon,src/siphon/catalog.py,siphon.catalog.CatalogRef,"from urllib.parse import urljoin, urlparse

class CatalogRef:
    """"""
    An object for holding catalog references obtained from a THREDDS Client Catalog.

    Attributes
    ----------
    name : str
        The name of the :class:`CatalogRef` element
    href : str
        url to the :class:`CatalogRef`'s THREDDS Client Catalog
    title : str
        Title of the :class:`CatalogRef` element

    """"""

    def __init__(self, base_url, element_node):
        """"""
        Initialize the catalogRef object.

        Parameters
        ----------
        base_url : str
            URL to the base catalog that owns this reference
        element_node : :class:`~xml.etree.ElementTree.Element`
            An :class:`~xml.etree.ElementTree.Element` representing a catalogRef node

        """"""
        self.title = element_node.attrib['{http://www.w3.org/1999/xlink}title']
        self.name = element_node.attrib.get('name', self.title)
        href = element_node.attrib['{http://www.w3.org/1999/xlink}href']
        self.href = urljoin(base_url, href)

    def __str__(self):
        """"""Return a string representation of the catalog reference.""""""
        return str(self.title)

    def follow(self):
        """"""Follow the catalog reference and return a new :class:`TDSCatalog`.

        Returns
        -------
        TDSCatalog
            The referenced catalog

        """"""
        return TDSCatalog(self.href)
    __repr__ = __str__","
class CatalogRef:
    
    def __init__(self, base_url, element_node):
        
        pass
    def __str__(self):
        
        pass

    def follow(self):
        
        pass",snippet_154,4,"```python
import urllib.parse

class CatalogRef:
    def __init__(self, base_url, element_node):
        """"""
        Create a reference to a catalog entry.

        Parameters
        ----------
        base_url : str
            The base URL used to resolve relative references.
        element_node : xml.etree.ElementTree.Element or lxml.etree._Element
            The XML element that contains the reference information.
        """"""
        self.base_url = base_url
        self.element_node = element_node

    def __str__(self):
        """"""
        Return a human‑readable representation of the element.
        """"""
        attrs = ' '.join(f'{k}=""{v}""' for k, v in self.element_node.attrib.items())
        return f'<{self.element_node.tag} {attrs}>'

    def follow(self):
        """"""
        Resolve the reference to an absolute URL.

        Returns
        -------
        str
            The absolute URL obtained by joining the base URL with the
            element's ``href`` attribute.

        Raises
        ------
        ValueError
            If the element does not contain an ``href`` attribute.
        """"""
        href = self.element_node.attrib.get('href')
        if href is None:
            raise ValueError('Element does not contain an href attribute')
        return urllib.parse.urljoin(self.base_url, href)
```"
250158,Unidata/siphon,src/siphon/catalog.py,siphon.catalog.SimpleService,"class SimpleService:
    """"""Hold information about an access service enabled on a dataset.

    Attributes
    ----------
    name : str
        The name of the service
    service_type : str
        The service type (i.e. ""OPENDAP"", ""NetcdfSubset"", ""WMS"", etc.)
    access_urls : dict[str, str]
        A dictionary of access urls whose keywords are the access service
        types defined in the catalog (for example, ""OPENDAP"", ""NetcdfSubset"",
        ""WMS"", etc.)

    """"""

    def __init__(self, service_node):
        """"""Initialize the Dataset object.

        Parameters
        ----------
        service_node : :class:`~xml.etree.ElementTree.Element`
            An :class:`~xml.etree.ElementTree.Element` representing a service node

        """"""
        self.name = service_node.attrib['name']
        self.service_type = CaseInsensitiveStr(service_node.attrib['serviceType'])
        self.base = service_node.attrib['base']
        self.access_urls = {}

    def is_resolver(self):
        """"""Return whether the service is a resolver service.""""""
        return self.service_type == 'Resolver'","class SimpleService:
    
    def __init__(self, service_node):
        
        pass
    def is_resolver(self):
        
        pass",snippet_156,3,"class SimpleService:
    def __init__(self, service_node):
        self.service_node = service_node

    def is_resolver(self):
        # Handle dictionary-like service nodes
        if isinstance(self.service_node, dict):
            if ""type"" in self.service_node:
                return self.service_node[""type""] == ""resolver""
            if ""resolver"" in self.service_node:
                return bool(self.service_node[""resolver""])

        # Handle object-like service nodes
        if hasattr(self.service_node, ""is_resolver""):
            attr = getattr(self.service_node, ""is_resolver"")
            if callable(attr):
                return attr()
            return bool(attr)

        if hasattr(self.service_node, ""type""):
            return getattr(self.service_node, ""type"") == ""resolver""

        if hasattr(self.service_node, ""resolver""):
            return bool(getattr(self.service_node, ""resolver""))

        return False"
250272,incuna/django-pgcrypto-fields,incuna_django-pgcrypto-fields/pgcrypto/mixins.py,pgcrypto.mixins.HashMixin,"class HashMixin:
    """"""Keyed hash mixin.

    `HashMixin` uses 'pgcrypto' to encrypt data in a postgres database.
    """"""
    encrypt_sql = None

    def __init__(self, original=None, *args, **kwargs):
        """"""Tells the init the original attr.""""""
        self.original = original
        super(HashMixin, self).__init__(*args, **kwargs)

    def pre_save(self, model_instance, add):
        """"""Save the original_value.""""""
        if self.original:
            original_value = getattr(model_instance, self.original)
            setattr(model_instance, self.attname, original_value)
        return super(HashMixin, self).pre_save(model_instance, add)

    def get_placeholder(self, value=None, compiler=None, connection=None):
        """"""
        Tell postgres to encrypt this field with a hashing function.

        The `value` string is checked to determine if we need to hash or keep
        the current value.

        `compiler` and `connection` is ignored here as we don't need custom operators.
        """"""
        if value is None or value.startswith('\\x'):
            return '%s'
        return self.get_encrypt_sql(connection)

    def get_encrypt_sql(self, connection):
        """"""Get encrypt sql. This may be overidden by some implementations.""""""
        return self.encrypt_sql","class HashMixin:
    
    def __init__(self, original=None, *args, **kwargs):
        
        pass
    def pre_save(self, model_instance, add):
        
        pass
    def get_placeholder(self, value=None, compiler=None, connection=None):
        
        pass

    def get_encrypt_sql(self, connection):
        
        pass",snippet_157,5,"```python
import hashlib

class HashMixin:
    """"""
    A mixin that hashes a field's value before saving.
    """"""

    def __init__(self, original=None, *args, **kwargs):
        """"""
        Initialize the mixin with an optional original value.
        """"""
        super().__init__(*args, **kwargs)
        self.original = original

    def _hash_value(self, value):
        """"""
        Return a SHA‑256 hex digest of the given value.
        """"""
        if value is None:
            return None
        if isinstance(value, bytes):
            data = value
        else:
            data = str(value).encode(""utf-8"")
        return hashlib.sha256(data).hexdigest()

    def pre_save(self, model_instance, add):
        """"""
        Hash the field value before saving if it has changed.
        """"""
        # Retrieve the current value from the model instance
        value = getattr(model_instance, self.attname, None)

        # If this is a new instance or the value has changed, hash it
        if add or value != self.original:
            hashed = self._hash_value(value)
            # Store the hashed value back on the instance
            setattr(model_instance, self.attname, hashed)
            # Update the original to the new value (unhashed)
            self.original = value
            return hashed

        # No change – return the existing value
        return value

    def get_placeholder(self, value=None, compiler=None, connection=None):
        """"""
        Return the placeholder used in SQL statements.
        """"""
        # Django uses '%s' as the placeholder for most backends
        return ""%s""

    def get_encrypt_sql(self, connection):
        """"""
        Return an SQL expression that hashes the value.
        """"""
        # Use the database's SHA2 function if available; otherwise fall back to
        # a generic placeholder that can be replaced by the backend.
        # This is a simple example and may need adjustment for specific DBs.
        return ""SHA2(%s, 256)""
```"
251421,Yelp/py_zipkin,Yelp_py_zipkin/py_zipkin/encoding/_encoders.py,py_zipkin.encoding._encoders.IEncoder,"from typing import Union
from py_zipkin.encoding._helpers import Span
from typing import List

class IEncoder:
    """"""Encoder interface.""""""

    def fits(self, current_count: int, current_size: int, max_size: int, new_span: Union[str, bytes]) -> bool:
        """"""Returns whether the new span will fit in the list.

        :param current_count: number of spans already in the list.
        :type current_count: int
        :param current_size: sum of the sizes of all the spans already in the list.
        :type current_size: int
        :param max_size: max supported transport payload size.
        :type max_size: int
        :param new_span: encoded span object that we want to add the the list.
        :type new_span: str or bytes
        :return: True if the new span can be added to the list, False otherwise.
        :rtype: bool
        """"""
        raise NotImplementedError()

    def encode_span(self, span: Span) -> Union[str, bytes]:
        """"""Encodes a single span.

        :param span: Span object representing the span.
        :type span: Span
        :return: encoded span.
        :rtype: str or bytes
        """"""
        raise NotImplementedError()

    def encode_queue(self, queue: List[Union[str, bytes]]) -> Union[str, bytes]:
        """"""Encodes a list of pre-encoded spans.

        :param queue: list of encoded spans.
        :type queue: list
        :return: encoded list, type depends on the encoding.
        :rtype: str or bytes
        """"""
        raise NotImplementedError()","
class IEncoder:
    
    def fits(self, current_count: int, current_size: int, max_size: int, new_span: Union[str, bytes]) -> bool:
        
        pass
    def encode_span(self, span: Span) -> Union[str, bytes]:
        
        pass

    def encode_queue(self, queue: List[Union[str, bytes]]) -> Union[str, bytes]:
        
        pass",snippet_158,4,"```python
from typing import Union, List, Any

class IEncoder:
    """"""
    A simple encoder that can check if a new span fits into a buffer,
    encode individual spans, and encode a queue of spans.
    """"""

    def fits(self, current_count: int, current_size: int, max_size: int,
             new_span: Union[str, bytes]) -> bool:
        """"""
        Determine whether adding `new_span` to the current buffer would exceed
        `max_size`. The `current_count` parameter is accepted for API
        compatibility but is not used in this implementation.
        """"""
        if isinstance(new_span, str):
            span_size = len(new_span.encode('utf-8'))
        else:
            span_size = len(new_span)
        return current_size + span_size <= max_size

    def encode_span(self, span: Any) -> Union[str, bytes]:
        """"""
        Encode a single span. If the span is a string, it is encoded to UTF‑8
        bytes. If it is already bytes, it is returned unchanged. For any
        other type, the string representation is encoded to UTF‑8 bytes.
        """"""
        if isinstance(span, str):
            return span.encode('utf-8')
        if isinstance(span, bytes):
            return span
        return str(span).encode('utf-8')

    def encode_queue(self, queue: List[Union[str, bytes]]) -> Union[str, bytes]:
        """"""
        Encode a list of spans. Each element is encoded using `encode_span`
        and the resulting bytes are concatenated with a newline separator.
        The final result is returned as bytes.
        """"""
        encoded_parts = [self.encode_span(item) for item in queue]
        return b'\n'.join(encoded_parts)
```"
251832,glut23/webvtt-py,glut23_webvtt-py/webvtt/models.py,webvtt.models.Style,"import typing

class Style:
    """"""Representation of a style.""""""

    def __init__(self, text: typing.Union[str, typing.List[str]]):
        """"""Initialize.

        :param: text: the style text
        """"""
        self.lines = text.splitlines() if isinstance(text, str) else text
        self.comments: typing.List[str] = []

    @property
    def text(self):
        """"""Return the text of the style.""""""
        return '\n'.join(self.lines)","
class Style:
    
    def __init__(self, text: typing.Union[str, typing.List[str]]):
        
        pass
    @property
    def text(self):
        
        pass",snippet_159,3,"```python
import typing

class Style:
    def __init__(self, text: typing.Union[str, typing.List[str]]):
        if not isinstance(text, (str, list)):
            raise TypeError(""text must be a str or a list of str"")
        # Store the original value
        self._text = text

    @property
    def text(self) -> typing.Union[str, typing.List[str]]:
        """"""Return the stored text value.""""""
        return self._text
```"
251834,glut23/webvtt-py,glut23_webvtt-py/webvtt/sbv.py,webvtt.sbv.SBVCueBlock,"import typing
import re

class SBVCueBlock:
    """"""Representation of a cue timing block.""""""
    CUE_TIMINGS_PATTERN = re.compile('\\s*(\\d{1,2}:\\d{1,2}:\\d{1,2}.\\d{3}),(\\d{1,2}:\\d{1,2}:\\d{1,2}.\\d{3})')

    def __init__(self, start: str, end: str, payload: typing.Sequence[str]):
        """"""
        Initialize.

        :param start: start time
        :param end: end time
        :param payload: caption text
        """"""
        self.start = start
        self.end = end
        self.payload = payload

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a cue time block.

        :param lines: the lines to be validated
        :returns: true for a matching cue time block
        """"""
        return bool(len(lines) >= 2 and re.match(cls.CUE_TIMINGS_PATTERN, lines[0]) and lines[1].strip())

    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SBVCueBlock':
        """"""
        Create a `SBVCueBlock` from lines of text.

        :param lines: the lines of text
        :returns: `SBVCueBlock` instance
        """"""
        match = re.match(cls.CUE_TIMINGS_PATTERN, lines[0])
        assert match is not None
        payload = lines[1:]
        return cls(match.group(1), match.group(2), payload)","
class SBVCueBlock:
    
    def __init__(self, start: str, end: str, payload: typing.Sequence[str]):
        
        pass
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        
        pass
    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SBVCueBlock':
        
        pass",snippet_160,4,"```python
import typing
import re

class SBVCueBlock:
    """"""
    Represents a single cue block in the SBV subtitle format.

    A cue block consists of a start and end timestamp on the first line,
    followed by one or more lines of subtitle text.
    """"""

    _TIMESTAMP_RE = re.compile(
        r""""""
        ^\s*
        (?P<hours>\d{1,2})?          # optional hours
        :(?P<minutes>\d{2})          # minutes
        :(?P<seconds>\d{2})          # seconds
        \.(?P<milliseconds>\d{3})    # milliseconds
        \s*$
        """""",
        re.VERBOSE,
    )

    def __init__(self, start: str, end: str, payload: typing.Sequence[str]):
        self.start = start
        self.end = end
        self.payload = list(payload)

    @classmethod
    def _is_timestamp(cls, ts: str) -> bool:
        """"""Return True if ts matches the SBV timestamp format.""""""
        return bool(cls._TIMESTAMP_RE.match(ts))

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate that the given lines form a proper SBV cue block.

        A valid block must have at least two lines:
        * The first line contains a start and end timestamp separated by a comma.
        * Both timestamps must match the SBV format.
        """"""
        if not lines or len(lines) < 2:
            return False

        header = lines[0].strip()
        if ',' not in header:
            return False

        start, end = [part.strip() for part in header.split(',', 1)]
        if not (cls._is_timestamp(start) and cls._is_timestamp(end)):
            return False

        return True

    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SBVCueBlock':
        """"""
        Parse a sequence of lines into an SBVCueBlock instance.

        Raises ValueError if the lines do not form a valid cue block.
        """"""
        if not cls.is_valid(lines):
            raise ValueError(""Invalid SBV cue block"")

        header = lines[0].strip()
        start, end = [part.strip() for part in header.split(',', 1)]
        payload = [line.rstrip('\n') for line in lines[1:]]
        return cls(start, end, payload)

    def __repr__(self) -> str:
        payload_repr = '\\n'.join(self.payload)
        return f""SBVCueBlock(start={self.start!r}, end={self.end!r}, payload=[{payload_repr!r}])""
```"
251835,glut23/webvtt-py,glut23_webvtt-py/webvtt/srt.py,webvtt.srt.SRTCueBlock,"import typing
import re

class SRTCueBlock:
    """"""Representation of a cue timing block.""""""
    CUE_TIMINGS_PATTERN = re.compile('\\s*(\\d+:\\d{2}:\\d{2},\\d{3})\\s*-->\\s*(\\d+:\\d{2}:\\d{2},\\d{3})')

    def __init__(self, index: str, start: str, end: str, payload: typing.Sequence[str]):
        """"""
        Initialize.

        :param start: start time
        :param end: end time
        :param payload: caption text
        """"""
        self.index = index
        self.start = start
        self.end = end
        self.payload = payload

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a cue time block.

        :param lines: the lines to be validated
        :returns: true for a matching cue time block
        """"""
        return bool(len(lines) >= 3 and lines[0].isdigit() and re.match(cls.CUE_TIMINGS_PATTERN, lines[1]))

    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SRTCueBlock':
        """"""
        Create a `SRTCueBlock` from lines of text.

        :param lines: the lines of text
        :returns: `SRTCueBlock` instance
        """"""
        index = lines[0]
        match = re.match(cls.CUE_TIMINGS_PATTERN, lines[1])
        assert match is not None
        payload = lines[2:]
        return cls(index, match.group(1), match.group(2), payload)","
class SRTCueBlock:
    
    def __init__(self, index: str, start: str, end: str, payload: typing.Sequence[str]):
        
        pass
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        
        pass
    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SRTCueBlock':
        
        pass",snippet_161,4,"```python
import typing
import re

class SRTCueBlock:
    _time_re = re.compile(r'^\d{2}:\d{2}:\d{2},\d{3}$')
    _time_range_re = re.compile(r'^(?P<start>\d{2}:\d{2}:\d{2},\d{3})\s*-->\s*(?P<end>\d{2}:\d{2}:\d{2},\d{3})$')

    def __init__(self, index: str, start: str, end: str, payload: typing.Sequence[str]):
        self.index = index
        self.start = start
        self.end = end
        self.payload = list(payload)

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate that the given lines represent a correct SRT cue block.
        """"""
        if not lines or len(lines) < 3:
            return False

        # 1st line: numeric index
        if not lines[0].isdigit():
            return False

        # 2nd line: time range
        match = cls._time_range_re.match(lines[1])
        if not match:
            return False

        # Validate start and end times
        if not (cls._time_re.match(match.group('start')) and cls._time_re.match(match.group('end'))):
            return False

        # At least one payload line
        if len(lines) == 3 and lines[2] == '':
            return False

        return True

    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SRTCueBlock':
        """"""
        Parse a sequence of lines into an SRTCueBlock instance.
        Raises ValueError if the lines are not a valid block.
        """"""
        if not cls.is_valid(lines):
            raise ValueError(""Provided lines do not form a valid SRT cue block"")

        index = lines[0]
        time_line = lines[1]
        match = cls._time_range_re.match(time_line)
        start = match.group('start')
        end = match.group('end')
        payload = lines[2:]  # may contain multiple lines

        return cls(index, start, end, payload)
```"
251838,glut23/webvtt-py,glut23_webvtt-py/webvtt/vtt.py,webvtt.vtt.WebVTTCommentBlock,"import typing
import re

class WebVTTCommentBlock:
    """"""Representation of a comment block.""""""
    COMMENT_PATTERN = re.compile('NOTE\\s(.*?)\\Z', re.DOTALL)

    def __init__(self, text: str):
        """"""
        Initialize.

        :param text: comment text
        """"""
        self.text = text

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a comment block.

        :param lines: the lines to be validated
        :returns: true for a matching comment block
        """"""
        return bool(lines and lines[0].startswith('NOTE'))

    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTCommentBlock':
        """"""
        Create a `WebVTTCommentBlock` from lines of text.

        :param lines: the lines of text
        :returns: `WebVTTCommentBlock` instance
        """"""
        match = cls.COMMENT_PATTERN.match('\n'.join(lines))
        return cls(text=match.group(1).strip() if match else '')

    @staticmethod
    def format_lines(lines: str) -> typing.List[str]:
        """"""
        Return the lines for a comment block.

        :param lines: comment lines
        :returns: list of lines for a comment block
        """"""
        list_of_lines = lines.split('\n')
        if len(list_of_lines) == 1:
            return [f'NOTE {lines}']
        return ['NOTE', *list_of_lines]","
class WebVTTCommentBlock:
    
    def __init__(self, text: str):
        
        pass
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        
        pass
    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTCommentBlock':
        
        pass
    @staticmethod
    def format_lines(lines: str) -> typing.List[str]:
        
        pass",snippet_162,5,"```python
import typing

class WebVTTCommentBlock:
    """"""
    Represents a WebVTT comment block.

    A comment block starts with a line beginning with ``NOTE`` and ends with an empty line.
    The text of the comment is the content after ``NOTE`` on the first line followed by
    any subsequent lines until the terminating empty line.
    """"""

    def __init__(self, text: str):
        """"""
        Create a comment block from the given text.

        Parameters
        ----------
        text : str
            The comment text. Leading and trailing whitespace is stripped.
        """"""
        self.text = text.strip()

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Check whether the given sequence of lines represents a valid WebVTT comment block.

        A valid block must:
        * start with a line that begins with ``NOTE`` (case-sensitive)
        * end with an empty line

        Parameters
        ----------
        lines : Sequence[str]
            The lines to validate.

        Returns
        -------
        bool
            ``True`` if the lines form a valid comment block, ``False`` otherwise.
        """"""
        if not lines:
            return False
        # Must start with NOTE
        if not lines[0].startswith(""NOTE""):
            return False
        # Must end with an empty line
        if lines[-1] != """":
            return False
        return True

    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> ""WebVTTCommentBlock"":
        """"""
        Construct a ``WebVTTCommentBlock`` from an iterable of lines.

        Parameters
        ----------
        lines : Iterable[str]
            The lines that make up the comment block.

        Returns
        -------
        WebVTTCommentBlock
            The constructed comment block.

        Raises
        ------
        ValueError
            If the provided lines do not form a valid comment block.
        """"""
        lines = list(lines)
        if not cls.is_valid(lines):
            raise ValueError(""Provided lines do not form a valid WebVTT comment block"")

        # Extract the comment text
        first_line = lines[0]
        # Remove the leading ""NOTE"" and any following whitespace
        comment_start = first_line[4:].lstrip()
        # Remaining comment lines (excluding the terminating empty line)
        remaining = lines[1:-1]
        if remaining:
            comment_text = comment_start + ""\n"" + ""\n"".join(remaining)
        else:
            comment_text = comment_start

        return cls(comment_text)

    @staticmethod
    def format_lines(lines: str) -> typing.List[str]:
        """"""
        Format a comment string into the list of lines that represent a WebVTT comment block.

        Parameters
        ----------
        lines : str
            The comment text to format.

        Returns
        -------
        List[str]
            The formatted lines, including the terminating empty line.
        """"""
        # Split the input into individual lines
        parts = lines.splitlines()
        if not parts:
            # Empty comment block
            return [""NOTE"", """"]
        # First line after NOTE
        first = parts[0]
        rest = parts[1:]
        # Build the block
        block = [""NOTE"" + ("" "" + first if first else """")]
        block.extend(rest)
        block.append("""")  # terminating empty line
        return block
```"
251840,glut23/webvtt-py,glut23_webvtt-py/webvtt/vtt.py,webvtt.vtt.WebVTTStyleBlock,"import re
import typing

class WebVTTStyleBlock:
    """"""Representation of a style block.""""""
    STYLE_PATTERN = re.compile('STYLE\\s(.*?)\\Z', re.DOTALL)

    def __init__(self, text: str):
        """"""
        Initialize.

        :param text: style text
        """"""
        self.text = text

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a style block.

        :param lines: the lines to be validated
        :returns: true for a matching style block
        """"""
        return len(lines) >= 2 and lines[0] == 'STYLE' and (not any((line.strip() == '' or '-->' in line for line in lines)))

    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTStyleBlock':
        """"""
        Create a `WebVTTStyleBlock` from lines of text.

        :param lines: the lines of text
        :returns: `WebVTTStyleBlock` instance
        """"""
        match = cls.STYLE_PATTERN.match('\n'.join(lines))
        return cls(text=match.group(1).strip() if match else '')

    @staticmethod
    def format_lines(lines: typing.List[str]) -> typing.List[str]:
        """"""
        Return the lines for a style block.

        :param lines: style lines
        :returns: list of lines for a style block
        """"""
        return ['STYLE', *lines]","
class WebVTTStyleBlock:
    
    def __init__(self, text: str):
        
        pass
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        
        pass
    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTStyleBlock':
        
        pass
    @staticmethod
    def format_lines(lines: typing.List[str]) -> typing.List[str]:
        
        pass",snippet_163,5,"```python
import typing

class WebVTTStyleBlock:
    """"""
    A minimal representation of a WebVTT style block.

    The block is expected to start with a line that begins with the keyword
    ``STYLE`` (case‑insensitive) and to contain at least one CSS rule
    enclosed in braces.  The class provides helpers to validate a sequence
    of lines, to construct an instance from an iterable of lines, and to
    format a list of lines for output.
    """"""

    def __init__(self, text: str):
        """"""
        Initialise the style block with the raw text representation.

        Parameters
        ----------
        text : str
            The raw text of the style block, including line breaks.
        """"""
        self.text = text

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Determine whether the supplied lines form a valid WebVTT style block.

        A block is considered valid if:
        * The first non‑empty line starts with ``STYLE`` (case‑insensitive).
        * The block contains at least one opening ``{`` and one closing ``}``.
        * The number of opening and closing braces match.

        Parameters
        ----------
        lines : Sequence[str]
            The lines to validate.

        Returns
        -------
        bool
            ``True`` if the lines form a valid style block, otherwise ``False``.
        """"""
        if not lines:
            return False

        # Find the first non‑empty line
        first_line = None
        for line in lines:
            if line.strip():
                first_line = line.strip()
                break
        if first_line is None or not first_line.upper().startswith(""STYLE""):
            return False

        # Count braces
        open_braces = 0
        close_braces = 0
        for line in lines:
            open_braces += line.count(""{"")
            close_braces += line.count(""}"")

        return open_braces > 0 and close_braces > 0 and open_braces == close_braces

    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTStyleBlock':
        """"""
        Create a ``WebVTTStyleBlock`` instance from an iterable of lines.

        Parameters
        ----------
        lines : Iterable[str]
            The lines that make up the style block.

        Returns
        -------
        WebVTTStyleBlock
            The constructed instance.
        """"""
        # Join lines with a newline to preserve the original formatting
        text = ""\n"".join(lines)
        return cls(text)

    @staticmethod
    def format_lines(lines: typing.List[str]) -> typing.List[str]:
        """"""
        Format a list of lines for output.

        The formatting rules are:
        * Strip leading and trailing whitespace from each line.
        * Remove any empty lines.
        * Ensure that each line ends with a single newline character
          (the caller is responsible for joining them if needed).

        Parameters
        ----------
        lines : List[str]
            The lines to format.

        Returns
        -------
        List[str]
            The formatted lines.
        """"""
        formatted = []
        for line in lines:
            stripped = line.strip()
            if stripped:
                formatted.append(stripped)
        return formatted
```"
251866,Shoobx/xmldiff,Shoobx_xmldiff/xmldiff/diff_match_patch.py,xmldiff.diff_match_patch.patch_obj,"import urllib.parse

class patch_obj:
    """"""Class representing one patch operation.""""""

    def __init__(self):
        """"""Initializes with an empty list of diffs.""""""
        self.diffs = []
        self.start1 = None
        self.start2 = None
        self.length1 = 0
        self.length2 = 0

    def __str__(self):
        """"""Emulate GNU diff's format.
        Header: @@ -382,8 +481,9 @@
        Indices are printed as 1-based, not 0-based.

        Returns:
          The GNU diff string.
        """"""
        if self.length1 == 0:
            coords1 = str(self.start1) + ',0'
        elif self.length1 == 1:
            coords1 = str(self.start1 + 1)
        else:
            coords1 = str(self.start1 + 1) + ',' + str(self.length1)
        if self.length2 == 0:
            coords2 = str(self.start2) + ',0'
        elif self.length2 == 1:
            coords2 = str(self.start2 + 1)
        else:
            coords2 = str(self.start2 + 1) + ',' + str(self.length2)
        text = ['@@ -', coords1, ' +', coords2, ' @@\n']
        for op, data in self.diffs:
            if op == diff_match_patch.DIFF_INSERT:
                text.append('+')
            elif op == diff_match_patch.DIFF_DELETE:
                text.append('-')
            elif op == diff_match_patch.DIFF_EQUAL:
                text.append(' ')
            data = data.encode('utf-8')
            text.append(urllib.parse.quote(data, ""!~*'();/?:@&=+$,# "") + '\n')
        return ''.join(text)","
class patch_obj:
    
    def __init__(self):
        
        pass
    def __str__(self):
        
        pass",snippet_164,3,"class patch_obj:
    def __init__(self):
        # Initialize an empty dictionary to hold patch data
        self._patches = {}

    def __str__(self):
        # Return a readable string representation of the patch object
        return f""patch_obj(patches={self._patches})"""
252656,happyleavesaoc/aoc-mgz,happyleavesaoc_aoc-mgz/mgz/model/inputs.py,mgz.model.inputs.Inputs,"from mgz.model.definitions import Input
from mgz.fast import Action as ActionEnum

class Inputs:
    """"""Normalize player inputs.""""""

    def __init__(self, gaia):
        """"""Initialize.""""""
        self._gaia = gaia
        self._buildings = {}
        self._oid_cache = {}
        self.inputs = []

    def add_chat(self, chat):
        """"""Add chat input.""""""
        self.inputs.append(Input(chat.timestamp, 'Chat', None, dict(message=chat.message), chat.player, None))

    def add_action(self, action):
        """"""Add action input.""""""
        if action.type in (ActionEnum.DE_TRANSFORM, ActionEnum.POSTGAME):
            return
        name = ACTION_TRANSLATE.get(action.type, action.type.name).replace('_', ' ').title()
        param = None
        if 'object_ids' in action.payload and action.payload['object_ids']:
            self._oid_cache[action.type] = action.payload['object_ids']
        elif action.type in self._oid_cache:
            action.payload['object_ids'] = self._oid_cache[action.type]
        if action.type is ActionEnum.SPECIAL:
            name = action.payload['order']
        elif action.type is ActionEnum.GAME:
            name = action.payload['command']
            if name == 'Speed':
                param = action.payload['speed']
        elif action.type is ActionEnum.STANCE:
            name = 'Stance'
            param = action.payload['stance']
        elif action.type is ActionEnum.FORMATION:
            name = 'Formation'
            param = action.payload['formation']
        elif action.type is ActionEnum.ORDER and action.payload['target_id'] in self._gaia:
            name = 'Gather'
            param = self._gaia[action.payload['target_id']]
        elif action.type is ActionEnum.ORDER and action.position and (action.position.hash() in self._buildings):
            name = 'Target'
            param = self._buildings[action.position.hash()]
        elif action.type is ActionEnum.GATHER_POINT:
            if action.payload['target_id'] in self._gaia:
                param = self._gaia[action.payload['target_id']]
            elif action.position and action.position.hash() in self._buildings:
                if len(action.payload['object_ids']) == 1 and action.payload['object_ids'][0] == action.payload['target_id']:
                    name = 'Spawn'
                param = self._buildings[action.position.hash()]
        elif action.type in (ActionEnum.BUY, ActionEnum.SELL):
            action.payload['amount'] *= 100
        elif action.type is ActionEnum.BUILD:
            param = action.payload['building']
            if action.position.hash() in self._buildings:
                if self._buildings[action.position.hash()] == 'Farm' and action.payload['building'] == 'Farm':
                    name = 'Reseed'
            self._buildings[action.position.hash()] = action.payload['building']
        elif action.type in (ActionEnum.QUEUE, ActionEnum.DE_QUEUE):
            param = action.payload['unit']
        elif action.type is ActionEnum.RESEARCH:
            param = action.payload['technology']
        new_input = Input(action.timestamp, name, param, action.payload, action.player, action.position)
        self.inputs.append(new_input)
        return new_input","
class Inputs:
    
    def __init__(self, gaia):
        
        pass
    def add_chat(self, chat):
        
        pass

    def add_action(self, action):
        
        pass",snippet_165,4,"class Inputs:
    def __init__(self, gaia):
        self.gaia = gaia
        self.chats = []
        self.actions = []

    def add_chat(self, chat):
        self.chats.append(chat)

    def add_action(self, action):
        self.actions.append(action)"
256858,cherrypy/cheroot,cheroot/server.py,cheroot.server.Gateway,"class Gateway:
    """"""Base class to interface HTTPServer with other systems, such as WSGI.""""""

    def __init__(self, req):
        """"""Initialize Gateway instance with request.

        Args:
            req (HTTPRequest): current HTTP request
        """"""
        self.req = req

    def respond(self):
        """"""Process the current request. Must be overridden in a subclass.""""""
        raise NotImplementedError","class Gateway:
    
    def __init__(self, req):
        
        pass
    def respond(self):
        
        pass",snippet_168,3,"class Gateway:
    def __init__(self, req):
        self.req = req

    def respond(self):
        return {
            ""status"": 200,
            ""headers"": {""Content-Type"": ""application/json""},
            ""body"": {""received"": self.req}
        }"
258632,chovanecm/sacredboard,chovanecm_sacredboard/sacredboard/app/data/datastorage.py,sacredboard.app.data.datastorage.Cursor,"class Cursor:
    """"""Interface that abstracts the cursor object returned from databases.""""""

    def __init__(self):
        """"""Declare a new cursor to iterate over runs.""""""
        pass

    def count(self):
        """"""Return the number of items in this cursor.""""""
        raise NotImplementedError()

    def __iter__(self):
        """"""Iterate over elements.""""""
        raise NotImplementedError()","class Cursor:
    
    def __init__(self):
        
        pass
    def count(self):
        
        pass
    def __iter__(self):
        
        pass",snippet_169,4,"class Cursor:
    def __init__(self):
        self._items = []

    def count(self):
        return len(self._items)

    def __iter__(self):
        return iter(self._items)

    # Optional helper to add items to the cursor
    def add(self, item):
        self._items.append(item)"
258643,chovanecm/sacredboard,chovanecm_sacredboard/sacredboard/app/data/metricsdao.py,sacredboard.app.data.metricsdao.MetricsDAO,"class MetricsDAO:
    """"""
    Interface for accessing Sacred metrics.

    Issue: https://github.com/chovanecm/sacredboard/issues/58

    Extended because of: https://github.com/chovanecm/sacredboard/issues/66
    """"""

    def get(self, run_id, metric_id):
        """"""
        Read a metric of the given id and run.

        The returned object has the following format (timestamps are datetime
         objects).

        .. code::

            {""steps"": [0,1,20,40,...],
            ""timestamps"": [timestamp1,timestamp2,timestamp3,...],
            ""values"": [0,1 2,3,4,5,6,...],
            ""name"": ""name of the metric"",
            ""metric_id"": ""metric_id"",
            ""run_id"": ""run_id""}

        :param run_id: ID of the Run that the metric belongs to.
        :param metric_id: The ID fo the metric.
        :return: The whole metric as specified.

        :raise NotFoundError
        """"""
        raise NotImplementedError('The MetricsDAO class is abstract.')

    def delete(self, run_id):
        """"""
        Delete all metrics belonging to the given run.

        :param run_id: ID of the Run that the metric belongs to.
        """"""
        raise NotImplementedError('The MetricsDAO class is abstract.')","class MetricsDAO:
    
    def get(self, run_id, metric_id):
        
        pass
    def delete(self, run_id):
        
        pass",snippet_170,3,"```python
class MetricsDAO:
    def __init__(self):
        # In‑memory storage: {run_id: {metric_id: value}}
        self._store = {}

    def get(self, run_id, metric_id):
        """"""Return the metric value for the given run_id and metric_id, or None if not found.""""""
        return self._store.get(run_id, {}).get(metric_id)

    def delete(self, run_id):
        """"""Remove all metrics associated with the given run_id.""""""
        self._store.pop(run_id, None)
```"
258651,chovanecm/sacredboard,chovanecm_sacredboard/sacredboard/app/data/rundao.py,sacredboard.app.data.rundao.RunDAO,"class RunDAO:
    """"""
    Interface for accessing Runs.

    Issue: https://github.com/chovanecm/sacredboard/issues/69
    """"""

    def get(self, run_id):
        """"""
        Return the run associated with the id.

        :raise NotFoundError when not found
        """"""
        raise NotImplementedError('RunDAO is abstract.')

    def get_runs(self, sort_by=None, sort_direction=None, start=0, limit=None, query={'type': 'and', 'filters': []}):
        """"""Return all runs that match the query.""""""
        raise NotImplementedError('RunDAO is abstract.')

    def delete(self, run_id):
        """"""
        Delete run with the given id from the backend.

        :param run_id: Id of the run to delete.
        :raise NotImplementedError If not supported by the backend.
        :raise DataSourceError General data source error.
        :raise NotFoundError The run was not found. (Some backends may succeed
        even if the run does not exist.
        """"""
        raise NotImplementedError('This database data source does not currently support this operation.')","class RunDAO:
    
    def get(self, run_id):
        
        pass
    def get_runs(self, sort_by=None, sort_direction=None, start=0, limit=None, query={'type': 'and', 'filters': []}):
        
        pass
    def delete(self, run_id):
        
        pass",snippet_171,4,"```python
class RunDAO:
    """"""
    A simple in-memory DAO for run objects.
    Each run is expected to be a dictionary containing at least an 'id' key.
    """"""

    def __init__(self, runs=None):
        """"""
        Initialize the DAO with an optional list of run dictionaries.
        """"""
        self._runs = {}
        if runs:
            for run in runs:
                if 'id' not in run:
                    raise ValueError(""Each run must have an 'id' field"")
                self._runs[run['id']] = run

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------
    def get(self, run_id):
        """"""
        Retrieve a run by its ID.
        Returns the run dictionary or None if not found.
        """"""
        return self._runs.get(run_id)

    def get_runs(self, sort_by=None, sort_direction=None,
                 start=0, limit=None,
                 query={'type': 'and', 'filters': []}):
        """"""
        Retrieve a list of runs with optional filtering, sorting, and pagination.

        Parameters
        ----------
        sort_by : str, optional
            Field name to sort by.
        sort_direction : str, optional
            'asc' for ascending, 'desc' for descending. Defaults to ascending.
        start : int, optional
            Zero-based index of the first result to return.
        limit : int, optional
            Maximum number of results to return. If None, return all after start.
        query : dict, optional
            Dictionary with keys:
                - 'type': 'and' or 'or' (currently only 'and' is supported)
                - 'filters': list of filter dicts, each with:
                    - 'field': field name
                    - 'op': operator ('eq', 'ne', 'lt', 'lte', 'gt', 'gte',
                                 'contains', 'in', 'not_in')
                    - 'value': value to compare against

        Returns
        -------
        list of run dictionaries
        """"""
        # Start with all runs
        runs = list(self._runs.values())

        # Apply filtering
        if query and isinstance(query, dict):
            filters = query.get('filters', [])
            if filters:
                runs = [run for run in runs if self._matches_filters(run, filters, query.get('type', 'and'))]

        # Apply sorting
        if sort_by:
            reverse = sort_direction == 'desc'
            runs.sort(key=lambda r: r.get(sort_by), reverse=reverse)

        # Apply pagination
        if start < 0:
            start = 0
        if limit is None:
            return runs[start:]
        else:
            return runs[start:start + limit]

    def delete(self, run_id):
        """"""
        Delete a run by its ID.
        Returns True if the run was deleted, False if it did not exist.
        """"""
        return self._runs.pop(run_id, None) is not None

    # ------------------------------------------------------------------
    # Helper methods
    # ------------------------------------------------------------------
    def _matches_filters(self, run, filters, logic_type='and'):
        """"""
        Check if a run matches all (or any) of the provided filters.
        """"""
        if logic_type == 'and':
            return all(self._match_filter(run, f) for f in filters)
        elif logic_type == 'or':
            return any(self._match_filter(run, f) for f in filters)
        else:
            raise ValueError(f""Unsupported logic type: {logic_type}"")

    def _match_filter(self, run, filter_dict):
        """"""
        Evaluate a single filter against a run.
        """"""
        field = filter_dict.get('field')
        op = filter_dict.get('op')
        value = filter_dict.get('value')

        if field not in run:
            return False

        run_value = run[field]

        if op == 'eq':
            return run_value == value
        if op == 'ne':
            return run_value != value
        if op == 'lt':
            return run_value < value
        if op == 'lte':
            return run_value <= value
        if op == 'gt':
            return run_value > value
        if op == 'gte':
            return run_value >= value
        if op == 'contains':
            return value in run_value if isinstance(run_value, (str, list, tuple, set)) else False
        if op == 'in':
            return run_value in value if isinstance(value, (list, tuple, set)) else False
        if op == 'not_in':
            return run_value not in value if isinstance(value, (list, tuple, set)) else False

        raise ValueError(f""Unsupported operator: {op}"")

    # ------------------------------------------------------------------
    # Convenience methods (not part of the original skeleton)
    # ------------------------------------------------------------------
    def add(self, run):
        """"""
        Add a new run to the DAO.
        """"""
        if 'id' not in run:
            raise ValueError(""Run must have an 'id' field"")
        self._runs[run['id']] = run

    def update(self, run_id, updates):
        """"""
        Update fields of an existing run.
        """"""
        if run_id not in self._runs:
            raise KeyError(f""Run with id {run_id} not found"")
        self._runs[run_id].update(updates)
```"
259541,mozilla-iot/webthing-python,mozilla-iot_webthing-python/webthing/server.py,webthing.server.MultipleThings,"class MultipleThings:
    """"""A container for multiple things.""""""

    def __init__(self, things, name):
        """"""
        Initialize the container.

        things -- the things to store
        name -- the mDNS server name
        """"""
        self.things = things
        self.name = name

    def get_thing(self, idx):
        """"""
        Get the thing at the given index.

        idx -- the index
        """"""
        try:
            idx = int(idx)
        except ValueError:
            return None
        if idx < 0 or idx >= len(self.things):
            return None
        return self.things[idx]

    def get_things(self):
        """"""Get the list of things.""""""
        return self.things

    def get_name(self):
        """"""Get the mDNS server name.""""""
        return self.name","class MultipleThings:
    
    def __init__(self, things, name):
        
        pass
    def get_thing(self, idx):
        
        pass
    def get_things(self):
        
        pass

    def get_name(self):
        
        pass",snippet_172,5,"class MultipleThings:
    
    def __init__(self, things, name):
        self.things = things
        self.name = name

    def get_thing(self, idx):
        return self.things[idx]

    def get_things(self):
        return self.things

    def get_name(self):
        return self.name"
259544,mozilla-iot/webthing-python,mozilla-iot_webthing-python/webthing/server.py,webthing.server.SingleThing,"class SingleThing:
    """"""A container for a single thing.""""""

    def __init__(self, thing):
        """"""
        Initialize the container.

        thing -- the thing to store
        """"""
        self.thing = thing

    def get_thing(self, _=None):
        """"""Get the thing at the given index.""""""
        return self.thing

    def get_things(self):
        """"""Get the list of things.""""""
        return [self.thing]

    def get_name(self):
        """"""Get the mDNS server name.""""""
        return self.thing.title","class SingleThing:
    
    def __init__(self, thing):
        
        pass
    def get_thing(self, _=None):
        
        pass
    def get_things(self):
        
        pass

    def get_name(self):
        
        pass",snippet_173,5,"class SingleThing:
    def __init__(self, thing):
        self._thing = thing

    def get_thing(self, _=None):
        return self._thing

    def get_things(self):
        return [self._thing]

    def get_name(self):
        if hasattr(self._thing, ""name""):
            return self._thing.name
        return str(self._thing)"
259548,mozilla-iot/webthing-python,mozilla-iot_webthing-python/webthing/subscriber.py,webthing.subscriber.Subscriber,"class Subscriber:
    """"""Abstract Subscriber class.""""""

    def update_property(self, property_):
        """"""
        Send an update about a Property.

        :param property_: Property
        """"""
        raise NotImplementedError

    def update_action(self, action):
        """"""
        Send an update about an Action.

        :param action: Action
        """"""
        raise NotImplementedError

    def update_event(self, event):
        """"""
        Send an update about an Event.

        :param event: Event
        """"""
        raise NotImplementedError","class Subscriber:
    
    def update_property(self, property_):
        
        pass
    def update_action(self, action):
        
        pass
    def update_event(self, event):
        
        pass",snippet_174,4,"```python
class Subscriber:
    def __init__(self):
        self._properties = []
        self._actions = []
        self._events = []

    def update_property(self, property_):
        """"""Record a property update.""""""
        self._properties.append(property_)

    def update_action(self, action):
        """"""Record an action update.""""""
        self._actions.append(action)

    def update_event(self, event):
        """"""Record an event update.""""""
        self._events.append(event)

    @property
    def properties(self):
        return list(self._properties)

    @property
    def actions(self):
        return list(self._actions)

    @property
    def events(self):
        return list(self._events)

    def __repr__(self):
        return (
            f""<Subscriber properties={self._properties} ""
            f""actions={self._actions} events={self._events}>""
        )
```"
261819,user-cont/conu,user-cont_conu/conu/apidefs/image.py,conu.apidefs.image.S2Image,"class S2Image:
    """"""
    Additional functionality related to s2i-enabled container images
    """"""

    def extend(self, source, new_image_name, s2i_args=None):
        """"""
        extend this s2i-enabled image using provided source, raises ConuException if
        `s2i build` fails

        :param source: str, source used to extend the image, can be path or url
        :param new_image_name: str, name of the new, extended image
        :param s2i_args: list of str, additional options and arguments provided to `s2i build`
        :return: S2Image instance
        """"""
        raise NotImplementedError('extend method is not implemented')

    def usage(self):
        """"""
        Provide output of `s2i usage`

        :return: str
        """"""
        raise NotImplementedError('usage method is not implemented')","class S2Image:
    
    def extend(self, source, new_image_name, s2i_args=None):
        
        pass
    def usage(self):
        
        pass",snippet_175,3,"```python
import subprocess
from typing import List, Dict, Union, Optional


class S2Image:
    """"""
    A small helper wrapper around the `s2i` (Source‑to‑Image) command line tool.
    """"""

    def __init__(self, s2i_path: str = ""s2i"") -> None:
        """"""
        Parameters
        ----------
        s2i_path : str, optional
            Path to the `s2i` executable. Defaults to ``""s2i""`` which assumes
            the binary is on the system ``PATH``.
        """"""
        self.s2i_path = s2i_path

    def extend(
        self,
        source: str,
        new_image_name: str,
        s2i_args: Optional[Union[List[str], Dict[str, str], str]] = None,
    ) -> str:
        """"""
        Build a new image from the given source using the `s2i` tool.

        Parameters
        ----------
        source : str
            Path to the source directory or a Git URL.
        new_image_name : str
            Desired name of the resulting image.
        s2i_args : list | dict | str, optional
            Additional arguments to pass to `s2i`. If a dictionary is supplied,
            it is converted to ``key=value`` pairs. If a string is supplied,
            it is treated as a single argument.

        Returns
        -------
        str
            The standard output produced by the `s2i` command.

        Raises
        ------
        RuntimeError
            If the `s2i` command exits with a non‑zero status.
        TypeError
            If ``s2i_args`` is not one of the accepted types.
        """"""
        if s2i_args is None:
            s2i_args = []

        # Normalise the arguments
        if isinstance(s2i_args, dict):
            s2i_args = [f""{k}={v}"" for k, v in s2i_args.items()]
        elif isinstance(s2i_args, str):
            s2i_args = [s2i_args]
        elif not isinstance(s2i_args, list):
            raise TypeError(
                ""s2i_args must be a list, dict, or str, got ""
                f""{type(s2i_args).__name__}""
            )

        cmd = [self.s2i_path, ""build"", source, new_image_name] + s2i_args

        result = subprocess.run(
            cmd, capture_output=True, text=True, check=False
        )

        if result.returncode != 0:
            raise RuntimeError(
                f""s2i build failed (exit {result.returncode}):\n{result.stderr}""
            )

        return result.stdout

    def usage(self) -> None:
        """"""
        Print a short help message describing how to use the :class:`S2Image`
        wrapper.
        """"""
        help_msg = (
            ""S2Image helper\n""
            ""==============\n""
            ""Methods\n""
            ""-------\n""
            ""extend(source, new_image_name, s2i_args=None)\n""
            ""    Build a new image using the Source‑to‑Image tool.\n""
            ""    Parameters:\n""
            ""        source (str): Path to source code or Git URL.\n""
            ""        new_image_name (str): Desired image name.\n""
            ""        s2i_args (list|dict|str, optional): Extra arguments for s2i.\n""
            ""\n""
            ""usage()\n""
            ""    Print this help message.\n""
        )
        print(help_msg)
```"
264367,materialsproject/custodian,materialsproject_custodian/src/custodian/ansible/interpreter.py,custodian.ansible.interpreter.Modder,"from custodian.ansible.actions import DictActions
import re

class Modder:
    """"""
    Class to modify a dict/file/any object using a mongo-like language.
    Keywords are mostly adopted from mongo's syntax, but instead of $, an
    underscore precedes action keywords. This is so that the modification can
    be inserted into a mongo db easily.

    Allowable actions are supplied as a list of classes as an argument. Refer
    to the action classes on what the actions do. Action classes are in
    pymatpro.ansible.actions.

    Examples:
    >>> modder = Modder()
    >>> dct = {""Hello"": ""World""}
    >>> mod = {'_set': {'Hello':'Universe', 'Bye': 'World'}}
    >>> modder.modify(mod, dct)
    >>> dct['Bye']
    'World'
    >>> dct['Hello']
    'Universe'
    """"""

    def __init__(self, actions=None, strict=True, directory='./') -> None:
        """"""Initialize a Modder from a list of supported actions.

        Args:
            actions ([Action]): A sequence of supported actions. See
                :mod:`custodian.ansible.actions`. Default is None,
                which means only DictActions are supported.
            strict (bool): Indicating whether to use strict mode. In non-strict
                mode, unsupported actions are simply ignored without any
                errors raised. In strict mode, if an unsupported action is
                supplied, a ValueError is raised. Defaults to True.
            directory (str): The directory containing the files to be modified.
                Defaults to ""./"".
        """"""
        self.supported_actions = {}
        actions = actions if actions is not None else [DictActions]
        for action in actions:
            for attr in dir(action):
                if not re.match('__\\w+__', attr) and callable(getattr(action, attr)):
                    self.supported_actions[f'_{attr}'] = getattr(action, attr)
        self.strict = strict
        self.directory = directory

    def modify(self, modification, obj) -> None:
        """"""
        Note that modify makes actual in-place modifications. It does not
        return a copy.

        Args:
            modification (dict): Modification must be {action_keyword :
                settings}. E.g., {'_set': {'Hello':'Universe', 'Bye': 'World'}}
            obj (dict/str/object): Object to modify depending on actions. For
                example, for DictActions, obj will be a dict to be modified.
                For FileActions, obj will be a string with a full pathname to a
                file.
        """"""
        for action, settings in modification.items():
            if action in self.supported_actions:
                self.supported_actions[action](obj, settings, directory=self.directory)
            elif self.strict:
                raise ValueError(f'{action} is not a supported action!')

    def modify_object(self, modification, obj):
        """"""
        Modify an object that supports pymatgen's as_dict() and from_dict API.

        Args:
            modification (dict): Modification must be {action_keyword :
                settings}. E.g., {'_set': {'Hello':'Universe', 'Bye': 'World'}}
            obj (object): Object to modify
        """"""
        dct = obj.as_dict()
        self.modify(modification, dct)
        return obj.from_dict(dct)","
class Modder:
    '''
    Class to modify a dict/file/any object using a mongo-like language.
    Keywords are mostly adopted from mongo's syntax, but instead of $, an
    underscore precedes action keywords. This is so that the modification can
    be inserted into a mongo db easily.
    Allowable actions are supplied as a list of classes as an argument. Refer
    to the action classes on what the actions do. Action classes are in
    pymatpro.ansible.actions.
    Examples:
    >>> modder = Modder()
    >>> dct = {""Hello"": ""World""}
    >>> mod = {'_set': {'Hello':'Universe', 'Bye': 'World'}}
    >>> modder.modify(mod, dct)
    >>> dct['Bye']
    'World'
    >>> dct['Hello']
    'Universe'
    '''
    def __init__(self, actions=None, strict=True, directory='./') -> None:
        
        pass

    def modify(self, modification, obj) -> None:
        
        pass

    def modify_object(self, modification, obj):
        
        pass",snippet_176,4,"```python
import json
import os
from typing import Any, Dict, List, Optional, Type


class _BaseAction:
    """"""Base class for all actions.""""""

    keyword: str = """"

    def apply(self, value: Any, obj: Any) -> None:
        """"""Apply the action to the object.""""""
        raise NotImplementedError


class _SetAction(_BaseAction):
    keyword = ""_set""

    def apply(self, value: Dict[str, Any], obj: Dict[str, Any]) -> None:
        if not isinstance(value, dict):
            raise ValueError(f""{self.keyword} expects a dict, got {type(value)}"")
        for k, v in value.items():
            obj[k] = v


class _UnsetAction(_BaseAction):
    keyword = ""_unset""

    def apply(self, value: List[str], obj: Dict[str, Any]) -> None:
        if not isinstance(value, list):
            raise ValueError(f""{self.keyword} expects a list, got {type(value)}"")
        for k in value:
            obj.pop(k, None)


class Modder:
    """"""
    Class to modify a dict/file/any object using a mongo-like language.
    Keywords are mostly adopted from mongo's syntax, but instead of `$`, an
    underscore precedes action keywords. This is so that the modification can
    be inserted into a mongo db easily.
    Allowable actions are supplied as a list of classes as an argument. Refer
    to the action classes on what the actions do. Action classes are in
    pymatpro.ansible.actions.
    Examples:
    >>> modder = Modder()
    >>> dct = {""Hello"": ""World""}
    >>> mod = {'_set': {'Hello':'Universe', 'Bye': 'World'}}
    >>> modder.modify(mod, dct)
    >>> dct['Bye']
    'World'
    >>> dct['Hello']
    'Universe'
    """"""

    def __init__(
        self,
        actions: Optional[List[Type[_BaseAction]]] = None,
        strict: bool = True,
        directory: str = ""./"",
    ) -> None:
        """"""
        Parameters
        ----------
        actions : list of action classes
            List of classes that implement the action logic. If None, a
            default set of actions is used.
        strict : bool
            If True, unknown actions raise an error. If False, they are ignored.
        directory : str
            Base directory for file operations (not used in this minimal
            implementation).
        """"""
        self.strict = strict
        self.directory = directory

        # Default actions if none provided
        if actions is None:
            actions = [_SetAction, _UnsetAction]

        # Instantiate actions and map keyword to instance
        self.action_map: Dict[str, _BaseAction] = {}
        for act_cls in actions:
            if not issubclass(act_cls, _BaseAction):
                raise TypeError(f""Action {act_cls} must subclass _BaseAction"")
            act = act_cls()
            if act.keyword in self.action_map:
                raise ValueError(f""Duplicate action keyword: {act.keyword}"")
            self.action_map[act.keyword] = act

    def modify(self, modification: Any, obj: Any) -> None:
        """"""
        Apply a modification to an object.

        Parameters
        ----------
        modification : dict or list
            The modification specification.
        obj : dict or file path
            The object to modify. If a string, it is treated as a file path
            and the file is loaded as JSON, modified, and written back.
        """"""
        if isinstance(obj, str):
            # Treat as file path
            path = os.path.join(self.directory, obj)
            with open(path, ""r"", encoding=""utf-8"") as f:
                data = json.load(f)
            self.modify_object(modification, data)
            with open(path, ""w"", encoding=""utf-8"") as f:
                json.dump(data, f, indent=2)
        elif isinstance(obj, dict):
            self.modify_object(modification, obj)
        else:
            raise TypeError(f""Unsupported object type: {type(obj)}"")

    def modify_object(self, modification: Any, obj: Any) -> None:
        """"""
        Recursively apply modifications to a dict-like object.

        Parameters
        ----------
        modification : dict
            The modification specification.
        obj : dict
            The target dictionary to modify.
        """"""
        if not isinstance(modification, dict):
            raise ValueError(""Modification must be a dict"")

        for key, value in modification.items():
            if key.startswith(""_""):
                # Action keyword
                action = self.action_map.get(key)
                if action is None:
                    if self.strict:
                        raise KeyError(f""Unknown action keyword: {key}"")
                    else:
                        continue
                action.apply(value, obj)
            else:
                # Nested modification
                if key not in obj or not isinstance(obj[key], dict):
                    # Create nested dict if missing or not a dict
                    obj[key] = {}
                if isinstance(value, dict):
                    self.modify_object(value, obj[key])
                else:
                    # Direct assignment for non-dict values
                    obj[key] = value
```"
264406,materialsproject/custodian,materialsproject_custodian/src/custodian/utils.py,custodian.utils.tracked_lru_cache,"import functools
from typing import ClassVar

class tracked_lru_cache:
    """"""
    Decorator wrapping the functools.lru_cache adding a tracking of the
    functions that have been wrapped.

    Exposes a method to clear the cache of all the wrapped functions.

    Used to cache the parsed outputs in handlers/validators, to avoid
    multiple parsing of the same file.
    Allows Custodian to clear the cache after all the checks have been performed.
    """"""
    cached_functions: ClassVar = set()

    def __init__(self, func) -> None:
        """"""
        Args:
            func: function to be decorated.
        """"""
        self.func = functools.lru_cache(func)
        functools.update_wrapper(self, func)
        self.cache_info = self.func.cache_info
        self.cache_clear = self.func.cache_clear

    def __call__(self, *args, **kwargs):
        """"""Call the decorated function.""""""
        result = self.func(*args, **kwargs)
        self.cached_functions.add(self.func)
        return result

    @classmethod
    def tracked_cache_clear(cls) -> None:
        """"""Clear the cache of all the decorated functions.""""""
        while cls.cached_functions:
            f = cls.cached_functions.pop()
            f.cache_clear()","
class tracked_lru_cache:
    
    def __init__(self, func) -> None:
        
        pass
    def __call__(self, *args, **kwargs):
        
        pass
    @classmethod
    def tracked_cache_clear(cls) -> None:
        
        pass",snippet_177,4,"```python
import collections
import functools
import weakref
from typing import Any, Callable, Tuple, FrozenSet


class tracked_lru_cache:
    """"""
    A simple LRU cache decorator that tracks hits and misses.
    The cache is unlimited in size unless a maxsize is specified.
    """"""

    # Registry of all decorator instances for global clearing
    _instances = weakref.WeakSet()

    def __init__(self, func: Callable) -> None:
        if not callable(func):
            raise TypeError(""tracked_lru_cache can only decorate callables"")
        self._func = func
        self._cache: ""collections.OrderedDict[Tuple[Any, FrozenSet[Tuple[Any, Any]]], Any]"" = collections.OrderedDict()
        self.hits: int = 0
        self.misses: int = 0
        # Preserve function metadata
        functools.update_wrapper(self, func)
        # Register instance
        self._instances.add(self)

    def __call__(self, *args: Any, **kwargs: Any) -> Any:
        # Build a hashable key from args and kwargs
        key = (args, frozenset(kwargs.items()))
        if key in self._cache:
            # Cache hit: move to end to mark as recently used
            self._cache.move_to_end(key)
            self.hits += 1
            return self._cache[key]
        # Cache miss: compute value and store
        result = self._func(*args, **kwargs)
        self._cache[key] = result
        self._cache.move_to_end(key)
        self.misses += 1
        return result

    @classmethod
    def tracked_cache_clear(cls) -> None:
        """"""
        Clear the cache and reset hit/miss counters for all tracked_lru_cache instances.
        """"""
        for instance in list(cls._instances):
            instance._cache.clear()
            instance.hits = 0
            instance.misses = 0
```"
265199,Anaconda-Platform/anaconda-client,Anaconda-Platform_anaconda-client/binstar_client/utils/tables.py,binstar_client.utils.tables.TableCell,"import typing

class TableCell:
    """"""
    General definition of a table cell.

    :param kind: Kind of the cell (used for styling purposes, see :class:`~TableDesign`).
    :param value: Exact content of the cell.
    :param alignment: How text should be aligned in the cell.
    """"""
    __slots__ = ('alignment', 'kind', 'value')

    def __init__(self, kind: str, value: typing.Any, *, alignment: 'Alignment'='<') -> None:
        """"""Initialize new :class:`~TableCell` instance.""""""
        if value is None:
            value = ''
        self.alignment: 'Alignment' = alignment
        self.kind: str = kind
        self.value: str = str(value)

    def __repr__(self) -> str:
        """"""Prepare a string representation of the instance.""""""
        return f'{type(self).__name__}(kind={self.kind!r}, value={self.value!r}, alignment={self.alignment!r})'

    def __str__(self) -> str:
        """"""Prepare a string representation of the instance.""""""
        return self.value","
class TableCell:
    
    def __init__(self, kind: str, value: typing.Any, *, alignment: 'Alignment'='<') -> None:
        
        pass
    def __repr__(self) -> str:
        
        pass

    def __str__(self) -> str:
        
        pass",snippet_178,4,"```python
import typing

Alignment = str  # simple alias for alignment specifier


class TableCell:
    """"""
    Represents a single cell in a table.

    Parameters
    ----------
    kind : str
        The kind of the cell (e.g., 'header', 'data', etc.).
    value : Any
        The value stored in the cell.
    alignment : Alignment, optional
        Alignment specifier for the cell content. Defaults to '<' (left).
    """"""

    def __init__(self, kind: str, value: typing.Any, *, alignment: Alignment = ""<"") -> None:
        self.kind = kind
        self.value = value
        self.alignment = alignment

    def __repr__(self) -> str:
        return (
            f""{self.__class__.__name__}(""
            f""kind={self.kind!r}, ""
            f""value={self.value!r}, ""
            f""alignment={self.alignment!r})""
        )

    def __str__(self) -> str:
        return str(self.value)
```"
266878,facelessuser/wcmatch,facelessuser_wcmatch/wcmatch/util.py,wcmatch.util.Immutable,"from typing import Any, Callable, AnyStr, Match, Pattern

class Immutable:
    """"""Immutable.""""""
    __slots__: tuple[Any, ...] = ()

    def __init__(self, **kwargs: Any) -> None:
        """"""Initialize.""""""
        for k, v in kwargs.items():
            super(Immutable, self).__setattr__(k, v)

    def __setattr__(self, name: str, value: Any) -> None:
        """"""Prevent mutability.""""""
        raise AttributeError('Class is immutable!')","
class Immutable:
    
    def __init__(self, **kwargs: Any) -> None:
        
        pass
    def __setattr__(self, name: str, value: Any) -> None:
        
        pass",snippet_180,3,"```python
from typing import Any


class Immutable:
    def __init__(self, **kwargs: Any) -> None:
        # Set all provided attributes directly, bypassing __setattr__
        for key, value in kwargs.items():
            object.__setattr__(self, key, value)
        # Mark the instance as frozen
        object.__setattr__(self, ""_frozen"", True)

    def __setattr__(self, name: str, value: Any) -> None:
        # Allow setting the _frozen flag itself
        if name == ""_frozen"":
            object.__setattr__(self, name, value)
            return

        # If the instance is frozen, disallow any further attribute changes
        if getattr(self, ""_frozen"", False):
            raise AttributeError(f""Cannot modify immutable instance: {name}"")

        # Otherwise, allow setting the attribute
        object.__setattr__(self, name, value)
```"
269275,CLARIAH/grlc,CLARIAH_grlc/src/fileLoaders.py,src.fileLoaders.BaseLoader,"from grlc.queryTypes import qType, guessQueryType
import json

class BaseLoader:
    """"""Base class for File Loaders""""""

    def getTextForName(self, query_name):
        """"""Return the query text and query type for the given query name.
        Note that file extention is not part of the query name. For example,
        for `query_name='query1'` would return the content of file `query1.rq`
        from the loader's source (assuming such file exists).""""""
        candidateNames = [query_name + '.rq', query_name + '.sparql', query_name + '.tpf', query_name + '.json']
        candidates = [(name, guessQueryType(name)) for name in candidateNames]
        for queryFullName, queryType in candidates:
            queryText = self._getText(queryFullName)
            if queryText:
                if queryType == qType['JSON']:
                    queryText = json.loads(queryText)
                    if 'proto' not in queryText and '@graph' not in queryText:
                        continue
                return (queryText, queryType)
        return ('', None)

    def _getText(self, queryFullName):
        """"""To be implemented by sub-classes.
        Returns None if the file does not exist.""""""
        raise NotImplementedError('Subclasses must override _getText()!')

    def fetchFiles(self):
        """"""To be implemented by sub-classes""""""
        raise NotImplementedError('Subclasses must override fetchFiles()!')","
class BaseLoader:
    
    def getTextForName(self, query_name):
        
        pass
    def _getText(self, queryFullName):
        
        pass

    def fetchFiles(self):
        
        pass",snippet_181,4,"```python
import os

class BaseLoader:
    """"""
    A simple loader that reads text files from a directory.
    Each file is identified by its base name (without extension).
    """"""

    def __init__(self, directory, file_extension="".txt""):
        """"""
        Parameters
        ----------
        directory : str
            Path to the directory containing the text files.
        file_extension : str, optional
            Extension of the files to load (default: "".txt"").
        """"""
        self.directory = os.path.abspath(directory)
        self.file_extension = file_extension
        self._files = None          # mapping: name -> full path
        self._cache = {}            # mapping: name -> file content

    def fetchFiles(self):
        """"""
        Scan the directory for files with the specified extension and
        build a mapping from file base name to full path.

        Returns
        -------
        dict
            Mapping of file names (without extension) to absolute file paths.
        """"""
        if self._files is None:
            self._files = {}
            for root, _, files in os.walk(self.directory):
                for fname in files:
                    if fname.lower().endswith(self.file_extension):
                        name = os.path.splitext(fname)[0]
                        full_path = os.path.join(root, fname)
                        self._files[name] = full_path
        return self._files

    def _getText(self, queryFullName):
        """"""
        Read the content of a file.

        Parameters
        ----------
        queryFullName : str
            Absolute path to the file.

        Returns
        -------
        str
            The file's content.

        Raises
        ------
        FileNotFoundError
            If the file does not exist.
        """"""
        if not os.path.isfile(queryFullName):
            raise FileNotFoundError(f""File not found: {queryFullName}"")
        with open(queryFullName, ""r"", encoding=""utf-8"") as f:
            return f.read()

    def getTextForName(self, query_name):
        """"""
        Retrieve the text content for a given file name.

        Parameters
        ----------
        query_name : str
            The base name of the file (without extension).

        Returns
        -------
        str
            The file's content.

        Raises
        ------
        KeyError
            If the name is not found among the loaded files.
        """"""
        if query_name in self._cache:
            return self._cache[query_name]

        files = self.fetchFiles()
        if query_name not in files:
            raise KeyError(f""Name '{query_name}' not found in directory '{self.directory}'"")

        full_path = files[query_name]
        text = self._getText(full_path)
        self._cache[query_name] = text
        return text
```"
271814,shoebot/shoebot,shoebot_shoebot/lib/photobot/__init__.py,photobot.Blend,"from PIL import Image, ImageChops, ImageFilter, ImageEnhance, ImageOps, ImageDraw, ImageStat

class Blend:
    """"""Layer blending modes.

    Implements additional blending modes to those present in PIL.
    These blending functions can not be used separately from
    the canvas.flatten() method, where the alpha compositing
    of two layers is handled.

    Since these blending are not part of a C library,
    but pure Python, they take forever to process.
    """"""

    def overlay(self, img1, img2):
        """"""Applies the overlay blend mode.

        Overlays image img2 on image img1. The overlay pixel combines
        multiply and screen: it multiplies dark pixels values and screen
        light values. Returns a composite image with the alpha channel
        retained.
        """"""
        p1 = list(img1.getdata())
        p2 = list(img2.getdata())
        for i in range(len(p1)):
            p3 = ()
            for j in range(len(p1[i])):
                a = p1[i][j] / 255.0
                b = p2[i][j] / 255.0
                if j == 3:
                    d = min(a, b)
                elif a > 0.5:
                    d = 2 * (a + b - a * b) - 1
                else:
                    d = 2 * a * b
                p3 += (int(d * 255),)
            p1[i] = p3
        img = Image.new('RGBA', img1.size, 255)
        img.putdata(p1)
        return img

    def hue(self, img1, img2):
        """"""Applies the hue blend mode.

        Hues image img1 with image img2. The hue filter replaces the
        hues of pixels in img1 with the hues of pixels in img2. Returns
        a composite image with the alpha channel retained.
        """"""
        import colorsys
        p1 = list(img1.getdata())
        p2 = list(img2.getdata())
        for i in range(len(p1)):
            r1, g1, b1, a1 = p1[i]
            r1 = r1 / 255.0
            g1 = g1 / 255.0
            b1 = b1 / 255.0
            h1, s1, v1 = colorsys.rgb_to_hsv(r1, g1, b1)
            r2, g2, b2, a2 = p2[i]
            r2 = r2 / 255.0
            g2 = g2 / 255.0
            b2 = b2 / 255.0
            h2, s2, v2 = colorsys.rgb_to_hsv(r2, g2, b2)
            r3, g3, b3 = colorsys.hsv_to_rgb(h2, s1, v1)
            r3 = int(r3 * 255)
            g3 = int(g3 * 255)
            b3 = int(b3 * 255)
            p1[i] = (r3, g3, b3, a1)
        img = Image.new('RGBA', img1.size, 255)
        img.putdata(p1)
        return img

    def color(self, img1, img2):
        """"""Applies the color blend mode.

        Colorize image img1 with image img2. The color filter replaces
        the hue and saturation of pixels in img1 with the hue and
        saturation of pixels in img2. Returns a composite image with the
        alpha channel retained.
        """"""
        import colorsys
        p1 = list(img1.getdata())
        p2 = list(img2.getdata())
        for i in range(len(p1)):
            r1, g1, b1, a1 = p1[i]
            r1 = r1 / 255.0
            g1 = g1 / 255.0
            b1 = b1 / 255.0
            h1, s1, v1 = colorsys.rgb_to_hsv(r1, g1, b1)
            r2, g2, b2, a2 = p2[i]
            r2 = r2 / 255.0
            g2 = g2 / 255.0
            b2 = b2 / 255.0
            h2, s2, v2 = colorsys.rgb_to_hsv(r2, g2, b2)
            r3, g3, b3 = colorsys.hsv_to_rgb(h2, s2, v1)
            r3 = int(r3 * 255)
            g3 = int(g3 * 255)
            b3 = int(b3 * 255)
            p1[i] = (r3, g3, b3, a1)
        img = Image.new('RGBA', img1.size, 255)
        img.putdata(p1)
        return img","
class Blend:
    
    def overlay(self, img1, img2):
        
        pass
    def hue(self, img1, img2):
        
        pass

    def color(self, img1, img2):
        
        pass",snippet_182,4,"```python
import cv2
import numpy as np

class Blend:
    def _check_shape(self, img1, img2):
        if img1.shape != img2.shape:
            raise ValueError(""Images must have the same shape"")
        if img1.dtype != np.uint8 or img2.dtype != np.uint8:
            raise ValueError(""Images must be of type uint8"")

    def overlay(self, img1, img2):
        """"""
        Overlay blend mode.
        """"""
        self._check_shape(img1, img2)
        # Convert to float for computation
        a = img1.astype(np.float32) / 255.0
        b = img2.astype(np.float32) / 255.0

        # Overlay formula
        out = np.where(a < 0.5,
                       2 * a * b,
                       1 - 2 * (1 - a) * (1 - b))

        out = (out * 255).clip(0, 255).astype(np.uint8)
        return out

    def hue(self, img1, img2):
        """"""
        Hue blend mode: replace hue of img1 with hue of img2.
        """"""
        self._check_shape(img1, img2)
        hsv1 = cv2.cvtColor(img1, cv2.COLOR_BGR2HSV).astype(np.float32)
        hsv2 = cv2.cvtColor(img2, cv2.COLOR_BGR2HSV).astype(np.float32)

        # Replace hue channel
        hsv1[..., 0] = hsv2[..., 0]
        # Convert back to BGR
        out = cv2.cvtColor(hsv1.astype(np.uint8), cv2.COLOR_HSV2BGR)
        return out

    def color(self, img1, img2):
        """"""
        Color blend mode: replace saturation and value of img1 with those of img2.
        """"""
        self._check_shape(img1, img2)
        hsv1 = cv2.cvtColor(img1, cv2.COLOR_BGR2HSV).astype(np.float32)
        hsv2 = cv2.cvtColor(img2, cv2.COLOR_BGR2HSV).astype(np.float32)

        # Replace saturation and value channels
        hsv1[..., 1] = hsv2[..., 1]
        hsv1[..., 2] = hsv2[..., 2]
        # Convert back to BGR
        out = cv2.cvtColor(hsv1.astype(np.uint8), cv2.COLOR_HSV2BGR)
        return out
```"
272626,cackharot/suds-py3,cackharot_suds-py3/suds/reader.py,suds.reader.Reader,"from suds.plugin import PluginContainer
import hashlib

class Reader:
    """"""
    The reader provides integration with cache.
    @ivar options: An options object.
    @type options: I{Options}
    """"""

    def __init__(self, options):
        """"""
        @param options: An options object.
        @type options: I{Options}
        """"""
        self.options = options
        self.plugins = PluginContainer(options.plugins)

    def mangle(self, name, x):
        """"""
        Mangle the name by hashing the I{name} and appending I{x}.
        @return: the mangled name.
        """"""
        h = hashlib.sha256(name.encode('utf8')).hexdigest()
        return '%s-%s' % (h, x)","
class Reader:
    
    def __init__(self, options):
        
        pass
    def mangle(self, name, x):
        
        pass",snippet_183,3,"```python
import hashlib

class Reader:
    def __init__(self, options):
        """"""
        Initialize the Reader with a dictionary of options.
        Supported options:
            - prefix: string to prepend to the name
            - suffix: string to append to the name
            - separator: string used between name and x when hashing
            - hash: boolean, if True use a SHA-256 hash of the name and x
        """"""
        if not isinstance(options, dict):
            raise TypeError(""options must be a dictionary"")
        self.options = options

    def mangle(self, name, x):
        """"""
        Return a mangled version of `name` based on the options.
        If the 'hash' option is True, return a SHA-256 hex digest of
        the concatenation of name, separator, and x (converted to string).
        Otherwise, return the name with optional prefix and suffix.
        """"""
        if not isinstance(name, str):
            raise TypeError(""name must be a string"")

        # Handle hashing first
        if self.options.get(""hash""):
            separator = self.options.get(""separator"", ""_"")
            to_hash = f""{name}{separator}{x}""
            return hashlib.sha256(to_hash.encode(""utf-8"")).hexdigest()

        # Build the mangled name with prefix/suffix
        prefix = self.options.get(""prefix"", """")
        suffix = self.options.get(""suffix"", """")
        return f""{prefix}{name}{suffix}""
```"
272646,cackharot/suds-py3,cackharot_suds-py3/suds/sax/enc.py,suds.sax.enc.Encoder,"import re

class Encoder:
    """"""
    An XML special character encoder/decoder.
    @cvar encodings: A mapping of special characters encoding.
    @type encodings: [(str,str)]
    @cvar decodings: A mapping of special characters decoding.
    @type decodings: [(str,str)]
    @cvar special: A list of special characters
    @type special: [char]
    """"""
    encodings = (('&', '&amp;'), ('<', '&lt;'), ('>', '&gt;'), ('""', '&quot;'), (""'"", '&apos;'))
    decodings = (('&lt;', '<'), ('&gt;', '>'), ('&quot;', '""'), ('&apos;', ""'""), ('&amp;', '&'))
    special = ('&', '<', '>', '""', ""'"")

    def needsEncoding(self, s):
        """"""
        Get whether string I{s} contains special characters.
        @param s: A string to check.
        @type s: str
        @return: True if needs encoding.
        @rtype: boolean
        """"""
        if isinstance(s, str):
            for c in self.special:
                if c in s:
                    return True
        return False

    def encode(self, s):
        """"""
        Encode special characters found in string I{s}.
        @param s: A string to encode.
        @type s: str
        @return: The encoded string.
        @rtype: str
        """"""
        if isinstance(s, str) and self.needsEncoding(s):
            for x in self.encodings:
                s = re.sub(x[0], x[1], s)
        return s

    def decode(self, s):
        """"""
        Decode special characters encodings found in string I{s}.
        @param s: A string to decode.
        @type s: str
        @return: The decoded string.
        @rtype: str
        """"""
        if isinstance(s, str) and '&' in s:
            for x in self.decodings:
                s = s.replace(x[0], x[1])
        return s","
class Encoder:
    
    def needsEncoding(self, s):
        
        pass
    def encode(self, s):
        
        pass

    def decode(self, s):
        
        pass",snippet_184,4,
272664,cackharot/suds-py3,cackharot_suds-py3/suds/transport/__init__.py,suds.transport.Transport,"class Transport:
    """"""
    The transport I{interface}.
    """"""

    def __init__(self):
        """"""
        Constructor.
        """"""
        from suds.transport.options import Options
        self.options = Options()
        del Options

    def open(self, request):
        """"""
        Open the url in the specified request.
        @param request: A transport request.
        @type request: L{Request}
        @return: An input stream.
        @rtype: stream
        @raise TransportError: On all transport errors.
        """"""
        raise Exception('not-implemented')

    def send(self, request):
        """"""
        Send soap message.  Implementations are expected to handle:
            - proxies
            - I{http} headers
            - cookies
            - sending message
            - brokering exceptions into L{TransportError}
        @param request: A transport request.
        @type request: L{Request}
        @return: The reply
        @rtype: L{Reply}
        @raise TransportError: On all transport errors.
        """"""
        raise Exception('not-implemented')","class Transport:
    
    def __init__(self):
        
        pass
    def open(self, request):
        
        pass
    def send(self, request):
        
        pass",snippet_185,4,"```python
import urllib.request
import urllib.error

class Transport:
    def __init__(self):
        self._request = None
        self._response = None

    def open(self, request):
        """"""
        Prepare a request for sending.

        Parameters
        ----------
        request : dict
            Dictionary containing request details. Expected keys:
            - 'method': HTTP method (default 'GET')
            - 'url': Target URL (required)
            - 'headers': Dictionary of HTTP headers (optional)
            - 'body': Request body as str or bytes (optional)
        """"""
        if not isinstance(request, dict):
            raise TypeError(""request must be a dict"")

        method = request.get(""method"", ""GET"").upper()
        url = request.get(""url"")
        if not url:
            raise ValueError(""request must contain a 'url' key"")

        headers = request.get(""headers"", {})
        if not isinstance(headers, dict):
            raise TypeError(""'headers' must be a dict"")

        body = request.get(""body"")
        if body is not None:
            if isinstance(body, str):
                body = body.encode(""utf-8"")
            elif not isinstance(body, (bytes, bytearray)):
                raise TypeError(""'body' must be str, bytes, or bytearray"")

        self._request = urllib.request.Request(
            url=url,
            data=body,
            headers=headers,
            method=method,
        )

    def send(self, request=None):
        """"""
        Send the prepared request and return the response body.

        Parameters
        ----------
        request : dict, optional
            If provided, the request will be opened before sending.

        Returns
        -------
        bytes
            The raw response body.
        """"""
        if request is not None:
            self.open(request)

        if self._request is None:
            raise RuntimeError(""No request has been opened"")

        try:
            with urllib.request.urlopen(self._request) as resp:
                self._response = resp.read()
        except urllib.error.HTTPError as e:
            # Return the error body if available
            self._response = e.read()
        except urllib.error.URLError as e:
            raise RuntimeError(f""Failed to send request: {e}"") from e

        return self._response
```"
272699,cackharot/suds-py3,cackharot_suds-py3/suds/xsd/doctor.py,suds.xsd.doctor.TnsFilter,"class TnsFilter:
    """"""
    Target Namespace filter.
    @ivar tns: A list of target namespaces.
    @type tns: [str,...]
    """"""

    def __init__(self, *tns):
        """"""
        @param tns: A list of target namespaces.
        @type tns: [str,...]
        """"""
        self.tns = []
        self.add(*tns)

    def add(self, *tns):
        """"""
        Add I{targetNamesapces} to be added.
        @param tns: A list of target namespaces.
        @type tns: [str,...]
        """"""
        self.tns += tns

    def match(self, root, ns):
        """"""
        Match by I{targetNamespace} excluding those that
        are equal to the specified namespace to prevent
        adding an import to itself.
        @param root: A schema root.
        @type root: L{Element}
        """"""
        tns = root.get('targetNamespace')
        if len(self.tns):
            matched = tns in self.tns
        else:
            matched = 1
        itself = ns == tns
        return matched and (not itself)","class TnsFilter:
    
    def __init__(self, *tns):
        
        pass
    def add(self, *tns):
        
        pass
    def match(self, root, ns):
        
        pass",snippet_186,4,"```python
class TnsFilter:
    """"""
    A simple filter for target namespaces (TNS).  It stores a set of namespace
    strings and can be queried to see if a given namespace or the namespace of
    an XML element matches any of the stored values.

    The filter accepts a wildcard '*' which matches any namespace.
    """"""

    def __init__(self, *tns):
        """"""
        Initialise the filter with zero or more namespace strings.

        Parameters
        ----------
        *tns : str
            Namespace strings to initialise the filter with.
        """"""
        self._tns = set(tns)

    def add(self, *tns):
        """"""
        Add one or more namespace strings to the filter.

        Parameters
        ----------
        *tns : str
            Namespace strings to add.
        """"""
        self._tns.update(tns)

    def _extract_ns(self, tag):
        """"""
        Extract the namespace from an XML tag string.

        Parameters
        ----------
        tag : str
            The tag string, e.g. '{http://example.com}local'.

        Returns
        -------
        str or None
            The namespace URI if present, otherwise None.
        """"""
        if isinstance(tag, str) and tag.startswith('{'):
            end = tag.find('}')
            if end != -1:
                return tag[1:end]
        return None

    def match(self, root, ns):
        """"""
        Determine whether the supplied namespace or the namespace of the
        supplied root element matches any of the stored namespaces.

        Parameters
        ----------
        root : xml.etree.ElementTree.Element or str or None
            The root element (or its tag string) whose namespace may be
            checked.  If None, only the supplied `ns` is considered.
        ns : str or None
            A namespace string to check against the filter.

        Returns
        -------
        bool
            True if a match is found, False otherwise.
        """"""
        # Wildcard matches everything
        if '*' in self._tns:
            return True

        # Check the explicit namespace argument
        if ns is not None and ns in self._tns:
            return True

        # Check the namespace of the root element/tag
        if root is not None:
            # If root is an Element, get its tag
            tag = getattr(root, 'tag', None)
            if tag is None:
                # If root itself is a string, treat it as a tag
                tag = root
            ns_root = self._extract_ns(tag)
            if ns_root is not None and ns_root in self._tns:
                return True

        return False
```"
272711,cackharot/suds-py3,cackharot_suds-py3/suds/xsd/sxbase.py,suds.xsd.sxbase.NodeFinder,"class NodeFinder:
    """"""
    Find nodes based on flexable criteria.  The I{matcher} is
    may be any object that implements a match(n) method.
    @ivar matcher: An object used as criteria for match.
    @type matcher: I{any}.match(n)
    @ivar limit: Limit the number of matches.  0=unlimited.
    @type limit: int
    """"""

    def __init__(self, matcher, limit=0):
        """"""
        @param matcher: An object used as criteria for match.
        @type matcher: I{any}.match(n)
        @param limit: Limit the number of matches.  0=unlimited.
        @type limit: int
        """"""
        self.matcher = matcher
        self.limit = limit

    def find(self, node, list):
        """"""
        Traverse the tree looking for matches.
        @param node: A node to match on.
        @type node: L{SchemaObject}
        @param list: A list to fill.
        @type list: list
        """"""
        if self.matcher.match(node):
            list.append(node)
            self.limit -= 1
            if self.limit == 0:
                return
        for c in node.rawchildren:
            self.find(c, list)
        return self","class NodeFinder:
    
    def __init__(self, matcher, limit=0):
        
        pass
    def find(self, node, list):
        
        pass",snippet_187,3,"```python
class NodeFinder:
    def __init__(self, matcher, limit=0):
        """"""
        Create a NodeFinder.

        Parameters
        ----------
        matcher : callable
            A function that receives a node and returns ``True`` if the node
            matches the desired condition.
        limit : int, optional
            Maximum number of matches to collect. 0 means no limit.
        """"""
        self.matcher = matcher
        self.limit = limit

    def find(self, node, result):
        """"""
        Recursively search *node* and its descendants, appending matching nodes
        to *result*.

        Parameters
        ----------
        node : any
            The root node to start searching from.
        result : list
            A list that will be populated with matching nodes.
        """"""
        # Stop if we already reached the limit
        if self.limit and len(result) >= self.limit:
            return

        # Check the current node
        try:
            if self.matcher(node):
                result.append(node)
                if self.limit and len(result) >= self.limit:
                    return
        except Exception:
            # If matcher raises, ignore this node
            pass

        # Determine children of the node
        children = None
        if hasattr(node, ""children""):
            children = node.children
        elif hasattr(node, ""childNodes""):
            children = node.childNodes
        elif hasattr(node, ""getchildren""):
            try:
                children = node.getchildren()
            except Exception:
                children = None
        elif isinstance(node, (list, tuple)):
            children = node
        else:
            # Try to iterate over the node if it is iterable
            try:
                children = list(node)
            except Exception:
                children = None

        # Recurse into children
        if children:
            for child in children:
                if self.limit and len(result) >= self.limit:
                    break
                self.find(child, result)
```"
279001,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.AES_CTR_Mechanism,"import PyKCS11.LowLevel

class AES_CTR_Mechanism:
    """"""CKM_AES_CTR encryption mechanism""""""

    def __init__(self, counterBits, counterBlock):
        """"""
        :param counterBits: the number of incremented bits in the counter block
        :param counterBlock: a 16-byte initial value of the counter block
        """"""
        self._param = PyKCS11.LowLevel.CK_AES_CTR_PARAMS()
        self._source_cb = ckbytelist(counterBlock)
        self._param.ulCounterBits = counterBits
        self._param.cb = self._source_cb
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_AES_CTR
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_AES_CTR_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class AES_CTR_Mechanism:
    
    def __init__(self, counterBits, counterBlock):
        
        pass
    def to_native(self):
        
        pass",snippet_190,3,"```python
class AES_CTR_Mechanism:
    def __init__(self, counterBits, counterBlock):
        if not isinstance(counterBits, int):
            raise TypeError(""counterBits must be an integer"")
        if not isinstance(counterBlock, (bytes, bytearray)):
            raise TypeError(""counterBlock must be bytes or bytearray"")
        self.counterBits = counterBits
        self.counterBlock = bytes(counterBlock)

    def to_native(self):
        return {
            'counterBits': self.counterBits,
            'counterBlock': self.counterBlock
        }
```"
279002,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.AES_GCM_Mechanism,"import PyKCS11.LowLevel

class AES_GCM_Mechanism:
    """"""CKM_AES_GCM warpping mechanism""""""

    def __init__(self, iv, aad, tagBits):
        """"""
        :param iv: initialization vector
        :param aad: additional authentication data
        :param tagBits: length of authentication tag in bits
        """"""
        self._param = PyKCS11.LowLevel.CK_GCM_PARAMS()
        self._source_iv = ckbytelist(iv)
        self._param.pIv = self._source_iv
        self._param.ulIvLen = len(self._source_iv)
        self._source_aad = ckbytelist(aad)
        self._param.pAAD = self._source_aad
        self._param.ulAADLen = len(self._source_aad)
        self._param.ulTagBits = tagBits
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_AES_GCM
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_GCM_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class AES_GCM_Mechanism:
    
    def __init__(self, iv, aad, tagBits):
        
        pass
    def to_native(self):
        
        pass",snippet_191,3,"class AES_GCM_Mechanism:
    def __init__(self, iv, aad, tagBits):
        if not isinstance(iv, (bytes, bytearray)):
            raise TypeError(""iv must be bytes"")
        if not isinstance(aad, (bytes, bytearray)):
            raise TypeError(""aad must be bytes"")
        if not isinstance(tagBits, int):
            raise TypeError(""tagBits must be an integer"")
        self.iv = bytes(iv)
        self.aad = bytes(aad)
        self.tagBits = tagBits

    def to_native(self):
        return {
            'iv': self.iv,
            'aad': self.aad,
            'tagBits': self.tagBits
        }"
279010,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.CONCATENATE_BASE_AND_KEY_Mechanism,"import PyKCS11.LowLevel

class CONCATENATE_BASE_AND_KEY_Mechanism:
    """"""CKM_CONCATENATE_BASE_AND_KEY key derivation mechanism""""""

    def __init__(self, encKey):
        """"""
        :param encKey: a handle of encryption key
        """"""
        self._encKey = encKey
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_CONCATENATE_BASE_AND_KEY
        self._mech.pParameter = self._encKey
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_OBJECT_HANDLE_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class CONCATENATE_BASE_AND_KEY_Mechanism:
    
    def __init__(self, encKey):
        
        pass
    def to_native(self):
        
        pass",snippet_192,3,"class CONCATENATE_BASE_AND_KEY_Mechanism:
    def __init__(self, encKey):
        self.encKey = encKey

    def to_native(self):
        if isinstance(self.encKey, bytes):
            key_str = self.encKey.decode('utf-8', errors='ignore')
        else:
            key_str = str(self.encKey)
        return ""BASE"" + key_str"
279012,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.CkClass,"class CkClass:
    """"""
    Base class for CK_* classes
    """"""
    flags_dict = {}
    fields = {}
    flags = 0

    def flags2text(self):
        """"""
        parse the `self.flags` field and create a list of `CKF_*` strings
        corresponding to bits set in flags

        :return: a list of strings
        :rtype: list
        """"""
        r = []
        for k, v in self.flags_dict.items():
            if self.flags & k:
                r.append(v)
        return r

    def state2text(self):
        """"""
        Dummy method. Will be overwriden if necessary
        """"""
        return ''

    def to_dict(self):
        """"""
        convert the fields of the object into a dictionnary
        """"""
        dico = {}
        for field in self.fields:
            if field == 'flags':
                dico[field] = self.flags2text()
            elif field == 'state':
                dico[field] = self.state2text()
            else:
                dico[field] = self.__dict__[field]
        return dico

    def __str__(self):
        """"""
        text representation of the object
        """"""
        dico = self.to_dict()
        lines = []
        for key in sorted(dico.keys()):
            ck_type = self.fields[key]
            if ck_type == 'flags':
                flags = ', '.join(dico[key])
                lines.append(f'{key}: {flags}')
            elif ck_type == 'pair':
                p1, p2 = dico[key]
                lines.append(f'{key}: {p1}.{p2}')
            else:
                lines.append(f'{key}: {dico[key]}')
        return '\n'.join(lines)","class CkClass:
    
    def flags2text(self):
        
        pass
    def state2text(self):
        
        pass
    def to_dict(self):
        
        pass

    def __str__(self):
        
        pass",snippet_193,5,"```python
class CkClass:
    # Default mappings for flags and state.  Sub‑classes can override.
    FLAGS_MAP = {
        0x01: ""READ"",
        0x02: ""WRITE"",
        0x04: ""EXECUTE"",
    }
    STATE_MAP = {
        0: ""UNKNOWN"",
        1: ""ACTIVE"",
        2: ""INACTIVE"",
        3: ""ERROR"",
    }

    def flags2text(self):
        """"""
        Convert the integer bitmask stored in `self.flags` into a comma‑separated
        string of flag names.  If `self.flags` is not an integer or is missing,
        an empty string is returned.
        """"""
        flags = getattr(self, ""flags"", 0)
        if not isinstance(flags, int):
            return """"
        names = [name for bit, name in self.FLAGS_MAP.items() if flags & bit]
        return "", "".join(names)

    def state2text(self):
        """"""
        Convert the value stored in `self.state` into a human‑readable string.
        If the state is not present in STATE_MAP, its raw value is returned as a
        string.
        """"""
        state = getattr(self, ""state"", None)
        return self.STATE_MAP.get(state, str(state))

    def to_dict(self):
        """"""
        Return a dictionary representation of the instance.  All public
        attributes (those not starting with an underscore) are included.  The
        textual representations of flags and state are added under the keys
        'flags_text' and 'state_text'.
        """"""
        data = {k: v for k, v in self.__dict__.items() if not k.startswith(""_"")}
        data[""flags_text""] = self.flags2text()
        data[""state_text""] = self.state2text()
        return data

    def __str__(self):
        """"""
        Return a concise string representation of the instance.  It uses the
        dictionary representation for readability.
        """"""
        return f""{self.__class__.__name__}({self.to_dict()})""
```"
279015,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.EXTRACT_KEY_FROM_KEY_Mechanism,"import PyKCS11.LowLevel

class EXTRACT_KEY_FROM_KEY_Mechanism:
    """"""CKM_EXTRACT_KEY_FROM_KEY key derivation mechanism""""""

    def __init__(self, extractParams):
        """"""
        :param extractParams: the index of the first bit of the original
        key to be used in the newly-derived key.  For example if
        extractParams=5 then the 5 first bits are skipped and not used.
        """"""
        self._param = PyKCS11.LowLevel.CK_EXTRACT_PARAMS()
        self._param.assign(extractParams)
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_EXTRACT_KEY_FROM_KEY
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_EXTRACT_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class EXTRACT_KEY_FROM_KEY_Mechanism:
    
    def __init__(self, extractParams):
        
        pass
    def to_native(self):
        
        pass",snippet_194,3,
279016,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.KEY_DERIVATION_STRING_DATA_MechanismBase,"import PyKCS11.LowLevel

class KEY_DERIVATION_STRING_DATA_MechanismBase:
    """"""Base class for mechanisms using derivation string data""""""

    def __init__(self, data, mechType):
        """"""
        :param data: a byte array to concatenate the key with
        :param mechType: mechanism type
        """"""
        self._param = PyKCS11.LowLevel.CK_KEY_DERIVATION_STRING_DATA()
        self._data = ckbytelist(data)
        self._param.pData = self._data
        self._param.ulLen = len(self._data)
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = mechType
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_KEY_DERIVATION_STRING_DATA_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class KEY_DERIVATION_STRING_DATA_MechanismBase:
    
    def __init__(self, data, mechType):
        
        pass
    def to_native(self):
        
        pass",snippet_195,3,"class KEY_DERIVATION_STRING_DATA_MechanismBase:
    """"""
    Base class for key derivation mechanisms that use a string (bytes) as data.
    """"""

    def __init__(self, data, mechType):
        """"""
        Initialize the mechanism with the given data and mechanism type.

        Parameters
        ----------
        data : bytes or bytearray or str
            The data to be used for the key derivation. If a string is provided,
            it will be encoded using UTF-8.
        mechType : int
            The numeric identifier of the mechanism type.
        """"""
        if not isinstance(mechType, int):
            raise TypeError(f""mechType must be an integer, got {type(mechType).__name__}"")

        if isinstance(data, str):
            data = data.encode(""utf-8"")

        if not isinstance(data, (bytes, bytearray)):
            raise TypeError(f""data must be bytes, bytearray, or str, got {type(data).__name__}"")

        self.data = data
        self.mechType = mechType

    def to_native(self):
        """"""
        Convert the mechanism to a native representation suitable for the
        underlying cryptographic library.

        Returns
        -------
        tuple
            A tuple of the form (mechType, data) where mechType is an integer
            and data is a bytes object.
        """"""
        return (self.mechType, self.data)"
279017,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.Mechanism,"import PyKCS11.LowLevel

class Mechanism:
    """"""Wraps CK_MECHANISM""""""

    def __init__(self, mechanism, param=None):
        """"""
        :param mechanism: the mechanism to be used
        :type mechanism: integer, any `CKM_*` value
        :param param: data to be used as crypto operation parameter
          (i.e. the IV for some algorithms)
        :type param: string or list/tuple of bytes

        :see: :func:`Session.decrypt`, :func:`Session.sign`
        """"""
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = mechanism
        self._param = None
        if param:
            self._param = ckbytelist(param)
            self._mech.pParameter = self._param
            self._mech.ulParameterLen = len(param)

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class Mechanism:
    
    def __init__(self, mechanism, param=None):
        
        pass
    def to_native(self):
        
        pass",snippet_196,3,"class Mechanism:
    def __init__(self, mechanism, param=None):
        """"""
        Initialize a Mechanism instance.

        Parameters
        ----------
        mechanism : Any
            The core mechanism object or identifier.
        param : Any, optional
            Optional parameter(s) associated with the mechanism.
        """"""
        self.mechanism = mechanism
        self.param = param

    def to_native(self):
        """"""
        Convert the Mechanism instance into a native representation.

        Returns
        -------
        Any
            If `param` is None, returns the mechanism itself.
            Otherwise, returns a tuple of (mechanism, param).
        """"""
        if self.param is None:
            return self.mechanism
        return (self.mechanism, self.param)"
279020,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.RSAOAEPMechanism,"import PyKCS11.LowLevel

class RSAOAEPMechanism:
    """"""RSA OAEP Wrapping mechanism""""""

    def __init__(self, hashAlg, mgf, label=None):
        """"""
        :param hashAlg: the hash algorithm to use (like `CKM_SHA256`)
        :param mgf: the mask generation function to use (like
          `CKG_MGF1_SHA256`)
        :param label: the (optional) label to use
        """"""
        self._param = PyKCS11.LowLevel.CK_RSA_PKCS_OAEP_PARAMS()
        self._param.hashAlg = hashAlg
        self._param.mgf = mgf
        self._source = None
        self._param.source = CKZ_DATA_SPECIFIED
        if label:
            self._source = ckbytelist(label)
            self._param.ulSourceDataLen = len(self._source)
        else:
            self._param.ulSourceDataLen = 0
        self._param.pSourceData = self._source
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_RSA_PKCS_OAEP
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_RSA_PKCS_OAEP_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class RSAOAEPMechanism:
    
    def __init__(self, hashAlg, mgf, label=None):
        
        pass
    def to_native(self):
        
        pass",snippet_197,3,"```python
from cryptography.hazmat.primitives.asymmetric import padding
from cryptography.hazmat.primitives import hashes


class RSAOAEPMechanism:
    """"""
    Wrapper for RSA OAEP padding mechanism.

    Parameters
    ----------
    hashAlg : type
        A hash algorithm class from cryptography.hazmat.primitives.hashes
        (e.g., hashes.SHA256).
    mgf : type
        A mask generation function class from cryptography.hazmat.primitives.asymmetric.padding
        (e.g., padding.MGF1).
    label : bytes, optional
        Optional label for OAEP padding. Defaults to None.
    """"""

    def __init__(self, hashAlg, mgf, label=None):
        if not isinstance(label, (bytes, type(None))):
            raise TypeError(""label must be bytes or None"")
        self.hashAlg = hashAlg
        self.mgf = mgf
        self.label = label

    def to_native(self):
        """"""
        Convert to the native cryptography OAEP padding object.

        Returns
        -------
        cryptography.hazmat.primitives.asymmetric.padding.OAEP
        """"""
        # Instantiate the hash algorithm
        hash_instance = self.hashAlg()
        # Instantiate the MGF with the same hash algorithm
        mgf_instance = self.mgf(hash_instance)
        # Create the OAEP padding object
        return padding.OAEP(
            mgf=mgf_instance,
            algorithm=hash_instance,
            label=self.label
        )
```"
279021,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.RSA_PSS_Mechanism,"import PyKCS11.LowLevel

class RSA_PSS_Mechanism:
    """"""RSA PSS Wrapping mechanism""""""

    def __init__(self, mecha, hashAlg, mgf, sLen):
        """"""
        :param mecha: the mechanism to use (like
          `CKM_SHA384_RSA_PKCS_PSS`)
        :param hashAlg: the hash algorithm to use (like `CKM_SHA384`)
        :param mgf: the mask generation function to use (like
          `CKG_MGF1_SHA384`)
        :param sLen: length, in bytes, of the salt value used in the PSS
          encoding (like 0 or the message length)
        """"""
        self._param = PyKCS11.LowLevel.CK_RSA_PKCS_PSS_PARAMS()
        self._param.hashAlg = hashAlg
        self._param.mgf = mgf
        self._param.sLen = sLen
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = mecha
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_RSA_PKCS_PSS_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class RSA_PSS_Mechanism:
    
    def __init__(self, mecha, hashAlg, mgf, sLen):
        
        pass
    def to_native(self):
        
        pass",snippet_198,3,"```python
# rsa_pss_mechanism.py

from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.asymmetric import padding


class RSA_PSS_Mechanism:
    """"""
    A lightweight wrapper that represents an RSA-PSS signing or verification
    mechanism.  It stores the chosen hash algorithm, mask generation function
    (MGF) and salt length, and can produce a native `cryptography` padding
    object via :meth:`to_native`.

    Parameters
    ----------
    mecha : str
        The mechanism name (currently only 'PSS' is supported).
    hashAlg : str
        The hash algorithm name, e.g. 'SHA256', 'SHA512', etc.
    mgf : str
        The mask generation function name.  Only 'MGF1' is supported.
    sLen : int
        The salt length in bytes.  Use ``padding.PSS.MAX_LENGTH`` for the
        maximum allowed value.
    """"""

    _HASH_ALG_MAP = {
        'SHA1': hashes.SHA1,
        'SHA224': hashes.SHA224,
        'SHA256': hashes.SHA256,
        'SHA384': hashes.SHA384,
        'SHA512': hashes.SHA512,
        'SHA3_224': hashes.SHA3_224,
        'SHA3_256': hashes.SHA3_256,
        'SHA3_384': hashes.SHA3_384,
        'SHA3_512': hashes.SHA3_512,
    }

    _MGF_MAP = {
        'MGF1': padding.MGF1,
    }

    def __init__(self, mecha, hashAlg, mgf, sLen):
        if mecha.upper() != 'PSS':
            raise ValueError(f""Unsupported mechanism '{mecha}'. Only 'PSS' is supported."")
        self.mecha = mecha.upper()

        hash_cls = self._HASH_ALG_MAP.get(hashAlg.upper())
        if hash_cls is None:
            raise ValueError(f""Unsupported hash algorithm '{hashAlg}'."")
        self.hash_alg = hash_cls()

        mgf_cls = self._MGF_MAP.get(mgf.upper())
        if mgf_cls is None:
            raise ValueError(f""Unsupported MGF '{mgf}'."")
        self.mgf = mgf_cls(self.hash_alg)

        if not isinstance(sLen, int) or sLen < 0:
            raise ValueError(f""Salt length must be a non‑negative integer, got {sLen}."")
        self.sLen = sLen

    def to_native(self):
        """"""
        Return a native `cryptography` padding.PSS object configured with the
        stored parameters.

        Returns
        -------
        cryptography.hazmat.primitives.asymmetric.padding.PSS
        """"""
        return padding.PSS(
            mgf=self.mgf,
            salt_length=self.sLen
        )
```"
281845,datadotworld/data.world-py,datadotworld_data.world-py/datadotworld/datadotworld.py,datadotworld.datadotworld.UriParam,"class UriParam:
    """"""Represents a URI value as a parameter to a SPARQL query""""""

    def __init__(self, uri):
        """"""
        Initialize the UriParam value
        :param uri: the uri value to wrap
        """"""
        self._uri = uri

    def __repr__(self):
        """"""
        The official string representation for the URI
        :return: the string representation for the URI
        """"""
        return self._uri","class UriParam:
    
    def __init__(self, uri):
        
        pass
    def __repr__(self):
        
        pass",snippet_199,3,"```python
import urllib.parse

class UriParam:
    """"""
    A simple wrapper around urllib.parse.urlparse that stores the parsed
    components of a URI and provides a convenient representation.
    """"""

    def __init__(self, uri: str):
        """"""
        Parse the given URI and store its components.

        Parameters
        ----------
        uri : str
            The URI to parse.
        """"""
        if not isinstance(uri, str):
            raise TypeError(f""uri must be a string, got {type(uri).__name__}"")

        self._parsed = urllib.parse.urlparse(uri)

        # Individual components
        self.scheme = self._parsed.scheme
        self.netloc = self._parsed.netloc
        self.path = self._parsed.path
        self.params = self._parsed.params
        self.query = self._parsed.query
        self.fragment = self._parsed.fragment

        # Query parameters as a dict of lists
        self.query_params = urllib.parse.parse_qs(self.query, keep_blank_values=True)

    def __repr__(self) -> str:
        """"""
        Return a string representation of the UriParam instance.
        """"""
        return (
            f""{self.__class__.__name__}(""
            f""scheme={self.scheme!r}, ""
            f""netloc={self.netloc!r}, ""
            f""path={self.path!r}, ""
            f""params={self.params!r}, ""
            f""query={self.query!r}, ""
            f""fragment={self.fragment!r})""
        )
```"
282529,quora/qcore,quora_qcore/qcore/events.py,qcore.events.EventInterceptor,"class EventInterceptor:
    """"""A context object helping to temporarily intercept
    a set of events on an object exposing a set of event hooks.

    """"""

    def __init__(self, source, **events):
        """"""
        Constructor.

        :param source: the object exposing a set of event hook properies
        :param events: a set of event_hook_name=event_handler pairs specifying
                       which events to intercept.
        """"""
        self.source = source
        self.events = events

    def __enter__(self):
        """"""Starts event interception.""""""
        source = self.source
        for name, handler in self.events.items():
            hook = getattr(source, name)
            hook.subscribe(handler)

    def __exit__(self, typ, value, traceback):
        """"""Stops event interception.""""""
        source = self.source
        for name, handler in self.events.items():
            hook = getattr(source, name)
            hook.unsubscribe(handler)","class EventInterceptor:
    
    def __init__(self, source, **events):
        
        pass
    def __enter__(self):
        
        pass
    def __exit__(self, typ, value, traceback):
        
        pass",snippet_200,4,"```python
import functools

class EventInterceptor:
    """"""
    Context‑manager that temporarily replaces methods on a source object
    with wrappers that invoke user‑supplied callbacks before delegating
    to the original method.

    Example
    -------
    >>> class Foo:
    ...     def bar(self, x):
    ...         return x * 2
    ...
    >>> def on_bar(self, x):
    ...     print(f""bar called with {x}"")
    ...
    >>> foo = Foo()
    >>> with EventInterceptor(foo, bar=on_bar):
    ...     foo.bar(5)          # prints ""bar called with 5"" and returns 10
    """"""

    def __init__(self, source, **events):
        """"""
        Parameters
        ----------
        source : object
            The object whose methods will be intercepted.
        **events : callable
            Mapping of method names to callback functions. Each callback
            will be called with the same arguments as the original method.
        """"""
        self._source = source
        self._events = events
        self._originals = {}

    def __enter__(self):
        for name, callback in self._events.items():
            if not hasattr(self._source, name):
                continue
            original = getattr(self._source, name)
            if not callable(original):
                continue
            self._originals[name] = original

            @functools.wraps(original)
            def wrapper(*args, _orig=original, _cb=callback, **kwargs):
                _cb(*args, **kwargs)
                return _orig(*args, **kwargs)

            setattr(self._source, name, wrapper)
        return self

    def __exit__(self, typ, value, traceback):
        for name, original in self._originals.items():
            setattr(self._source, name, original)
        self._originals.clear()
        # Do not suppress exceptions
        return False
```"
284811,empymod/empymod,empymod/filters.py,empymod.filters.DigitalFilter,"import numpy as np
import os

class DigitalFilter:
    """"""Simple Class for Digital Linear Filters.


    Parameters
    ----------
    name : str
        Name of the DFL.

    savename = str
        Name with which the filter is saved. If None (default) it is set to the
        same value as `name`.

    filter_coeff = list of str
        By default, the following filter coefficients are checked:

            ``filter_coeff = ['j0', 'j1', 'sin', 'cos']``

        This accounts for the standard Hankel and Fourier DLF in CSEM
        modelling. However, additional coefficient names can be provided via
        this parameter (in list format).

    """"""

    def __init__(self, name, savename=None, filter_coeff=None):
        """"""Add filter name.""""""
        self.name = name
        if savename is None:
            self.savename = name
        else:
            self.savename = savename
        self.filter_coeff = ['j0', 'j1', 'sin', 'cos']
        if filter_coeff is not None:
            self.filter_coeff.extend(filter_coeff)

    def tofile(self, path='filters'):
        """"""Save filter values to ASCII-files.

        Store the filter base and the filter coefficients in separate files
        in the directory `path`; `path` can be a relative or absolute path.

        Examples
        --------
        >>> import empymod
        >>> # Load a filter
        >>> filt = empymod.filters.Hankel().wer_201_2018
        >>> # Save it to pure ASCII-files
        >>> filt.tofile()
        >>> # This will save the following three files:
        >>> #    ./filters/wer_201_2018_base.txt
        >>> #    ./filters/wer_201_2018_j0.txt
        >>> #    ./filters/wer_201_2018_j1.txt

        """"""
        name = self.savename
        path = os.path.abspath(path)
        os.makedirs(path, exist_ok=True)
        basefile = os.path.join(path, name + '_base.txt')
        with open(basefile, 'w') as f:
            self.base.tofile(f, sep='\n')
        for val in self.filter_coeff:
            if hasattr(self, val):
                attrfile = os.path.join(path, name + '_' + val + '.txt')
                with open(attrfile, 'w') as f:
                    getattr(self, val).tofile(f, sep='\n')

    def fromfile(self, path='filters'):
        """"""Load filter values from ASCII-files.

        Load filter base and filter coefficients from ASCII files in the
        directory `path`; `path` can be a relative or absolute path.

        Examples
        --------
        >>> import empymod
        >>> # Create an empty filter;
        >>> # Name has to be the base of the text files
        >>> filt = empymod.filters.DigitalFilter('my-filter')
        >>> # Load the ASCII-files
        >>> filt.fromfile()
        >>> # This will load the following three files:
        >>> #    ./filters/my-filter_base.txt
        >>> #    ./filters/my-filter_j0.txt
        >>> #    ./filters/my-filter_j1.txt
        >>> # and store them in filt.base, filt.j0, and filt.j1.

        """"""
        name = self.savename
        path = os.path.abspath(path)
        basefile = os.path.join(path, name + '_base.txt')
        with open(basefile, 'r') as f:
            self.base = np.fromfile(f, sep='\n')
        for val in self.filter_coeff:
            attrfile = os.path.join(path, name + '_' + val + '.txt')
            if os.path.isfile(attrfile):
                with open(attrfile, 'r') as f:
                    setattr(self, val, np.fromfile(f, sep='\n'))
        self.factor = np.around([self.base[1] / self.base[0]], 15)","
class DigitalFilter:
    '''Simple Class for Digital Linear Filters.
    Parameters
    ----------
    name : str
        Name of the DFL.
    savename = str
        Name with which the filter is saved. If None (default) it is set to the
        same value as `name`.
    filter_coeff = list of str
        By default, the following filter coefficients are checked:
            ``filter_coeff = ['j0', 'j1', 'sin', 'cos']``
        This accounts for the standard Hankel and Fourier DLF in CSEM
        modelling. However, additional coefficient names can be provided via
        this parameter (in list format).
    '''
    def __init__(self, name, savename=None, filter_coeff=None):
        
        pass

    def tofile(self, path='filters'):
        '''Save filter values to ASCII-files.
        Store the filter base and the filter coefficients in separate files
        in the directory `path`; `path` can be a relative or absolute path.
        Examples
        --------
        >>> import empymod
        >>> # Load a filter
        >>> filt = empymod.filters.Hankel().wer_201_2018
        >>> # Save it to pure ASCII-files
        >>> filt.tofile()
        >>> # This will save the following three files:
        >>> #    ./filters/wer_201_2018_base.txt
        >>> #    ./filters/wer_201_2018_j0.txt
        >>> #    ./filters/wer_201_2018_j1.txt
        '''
        pass

    def fromfile(self, path='filters'):
        '''Load filter values from ASCII-files.
        Load filter base and filter coefficients from ASCII files in the
        directory `path`; `path` can be a relative or absolute path.
        Examples
        --------
        >>> import empymod
        >>> # Create an empty filter;
        >>> # Name has to be the base of the text files
        >>> filt = empymod.filters.DigitalFilter('my-filter')
        >>> # Load the ASCII-files
        >>> filt.fromfile()
        >>> # This will load the following three files:
        >>> #    ./filters/my-filter_base.txt
        >>> #    ./filters/my-filter_j0.txt
        >>> #    ./filters/my-filter_j1.txt
        >>> # and store them in filt.base, filt.j0, and filt.j1.
        '''
        pass",snippet_201,4,"```python
import os
import numpy as np


class DigitalFilter:
    '''Simple Class for Digital Linear Filters.
    Parameters
    ----------
    name : str
        Name of the DFL.
    savename : str, optional
        Name with which the filter is saved. If None (default) it is set to the
        same value as `name`.
    filter_coeff : list of str, optional
        By default, the following filter coefficients are checked:
            ``filter_coeff = ['j0', 'j1', 'sin', 'cos']``
        This accounts for the standard Hankel and Fourier DLF in CSEM
        modelling. However, additional coefficient names can be provided via
        this parameter (in list format).
    '''
    def __init__(self, name, savename=None, filter_coeff=None):
        self.name = name
        self.savename = savename if savename is not None else name
        self.filter_coeff = filter_coeff if filter_coeff is not None else ['j0', 'j1', 'sin', 'cos']
        # Base and coefficient arrays will be set later
        self.base = None
        for coeff in self.filter_coeff:
            setattr(self, coeff, None)

    def tofile(self, path='filters'):
        '''Save filter values to ASCII-files.
        Store the filter base and the filter coefficients in separate files
        in the directory `path`; `path` can be a relative or absolute path.
        '''
        # Ensure directory exists
        os.makedirs(path, exist_ok=True)

        # Write base
        if self.base is not None:
            base_file = os.path.join(path, f'{self.savename}_base.txt')
            np.savetxt(base_file, self.base, fmt='%.18e')

        # Write each coefficient
        for coeff in self.filter_coeff:
            arr = getattr(self, coeff, None)
            if arr is not None:
                coeff_file = os.path.join(path, f'{self.savename}_{coeff}.txt')
                np.savetxt(coeff_file, arr, fmt='%.18e')

    def fromfile(self, path='filters'):
        '''Load filter values from ASCII-files.
        Load filter base and filter coefficients from ASCII files in the
        directory `path`; `path` can be a relative or absolute path.
        '''
        # Read base
        base_file = os.path.join(path, f'{self.savename}_base.txt')
        if not os.path.isfile(base_file):
            raise FileNotFoundError(f'Base file not found: {base_file}')
        self.base = np.loadtxt(base_file)

        # Read each coefficient
        for coeff in self.filter_coeff:
            coeff_file = os.path.join(path, f'{self.savename}_{coeff}.txt')
            if not os.path.isfile(coeff_file):
                raise FileNotFoundError(f'Coefficient file not found: {coeff_file}')
            setattr(self, coeff, np.loadtxt(coeff_file))
```"
285266,Samreay/ChainConsumer,src/chainconsumer/kde.py,chainconsumer.kde.MegKDE,"import numpy as np
from scipy import spatial

class MegKDE:
    """"""Matched Elliptical Gaussian Kernel Density Estimator

    Adapted from the algorithm specified in the BAMBIS's model specified Wolf 2017
    to support weighted samples.
    """"""

    def __init__(self, train: np.ndarray, weights: np.ndarray | None=None, truncation: float=3.0, nmin: int=4, factor: float=1.0):
        """"""
        Args:
            train (np.ndarray): The training data set. Should be a 1D array of samples or a 2D array
                of shape (n_samples, n_dim).
            weights (np.ndarray, optional): An array of weights. If not specified, equal weights are assumed.
            truncation (float, optional): The maximum deviation (in sigma) to use points in the KDE
            nmin (int, optional): The minimum number of points required to estimate the density
            factor (float, optional): Send bandwidth to this factor of the data estimate
        """"""
        self.truncation = truncation
        self.nmin = nmin
        self.train = train
        if len(train.shape) == 1:
            train = np.atleast_2d(train).T
        self.num_points, self.num_dim = train.shape
        if weights is None:
            weights = np.ones(self.num_points)
        self.weights = weights
        self.mean = np.average(train, weights=weights, axis=0)
        dx = train - self.mean
        cov = np.atleast_2d(np.cov(dx.T, aweights=weights))
        self.A = np.linalg.cholesky(np.linalg.inv(cov))
        self.d = np.dot(dx, self.A)
        self.tree = spatial.cKDTree(self.d)
        self.sigma = 2.0 * factor * np.power(self.num_points, -1.0 / (4 + self.num_dim))
        self.sigma_fact = -0.5 / (self.sigma * self.sigma)

    def evaluate(self, data: np.ndarray) -> np.ndarray:
        """"""Estimate un-normalised probability density at target points

        Args:
            data (np.ndarray): 2D array of shape (n_samples, n_dim).

        Returns:
            np.ndarray: A `(n_samples)` length array of estimates

        """"""
        if len(data.shape) == 1 and self.num_dim == 1:
            data = np.atleast_2d(data).T
        _d = np.dot(data - self.mean, self.A)
        neighbors = self.tree.query_ball_point(_d, self.sigma * self.truncation)
        out = []
        for i, n in enumerate(neighbors):
            if len(n) >= self.nmin:
                diff = self.d[n, :] - _d[i]
                distsq = np.sum(diff * diff, axis=1)
            else:
                dist, n = self.tree.query(_d[i], k=self.nmin)
                distsq = dist * dist
            out.append(np.sum(self.weights[n] * np.exp(self.sigma_fact * distsq)))
        return np.array(out)","
class MegKDE:
    
    def __init__(self, train: np.ndarray, weights: np.ndarray | None=None, truncation: float=3.0, nmin: int=4, factor: float=1.0):
        
        pass
    def evaluate(self, data: np.ndarray) -> np.ndarray:
        
        pass",snippet_203,3,"```python
import numpy as np

class MegKDE:
    """"""
    A simple multivariate kernel density estimator with optional weights,
    bandwidth scaling, truncation, and a minimum neighbor threshold.
    """"""

    def __init__(
        self,
        train: np.ndarray,
        weights: np.ndarray | None = None,
        truncation: float = 3.0,
        nmin: int = 4,
        factor: float = 1.0,
    ):
        """"""
        Parameters
        ----------
        train : np.ndarray
            Training data of shape (n_samples, n_features).
        weights : np.ndarray | None, optional
            Weights for each training sample. If None, equal weights are used.
        truncation : float, optional
            Truncation threshold in units of bandwidth (default 3.0).
        nmin : int, optional
            Minimum number of neighbors to consider when computing density.
            (Not used in this implementation but kept for API compatibility.)
        factor : float, optional
            Scaling factor for the bandwidth (default 1.0).
        """"""
        self.train = np.asarray(train, dtype=float)
        if self.train.ndim == 1:
            self.train = self.train[:, None]  # make it 2D

        self.n_samples, self.n_features = self.train.shape

        if weights is None:
            self.weights = np.ones(self.n_samples, dtype=float)
        else:
            self.weights = np.asarray(weights, dtype=float)
            if self.weights.shape != (self.n_samples,):
                raise ValueError(""weights must have shape (n_samples,)"")
        # Normalize weights to sum to 1
        wsum = self.weights.sum()
        if wsum == 0:
            raise ValueError(""sum of weights must be > 0"")
        self.weights = self.weights / wsum

        self.truncation = float(truncation)
        self.nmin = int(nmin)
        self.factor = float(factor)

        # Bandwidth estimation (Silverman's rule of thumb)
        # For each dimension: h_j = factor * (4 * sigma_j^5 / (3 * n))^(1/5)
        sigma = np.std(self.train, axis=0, ddof=1)
        # Avoid zero std
        sigma = np.where(sigma == 0, 1e-6, sigma)
        self.bandwidth = self.factor * (4 * sigma**5 / (3 * self.n_samples)) ** (1 / 5)
        # Ensure bandwidth is positive
        self.bandwidth = np.maximum(self.bandwidth, 1e-6)

        # Precompute constants
        self.norm_const = 1.0 / (self.n_samples * np.prod(self.bandwidth) * (2 * np.pi) ** (self.n_features / 2))

    def evaluate(self, data: np.ndarray) -> np.ndarray:
        """"""
        Evaluate the KDE at the given data points.

        Parameters
        ----------
        data : np.ndarray
            Query points of shape (m_samples, n_features).

        Returns
        -------
        densities : np.ndarray
            Estimated densities at each query point, shape (m_samples,).
        """"""
        data = np.asarray(data, dtype=float)
        if data.ndim == 1:
            data = data[:, None]
        if data.shape[1] != self.n_features:
            raise ValueError(""data must have the same number of features as train"")

        # Compute differences: shape (m, n, d)
        diff = (data[:, None, :] - self.train[None, :, :]) / self.bandwidth[None, None, :]

        # Truncate: mask where all dimensions within truncation
        mask = np.all(np.abs(diff) <= self.truncation, axis=2)

        # Compute Gaussian kernel product
        # K(u) = exp(-0.5 * sum(u^2)) / (2π)^(d/2)
        # We already have norm_const including (2π)^(d/2)
        sq_norm = np.sum(diff**2, axis=2)
        kernel_vals = np.exp(-0.5 * sq_norm) * mask

        # Weighted sum over training points
        weighted_kernel = kernel_vals * self.weights[None, :]

        densities = self.norm_const * np.sum(weighted_kernel, axis=1)

        return densities
```"
285844,4Catalyzer/flask-resty,4Catalyzer_flask-resty/flask_resty/authentication.py,flask_resty.authentication.AuthenticationBase,"class AuthenticationBase:
    """"""Base class for API authentication components.

    Authentication components are responsible for extracting the request
    credentials, if any. They should raise a 401 if the credentials are
    invalid, but should provide `None` for unauthenticated users.

    Flask-RESTy provides an implementation using `JSON Web Tokens`_  but you
    can use any authentication component by extending
    :py:class:`AuthenticationBase` and implementing
    :py:meth:`get_request_credentials`.

    .. _JSON Web Tokens: https://jwt.io/
    """"""

    def authenticate_request(self):
        """"""Store the request credentials in the
        :py:class:`flask.ctx.AppContext`.

        .. warning::

            No validation is performed by Flask-RESTy. It is up to the
            implementor to validate the request in
            :py:meth:`get_request_credentials`.
        """"""
        set_request_credentials(self.get_request_credentials())

    def get_request_credentials(self):
        """"""Get the credentials for the current request.

        Typically this is done by inspecting :py:data:`flask.request`.

        .. warning::

            Implementing classes **must** raise an exception on authentication
            failure. A 401 Unauthorized :py:class:`ApiError` is recommended.

        :return: The credentials for the current request.
        """"""
        raise NotImplementedError()","class AuthenticationBase:
    
    def authenticate_request(self):
        
        pass
    def get_request_credentials(self):
        
        pass",snippet_204,3,"class AuthenticationBase:
    def authenticate_request(self):
        raise NotImplementedError(""Subclasses must implement authenticate_request"")

    def get_request_credentials(self):
        raise NotImplementedError(""Subclasses must implement get_request_credentials"")"
285856,4Catalyzer/flask-resty,4Catalyzer_flask-resty/flask_resty/filtering.py,flask_resty.filtering.ArgFilterBase,"class ArgFilterBase:
    """"""An abstract specification of a filter from a query argument.

    Implementing classes must provide :py:meth:`maybe_set_arg_name` and
    :py:meth:`filter_query`.
    """"""

    def maybe_set_arg_name(self, arg_name):
        """"""Set the name of the argument to which this filter is bound.

        :param str arg_name: The name of the field to filter against.
        :raises: :py:class:`NotImplementedError` if no implementation is
            provided.
        """"""
        raise NotImplementedError()

    def filter_query(self, query, view, arg_value):
        """"""Filter the query.

        :param query: The query to filter.
        :type query: :py:class:`sqlalchemy.orm.query.Query`
        :param view: The view with the model we wish to filter for.
        :type view: :py:class:`ModelView`
        :param str arg_value: The filter specification
        :return: The filtered query
        :rtype: :py:class:`sqlalchemy.orm.query.Query`
        :raises: :py:class:`NotImplementedError` if no implementation is
            provided.
        """"""
        raise NotImplementedError()","class ArgFilterBase:
    
    def maybe_set_arg_name(self, arg_name):
        
        pass
    def filter_query(self, query, view, arg_value):
        
        pass",snippet_205,3,"```python
class ArgFilterBase:
    """"""
    Base class for argument‑based query filtering.

    Subclasses should set ``self.arg_name`` to the name of the attribute
    on the model that should be filtered.  The ``filter_query`` method
    applies the filter to a SQLAlchemy query object.
    """"""

    def maybe_set_arg_name(self, arg_name):
        """"""
        Set the attribute name to filter on if a non‑empty value is provided.

        Parameters
        ----------
        arg_name : str or None
            The name of the attribute to filter on.  If ``None`` or an empty
            string, the current ``arg_name`` is left unchanged.
        """"""
        if arg_name:
            self.arg_name = arg_name

    def filter_query(self, query, view, arg_value):
        """"""
        Apply a filter to the given query based on ``arg_value``.

        Parameters
        ----------
        query : sqlalchemy.orm.query.Query
            The query to filter.
        view : sqlalchemy.ext.declarative.api.DeclarativeMeta
            The model class that contains the attribute to filter on.
        arg_value : Any
            The value to filter by.  If ``None`` or an empty string, the
            original query is returned unchanged.  If ``arg_value`` is a
            list/tuple/set, an ``IN`` filter is applied.

        Returns
        -------
        sqlalchemy.orm.query.Query
            The filtered query.
        """"""
        # Do nothing if no value is provided
        if arg_value is None or (isinstance(arg_value, str) and not arg_value.strip()):
            return query

        # Resolve the attribute on the model
        try:
            column = getattr(view, self.arg_name)
        except AttributeError as exc:
            raise AttributeError(
                f""Model '{view.__name__}' has no attribute '{self.arg_name}'""
            ) from exc

        # Apply the appropriate filter
        if isinstance(arg_value, (list, tuple, set)):
            return query.filter(column.in_(arg_value))
        else:
            return query.filter(column == arg_value)
```"
285958,tcalmant/python-javaobj,tcalmant_python-javaobj/javaobj/constants.py,javaobj.constants.StreamCodeDebug,"class StreamCodeDebug:
    """"""
    Codes utility methods
    """"""

    @staticmethod
    def op_id(op_id):
        """"""
        Returns the name of the given OP Code
        :param op_id: OP Code
        :return: Name of the OP Code
        """"""
        try:
            return TerminalCode(op_id).name
        except ValueError:
            return '<unknown TC:{0}>'.format(op_id)

    @staticmethod
    def type_code(type_id):
        """"""
        Returns the name of the given Type Code
        :param type_id: Type code
        :return: Name of the type code
        """"""
        try:
            return TypeCode(type_id).name
        except ValueError:
            return '<unknown TypeCode:{0}>'.format(type_id)

    @staticmethod
    def flags(flags):
        """"""
        Returns the names of the class description flags found in the given
        integer

        :param flags: A class description flag entry
        :return: The flags names as a single string
        """"""
        names = sorted((key.name for key in ClassDescFlags if key & flags))
        return ', '.join(names)","class StreamCodeDebug:
    
    @staticmethod
    def op_id(op_id):
        
        pass
    @staticmethod
    def type_code(type_id):
        
        pass
    @staticmethod
    def flags(flags):
        
        pass",snippet_206,4,"class StreamCodeDebug:
    _op_map = {
        0: 'NOP',
        1: 'LOAD',
        2: 'STORE',
        3: 'ADD',
        4: 'SUB',
        5: 'MUL',
        6: 'DIV',
        7: 'JMP',
        8: 'JZ',
        9: 'CALL',
        10: 'RET',
    }

    _type_map = {
        0: 'int',
        1: 'float',
        2: 'str',
        3: 'bool',
        4: 'list',
        5: 'dict',
    }

    _flag_map = {
        0x01: 'READ',
        0x02: 'WRITE',
        0x04: 'EXECUTE',
        0x08: 'DEBUG',
        0x10: 'HIDDEN',
    }

    @staticmethod
    def op_id(op_id):
        return StreamCodeDebug._op_map.get(op_id, f'UNKNOWN_OP_{op_id}')

    @staticmethod
    def type_code(type_id):
        return StreamCodeDebug._type_map.get(type_id, f'UNKNOWN_TYPE_{type_id}')

    @staticmethod
    def flags(flags):
        names = [name for bit, name in StreamCodeDebug._flag_map.items() if flags & bit]
        return '|'.join(names) if names else 'NONE'"
285972,tcalmant/python-javaobj,tcalmant_python-javaobj/javaobj/v2/api.py,javaobj.v2.api.IJavaStreamParser,"class IJavaStreamParser:
    """"""
    API of the Java stream parser
    """"""

    def run(self):
        """"""
        Parses the input stream
        """"""
        raise NotImplementedError

    def dump(self, content):
        """"""
        Dumps to a string the given objects
        """"""
        raise NotImplementedError

    def _read_content(self, type_code, block_data, class_desc=None):
        """"""
        Parses the next content. Use with care (use only in a transformer)
        """"""","class IJavaStreamParser:
    
    def run(self):
        
        pass
    def dump(self, content):
        
        pass
    def _read_content(self, type_code, block_data, class_desc=None):
        
        pass",snippet_207,4,"```python
import struct
import sys
import json

class IJavaStreamParser:
    # Type codes
    TC_NULL = 0x70
    TC_REFERENCE = 0x71
    TC_CLASSDESC = 0x72
    TC_OBJECT = 0x73
    TC_STRING = 0x74
    TC_ARRAY = 0x75
    TC_CLASS = 0x76
    TC_BLOCKDATA = 0x77
    TC_ENDBLOCKDATA = 0x78
    TC_RESET = 0x79
    TC_BLOCKDATALONG = 0x7A
    TC_EXCEPTION = 0x7B
    TC_LONGSTRING = 0x7C
    TC_PROXYCLASSDESC = 0x7D
    TC_ENUM = 0x7E

    def __init__(self, file_path):
        self.file_path = file_path
        self.objects = []

    def run(self):
        with open(self.file_path, ""rb"") as f:
            # Read stream header
            header = f.read(4)
            if len(header) < 4:
                raise ValueError(""File too short for Java serialization header"")
            magic, version = struct.unpack("">HH"", header)
            if magic != 0xACED or version != 0x0005:
                raise ValueError(""Not a valid Java serialization stream"")
            # Parse blocks until EOF
            while True:
                type_code_raw = f.read(1)
                if not type_code_raw:
                    break  # EOF
                type_code = type_code_raw[0]
                obj = self._read_content(type_code, f)
                self.objects.append(obj)

    def dump(self, content=None):
        if content is None:
            content = self.objects
        # Pretty‑print JSON representation
        print(json.dumps(content, indent=2, default=str))

    def _read_content(self, type_code, f, class_desc=None):
        if type_code == self.TC_NULL:
            return None
        elif type_code == self.TC_REFERENCE:
            # 4-byte handle
            handle_bytes = f.read(4)
            if len(handle_bytes) < 4:
                raise ValueError(""Unexpected EOF while reading reference handle"")
            handle = struct.unpack("">I"", handle_bytes)[0]
            return {""type"": ""TC_REFERENCE"", ""handle"": handle}
        elif type_code == self.TC_CLASSDESC:
            # Read class name (TC_STRING)
            class_name = self._read_content(self.TC_STRING, f)
            # serialVersionUID (8 bytes)
            svuid_bytes = f.read(8)
            if len(svuid_bytes) < 8:
                raise ValueError(""Unexpected EOF while reading serialVersionUID"")
            svuid = struct.unpack("">q"", svuid_bytes)[0]
            # classDescFlags (1 byte)
            flags_bytes = f.read(1)
            if len(flags_bytes) < 1:
                raise ValueError(""Unexpected EOF while reading classDescFlags"")
            flags = flags_bytes[0]
            # field count (2 bytes)
            field_count_bytes = f.read(2)
            if len(field_count_bytes) < 2:
                raise ValueError(""Unexpected EOF while reading field count"")
            field_count = struct.unpack("">H"", field_count_bytes)[0]
            fields = []
            for _ in range(field_count):
                # field type code (1 byte)
                ftype = f.read(1)[0]
                # field name (TC_STRING)
                fname = self._read_content(self.TC_STRING, f)
                # field class name (TC_STRING) if type is 'L' or '['
                if chr(ftype) in ('L', '['):
                    fclass = self._read_content(self.TC_STRING, f)
                else:
                    fclass = None
                fields.append({""type"": chr(ftype), ""name"": fname, ""class"": fclass})
            # class annotations (TC_BLOCKDATA until TC_ENDBLOCKDATA)
            annotations = []
            while True:
                ann_type = f.read(1)
                if not ann_type:
                    raise ValueError(""Unexpected EOF while reading class annotations"")
                ann_code = ann_type[0]
                if ann_code == self.TC_ENDBLOCKDATA:
                    break
                elif ann_code == self.TC_BLOCKDATA:
                    length = f.read(1)[0]
                    data = f.read(length)
                    annotations.append(data)
                else:
                    # Skip unknown annotation types
                    pass
            # super class desc
            super_desc = self._read_content(type_code, f)
            return {
                ""type"": ""TC_CLASSDESC"",
                ""class_name"": class_name,
                ""serialVersionUID"": svuid,
                ""flags"": flags,
                ""fields"": fields,
                ""annotations"": annotations,
                ""super_desc"": super_desc,
            }
        elif type_code == self.TC_OBJECT:
            # class desc
            class_desc = self._read_content(self.TC_CLASSDESC, f)
            # field values
            field_values = []
            for field in class_desc.get(""fields"", []):
                val = self._read_content(field.get(""type"", 0), f)
                field_values.append({""name"": field[""name""], ""value"": val})
            return {""type"": ""TC_OBJECT"", ""class_desc"": class_desc, ""fields"": field_values}
        elif type_code == self.TC_STRING:
            # string length (2 bytes)
            str_len_bytes = f.read(2)
            if len(str_len_bytes) < 2:
                raise ValueError(""Unexpected EOF while reading string length"")
            str_len = struct.unpack("">H"", str_len_bytes)[0]
            str_bytes = f.read(str_len)
            if len(str_bytes) < str_len:
                raise ValueError(""Unexpected EOF while reading string data"")
            return str_bytes.decode(""utf-8"", errors=""replace"")
        elif type_code == self.TC_BLOCKDATA:
            length = f.read(1)[0]
            data = f.read(length)
            if len(data) < length:
                raise ValueError(""Unexpected EOF while reading blockdata"")
            return {""type"": ""TC_BLOCKDATA"","
287233,materialsvirtuallab/monty,materialsvirtuallab_monty/src/monty/design_patterns.py,monty.design_patterns.NullFile,"import os

class NullFile:
    """"""A file object that is associated to /dev/null.""""""

    def __new__(cls):
        """"""
        Pass through.
        """"""
        return open(os.devnull, 'w')

    def __init__(self):
        """"""no-op""""""","
class NullFile:
    
    def __new__(cls):
        
        pass
    def __init__(self):
        
        pass",snippet_208,3,"```python
class NullFile:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(NullFile, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        # No initialization needed for a null file
        pass

    # File-like methods
    def write(self, *args, **kwargs):
        # Discard all writes
        return 0

    def read(self, *args, **kwargs):
        # Return empty string for any read
        return ''

    def readline(self, *args, **kwargs):
        return ''

    def readlines(self, *args, **kwargs):
        return []

    def close(self):
        # Nothing to close
        pass

    def flush(self):
        # Nothing to flush
        pass

    # Context manager support
    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        # Do not suppress exceptions
        return False

    # Iterator support
    def __iter__(self):
        return iter([])
```"
287394,common-workflow-language/schema_salad,common-workflow-language_schema_salad/schema_salad/cpp_codegen.py,schema_salad.cpp_codegen.ClassDefinition,"from typing import IO, Any, Optional, Union, cast

class ClassDefinition:
    """"""Prototype of a class.""""""

    def __init__(self, name: str):
        """"""Initialize the class definition with a name.""""""
        self.fullName = name
        self.extends: list[dict[str, str]] = []
        self.specializationTypes: list[str] = []
        self.allfields: list[FieldDefinition] = []
        self.fields: list[FieldDefinition] = []
        self.abstract = False
        self.namespace, self.classname = split_name(name)
        self.namespace = safenamespacename(self.namespace)
        self.classname = safename(self.classname)

    def writeFwdDeclaration(self, target: IO[str], fullInd: str, ind: str) -> None:
        """"""Write forward declaration.""""""
        target.write(f'{fullInd}namespace {self.namespace} {{ struct {self.classname}; }}\n')

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, common_namespace: str) -> None:
        """"""Write definition of the class.""""""
        target.write(f'{fullInd}namespace {self.namespace} {{\n')
        target.write(f'{fullInd}struct {self.classname}')
        extends = list(map(safename2, self.extends))
        override = ''
        virtual = 'virtual '
        if len(self.extends) > 0:
            target.write(f'\n{fullInd}{ind}: ')
            target.write(f'\n{fullInd}{ind}, '.join(extends))
            override = ' override'
            virtual = ''
        target.write(' {\n')
        for field in self.fields:
            field.writeDefinition(target, fullInd + ind, ind, self.namespace)
        if self.abstract:
            target.write(f'{fullInd}{ind}virtual ~{self.classname}() = 0;\n')
        else:
            target.write(f'{fullInd}{ind}{virtual}~{self.classname}(){override} = default;\n')
        target.write(f'{fullInd}{ind}{virtual}auto toYaml([[maybe_unused]] {common_namespace}::store_config const& config) const -> YAML::Node{override};\n')
        target.write(f'{fullInd}{ind}{virtual}void fromYaml(YAML::Node const& n){override};\n')
        target.write(f'{fullInd}}};\n')
        target.write(f'{fullInd}}}\n\n')

    def writeImplDefinition(self, target: IO[str], fullInd: str, ind: str, common_namespace: str) -> None:
        """"""Write definition with implementation.""""""
        extends = list(map(safename2, self.extends))
        if self.abstract:
            target.write(f'{fullInd}inline {self.namespace}::{self.classname}::~{self.classname}() = default;\n')
        target.write(f'{fullInd}inline auto {self.namespace}::{self.classname}::toYaml([[maybe_unused]] ::{common_namespace}::store_config const& config) const -> YAML::Node {{\n{fullInd}{ind}using ::{common_namespace}::toYaml;\n{fullInd}{ind}auto n = YAML::Node{{}};\n{fullInd}{ind}if (config.generateTags) {{\n{fullInd}{ind}{ind}n.SetTag(""{self.classname}"");\n{fullInd}{ind}}}\n')
        for e in extends:
            target.write(f'{fullInd}{ind}n = mergeYaml(n, {e}::toYaml(config));\n')
        for field in self.fields:
            fieldname = safename(field.name)
            target.write(f'{fullInd}{ind}{{\n')
            target.write(f'{fullInd}{ind}{ind} auto member = toYaml(*{fieldname}, config);\n')
            if field.typeDSL:
                target.write(f'{fullInd}{ind}{ind} member = simplifyType(member, config);\n')
            target.write(f'{fullInd}{ind}{ind} member = convertListToMap(member, {q(field.mapSubject)}, {q(field.mapPredicate)}, config);\n')
            target.write(f'{fullInd}{ind}{ind}addYamlField(n, {q(field.name)}, member);\n')
            target.write(f'{fullInd}{ind}}}\n')
        target.write(f'{fullInd}{ind}return n;\n{fullInd}}}\n')
        functionname = f'{self.namespace}::{self.classname}::fromYaml'
        target.write(f'{fullInd}inline void {functionname}([[maybe_unused]] YAML::Node const& n) {{\n{fullInd}{ind}using ::{common_namespace}::fromYaml;\n')
        for e in extends:
            target.write(f'{fullInd}{ind}{e}::fromYaml(n);\n')
        for field in self.fields:
            fieldname = safename(field.name)
            expandType = ''
            if field.typeDSL:
                expandType = 'expandType'
            target.write(f'{fullInd}{ind}{{\n{fullInd}{ind}{ind}auto nodeAsList = convertMapToList(n[{q(field.name)}], {q(field.mapSubject)}, {q(field.mapPredicate)});\n{fullInd}{ind}{ind}auto expandedNode = {expandType}(nodeAsList);\n{fullInd}{ind}{ind}fromYaml(expandedNode, *{fieldname});\n{fullInd}{ind}}}\n')
        target.write(f'{fullInd}}}\n')
        if not self.abstract:
            e = f'::{self.namespace}::{self.classname}'
            target.write(f'namespace {common_namespace} {{\ntemplate <>\nstruct DetectAndExtractFromYaml<{e}> {{\n    auto operator()(YAML::Node const& n) const -> std::optional<{e}> {{\n        if (!n.IsDefined()) return std::nullopt;\n        if (!n.IsMap()) return std::nullopt;\n        auto res = {e}{{}};\n\n')
            for field in self.fields:
                fieldname = safename(field.name)
                target.write(f'        if constexpr (::{common_namespace}::IsConstant<decltype(res.{fieldname})::value_t>::value) try {{\n            fromYaml(n[{q(field.name)}], *res.{fieldname});\n            fromYaml(n, res);\n            return res;\n        }} catch(...) {{}}\n\n')
            target.write('        return std::nullopt;\n    }\n};\n}\n')","
class ClassDefinition:
    
    def __init__(self, name: str):
        
        pass
    def writeFwdDeclaration(self, target: IO[str], fullInd: str, ind: str) -> None:
        
        pass

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, common_namespace: str) -> None:
        
        pass

    def writeImplDefinition(self, target: IO[str], fullInd: str, ind: str, common_namespace: str) -> None:
        
        pass",snippet_209,5,"```python
from typing import IO, Any

class ClassDefinition:
    def __init__(self, name: str):
        """"""
        Create a new class definition with the given name.
        """"""
        self.name = name

    def writeFwdDeclaration(self, target: IO[str], fullInd: str, ind: str) -> None:
        """"""
        Write a forward declaration of the class to the target stream.
        """"""
        target.write(f""{fullInd}class {self.name};\n"")

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, common_namespace: str) -> None:
        """"""
        Write the full class definition (header) to the target stream.
        """"""
        target.write(f""{fullInd}namespace {common_namespace} {{\n"")
        target.write(f""{fullInd}class {self.name} {{\n"")
        target.write(f""{fullInd}{ind}public:\n"")
        target.write(f""{fullInd}{ind}    // TODO: add members\n"")
        target.write(f""{fullInd}}};\n"")
        target.write(f""{fullInd}}}\n"")

    def writeImplDefinition(self, target: IO[str], fullInd: str, ind: str, common_namespace: str) -> None:
        """"""
        Write the implementation section for the class to the target stream.
        """"""
        target.write(f""{fullInd}// Implementation of {self.name} methods\n"")
        target.write(f""{fullInd}namespace {common_namespace} {{\n"")
        target.write(f""{fullInd}{ind}// TODO: implement methods\n"")
        target.write(f""{fullInd}}}\n"")
```"
287396,common-workflow-language/schema_salad,common-workflow-language_schema_salad/schema_salad/cpp_codegen.py,schema_salad.cpp_codegen.EnumDefinition,"from typing import IO, Any, Optional, Union, cast

class EnumDefinition:
    """"""Prototype of a enum.""""""

    def __init__(self, name: str, values: list[str]):
        """"""Initialize enum definition with a name and possible values.""""""
        self.name = name
        self.values = values
        self.namespace, self.classname = split_name(name)
        self.namespace = safenamespacename(self.namespace)
        self.classname = safename(self.classname)

    def writeDefinition(self, target: IO[str], ind: str, common_namespace: str) -> None:
        """"""Write enum definition to output.""""""
        namespace = ''
        if len(self.name.split('#')) == 2:
            namespace, classname = split_name(self.name)
            namespace = safenamespacename(namespace)
            classname = safename(classname)
            name = namespace + '::' + classname
        else:
            name = safename(self.name)
            classname = name
        if len(namespace) > 0:
            target.write(f'namespace {namespace} {{\n')
        target.write(f'enum class {classname} : unsigned int {{\n{ind}')
        target.write(f',\n{ind}'.join(map(safename, self.values)))
        target.write('\n};\n')
        target.write(f'inline auto to_string({classname} v) {{\n')
        target.write(f'{ind}static auto m = std::vector<std::string_view> {{\n')
        target.write(f'{ind}    ""')
        target.write(f'"",\n{ind}    ""'.join(self.values))
        target.write(f'""\n{ind}}};\n')
        target.write(f'{ind}using U = std::underlying_type_t<{name}>;\n')
        target.write(f'{ind}return m.at(static_cast<U>(v));\n}}\n')
        if len(namespace) > 0:
            target.write('}\n')
        target.write(f'inline void to_enum(std::string_view v, {name}& out) {{\n')
        target.write(f'{ind}static auto m = std::map<std::string, {name}, std::less<>> {{\n')
        for v in self.values:
            target.write(f'{ind}{ind}{{{q(v)}, {name}::{safename(v)}}},\n')
        target.write(f'{ind}}};\n{ind}auto iter = m.find(v);\n')
        target.write(f'{ind}if (iter == m.end()) throw bool{{}};\n')
        target.write(f'{ind}out = iter->second;\n}}\n')
        target.write(f'namespace {common_namespace} {{\n')
        target.write(f'inline auto toYaml({name} v, [[maybe_unused]] ::{common_namespace}::store_config const& config) {{\n')
        target.write(f'{ind}auto n = YAML::Node{{std::string{{to_string(v)}}}};\n')
        target.write(f'{ind}if (config.generateTags) n.SetTag(""{name}"");\n')
        target.write(f'{ind}return n;\n}}\n')
        target.write(f'inline void fromYaml(YAML::Node n, {name}& out) {{\n')
        target.write(f'{ind}to_enum(n.as<std::string>(), out);\n}}\n')
        if len(self.values):
            target.write(f'template <> struct IsConstant<{name}> : std::true_type {{}};\n')
        target.write('}\n')
        target.write('\n')","
class EnumDefinition:
    
    def __init__(self, name: str, values: list[str]):
        
        pass
    def writeDefinition(self, target: IO[str], ind: str, common_namespace: str) -> None:
        
        pass",snippet_210,3,"```python
from typing import IO, List


class EnumDefinition:
    """"""
    Represents an enumeration definition that can be written to a target stream.
    """"""

    def __init__(self, name: str, values: List[str]):
        """"""
        Initialize the enum definition.

        :param name: The name of the enum.
        :param values: A list of enum member names.
        """"""
        self.name = name
        self.values = values

    def _sanitize(self, value: str) -> str:
        """"""
        Sanitize a value to be a valid identifier: replace spaces and hyphens with underscores,
        remove leading digits, and strip invalid characters.

        :param value: The raw enum member name.
        :return: A sanitized identifier.
        """"""
        import re

        # Replace spaces and hyphens with underscores
        sanitized = re.sub(r""[ \-]+"", ""_"", value)

        # Remove any character that is not a letter, digit, or underscore
        sanitized = re.sub(r""[^\w]"", """", sanitized)

        # If it starts with a digit, prefix with an underscore
        if re.match(r""^\d"", sanitized):
            sanitized = ""_"" + sanitized

        return sanitized

    def writeDefinition(self, target: IO[str], ind: str, common_namespace: str) -> None:
        """"""
        Write the enum definition to the target stream.

        The output format is a C++-style enum wrapped in a namespace if provided.

        Example output:

            namespace MyNamespace {
                enum MyEnum {
                    VALUE_ONE,
                    VALUE_TWO
                };
            }

        :param target: The stream to write to.
        :param ind: The indentation string to use for each level.
        :param common_namespace: The namespace to wrap the enum in; if empty, no namespace is used.
        """"""
        # Write namespace opening if needed
        if common_namespace:
            target.write(f""{ind}namespace {common_namespace} {{\n"")
            namespace_ind = ind + ind  # increase indentation for namespace contents
        else:
            namespace_ind = ind

        # Write enum declaration
        target.write(f""{namespace_ind}enum {self.name} {{\n"")

        # Write enum values
        for idx, raw_value in enumerate(self.values):
            value = self._sanitize(raw_value)
            comma = "","" if idx < len(self.values) - 1 else """"
            target.write(f""{namespace_ind}{ind}{value}{comma}\n"")

        # Close enum
        target.write(f""{namespace_ind}}};\n"")

        # Close namespace if opened
        if common_namespace:
            target.write(f""{ind}}}\n"")
```"
287397,common-workflow-language/schema_salad,common-workflow-language_schema_salad/schema_salad/cpp_codegen.py,schema_salad.cpp_codegen.FieldDefinition,"from typing import IO, Any, Optional, Union, cast

class FieldDefinition:
    """"""Prototype of a single field from a class definition.""""""

    def __init__(self, name: str, typeStr: str, optional: bool, mapSubject: str, mapPredicate: str, typeDSL: bool):
        """"""Initialize field definition.

        Creates a new field with name, its type, optional and which field to use to convert
        from list to map (or empty if it is not possible)
        """"""
        self.name = name
        self.typeStr = typeStr
        self.optional = optional
        self.mapSubject = mapSubject
        self.mapPredicate = mapPredicate
        self.typeDSL = typeDSL

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, namespace: str) -> None:
        """"""Write a C++ definition for the class field.""""""
        name = safename(self.name)
        typeStr = self.typeStr.replace(namespace + '::', '')
        target.write(f'{fullInd}heap_object<{typeStr}> {name};\n')","
class FieldDefinition:
    
    def __init__(self, name: str, typeStr: str, optional: bool, mapSubject: str, mapPredicate: str, typeDSL: bool):
        
        pass
    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, namespace: str) -> None:
        
        pass",snippet_211,3,"```python
from typing import IO, Any


class FieldDefinition:
    """"""
    Represents a field definition in a DSL or schema generator.

    Attributes
    ----------
    name : str
        The name of the field.
    typeStr : str
        The type of the field as a string.
    optional : bool
        Whether the field is optional.
    mapSubject : str
        Mapping subject string (used in RDF or similar contexts).
    mapPredicate : str
        Mapping predicate string.
    typeDSL : bool
        Flag indicating if the type is a DSL type.
    """"""

    def __init__(
        self,
        name: str,
        typeStr: str,
        optional: bool,
        mapSubject: str,
        mapPredicate: str,
        typeDSL: bool,
    ):
        self.name = name
        self.typeStr = typeStr
        self.optional = optional
        self.mapSubject = mapSubject
        self.mapPredicate = mapPredicate
        self.typeDSL = typeDSL

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, namespace: str) -> None:
        """"""
        Write the field definition to the given target stream.

        Parameters
        ----------
        target : IO[Any]
            The output stream to write to.
        fullInd : str
            The indentation string for the top-level line.
        ind : str
            The indentation string for nested lines.
        namespace : str
            Namespace to prepend to the type if it is not already fully qualified.
        """"""
        # Resolve the type string with namespace if needed
        if namespace and not self.typeStr.startswith(namespace):
            type_str = f""{namespace}.{self.typeStr}""
        else:
            type_str = self.typeStr

        # Optional marker
        opt_marker = ""?"" if self.optional else """"

        # Write the main field line
        target.write(f""{fullInd}field {self.name}{opt_marker}: {type_str}\n"")

        # Write additional metadata lines
        target.write(f""{fullInd}{ind}mapSubject: {self.mapSubject}\n"")
        target.write(f""{fullInd}{ind}mapPredicate: {self.mapPredicate}\n"")
        target.write(f""{fullInd}{ind}typeDSL: {self.typeDSL}\n"")
```"
287519,kinegratii/borax,kinegratii_borax/borax/calendars/utils.py,borax.calendars.utils.ThreeNineUtils,"from borax.calendars.lunardate import LunarDate, TextUtils, TermUtils
from collections import OrderedDict
from datetime import date, datetime, timedelta
from typing import Union, Dict

class ThreeNineUtils:
    """"""三伏数九天工具函数
    """"""

    @staticmethod
    def get_39days(year: int) -> Dict[str, date]:
        """"""获取公历year年的三伏数九天对应的公历日期。
        """"""
        day13 = TermUtils.day_start_from_term(year, '夏至', 3, '庚')
        day23 = day13 + timedelta(days=10)
        day33 = TermUtils.day_start_from_term(year, '立秋', 1, '庚')
        day19 = TermUtils.day_start_from_term(year, '冬至', 0)
        days = OrderedDict({'初伏': day13, '中伏': day23, '末伏': day33, '一九': day19})
        for i, dc in enumerate(TextUtils.DAYS_CN[1:10], start=1):
            days[f'{dc}九'] = day19 + timedelta(days=(i - 1) * 9)
        return days

    @staticmethod
    def get_39label(date_obj: Union[date, LunarDate]) -> str:
        """"""返回三伏数九天对应的标签，如果不是，返回空字符串。
        """"""
        if isinstance(date_obj, LunarDate):
            sd = date_obj.to_solar_date()
        else:
            sd = date_obj
        if sd.month in (4, 5, 6, 10, 11):
            return ''
        year = sd.year - bool(sd.month < 4)
        days = ThreeNineUtils.get_39days(year)
        for vs in list(days.items()):
            label, sd = vs
            range_len = -1
            if label in ['初伏', '末伏']:
                range_len = 10
            elif label == '中伏':
                range_len = (days['末伏'] - days['中伏']).days
            elif '九' in label:
                range_len = 9
            offset = (date_obj - sd).days
            if 0 <= offset <= range_len - 1:
                return f'{label}第{offset + 1}天'
        return ''","
class ThreeNineUtils:
    
    @staticmethod
    def get_39days(year: int) -> Dict[str, date]:
        
        pass
    @staticmethod
    def get_39label(date_obj: Union[date, LunarDate]) -> str:
        
        pass",snippet_212,3,
292643,swistakm/graceful,swistakm_graceful/src/graceful/authentication.py,graceful.authentication.BaseAuthenticationMiddleware,"class BaseAuthenticationMiddleware:
    """"""Base class for all authentication middleware classes.

    Args:
        user_storage (BaseUserStorage): a storage object used to retrieve
            user object using their identifier lookup.
        name (str): custom name of the authentication middleware useful
            for handling custom user storage backends. Defaults to middleware
            class name.

    .. versionadded:: 0.4.0
    """"""
    challenge = None
    only_with_storage = False

    def __init__(self, user_storage=None, name=None):
        """"""Initialize authentication middleware.""""""
        self.user_storage = user_storage
        self.name = name if name else self.__class__.__name__
        if self.only_with_storage and (not isinstance(self.user_storage, BaseUserStorage)):
            raise ValueError('{} authentication middleware requires valid storage. Got {}.'.format(self.__class__.__name__, self.user_storage))

    def process_resource(self, req, resp, resource, uri_kwargs=None):
        """"""Process resource after routing to it.

        This is basic falcon middleware handler.

        Args:
            req (falcon.Request): request object
            resp (falcon.Response): response object
            resource (object): resource object matched by falcon router
            uri_kwargs (dict): additional keyword argument from uri template.
                For ``falcon<1.0.0`` this is always ``None``
        """"""
        if 'user' in req.context:
            return
        identifier = self.identify(req, resp, resource, uri_kwargs)
        user = self.try_storage(identifier, req, resp, resource, uri_kwargs)
        if user is not None:
            req.context['user'] = user
        elif self.challenge is not None:
            req.context.setdefault('challenges', list()).append(self.challenge)

    def identify(self, req, resp, resource, uri_kwargs):
        """"""Identify the user that made the request.

        Args:
            req (falcon.Request): request object
            resp (falcon.Response): response object
            resource (object): resource object matched by falcon router
            uri_kwargs (dict): additional keyword argument from uri template.
                For ``falcon<1.0.0`` this is always ``None``

        Returns:
            object: a user object (preferably a dictionary).
        """"""
        raise NotImplementedError

    def try_storage(self, identifier, req, resp, resource, uri_kwargs):
        """"""Try to find user in configured user storage object.

        Args:
            identifier: User identifier.

        Returns:
            user object.
        """"""
        if identifier is None:
            user = None
        elif self.user_storage is not None:
            user = self.user_storage.get_user(self, identifier, req, resp, resource, uri_kwargs)
        elif self.user_storage is None and (not self.only_with_storage):
            user = {'identified_with': self, 'identifier': identifier}
        else:
            user = None
        return user","class BaseAuthenticationMiddleware:
    
    def __init__(self, user_storage=None, name=None):
        
        pass
    def process_resource(self, req, resp, resource, uri_kwargs=None):
        
        pass
    def identify(self, req, resp, resource, uri_kwargs):
        
        pass

    def try_storage(self, identifier, req, resp, resource, uri_kwargs):
        
        pass",snippet_213,5,"```python
import falcon


class BaseAuthenticationMiddleware:
    """"""
    A simple authentication middleware that extracts a bearer token from the
    `Authorization` header, looks up a user in the provided storage, and
    attaches the user object to the request context.
    """"""

    def __init__(self, user_storage=None, name=None):
        """"""
        Parameters
        ----------
        user_storage : Mapping or callable, optional
            Storage that can return a user object given an identifier.
            If a mapping, the identifier is used as a key.
            If a callable, it is called with the identifier.
            If it has a `get_user` method, that method is used.
        name : str, optional
            Name used as the key in `req.context` to store the user.
        """"""
        self.user_storage = user_storage
        self.name = name or ""user""

    def process_resource(self, req, resp, resource, uri_kwargs=None):
        """"""
        Called by Falcon before the resource method is invoked.
        Identifies the user and stores it in the request context.
        """"""
        identifier = self.identify(req, resp, resource, uri_kwargs)
        if identifier:
            user = self.try_storage(identifier, req, resp, resource, uri_kwargs)
            req.context[self.name] = user
        else:
            req.context[self.name] = None

    def identify(self, req, resp, resource, uri_kwargs):
        """"""
        Extracts an identifier (token) from the `Authorization` header.
        Supports the `Bearer <token>` scheme.
        """"""
        auth_header = req.get_header(""Authorization"")
        if not auth_header:
            return None

        parts = auth_header.split()
        if len(parts) != 2:
            return None

        scheme, token = parts
        if scheme.lower() != ""bearer"":
            return None

        return token

    def try_storage(self, identifier, req, resp, resource, uri_kwargs):
        """"""
        Looks up the user in the configured storage.
        Raises `falcon.HTTPUnauthorized` if the user cannot be found.
        """"""
        if self.user_storage is None:
            raise falcon.HTTPUnauthorized(
                ""Authentication required"",
                ""No user storage configured"",
            )

        # Mapping lookup
        if isinstance(self.user_storage, dict):
            user = self.user_storage.get(identifier)
        # Object with get_user method
        elif hasattr(self.user_storage, ""get_user""):
            user = self.user_storage.get_user(identifier)
        # Callable
        elif callable(self.user_storage):
            user = self.user_storage(identifier)
        else:
            user = None

        if user is None:
            raise falcon.HTTPUnauthorized(
                ""Authentication required"",
                ""Invalid credentials"",
            )

        return user
```"
292765,pytroll/pyspectral,pytroll_pyspectral/rsr_convert_scripts/avhrr1_rsr.py,avhrr1_rsr.AvhrrRSR,"import numpy as np
from pyspectral.config import get_config
from xlrd import open_workbook
import os

class AvhrrRSR:
    """"""Container for the NOAA AVHRR-1 RSR data.""""""

    def __init__(self, wavespace='wavelength'):
        """"""Initialize the AVHRR-1 RSR class.""""""
        options = get_config()
        self.avhrr_path = options['avhrr/1'].get('path')
        if not os.path.exists(self.avhrr_path):
            self.avhrr1_path = os.path.join(DATA_PATH, options['avhrr/1'].get('filename'))
        self.output_dir = options.get('rsr_dir', './')
        self.rsr = {}
        for satname in AVHRR1_SATELLITES:
            self.rsr[satname] = {}
            for chname in AVHRR_BAND_NAMES['avhrr/1']:
                self.rsr[satname][chname] = {'wavelength': None, 'response': None}
        self._load()
        self.wavespace = wavespace
        if wavespace not in ['wavelength', 'wavenumber']:
            raise AttributeError('wavespace has to be either ' + ""'wavelength' or 'wavenumber'!"")
        self.unit = 'micrometer'
        if wavespace == 'wavenumber':
            self.convert2wavenumber()

    def _load(self, scale=1.0):
        """"""Load the AVHRR RSR data for the band requested.""""""
        wb_ = open_workbook(self.avhrr_path)
        sheet_names = []
        for sheet in wb_.sheets():
            if sheet.name in ['Kleespies Data']:
                print('Skip sheet...')
                continue
            ch_name = CHANNEL_NAMES.get(sheet.name.strip())
            if not ch_name:
                break
            sheet_names.append(sheet.name.strip())
            header = sheet.col_values(0, start_rowx=0, end_rowx=2)
            platform_name = header[0].strip('# ')
            unit = header[1].split('Wavelength (')[1].strip(')')
            scale = get_scale_from_unit(unit)
            wvl = sheet.col_values(0, start_rowx=2)
            is_comment = True
            idx = 0
            while is_comment:
                item = wvl[::-1][idx]
                if isinstance(item, str):
                    idx = idx + 1
                else:
                    break
            ndim = len(wvl) - idx
            wvl = wvl[0:ndim]
            if platform_name == 'TIROS-N':
                wvl = adjust_typo_avhrr1_srf_only_xls_file(platform_name, wvl)
            response = sheet.col_values(1, start_rowx=2, end_rowx=2 + ndim)
            wavelength = np.array(wvl) * scale
            response = np.array(response)
            self.rsr[platform_name][ch_name]['wavelength'] = wavelength
            self.rsr[platform_name][ch_name]['response'] = response","
class AvhrrRSR:
    
    def __init__(self, wavespace='wavelength'):
        
        pass
    def _load(self, scale=1.0):
        
        pass",snippet_215,3,"```python
import numpy as np
from scipy.interpolate import interp1d


class AvhrrRSR:
    """"""
    A minimal implementation of an AVHRR Radiance Spectral Response (RSR) class.
    The class generates a synthetic spectral response curve that can be scaled
    and queried at arbitrary wavelengths.
    """"""

    def __init__(self, wavespace: str = ""wavelength""):
        """"""
        Parameters
        ----------
        wavespace : str, optional
            The unit of the wavelength axis.  Currently only 'wavelength' is
            supported, but the attribute is kept for compatibility with
            potential future extensions.
        """"""
        self.wavespace = wavespace
        self._load()

    def _load(self, scale: float = 1.0):
        """"""
        Load (or generate) the spectral response data.

        The synthetic response is a Gaussian centered at 1.0 µm with a
        standard deviation of 0.3 µm.  The response is scaled by the
        ``scale`` factor.

        Parameters
        ----------
        scale : float, optional
            Multiplicative factor applied to the response curve.
        """"""
        # Define a wavelength grid from 0.4 to 2.5 µm (1000 points)
        self.wavelengths = np.linspace(0.4, 2.5, 1000)

        # Gaussian response centered at 1.0 µm, sigma = 0.3 µm
        self.response = np.exp(
            -0.5 * ((self.wavelengths - 1.0) / 0.3) ** 2
        ) * scale

        # Create an interpolation function for fast lookup
        self._interp = interp1d(
            self.wavelengths,
            self.response,
            kind=""linear"",
            bounds_error=False,
            fill_value=0.0,
        )

    def get_response(self, wavelength: float) -> float:
        """"""
        Return the spectral response at a given wavelength.

        Parameters
        ----------
        wavelength : float
            Wavelength (µm) at which to evaluate the response.

        Returns
        -------
        float
            Interpolated response value.
        """"""
        return float(self._interp(wavelength))

    def __repr__(self) -> str:
        return (
            f""<AvhrrRSR wavespace={self.wavespace!r} ""
            f""points={len(self.wavelengths)}> ""
        )
```"
293144,Kentzo/Power,Kentzo_Power/power/common.py,power.common.PowerManagementObserver,"from abc import ABCMeta, abstractmethod

class PowerManagementObserver:
    """"""
    Base class for PowerManagement observers.
    Do not make assumptions in what thread or event loop these methods are called.
    """"""
    __metaclass__ = ABCMeta

    @abstractmethod
    def on_power_sources_change(self, power_management):
        """"""
        @param power_management: Instance of PowerManagement posted notification
        """"""
        pass

    @abstractmethod
    def on_time_remaining_change(self, power_management):
        """"""
        @param power_management: Instance of PowerManagement posted notification
        """"""
        pass","
class PowerManagementObserver:
    
    @abstractmethod
    def on_power_sources_change(self, power_management):
        
        pass
    @abstractmethod
    def on_time_remaining_change(self, power_management):
        
        pass",snippet_216,3,"```python
from abc import ABC, abstractmethod

class PowerManagementObserver(ABC):
    @abstractmethod
    def on_power_sources_change(self, power_management):
        """"""Called when the power sources change.""""""
        pass

    @abstractmethod
    def on_time_remaining_change(self, power_management):
        """"""Called when the remaining time changes.""""""
        pass
```"
293849,hasgeek/coaster,hasgeek_coaster/src/coaster/logger.py,coaster.logger.FilteredValueIndicator,"class FilteredValueIndicator:
    """"""Represent a filtered value.""""""

    def __str__(self) -> str:
        """"""Filter str.""""""
        return '[Filtered]'

    def __repr__(self) -> str:
        """"""Filter repr.""""""
        return '[Filtered]'","class FilteredValueIndicator:
    
    def __str__(self) -> str:
        
        pass
    def __repr__(self) -> str:
        
        pass",snippet_217,3,"class FilteredValueIndicator:
    def __init__(self, value, filtered: bool = False):
        self.value = value
        self.filtered = filtered

    def __str__(self) -> str:
        return str(self.value) if not self.filtered else ""Filtered""

    def __repr__(self) -> str:
        return f""{self.__class__.__name__}(value={self.value!r}, filtered={self.filtered!r})"""
293851,hasgeek/coaster,hasgeek_coaster/src/coaster/logger.py,coaster.logger.RepeatValueIndicator,"class RepeatValueIndicator:
    """"""Represent a repeating value.""""""

    def __init__(self, key: str) -> None:
        """"""Init with key.""""""
        self.key = key

    def __repr__(self) -> str:
        """"""Return representation.""""""
        return f'<same as prior {self.key!r}>'
    __str__ = __repr__","class RepeatValueIndicator:
    
    def __init__(self, key: str) -> None:
        
        pass
    def __repr__(self) -> str:
        
        pass",snippet_218,3,"class RepeatValueIndicator:
    
    def __init__(self, key: str) -> None:
        if not isinstance(key, str):
            raise TypeError(""key must be a string"")
        self.key = key
    
    def __repr__(self) -> str:
        return f""RepeatValueIndicator(key={self.key!r})"""
295935,cloudsmith-io/cloudsmith-cli,cloudsmith-io_cloudsmith-cli/cloudsmith_cli/core/ratelimits.py,cloudsmith_cli.core.ratelimits.RateLimitsInfo,"import datetime

class RateLimitsInfo:
    """"""Data for rate limits.""""""
    interval = None
    limit = None
    remaining = None
    reset = None
    throttled = None

    def __str__(self):
        """"""Get rate limit information as text.""""""
        return 'Throttled: %(throttled)s, Remaining: %(remaining)d/%(limit)d, Interval: %(interval)f, Reset: %(reset)s' % {'throttled': 'Yes' if self.throttled else 'No', 'remaining': self.remaining, 'limit': self.limit, 'interval': self.interval, 'reset': self.reset}

    @classmethod
    def from_dict(cls, data):
        """"""Create RateLimitsInfo from a dictionary.""""""
        info = RateLimitsInfo()
        if 'interval' in data:
            info.interval = float(data['interval'])
        if 'limit' in data:
            info.limit = int(data['limit'])
        if 'remaining' in data:
            info.remaining = int(data['remaining'])
        if 'reset' in data:
            info.reset = datetime.datetime.utcfromtimestamp(int(data['reset']))
        if 'throtted' in data:
            info.throttled = bool(data['throttled'])
        else:
            info.throttled = info.remaining == 0
        return info

    @classmethod
    def from_headers(cls, headers):
        """"""Create RateLimitsInfo from HTTP headers.""""""
        try:
            data = {'interval': headers['X-RateLimit-Interval'], 'limit': headers['X-RateLimit-Limit'], 'remaining': headers['X-RateLimit-Remaining'], 'reset': headers['X-RateLimit-Reset']}
        except KeyError:
            data = {}
        return cls.from_dict(data)","
class RateLimitsInfo:
    
    def __str__(self):
        
        pass
    @classmethod
    def from_dict(cls, data):
        
        pass
    @classmethod
    def from_headers(cls, headers):
        
        pass",snippet_220,4,"```python
from __future__ import annotations

from datetime import datetime
from typing import Any, Dict, Optional


class RateLimitsInfo:
    """"""
    Holds information about API rate limits.

    Attributes
    ----------
    limit : Optional[int]
        The maximum number of requests allowed in the current window.
    remaining : Optional[int]
        The number of requests remaining in the current window.
    reset : Optional[int]
        Unix timestamp (seconds) when the current window resets.
    used : Optional[int]
        The number of requests used in the current window.
    """"""

    def __init__(
        self,
        limit: Optional[int] = None,
        remaining: Optional[int] = None,
        reset: Optional[int] = None,
        used: Optional[int] = None,
    ) -> None:
        self.limit = limit
        self.remaining = remaining
        self.reset = reset
        self.used = used

    def __str__(self) -> str:
        parts = []
        if self.limit is not None:
            parts.append(f""limit={self.limit}"")
        if self.remaining is not None:
            parts.append(f""remaining={self.remaining}"")
        if self.reset is not None:
            # Show both raw timestamp and human‑readable form
            ts = datetime.utcfromtimestamp(self.reset).strftime(""%Y-%m-%d %H:%M:%S UTC"")
            parts.append(f""reset={self.reset} ({ts})"")
        if self.used is not None:
            parts.append(f""used={self.used}"")
        return f""RateLimitsInfo({', '.join(parts)})""

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> ""RateLimitsInfo"":
        """"""
        Create an instance from a dictionary that may contain any of the
        following keys: 'limit', 'remaining', 'reset', 'used'.
        """"""
        if not isinstance(data, dict):
            raise TypeError(""data must be a dict"")

        def _int_or_none(value: Any) -> Optional[int]:
            try:
                return int(value)
            except (TypeError, ValueError):
                return None

        return cls(
            limit=_int_or_none(data.get(""limit"")),
            remaining=_int_or_none(data.get(""remaining"")),
            reset=_int_or_none(data.get(""reset"")),
            used=_int_or_none(data.get(""used"")),
        )

    @classmethod
    def from_headers(cls, headers: Dict[str, Any]) -> ""RateLimitsInfo"":
        """"""
        Create an instance from HTTP headers. Header names are matched
        case‑insensitively. Common header names are:

        - X-RateLimit-Limit
        - X-RateLimit-Remaining
        - X-RateLimit-Reset
        - X-RateLimit-Used
        """"""
        if not isinstance(headers, dict):
            raise TypeError(""headers must be a dict"")

        # Normalise header names to lower case for case‑insensitive lookup
        lower_headers = {k.lower(): v for k, v in headers.items()}

        def _int_or_none(key: str) -> Optional[int]:
            val = lower_headers.get(key.lower())
            try:
                return int(val)
            except (TypeError, ValueError):
                return None

        return cls(
            limit=_int_or_none(""x-ratelimit-limit""),
            remaining=_int_or_none(""x-ratelimit-remaining""),
            reset=_int_or_none(""x-ratelimit-reset""),
            used=_int_or_none(""x-ratelimit-used""),
        )
```"
296505,CZ-NIC/python-rt,CZ-NIC_python-rt/rt/rest2.py,rt.rest2.Attachment,"import dataclasses
import base64

@dataclasses.dataclass
class Attachment:
    """"""Dataclass representing an attachment.""""""
    file_name: str
    file_type: str
    file_content: bytes

    def to_dict(self) -> dict[str, str]:
        """"""Convert to a dictionary for submitting to the REST API.""""""
        return {'FileName': self.file_name, 'FileType': self.file_type, 'FileContent': base64.b64encode(self.file_content).decode('utf-8')}

    def multipart_form_element(self) -> tuple[str, bytes, str]:
        """"""Convert to a tuple as required for multipart-form-data submission.""""""
        return (self.file_name, self.file_content, self.file_type)","@dataclasses.dataclass
class Attachment:
    
    def to_dict(self) -> dict[str, str]:
        
        pass
    def multipart_form_element(self) -> tuple[str, bytes, str]:
        
        pass",snippet_221,3,"```python
import dataclasses
from typing import Tuple, Dict

@dataclasses.dataclass
class Attachment:
    filename: str
    content: bytes
    content_type: str = ""application/octet-stream""

    def to_dict(self) -> Dict[str, str]:
        return {
            ""filename"": self.filename,
            ""content_type"": self.content_type,
        }

    def multipart_form_element(self) -> Tuple[str, bytes, str]:
        return (self.filename, self.content, self.content_type)
```"
297960,theolind/pymysensors,theolind_pymysensors/mysensors/transport.py,mysensors.transport.Transport,"class Transport:
    """"""Handle gateway transport.

    I/O is allowed in this class. This class should host methods that
    are related to the gateway transport type.
    """"""

    def __init__(self, gateway, connect, timeout=1.0, reconnect_timeout=10.0, **kwargs):
        """"""Set up transport.""""""
        self._connect = connect
        self.can_log = False
        self.connect_task = None
        self.gateway = gateway
        self.protocol = None
        self.reconnect_timeout = reconnect_timeout
        self.timeout = timeout

    def disconnect(self):
        """"""Disconnect from the transport.""""""
        if not self.protocol or not self.protocol.transport:
            self.protocol = None
            return
        _LOGGER.info('Disconnecting from gateway')
        self.protocol.transport.close()
        self.protocol = None

    def send(self, message):
        """"""Write a message to the gateway.""""""
        if not message or not self.protocol or (not self.protocol.transport):
            return
        if not self.can_log:
            _LOGGER.debug('Sending %s', message.strip())
        try:
            self.protocol.transport.write(message.encode())
        except OSError as exc:
            _LOGGER.error('Failed writing to transport %s: %s', self.protocol.transport, exc)
            self.protocol.transport.close()
            self.protocol.conn_lost_callback()","class Transport:
    
    def __init__(self, gateway, connect, timeout=1.0, reconnect_timeout=10.0, **kwargs):
        
        pass
    def disconnect(self):
        
        pass
    def send(self, message):
        
        pass",snippet_222,4,"```python
import socket
import time
from typing import Any, Callable, Tuple, Optional


class Transport:
    """"""
    A simple transport wrapper that manages a socket connection.

    Parameters
    ----------
    gateway : Tuple[str, int]
        The host and port to connect to.
    connect : Callable[[Tuple[str, int], **Any], socket.socket]
        A callable that returns a connected socket. It should accept the
        gateway tuple as the first positional argument and any additional
        keyword arguments.
    timeout : float, optional
        Timeout in seconds for socket operations. Default is 1.0.
    reconnect_timeout : float, optional
        Seconds to wait before attempting to reconnect after a failure.
        Default is 10.0.
    **kwargs
        Additional keyword arguments forwarded to the `connect` callable.
    """"""

    def __init__(
        self,
        gateway: Tuple[str, int],
        connect: Callable[[Tuple[str, int], **Any], socket.socket],
        timeout: float = 1.0,
        reconnect_timeout: float = 10.0,
        **kwargs: Any,
    ) -> None:
        self.gateway = gateway
        self.connect = connect
        self.timeout = timeout
        self.reconnect_timeout = reconnect_timeout
        self._connect_kwargs = kwargs
        self._sock: Optional[socket.socket] = None
        self._connect()

    def _connect(self) -> None:
        """"""Establish a new socket connection.""""""
        if self._sock:
            try:
                self._sock.close()
            except Exception:
                pass
            self._sock = None

        try:
            self._sock = self.connect(self.gateway, **self._connect_kwargs)
            if self._sock:
                self._sock.settimeout(self.timeout)
        except Exception as exc:
            raise ConnectionError(f""Failed to connect to {self.gateway}: {exc}"") from exc

    def disconnect(self) -> None:
        """"""Close the current socket connection.""""""
        if self._sock:
            try:
                self._sock.shutdown(socket.SHUT_RDWR)
            except Exception:
                pass
            try:
                self._sock.close()
            except Exception:
                pass
            finally:
                self._sock = None

    def send(self, message: Any) -> None:
        """"""
        Send a message over the transport.

        Parameters
        ----------
        message : Any
            The data to send. If a string, it will be encoded to UTF-8 bytes.
            Otherwise, it must be a bytes-like object.
        """"""
        if not self._sock:
            self._connect()

        data: bytes
        if isinstance(message, str):
            data = message.encode(""utf-8"")
        elif isinstance(message, (bytes, bytearray)):
            data = bytes(message)
        else:
            raise TypeError(""Message must be str, bytes, or bytearray"")

        try:
            self._sock.sendall(data)
        except Exception:
            # Attempt to reconnect and resend once
            self.disconnect()
            time.sleep(self.reconnect_timeout)
            self._connect()
            if not self._sock:
                raise ConnectionError(""Reconnection failed; cannot send message"")
            try:
                self._sock.sendall(data)
            except Exception as exc:
                raise ConnectionError(f""Failed to send message after reconnection: {exc}"") from exc
```"
298148,edx/event-tracking,edx_event-tracking/eventtracking/backends/logger.py,eventtracking.backends.logger.LoggerBackend,"import logging
import json

class LoggerBackend:
    """"""
    Event tracker backend that uses a python logger.

    Events are logged to the INFO level as JSON strings.
    """"""

    def __init__(self, **kwargs):
        """"""
        Event tracker backend that uses a python logger.

        `name` is an identifier for the logger, which should have
            been configured using the default python mechanisms.
        """"""
        name = kwargs.get('name', None)
        self.max_event_size = kwargs.get('max_event_size', MAX_EVENT_SIZE)
        self.event_logger = logging.getLogger(name)
        level = kwargs.get('level', 'info')
        self.log = getattr(self.event_logger, level.lower())

    def send(self, event):
        """"""Send the event to the standard python logger""""""
        event_str = json.dumps(event, cls=DateTimeJSONEncoder)
        if self.max_event_size is None or len(event_str) <= self.max_event_size:
            self.log(event_str)","
class LoggerBackend:
    
    def __init__(self, **kwargs):
        
        pass
    def send(self, event):
        
        pass",snippet_223,3,"```python
import logging

class LoggerBackend:
    def __init__(self, **kwargs):
        self.name = kwargs.get('name', 'LoggerBackend')
        self.level = kwargs.get('level', 'INFO')
        self.file = kwargs.get('file')
        self.formatter = kwargs.get(
            'formatter',
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )

        self.logger = logging.getLogger(self.name)
        self.logger.setLevel(getattr(logging, self.level.upper(), logging.INFO))

        # Remove existing handlers to avoid duplicate logs
        for h in self.logger.handlers[:]:
            self.logger.removeHandler(h)

        if self.file:
            handler = logging.FileHandler(self.file)
        else:
            handler = logging.StreamHandler()

        handler.setFormatter(logging.Formatter(self.formatter))
        self.logger.addHandler(handler)

    def send(self, event):
        if isinstance(event, Exception):
            self.logger.exception(str(event))
        else:
            self.logger.info(str(event))
```"
303020,UpCloudLtd/upcloud-python-api,UpCloudLtd_upcloud-python-api/upcloud_api/cloud_manager/host_mixin.py,upcloud_api.cloud_manager.host_mixin.HostManager,"from upcloud_api.api import API
from upcloud_api.host import Host

class HostManager:
    """"""
    Functions for managing hosts. Intended to be used as a mixin for CloudManager.
    """"""
    api: API

    def get_hosts(self):
        """"""
        Returns a list of available hosts, along with basic statistics of them when available.
        """"""
        url = '/host'
        res = self.api.get_request(url)
        return [Host(**host) for host in res['hosts']['host']]

    def get_host(self, id: str) -> Host:
        """"""
        Returns detailed information about a specific host.
        """"""
        url = f'/host/{id}'
        res = self.api.get_request(url)
        return Host(**res['host'])

    def modify_host(self, host: str, description: str) -> Host:
        """"""
        Modifies description of a specific host.
        """"""
        url = f'/host/{host}'
        body = {'host': {'description': description}}
        res = self.api.patch_request(url, body)
        return Host(**res['host'])","
class HostManager:
    
    def get_hosts(self):
        
        pass
    def get_hosts(self):
        
        pass

    def modify_host(self, host: str, description: str) -> Host:
        
        pass",snippet_224,4,"```python
from dataclasses import dataclass, field
from typing import List, Optional


@dataclass
class Host:
    host: str
    description: str = """"


class HostManager:
    def __init__(self):
        self._hosts: List[Host] = []

    def get_hosts(self) -> List[Host]:
        """"""Return a copy of the list of hosts.""""""
        return list(self._hosts)

    def add_host(self, host: str, description: str = """") -> Host:
        """"""Add a new host to the manager.""""""
        new_host = Host(host=host, description=description)
        self._hosts.append(new_host)
        return new_host

    def modify_host(self, host: str, description: str) -> Host:
        """"""Modify the description of an existing host.""""""
        for h in self._hosts:
            if h.host == host:
                h.description = description
                return h
        raise ValueError(f""Host '{host}' not found."")
```"
304970,threeML/astromodels,threeML_astromodels/astromodels/functions/template_model.py,astromodels.functions.template_model.TemplateFile,"import numpy as np
import h5py
from dataclasses import dataclass
import collections
from typing import Dict, List, Optional, Union

@dataclass
class TemplateFile:
    """"""
    simple container to read and write
    the data to an hdf5 file

    """"""
    name: str
    description: str
    grid: np.ndarray
    parameters: Dict[str, np.ndarray]
    parameter_order: List[str]
    energies: np.ndarray
    interpolation_degree: int
    spline_smoothing_factor: float

    def save(self, file_name: str):
        """"""
        serialize the contents to a file

        :param file_name:
        :type file_name: str
        :returns:

        """"""
        with h5py.File(file_name, 'w') as f:
            f.attrs['name'] = self.name
            f.attrs['description'] = self.description
            f.attrs['interpolation_degree'] = self.interpolation_degree
            f.attrs['spline_smoothing_factor'] = self.spline_smoothing_factor
            f.create_dataset('energies', data=self.energies, compression='gzip')
            f.create_dataset('grid', data=self.grid, compression='gzip')
            dt = h5py.special_dtype(vlen=str)
            po = np.array(self.parameter_order, dtype=dt)
            f.create_dataset('parameter_order', data=po)
            par_group = f.create_group('parameters')
            for k in self.parameter_order:
                par_group.create_dataset(k, data=self.parameters[k], compression='gzip')

    @classmethod
    def from_file(cls, file_name: str):
        """"""
        read contents from a file

        :param cls:
        :type cls:
        :param file_name:
        :type file_name: str
        :returns:

        """"""
        with h5py.File(file_name, 'r') as f:
            name = f.attrs['name']
            description = f.attrs['description']
            interpolation_degree = f.attrs['interpolation_degree']
            spline_smoothing_factor = f.attrs['spline_smoothing_factor']
            energies = f['energies'][()]
            parameter_order = f['parameter_order'][()]
            grid = f['grid'][()]
            parameters = collections.OrderedDict()
            for k in parameter_order:
                parameters[k] = f['parameters'][k][()]
        return cls(name=name, description=description, interpolation_degree=interpolation_degree, spline_smoothing_factor=spline_smoothing_factor, energies=energies, parameter_order=parameter_order, parameters=parameters, grid=grid)","@dataclass
class TemplateFile:
    
    def save(self, file_name: str):
        
        pass
    @classmethod
    def from_file(cls, file_name: str):
        
        pass",snippet_227,3,"```python
from dataclasses import dataclass

@dataclass
class TemplateFile:
    content: str = """"

    def save(self, file_name: str):
        """"""Write the template content to the specified file.""""""
        with open(file_name, ""w"", encoding=""utf-8"") as f:
            f.write(self.content)

    @classmethod
    def from_file(cls, file_name: str):
        """"""Create a TemplateFile instance from the contents of a file.""""""
        with open(file_name, ""r"", encoding=""utf-8"") as f:
            content = f.read()
        return cls(content=content)
```"
305605,NASA-AMMOS/AIT-Core,ait/core/tlm.py,core.tlm.PacketContext,"class PacketContext:
    """"""PacketContext

    A PacketContext provides a simple wrapper around a Packet so that
    field accesses of the form:

        packet.fieldname

    may also be specified as:

        packet[fieldname]

    This latter syntax allows a PacketContext to be used as a symbol
    table when evaluating PacketExpressions.
    """"""
    __slots__ = ['_packet']

    def __init__(self, packet):
        """"""Creates a new PacketContext for the given Packet.""""""
        self._packet = packet

    def __getitem__(self, name):
        """"""Returns packet[name]""""""
        result = None
        if self._packet._hasattr(name):
            result = self._packet._getattr(name)
        else:
            msg = ""Packet '%s' has no field '%s'""
            values = (self._packet._defn.name, name)
            raise KeyError(msg % values)
        return result","class PacketContext:
    
    def __init__(self, packet):
        
        pass
    def __getitem__(self, name):
        
        pass",snippet_229,3,"class PacketContext:
    def __init__(self, packet):
        self.packet = packet

    def __getitem__(self, name):
        if isinstance(self.packet, dict):
            return self.packet[name]
        return getattr(self.packet, name)"
305610,NASA-AMMOS/AIT-Core,ait/core/tlm.py,core.tlm.RawPacket,"class RawPacket:
    """"""RawPacket

    Wraps a packet such that:

        packet.raw.fieldname

    returns the value of fieldname as a raw value with no enumeration
    substitutions or DN to EU conversions applied.
    """"""
    __slots__ = ['_packet']

    def __init__(self, packet):
        """"""Creates a new RawPacket based on the given Packet.""""""
        self._packet = packet

    def __getattr__(self, fieldname):
        """"""Returns the value of the given packet fieldname as a raw
        value with no DN to EU conversion applied.
        """"""
        return self._packet._getattr(fieldname, raw=True)","class RawPacket:
    
    def __init__(self, packet):
        
        pass
    def __getattr__(self, fieldname):
        
        pass",snippet_230,3,"class RawPacket:
    def __init__(self, packet):
        self.packet = packet

    def __getattr__(self, fieldname):
        # Try to get attribute from the underlying packet object
        if hasattr(self.packet, fieldname):
            return getattr(self.packet, fieldname)
        # If the packet is a mapping, try to fetch the key
        if isinstance(self.packet, dict) and fieldname in self.packet:
            return self.packet[fieldname]
        # If the packet is a sequence of bytes and the fieldname is an integer index
        if isinstance(self.packet, (bytes, bytearray)) and fieldname.isdigit():
            idx = int(fieldname)
            try:
                return self.packet[idx]
            except IndexError:
                pass
        raise AttributeError(f""'{type(self.packet).__name__}' object has no attribute '{fieldname}'"")"
305613,NASA-AMMOS/AIT-Core,ait/core/tlm.py,core.tlm.WordArray,"class WordArray:
    """"""WordArrays are somewhat analogous to Python bytearrays, but
    currently much more limited in functionality.  They provide a
    readonly view of a bytearray addressable and iterable as a sequence
    of 16-bit words.  This is convenient for telemetry processing as
    packets are often more naturally addressable on word, as opposed to
    byte, boundaries.
    """"""
    __slots__ = ['_bytes']

    def __init__(self, bytes):
        """"""Creates a new wordarray from the given bytearray.

        The given bytearray should contain an even number of bytes.  If
        odd, the last byte is ignored.
        """"""
        self._bytes = bytes

    def __getitem__(self, key):
        """"""Returns the words in this wordarray at the given Python slice
        or word at the given integer index.""""""
        length = len(self)
        if isinstance(key, slice):
            return [self[n] for n in range(*key.indices(length))]
        elif isinstance(key, int):
            if key < 0:
                key += length
            if key >= length:
                msg = 'wordarray index (%d) is out of range [0 %d].'
                raise IndexError(msg % (key, length - 1))
            index = 2 * key
            return self._bytes[index] << 8 | self._bytes[index + 1]
        else:
            raise TypeError('wordarray indices must be integers.')

    def __len__(self):
        """"""Returns the number of words in this wordarray.""""""
        return len(self._bytes) / 2","class WordArray:
    
    def __init__(self, bytes):
        
        pass
    def __getitem__(self, key):
        
        pass
    def __len__(self):
        
        pass",snippet_231,4,"```python
class WordArray:
    """"""
    A simple container that stores a sequence of 32‑bit words derived from a
    bytes‑like object.  Words are stored in big‑endian order.
    """"""

    def __init__(self, bytes_obj):
        """"""
        Convert a bytes‑like object into a list of 32‑bit words.

        Parameters
        ----------
        bytes_obj : bytes or bytearray
            The source data.  If the length is not a multiple of 4, the last
            word is padded with zero bytes on the right.
        """"""
        if not isinstance(bytes_obj, (bytes, bytearray)):
            raise TypeError(""bytes_obj must be bytes or bytearray"")

        self._words = []
        # Process each 4‑byte block
        for i in range(0, len(bytes_obj), 4):
            block = bytes_obj[i:i + 4]
            # Pad the block to 4 bytes if necessary
            if len(block) < 4:
                block = block + b'\x00' * (4 - len(block))
            # Convert to a 32‑bit integer (big‑endian)
            word = (
                (block[0] << 24)
                | (block[1] << 16)
                | (block[2] << 8)
                | block[3]
            )
            self._words.append(word)

    def __len__(self):
        """"""Return the number of 32‑bit words.""""""
        return len(self._words)

    def __getitem__(self, key):
        """"""
        Retrieve a word or a slice of words.

        Parameters
        ----------
        key : int or slice
            If an integer, returns the word at that index (supports negative
            indices).  If a slice, returns a new WordArray containing the
            sliced words.

        Returns
        -------
        int or WordArray
            The requested word or a new WordArray.
        """"""
        if isinstance(key, slice):
            # Return a new WordArray with the sliced words
            sliced_words = self._words[key]
            # Construct a new WordArray from the sliced words
            new_obj = WordArray(b'')
            new_obj._words = sliced_words
            return new_obj
        elif isinstance(key, int):
            return self._words[key]
        else:
            raise TypeError(""Index must be an int or slice"")

    def __repr__(self):
        return f""WordArray({self._words!r})""
```"
306020,ihmeuw/vivarium,ihmeuw_vivarium/src/vivarium/framework/lookup/interpolation.py,vivarium.framework.lookup.interpolation.Order0Interp,"import numpy as np
import pandas as pd
from collections.abc import Hashable, Sequence

class Order0Interp:
    """"""A callable that returns the result of order 0 interpolation over input data.

    Attributes
    ----------
    data
        The data from which to build the interpolation.
    value_columns
        Columns to be interpolated.
    extrapolate
        Whether or not to extrapolate beyond the edge of supplied bins.
    parameter_bins
        A dictionary where they keys are a tuple of the form
        (column name used in call, column name for left bin edge, column name for right bin edge)
        and the values are dictionaries of the form {""bins"": [ordered left edges of bins],
        ""max"": max right edge (used when extrapolation not allowed)}.

    """"""

    def __init__(self, data: pd.DataFrame, continuous_parameters: Sequence[Sequence[str]], value_columns: list[str], extrapolate: bool, validate: bool):
        """"""
        Parameters
        ----------
        data
            Data frame used to build interpolation.
        continuous_parameters
            Parameter columns. Should be of form (column name used in call,
            column name for left bin edge, column name for right bin edge)
            or column name. Assumes left bin edges are inclusive and
            right exclusive.
        value_columns
            Columns to be interpolated.
        extrapolate
            Whether or not to extrapolate beyond the edge of supplied bins.
        validate
            Whether or not to validate the data.
        """"""
        if validate:
            check_data_complete(data, continuous_parameters)
        self.data = data.copy()
        self.value_columns = value_columns
        self.extrapolate = extrapolate
        self.parameter_bins = {}
        for p in continuous_parameters:
            left_edge = self.data[p[1]].drop_duplicates().sort_values()
            max_right = self.data[p[2]].drop_duplicates().max()
            self.parameter_bins[tuple(p)] = {'bins': left_edge.reset_index(drop=True), 'max': max_right}

    def __call__(self, interpolants: pd.DataFrame) -> pd.DataFrame:
        """"""Find the bins for each parameter for each interpolant in interpolants
        and return the values from data there.

        Parameters
        ----------
        interpolants
            Data frame containing the parameters to interpolate..

        Returns
        -------
            A table with the interpolated values for the given interpolants.
        """"""
        interpolant_bins = pd.DataFrame(index=interpolants.index)
        merge_cols = []
        for cols, d in self.parameter_bins.items():
            bins = d['bins']
            max_right = d['max']
            merge_cols.append(cols[1])
            interpolant_col = interpolants[cols[0]]
            if not self.extrapolate and (interpolant_col.min() < bins[0] or interpolant_col.max() >= max_right):
                raise ValueError(f'Extrapolation outside of bins used to set up interpolation is only allowed when explicitly set in creation of Interpolation. Extrapolation is currently off for this interpolation, and parameter {cols[0]} includes data outside of original bins.')
            bin_indices = np.digitize(interpolant_col, bins.tolist())
            bin_indices[bin_indices > 0] -= 1
            interpolant_bins[cols[1]] = bins.loc[bin_indices].values
        index = interpolant_bins.index
        interp_vals = interpolant_bins.merge(self.data, how='left', on=merge_cols).set_index(index)
        return interp_vals[self.value_columns]","
class Order0Interp:
    
    def __init__(self, data: pd.DataFrame, continuous_parameters: Sequence[Sequence[str]], value_columns: list[str], extrapolate: bool, validate: bool):
        
        pass
    def __call__(self, interpolants: pd.DataFrame) -> pd.DataFrame:
        
        pass",snippet_232,3,"```python
import pandas as pd
import numpy as np
from typing import Sequence, List

class Order0Interp:
    """"""
    Zero‑order (nearest‑neighbour) interpolation for tabular data.

    Parameters
    ----------
    data : pd.DataFrame
        Reference data containing the continuous parameters and the values to interpolate.
    continuous_parameters : Sequence[Sequence[str]]
        A sequence of sequences of column names that are treated as continuous parameters.
        All inner sequences are flattened and used to compute distances.
    value_columns : list[str]
        Column names in ``data`` whose values will be returned for each interpolant.
    extrapolate : bool
        If ``False`` an error is raised when an interpolant lies outside the bounds of
        the reference data. If ``True`` the nearest neighbour is returned regardless
        of bounds.
    validate : bool
        If ``True`` basic validation of column names is performed.
    """"""

    def __init__(
        self,
        data: pd.DataFrame,
        continuous_parameters: Sequence[Sequence[str]],
        value_columns: List[str],
        extrapolate: bool,
        validate: bool,
    ):
        self.data = data.reset_index(drop=True)
        # Flatten continuous parameters
        self.continuous_params = [
            param for group in continuous_parameters for param in group
        ]
        self.value_columns = value_columns
        self.extrapolate = extrapolate
        self.validate = validate

        if self.validate:
            missing_in_data = set(self.continuous_params + self.value_columns) - set(
                self.data.columns
            )
            if missing_in_data:
                raise ValueError(
                    f""Columns missing in data: {sorted(missing_in_data)}""
                )

        # Pre‑compute bounds for extrapolation check
        self._bounds_min = self.data[self.continuous_params].min()
        self._bounds_max = self.data[self.continuous_params].max()

        # Convert data to numpy for fast distance calculation
        self._data_np = self.data[self.continuous_params].to_numpy()

    def __call__(self, interpolants: pd.DataFrame) -> pd.DataFrame:
        """"""
        Interpolate the values for the given interpolants.

        Parameters
        ----------
        interpolants : pd.DataFrame
            DataFrame containing the continuous parameters for which values are requested.

        Returns
        -------
        pd.DataFrame
            DataFrame with the same index as ``interpolants`` and columns ``value_columns``.
        """"""
        if self.validate:
            missing_in_interp = set(self.continuous_params) - set(interpolants.columns)
            if missing_in_interp:
                raise ValueError(
                    f""Interpolants missing required columns: {sorted(missing_in_interp)}""
                )

        # Convert interpolants to numpy
        interp_np = interpolants[self.continuous_params].to_numpy()

        # Check bounds if extrapolation not allowed
        if not self.extrapolate:
            out_of_bounds = (
                (interp_np < self._bounds_min.to_numpy()) | (interp_np > self._bounds_max.to_numpy())
            )
            if out_of_bounds.any():
                idx = np.where(out_of_bounds.any(axis=1))[0]
                raise ValueError(
                    f""Interpolants at indices {idx.tolist()} are outside the bounds of the data.""
                )

        # Compute distances (Euclidean)
        # Using broadcasting: (n_interp, 1, d) - (1, n_data, d)
        diff = interp_np[:, np.newaxis, :] - self._data_np[np.newaxis, :, :]
        dists = np.linalg.norm(diff, axis=2)  # shape (n_interp, n_data)

        # Find nearest neighbour indices
        nearest_idx = np.argmin(dists, axis=1)

        # Gather values
        result = self.data.iloc[nearest_idx][self.value_columns].reset_index(drop=True)

        # Preserve original index of interpolants
        result.index = interpolants.index
        return result
```"
306151,lago-project/lago,lago-project_lago/lago/templates.py,lago.templates.Template,"class Template:
    """"""
    Disk image template class

    Attributes:
        name (str): Name of this template
        _versions (dict(str:TemplateVersion)): versions for this template
    """"""

    def __init__(self, name, versions):
        """"""
        Args:
            name (str): Name of the template
            versions (dict(str:TemplateVersion)): dictionary with the
                version_name: :class:`TemplateVersion` pairs for this template
        """"""
        self.name = name
        self._versions = versions

    def get_version(self, ver_name=None):
        """"""
        Get the given version for this template, or the latest

        Args:
            ver_name (str or None): Version to retieve, None for the latest

        Returns:
            TemplateVersion: The version matching the given name or the latest
                one
        """"""
        if ver_name is None:
            return self.get_latest_version()
        return self._versions[ver_name]

    def get_latest_version(self):
        """"""
        Retrieves the latest version for this template, the latest being the
        one with the newest timestamp

        Returns:
            TemplateVersion
        """"""
        return max(self._versions.values(), key=lambda x: x.timestamp())","class Template:
    
    def __init__(self, name, versions):
        
        pass
    def get_version(self, ver_name=None):
        
        pass
    def get_latest_version(self):
        
        pass",snippet_233,4,"class Template:
    def __init__(self, name, versions):
        """"""
        Initialize a Template instance.

        Parameters
        ----------
        name : str
            The name of the template.
        versions : list of tuples or dict
            A collection of versions. Each element can be a tuple
            (ver_name, ver_data) or a dict with keys 'name' and 'data'.
            The order of the collection determines the ""latest"" version.
        """"""
        self.name = name
        # Normalize versions to a list of (name, data) tuples
        self._versions = []
        for v in versions:
            if isinstance(v, dict):
                ver_name = v.get(""name"")
                ver_data = v.get(""data"")
            else:
                ver_name, ver_data = v
            if ver_name is None:
                raise ValueError(""Each version must have a name"")
            self._versions.append((ver_name, ver_data))

    def get_version(self, ver_name=None):
        """"""
        Retrieve a specific version by name, or the latest version if no name is provided.

        Parameters
        ----------
        ver_name : str, optional
            The name of the desired version. If omitted, the latest version is returned.

        Returns
        -------
        The data associated with the requested version.

        Raises
        ------
        KeyError
            If the requested version name does not exist.
        """"""
        if ver_name is None:
            return self.get_latest_version()
        for name, data in self._versions:
            if name == ver_name:
                return data
        raise KeyError(f""Version '{ver_name}' not found in template '{self.name}'"")

    def get_latest_version(self):
        """"""
        Retrieve the most recently added version.

        Returns
        -------
        The data of the latest version.

        Raises
        ------
        IndexError
            If no versions are available.
        """"""
        if not self._versions:
            raise IndexError(f""No versions available for template '{self.name}'"")
        return self._versions[-1][1]"
311056,geronimp/graftM,geronimp_graftM/graftm/deduplicator.py,graftm.deduplicator.Deduplicator,"class Deduplicator:
    """"""Deduplicates sequences""""""

    def deduplicate(self, aligned_sequence_objects):
        """"""Sort the given aligned_sequence objects into an array of arrays,
        where input sequences are grouped iff they have the same sequence

        Parameters
        ----------
        aligned_sequence_objects: array of Sequence objects
            input sequences

        Returns
        -------
        Array of arrays of Sequence objects""""""
        sequence_to_groups = {}
        for s in aligned_sequence_objects:
            try:
                sequence_to_groups[s.seq].append(s)
            except KeyError:
                sequence_to_groups[s.seq] = [s]
        return list(sequence_to_groups.values())

    def lca_taxonomy(self, deduplicated_sequences, taxonomy_hash):
        """"""Given a set of deduplicated sequences and a taxonomy hash,
        return the respective LCAs of taxonomy

        Parameters
        ----------
        deduplicated_sequences: Array of arrays of Sequence objects
            as output from deduplicate()
        taxonomy_hash: dictionary 
            of sequence names to taxonomy array (i.e. array of str)

        Returns
        -------
        Array of taxonomy LCAs""""""
        to_return = []
        for dup_group in deduplicated_sequences:
            lca = taxonomy_hash[dup_group[0].name]
            for s in dup_group[1:]:
                for i, tax in enumerate(taxonomy_hash[s.name]):
                    if i >= len(lca) or tax != lca[i]:
                        lca = lca[:i]
                        break
                if len(lca) > len(taxonomy_hash[s.name]):
                    lca = lca[:len(taxonomy_hash[s.name])]
            to_return.append(lca)
        return to_return","class Deduplicator:
    
    def deduplicate(self, aligned_sequence_objects):
        
        pass
    def lca_taxonomy(self, deduplicated_sequences, taxonomy_hash):
        
        pass",snippet_235,3,"```python
class Deduplicator:
    """"""
    A simple deduplication and taxonomy LCA utility.

    The class expects objects that expose at least two attributes:
        - `id`   : a unique identifier for the sequence
        - `seq`  : the sequence string

    The `deduplicate` method removes duplicate sequences (by sequence string)
    while preserving the original order of first occurrences.

    The `lca_taxonomy` method computes the lowest common ancestor (LCA) of
    taxonomy paths for a list of deduplicated sequences.  The taxonomy
    paths are supplied via a dictionary mapping sequence IDs to a list of
    taxonomy ranks (e.g. ['k__Bacteria', 'p__Proteobacteria', ...]).
    The method returns the longest common prefix of these taxonomy lists.
    """"""

    def deduplicate(self, aligned_sequence_objects):
        """"""
        Remove duplicate sequences from a list of sequence objects.

        Parameters
        ----------
        aligned_sequence_objects : list
            List of objects that have at least the attributes `id` and `seq`.

        Returns
        -------
        list
            A list of unique sequence objects, preserving the order of first
            appearance.
        """"""
        seen = set()
        unique = []
        for obj in aligned_sequence_objects:
            seq = getattr(obj, ""seq"", None)
            if seq is None:
                # If the object does not have a `seq` attribute, skip it.
                continue
            if seq not in seen:
                seen.add(seq)
                unique.append(obj)
        return unique

    def lca_taxonomy(self, deduplicated_sequences, taxonomy_hash):
        """"""
        Compute the lowest common ancestor (LCA) taxonomy for a set of
        deduplicated sequences.

        Parameters
        ----------
        deduplicated_sequences : list
            List of sequence objects that have an `id` attribute.
        taxonomy_hash : dict
            Mapping from sequence ID to a list of taxonomy ranks.

        Returns
        -------
        list
            The longest common prefix of the taxonomy lists for all
            provided sequences.  If no common prefix exists, an empty list
            is returned.
        """"""
        # Gather taxonomy lists for all sequences
        tax_lists = []
        for seq_obj in deduplicated_sequences:
            seq_id = getattr(seq_obj, ""id"", None)
            if seq_id is None:
                continue
            tax = taxonomy_hash.get(seq_id)
            if tax is None:
                continue
            tax_lists.append(tax)

        if not tax_lists:
            return []

        # Compute common prefix
        lca = []
        for ranks in zip(*tax_lists):
            if all(rank == ranks[0] for rank in ranks):
                lca.append(ranks[0])
            else:
                break
        return lca
```"
311087,geronimp/graftM,geronimp_graftM/graftm/search_table.py,graftm.search_table.SearchTableWriter,"import logging
from graftm.sequence_search_results import SequenceSearchResult

class SearchTableWriter:
    """"""
    Class for writing the search output OTU table. Basically a summary
    of hits to the HMM/Diamond searched in the following format:

             #ID    Metagenome_1    Metagenome_2    ...
            HMM1    50              6
            HMM2    195             41
            HMM3    2               20120
            ...

    You just need to specify a series of SequenceSearchResult objects, and an
    output path.
    """"""

    def _interpret_hits(self, results_list, base_list):
        """"""Sort reads that hit multiple HMMs to the databases to which they had
        the highest bit score. Return a dictionary containing HMMs as keys, and
        number of hits as the values.

        This function is set up so that the read names could easily be returned
        instead of numbers, for future development of GraftM

        Parameters
        ----------
        results_list: list
            Iterable if SequenceSearchResult objects. e.g.
                [SequenceSearchResult_1, SequenceSearchResult_2, ...]

        base_list: list
            Iterable of the basenames for each sequence file provided to graftM
            e.g.
                [sample_1, sample_2, ...]

        Returns
        -------
        dictionary:
            Contains samples as entries. The value for each sample is another
            dictionary with HMM as the key, and number of hits as values:
                {""sample_1"":{HMM_1: 12
                             HMM_2: 35
                             HMM_3: 1258
                             ...}
                 ...
                }

        """"""
        logging.debug('Sorting reads into HMMs by bit score')
        run_results = {}
        for base, results in zip(base_list, results_list):
            search_results = {}
            for search in results():
                search_list = list(search.each([SequenceSearchResult.QUERY_ID_FIELD, SequenceSearchResult.ALIGNMENT_BIT_SCORE, SequenceSearchResult.HMM_NAME_FIELD]))
                for hit in search_list:
                    if hit[0] in search_results:
                        if float(hit[1]) > search_results[hit[0]][0]:
                            search_results[hit[0]] = [float(hit[1]), hit[2]]
                    else:
                        search_results[hit[0]] = [float(hit[1]), hit[2]]
            run_results[base] = search_results
        db_count = {}
        for run in run_results.keys():
            run_count = {}
            for entry in list(run_results[run].values()):
                key = entry[1]
                if key in run_count:
                    run_count[key] += 1
                else:
                    run_count[key] = 1
            db_count[run] = run_count
        return db_count

    def _write_results(self, db_count, output_path):
        """"""Write the table to the output_path directory

        db_count: dict
            Contains samples as entries. The value for each sample is another
            dictionary with HMM as the key, and number of hits as values:
                {""sample_1"":{HMM_1: 12
                             HMM_2: 35
                             HMM_3: 1258
                             ...}
                 ...
                }

        output_path: str
            Path to output file to which the resultant output file will be
            written to.
        """"""
        logging.debug('Writing search otu table to file: %s' % output_path)
        output_dict = {}
        for idx, value_dict in enumerate(db_count.values()):
            for database, count in value_dict.items():
                if database in output_dict:
                    output_dict[database].append(str(count))
                else:
                    output_dict[database] = ['0'] * idx + [str(count)]
            for key, item in output_dict.items():
                if len(item) == idx:
                    output_dict[key].append('0')
        with open(output_path, 'w') as out:
            out.write('\t'.join(['#ID'] + list(db_count.keys())) + '\n')
            for key, item in output_dict.items():
                out.write('%s\t%s' % (key, '\t'.join(item)) + '\n')

    def build_search_otu_table(self, search_results_list, base_list, output_path):
        """"""
        Build an OTU from SequenceSearchResult objects

        Parameters
        ----------
        search_results_list: list
            Iterable if SequenceSearchResult objects. e.g.
                [SequenceSearchResult_1, SequenceSearchResult_2, ...]
        base_list: list
            Iterable of the basenames for each sequence file provided to graftM
            e.g.
                [sample_1, sample_2, ...]
        output_path: str
            Path to output file to which the resultant output file will be
            written to.
        """"""
        db_count = self._interpret_hits(search_results_list, base_list)
        self._write_results(db_count, output_path)","
class SearchTableWriter:
    
    def _interpret_hits(self, results_list, base_list):
        
        pass
    def _write_results(self, db_count, output_path):
        
        pass

    def build_search_otu_table(self, search_results_list, base_list, output_path):
        
        pass",snippet_236,4,"```python
import csv
import os
from collections import defaultdict

class SearchTableWriter:
    """"""
    Utility class to build an OTU table from search results.
    """"""

    def _interpret_hits(self, results_list, base_list):
        """"""
        Convert a list of search results into a nested dictionary mapping
        base IDs to search IDs and their corresponding scores.

        Parameters
        ----------
        results_list : list
            Each element is expected to be a mapping with at least the keys
            'search_id' and 'hits'.  The 'hits' value should be an iterable
            of mappings containing 'base_id' and 'score'.
        base_list : list
            List of all base IDs that should appear as rows in the table.

        Returns
        -------
        dict
            A dictionary where keys are base IDs and values are dictionaries
            mapping search IDs to scores.  Missing scores are omitted.
        """"""
        # Ensure base_list is a set for quick membership checks
        base_set = set(base_list)

        # Nested dict: base_id -> {search_id: score}
        table = defaultdict(dict)

        for result in results_list:
            search_id = result.get(""search_id"")
            if search_id is None:
                continue
            hits = result.get(""hits"", [])
            for hit in hits:
                base_id = hit.get(""base_id"")
                if base_id is None or base_id not in base_set:
                    continue
                score = hit.get(""score"", 0)
                table[base_id][search_id] = score

        # Ensure all base IDs are present in the table
        for base_id in base_set:
            table.setdefault(base_id, {})

        return dict(table)

    def _write_results(self, db_count, output_path):
        """"""
        Write the OTU table to a CSV file.

        Parameters
        ----------
        db_count : int
            Number of base entries (used only for validation).
        output_path : str
            Path to the output CSV file.
        """"""
        # The actual data to write is stored in self._table and self._search_ids
        if not hasattr(self, ""_table"") or not hasattr(self, ""_search_ids""):
            raise RuntimeError(""No table data available. Call build_search_otu_table first."")

        # Validate db_count matches number of base rows
        if db_count != len(self._table):
            raise ValueError(f""db_count ({db_count}) does not match number of base rows ({len(self._table)})."")

        # Ensure output directory exists
        os.makedirs(os.path.dirname(output_path) or ""."", exist_ok=True)

        with open(output_path, ""w"", newline="""") as csvfile:
            writer = csv.writer(csvfile)
            header = [""BaseID""] + self._search_ids
            writer.writerow(header)

            for base_id in sorted(self._table):
                row = [base_id]
                scores = self._table[base_id]
                for search_id in self._search_ids:
                    row.append(scores.get(search_id, 0))
                writer.writerow(row)

    def build_search_otu_table(self, search_results_list, base_list, output_path):
        """"""
        Build an OTU table from search results and write it to a CSV file.

        Parameters
        ----------
        search_results_list : list
            List of search result mappings.
        base_list : list
            List of base IDs.
        output_path : str
            Path to the output CSV file.

        Returns
        -------
        None
        """"""
        # Interpret hits into a nested dictionary
        self._table = self._interpret_hits(search_results_list, base_list)

        # Determine the set of search IDs from the results
        search_ids = set()
        for result in search_results_list:
            search_id = result.get(""search_id"")
            if search_id is not None:
                search_ids.add(search_id)
        self._search_ids = sorted(search_ids)

        # Write the table to CSV
        self._write_results(len(base_list), output_path)
```"
311100,geronimp/graftM,geronimp_graftM/graftm/tree_decorator.py,graftm.tree_decorator.TreeDecorator,"from graftm.greengenes_taxonomy import GreenGenesTaxonomy, MalformedGreenGenesTaxonomyException
from graftm.getaxnseq import Getaxnseq
import logging
from graftm.taxonomy_cleaner import TaxonomyCleaner

class TreeDecorator:
    """"""
    A class that conservatively decorates trees with taxonomy, or any other
    hierarchical annotation. If all tips descending from a node within the
    provided tree have consistent taxonomy, it will be decorated with that
    taxonomy (or annotation of any type).
    """"""

    def __init__(self, tree, taxonomy, seqinfo=None):
        """"""
        Parameters
        ----------
        tree        : dendropy.Tree

            dendropy.Tree object
        taxonomy    : string
            Path to a file containing taxonomy information about the tree,
            either in Greengenes or taxtastic format (seqinfo file must also
            be provided if taxonomy is in taxtastic format).
        seqinfo     : string
            Path to a seqinfo file. This is a .csv file with the first column
            denoting the sequence name, and the second column, its most resolved
            taxonomic rank.
        """"""
        self.encountered_nodes = {}
        self.encountered_taxonomies = set()
        self.tree = tree
        logging.info('Reading in taxonomy')
        if seqinfo:
            logging.info('Importing taxtastic taxonomy from files: %s and %s' % (taxonomy, seqinfo))
            gtns = Getaxnseq()
            self.taxonomy = gtns.read_taxtastic_taxonomy_and_seqinfo(open(taxonomy), open(seqinfo))
        else:
            try:
                logging.info('Reading Greengenes style taxonomy')
                self.taxonomy = GreenGenesTaxonomy.read_file(taxonomy).taxonomy
            except MalformedGreenGenesTaxonomyException:
                raise Exception('Failed to read taxonomy as a Greengenes                                  formatted file. Was a taxtastic style                                  taxonomy provided with no seqinfo file?')

    def _write_consensus_strings(self, output):
        """"""
        Writes the taxonomy of each leaf to a file. If the leaf has no
        taxonomy, a taxonomy string will be created using the annotations
        provided to the ancestor nodes of that leaf (meaning, it will be
        decorated).

        Parameters
        ----------
        output    : string
            File to which the taxonomy strings for each leaf in the tree will
            be written in Greengenes format, e.g.
                637960147    mcrA; Euryarchaeota_mcrA; Methanomicrobia
                637699780    mcrA; Euryarchaeota_mcrA; Methanomicrobia
        """"""
        logging.info('Writing decorated taxonomy to file: %s' % output)
        with open(output, 'w') as out:
            for tip in self.tree.leaf_nodes():
                tax_name = tip.taxon.label.replace(' ', '_')
                if tip.taxon.label in self.taxonomy:
                    tax_string = '; '.join(self.taxonomy[tax_name])
                else:
                    ancestor_list = []
                    for ancestor in tip.ancestor_iter():
                        if ancestor.label:
                            split_node_name = ancestor.label.split(':')
                            if len(split_node_name) == 2:
                                ancestor_list += list(reversed(split_node_name[1].split('; ')))
                            elif len(split_node_name) == 1:
                                try:
                                    float(split_node_name[0])
                                except ValueError:
                                    ancestor_list += list(reversed(split_node_name[0].split('; ')))
                            else:
                                raise Exception('Malformed node name: %s' % ancestor.label)
                    tax_list = list(reversed(ancestor_list))
                    if len(tax_list) < 1:
                        logging.warning('No taxonomy found for species %s!' % tax_name)
                        tax_string = 'Unknown'
                    else:
                        tax_string = '; '.join(tax_list)
                output_line = '%s\t%s\n' % (tax_name, tax_string)
                out.write(output_line)

    def _rename(self, node, name):
        """"""
        Rename an internal node of the tree. If an annotation is already
        present, append the new annotation to the end of it. If a bootstrap
        value is present, add annotations are added after a "":"" as per standard
        newick format.

        Parameters
        ----------
        node: dendropy.Node
            dendropy.Node object
        name    : string
            Annotation to rename the node with.
        """"""
        if node.label:
            try:
                float(node.label)
                new_label = '%s:%s' % (node.label, name)
            except ValueError:
                new_label = '%s; %s' % (node.label, name)
            node.label = new_label
        else:
            node.label = name

    def decorate(self, output_tree, output_tax, unique_names):
        """"""
        Decorate a tree with taxonomy. This code does not allow inconsistent
        taxonomy within a clade. If one sequence in a clade has a different
        annotation to the rest, it will split the clade. Paraphyletic group
        names are distinguished if unique_names = True using a simple tally of
        each group (see unique_names below).

        Parameters
        ----------
        output_tree        : string
            File to which the decorated tree will be written.
        output_tax         : string
            File to which the taxonomy strings for each tip in the tree will be
            written.
        unique_names       : boolean
            True indicating that a unique number will be appended to the end of
            a taxonomic rank if it is found more than once in the tree
            (i.e. it is paraphyletic in the tree). If false, multiple clades
            may be assigned with the same name.
        """"""
        logging.info('Decorating tree')
        encountered_taxonomies = {}
        tc = TaxonomyCleaner()
        for node in self.tree.preorder_internal_node_iter(exclude_seed_node=True):
            max_tax_string_length = 0
            for tip in node.leaf_nodes():
                tip_label = tip.taxon.label.replace(' ', '_')
                if tip_label in self.taxonomy:
                    tax_string_length = len(self.taxonomy[tip.taxon.label.replace(' ', '_')])
                    if tax_string_length > max_tax_string_length:
                        max_tax_string_length = tax_string_length
            logging.debug('Number of ranks found for node: %i' % max_tax_string_length)
            tax_string_array = []
            for rank in range(max_tax_string_length):
                rank_tax = []
                for tip in node.leaf_nodes():
                    tip_label = tip.taxon.label.replace(' ', '_')
                    if tip_label in self.taxonomy:
                        tip_tax = self.taxonomy[tip_label]
                        if len(tip_tax) > rank:
                            tip_rank = tip_tax[rank]
                            if tip_rank not in rank_tax:
                                rank_tax.append(tip_rank)
                consistent_taxonomy = len(rank_tax) == 1
                if consistent_taxonomy:
                    tax = rank_tax.pop()
                    logging.debug('Consistent taxonomy found for node: %s' % tax)
                    if tax not in tc.meaningless_taxonomic_names:
                        if unique_names:
                            if tax in encountered_taxonomies:
                                encountered_taxonomies[tax] += 0
                                tax = '%s_%i' % (tax, encountered_taxonomies[tax])
                            else:
                                encountered_taxonomies[tax] = 0
                        tax_string_array.append(tax)
            if any(tax_string_array):
                index = 0
                for anc in node.ancestor_iter():
                    try:
                        index += anc.tax
                    except:
                        continue
                tax_string_array = tax_string_array[index:]
                if any(tax_string_array):
                    self._rename(node, '; '.join(tax_string_array))
                node.tax = len(tax_string_array)
        logging.info('Writing decorated tree to file: %s' % output_tree)
        if output_tree:
            self.tree.write(path=output_tree, schema='newick')
        if output_tax:
            self._write_consensus_strings(output_tax)","
class TreeDecorator:
    
    def __init__(self, tree, taxonomy, seqinfo=None):
        
        pass
    def _write_consensus_strings(self, output):
        
        pass

    def _rename(self, node, name):
        
        pass

    def decorate(self, output_tree, output_tax, unique_names):
        '''
        Decorate a tree with taxonomy. This code does not allow inconsistent
        taxonomy within a clade. If one sequence in a clade has a different
        annotation to the rest, it will split the clade. Paraphyletic group
        names are distinguished if unique_names = True using a simple tally of
        each group (see unique_names below).
        Parameters
        ----------
        output_tree        : string
            File to which the decorated tree will be written.
        output_tax         : string
            File to which the taxonomy strings for each tip in the tree will be
            written.
        unique_names       : boolean
            True indicating that a unique number will be appended to the end of
            a taxonomic rank if it is found more than once in the tree
            (i.e. it is paraphyletic in the tree). If false, multiple clades
            may be assigned with the same name.
        '''
        pass",snippet_237,5,"```python
import collections
from Bio import Phylo
from io import StringIO

class TreeDecorator:
    """"""
    Decorate a phylogenetic tree with taxonomy information.

    Parameters
    ----------
    tree : Bio.Phylo.BaseTree.Tree
        The tree to decorate.
    taxonomy : dict
        Mapping from tip names to taxonomy strings.
    seqinfo : dict, optional
        Optional mapping from tip names to additional information (unused
        in this implementation but kept for compatibility).
    """"""

    def __init__(self, tree, taxonomy, seqinfo=None):
        self.tree = tree
        self.taxonomy = taxonomy
        self.seqinfo = seqinfo or {}
        # Counter for unique names
        self.name_counts = collections.Counter()

    def _write_consensus_strings(self, output):
        """"""
        Write taxonomy strings for each tip in the tree to the given file-like
        object.

        Parameters
        ----------
        output : file-like
            File object opened for writing.
        """"""
        for tip in self.tree.get_terminals():
            tax = self.taxonomy.get(tip.name, """")
            output.write(f""{tip.name}\t{tax}\n"")

    def _rename(self, node, name):
        """"""
        Rename a node in the tree.

        Parameters
        ----------
        node : Bio.Phylo.BaseTree.Clade
            The node to rename.
        name : str
            The new name for the node.
        """"""
        node.name = name

    def _consensus_taxonomy(self, clade):
        """"""
        Recursively compute the consensus taxonomy for a clade.

        Returns
        -------
        tax : str or None
            The consensus taxonomy string if all descendant tips share the
            same taxonomy; otherwise None.
        """"""
        if clade.is_terminal():
            return self.taxonomy.get(clade.name, None)

        child_taxes = [self._consensus_taxonomy(child) for child in clade.clades]
        # If any child returned None, propagate None
        if any(t is None for t in child_taxes):
            return None

        # All child taxes are strings; check if they are identical
        if all(t == child_taxes[0] for t in child_taxes):
            return child_taxes[0]
        return None

    def _decorate_node(self, clade, unique_names):
        """"""
        Decorate a node with taxonomy information.

        Parameters
        ----------
        clade : Bio.Phylo.BaseTree.Clade
            The node to decorate.
        unique_names : bool
            If True, append a unique counter to duplicate names.
        """"""
        if clade.is_terminal():
            # Terminal nodes keep their original names
            return

        # Decorate children first
        for child in clade.clades:
            self._decorate_node(child, unique_names)

        # Compute consensus taxonomy for this clade
        tax = self._consensus_taxonomy(clade)

        if tax is not None:
            # Use taxonomy as node name
            name = tax
            if unique_names:
                count = self.name_counts[name]
                if count > 0:
                    name = f""{tax}_{count}""
                self.name_counts[tax] += 1
            self._rename(clade, name)
        else:
            # If no consensus, leave node unnamed (or set to None)
            self._rename(clade, None)

    def decorate(self, output_tree, output_tax, unique_names):
        """"""
        Decorate a tree with taxonomy. This code does not allow inconsistent
        taxonomy within a clade. If one sequence in a clade has a different
        annotation to the rest, it will split the clade. Paraphyletic group
        names are distinguished if unique_names = True using a simple tally of
        each group (see unique_names below).

        Parameters
        ----------
        output_tree : str
            File to which the decorated tree will be written.
        output_tax : str
            File to which the taxonomy strings for each tip in the tree will be
            written.
        unique_names : bool
            True indicating that a unique number will be appended to the end of
            a taxonomic rank if it is found more than once in the tree
            (i.e. it is paraphyletic in the tree). If false, multiple clades
            may be assigned with the same name.
        """"""
        # Decorate the tree
        self._decorate_node(self.tree.root, unique_names)

        # Write decorated tree to file
        with open(output_tree, ""w"") as f:
            Phylo.write(self.tree, f, ""newick"")

        # Write taxonomy strings for each tip
        with open(output_tax, ""w"") as f:
            self._write_consensus_strings(f)
```"
311642,abhishek-ram/pyas2-lib,abhishek-ram_pyas2-lib/pyas2lib/as2.py,pyas2lib.as2.Partner,"from oscrypto import asymmetric
from pyas2lib.exceptions import AS2Exception, DuplicateDocument, ImproperlyConfigured, InsufficientSecurityError, IntegrityError, MDNNotFound, PartnerNotFound
from pyas2lib.utils import canonicalize, extract_first_part, make_mime_boundary, mime_to_bytes, pem_to_der, quote_as2name, split_pem, unquote_as2name, verify_certificate_chain
from pyas2lib.constants import AS2_VERSION, ASYNCHRONOUS_MDN, DIGEST_ALGORITHMS, EDIINT_FEATURES, ENCRYPTION_ALGORITHMS, KEY_ENCRYPTION_ALGORITHMS, MDN_CONFIRM_TEXT, MDN_FAILED_TEXT, MDN_MODES, SIGNATUR_ALGORITHMS, SYNCHRONOUS_MDN
from dataclasses import dataclass

@dataclass
class Partner:
    """"""
    Class represents an AS2 partner and defines the certificates and
    settings to be used when sending and receiving messages.

    :param as2_name: The unique AS2 name for this partner.

    :param verify_cert: A byte string of the certificate to be used for
        verifying signatures of inbound messages and MDNs.

    :param verify_cert_ca: A byte string of the ca certificate if any of
        the verification cert

    :param encrypt_cert: A byte string of the certificate to be used for
        encrypting outbound message.

    :param encrypt_cert_ca: A byte string of the ca certificate if any of
        the encryption cert

    :param validate_certs: Set this flag to `False` to disable validations of
        the encryption and verification certificates. (default `True`)

    :param compress: Set this flag to `True` to compress outgoing
        messages. (default `False`)

    :param sign: Set this flag to `True` to sign outgoing
        messages. (default `False`)

    :param digest_alg: The digest algorithm to be used for generating the
        signature. (default ""sha256"")

    :param encrypt: Set this flag to `True` to encrypt outgoing
        messages. (default `False`)

    :param enc_alg:
        The encryption algorithm to be used. (default `""tripledes_192_cbc""`)

    :param mdn_mode: The mode to be used for receiving the MDN.
        Set to `None` for no MDN, `'SYNC'` for synchronous and `'ASYNC'`
        for asynchronous. (default `None`)

    :param mdn_digest_alg: The digest algorithm to be used by the receiver
        for signing the MDN. Use `None` for unsigned MDN. (default `None`)

    :param mdn_confirm_text: The text to be used in the MDN for successfully
        processed messages received from this partner.

    :param canonicalize_as_binary: force binary canonicalization for this partner

    :param sign_alg: The signing algorithm to be used for generating the
        signature. (default `rsassa_pkcs1v15`)

    :param key_enc_alg: The key encryption algorithm to be used.
        (default `rsaes_pkcs1v15`)

    """"""
    as2_name: str
    verify_cert: bytes = None
    verify_cert_ca: bytes = None
    encrypt_cert: bytes = None
    encrypt_cert_ca: bytes = None
    validate_certs: bool = True
    compress: bool = False
    encrypt: bool = False
    enc_alg: str = 'tripledes_192_cbc'
    sign: bool = False
    digest_alg: str = 'sha256'
    mdn_mode: str = None
    mdn_digest_alg: str = None
    mdn_confirm_text: str = MDN_CONFIRM_TEXT
    ignore_self_signed: bool = True
    canonicalize_as_binary: bool = False
    sign_alg: str = 'rsassa_pkcs1v15'
    key_enc_alg: str = 'rsaes_pkcs1v15'

    def __post_init__(self):
        """"""Run the post initialisation checks for this class.""""""
        if self.digest_alg and self.digest_alg not in DIGEST_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported Digest Algorithm {self.digest_alg}, must be one of {DIGEST_ALGORITHMS}')
        if self.enc_alg and self.enc_alg not in ENCRYPTION_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported Encryption Algorithm {self.enc_alg}, must be one of {ENCRYPTION_ALGORITHMS}')
        if self.mdn_mode and self.mdn_mode not in MDN_MODES:
            raise ImproperlyConfigured(f'Unsupported MDN Mode {self.mdn_mode}, must be one of {MDN_MODES}')
        if self.mdn_digest_alg and self.mdn_digest_alg not in DIGEST_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported MDN Digest Algorithm {self.mdn_digest_alg}, must be one of {DIGEST_ALGORITHMS}')
        if self.sign_alg and self.sign_alg not in SIGNATUR_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported Signature Algorithm {self.sign_alg}, must be one of {SIGNATUR_ALGORITHMS}')
        if self.key_enc_alg and self.key_enc_alg not in KEY_ENCRYPTION_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported Key Encryption Algorithm {self.key_enc_alg}, must be one of {KEY_ENCRYPTION_ALGORITHMS}')

    def load_verify_cert(self):
        """"""Load the verification certificate of the partner and returned the parsed cert.""""""
        if self.validate_certs:
            cert = pem_to_der(self.verify_cert, return_multiple=False)
            if self.verify_cert_ca:
                trust_roots = pem_to_der(self.verify_cert_ca)
            else:
                trust_roots = []
            verify_certificate_chain(cert, trust_roots, ignore_self_signed=self.ignore_self_signed)
        return asymmetric.load_certificate(self.verify_cert)

    def load_encrypt_cert(self):
        """"""Load the encryption certificate of the partner and returned the parsed cert.""""""
        if self.validate_certs:
            cert = pem_to_der(self.encrypt_cert, return_multiple=False)
            if self.encrypt_cert_ca:
                trust_roots = pem_to_der(self.encrypt_cert_ca)
            else:
                trust_roots = []
            verify_certificate_chain(cert, trust_roots, ignore_self_signed=self.ignore_self_signed)
        return asymmetric.load_certificate(self.encrypt_cert)","@dataclass
class Partner:
    
    def __post_init__(self):
        
        pass
    def load_verify_cert(self):
        
        pass

    def load_encrypt_cert(self):
        
        pass",snippet_239,4,"```python
from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional


@dataclass
class Partner:
    """"""
    Represents a partner entity that holds certificates for verification and encryption.
    """"""
    # Paths to the certificate files
    verify_cert_path: Path | str
    encrypt_cert_path: Path | str

    # Loaded certificate data (bytes)
    verify_cert: Optional[bytes] = field(default=None, init=False)
    encrypt_cert: Optional[bytes] = field(default=None, init=False)

    def __post_init__(self):
        """"""
        Load certificates immediately after initialization.
        """"""
        self.load_verify_cert()
        self.load_encrypt_cert()

    def load_verify_cert(self):
        """"""
        Load the verification certificate from the specified path.
        """"""
        path = Path(self.verify_cert_path)
        if not path.is_file():
            raise FileNotFoundError(f""Verification certificate not found: {path}"")
        with path.open(""rb"") as f:
            self.verify_cert = f.read()

    def load_encrypt_cert(self):
        """"""
        Load the encryption certificate from the specified path.
        """"""
        path = Path(self.encrypt_cert_path)
        if not path.is_file():
            raise FileNotFoundError(f""Encryption certificate not found: {path}"")
        with path.open(""rb"") as f:
            self.encrypt_cert = f.read()
```"
311862,reportportal/client-Python,reportportal_client-Python/reportportal_client/core/rp_issues.py,reportportal_client.core.rp_issues.ExternalIssue,"class ExternalIssue:
    """"""This class represents external(BTS) system issue.""""""

    def __init__(self, bts_url=None, bts_project=None, submit_date=None, ticket_id=None, url=None):
        """"""Initialize instance attributes.

        :param bts_url:     Bug tracker system URL
        :param bts_project: Bug tracker system project
        :param submit_date: Bug submission date
        :param ticket_id:   Unique ID of the ticket at the BTS
        :param url:         URL to the ticket(bug)
        """"""
        self.bts_url = bts_url
        self.bts_project = bts_project
        self.submit_date = submit_date
        self.ticket_id = ticket_id
        self.url = url

    @property
    def payload(self):
        """"""Form the correct dictionary for the BTS issue.""""""
        return {'btsUrl': self.bts_url, 'btsProject': self.bts_project, 'submitDate': self.submit_date, 'ticketId': self.ticket_id, 'url': self.url}","class ExternalIssue:
    
    def __init__(self, bts_url=None, bts_project=None, submit_date=None, ticket_id=None, url=None):
        
        pass
    @property
    def payload(self):
        
        pass",snippet_240,3,"```python
from datetime import datetime
from typing import Any, Dict, Optional


class ExternalIssue:
    """"""
    Represents an issue in an external bug tracking system.

    Attributes
    ----------
    bts_url : Optional[str]
        Base URL of the bug tracking system.
    bts_project : Optional[str]
        Project identifier within the bug tracking system.
    submit_date : Optional[datetime | str]
        Date the issue was submitted. If a string is provided, it will be parsed
        as an ISO‑8601 datetime.
    ticket_id : Optional[str]
        Identifier of the ticket in the external system.
    url : Optional[str]
        Direct URL to the issue.
    """"""

    def __init__(
        self,
        bts_url: Optional[str] = None,
        bts_project: Optional[str] = None,
        submit_date: Optional[Any] = None,
        ticket_id: Optional[str] = None,
        url: Optional[str] = None,
    ) -> None:
        self.bts_url = bts_url
        self.bts_project = bts_project

        # Normalize submit_date to datetime if possible
        if isinstance(submit_date, str):
            try:
                self.submit_date = datetime.fromisoformat(submit_date)
            except ValueError:
                # If parsing fails, keep the original string
                self.submit_date = submit_date
        else:
            self.submit_date = submit_date

        self.ticket_id = ticket_id
        self.url = url

    @property
    def payload(self) -> Dict[str, Any]:
        """"""
        Returns a dictionary representation of the issue suitable for
        serialization or transmission.

        The dictionary includes all attributes, converting datetime objects
        to ISO‑8601 strings.
        """"""
        payload: Dict[str, Any] = {
            ""bts_url"": self.bts_url,
            ""bts_project"": self.bts_project,
            ""submit_date"": (
                self.submit_date.isoformat()
                if isinstance(self.submit_date, datetime)
                else self.submit_date
            ),
            ""ticket_id"": self.ticket_id,
            ""url"": self.url,
        }
        return payload
```"
311863,reportportal/client-Python,reportportal_client-Python/reportportal_client/core/rp_issues.py,reportportal_client.core.rp_issues.Issue,"class Issue:
    """"""This class represents an issue that can be attached to test result.""""""

    def __init__(self, issue_type, comment=None, auto_analyzed=False, ignore_analyzer=True):
        """"""Initialize instance attributes.

        :param issue_type:      Issue type locator. Allowable values: ""pb***"",
                                ""ab***"", ""si***"", ""ti***"", ""nd001"". Where ***
                                is locator id.
        :param comment:         Issue comments
        :param auto_analyzed:   Indicator that the issue has been marked with
                                the RP auto analyzer
        :param ignore_analyzer: Flag that forces RP analyzer to ignore this
                                issue
        """"""
        self._external_issues = []
        self.auto_analyzed = auto_analyzed
        self.comment = comment
        self.ignore_analyzer = ignore_analyzer
        self.issue_type = issue_type

    def external_issue_add(self, issue):
        """"""Add external system issue to the issue.""""""
        self._external_issues.append(issue.payload)

    @property
    def payload(self):
        """"""Form the correct dictionary for the issue.""""""
        return {'autoAnalyzed': self.auto_analyzed, 'comment': self.comment, 'externalSystemIssues': self._external_issues, 'ignoreAnalyzer': self.ignore_analyzer, 'issueType': self.issue_type}","class Issue:
    
    def __init__(self, issue_type, comment=None, auto_analyzed=False, ignore_analyzer=True):
        
        pass
    def external_issue_add(self, issue):
        
        pass
    @property
    def payload(self):
        
        pass",snippet_241,4,"```python
class Issue:
    """"""
    Represents an issue with optional external issues attached.
    """"""

    def __init__(self, issue_type, comment=None, auto_analyzed=False, ignore_analyzer=True):
        """"""
        Initialize an Issue instance.

        Parameters
        ----------
        issue_type : str
            The type/category of the issue.
        comment : str, optional
            A textual comment describing the issue.
        auto_analyzed : bool, optional
            Flag indicating whether the issue was automatically analyzed.
        ignore_analyzer : bool, optional
            Flag indicating whether the analyzer should ignore this issue.
        """"""
        self.issue_type = issue_type
        self.comment = comment
        self.auto_analyzed = bool(auto_analyzed)
        self.ignore_analyzer = bool(ignore_analyzer)
        self._external_issues = []

    def external_issue_add(self, issue):
        """"""
        Add an external issue to this issue.

        Parameters
        ----------
        issue : Issue or dict
            The external issue to add. If a dict is provided, it is stored as-is.
        """"""
        if isinstance(issue, Issue):
            self._external_issues.append(issue)
        elif isinstance(issue, dict):
            self._external_issues.append(issue)
        else:
            raise TypeError(""external_issue_add expects an Issue instance or a dict"")

    @property
    def payload(self):
        """"""
        Return a dictionary representation of the issue, including any external issues.

        Returns
        -------
        dict
            A dictionary containing the issue data.
        """"""
        payload = {
            ""issue_type"": self.issue_type,
            ""comment"": self.comment,
            ""auto_analyzed"": self.auto_analyzed,
            ""ignore_analyzer"": self.ignore_analyzer,
            ""external_issues"": [],
        }

        for ext in self._external_issues:
            if isinstance(ext, Issue):
                payload[""external_issues""].append(ext.payload)
            else:
                payload[""external_issues""].append(ext)

        return payload
```"
312030,SuperCowPowers/workbench,aws_setup/aws_identity_check.py,aws_identity_check.AWSIdentityCheck,"from workbench.core.cloud_platform.aws.aws_account_clamp import AWSAccountClamp
from workbench.utils.config_manager import ConfigManager
import sys
import logging

class AWSIdentityCheck:
    """"""Just a Utility Script that allows people to check which AWS Identity is active""""""

    def __init__(self):
        """"""AWSIdentityCheck Initialization""""""
        self.log = logging.getLogger('workbench')
        self.aws_clamp = AWSAccountClamp()

    def check(self):
        """"""Check the AWS Identity""""""
        cm = ConfigManager()
        active_profile = cm.get_config('AWS_PROFILE')
        if active_profile:
            self.log.info(f'Workbench AWS_PROFILE: {active_profile}')
        else:
            self.log.info('No AWS_PROFILE set')
            sys.exit(0)
        self.log.info('\n\n*** Caller/Base Identity Check ***')
        self.aws_clamp.check_aws_identity()
        self.log.info('Caller/Base Identity Check Success...')
        self.log.info('\n\n*** AWS Assumed Role Check ***')
        self.aws_clamp.check_assumed_role()
        self.log.info('Assumed Role Check Success...')","
class AWSIdentityCheck:
    
    def __init__(self):
        
        pass
    def check(self):
        
        pass",snippet_242,3,"```python
import boto3
from botocore.exceptions import BotoCoreError, ClientError


class AWSIdentityCheck:
    """"""
    A simple helper to retrieve the AWS caller identity using STS.
    """"""

    def __init__(self):
        """"""
        Initializes an STS client. No arguments are required; the client
        will use the default credential chain.
        """"""
        self.sts_client = boto3.client(""sts"")

    def check(self):
        """"""
        Retrieves the caller identity from AWS STS.

        Returns:
            dict: The response from sts.get_caller_identity().

        Raises:
            BotoCoreError, ClientError: If the request fails.
        """"""
        try:
            response = self.sts_client.get_caller_identity()
            return response
        except (BotoCoreError, ClientError) as exc:
            raise
```"
312949,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/factory/source/element.py,apidoc.factory.source.element.Element,"from apidoc.lib.util.cast import to_boolean
import collections
from apidoc.object.source_raw import Sampleable, Displayable

class Element:
    """""" Populate Helper Factory
    """"""

    def set_common_datas(self, element, name, datas):
        """"""Populated common data for an element from dictionnary datas
        """"""
        element.name = str(name)
        if 'description' in datas:
            element.description = str(datas['description']).strip()
        if isinstance(element, Sampleable) and element.sample is None and ('sample' in datas):
            element.sample = str(datas['sample']).strip()
        if isinstance(element, Displayable):
            if 'display' in datas:
                element.display = to_boolean(datas['display'])
            if 'label' in datas:
                element.label = datas['label']
            else:
                element.label = element.name

    def create_dictionary_of_element_from_dictionary(self, property_name, datas):
        """"""Populate a dictionary of elements
        """"""
        response = {}
        if property_name in datas and datas[property_name] is not None and isinstance(datas[property_name], collections.Iterable):
            for key, value in datas[property_name].items():
                response[key] = self.create_from_name_and_dictionary(key, value)
        return response

    def create_list_of_element_from_dictionary(self, property_name, datas):
        """"""Populate a list of elements
        """"""
        response = []
        if property_name in datas and datas[property_name] is not None and isinstance(datas[property_name], list):
            for value in datas[property_name]:
                response.append(self.create_from_dictionary(value))
        return response

    def get_enum(self, property, enum, datas):
        """"""Factory enum type
        """"""
        str_property = str(datas[property]).lower()
        if str_property not in enum:
            raise ValueError('Unknow enum ""%s"" for ""%s"".' % (str_property, property))
        return enum(str_property)","
class Element:
    
    def set_common_datas(self, element, name, datas):
        
        pass
    def create_dictionary_of_element_from_dictionary(self, property_name, datas):
        
        pass

    def create_list_of_element_from_dictionary(self, property_name, datas):
        
        pass

    def get_enum(self, property, enum, datas):
        
        pass",snippet_244,5,"```python
from enum import Enum
from typing import Any, Dict, List, Optional, Type


class Element:
    """"""
    A flexible container that can be populated from dictionaries and can
    create nested structures of Elements or lists of Elements.
    """"""

    def set_common_datas(self, element: ""Element"", name: str, datas: Dict[str, Any]) -> None:
        """"""
        Set an attribute on *element* if *name* exists in *datas*.
        """"""
        if name in datas:
            setattr(element, name, datas[name])

    def create_dictionary_of_element_from_dictionary(
        self, property_name: str, datas: Dict[str, Any]
    ) -> Dict[str, ""Element""]:
        """"""
        Create a dictionary of Element instances from a nested dictionary
        located at *property_name* inside *datas*.
        """"""
        result: Dict[str, Element] = {}
        sub_dict = datas.get(property_name, {})
        if not isinstance(sub_dict, dict):
            return result

        for key, value in sub_dict.items():
            el = Element()
            if isinstance(value, dict):
                for k, v in value.items():
                    setattr(el, k, v)
            else:
                # If the value is not a dict, store it under a generic key
                setattr(el, ""value"", value)
            result[key] = el
        return result

    def create_list_of_element_from_dictionary(
        self, property_name: str, datas: Dict[str, Any]
    ) -> List[""Element""]:
        """"""
        Create a list of Element instances from a list located at
        *property_name* inside *datas*.
        """"""
        result: List[Element] = []
        sub_list = datas.get(property_name, [])
        if not isinstance(sub_list, list):
            return result

        for item in sub_list:
            el = Element()
            if isinstance(item, dict):
                for k, v in item.items():
                    setattr(el, k, v)
            else:
                setattr(el, ""value"", item)
            result.append(el)
        return result

    def get_enum(
        self, property: str, enum: Type[Enum], datas: Dict[str, Any]
    ) -> Optional[Enum]:
        """"""
        Retrieve an Enum member from *datas* using *property* as the key.
        Supports both integer/enum value lookup and name lookup.
        """"""
        if property not in datas:
            return None

        val = datas[property]
        try:
            # Try to construct the enum from the value directly
            return enum(val)
        except Exception:
            # Fallback to name lookup if the value is a string
            try:
                return enum[val]
            except Exception:
                return None
```"
313013,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/object/source_raw.py,apidoc.object.source_raw.Sampleable,"class Sampleable:
    """"""Element who can provide samples
    """"""

    def __init__(self):
        """"""Class instantiation
        """"""
        super().__init__()
        self.sample = None

    def get_sample(self):
        """"""Return the a sample for the element
        """"""
        if self.sample is None:
            return self.get_default_sample()
        return self.sample

    def get_default_sample(self):
        """"""Return default value for the element
        """"""
        return 'my_%s' % self.name","class Sampleable:
    
    def __init__(self):
        
        pass
    def get_sample(self):
        
        pass
    def get_default_sample(self):
        
        pass",snippet_245,4,"import random

class Sampleable:
    def __init__(self):
        pass

    def get_sample(self):
        return random.random()

    def get_default_sample(self):
        return 0.5"
313027,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/service/config.py,apidoc.service.config.Config,"import os
from apidoc.object.config import Config as ConfigObject

class Config:
    """"""Provide tool to managed config
    """"""

    def validate(self, config):
        """"""Validate that the source file is ok
        """"""
        if not isinstance(config, ConfigObject):
            raise Exception('Config object expected')
        if config['output']['componants'] not in ('local', 'remote', 'embedded', 'without'):
            raise ValueError('Unknown componant ""%s"".' % config['output']['componants'])
        if config['output']['layout'] not in ('default', 'content-only'):
            raise ValueError('Unknown layout ""%s"".' % config['output']['layout'])
        if config['input']['locations'] is not None:
            unknown_locations = [x for x in config['input']['locations'] if not os.path.exists(x)]
            if len(unknown_locations) > 0:
                raise ValueError('Location%s ""%s"" does not exists' % ('s' if len(unknown_locations) > 1 else '', '"" and ""'.join(unknown_locations)))
            config['input']['locations'] = [os.path.realpath(x) for x in config['input']['locations']]
        if config['input']['arguments'] is not None:
            if not isinstance(config['input']['arguments'], dict):
                raise ValueError('Sources arguments ""%s"" are not a dict' % config['input']['arguments'])

    def get_template_from_config(self, config):
        """"""Retrieve a template path from the config object
        """"""
        if config['output']['template'] == 'default':
            return os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'template', 'default.html')
        else:
            return os.path.abspath(config['output']['template'])","
class Config:
    
    def validate(self, config):
        
        pass
    def get_template_from_config(self, config):
        
        pass",snippet_246,3,"```python
class Config:
    def validate(self, config):
        if not isinstance(config, dict):
            raise TypeError(""config must be a dictionary"")
        if ""template"" not in config:
            raise KeyError(""config missing required 'template' key"")
        if not isinstance(config[""template""], str):
            raise TypeError(""'template' value must be a string"")
        return True

    def get_template_from_config(self, config):
        self.validate(config)
        return config[""template""]
```"
313029,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/service/merger.py,apidoc.service.merger.Merger,"from apidoc.lib.util.cast import to_boolean

class Merger:
    """"""Provide tool to merge elements
    """"""

    def merge_extends(self, target, extends, inherit_key='inherit', inherit=False):
        """"""Merge extended dicts
        """"""
        if isinstance(target, dict):
            if inherit and inherit_key in target and (not to_boolean(target[inherit_key])):
                return
            if not isinstance(extends, dict):
                raise ValueError('Unable to merge: Dictionnary expected')
            for key in extends:
                if key not in target:
                    target[str(key)] = extends[key]
                else:
                    self.merge_extends(target[key], extends[key], inherit_key, True)
        elif isinstance(target, list):
            if not isinstance(extends, list):
                raise ValueError('Unable to merge: List expected')
            target += extends

    def merge_sources(self, datas):
        """"""Merge sources files
        """"""
        datas = [data for data in datas if data is not None]
        if len(datas) == 0:
            raise ValueError('Data missing')
        if len(datas) == 1:
            return datas[0]
        if isinstance(datas[0], list):
            if len([x for x in datas if not isinstance(x, list)]) > 0:
                raise TypeError('Unable to merge: List expected')
            base = []
            for x in datas:
                base = base + x
            return base
        if isinstance(datas[0], dict):
            if len([x for x in datas if not isinstance(x, dict)]) > 0:
                raise TypeError('Unable to merge: Dictionnary expected')
            result = {}
            for element in datas:
                for key in element:
                    if key in result:
                        result[key] = self.merge_sources([result[key], element[key]])
                    else:
                        result[key] = element[key]
            return result
        if len([x for x in datas if isinstance(x, (dict, list))]) > 0:
            raise TypeError('Unable to merge: List not expected')
        raise ValueError('Unable to merge: Conflict')

    def merge_configs(self, config, datas):
        """"""Merge configs files
        """"""
        if not isinstance(config, dict) or len([x for x in datas if not isinstance(x, dict)]) > 0:
            raise TypeError('Unable to merge: Dictionnary expected')
        for key, value in config.items():
            others = [x[key] for x in datas if key in x]
            if len(others) > 0:
                if isinstance(value, dict):
                    config[key] = self.merge_configs(value, others)
                else:
                    config[key] = others[-1]
        return config","
class Merger:
    
    def merge_extends(self, target, extends, inherit_key='inherit', inherit=False):
        
        pass
    def merge_sources(self, datas):
        
        pass

    def merge_configs(self, config, datas):
        
        pass",snippet_247,4,"```python
class Merger:
    """"""
    Utility class for merging configuration dictionaries.
    """"""

    def merge_extends(self, target, extends, inherit_key='inherit', inherit=False):
        """"""
        Merge the `extends` dictionary into the `target` dictionary.

        Parameters
        ----------
        target : dict
            The dictionary that will receive the merged values.
        extends : dict
            The dictionary whose values will be merged into `target`.
        inherit_key : str, optional
            Key used in `extends` to indicate whether inheritance should occur.
        inherit : bool, optional
            If True, perform the merge. If False, return `target` unchanged.
            If None, the value of `extends[inherit_key]` (if present) is used.

        Returns
        -------
        dict
            The merged dictionary (the same object as `target`).
        """"""
        if not isinstance(target, dict) or not isinstance(extends, dict):
            return target

        # Determine whether to inherit
        if inherit is None:
            inherit = extends.get(inherit_key, False)

        if not inherit:
            return target

        for key, value in extends.items():
            if key == inherit_key:
                continue

            if key in target:
                tgt_val = target[key]
                # Recursive merge for nested dictionaries
                if isinstance(tgt_val, dict) and isinstance(value, dict):
                    target[key] = self.merge_extends(tgt_val, value, inherit_key, inherit=True)
                # Concatenate lists
                elif isinstance(tgt_val, list) and isinstance(value, list):
                    target[key] = tgt_val + value
                # Override other types
                else:
                    target[key] = value
            else:
                target[key] = value

        return target

    def merge_sources(self, datas):
        """"""
        Merge a list of dictionaries into a single dictionary.

        Parameters
        ----------
        datas : list of dict
            The list of dictionaries to merge. Later dictionaries override earlier ones.

        Returns
        -------
        dict
            The merged dictionary.
        """"""
        result = {}
        for data in datas:
            if isinstance(data, dict):
                result = self.merge_extends(result, data, inherit=True)
        return result

    def merge_configs(self, config, datas):
        """"""
        Merge a configuration dictionary with a list of source dictionaries.

        Parameters
        ----------
        config : dict
            The base configuration dictionary.
        datas : list of dict
            The list of source dictionaries to merge into `config`.

        Returns
        -------
        dict
            The merged configuration dictionary.
        """"""
        if not isinstance(config, dict):
            config = {}

        merged_sources = self.merge_sources(datas)
        return self.merge_extends(config, merged_sources, inherit=True)
```"
313030,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/service/parser.py,apidoc.service.parser.Parser,"import os
import yaml
import json

class Parser:
    """"""Provide tools to parse files
    """"""

    def load_from_file(self, file_path, format=None):
        """"""Return dict from a file config
        """"""
        if format is None:
            base_name, file_extension = os.path.splitext(file_path)
            if file_extension in ('.yaml', '.yml'):
                format = 'yaml'
            elif file_extension in '.json':
                format = 'json'
            else:
                raise ValueError('Config file ""%s"" undetermined' % file_extension)
        if format == 'yaml':
            return yaml.load(open(file_path), Loader=yaml.CSafeLoader if yaml.__with_libyaml__ else yaml.SafeLoader)
        elif format == 'json':
            return json.load(open(file_path))
        else:
            raise ValueError('Format ""%s"" unknwon' % format)

    def load_all_from_directory(self, directory_path):
        """"""Return a list of dict from a directory containing files
        """"""
        datas = []
        for root, folders, files in os.walk(directory_path):
            for f in files:
                datas.append(self.load_from_file(os.path.join(root, f)))
        return datas","
class Parser:
    
    def load_from_file(self, file_path, format=None):
        
        pass
    def load_all_from_directory(self, directory_path):
        
        pass",snippet_248,3,"```python
import os
import json
import csv

try:
    import yaml
except ImportError:  # pragma: no cover
    yaml = None


class Parser:
    """"""
    A simple file parser that can load JSON, YAML, and CSV files.
    """"""

    def load_from_file(self, file_path, format=None):
        """"""
        Load and parse a single file.

        Parameters
        ----------
        file_path : str
            Path to the file to load.
        format : str, optional
            Explicit format to use ('json', 'yaml', 'yml', 'csv').
            If None, the format is inferred from the file extension.

        Returns
        -------
        object
            The parsed data structure.

        Raises
        ------
        ValueError
            If the format is unsupported or cannot be inferred.
        FileNotFoundError
            If the file does not exist.
        """"""
        if not os.path.isfile(file_path):
            raise FileNotFoundError(f""File not found: {file_path}"")

        if format is None:
            _, ext = os.path.splitext(file_path)
            ext = ext.lower()
            if ext == "".json"":
                format = ""json""
            elif ext in {"".yaml"", "".yml""}:
                format = ""yaml""
            elif ext == "".csv"":
                format = ""csv""
            else:
                raise ValueError(f""Cannot infer format from extension '{ext}'. ""
                                 f""Specify the 'format' argument."")

        format = format.lower()

        with open(file_path, ""r"", encoding=""utf-8"") as f:
            if format == ""json"":
                return json.load(f)
            elif format in {""yaml"", ""yml""}:
                if yaml is None:
                    raise ImportError(""PyYAML is required for YAML parsing."")
                return yaml.safe_load(f)
            elif format == ""csv"":
                reader = csv.DictReader(f)
                return list(reader)
            else:
                raise ValueError(f""Unsupported format: {format}"")

    def load_all_from_directory(self, directory_path):
        """"""
        Load and parse all supported files in a directory.

        Parameters
        ----------
        directory_path : str
            Path to the directory containing files to load.

        Returns
        -------
        dict
            Mapping from file names to parsed data structures.
        """"""
        if not os.path.isdir(directory_path):
            raise NotADirectoryError(f""Not a directory: {directory_path}"")

        results = {}
        for entry in os.scandir(directory_path):
            if entry.is_file():
                try:
                    data = self.load_from_file(entry.path)
                    results[entry.name] = data
                except Exception:
                    # Skip files that cannot be parsed
                    continue
        return results
```"
313032,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/service/template.py,apidoc.service.template.Template,"import sys
import logging
import shutil
import os

class Template:
    """"""Provide tool to managed templates
    """"""

    def __init__(self):
        """"""Class instantiation
        """"""
        self.input = 'default.html'
        self.output = 'stdout'
        self.env = None

    def render(self, sources, config, out=sys.stdout):
        """"""Render the documentation as defined in config Object
        """"""
        logger = logging.getLogger()
        template = self.env.get_template(self.input)
        output = template.render(sources=sources, layout=config['output']['layout'], config=config['output'])
        if self.output == 'stdout':
            out.write(output)
        else:
            dir = os.path.dirname(self.output)
            if dir and (not os.path.exists(dir)):
                try:
                    os.makedirs(dir)
                except IOError as ioerror:
                    logger.error('Error on creating dir ""{}"": {}'.format(dir, str(ioerror)))
                    return
            if config['output']['template'] == 'default':
                if config['output']['componants'] == 'local':
                    for template_dir in self.env.loader.searchpath:
                        files = (os.path.join(template_dir, 'resource', 'js', 'combined.js'), os.path.join(template_dir, 'resource', 'css', 'combined.css'), os.path.join(template_dir, 'resource', 'font', 'apidoc.eot'), os.path.join(template_dir, 'resource', 'font', 'apidoc.woff'), os.path.join(template_dir, 'resource', 'font', 'apidoc.ttf'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.eot'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.woff'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.ttf'))
                        for file in files:
                            filename = os.path.basename(file)
                            dirname = os.path.basename(os.path.dirname(file))
                            if not os.path.exists(os.path.join(dir, dirname)):
                                os.makedirs(os.path.join(dir, dirname))
                            if os.path.exists(file):
                                shutil.copyfile(file, os.path.join(dir, dirname, filename))
                            else:
                                logger.warn('Missing resource file ""%s"". If you run apidoc in virtualenv, run ""%s""' % (filename, 'python setup.py resources'))
                if config['output']['componants'] == 'remote':
                    for template_dir in self.env.loader.searchpath:
                        files = (os.path.join(template_dir, 'resource', 'js', 'combined.js'), os.path.join(template_dir, 'resource', 'css', 'combined-embedded.css'), os.path.join(template_dir, 'resource', 'font', 'apidoc.eot'), os.path.join(template_dir, 'resource', 'font', 'apidoc.woff'), os.path.join(template_dir, 'resource', 'font', 'apidoc.ttf'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.eot'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.woff'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.ttf'))
                        for file in files:
                            filename = os.path.basename(file)
                            dirname = os.path.basename(os.path.dirname(file))
                            if not os.path.exists(os.path.join(dir, dirname)):
                                os.makedirs(os.path.join(dir, dirname))
                            if os.path.exists(file):
                                shutil.copyfile(file, os.path.join(dir, dirname, filename))
                            else:
                                logger.warn('Missing resource file ""%s"". If you run apidoc in virtualenv, run ""%s""' % (filename, 'python setup.py resources'))
            open(self.output, 'w').write(output)","
class Template:
    
    def __init__(self):
        
        pass
    def render(self, sources, config, out=sys.stdout):
        
        pass",snippet_249,3,"```python
import os
import sys
from string import Template as StrTemplate

class Template:
    def __init__(self):
        pass

    def render(self, sources, config, out=sys.stdout):
        """"""
        Render a template from the given sources using the provided config dictionary.
        `sources` can be a single string (template text or file path) or an iterable of such.
        The rendered output is written to the file-like object `out`.
        """"""
        # Gather template text
        if isinstance(sources, (str, bytes)):
            sources = [sources]
        template_parts = []
        for src in sources:
            if isinstance(src, (bytes, bytearray)):
                template_parts.append(src.decode('utf-8'))
            elif isinstance(src, str):
                if os.path.isfile(src):
                    with open(src, 'r', encoding='utf-8') as f:
                        template_parts.append(f.read())
                else:
                    template_parts.append(src)
            else:
                raise TypeError(f""Unsupported source type: {type(src)}"")
        template_text = ''.join(template_parts)

        # Create a string.Template and substitute
        tmpl = StrTemplate(template_text)
        rendered = tmpl.safe_substitute(config)

        # Write to output
        out.write(rendered)
        if hasattr(out, 'flush'):
            out.flush()
```"
313488,adafruit/Adafruit_CircuitPython_framebuf,adafruit_Adafruit_CircuitPython_framebuf/adafruit_framebuf.py,adafruit_framebuf.MHMSBFormat,"class MHMSBFormat:
    """"""MHMSBFormat""""""

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        """"""Set a given pixel to a color.""""""
        index = (y * framebuf.stride + x) // 8
        offset = 7 - x & 7
        framebuf.buf[index] = framebuf.buf[index] & ~(1 << offset) | (color != 0) << offset

    @staticmethod
    def get_pixel(framebuf, x, y):
        """"""Get the color of a given pixel""""""
        index = (y * framebuf.stride + x) // 8
        offset = 7 - x & 7
        return framebuf.buf[index] >> offset & 1

    @staticmethod
    def fill(framebuf, color):
        """"""completely fill/clear the buffer with a color""""""
        if color:
            fill = 255
        else:
            fill = 0
        for i in range(len(framebuf.buf)):
            framebuf.buf[i] = fill

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        """"""Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.""""""
        for _x in range(x, x + width):
            offset = 7 - _x & 7
            for _y in range(y, y + height):
                index = (_y * framebuf.stride + _x) // 8
                framebuf.buf[index] = framebuf.buf[index] & ~(1 << offset) | (color != 0) << offset","class MHMSBFormat:
    
    @staticmethod
    def set_pixel(framebuf, x, y, color):
        
        pass
    @staticmethod
    def get_pixel(framebuf, x, y):
        
        pass
    @staticmethod
    def fill(framebuf, color):
        
        pass
    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        
        pass",snippet_250,5,"```python
class MHMSBFormat:
    @staticmethod
    def _is_bytearray(buf):
        return isinstance(buf, (bytearray, memoryview))

    @staticmethod
    def _get_dimensions(buf):
        if hasattr(buf, ""width"") and hasattr(buf, ""height""):
            return buf.width, buf.height
        if isinstance(buf, list) and buf:
            return len(buf[0]), len(buf)
        raise ValueError(""Unsupported frame buffer format"")

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        width, height = MHMSBFormat._get_dimensions(framebuf)
        if not (0 <= x < width and 0 <= y < height):
            return
        if MHMSBFormat._is_bytearray(framebuf):
            idx = (y * width + x) // 8
            bit = 7 - ((x % 8))
            if color:
                framebuf[idx] |= (1 << bit)
            else:
                framebuf[idx] &= ~(1 << bit)
        else:
            framebuf[y][x] = 1 if color else 0

    @staticmethod
    def get_pixel(framebuf, x, y):
        width, height = MHMSBFormat._get_dimensions(framebuf)
        if not (0 <= x < width and 0 <= y < height):
            return 0
        if MHMSBFormat._is_bytearray(framebuf):
            idx = (y * width + x) // 8
            bit = 7 - ((x % 8))
            return (framebuf[idx] >> bit) & 1
        else:
            return 1 if framebuf[y][x] else 0

    @staticmethod
    def fill(framebuf, color):
        width, height = MHMSBFormat._get_dimensions(framebuf)
        if MHMSBFormat._is_bytearray(framebuf):
            byte_val = 0xFF if color else 0x00
            framebuf[:] = bytearray([byte_val] * len(framebuf))
        else:
            for y in range(height):
                for x in range(width):
                    framebuf[y][x] = 1 if color else 0

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        buf_width, buf_height = MHMSBFormat._get_dimensions(framebuf)
        # Clamp rectangle to buffer bounds
        x0 = max(0, x)
        y0 = max(0, y)
        x1 = min(buf_width, x + width)
        y1 = min(buf_height, y + height)
        for yy in range(y0, y1):
            for xx in range(x0, x1):
                MHMSBFormat.set_pixel(framebuf, xx, yy, color)
```"
313489,adafruit/Adafruit_CircuitPython_framebuf,adafruit_Adafruit_CircuitPython_framebuf/adafruit_framebuf.py,adafruit_framebuf.MVLSBFormat,"class MVLSBFormat:
    """"""MVLSBFormat""""""

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        """"""Set a given pixel to a color.""""""
        index = (y >> 3) * framebuf.stride + x
        offset = y & 7
        framebuf.buf[index] = framebuf.buf[index] & ~(1 << offset) | (color != 0) << offset

    @staticmethod
    def get_pixel(framebuf, x, y):
        """"""Get the color of a given pixel""""""
        index = (y >> 3) * framebuf.stride + x
        offset = y & 7
        return framebuf.buf[index] >> offset & 1

    @staticmethod
    def fill(framebuf, color):
        """"""completely fill/clear the buffer with a color""""""
        if color:
            fill = 255
        else:
            fill = 0
        for i in range(len(framebuf.buf)):
            framebuf.buf[i] = fill

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        """"""Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.""""""
        while height > 0:
            index = (y >> 3) * framebuf.stride + x
            offset = y & 7
            for w_w in range(width):
                framebuf.buf[index + w_w] = framebuf.buf[index + w_w] & ~(1 << offset) | (color != 0) << offset
            y += 1
            height -= 1","class MVLSBFormat:
    
    @staticmethod
    def set_pixel(framebuf, x, y, color):
        
        pass
    @staticmethod
    def get_pixel(framebuf, x, y):
        
        pass
    @staticmethod
    def fill(framebuf, color):
        
        pass
    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        
        pass",snippet_251,5,"```python
class MVLSBFormat:
    @staticmethod
    def set_pixel(framebuf, x, y, color):
        """"""
        Set a pixel in a 4‑bit per pixel buffer stored as a list of bytearrays.
        Each byte contains two pixels: lower nibble is pixel 0, upper nibble is pixel 1.
        """"""
        color &= 0x0F  # keep only 4 bits
        byte_index = x // 2
        nibble = x % 2
        if nibble == 0:
            # lower nibble
            framebuf[y][byte_index] = (framebuf[y][byte_index] & 0xF0) | color
        else:
            # upper nibble
            framebuf[y][byte_index] = (framebuf[y][byte_index] & 0x0F) | (color << 4)

    @staticmethod
    def get_pixel(framebuf, x, y):
        """"""
        Retrieve a pixel value from the buffer.
        """"""
        byte_index = x // 2
        nibble = x % 2
        byte = framebuf[y][byte_index]
        if nibble == 0:
            return byte & 0x0F
        else:
            return (byte >> 4) & 0x0F

    @staticmethod
    def fill(framebuf, color):
        """"""
        Fill the entire buffer with the given color.
        """"""
        color &= 0x0F
        packed = (color << 4) | color
        for row in framebuf:
            for i in range(len(row)):
                row[i] = packed

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        """"""
        Fill a rectangle area with the given color.
        """"""
        color &= 0x0F
        packed = (color << 4) | color
        for row_index in range(y, y + height):
            row = framebuf[row_index]
            for col_index in range(x, x + width):
                byte_index = col_index // 2
                nibble = col_index % 2
                if nibble == 0:
                    row[byte_index] = (row[byte_index] & 0xF0) | color
                else:
                    row[byte_index] = (row[byte_index] & 0x0F) | (color << 4)
```"
313491,adafruit/Adafruit_CircuitPython_framebuf,adafruit_Adafruit_CircuitPython_framebuf/adafruit_framebuf.py,adafruit_framebuf.RGB888Format,"class RGB888Format:
    """"""RGB888Format""""""

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        """"""Set a given pixel to a color.""""""
        index = (y * framebuf.stride + x) * 3
        if isinstance(color, tuple):
            framebuf.buf[index:index + 3] = bytes(color)
        else:
            framebuf.buf[index:index + 3] = bytes((color >> 16 & 255, color >> 8 & 255, color & 255))

    @staticmethod
    def get_pixel(framebuf, x, y):
        """"""Get the color of a given pixel""""""
        index = (y * framebuf.stride + x) * 3
        return framebuf.buf[index] << 16 | framebuf.buf[index + 1] << 8 | framebuf.buf[index + 2]

    @staticmethod
    def fill(framebuf, color):
        """"""completely fill/clear the buffer with a color""""""
        fill = (color >> 16 & 255, color >> 8 & 255, color & 255)
        for i in range(0, len(framebuf.buf), 3):
            framebuf.buf[i:i + 3] = bytes(fill)

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        """"""Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.""""""
        fill = (color >> 16 & 255, color >> 8 & 255, color & 255)
        for _x in range(x, x + width):
            for _y in range(y, y + height):
                index = (_y * framebuf.stride + _x) * 3
                framebuf.buf[index:index + 3] = bytes(fill)","class RGB888Format:
    
    @staticmethod
    def set_pixel(framebuf, x, y, color):
        
        pass
    @staticmethod
    def get_pixel(framebuf, x, y):
        
        pass
    @staticmethod
    def fill(framebuf, color):
        
        pass
    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        
        pass",snippet_252,5,"```python
class RGB888Format:
    @staticmethod
    def set_pixel(framebuf, x, y, color):
        """"""
        Set the pixel at (x, y) to the given RGB888 color.

        Parameters
        ----------
        framebuf : bytearray or memoryview
            The frame buffer containing the image data. It must have
            attributes `width` and `height` that specify the image
            dimensions.
        x, y : int
            The coordinates of the pixel to set.
        color : tuple or list of 3 ints
            The RGB888 color to write. Each component must be in the
            range 0–255.
        """"""
        width = getattr(framebuf, ""width"", None)
        height = getattr(framebuf, ""height"", None)
        if width is None or height is None:
            raise AttributeError(""framebuf must have 'width' and 'height' attributes"")

        if not (0 <= x < width and 0 <= y < height):
            raise ValueError(""pixel coordinates out of bounds"")

        idx = (y * width + x) * 3
        framebuf[idx] = color[0]
        framebuf[idx + 1] = color[1]
        framebuf[idx + 2] = color[2]

    @staticmethod
    def get_pixel(framebuf, x, y):
        """"""
        Retrieve the RGB888 color of the pixel at (x, y).

        Parameters
        ----------
        framebuf : bytearray or memoryview
            The frame buffer containing the image data. It must have
            attributes `width` and `height` that specify the image
            dimensions.
        x, y : int
            The coordinates of the pixel to read.

        Returns
        -------
        tuple
            The RGB888 color as a tuple of three integers.
        """"""
        width = getattr(framebuf, ""width"", None)
        height = getattr(framebuf, ""height"", None)
        if width is None or height is None:
            raise AttributeError(""framebuf must have 'width' and 'height' attributes"")

        if not (0 <= x < width and 0 <= y < height):
            raise ValueError(""pixel coordinates out of bounds"")

        idx = (y * width + x) * 3
        return (framebuf[idx], framebuf[idx + 1], framebuf[idx + 2])

    @staticmethod
    def fill(framebuf, color):
        """"""
        Fill the entire frame buffer with the given RGB888 color.

        Parameters
        ----------
        framebuf : bytearray or memoryview
            The frame buffer containing the image data. It must have
            attributes `width` and `height` that specify the image
            dimensions.
        color : tuple or list of 3 ints
            The RGB888 color to use for filling.
        """"""
        width = getattr(framebuf, ""width"", None)
        height = getattr(framebuf, ""height"", None)
        if width is None or height is None:
            raise AttributeError(""framebuf must have 'width' and 'height' attributes"")

        r, g, b = color
        pixel_bytes = bytes((r, g, b))
        # Repeat the pixel bytes for the entire buffer
        framebuf[:] = pixel_bytes * (width * height)

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        """"""
        Fill a rectangular region of the frame buffer with the given RGB888 color.

        Parameters
        ----------
        framebuf : bytearray or memoryview
            The frame buffer containing the image data. It must have
            attributes `width` and `height` that specify the image
            dimensions.
        x, y : int
            The top-left corner of the rectangle.
        width, height : int
            The dimensions of the rectangle.
        color : tuple or list of 3 ints
            The RGB888 color to use for filling.
        """"""
        buf_width = getattr(framebuf, ""width"", None)
        buf_height = getattr(framebuf, ""height"", None)
        if buf_width is None or buf_height is None:
            raise AttributeError(""framebuf must have 'width' and 'height' attributes"")

        if x < 0 or y < 0 or x + width > buf_width or y + height > buf_height:
            raise ValueError(""rectangle out of bounds"")

        r, g, b = color
        for row in range(y, y + height):
            base = (row * buf_width + x) * 3
            for col in range(width):
                idx = base + col * 3
                framebuf[idx] = r
                framebuf[idx + 1] = g
                framebuf[idx + 2] = b
```"
319441,pysal/spglm,spglm/varfuncs.py,spglm.varfuncs.VarianceFunction,"import numpy as np

class VarianceFunction:
    """"""
    Relates the variance of a random variable to its mean. Defaults to 1.

    Methods
    -------
    call
        Returns an array of ones that is the same shape as `mu`

    Notes
    -----
    After a variance function is initialized, its call method can be used.

    Alias for VarianceFunction:
    constant = VarianceFunction()

    See also
    --------
    statsmodels.family.family
    """"""

    def __call__(self, mu):
        """"""
        Default variance function

        Parameters
        -----------
        mu : array-like
            mean parameters

        Returns
        -------
        v : array
            ones(mu.shape)
        """"""
        mu = np.asarray(mu)
        return np.ones(mu.shape, np.float64)

    def deriv(self, mu):
        """"""
        Derivative of the variance function v'(mu)
        """"""
        from statsmodels.tools.numdiff import approx_fprime_cs
        return np.diag(approx_fprime_cs(mu, self))","
class VarianceFunction:
    '''
    Relates the variance of a random variable to its mean. Defaults to 1.
    Methods
    -------
    call
        Returns an array of ones that is the same shape as `mu`
    Notes
    -----
    After a variance function is initialized, its call method can be used.
    Alias for VarianceFunction:
    constant = VarianceFunction()
    See also
    --------
    statsmodels.family.family
    '''
    def __call__(self, mu):
        
        pass

    def deriv(self, mu):
        
        pass",snippet_256,3,"```python
import numpy as np

class VarianceFunction:
    '''
    Relates the variance of a random variable to its mean. Defaults to 1.
    Methods
    -------
    call
        Returns an array of ones that is the same shape as `mu`
    Notes
    -----
    After a variance function is initialized, its call method can be used.
    Alias for VarianceFunction:
    constant = VarianceFunction()
    See also
    --------
    statsmodels.family.family
    '''
    def __call__(self, mu):
        return np.ones_like(mu)

    def deriv(self, mu):
        return np.zeros_like(mu)

# Alias
constant = VarianceFunction()
```"
320689,optimizely/python-sdk,optimizely_python-sdk/optimizely/user_profile.py,optimizely.user_profile.UserProfileService,"from typing import Any, Optional

class UserProfileService:
    """""" Class encapsulating user profile service functionality.
  Override with your own implementation for storing and retrieving the user profile. """"""

    def lookup(self, user_id: str) -> dict[str, Any]:
        """""" Fetch the user profile dict corresponding to the user ID.

    Args:
      user_id: ID for user whose profile needs to be retrieved.

    Returns:
      Dict representing the user's profile.
    """"""
        return UserProfile(user_id).__dict__

    def save(self, user_profile: dict[str, Any]) -> None:
        """""" Save the user profile dict sent to this method.

    Args:
      user_profile: Dict representing the user's profile.
    """"""
        pass","
class UserProfileService:
    
    def lookup(self, user_id: str) -> dict[str, Any]:
        
        pass
    def save(self, user_profile: dict[str, Any]) -> None:
        
        pass",snippet_257,3,"```python
from typing import Any, Dict

class UserProfileService:
    _store: Dict[str, Dict[str, Any]] = {}

    def lookup(self, user_id: str) -> Dict[str, Any]:
        if user_id in self._store:
            return self._store[user_id]
        raise KeyError(f""User profile for id '{user_id}' not found"")

    def save(self, user_profile: Dict[str, Any]) -> None:
        if not isinstance(user_profile, dict):
            raise TypeError(""user_profile must be a dict"")
        user_id = user_profile.get(""user_id"")
        if not user_id:
            raise ValueError(""user_profile must contain 'user_id'"")
        self._store[user_id] = user_profile
```"
322746,wilson-eft/wilson,wilson-eft_wilson/wilson/classes.py,wilson.classes.RGsolution,"import numpy as np
from math import log, e

class RGsolution:
    """"""Class representing a continuous (interpolated) solution to the
    SMEFT RGEs to be used for plotting.""""""

    def __init__(self, fun, scale_min, scale_max):
        """"""Initialize.

        Parameters:

        - fun: function of the scale that is expected to return a
        dictionary with the RGE solution and to accept vectorized input.
        - scale_min, scale_max: lower and upper boundaries of the scale
        """"""
        self.fun = fun
        self.scale_min = scale_min
        self.scale_max = scale_max

    def plotdata(self, key, part='re', scale='log', steps=50):
        """"""Return a tuple of arrays x, y that can be fed to plt.plot,
        where x is the scale in GeV and y is the parameter of interest.

        Parameters:

        - key: dicionary key of the parameter to be plotted (e.g. a WCxf
          coefficient name or a SM parameter like 'g')
        - part: plot the real part 're' (default) or the imaginary part 'im'
        - scale: 'log'; make the x steps logarithmically distributed; for
          'linear', linearly distributed
        - steps: steps in x to take (default: 50)
        """"""
        if scale == 'log':
            x = np.logspace(log(self.scale_min), log(self.scale_max), steps, base=e)
        elif scale == 'linear':
            x = np.linspace(self.scale_min, self.scale_max, steps)
        y = self.fun(x)
        y = np.array([d[key] for d in y])
        if part == 're':
            return (x, y.real)
        elif part == 'im':
            return (x, y.imag)

    def plot(self, key, part='re', scale='log', steps=50, legend=True, plotargs={}):
        """"""Plot the RG evolution of parameter `key`.

        Parameters:

        - part, scale, steps: see `plotdata`
        - legend: boolean, show the legend (default: True)
        - plotargs: dictionary of arguments to be passed to plt.plot
        """"""
        try:
            import matplotlib.pyplot as plt
        except ImportError:
            raise ImportError('Please install matplotlib if you want to use the plot method')
        pdat = self.plotdata(key, part=part, scale=scale, steps=steps)
        plt.plot(*pdat, label=key, **plotargs)
        if scale == 'log':
            plt.xscale('log')
        if legend:
            plt.legend()","
class RGsolution:
    
    def __init__(self, fun, scale_min, scale_max):
        
        pass
    def plotdata(self, key, part='re', scale='log', steps=50):
        
        pass

    def plotdata(self, key, part='re', scale='log', steps=50):
        
        pass",snippet_258,4,"```python
import numpy as np
import matplotlib.pyplot as plt

class RGsolution:
    def __init__(self, fun, scale_min, scale_max):
        """"""
        Parameters
        ----------
        fun : callable
            Function that takes a scale value and returns either a scalar,
            a complex number, or a dictionary/array of values.
        scale_min : float
            Minimum scale value.
        scale_max : float
            Maximum scale value.
        """"""
        self.fun = fun
        self.scale_min = scale_min
        self.scale_max = scale_max

    def plotdata(self, key, part='re', scale='log', steps=50):
        """"""
        Plot the data returned by `fun` over the specified scale range.

        Parameters
        ----------
        key : str or int or None
            If the output of `fun` is a dict or array, this selects the
            element to plot. If None, the entire output is plotted.
        part : {'re', 'im', 'abs', 'phase'}, optional
            Which part of the data to plot. Default is 're'.
        scale : {'log', 'linear'}, optional
            Scale of the x-axis. Default is 'log'.
        steps : int, optional
            Number of points in the scale grid. Default is 50.
        """"""
        # Generate scale values
        if scale == 'log':
            scales = np.logspace(np.log10(self.scale_min), np.log10(self.scale_max), steps)
        else:
            scales = np.linspace(self.scale_min, self.scale_max, steps)

        # Evaluate the function
        values = []
        for s in scales:
            val = self.fun(s)
            # If val is a dict or array, extract the requested key/element
            if isinstance(val, dict):
                if key is None:
                    raise ValueError(""Key must be specified for dictionary output."")
                val = val[key]
            elif isinstance(val, (list, tuple, np.ndarray)):
                if key is None:
                    raise ValueError(""Key must be specified for array/list output."")
                val = val[key]
            values.append(val)

        values = np.array(values)

        # Select part of the data
        if np.iscomplexobj(values):
            if part == 're':
                plot_vals = np.real(values)
            elif part == 'im':
                plot_vals = np.imag(values)
            elif part == 'abs':
                plot_vals = np.abs(values)
            elif part == 'phase':
                plot_vals = np.angle(values)
            else:
                raise ValueError(f""Unknown part '{part}'."")
        else:
            plot_vals = values

        # Plot
        plt.figure(figsize=(8, 5))
        plt.plot(scales, plot_vals, marker='o', linestyle='-')
        plt.xscale(scale)
        plt.xlabel('Scale')
        plt.ylabel(f'{key} ({part})')
        plt.title(f'Plot of {key} vs Scale')
        plt.grid(True, which='both', ls='--', lw=0.5)
        plt.tight_layout()
        plt.show()
```"
322803,wilson-eft/wilson,wilson-eft_wilson/wilson/wcxf/classes.py,wilson.wcxf.classes.WCxf,"import yaml

class WCxf:
    """"""Base class for WCxf files (not meant to be used directly).""""""

    @classmethod
    def load(cls, stream, **kwargs):
        """"""Load the object data from a JSON or YAML file.""""""
        wcxf = _load_yaml_json(stream, **kwargs)
        return cls(**wcxf)

    def dump(self, stream=None, fmt='json', **kwargs):
        """"""Dump the object data to a JSON or YAML file.

        Optional arguments:

        - `stream`: if None (default), return a string. Otherwise,
          should be a writable file-like object
        - `fmt`: format, should be 'json' (default) or 'yaml'

        Additional keyword arguments will be passed to the `json.dump(s)`
        or `yaml.dump` methods.
        """"""
        d = {k: v for k, v in self.__dict__.items() if k[0] != '_'}
        if fmt.lower() == 'json':
            indent = kwargs.pop('indent', 2)
            return _dump_json(d, stream=stream, indent=indent, **kwargs)
        elif fmt.lower() == 'yaml':
            default_flow_style = kwargs.pop('default_flow_style', False)
            return yaml.dump(d, stream, default_flow_style=default_flow_style, **kwargs)
        else:
            raise ValueError(f""Format {fmt} unknown: use 'json' or 'yaml'."")","
class WCxf:
    
    @classmethod
    def load(cls, stream, **kwargs):
        
        pass
    def dump(self, stream=None, fmt='json', **kwargs):
        
        pass",snippet_259,3,"```python
import json
import os
from typing import Any, Dict, Optional, Union

# Optional YAML support – fall back to JSON if unavailable
try:
    import yaml  # type: ignore
    _YAML_AVAILABLE = True
except Exception:
    _YAML_AVAILABLE = False


class WCxf:
    """"""
    Minimal implementation of a WCxf (Weakly Coupled Effective Field Theory) container.
    It can load from and dump to JSON or YAML streams.
    """"""

    def __init__(self, data: Optional[Dict[str, Any]] = None):
        self._data: Dict[str, Any] = data or {}

    @classmethod
    def load(
        cls,
        stream: Union[str, os.PathLike, Any],
        fmt: str = ""json"",
        **kwargs: Any,
    ) -> ""WCxf"":
        """"""
        Load a WCxf instance from a stream or file path.

        Parameters
        ----------
        stream : str | PathLike | file-like
            If a string or PathLike, it is treated as a file path.
            Otherwise, it must have a ``read`` method returning a string.
        fmt : str, optional
            Format of the input. Supported: 'json', 'yaml'.
            Default is 'json'.
        **kwargs
            Additional keyword arguments passed to the underlying parser.

        Returns
        -------
        WCxf
            A new instance populated with the parsed data.
        """"""
        # Resolve the actual content string
        if isinstance(stream, (str, os.PathLike)):
            if not os.path.exists(stream):
                raise FileNotFoundError(f""File not found: {stream}"")
            with open(stream, ""r"", encoding=""utf-8"") as f:
                content = f.read()
        else:
            # Assume file-like object
            try:
                content = stream.read()
            except AttributeError as exc:
                raise TypeError(
                    ""stream must be a file path or a file-like object with a read() method""
                ) from exc

        # Parse according to format
        fmt = fmt.lower()
        if fmt == ""json"":
            data = json.loads(content, **kwargs)
        elif fmt in (""yaml"", ""yml""):
            if not _YAML_AVAILABLE:
                raise RuntimeError(
                    ""YAML support requires PyYAML to be installed.""
                )
            data = yaml.safe_load(content)
        else:
            raise ValueError(f""Unsupported format: {fmt!r}"")

        if not isinstance(data, dict):
            raise ValueError(""Parsed data must be a dictionary"")

        return cls(data)

    def dump(
        self,
        stream: Optional[Union[str, os.PathLike, Any]] = None,
        fmt: str = ""json"",
        **kwargs: Any,
    ) -> Optional[str]:
        """"""
        Dump the WCxf instance to a stream or return a string.

        Parameters
        ----------
        stream : str | PathLike | file-like | None, optional
            If provided, the output is written to this stream or file path.
            If None, the serialized string is returned.
        fmt : str, optional
            Output format: 'json' or 'yaml'. Default is 'json'.
        **kwargs
            Additional keyword arguments passed to the serializer.

        Returns
        -------
        str | None
            The serialized string if stream is None; otherwise None.
        """"""
        fmt = fmt.lower()
        if fmt == ""json"":
            serialized = json.dumps(
                self._data,
                indent=kwargs.pop(""indent"", 4),
                sort_keys=kwargs.pop(""sort_keys"", True),
                **kwargs,
            )
        elif fmt in (""yaml"", ""yml""):
            if not _YAML_AVAILABLE:
                raise RuntimeError(
                    ""YAML support requires PyYAML to be installed.""
                )
            # yaml.safe_dump expects a dict; ensure we pass the data
            serialized = yaml.safe_dump(
                self._data,
                default_flow_style=False,
                sort_keys=kwargs.pop(""sort_keys"", True),
                **kwargs,
            )
        else:
            raise ValueError(f""Unsupported format: {fmt!r}"")

        if stream is None:
            return serialized

        # Write to the provided stream
        if isinstance(stream, (str, os.PathLike)):
            with open(stream, ""w"", encoding=""utf-8"") as f:
                f.write(serialized)
        else:
            try:
                stream.write(serialized)
            except AttributeError as exc:
                raise TypeError(
                    ""stream must be a file path or a file-like object with a write() method""
                ) from exc

        return None

    # Convenience methods for attribute access
    def __getitem__(self, key: str) -> Any:
        return self._data[key]

    def __setitem__(self, key: str, value: Any) -> None:
        self._data[key] = value

    def __repr__(self) -> str:
        return f""{self.__class__.__name__}({self._data!r})""
```"
322855,jhermann/rituals,jhermann_rituals/src/rituals/util/antglob.py,rituals.util.antglob.Pattern,"class Pattern:
    """"""A single pattern for either inclusion or exclusion.""""""

    def __init__(self, spec, inclusive):
        """"""Create regex-based pattern matcher from glob `spec`.""""""
        self.compiled = compile_glob(spec.rstrip('/'))
        self.inclusive = inclusive
        self.is_dir = spec.endswith('/')

    def __str__(self):
        """"""Return inclusiveness indicator and original glob pattern.""""""
        return ('+' if self.inclusive else '-') + self.compiled.pattern

    def matches(self, path):
        """"""Check this pattern against given `path`.""""""
        return bool(self.compiled.match(path))","class Pattern:
    
    def __init__(self, spec, inclusive):
        
        pass
    def __str__(self):
        
        pass
    def matches(self, path):
        
        pass",snippet_260,4,"```python
import fnmatch

class Pattern:
    """"""
    A simple pattern matcher that supports glob-style patterns.
    The `inclusive` flag determines whether the pattern is used to include
    or exclude paths.
    """"""

    def __init__(self, spec, inclusive):
        """"""
        Parameters
        ----------
        spec : str
            The glob pattern to match against.
        inclusive : bool
            If True, `matches` returns True when the path matches the pattern.
            If False, `matches` returns True when the path does NOT match the pattern.
        """"""
        self.spec = spec
        self.inclusive = inclusive

    def __str__(self):
        return f""Pattern(spec={self.spec!r}, inclusive={self.inclusive})""

    def matches(self, path):
        """"""
        Check if the given path matches the pattern.

        Parameters
        ----------
        path : str
            The path to test against the pattern.

        Returns
        -------
        bool
            True if the path matches the pattern according to the inclusive flag.
        """"""
        matched = fnmatch.fnmatch(path, self.spec)
        return matched if self.inclusive else not matched
```"
323184,SoftwareDefinedBuildings/XBOS,apps/Data_quality_analysis/Import_Data.py,Data_quality_analysis.Import_Data.Import_Data,"import pandas as pd
import glob
import numpy as np
import os

class Import_Data:
    """""" This class imports data from csv files. """"""

    def __init__(self):
        """""" Constructor: Store the imported data. """"""
        self.data = pd.DataFrame()

    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        """""" Imports csv file(s) and stores the result in data.

        Note
        ----
        1. If folder exists out of current directory, folder_name should contain correct regex
        2. Assuming there's no file called ""\\*.csv""

        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '\\*', i.e. all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.', i.e. current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe

        """"""
        if isinstance(file_name, str) and isinstance(folder_name, str):
            try:
                self.data = self._load_csv(file_name, folder_name, head_row, index_col, convert_col, concat_files)
            except Exception as e:
                raise e
        elif isinstance(file_name, list) and isinstance(folder_name, str):
            for i, file in enumerate(file_name):
                if isinstance(head_row, list):
                    _head_row = head_row[i]
                else:
                    _head_row = head_row
                if isinstance(index_col, list):
                    _index_col = index_col[i]
                else:
                    _index_col = index_col
                try:
                    data_tmp = self._load_csv(file, folder_name, _head_row, _index_col, convert_col, concat_files)
                    if concat_files:
                        self.data = self.data.append(data_tmp, ignore_index=False, verify_integrity=False)
                    else:
                        self.data = self.data.join(data_tmp, how='outer')
                except Exception as e:
                    raise e
        else:
            raise NotImplementedError(""Filename and Folder name can't both be of type list."")

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        """""" Load single csv file.

        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '*' - all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.' - current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe

        Returns
        -------
        pd.DataFrame()
            Dataframe containing csv data

        """"""
        if file_name == '*':
            if not os.path.isdir(folder_name):
                raise OSError('Folder does not exist.')
            else:
                file_name_list = sorted(glob.glob(folder_name + '*.csv'))
                if not file_name_list:
                    raise OSError('Either the folder does not contain any csv files or invalid folder provided.')
                else:
                    self.import_csv(file_name=file_name_list, head_row=head_row, index_col=index_col, convert_col=convert_col, concat_files=concat_files)
                    return self.data
        elif not os.path.isdir(folder_name):
            raise OSError('Folder does not exist.')
        else:
            path = os.path.join(folder_name, file_name)
            if head_row > 0:
                data = pd.read_csv(path, index_col=index_col, skiprows=[i for i in range(head_row - 1)])
            else:
                data = pd.read_csv(path, index_col=index_col)
            try:
                data.index = pd.to_datetime(data.index, format='%m/%d/%y %H:%M')
            except:
                data.index = pd.to_datetime(data.index, dayfirst=False, infer_datetime_format=True)
        if convert_col:
            for col in data.columns:
                if data[col].dtype != np.number:
                    data[col] = pd.to_numeric(data[col], errors='coerce')
        return data","
class Import_Data:
    
    def __init__(self):
        
        pass
    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        
        pass

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        
        pass",snippet_261,4,"```python
import os
import glob
import pandas as pd


class Import_Data:
    def __init__(self):
        pass

    def import_csv(
        self,
        file_name=""*"",
        folder_name=""."",
        head_row=0,
        index_col=0,
        convert_col=True,
        concat_files=False,
    ):
        """"""
        Public method to import CSV files.

        Parameters
        ----------
        file_name : str, optional
            File pattern to match (default '*', meaning all files).
        folder_name : str, optional
            Directory to search for files (default '.').
        head_row : int, optional
            Row number to use as header (default 0).
        index_col : int or str, optional
            Column to set as index (default 0).
        convert_col : bool, optional
            If True, attempt to convert columns to numeric where possible.
        concat_files : bool, optional
            If True, concatenate all matched files into a single DataFrame.

        Returns
        -------
        pd.DataFrame
            The loaded (and possibly concatenated) DataFrame.
        """"""
        return self._load_csv(
            file_name, folder_name, head_row, index_col, convert_col, concat_files
        )

    def _load_csv(
        self,
        file_name,
        folder_name,
        head_row,
        index_col,
        convert_col,
        concat_files,
    ):
        # Resolve the file pattern
        pattern = os.path.join(folder_name, file_name)
        files = glob.glob(pattern)

        if not files:
            raise FileNotFoundError(f""No files matched pattern: {pattern}"")

        dfs = []
        for f in files:
            try:
                df = pd.read_csv(
                    f,
                    header=head_row,
                    index_col=index_col,
                    dtype=str,  # read as string first to allow conversion
                )
            except Exception as e:
                raise ValueError(f""Error reading {f}: {e}"")

            if convert_col:
                # Attempt to convert each column to numeric where possible
                for col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors=""ignore"")

            dfs.append(df)

        if concat_files:
            # Concatenate along rows, aligning columns
            result = pd.concat(dfs, axis=0, ignore_index=False, sort=False)
        else:
            # Return the first DataFrame if not concatenating
            result = dfs[0]

        return result
```"
323187,SoftwareDefinedBuildings/XBOS,apps/Data_quality_analysis/Plot_Data.py,Data_quality_analysis.Plot_Data.Plot_Data,"import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

class Plot_Data:
    """""" This class contains functions for displaying various plots.

    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.

    """"""
    count = 1

    def __init__(self, figsize=(18, 5)):
        """""" Constructor.

        Parameters
        ----------
        figsize : tuple
            Size of figure.

        """"""
        self.figsize = figsize

    def correlation_plot(self, data):
        """""" Create heatmap of Pearson's correlation coefficient.

        Parameters
        ----------
        data    : pd.DataFrame()
            Data to display.

        Returns
        -------
        matplotlib.figure
            Heatmap.

        """"""
        fig = plt.figure(Plot_Data.count)
        corr = data.corr()
        ax = sns.heatmap(corr)
        Plot_Data.count += 1
        return fig

    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        """""" Create baseline and projection plots.

        Parameters
        ----------
        y_true              : pd.Series()
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.Dataframe()
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.

        Returns
        -------
        matplotlib.figure
            Baseline plot

        """"""
        fig = plt.figure(Plot_Data.count)
        if projection_period:
            nrows = len(baseline_period) + len(projection_period) / 2
        else:
            nrows = len(baseline_period) / 2
        base_df = pd.DataFrame()
        base_df['y_true'] = y_true
        base_df['y_pred'] = y_pred
        ax1 = fig.add_subplot(nrows, 1, 1)
        base_df.plot(ax=ax1, figsize=self.figsize, title='Baseline Period ({}-{}). \nBest Model: {}. \nBaseline Adj R2: {}. \nSite: {}.'.format(baseline_period[0], baseline_period[1], model_name, adj_r2, site))
        if projection_period:
            num_plot = 2
            for i in range(0, len(projection_period), 2):
                ax = fig.add_subplot(nrows, 1, num_plot)
                period = slice(projection_period[i], projection_period[i + 1])
                project_df = pd.DataFrame()
                try:
                    project_df['y_true'] = data.loc[period, output_col]
                    project_df['y_pred'] = model.predict(data.loc[period, input_col])
                    project_df['y_pred'][project_df['y_pred'] < 0] = 0
                    project_df.plot(ax=ax, figsize=self.figsize, title='Projection Period ({}-{})'.format(projection_period[i], projection_period[i + 1]))
                    num_plot += 1
                    fig.tight_layout()
                    Plot_Data.count += 1
                    return (fig, project_df['y_true'], project_df['y_pred'])
                except:
                    raise TypeError('If projecting into the future, please specify project_ind_col that has data available                                         in the future time period requested.')
        return (fig, None, None)","
class Plot_Data:
    
    def __init__(self, figsize=(18, 5)):
        
        pass
    def correlation_plot(self, data):
        
        pass

    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        
        pass",snippet_262,4,"```python
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

class Plot_Data:
    def __init__(self, figsize=(18, 5)):
        """"""
        Initialize the Plot_Data object.

        Parameters
        ----------
        figsize : tuple, optional
            Size of the figures to be created. Default is (18, 5).
        """"""
        self.figsize = figsize

    def correlation_plot(self, data):
        """"""
        Plot a heatmap of the correlation matrix of the provided DataFrame.

        Parameters
        ----------
        data : pandas.DataFrame
            DataFrame containing numeric columns to compute correlations.
        """"""
        if not isinstance(data, pd.DataFrame):
            raise TypeError(""`data` must be a pandas DataFrame"")

        corr = data.corr()
        plt.figure(figsize=self.figsize)
        sns.heatmap(
            corr,
            annot=True,
            fmt="".2f"",
            cmap=""coolwarm"",
            square=True,
            cbar_kws={""shrink"": 0.8}
        )
        plt.title(""Correlation Matrix"", fontsize=16)
        plt.tight_layout()
        plt.show()

    def baseline_projection_plot(
        self,
        y_true,
        y_pred,
        baseline_period,
        projection_period,
        model_name,
        adj_r2,
        data,
        input_col,
        output_col,
        model,
        site
    ):
        """"""
        Plot observed vs predicted values with baseline and projection periods shaded.

        Parameters
        ----------
        y_true : array-like or pandas.Series
            Observed target values.
        y_pred : array-like or pandas.Series
            Predicted target values.
        baseline_period : tuple
            (start, end) dates for the baseline period.
        projection_period : tuple
            (start, end) dates for the projection period.
        model_name : str
            Name of the model used.
        adj_r2 : float
            Adjusted R² value of the model.
        data : pandas.DataFrame
            DataFrame containing the time index or a 'date' column.
        input_col : str
            Label for the x‑axis (typically the time variable).
        output_col : str
            Label for the y‑axis (target variable).
        model : object
            The model object (used only for its string representation).
        site : str
            Site or location name.
        """"""
        # Validate inputs
        if not isinstance(data, pd.DataFrame):
            raise TypeError(""`data` must be a pandas DataFrame"")

        # Determine time axis
        if isinstance(data.index, pd.DatetimeIndex):
            time = data.index
        elif ""date"" in data.columns:
            time = pd.to_datetime(data[""date""])
        else:
            # Fallback: use integer index
            time = np.arange(len(y_true))

        # Ensure y_true and y_pred are pandas Series with the same index
        if not isinstance(y_true, pd.Series):
            y_true = pd.Series(y_true, index=time)
        if not isinstance(y_pred, pd.Series):
            y_pred = pd.Series(y_pred, index=time)

        # Create figure
        plt.figure(figsize=self.figsize)

        # Plot observed and predicted
        plt.plot(time, y_true, label=""Observed"", color=""black"", linewidth=2)
        plt.plot(time, y_pred, label=""Predicted"", color=""red"", linestyle=""--"", linewidth=2)

        # Shade baseline period
        baseline_start, baseline_end = baseline_period
        baseline_start_ts = pd.to_datetime(baseline_start)
        baseline_end_ts = pd.to_datetime(baseline_end)
        plt.axvspan(
            baseline_start_ts,
            baseline_end_ts,
            color=""blue"",
            alpha=0.1,
            label=""Baseline Period"",
        )

        # Shade projection period
        proj_start, proj_end = projection_period
        proj_start_ts = pd.to_datetime(proj_start)
        proj_end_ts = pd.to_datetime(proj_end)
        plt.axvspan(
            proj_start_ts,
            proj_end_ts,
            color=""green"",
            alpha=0.1,
            label=""Projection Period"",
        )

        # Title and labels
        title = (
            f""{site} - {model_name} (Adj R² = {adj_r2:.3f})""
            f""\nModel: {model}""
        )
        plt.title(title, fontsize=14, pad=20)
        plt.xlabel(input_col, fontsize=12)
        plt.ylabel(output_col, fontsize=12)

        # Legend
        plt.legend(loc=""upper left"", fontsize=10)

        # Layout and show
        plt.tight_layout()
        plt.show()
```"
323204,SoftwareDefinedBuildings/XBOS,apps/data_analysis/XBOS_data_analytics/Import_Data.py,XBOS_data_analytics.Import_Data.Import_Data,"import os
import pandas as pd
import numpy as np
import glob

class Import_Data:
    """""" This class imports data from csv files """"""

    def __init__(self):
        """""" Constructor.

        This class stores the imported data.

        """"""
        self.data = pd.DataFrame()

    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        """""" Imports csv file(s) and stores the result in data.

        Note
        ----
        1. If folder exists out of current directory, folder_name should contain correct regex
        2. Assuming there's no file called ""\\*.csv""

        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '\\*', i.e. all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.', i.e. current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe

        """"""
        if isinstance(file_name, str) and isinstance(folder_name, str):
            try:
                self.data = self._load_csv(file_name, folder_name, head_row, index_col, convert_col, concat_files)
            except Exception as e:
                raise e
        elif isinstance(file_name, list) and isinstance(folder_name, str):
            for i, file in enumerate(file_name):
                if isinstance(head_row, list):
                    _head_row = head_row[i]
                else:
                    _head_row = head_row
                if isinstance(index_col, list):
                    _index_col = index_col[i]
                else:
                    _index_col = index_col
                try:
                    data_tmp = self._load_csv(file, folder_name, _head_row, _index_col, convert_col, concat_files)
                    if concat_files:
                        self.data = self.data.append(data_tmp, ignore_index=False, verify_integrity=False)
                    else:
                        self.data = self.data.join(data_tmp, how='outer')
                except Exception as e:
                    raise e
        else:
            raise NotImplementedError(""Filename and Folder name can't both be of type list."")

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        """""" Load single csv file.

        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '*' - all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.' - current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe

        Returns
        -------
        pd.DataFrame()
            Dataframe containing csv data

        """"""
        if file_name == '*':
            if not os.path.isdir(folder_name):
                raise OSError('Folder does not exist.')
            else:
                file_name_list = sorted(glob.glob(folder_name + '*.csv'))
                if not file_name_list:
                    raise OSError('Either the folder does not contain any csv files or invalid folder provided.')
                else:
                    self.import_csv(file_name=file_name_list, head_row=head_row, index_col=index_col, convert_col=convert_col, concat_files=concat_files)
                    return self.data
        elif not os.path.isdir(folder_name):
            raise OSError('Folder does not exist.')
        else:
            path = os.path.join(folder_name, file_name)
            if head_row > 0:
                data = pd.read_csv(path, index_col=index_col, skiprows=[i for i in range(head_row - 1)])
            else:
                data = pd.read_csv(path, index_col=index_col)
            try:
                data.index = pd.to_datetime(data.index, format='%m/%d/%y %H:%M')
            except:
                data.index = pd.to_datetime(data.index, dayfirst=False, infer_datetime_format=True)
        if convert_col:
            for col in data.columns:
                if data[col].dtype != np.number:
                    data[col] = pd.to_numeric(data[col], errors='coerce')
        return data","
class Import_Data:
    
    def __init__(self):
        
        pass
    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        
        pass

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        
        pass",snippet_263,4,"```python
import os
import glob
import pandas as pd

class Import_Data:
    def __init__(self):
        self.data = None

    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0,
                   convert_col=True, concat_files=False):
        """"""
        Load CSV file(s) from the specified folder.

        Parameters
        ----------
        file_name : str, optional
            File name or glob pattern. Defaults to '*'.
        folder_name : str, optional
            Folder path. Defaults to current directory.
        head_row : int, optional
            Row number to use as header. Defaults to 0.
        index_col : int, optional
            Column number to use as index. Defaults to 0.
        convert_col : bool, optional
            If True, attempt to convert columns to numeric types. Defaults to True.
        concat_files : bool, optional
            If True, concatenate multiple files into a single DataFrame. Defaults to False.

        Returns
        -------
        pd.DataFrame or None
            Loaded DataFrame or None if no files found.
        """"""
        self.data = self._load_csv(file_name, folder_name, head_row,
                                   index_col, convert_col, concat_files)
        return self.data

    def _load_csv(self, file_name, folder_name, head_row, index_col,
                  convert_col, concat_files):
        # Resolve the full path pattern
        pattern = os.path.join(folder_name, file_name)
        file_list = glob.glob(pattern)

        if not file_list:
            return None

        dfs = []
        for f in file_list:
            try:
                df = pd.read_csv(f, header=head_row, index_col=index_col)
                if convert_col:
                    # Convert each column to numeric if possible
                    for col in df.columns:
                        df[col] = pd.to_numeric(df[col], errors='ignore')
                dfs.append(df)
            except Exception:
                # Skip files that cannot be read
                continue

        if not dfs:
            return None

        if concat_files:
            return pd.concat(dfs, ignore_index=False)
        else:
            # Return the first DataFrame if multiple files but concat not requested
            return dfs[0]
```"
323208,SoftwareDefinedBuildings/XBOS,apps/data_analysis/XBOS_data_analytics/Plot_Data.py,XBOS_data_analytics.Plot_Data.Plot_Data,"import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

class Plot_Data:
    """""" This class contains functions for displaying various plots.

    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.

    """"""
    count = 1

    def __init__(self, figsize=(18, 5)):
        """""" Constructor.

        Parameters
        ----------
        figsize : tuple
            Size of figure.

        """"""
        self.figsize = figsize

    def correlation_plot(self, data):
        """""" Create heatmap of Pearson's correlation coefficient.

        Parameters
        ----------
        data    : pd.DataFrame()
            Data to display.

        Returns
        -------
        matplotlib.figure
            Heatmap.

        """"""
        fig = plt.figure(Plot_Data.count)
        corr = data.corr()
        ax = sns.heatmap(corr)
        Plot_Data.count += 1
        return fig

    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        """""" Create baseline and projection plots.

        Parameters
        ----------
        y_true              : pd.Series()
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.Dataframe()
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.

        Returns
        -------
        matplotlib.figure
            Baseline plot

        """"""
        fig = plt.figure(Plot_Data.count)
        if projection_period:
            nrows = len(baseline_period) + len(projection_period) / 2
        else:
            nrows = len(baseline_period) / 2
        base_df = pd.DataFrame()
        base_df['y_true'] = y_true
        base_df['y_pred'] = y_pred
        ax1 = fig.add_subplot(nrows, 1, 1)
        base_df.plot(ax=ax1, figsize=self.figsize, title='Baseline Period ({}-{}). \nBest Model: {}. \nBaseline Adj R2: {}. \nSite: {}.'.format(baseline_period[0], baseline_period[1], model_name, adj_r2, site))
        if projection_period:
            num_plot = 2
            for i in range(0, len(projection_period), 2):
                ax = fig.add_subplot(nrows, 1, num_plot)
                period = slice(projection_period[i], projection_period[i + 1])
                project_df = pd.DataFrame()
                try:
                    project_df['y_true'] = data.loc[period, output_col]
                    project_df['y_pred'] = model.predict(data.loc[period, input_col])
                    project_df['y_pred'][project_df['y_pred'] < 0] = 0
                    project_df.plot(ax=ax, figsize=self.figsize, title='Projection Period ({}-{})'.format(projection_period[i], projection_period[i + 1]))
                    num_plot += 1
                    fig.tight_layout()
                    Plot_Data.count += 1
                    return (fig, project_df['y_true'], project_df['y_pred'])
                except:
                    raise SystemError('If projecting into the future, please specify project_ind_col that has data available                                         in the future time period requested.')
        return (fig, None, None)","
class Plot_Data:
    
    def __init__(self, figsize=(18, 5)):
        
        pass
    def correlation_plot(self, data):
        
        pass

    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        
        pass",snippet_264,4,"```python
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

class Plot_Data:
    def __init__(self, figsize=(18, 5)):
        """"""
        Initialize the Plot_Data object.

        Parameters
        ----------
        figsize : tuple, optional
            Size of the figure in inches. Default is (18, 5).
        """"""
        self.figsize = figsize

    def correlation_plot(self, data):
        """"""
        Plot a heatmap of the correlation matrix for numeric columns in the DataFrame.

        Parameters
        ----------
        data : pd.DataFrame
            DataFrame containing the data to plot.

        Returns
        -------
        matplotlib.figure.Figure
            The figure containing the correlation heatmap.
        """"""
        if not isinstance(data, pd.DataFrame):
            raise TypeError(""data must be a pandas DataFrame"")

        # Compute correlation matrix for numeric columns only
        corr = data.select_dtypes(include=[np.number]).corr()

        # Create figure
        fig, ax = plt.subplots(figsize=self.figsize)
        sns.heatmap(corr, annot=True, fmt="".2f"", cmap=""coolwarm"", ax=ax,
                    cbar_kws={""shrink"": .8})
        ax.set_title(""Correlation Matrix"", fontsize=16)
        plt.tight_layout()
        return fig

    def baseline_projection_plot(self,
                                 y_true,
                                 y_pred,
                                 baseline_period,
                                 projection_period,
                                 model_name,
                                 adj_r2,
                                 data,
                                 input_col,
                                 output_col,
                                 model,
                                 site):
        """"""
        Plot baseline and projection periods with actual vs predicted values.

        Parameters
        ----------
        y_true : array-like
            True target values.
        y_pred : array-like
            Predicted target values.
        baseline_period : tuple
            (start, end) for baseline period. Can be datetime or index.
        projection_period : tuple
            (start, end) for projection period. Can be datetime or index.
        model_name : str
            Name of the model.
        adj_r2 : float
            Adjusted R² value of the model.
        data : pd.DataFrame
            DataFrame containing the full dataset with a datetime index or a date column.
        input_col : str
            Name of the input column used for predictions.
        output_col : str
            Name of the output column (target).
        model : object
            The fitted model (not used directly in plotting but kept for API consistency).
        site : str
            Site identifier for labeling.

        Returns
        -------
        matplotlib.figure.Figure
            The figure containing the baseline and projection plots.
        """"""
        # Validate inputs
        if not isinstance(data, pd.DataFrame):
            raise TypeError(""data must be a pandas DataFrame"")

        # Ensure data has a datetime index or a date column
        if isinstance(data.index, pd.DatetimeIndex):
            date_col = data.index
        elif ""date"" in data.columns:
            date_col = pd.to_datetime(data[""date""])
        else:
            raise ValueError(""Data must have a datetime index or a 'date' column"")

        # Convert y_true and y_pred to pandas Series with the same index
        y_true_series = pd.Series(y_true, index=date_col)
        y_pred_series = pd.Series(y_pred, index=date_col)

        # Create figure with two subplots
        fig, axes = plt.subplots(2, 1, figsize=self.figsize, sharex=True)

        # Baseline plot
        ax_base = axes[0]
        base_mask = (date_col >= baseline_period[0]) & (date_col <= baseline_period[1])
        ax_base.plot(date_col[base_mask], y_true_series[base_mask], label=""Actual"", color=""black"")
        ax_base.plot(date_col[base_mask], y_pred_series[base_mask], label=""Predicted"", color=""red"", linestyle=""--"")
        ax_base.set_title(f""Baseline Period ({baseline_period[0]} to {baseline_period[1]})"")
        ax_base.set_ylabel(output_col)
        ax_base.legend()
        ax_base.grid(True)

        # Projection plot
        ax_proj = axes[1]
        proj_mask = (date_col >= projection_period[0]) & (date_col <= projection_period[1])
        ax_proj.plot(date_col[proj_mask], y_true_series[proj_mask], label=""Actual"", color=""black"")
        ax_proj.plot(date_col[proj_mask], y_pred_series[proj_mask], label=""Predicted"", color=""red"", linestyle=""--"")
        ax_proj.set_title(f""Projection Period ({projection_period[0]} to {projection_period[1]})"")
        ax_proj.set_ylabel(output_col)
        ax_proj.legend()
        ax_proj.grid(True)

        # Overall title
        fig.suptitle(f""{model_name} at {site} | Adjusted R²: {adj_r2:.3f}"", fontsize=18)

        # X-axis label
        axes[-1].set_xlabel(""Date"")

        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        return fig
```"
339242,pawelad/pymonzo,pawelad_pymonzo/src/pymonzo/client.py,pymonzo.client.MonzoAPI,"from pymonzo.exceptions import MonzoAPIError, NoSettingsFile
from authlib.integrations.base_client import OAuthError
from pymonzo.webhooks import WebhooksResource
from pathlib import Path
from pymonzo.attachments import AttachmentsResource
from json import JSONDecodeError
from pymonzo.transactions import TransactionsResource
from pymonzo.pots import PotsResource
from pymonzo.whoami import WhoAmIResource
from pymonzo.utils import get_authorization_response_url
from urllib.parse import urlparse
from typing import Any, Optional
from pymonzo.balance import BalanceResource
import webbrowser
from pymonzo.feed import FeedResource
from pymonzo.accounts import AccountsResource
from authlib.integrations.httpx_client import OAuth2Client
from pymonzo.settings import PyMonzoSettings

class MonzoAPI:
    """"""Monzo public API client.

    To use it, you need to create a new OAuth client in [Monzo Developer Portal].
    The `Redirect URLs` should be set to `http://localhost:6600/pymonzo` and
    `Confidentiality` should be set to `Confidential` if you'd like to automatically
    refresh the access token when it expires.

    You can now use `Client ID` and `Client secret` in [`pymonzo.MonzoAPI.authorize`][]
    to finish the OAuth 2 'Authorization Code Flow' and get the API access token
    (which is by default saved to disk and refreshed when expired).

    [Monzo Developer Portal]: https://developers.monzo.com/

    Note:
        Monzo API docs: https://docs.monzo.com/
    """"""
    api_url = 'https://api.monzo.com'
    authorization_endpoint = 'https://auth.monzo.com/'
    token_endpoint = 'https://api.monzo.com/oauth2/token'
    settings_path = Path.home() / '.pymonzo'

    def __init__(self, access_token: Optional[str]=None) -> None:
        """"""Initialize Monzo API client and mount all resources.

        It expects [`pymonzo.MonzoAPI.authorize`][] to be called beforehand, so
        it can load the local settings file containing the API access token. You
        can also explicitly pass the `access_token`, but it won't be able to
        automatically refresh it once it expires.

        Arguments:
            access_token: OAuth access token. You can obtain it (and by default, save
                it to disk, so it can refresh automatically) by running
                [`pymonzo.MonzoAPI.authorize`][]. Alternatively, you can get a
                temporary access token from the [Monzo Developer Portal].

                [Monzo Developer Portal]: https://developers.monzo.com/

        Raises:
            NoSettingsFile: When the access token wasn't passed explicitly and the
                settings file couldn't be loaded.

        """"""
        if access_token:
            self._settings = PyMonzoSettings(token={'access_token': access_token})
        else:
            try:
                self._settings = PyMonzoSettings.load_from_disk(self.settings_path)
            except (FileNotFoundError, JSONDecodeError) as e:
                raise NoSettingsFile('No settings file found. You need to either run `MonzoAPI.authorize(client_id, client_secret)` to get the authorization token (and save it to disk), or explicitly pass the `access_token`.') from e
        self.session = OAuth2Client(client_id=self._settings.client_id, client_secret=self._settings.client_secret, token=self._settings.token, authorization_endpoint=self.authorization_endpoint, token_endpoint=self.token_endpoint, token_endpoint_auth_method='client_secret_post', update_token=self._update_token, base_url=self.api_url)
        self.whoami = WhoAmIResource(client=self).whoami
        '\n        Mounted Monzo `whoami` endpoint. For more information see\n        [`pymonzo.whoami.WhoAmIResource.whoami`][].\n        '
        self.accounts = AccountsResource(client=self)
        '\n        Mounted Monzo `accounts` resource. For more information see\n        [`pymonzo.accounts.AccountsResource`][].\n        '
        self.attachments = AttachmentsResource(client=self)
        '\n        Mounted Monzo `attachments` resource. For more information see\n        [`pymonzo.attachments.AttachmentsResource`][].\n        '
        self.balance = BalanceResource(client=self)
        '\n        Mounted Monzo `balance` resource. For more information see\n        [`pymonzo.balance.BalanceResource`][].\n        '
        self.feed = FeedResource(client=self)
        '\n        Mounted Monzo `feed` resource. For more information see\n        [`pymonzo.feed.FeedResource`][].\n        '
        self.pots = PotsResource(client=self)
        '\n        Mounted Monzo `pots` resource. For more information see\n        [`pymonzo.pots.PotsResource`][].\n        '
        self.transactions = TransactionsResource(client=self)
        '\n        Mounted Monzo `transactions` resource. For more information see\n        [`pymonzo.transactions.TransactionsResource`][].\n        '
        self.webhooks = WebhooksResource(client=self)
        '\n        Mounted Monzo `webhooks` resource. For more information see\n        [`pymonzo.webhooks.WebhooksResource`][].\n        '

    @classmethod
    def authorize(cls, client_id: str, client_secret: str, *, save_to_disk: bool=True, redirect_uri: str='http://localhost:6600/pymonzo') -> dict:
        """"""Use OAuth 2 'Authorization Code Flow' to get Monzo API access token.

        By default, it also saves the token to disk, so it can be loaded during
        [`pymonzo.MonzoAPI`][] initialization.

        Note:
            Monzo API docs: https://docs.monzo.com/#authentication

        Arguments:
            client_id: OAuth client ID.
            client_secret: OAuth client secret.
            save_to_disk: Whether to save the token to disk.
            redirect_uri: Redirect URI specified in OAuth client.

        Returns:
            OAuth token.
        """"""
        client = OAuth2Client(client_id=client_id, client_secret=client_secret, redirect_uri=redirect_uri, token_endpoint_auth_method='client_secret_post')
        url, state = client.create_authorization_url(cls.authorization_endpoint)
        print(f'Please visit this URL to authorize: {url}')
        webbrowser.open(url)
        parsed_url = urlparse(redirect_uri)
        assert parsed_url.hostname is not None
        assert parsed_url.port is not None
        authorization_response = get_authorization_response_url(host=parsed_url.hostname, port=parsed_url.port)
        try:
            token = client.fetch_token(url=cls.token_endpoint, authorization_response=authorization_response)
        except (OAuthError, JSONDecodeError) as e:
            raise MonzoAPIError('Error while fetching API access token') from e
        if save_to_disk:
            settings = PyMonzoSettings(client_id=client_id, client_secret=client_secret, token=token)
            settings.save_to_disk(cls.settings_path)
        return token

    def _update_token(self, token: dict, **kwargs: Any) -> None:
        """"""Update settings with refreshed access token and save it to disk.

        Arguments:
            token: OAuth access token.
            **kwargs: Extra kwargs.
        """"""
        self._settings.token = token
        if self.settings_path.exists():
            self._settings.save_to_disk(self.settings_path)","
class MonzoAPI:
    
    def __init__(self, access_token: Optional[str]=None) -> None:
        
        pass
    @classmethod
    def authorize(cls, client_id: str, client_secret: str, *, save_to_disk: bool=True, redirect_uri: str='http://localhost:6600/pymonzo') -> dict:
        
        pass

    def _update_token(self, token: dict, **kwargs: Any) -> None:
        
        pass",snippet_268,4,"```python
import os
import json
import time
import uuid
from typing import Optional, Any, Dict

import requests


class MonzoAPI:
    """"""
    A minimal wrapper around the Monzo OAuth2 flow and token handling.
    """"""

    TOKEN_FILE = os.path.expanduser(""~/.pymonzo/token.json"")
    AUTH_URL = ""https://api.monzo.com/oauth2/authorize""
    TOKEN_URL = ""https://api.monzo.com/oauth2/token""
    DEFAULT_SCOPES = [
        ""account:read"",
        ""transaction:read"",
        ""card:read"",
    ]

    def __init__(self, access_token: Optional[str] = None) -> None:
        """"""
        Initialise the API client.

        If an access_token is supplied it will be used directly.
        Otherwise the client will attempt to load a token from disk.
        """"""
        self.access_token: Optional[str] = access_token
        self.refresh_token: Optional[str] = None
        self.expires_at: Optional[int] = None
        self.token: Optional[Dict[str, Any]] = None

        if not self.access_token:
            self._load_token_from_disk()

        if not self.access_token:
            raise ValueError(
                ""No access token supplied and no token found on disk. ""
                ""Please run MonzoAPI.authorize() first.""
            )

    @classmethod
    def authorize(
        cls,
        client_id: str,
        client_secret: str,
        *,
        save_to_disk: bool = True,
        redirect_uri: str = ""http://localhost:6600/pymonzo"",
    ) -> Dict[str, Any]:
        """"""
        Perform the OAuth2 authorization flow.

        The user is prompted to visit a URL and paste the resulting
        authorization code. The code is exchanged for an access token
        which is returned as a dictionary. If `save_to_disk` is True
        the token is persisted to ~/.pymonzo/token.json.
        """"""
        # Build the authorization URL
        state = uuid.uuid4().hex
        params = {
            ""client_id"": client_id,
            ""response_type"": ""code"",
            ""redirect_uri"": redirect_uri,
            ""scope"": "" "".join(cls.DEFAULT_SCOPES),
            ""state"": state,
        }
        auth_url = f""{cls.AUTH_URL}?{requests.compat.urlencode(params)}""

        print(""Please visit the following URL to authorize the application:"")
        print(auth_url)
        print(""\nAfter authorising, you will be redirected to a URL similar to:"")
        print(f""{redirect_uri}?code=YOUR_CODE&state={state}"")
        print(""\nPaste the 'code' parameter value below:"")

        code = input(""Authorization code: "").strip()
        if not code:
            raise ValueError(""No authorization code provided."")

        # Exchange the code for a token
        data = {
            ""grant_type"": ""authorization_code"",
            ""code"": code,
            ""client_id"": client_id,
            ""client_secret"": client_secret,
            ""redirect_uri"": redirect_uri,
        }
        response = requests.post(cls.TOKEN_URL, data=data)
        if response.status_code != 200:
            raise RuntimeError(
                f""Token request failed: {response.status_code} {response.text}""
            )
        token = response.json()

        # Persist token if requested
        if save_to_disk:
            cls._save_token_to_disk_static(token)

        return token

    def _update_token(self, token: Dict[str, Any], **kwargs: Any) -> None:
        """"""
        Update the client's token information.

        Parameters
        ----------
        token : dict
            The token dictionary returned by the Monzo API.
        kwargs : dict
            Optional keyword arguments. Supported keys:
                - save_to_disk (bool): whether to persist the token.
        """"""
        self.access_token = token.get(""access_token"")
        self.refresh_token = token.get(""refresh_token"")
        self.expires_at = token.get(""expires_at"")
        self.token = token

        if kwargs.get(""save_to_disk"", False):
            self._save_token_to_disk()

    # ------------------------------------------------------------------
    # Internal helpers
    # ------------------------------------------------------------------
    def _load_token_from_disk(self) -> None:
        """"""Load token from the default token file.""""""
        if not os.path.exists(self.TOKEN_FILE):
            return

        try:
            with open(self.TOKEN_FILE, ""r"", encoding=""utf-8"") as f:
                token = json.load(f)
        except Exception:
            return

        self.access_token = token.get(""access_token"")
        self.refresh_token = token.get(""refresh_token"")
        self.expires_at = token.get(""expires_at"")
        self.token = token

    def _save_token"
339313,chaoss/grimoirelab-kingarthur,arthur/tasks.py,arthur.tasks._TaskConfig,"import re
from grimoirelab_toolkit.introspect import find_class_properties

class _TaskConfig:
    """"""Abstract class to store task configuration options.

    This class defines how to store specific task configuration
    arguments such as scheduling or archiving options. It is not
    meant to be instantiated on its own.

    Configuration options must be defined using `property` and `setter`
    decorators. Setters must check whether the given value is valid
    or not. When it is invalid, a `ValueError` exception should be
    raised. The rationale behind this is to use these methods as
    parsers when `from_dict` class method is called. It will create
    a new instance of the subclass passing its properties from a
    dictionary.
    """"""
    KW_ARGS_ERROR_REGEX = re.compile(""^.+ got an unexpected keyword argument '(.+)'$"")

    def to_dict(self):
        """"""Returns a dict with the representation of this task configuration object.""""""
        properties = find_class_properties(self.__class__)
        config = {name: self.__getattribute__(name) for name, _ in properties}
        return config

    @classmethod
    def from_dict(cls, config):
        """"""Create an configuration object from a dictionary.

        Key,value pairs will be used to initialize a task configuration
        object. If 'config' contains invalid configuration parameters
        a `ValueError` exception will be raised.

        :param config: dictionary used to create an instance of this object

        :returns: a task config instance

        :raises ValueError: when an invalid configuration parameter is found
        """"""
        try:
            obj = cls(**config)
        except TypeError as e:
            m = cls.KW_ARGS_ERROR_REGEX.match(str(e))
            if m:
                raise ValueError(""unknown '%s' task config parameter"" % m.group(1))
            else:
                raise e
        else:
            return obj","
class _TaskConfig:
    
    def to_dict(self):
        
        pass
    @classmethod
    def from_dict(cls, config):
        
        pass",snippet_269,3,"```python
class _TaskConfig:
    """"""
    Simple configuration holder for tasks.

    The configuration is stored internally as a plain dictionary.
    """"""

    def __init__(self, config=None):
        """"""
        Create a new configuration instance.

        Parameters
        ----------
        config : dict, optional
            Initial configuration values. If omitted, an empty dictionary is used.
        """"""
        self._config = dict(config or {})

    def to_dict(self):
        """"""
        Return a shallow copy of the configuration dictionary.

        Returns
        -------
        dict
            The configuration data.
        """"""
        return self._config.copy()

    @classmethod
    def from_dict(cls, config):
        """"""
        Create a new configuration instance from a dictionary.

        Parameters
        ----------
        config : dict
            Configuration data.

        Returns
        -------
        _TaskConfig
            A new configuration instance.
        """"""
        return cls(config)
```"
341459,wright-group/WrightTools,wright-group_WrightTools/WrightTools/diagrams/WMEL.py,WrightTools.diagrams.WMEL.Subplot,"import numpy as np

class Subplot:
    """"""Subplot containing WMEL.""""""

    def __init__(self, ax, energies, number_of_interactions=4, title='', title_font_size=16, state_names=None, virtual=[None], state_font_size=14, state_text_buffer=0.5, label_side='left'):
        """"""Subplot.

        Parameters
        ----------
        ax : matplotlib axis
            The axis.
        energies : 1D array-like
            Energies (scaled between 0 and 1)
        number_of_interactions : integer
            Number of interactions in diagram.
        title : string (optional)
            Title of subplot. Default is empty string.
        state_names: list of str (optional)
            list of the names of the states
        virtual: list of ints (optional)
            list of indexes of any vitual energy states
        state_font_size: numtype (optional)
            font size for the state lables
        state_text_buffer: numtype (optional)
            space between the energy level bars and the state labels
        """"""
        self.ax = ax
        self.energies = energies
        self.interactions = number_of_interactions
        self.state_names = state_names
        for i in range(len(self.energies)):
            if i in virtual:
                linestyle = '--'
            else:
                linestyle = '-'
            self.ax.axhline(self.energies[i], color='k', linewidth=2, ls=linestyle, zorder=5)
        if isinstance(state_names, list):
            for i in range(len(self.energies)):
                if label_side == 'left':
                    ax.text(-state_text_buffer, energies[i], state_names[i], fontsize=state_font_size, verticalalignment='center', horizontalalignment='center')
                elif label_side == 'right':
                    ax.text(1 + state_text_buffer, energies[i], state_names[i], fontsize=state_font_size, verticalalignment='center', horizontalalignment='center')
        self.x_pos = np.linspace(0, 1, number_of_interactions)
        self.ax.set_xlim(-0.1, 1.1)
        self.ax.set_ylim(-0.01, 1.01)
        self.ax.axis('off')
        self.ax.set_title(title, fontsize=title_font_size)

    def add_arrow(self, index, between, kind, label='', head_length=10, head_aspect=1, font_size=14, color='k'):
        """"""Add an arrow to the WMEL diagram.

        Parameters
        ----------
        index : integer
            The interaction, or start and stop interaction for the arrow.
        between : 2-element iterable of integers
            The inital and final state of the arrow
        kind : {'ket', 'bra', 'outbra', 'outket'}
            The kind of interaction.
        label : string (optional)
            Interaction label. Default is empty string.
        head_length: number (optional)
            size of arrow head
        font_size : number (optional)
            Label font size. Default is 14.
        color : matplotlib color (optional)
            Arrow color. Default is black.

        Returns
        -------
        [line,arrow_head,text]
        """"""
        if hasattr(index, 'index'):
            x_pos = list(index)
        else:
            x_pos = [index] * 2
        x_pos = [np.linspace(0, 1, self.interactions)[i] for i in x_pos]
        y_pos = [self.energies[between[0]], self.energies[between[1]]]
        arrow_length = self.energies[between[1]] - self.energies[between[0]]
        arrow_end = self.energies[between[1]]
        if arrow_length > 0:
            direction = 1
        elif arrow_length < 0:
            direction = -1
        else:
            raise ValueError('between invalid!')
        length = abs(y_pos[0] - y_pos[1])
        if kind == 'ket':
            line = self.ax.plot(x_pos, y_pos, linestyle='-', color=color, linewidth=2, zorder=9)
        elif kind == 'bra':
            line = self.ax.plot(x_pos, y_pos, linestyle='--', color=color, linewidth=2, zorder=9)
        elif kind == 'out':
            yi = np.linspace(y_pos[0], y_pos[1], 100)
            xi = np.sin((yi - y_pos[0]) * int(1 / length * 20) * 2 * np.pi * length) / 40 + x_pos[0]
            line = self.ax.plot(xi[:-5], yi[:-5], linestyle='-', color=color, linewidth=2, solid_capstyle='butt', zorder=9)
        elif kind == 'outbra':
            yi = np.linspace(y_pos[0], y_pos[1], 100)
            xi = np.sin((yi - y_pos[0]) * int(1 / length * 20) * 2 * np.pi * length) / 40 + x_pos[0]
            counter = 0
            while counter - 13 <= len(yi):
                subyi = yi[counter:counter + 15]
                subxi = xi[counter:counter + 15]
                line = self.ax.plot(subxi[:-5], subyi[:-5], linestyle='-', color=color, linewidth=2, solid_capstyle='butt', zorder=9)
                counter += 13
        else:
            raise ValueError(""kind is not 'ket', 'bra', 'out' or 'outbra'."")
        dx = x_pos[1] - x_pos[0]
        dy = y_pos[1] - y_pos[0]
        xytext = (x_pos[1] - dx * 0.01, y_pos[1] - dy * 0.01)
        annotation = self.ax.annotate('', xy=(x_pos[1], y_pos[1]), xytext=xytext, arrowprops=dict(fc=color, ec=color, shrink=0, headwidth=head_length * head_aspect, headlength=head_length, linewidth=0, zorder=10), size=25)
        text = self.ax.text(np.mean(x_pos), -0.15, label, fontsize=font_size, horizontalalignment='center')
        return (line, annotation.arrow_patch, text)","
class Subplot:
    
    def __init__(self, ax, energies, number_of_interactions=4, title='', title_font_size=16, state_names=None, virtual=[None], state_font_size=14, state_text_buffer=0.5, label_side='left'):
        
        pass
    def add_arrow(self, index, between, kind, label='', head_length=10, head_aspect=1, font_size=14, color='k'):
        
        pass",snippet_271,3,"```python
import numpy as np
import matplotlib.pyplot as plt

class Subplot:
    """"""
    A simple helper class to draw energy level diagrams with arrows between states.
    """"""

    def __init__(
        self,
        ax,
        energies,
        number_of_interactions=4,
        title="""",
        title_font_size=16,
        state_names=None,
        virtual=[None],
        state_font_size=14,
        state_text_buffer=0.5,
        label_side=""left"",
    ):
        """"""
        Parameters
        ----------
        ax : matplotlib.axes.Axes
            The axis on which to draw the diagram.
        energies : list or array-like
            Energy values for each state (in arbitrary units).
        number_of_interactions : int, optional
            Number of interaction levels to display (default 4).
        title : str, optional
            Title of the subplot.
        title_font_size : int, optional
            Font size of the title.
        state_names : list of str, optional
            Names for each state. If None, default names are used.
        virtual : list, optional
            List of virtual states (currently unused).
        state_font_size : int, optional
            Font size for state labels.
        state_text_buffer : float, optional
            Horizontal buffer for state labels.
        label_side : {'left', 'right'}, optional
            Side on which to place state labels.
        """"""
        self.ax = ax
        self.energies = np.asarray(energies, dtype=float)
        self.n_states = len(self.energies)
        self.number_of_interactions = number_of_interactions
        self.title = title
        self.title_font_size = title_font_size
        self.state_names = (
            state_names if state_names is not None else [f""S{i}"" for i in range(self.n_states)]
        )
        self.virtual = virtual
        self.state_font_size = state_font_size
        self.state_text_buffer = state_text_buffer
        self.label_side = label_side

        # Compute y positions (equally spaced)
        self.y_positions = np.arange(self.n_states)

        # Draw horizontal lines for each state
        for i, y in enumerate(self.y_positions):
            self.ax.hlines(
                y,
                xmin=0,
                xmax=1,
                color=""k"",
                linewidth=1.5,
                label=self.state_names[i] if i == 0 else None,
            )

        # Label states
        for i, y in enumerate(self.y_positions):
            if self.label_side == ""left"":
                ha = ""right""
                x = -self.state_text_buffer
            else:
                ha = ""left""
                x = 1 + self.state_text_buffer
            self.ax.text(
                x,
                y,
                self.state_names[i],
                ha=ha,
                va=""center"",
                fontsize=self.state_font_size,
            )

        # Set title
        if self.title:
            self.ax.set_title(self.title, fontsize=self.title_font_size)

        # Set limits and hide ticks
        self.ax.set_xlim(-0.2, 1.2)
        self.ax.set_ylim(-0.5, self.n_states - 0.5)
        self.ax.set_xticks([])
        self.ax.set_yticks([])

        # Store arrows for potential future reference
        self.arrows = []

    def add_arrow(
        self,
        index,
        between,
        kind,
        label="""",
        head_length=10,
        head_aspect=1,
        font_size=14,
        color=""k"",
    ):
        """"""
        Draw an arrow between two states.

        Parameters
        ----------
        index : int
            Index of the starting state.
        between : int
            Index of the ending state.
        kind : str
            Type of transition (currently unused, but can be used to style arrows).
        label : str, optional
            Text label to place near the arrow.
        head_length : float, optional
            Length of the arrow head in points.
        head_aspect : float, optional
            Aspect ratio of the arrow head.
        font_size : int, optional
            Font size of the label.
        color : str, optional
            Color of the arrow.
        """"""
        if not (0 <= index < self.n_states and 0 <= between < self.n_states):
            raise ValueError(""State indices out of range."")

        y_start = self.y_positions[index]
        y_end = self.y_positions[between]

        # Determine arrow direction
        if y_start < y_end:
            # Upward arrow
            xy_start = (0.5, y_start)
            xy_end = (0.5, y_end)
        else:
            # Downward arrow
            xy_start = (0.5, y_start)
            xy_end = (0.5, y_end)

        # Arrow properties
        arrowprops = dict(
            arrowstyle=""->"",
            color=color,
            lw=1.5,
            head_width=head_length * head_aspect / 72.0,  # convert points to inches
            head_length=head_length / 72.0,
            shrinkA=0,
            shrinkB=0,
        )

        # Draw arrow
        self.ax.annotate(
            """",
            xy=xy_end,
            xytext=xy_start,
            arrowprops=arrowprops,
            va=""center"",
            ha=""center"",
        )

        # Add label if provided
        if label:
            mid_y = (y_start + y_end) / 2
            self.ax.text(
                0.5,
                mid_y,
                label,
                fontsize=font_size,
                ha=""center"",
                va=""center"",
                color=color,
            )

        # Store arrow info
        self.arrows.append(
            {
                ""start"": index,
                ""end"": between,
                ""kind"": kind,
                ""label"": label,
                ""color"": color,
            }
        )
```"
341765,peopledoc/django-agnocomplete,peopledoc_django-agnocomplete/agnocomplete/views.py,agnocomplete.views.UserContextFormViewMixin,"class UserContextFormViewMixin:
    """"""
    This mixin is injecting the context variable into the form kwargs
    """"""

    def get_agnocomplete_context(self):
        """"""
        Return the view current user.

        You may want to change this value by overrding this method.
        """"""
        return self.request.user

    def get_form_kwargs(self):
        """"""
        Return the form kwargs.

        This method injects the context variable, defined in
        :meth:`get_agnocomplete_context`. Override this method to adjust it to
        your needs.
        """"""
        data = super().get_form_kwargs()
        data.update({'user': self.get_agnocomplete_context()})
        return data","class UserContextFormViewMixin:
    
    def get_agnocomplete_context(self):
        
        pass
    def get_form_kwargs(self):
        
        pass",snippet_272,3,"```python
class UserContextFormViewMixin:
    """"""
    Mixin that injects the current user into the form context and form kwargs.
    """"""

    def get_agnocomplete_context(self):
        """"""
        Return a dictionary containing the current user.
        This can be used by templates or form widgets that need user‑specific data.
        """"""
        return {""user"": getattr(self, ""request"", None).user}

    def get_form_kwargs(self):
        """"""
        Extend the default form kwargs with the current user.
        """"""
        # Retrieve the base kwargs from the parent class if available
        base_kwargs = {}
        if hasattr(super(), ""get_form_kwargs""):
            base_kwargs = super().get_form_kwargs()
        # Inject the user into the kwargs
        base_kwargs.update({""user"": getattr(self, ""request"", None).user})
        return base_kwargs
```"
343261,danpoland/pyramid-restful-framework,danpoland_pyramid-restful-framework/pyramid_restful/pagination/base.py,pyramid_restful.pagination.base.BasePagination,"class BasePagination:
    """"""
    The base class each Pagination class should implement.
    """"""

    def paginate_query(self, query, request):
        """"""
        :param query: SQLAlchemy ``query``.
        :param request: The request from the view
        :return: The paginated date based on the provided query and request.
        """"""
        raise NotImplementedError('paginate_query() must be implemented.')

    def get_paginated_response(self, data):
        """"""
        :param data: The paginated data.
        :return: A response containing the paginated data.
        """"""
        raise NotImplementedError('get_paginated_response() must be implemented.')","class BasePagination:
    
    def paginate_query(self, query, request):
        
        pass
    def get_paginated_response(self, data):
        
        pass",snippet_273,3,"```python
class BasePagination:
    """"""
    A simple pagination helper that works with SQLAlchemy query objects.
    It expects the `request` to provide `page` and `page_size` parameters
    (either as a dict or an object with a `.get()` method).
    """"""

    def __init__(self, default_page_size=10, max_page_size=100):
        self.default_page_size = default_page_size
        self.max_page_size = max_page_size
        self.total = 0

    def _get_param(self, request, name, default):
        """"""
        Retrieve a parameter from the request. Supports dicts and objects
        with a `.get()` method (e.g., Flask's request.args).
        """"""
        if isinstance(request, dict):
            return request.get(name, default)
        try:
            return request.get(name, default)
        except Exception:
            return default

    def paginate_query(self, query, request):
        """"""
        Apply pagination to a SQLAlchemy query and return the paginated
        results as a list. The total number of items is stored in
        `self.total` for later use in the response.
        """"""
        # Extract pagination parameters
        page = self._get_param(request, ""page"", 1)
        page_size = self._get_param(request, ""page_size"", self.default_page_size)

        # Ensure numeric values and enforce bounds
        try:
            page = int(page)
            if page < 1:
                page = 1
        except Exception:
            page = 1

        try:
            page_size = int(page_size)
            if page_size < 1:
                page_size = self.default_page_size
        except Exception:
            page_size = self.default_page_size

        if page_size > self.max_page_size:
            page_size = self.max_page_size

        # Compute offset and limit
        offset = (page - 1) * page_size

        # Count total items before applying limit/offset
        # Remove ordering to avoid unnecessary overhead
        count_query = query.order_by(None)
        self.total = count_query.count()

        # Apply pagination
        paginated_query = query.limit(page_size).offset(offset)

        # Execute and return results
        return paginated_query.all()

    def get_paginated_response(self, data):
        """"""
        Build a standard paginated response dictionary.
        """"""
        return {
            ""count"": self.total,
            ""results"": data,
        }
```"
343270,danpoland/pyramid-restful-framework,danpoland_pyramid-restful-framework/pyramid_restful/permissions.py,pyramid_restful.permissions.BasePermission,"class BasePermission:
    """"""
    All permission classes should inherit from this class.
    """"""
    message = None

    def has_permission(self, request, view):
        """"""
        Checked on every request to a view. Return ``True`` if permission is granted else ``False``.

        :param request: The request sent to the view.
        :param view: The instance of the view being accessed.
        :return: Boolean
        """"""
        return True

    def has_object_permission(self, request, view, obj):
        """"""
        Checked when a request is for a specific object. Return ``True`` if permission is granted else ``False``.

        :param request: The request sent to the view.
        :param view:  The instance of the view being accessed.
        :param obj: The object being accessed.
        :return: Boolean
        """"""
        return True","class BasePermission:
    
    def has_permission(self, request, view):
        
        pass
    def has_object_permission(self, request, view, obj):
        
        pass",snippet_274,3,"```python
class BasePermission:
    """"""
    Base class for permission checks. Subclasses should override
    `has_permission` and/or `has_object_permission` to implement
    custom logic.
    """"""

    def has_permission(self, request, view):
        """"""
        Return True if the request should be permitted globally.
        """"""
        return True

    def has_object_permission(self, request, view, obj):
        """"""
        Return True if the request should be permitted for the given object.
        """"""
        return True
```"
344249,openid/JWTConnect-Python-CryptoJWT,src/cryptojwt/jws/__init__.py,cryptojwt.jws.Signer,"class Signer:
    """"""Abstract base class for signing algorithms.""""""

    def sign(self, msg, key):
        """"""Sign ``msg`` with ``key`` and return the signature.""""""
        raise NotImplementedError()

    def verify(self, msg, sig, key):
        """"""Return True if ``sig`` is a valid signature for ``msg``.""""""
        raise NotImplementedError()","class Signer:
    
    def sign(self, msg, key):
        
        pass
    def verify(self, msg, sig, key):
        
        pass",snippet_275,3,"```python
import hmac
import hashlib
import base64

class Signer:
    def sign(self, msg, key):
        if isinstance(msg, str):
            msg = msg.encode()
        if isinstance(key, str):
            key = key.encode()
        digest = hmac.new(key, msg, hashlib.sha256).digest()
        return base64.b64encode(digest).decode()

    def verify(self, msg, sig, key):
        if isinstance(msg, str):
            msg = msg.encode()
        if isinstance(key, str):
            key = key.encode()
        expected = hmac.new(key, msg, hashlib.sha256).digest()
        try:
            sig_bytes = base64.b64decode(sig)
        except Exception:
            return False
        return hmac.compare_digest(expected, sig_bytes)
```"
347062,adobe-apiplatform/umapi-client.py,umapi_client/api.py,umapi_client.api.QuerySingle,"class QuerySingle:
    """"""
    Look for a single object
    """"""

    def __init__(self, connection, object_type, url_params=None, query_params=None):
        """"""
        Provide the connection and query parameters when you create the query.

        :param connection: The Connection to run the query against
        :param object_type: The type of object being queried (e.g., ""user"" or ""group"")
        :param url_params: Query qualifiers that go in the URL path (e.g., a group name when querying users)
        :param query_params: Query qualifiers that go in the query string (e.g., a domain name)
        """"""
        self.conn = connection
        self.object_type = object_type
        self.url_params = url_params if url_params else []
        self.query_params = query_params if query_params else {}
        self._result = None

    def reload(self):
        """"""
        Rerun the query (lazily).
        The result will contain a value on the server side that have changed since the last run.
        :return: None
        """"""
        self._result = None

    def _fetch_result(self):
        """"""
        Fetch the queried object.
        """"""
        self._result = self.conn.query_single(self.object_type, self.url_params, self.query_params)

    def result(self):
        """"""
        Fetch the result, if we haven't already or if reload has been called.
        :return: the result object of the query.
        """"""
        if self._result is None:
            self._fetch_result()
        return self._result","class QuerySingle:
    
    def __init__(self, connection, object_type, url_params=None, query_params=None):
        
        pass
    def reload(self):
        
        pass
    def _fetch_result(self):
        
        pass

    def result(self):
        
        pass",snippet_276,5,"class QuerySingle:
    def __init__(self, connection, object_type, url_params=None, query_params=None):
        """"""
        Initialize a QuerySingle instance.

        :param connection: An object that provides a `get` method for HTTP requests.
        :param object_type: Either a class to instantiate with the fetched data or a string key.
        :param url_params: Optional dict or string to extend the URL path.
        :param query_params: Optional dict of query string parameters.
        """"""
        self.connection = connection
        self.object_type = object_type
        self.url_params = url_params
        self.query_params = query_params or {}
        self._result = None

    def reload(self):
        """"""
        Force a reload of the data from the remote source.
        """"""
        self._result = None
        return self._fetch_result()

    def _fetch_result(self):
        """"""
        Internal method to perform the HTTP GET request and parse the result.
        """"""
        # Build the base path
        if isinstance(self.object_type, str):
            path = f""/{self.object_type}""
        else:
            # Assume the class has a __name__ attribute
            path = f""/{self.object_type.__name__.lower()}""

        # Append URL parameters if provided
        if self.url_params:
            if isinstance(self.url_params, dict):
                # Append each value in order of insertion
                path += ""/"" + ""/"".join(str(v) for v in self.url_params.values())
            else:
                # Treat as a string path segment
                path += f""/{self.url_params}""

        # Perform the GET request
        try:
            response = self.connection.get(path, params=self.query_params)
        except Exception as exc:
            raise RuntimeError(f""Failed to fetch data from {path}: {exc}"") from exc

        # Raise for HTTP errors
        if hasattr(response, ""raise_for_status""):
            try:
                response.raise_for_status()
            except Exception as exc:
                raise RuntimeError(f""HTTP error while fetching {path}: {exc}"") from exc

        # Parse JSON
        try:
            data = response.json()
        except Exception as exc:
            raise ValueError(f""Response from {path} is not valid JSON: {exc}"") from exc

        # Instantiate object_type if it's a class
        if isinstance(self.object_type, type):
            try:
                if isinstance(data, dict):
                    result = self.object_type(**data)
                else:
                    # If data is not a dict, pass it as a single positional argument
                    result = self.object_type(data)
            except Exception as exc:
                raise ValueError(f""Failed to instantiate {self.object_type} with data: {exc}"") from exc
        else:
            # If object_type is not a class, just return the raw data
            result = data

        self._result = result
        return result

    def result(self):
        """"""
        Return the fetched result, fetching it if necessary.
        """"""
        if self._result is None:
            self._fetch_result()
        return self._result
"
347189,facelessuser/backrefs,facelessuser_backrefs/backrefs/util.py,backrefs.util.Immutable,"from typing import Any, Callable, AnyStr

class Immutable:
    """"""Immutable.""""""
    __slots__: tuple[Any, ...] = ()

    def __init__(self, **kwargs: Any) -> None:
        """"""Initialize.""""""
        for k, v in kwargs.items():
            super().__setattr__(k, v)

    def __setattr__(self, name: str, value: Any) -> None:
        """"""Prevent mutability.""""""
        raise AttributeError('Class is immutable!')","
class Immutable:
    
    def __init__(self, **kwargs: Any) -> None:
        
        pass
    def __setattr__(self, name: str, value: Any) -> None:
        
        pass",snippet_277,3,"```python
from typing import Any

class Immutable:
    def __init__(self, **kwargs: Any) -> None:
        # Set all provided attributes
        for key, value in kwargs.items():
            object.__setattr__(self, key, value)
        # Mark the instance as frozen
        object.__setattr__(self, ""_frozen"", True)

    def __setattr__(self, name: str, value: Any) -> None:
        # Allow setting attributes only before the instance is frozen
        if getattr(self, ""_frozen"", False):
            raise AttributeError(f""Cannot modify immutable instance: {name}"")
        object.__setattr__(self, name, value)
```"
347503,inveniosoftware/invenio-records,inveniosoftware_invenio-records/invenio_records/dumpers/base.py,invenio_records.dumpers.base.Dumper,"from copy import deepcopy

class Dumper:
    """"""Interface for dumpers.""""""

    def dump(self, record, data):
        """"""Dump a record that can be used a source document for the search engine.

        The job of this method is to create a Python dictionary from the record
        provided in the argument.

        If you overwrite this method without calling super, then you should
        ensure that you make a deep copy of the record dictionary, to avoid
        that changes to the dump affects the record.

        :param record: The record to dump.
        :param data: The initial dump data passed in by ``record.dumps()``.
        """"""
        data.update(deepcopy(dict(record)))
        return data

    def load(self, data, record_cls):
        """"""Load a record from the source document of a search engine hit.

        The job of this method, is to create a record of type ``record_cls``
        based on the input ``data``.

        :param data: A Python dictionary representing the data to load.
        :param records_cls: The record class to be constructed.
        :returns: A instance of ``record_cls``.
        """"""
        raise NotImplementedError()","
class Dumper:
    
    def dump(self, record, data):
        
        pass
    def load(self, data, record_cls):
        
        pass",snippet_278,3,"```python
class Dumper:
    def dump(self, record, data):
        """"""
        Populate the provided `data` dictionary with the attributes of `record`.
        If the record has a `to_dict` method, it will be used; otherwise the
        instance's `__dict__` is copied.
        """"""
        if hasattr(record, ""to_dict"") and callable(record.to_dict):
            data.update(record.to_dict())
        else:
            # Use the instance's __dict__ (works for dataclasses and plain objects)
            data.update(vars(record))

    def load(self, data, record_cls):
        """"""
        Create an instance of `record_cls` from the provided `data` dictionary.
        If the class has a `from_dict` classmethod, it will be used; otherwise
        the class is instantiated with keyword arguments.
        """"""
        if hasattr(record_cls, ""from_dict"") and callable(record_cls.from_dict):
            return record_cls.from_dict(data)
        return record_cls(**data)
```"
347507,inveniosoftware/invenio-records,inveniosoftware_invenio-records/invenio_records/dumpers/search.py,invenio_records.dumpers.search.SearchDumperExt,"class SearchDumperExt:
    """"""Interface for Search dumper extensions.""""""

    def dump(self, record, data):
        """"""Dump the data.""""""

    def load(self, data, record_cls):
        """"""Load the data.

        Reverse the changes made by the dump method.
        """"""","class SearchDumperExt:
    
    def dump(self, record, data):
        
        pass
    def load(self, data, record_cls):
        
        pass",snippet_279,3,"```python
from dataclasses import is_dataclass, asdict
from typing import Any, Dict, Type


class SearchDumperExt:
    """"""
    Utility class for serialising (dumping) and deserialising (loading) objects.
    Works with plain objects, dataclasses and simple dictionaries.
    """"""

    def dump(self, record: Any, data: Dict[str, Any]) -> None:
        """"""
        Serialise the attributes of *record* into the *data* dictionary.

        Parameters
        ----------
        record : Any
            The object to serialise. It can be a dataclass instance, a plain
            object with a ``__dict__`` or a dictionary.
        data : dict
            The dictionary that will receive the serialised data. It is
            modified in‑place.

        Notes
        -----
        * Private attributes (starting with an underscore) and callables are
          ignored.
        * If *record* is a dictionary, its items are merged into *data*.
        """"""
        if record is None:
            return

        # If the record is already a dictionary, merge it directly.
        if isinstance(record, dict):
            data.update(record)
            return

        # If the record is a dataclass, use asdict for a clean conversion.
        if is_dataclass(record):
            data.update(asdict(record))
            return

        # Fallback: iterate over the instance's __dict__.
        for key, value in vars(record).items():
            if key.startswith(""_"") or callable(value):
                continue
            data[key] = value

    def load(self, data: Dict[str, Any], record_cls: Type[Any]) -> Any:
        """"""
        Deserialise *data* into an instance of *record_cls*.

        Parameters
        ----------
        data : dict
            The dictionary containing the serialised data.
        record_cls : type
            The class to instantiate. It can be a dataclass or any class
            that accepts keyword arguments matching the keys in *data*.

        Returns
        -------
        Any
            An instance of *record_cls* populated with the values from *data*.
            If *record_cls* cannot be instantiated with ``**data`` a fallback
            approach is used: the class is instantiated without arguments and
            attributes are set individually.

        Notes
        -----
        * If *data* is ``None`` or empty, ``None`` is returned.
        """"""
        if not data:
            return None

        # Try to instantiate directly with keyword arguments.
        try:
            return record_cls(**data)
        except TypeError:
            # Fallback: create an empty instance and set attributes manually.
            obj = record_cls()
            for key, value in data.items():
                setattr(obj, key, value)
            return obj
```"
347521,inveniosoftware/invenio-records,inveniosoftware_invenio-records/invenio_records/systemfields/base.py,invenio_records.systemfields.base.SystemFieldContext,"class SystemFieldContext:
    """"""Base class for a system field context.

    A system field context is created once you access a field's attribute on
    a class. As the system field may be defined on a super class, this context
    allows us to know from which class the field was accessed.

    Normally you should subclass this class, and implement methods the methods
    on it that requires you to know the record class.
    """"""

    def __init__(self, field, record_cls):
        """"""Initialise the field context.""""""
        self._field = field
        self._record_cls = record_cls

    @property
    def field(self):
        """"""Access the field to prevent it from being overwritten.""""""
        return self._field

    @property
    def record_cls(self):
        """"""Record class to prevent it from being overwritten.""""""
        return self._record_cls","class SystemFieldContext:
    
    def __init__(self, field, record_cls):
        
        pass
    @property
    def field(self):
        
        pass
    @property
    def record_cls(self):
        
        pass",snippet_280,4,"class SystemFieldContext:
    
    def __init__(self, field, record_cls):
        self._field = field
        self._record_cls = record_cls

    @property
    def field(self):
        return self._field

    @property
    def record_cls(self):
        return self._record_cls"
348074,edx/edx-val,edx_edx-val/edxval/transcript_utils.py,edxval.transcript_utils.Transcript,"import json
from pysrt import SubRipFile, SubRipItem, SubRipTime
from edxval.exceptions import TranscriptsGenerationException
from pysrt.srtexc import Error

class Transcript:
    """"""
    Container for transcript methods.
    """"""
    SRT = 'srt'
    SJSON = 'sjson'

    @staticmethod
    def generate_sjson_from_srt(srt_subs):
        """"""
        Generate transcripts from sjson to SubRip (*.srt).

        Arguments:
            srt_subs(SubRip): ""SRT"" subs object

        Returns:
            Subs converted to ""SJSON"" format.
        """"""
        sub_starts = []
        sub_ends = []
        sub_texts = []
        for sub in srt_subs:
            sub_starts.append(sub.start.ordinal)
            sub_ends.append(sub.end.ordinal)
            sub_texts.append(sub.text.replace('\n', ' '))
        sjson_subs = {'start': sub_starts, 'end': sub_ends, 'text': sub_texts}
        return sjson_subs

    @staticmethod
    def generate_srt_from_sjson(sjson_subs):
        """"""
        Generate transcripts from sjson to SubRip (*.srt)

        Arguments:
            sjson_subs (dict): `sjson` subs.

        Returns:
            Subtitles in SRT format.
        """"""
        output = ''
        equal_len = len(sjson_subs['start']) == len(sjson_subs['end']) == len(sjson_subs['text'])
        if not equal_len:
            return output
        for i in range(len(sjson_subs['start'])):
            item = SubRipItem(index=i, start=SubRipTime(milliseconds=sjson_subs['start'][i]), end=SubRipTime(milliseconds=sjson_subs['end'][i]), text=sjson_subs['text'][i])
            output += str(item)
            output += '\n'
        return output

    @classmethod
    def convert(cls, content, input_format, output_format):
        """"""
        Convert transcript `content` from `input_format` to `output_format`.

        Arguments:
            content: Transcript content byte-stream.
            input_format: Input transcript format.
            output_format: Output transcript format.

        Accepted input formats: sjson, srt.
        Accepted output format: srt, sjson.

        Raises:
            TranscriptsGenerationException: On parsing the invalid srt
            content during conversion from srt to sjson.
        """"""
        assert input_format in ('srt', 'sjson')
        assert output_format in ('srt', 'sjson')
        try:
            content = content.decode('utf-8-sig')
        except UnicodeDecodeError:
            content = content.decode('latin-1')
        if input_format == output_format:
            return content
        if input_format == 'srt':
            if output_format == 'sjson':
                try:
                    srt_subs = SubRipFile.from_string(content, error_handling=SubRipFile.ERROR_RAISE)
                except Error as ex:
                    raise TranscriptsGenerationException(str(ex)) from ex
                return json.dumps(cls.generate_sjson_from_srt(srt_subs))
        if input_format == 'sjson':
            if output_format == 'srt':
                return cls.generate_srt_from_sjson(json.loads(content))","
class Transcript:
    
    @staticmethod
    def generate_sjson_from_srt(srt_subs):
        
        pass
    @staticmethod
    def generate_srt_from_sjson(sjson_subs):
        
        pass
    @classmethod
    def convert(cls, content, input_format, output_format):
        
        pass",snippet_282,4,"```python
import json
import re
from typing import List, Dict, Any


class Transcript:
    @staticmethod
    def generate_sjson_from_srt(srt_subs: str) -> str:
        """"""
        Convert SRT subtitle string to SJSON (JSON list of subtitle objects).
        Each subtitle object contains:
            - start: start time string ""HH:MM:SS,mmm""
            - end:   end time string ""HH:MM:SS,mmm""
            - text:  subtitle text (single string with newlines preserved)
        """"""
        # Normalize line endings
        srt_subs = srt_subs.replace('\r\n', '\n').replace('\r', '\n')
        # Split into blocks separated by blank lines
        blocks = [b.strip() for b in srt_subs.split('\n\n') if b.strip()]
        subs: List[Dict[str, Any]] = []

        time_pattern = re.compile(r'(\d{2}:\d{2}:\d{2},\d{3})\s*-->\s*(\d{2}:\d{2}:\d{2},\d{3})')

        for block in blocks:
            lines = block.split('\n')
            if len(lines) < 3:
                continue  # Not a valid subtitle block
            # First line is index (ignored)
            # Second line is time
            time_match = time_pattern.match(lines[1].strip())
            if not time_match:
                continue  # Invalid time format
            start, end = time_match.groups()
            # Remaining lines are text
            text = '\n'.join(lines[2:])
            subs.append({'start': start, 'end': end, 'text': text})

        return json.dumps(subs, ensure_ascii=False, indent=2)

    @staticmethod
    def generate_srt_from_sjson(sjson_subs: List[Dict[str, Any]]) -> str:
        """"""
        Convert SJSON (list of subtitle objects) to SRT subtitle string.
        Each subtitle object must contain 'start', 'end', and 'text'.
        """"""
        srt_blocks = []
        for idx, sub in enumerate(sjson_subs, start=1):
            start = sub.get('start')
            end = sub.get('end')
            text = sub.get('text', '')
            if not start or not end:
                continue  # Skip invalid entries
            block = f""{idx}\n{start} --> {end}\n{text}""
            srt_blocks.append(block)
        return '\n\n'.join(srt_blocks) + '\n'

    @classmethod
    def convert(cls, content: str, input_format: str, output_format: str) -> str:
        """"""
        Convert subtitle content between SRT and SJSON formats.
        Supported formats: 'srt', 'sjson'.
        """"""
        input_format = input_format.lower()
        output_format = output_format.lower()

        if input_format == output_format:
            return content  # No conversion needed

        if input_format == 'srt':
            # Parse SRT to SJSON
            sjson = json.loads(cls.generate_sjson_from_srt(content))
            if output_format == 'sjson':
                return json.dumps(sjson, ensure_ascii=False, indent=2)
            elif output_format == 'srt':
                return cls.generate_srt_from_sjson(sjson)
            else:
                raise ValueError(f""Unsupported output format: {output_format}"")

        elif input_format == 'sjson':
            # Parse SJSON to Python list
            try:
                subs = json.loads(content)
            except json.JSONDecodeError as e:
                raise ValueError(""Invalid JSON content"") from e
            if output_format == 'srt':
                return cls.generate_srt_from_sjson(subs)
            elif output_format == 'sjson':
                return json.dumps(subs, ensure_ascii=False, indent=2)
            else:
                raise ValueError(f""Unsupported output format: {output_format}"")

        else:
            raise ValueError(f""Unsupported input format: {input_format}"")
```"
350782,druids/django-chamber,druids_django-chamber/chamber/commands/__init__.py,chamber.commands.ProgressBarStream,"class ProgressBarStream:
    """"""
    OutputStream wrapper to remove default linebreak at line endings.
    """"""

    def __init__(self, stream):
        """"""
        Wrap the given stream.
        """"""
        self.stream = stream

    def write(self, *args, **kwargs):
        """"""
        Call the stream's write method without linebreaks at line endings.
        """"""
        return self.stream.write(*args, ending='', **kwargs)

    def flush(self):
        """"""
        Call the stream's flush method without any extra arguments.
        """"""
        return self.stream.flush()","class ProgressBarStream:
    
    def __init__(self, stream):
        
        pass
    def write(self, *args, **kwargs):
        
        pass
    def flush(self):
        
        pass",snippet_283,4,"class ProgressBarStream:
    def __init__(self, stream):
        self.stream = stream

    def write(self, *args, **kwargs):
        return self.stream.write(*args, **kwargs)

    def flush(self):
        return self.stream.flush()"
350920,pytroll/trollimage,pytroll_trollimage/trollimage/_xrimage_rasterio.py,trollimage._xrimage_rasterio.RIODataset,"from rasterio.enums import Resampling
from rasterio.windows import Window

class RIODataset:
    """"""A wrapper for a rasterio dataset.""""""

    def __init__(self, rfile, overviews=None, overviews_resampling=None, overviews_minsize=256):
        """"""Init the rasterio dataset.""""""
        self.rfile = rfile
        self.overviews = overviews
        if overviews_resampling is None:
            overviews_resampling = 'nearest'
        self.overviews_resampling = Resampling[overviews_resampling]
        self.overviews_minsize = overviews_minsize

    def __setitem__(self, key, item):
        """"""Put the data chunk in the image.""""""
        if len(key) == 3:
            indexes = list(range(key[0].start + 1, key[0].stop + 1, key[0].step or 1))
            y = key[1]
            x = key[2]
        else:
            indexes = 1
            y = key[0]
            x = key[1]
        chy_off = y.start
        chy = y.stop - y.start
        chx_off = x.start
        chx = x.stop - x.start
        self.rfile.write(item, window=Window(chx_off, chy_off, chx, chy), indexes=indexes)

    def close(self):
        """"""Close the file.""""""
        if self.overviews is not None:
            overviews = self.overviews
            if len(overviews) == 0:
                from rasterio.rio.overview import get_maximum_overview_level
                width = self.rfile.width
                height = self.rfile.height
                max_level = get_maximum_overview_level(width, height, self.overviews_minsize)
                overviews = [2 ** j for j in range(1, max_level + 1)]
            logger.debug('Building overviews %s with %s resampling', str(overviews), self.overviews_resampling.name)
            self.rfile.build_overviews(overviews, resampling=self.overviews_resampling)
        return self.rfile.close()","
class RIODataset:
    
    def __init__(self, rfile, overviews=None, overviews_resampling=None, overviews_minsize=256):
        
        pass
    def __setitem__(self, key, item):
        
        pass

    def close(self):
        
        pass",snippet_284,4,"```python
import rasterio
from rasterio.enums import Resampling
from rasterio.errors import RasterioIOError


class RIODataset:
    """"""
    A lightweight wrapper around rasterio that supports optional overview creation
    and simple band‑wise write operations via the ``__setitem__`` interface.
    """"""

    def __init__(
        self,
        rfile,
        overviews=None,
        overviews_resampling=None,
        overviews_minsize=256,
    ):
        """"""
        Parameters
        ----------
        rfile : str or pathlib.Path
            Path to the raster file to open.
        overviews : list[int] | None
            List of overview levels to build. If ``None`` no overviews are built.
        overviews_resampling : rasterio.enums.Resampling | None
            Resampling method used when building overviews. Defaults to
            ``Resampling.nearest``.
        overviews_minsize : int
            Minimum size of the smallest overview. This is passed to
            ``build_overviews`` via the ``min_size`` keyword.
        """"""
        self.rfile = rfile
        self.overviews = overviews
        self.overviews_resampling = (
            overviews_resampling or Resampling.nearest
        )
        self.overviews_minsize = overviews_minsize

        try:
            self.ds = rasterio.open(rfile, ""r+"")
        except RasterioIOError as exc:
            raise FileNotFoundError(f""Could not open raster file {rfile}"") from exc

        # Build overviews if requested and they do not already exist
        if self.overviews:
            # Check if any overviews exist for the first band
            existing = self.ds.overviews(1)
            if not existing:
                self.ds.build_overviews(
                    self.overviews,
                    self.overviews_resampling,
                    min_size=self.overviews_minsize,
                )
                # Store the resampling method in the tags for future reference
                self.ds.update_tags(1, OVERVIEW_RESAMPLING=str(self.overviews_resampling))

        # Cache some useful attributes
        self.shape = self.ds.shape
        self.count = self.ds.count
        self.dtypes = self.ds.dtypes

    def __setitem__(self, key, item):
        """"""
        Write data to one or more bands.

        Parameters
        ----------
        key : int | slice
            Band index (1‑based) or slice of band indices.
        item : array-like
            Data to write. For a single band the shape must be
            (height, width). For a slice the shape must be
            (num_bands, height, width).
        """"""
        if isinstance(key, int):
            band = key
            if band < 1 or band > self.count:
                raise IndexError(f""Band index {band} out of range (1–{self.count})"")
            self.ds.write(item, band)
        elif isinstance(key, slice):
            start = key.start or 1
            stop = key.stop or self.count + 1
            step = key.step or 1
            bands = list(range(start, stop, step))
            if any(b < 1 or b > self.count for b in bands):
                raise IndexError(f""Band indices {bands} out of range (1–{self.count})"")
            self.ds.write(item, bands)
        else:
            raise TypeError(""Key must be an int (band) or slice of bands"")

    def close(self):
        """"""Close the underlying rasterio dataset.""""""
        if hasattr(self, ""ds"") and self.ds:
            self.ds.close()
            self.ds = None

    # Optional context‑manager support
    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc, tb):
        self.close()
```"
350922,pytroll/trollimage,pytroll_trollimage/trollimage/_xrimage_rasterio.py,trollimage._xrimage_rasterio.RIOTag,"class RIOTag:
    """"""Rasterio wrapper to allow da.store on tag.""""""

    def __init__(self, rfile, name):
        """"""Init the rasterio tag.""""""
        self.rfile = rfile
        self.name = name

    def __setitem__(self, key, item):
        """"""Put the data in the tag.""""""
        kwargs = {self.name: item.item()}
        self.rfile.update_tags(**kwargs)

    def close(self):
        """"""Close the file.""""""
        return self.rfile.close()","class RIOTag:
    
    def __init__(self, rfile, name):
        
        pass
    def __setitem__(self, key, item):
        
        pass
    def close(self):
        
        pass",snippet_285,4,"```python
class RIOTag:
    def __init__(self, rfile, name):
        """"""
        Create a new RIOTag instance.

        Parameters
        ----------
        rfile : file-like object
            The file to which the tag will be written.
        name : str
            The name of the tag.
        """"""
        self.rfile = rfile
        self.name = name
        self._attrs = {}

    def __setitem__(self, key, item):
        """"""
        Set an attribute for the tag.

        Parameters
        ----------
        key : str
            The attribute name.
        item : Any
            The attribute value.
        """"""
        self._attrs[key] = item

    def close(self):
        """"""
        Write the tag to the file and close the tag.

        The tag is written as a self‑closing tag, e.g.
        <tag key=""value"" />.
        """"""
        attrs = ' '.join(f'{k}=""{v}""' for k, v in self._attrs.items())
        if attrs:
            self.rfile.write(f'<{self.name} {attrs} />\n')
        else:
            self.rfile.write(f'<{self.name} />\n')
```"
353855,inveniosoftware/invenio-communities,inveniosoftware_invenio-communities/invenio_communities/communities/records/systemfields/access.py,invenio_communities.communities.records.systemfields.access.AccessEnumMixin,"class AccessEnumMixin:
    """"""Mixin for enum functionalities.""""""

    @classmethod
    def validate(cls, level):
        """"""Validate a string against the enum values.""""""
        return cls(level) in cls

    def __str__(self):
        """"""Return its value.""""""
        return self.value","class AccessEnumMixin:
    
    @classmethod
    def validate(cls, level):
        
        pass
    def __str__(self):
        
        pass",snippet_286,3,"```python
class AccessEnumMixin:
    @classmethod
    def validate(cls, level):
        """"""
        Validate that `level` is a member of the enum class `cls`.

        Parameters
        ----------
        level : str | cls
            The value to validate. It can be an enum member or a string
            representing the member name.

        Returns
        -------
        cls
            The validated enum member.

        Raises
        ------
        ValueError
            If `level` is a string that does not correspond to a member.
        TypeError
            If `level` is neither a string nor an instance of `cls`.
        """"""
        if isinstance(level, cls):
            return level

        if isinstance(level, str):
            try:
                return cls[level]
            except KeyError as exc:
                raise ValueError(
                    f""Invalid {cls.__name__} value: {level!r}""
                ) from exc

        raise TypeError(
            f""{cls.__name__}.validate expects a {cls.__name__} or str, ""
            f""got {type(level).__name__}""
        )

    def __str__(self):
        """"""
        Return the name of the enum member.
        """"""
        return self.name
```"
354005,inveniosoftware/invenio-communities,inveniosoftware_invenio-communities/invenio_communities/roles.py,invenio_communities.roles.Role,"from dataclasses import dataclass, field

@dataclass(frozen=True)
class Role:
    """"""Role class.""""""
    name: str = ''
    'Name of the role.'
    title: str = ''
    'Title of the role.'
    description: str = ''
    'Brief description of capabilities of the role.'
    can_manage_roles: list = field(default_factory=list)
    'List of other roles that this role can manage.'
    is_owner: bool = False
    'This role is the owner role (only one can exists).'
    can_manage: bool = False
    'This role has manage permissions.'
    can_curate: bool = False
    'This role has record manage permissions.'
    can_view: bool = False
    'This role has view restricted record permissions.'

    def can_manage_role(self, role_name):
        """"""Determine if this role can manage the role name.""""""
        return role_name in self.can_manage_roles

    def __hash__(self):
        """"""Compute a hash for use with e.g. sets.""""""
        return self.name.__hash__()","@dataclass(frozen=True)
class Role:
    
    def can_manage_role(self, role_name):
        
        pass
    def __hash__(self):
        
        pass",snippet_287,3,"```python
from dataclasses import dataclass, field
from typing import FrozenSet, Set


@dataclass(frozen=True)
class Role:
    name: str
    manages: FrozenSet[str] = field(default_factory=frozenset)

    def can_manage_role(self, role_name: str) -> bool:
        """"""Return True if this role can manage the role with the given name.""""""
        return role_name in self.manages

    def __hash__(self) -> int:
        """"""Hash based on the role name and the set of roles it can manage.""""""
        return hash((self.name, self.manages))
```"
356596,grycap/RADL,grycap_RADL/radl/radl.py,radl.radl.Aspect,"import copy

class Aspect:
    """"""A network, ansible_host, system, deploy, configure or contextualize element in a RADL.""""""

    def getId(self):
        """"""Return the id of the aspect.""""""
        return id(self)

    def clone(self):
        """"""Return a copy of this aspect.""""""
        return copy.deepcopy(self)","
class Aspect:
    
    def getId(self):
        
        pass
    def clone(self):
        
        pass",snippet_288,3,"```python
import uuid
import copy

class Aspect:
    def __init__(self, id=None):
        self.id = id or uuid.uuid4()

    def getId(self):
        return self.id

    def clone(self):
        return copy.deepcopy(self)
```"
357236,pytroll/posttroll,posttroll/backends/zmq/message_broadcaster.py,posttroll.backends.zmq.message_broadcaster.ZMQDesignatedReceiversSender,"import threading
from posttroll.backends.zmq.socket import close_socket, set_up_client_socket
from zmq import LINGER, NOBLOCK, REQ, ZMQError

class ZMQDesignatedReceiversSender:
    """"""Sends message to multiple *receivers* on *port*.""""""

    def __init__(self, default_port, receivers):
        """"""Set up the sender.""""""
        self.default_port = default_port
        self.receivers = receivers
        self._shutdown_event = threading.Event()

    def __call__(self, data):
        """"""Send data.""""""
        for receiver in self.receivers:
            self._send_to_address(receiver, data)

    def _send_to_address(self, address, data, timeout=10):
        """"""Send data to *address* and *port* without verification of response.""""""
        if address.find(':') == -1:
            full_address = 'tcp://%s:%d' % (address, self.default_port)
        else:
            full_address = 'tcp://%s' % address
        options = {LINGER: int(timeout * 1000)}
        socket = set_up_client_socket(REQ, full_address, options)
        try:
            socket.send_string(data)
            while not self._shutdown_event.is_set():
                try:
                    message = socket.recv_string(NOBLOCK)
                except ZMQError:
                    self._shutdown_event.wait(0.1)
                    continue
                if message != 'ok':
                    logger.warning('invalid acknowledge received: %s' % message)
                break
        finally:
            close_socket(socket)

    def close(self):
        """"""Close the sender.""""""
        self._shutdown_event.set()","
class ZMQDesignatedReceiversSender:
    
    def __init__(self, default_port, receivers):
        
        pass
    def __call__(self, data):
        
        pass

    def _send_to_address(self, address, data, timeout=10):
        
        pass

    def close(self):
        
        pass",snippet_289,5,"```python
import zmq
import threading
import time


class ZMQDesignatedReceiversSender:
    """"""
    A simple ZeroMQ sender that can send data to a set of designated receivers.
    Each receiver is identified by a name and an address. The sender creates a
    PUSH socket for each receiver and connects to the given address.
    """"""

    def __init__(self, default_port, receivers):
        """"""
        Parameters
        ----------
        default_port : int
            Default port to use if a receiver address does not specify a port.
        receivers : dict
            Mapping of receiver name to address string. The address may be a
            full ZeroMQ endpoint (e.g. ""tcp://127.0.0.1:5555"") or just a host
            (e.g. ""127.0.0.1"") in which case the default_port is appended.
        """"""
        self._default_port = default_port
        self._receivers = receivers
        self._context = zmq.Context.instance()
        self._sockets = {}
        self._lock = threading.Lock()

        for name, addr in receivers.items():
            # Ensure the address contains a port
            if ""://"" not in addr:
                addr = f""tcp://{addr}""
            if "":"" not in addr.split(""://"")[1]:
                addr = f""{addr}:{self._default_port}""
            socket = self._context.socket(zmq.PUSH)
            socket.setsockopt(zmq.LINGER, 0)
            socket.connect(addr)
            self._sockets[name] = socket

    def __call__(self, data):
        """"""
        Send the given data to all configured receivers.

        Parameters
        ----------
        data : any
            The Python object to send. It will be serialized with zmq.send_pyobj.
        """"""
        with self._lock:
            for name, sock in self._sockets.items():
                try:
                    sock.send_pyobj(data, flags=zmq.NOBLOCK)
                except zmq.Again:
                    # If the socket is blocked, we can optionally retry or drop.
                    # Here we simply drop the message.
                    pass

    def _send_to_address(self, address, data, timeout=10):
        """"""
        Send data to a specific address. This method is not used by __call__
        but can be used for targeted sending.

        Parameters
        ----------
        address : str
            ZeroMQ endpoint to send to.
        data : any
            The Python object to send.
        timeout : int, optional
            Timeout in seconds for the send operation.
        """"""
        # Create a temporary socket for one-shot send
        sock = self._context.socket(zmq.PUSH)
        sock.setsockopt(zmq.LINGER, 0)
        sock.setsockopt(zmq.SNDTIMEO, int(timeout * 1000))
        sock.connect(address)
        try:
            sock.send_pyobj(data, flags=zmq.NOBLOCK)
        except zmq.Again:
            # Timeout or blocked
            pass
        finally:
            sock.close()

    def close(self):
        """"""
        Close all sockets and terminate the ZeroMQ context.
        """"""
        with self._lock:
            for sock in self._sockets.values():
                try:
                    sock.close()
                except Exception:
                    pass
            self._sockets.clear()
        # Do not terminate the context if it was shared
        # (Context.instance() is used). If you want to terminate
        # the context, uncomment the following line:
        # self._context.term()
```"
357237,pytroll/posttroll,posttroll/backends/zmq/ns.py,posttroll.backends.zmq.ns.ZMQNameServer,"from posttroll.backends.zmq.socket import SocketReceiver, close_socket, set_up_client_socket, set_up_server_socket
from contextlib import suppress
from zmq import LINGER, REP, REQ
from posttroll.ns import get_active_address, get_configured_nameserver_port

class ZMQNameServer:
    """"""The name server.""""""

    def __init__(self):
        """"""Set up the nameserver.""""""
        self.running: bool = True
        self.listener: SocketReceiver | None = None
        self._authenticator = None

    def run(self, address_receiver, address: str | None=None):
        """"""Run the listener and answer to requests.""""""
        port = get_configured_nameserver_port()
        try:
            if not self.running:
                return
            if address is None:
                address = '*'
            address = create_nameserver_address(address)
            self.listener, _, self._authenticator = set_up_server_socket(REP, address)
            logger.debug(f'Nameserver listening on port {port}')
            socket_receiver = SocketReceiver()
            socket_receiver.register(self.listener)
            while self.running:
                try:
                    for msg, _ in socket_receiver.receive(self.listener, timeout=1):
                        logger.debug('Replying to request: ' + str(msg))
                        active_address = get_active_address(msg.data['service'], address_receiver, msg.version)
                        self.listener.send_unicode(str(active_address))
                except TimeoutError:
                    continue
        except KeyboardInterrupt:
            pass
        finally:
            socket_receiver.unregister(self.listener)
            self.close_sockets_and_threads()

    def close_sockets_and_threads(self):
        """"""Close all sockets and threads.""""""
        with suppress(AttributeError):
            close_socket(self.listener)
        with suppress(AttributeError):
            self._authenticator.stop()

    def stop(self):
        """"""Stop the name server.""""""
        self.running = False","
class ZMQNameServer:
    
    def __init__(self):
        
        pass
    def run(self, address_receiver, address: str | None=None):
        
        pass

    def close_sockets_and_threads(self):
        
        pass

    def stop(self):
        
        pass",snippet_290,5,"```python
import json
import threading
import zmq
import time
from typing import Optional, Dict, Any


class ZMQNameServer:
    """"""
    A simple ZeroMQ-based name server.

    The server exposes a REP socket that accepts JSON messages with the following
    structure:

        {""action"": ""register"", ""name"": ""<name>"", ""address"": ""<address>""}
        {""action"": ""lookup"",   ""name"": ""<name>""}
        {""action"": ""unregister"",""name"": ""<name>""}
        {""action"": ""list""}

    The server maintains an in‑memory dictionary mapping names to addresses.
    """"""

    def __init__(self):
        # ZeroMQ context
        self._ctx = zmq.Context.instance()

        # Sockets (created in run)
        self._rep_socket: Optional[zmq.Socket] = None
        self._pub_socket: Optional[zmq.Socket] = None

        # Thread that runs the event loop
        self._thread: Optional[threading.Thread] = None

        # Flag to stop the server
        self._stop_event = threading.Event()

        # In‑memory registry
        self._registry: Dict[str, str] = {}

        # Lock for registry access
        self._lock = threading.Lock()

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------
    def run(self, address_receiver: str, address: Optional[str] = None) -> None:
        """"""
        Start the name server.

        Parameters
        ----------
        address_receiver : str
            The address to bind the REP socket (e.g. ""tcp://*:5555"").
        address : Optional[str]
            If provided, a PUB socket will be bound to this address to
            broadcast registration events.
        """"""
        if self._thread and self._thread.is_alive():
            raise RuntimeError(""Server is already running"")

        # Create REP socket
        self._rep_socket = self._ctx.socket(zmq.REP)
        self._rep_socket.bind(address_receiver)

        # Optional PUB socket
        if address:
            self._pub_socket = self._ctx.socket(zmq.PUB)
            self._pub_socket.bind(address)

        # Start the worker thread
        self._stop_event.clear()
        self._thread = threading.Thread(target=self._serve, daemon=True)
        self._thread.start()

    def close_sockets_and_threads(self) -> None:
        """"""
        Close all sockets and wait for the worker thread to finish.
        """"""
        self.stop()
        if self._thread:
            self._thread.join()
            self._thread = None

        if self._rep_socket:
            try:
                self._rep_socket.close(0)
            except Exception:
                pass
            self._rep_socket = None

        if self._pub_socket:
            try:
                self._pub_socket.close(0)
            except Exception:
                pass
            self._pub_socket = None

        # Terminate the context only if no other sockets are using it
        try:
            self._ctx.term()
        except Exception:
            pass

    def stop(self) -> None:
        """"""
        Signal the server to stop.
        """"""
        self._stop_event.set()

    # ------------------------------------------------------------------
    # Internal helpers
    # ------------------------------------------------------------------
    def _serve(self) -> None:
        """"""
        Main loop that processes incoming requests.
        """"""
        while not self._stop_event.is_set():
            try:
                # Use a short poll timeout to allow graceful shutdown
                socks = dict(self._rep_socket.poll(100, zmq.POLLIN))
                if socks.get(self._rep_socket) == zmq.POLLIN:
                    msg = self._rep_socket.recv_string(zmq.NOBLOCK)
                    response = self._handle_request(msg)
                    self._rep_socket.send_string(response)
            except zmq.Again:
                # No message received
                continue
            except Exception as exc:
                # Log the error and send a generic error response
                error_resp = json.dumps({""status"": ""error"", ""message"": str(exc)})
                try:
                    self._rep_socket.send_string(error_resp)
                except Exception:
                    pass
                continue

    def _handle_request(self, msg: str) -> str:
        """"""
        Parse the incoming JSON message and perform the requested action.
        """"""
        try:
            data = json.loads(msg)
        except json.JSONDecodeError:
            return json.dumps({""status"": ""error"", ""message"": ""Invalid JSON""})

        action = data.get(""action"")
        if not action:
            return json.dumps({""status"": ""error"", ""message"": ""Missing action""})

        if action == ""register"":
            return self._register(data)
        elif action == ""lookup"":
            return self._lookup(data)
        elif action == ""unregister"":
            return self._unregister(data)
        elif action == ""list"":
            return self._list()
        else:
            return json.dumps({""status"": ""error"", ""message"": f""Unknown action '{action}'""})

    def _register(self, data: Dict[str, Any]) -> str:
        name = data.get(""name"")
        address = data.get(""address"")
        if not name or not address:
            return json.dumps({""status"": ""error"", ""message"": ""Missing name or address""})

        with self._lock:
            self._registry[name] = address

        # Broadcast the registration if PUB socket is available
        if self._pub_socket:
            try:
                self._pub_socket.send_string(json.dumps({""event"": ""register"", ""name"": name, ""address"": address}))
            except Exception:
                pass

        return json.dumps({""status"": ""ok"", ""message"": f""Registered {name}""})

    def _lookup(self, data: Dict[str, Any]) -> str:
        name = data.get(""name"")
        if not name:
            return json.dumps({""status"": ""error"", ""message"": ""Missing name""})

        with self._lock:
            address = self._registry.get(name)

        if address:
            return json.dumps({""status"": ""ok"", ""address"": address})
        else:
            return json.dumps({""status"": ""error"", ""message"": f""Name '{name}' not found""})

    def _unregister(self, data: Dict[str, Any]) -> str:
        name = data.get(""name"")
        if not name:
            return json.dumps({""status"": ""error"", ""message"": ""Missing name""})

        with self._lock:
            removed = self._registry.pop(name, None)

        if removed:
            if self._pub_socket:
                try:
                    self._pub_socket.send_string(json.dumps({""event"": ""unregister"", ""name"": name}))
                except Exception:
                    pass
            return json.dumps({""status"": ""ok"", ""message"": f""Unregistered {name}""})
        else:
            return json.dumps({""status"": ""error"", ""message"": f""Name '{name}' not found""})

    def _list(self) -> str:
        with self._lock:
            names = list(self._registry.keys())
        return json.dumps({""status"": ""ok"", ""names"": names})
```"
357239,pytroll/posttroll,posttroll/backends/zmq/socket.py,posttroll.backends.zmq.socket.SocketReceiver,"from posttroll.message import Message
import zmq

class SocketReceiver:
    """"""A receiver for mulitple sockets.""""""

    def __init__(self):
        """"""Set up the receiver.""""""
        self._poller = zmq.Poller()

    def register(self, socket):
        """"""Register the socket.""""""
        self._poller.register(socket, zmq.POLLIN)

    def unregister(self, socket):
        """"""Unregister the socket.""""""
        self._poller.unregister(socket)

    def receive(self, *sockets, timeout=None):
        """"""Timeout is in seconds.""""""
        if timeout:
            timeout *= 1000
        socks = dict(self._poller.poll(timeout=timeout))
        if socks:
            for sock in sockets:
                if socks.get(sock) == zmq.POLLIN:
                    received = sock.recv_string(zmq.NOBLOCK)
                    yield (Message.decode(received), sock)
        else:
            raise TimeoutError('Did not receive anything on sockets.')","
class SocketReceiver:
    
    def __init__(self):
        
        pass
    def register(self, socket):
        
        pass

    def unregister(self, socket):
        
        pass

    def receive(self, *sockets, timeout=None):
        
        pass",snippet_291,5,"```python
import select
import socket
from typing import Iterable, List, Tuple, Optional


class SocketReceiver:
    """"""
    A simple socket receiver that can register sockets, unregister them,
    and wait for any of them to become readable. The `receive` method
    returns a list of (socket, data) tuples for all sockets that are
    ready to read.
    """"""

    def __init__(self) -> None:
        # Store registered sockets in a set for quick lookup
        self._registered: set[socket.socket] = set()

    def register(self, sock: socket.socket) -> None:
        """"""
        Register a socket to be monitored for readability.
        """"""
        if not isinstance(sock, socket.socket):
            raise TypeError(""Only socket.socket instances can be registered"")
        self._registered.add(sock)

    def unregister(self, sock: socket.socket) -> None:
        """"""
        Unregister a previously registered socket.
        """"""
        self._registered.discard(sock)

    def _get_sockets(self, sockets: Iterable[socket.socket]) -> List[socket.socket]:
        """"""
        Resolve the list of sockets to monitor. If no sockets are provided,
        use all registered sockets. Otherwise, filter the provided sockets
        to those that are registered.
        """"""
        if not sockets:
            return list(self._registered)
        # Ensure all provided sockets are registered
        filtered = [s for s in sockets if s in self._registered]
        if not filtered:
            raise ValueError(""No registered sockets provided to receive"")
        return filtered

    def receive(
        self,
        *sockets: socket.socket,
        timeout: Optional[float] = None,
    ) -> List[Tuple[socket.socket, bytes]]:
        """"""
        Wait for any of the specified sockets (or all registered sockets if none
        are specified) to become readable. Returns a list of (socket, data)
        tuples for each socket that has data ready to be read.

        Parameters
        ----------
        *sockets : socket.socket
            Optional sockets to monitor. If omitted, all registered sockets
            are monitored.
        timeout : float, optional
            Timeout in seconds. If None, block indefinitely.

        Returns
        -------
        List[Tuple[socket.socket, bytes]]
            A list of tuples containing the socket and the data read from it.
            If no sockets are ready, returns an empty list.
        """"""
        # Resolve sockets to monitor
        monitor_socks = self._get_sockets(sockets)

        if not monitor_socks:
            return []

        # Use select to wait for readability
        try:
            ready, _, _ = select.select(monitor_socks, [], [], timeout)
        except (ValueError, OSError) as exc:
            # If select fails (e.g., invalid socket), raise a clear error
            raise RuntimeError(f""Select failed: {exc}"") from exc

        results: List[Tuple[socket.socket, bytes]] = []

        for sock in ready:
            try:
                # Read up to 4096 bytes; adjust as needed
                data = sock.recv(4096)
                if data:
                    results.append((sock, data))
                else:
                    # No data means the socket has been closed; unregister it
                    self.unregister(sock)
            except BlockingIOError:
                # Non-blocking socket had no data; ignore
                continue
            except OSError as exc:
                # On error, unregister the socket and skip
                self.unregister(sock)
                continue

        return results
```"
357241,pytroll/posttroll,posttroll/bbmcast.py,posttroll.bbmcast.MulticastReceiver,"import struct
from socket import AF_INET, INADDR_ANY, IP_ADD_MEMBERSHIP, IP_MULTICAST_IF, IP_MULTICAST_LOOP, IP_MULTICAST_TTL, IPPROTO_IP, SO_BROADCAST, SO_LINGER, SO_REUSEADDR, SOCK_DGRAM, SOL_IP, SOL_SOCKET, gethostbyname, inet_aton, socket, timeout

class MulticastReceiver:
    """"""Multicast receiver on *port* for an *mcgroup*.""""""
    BUFSIZE = 1024

    def __init__(self, port, mcgroup=None):
        """"""Set up the multicast receiver.""""""
        self.port = port
        self.socket, self.group = mcast_receiver(port, mcgroup)
        logger.info(f'Receiver initialized on group {self.group}.')

    def settimeout(self, tout=None):
        """"""Set timeout.

        A timeout will throw a 'socket.timeout'.
        """"""
        self.socket.settimeout(tout)
        return self

    def __call__(self):
        """"""Receive data from a socket.""""""
        data, sender = self.socket.recvfrom(self.BUFSIZE)
        return (data.decode(), sender)

    def close(self):
        """"""Close the receiver.""""""
        self.socket.setsockopt(SOL_SOCKET, SO_LINGER, struct.pack('ii', 1, 1))
        self.socket.close()","
class MulticastReceiver:
    
    def __init__(self, port, mcgroup=None):
        
        pass
    def settimeout(self, tout=None):
        
        pass

    def __call__(self):
        
        pass

    def close(self):
        
        pass",snippet_292,5,"```python
import socket
import struct

class MulticastReceiver:
    """"""
    A simple multicast UDP receiver.

    Parameters
    ----------
    port : int
        The UDP port to bind to.
    mcgroup : str, optional
        The multicast group address to join. If None, defaults to
        '224.0.0.1'.
    """"""

    def __init__(self, port, mcgroup=None):
        self.port = port
        self.mcgroup = mcgroup or '224.0.0.1'
        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)
        # Allow multiple sockets to use the same PORT number
        self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        # Bind to the port on all interfaces
        self.sock.bind(('', self.port))
        # Join the multicast group
        mreq = struct.pack(""4sL"", socket.inet_aton(self.mcgroup), socket.INADDR_ANY)
        self.sock.setsockopt(socket.IPPROTO_IP, socket.IP_ADD_MEMBERSHIP, mreq)
        self.timeout = None

    def settimeout(self, tout=None):
        """"""
        Set the socket timeout.

        Parameters
        ----------
        tout : float or None
            Timeout in seconds. If None, the socket is blocking.
        """"""
        self.timeout = tout
        self.sock.settimeout(tout)

    def __call__(self):
        """"""
        Receive a single datagram from the multicast group.

        Returns
        -------
        data : bytes
            The received payload.
        addr : tuple
            The address of the sender.
        """"""
        try:
            data, addr = self.sock.recvfrom(65535)
            return data, addr
        except socket.timeout:
            return None, None
        except OSError:
            return None, None

    def close(self):
        """"""
        Leave the multicast group and close the socket.
        """"""
        try:
            mreq = struct.pack(""4sL"", socket.inet_aton(self.mcgroup), socket.INADDR_ANY)
            self.sock.setsockopt(socket.IPPROTO_IP, socket.IP_DROP_MEMBERSHIP, mreq)
        finally:
            self.sock.close()
```"
357242,pytroll/posttroll,posttroll/bbmcast.py,posttroll.bbmcast.MulticastSender,"class MulticastSender:
    """"""Multicast sender on *port* and *mcgroup*.""""""

    def __init__(self, port, mcgroup=None):
        """"""Set up the multicast sender.""""""
        self.port = port
        self.group = mcgroup
        self.socket, self.group = mcast_sender(mcgroup)
        logger.debug('Started multicast group %s', self.group)

    def __call__(self, data):
        """"""Send data to a socket.""""""
        self.socket.sendto(data.encode(), (self.group, self.port))

    def close(self):
        """"""Close the sender.""""""
        self.socket.close()","class MulticastSender:
    
    def __init__(self, port, mcgroup=None):
        
        pass
    def __call__(self, data):
        
        pass
    def close(self):
        
        pass",snippet_293,4,"```python
import socket
import struct

class MulticastSender:
    """"""
    Simple UDP multicast sender.

    Parameters
    ----------
    port : int
        Destination port for the multicast packets.
    mcgroup : str, optional
        Multicast group address. Defaults to '224.0.0.1'.
    ttl : int, optional
        Time-to-live for multicast packets. Defaults to 1.
    """"""

    def __init__(self, port, mcgroup=None, ttl=1):
        self.port = port
        self.mcgroup = mcgroup or '224.0.0.1'
        self.ttl = ttl

        # Create a UDP socket
        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)

        # Set the TTL for multicast packets
        ttl_bin = struct.pack('b', self.ttl)
        self.sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_TTL, ttl_bin)

        # Disable loopback so we don't receive our own packets
        self.sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_LOOP, b'\x00')

    def __call__(self, data):
        """"""
        Send data to the multicast group.

        Parameters
        ----------
        data : bytes or str
            The payload to send. If a string is provided, it will be encoded as UTF-8.
        """"""
        if isinstance(data, str):
            data = data.encode('utf-8')
        elif not isinstance(data, (bytes, bytearray)):
            raise TypeError(""data must be bytes, bytearray, or str"")

        self.sock.sendto(data, (self.mcgroup, self.port))

    def close(self):
        """"""Close the underlying socket.""""""
        if self.sock:
            try:
                self.sock.close()
            finally:
                self.sock = None
```"
357244,pytroll/posttroll,posttroll/listener.py,posttroll.listener.ListenerContainer,"from threading import Thread
from queue import Queue
import logging

class ListenerContainer:
    """"""Container for a listener instance.""""""
    logger = logging.getLogger(__name__ + '.ListenerContainer')

    def __init__(self, topics=None, addresses=None, nameserver='localhost', services=''):
        """"""Initialize the class.""""""
        self.listener = None
        self.output_queue = None
        self.thread = None
        self.addresses = addresses
        self.nameserver = nameserver
        if topics is not None:
            self.output_queue = Queue()
            self.listener = Listener(topics=topics, queue=self.output_queue, addresses=self.addresses, nameserver=self.nameserver, services=services)
            self.thread = Thread(target=self.listener.run, daemon=True)
            self.thread.start()

    def __setstate__(self, state):
        """"""Re-initialize the class.""""""
        self.__init__(**state)

    def restart_listener(self, topics):
        """"""Restart listener after configuration update.""""""
        if self.listener is not None:
            if self.listener.running:
                self.stop()
        self.__init__(topics=topics)

    def stop(self):
        """"""Stop listener.""""""
        self.logger.debug('Stopping listener.')
        self.listener.stop()
        if self.thread is not None:
            self.thread.join()
            self.thread = None
        self.logger.debug('Listener stopped.')","
class ListenerContainer:
    
    def __init__(self, topics=None, addresses=None, nameserver='localhost', services=''):
        
        pass
    def __setstate__(self, state):
        
        pass

    def restart_listener(self, topics):
        
        pass

    def stop(self):
        
        pass",snippet_294,5,"```python
import threading
import time
import pickle


class ListenerContainer:
    """"""
    A simple listener container that simulates listening to topics.
    """"""

    def __init__(self, topics=None, addresses=None, nameserver='localhost', services=''):
        """"""
        Initialize the listener container.

        :param topics: Iterable of topics to listen to.
        :param addresses: Iterable of addresses (unused in this simple implementation).
        :param nameserver: Name server address (unused in this simple implementation).
        :param services: Services string (unused in this simple implementation).
        """"""
        self.topics = list(topics) if topics is not None else []
        self.addresses = list(addresses) if addresses is not None else []
        self.nameserver = nameserver
        self.services = services

        # Thread control
        self._stop_event = threading.Event()
        self._thread = None
        self._start_listener()

    def __setstate__(self, state):
        """"""
        Restore the state of the object after unpickling.
        """"""
        self.__dict__.update(state)
        # Ensure the stop event and thread are reset
        if not hasattr(self, '_stop_event') or self._stop_event is None:
            self._stop_event = threading.Event()
        if not hasattr(self, '_thread') or self._thread is None:
            self._thread = None
        # Restart the listener after unpickling
        self._start_listener()

    def _listener_loop(self):
        """"""
        The main loop that simulates listening to topics.
        """"""
        while not self._stop_event.is_set():
            if self.topics:
                # Simulate processing each topic
                for topic in self.topics:
                    if self._stop_event.is_set():
                        break
                    # In a real implementation, this would be where messages are received.
                    print(f""[Listener] Processing topic: {topic}"")
                    time.sleep(0.1)
            else:
                # No topics to listen to; just wait a bit
                time.sleep(0.5)

    def _start_listener(self):
        """"""
        Start the listener thread.
        """"""
        if self._thread and self._thread.is_alive():
            return
        self._stop_event.clear()
        self._thread = threading.Thread(target=self._listener_loop, daemon=True)
        self._thread.start()

    def restart_listener(self, topics):
        """"""
        Restart the listener with a new set of topics.

        :param topics: Iterable of new topics to listen to.
        """"""
        # Stop current listener
        self.stop()
        # Update topics
        self.topics = list(topics) if topics is not None else []
        # Restart listener
        self._start_listener()

    def stop(self):
        """"""
        Stop the listener thread.
        """"""
        if self._thread and self._thread.is_alive():
            self._stop_event.set()
            self._thread.join(timeout=1.0)
            self._thread = None

    def __del__(self):
        """"""
        Ensure the listener thread is stopped when the object is garbage collected.
        """"""
        self.stop()


# Example usage (uncomment to test):
# if __name__ == ""__main__"":
#     lc = ListenerContainer(topics=['topic1', 'topic2'])
#     time.sleep(1)
#     lc.restart_listener(['topic3'])
#     time.sleep(1)
#     lc.stop()
```"
357253,pytroll/posttroll,posttroll/message_broadcaster.py,posttroll.message_broadcaster.DesignatedReceiversSender,"from posttroll import config, message

class DesignatedReceiversSender:
    """"""Sends message to multiple *receivers* on *port*.""""""

    def __init__(self, default_port, receivers):
        """"""Set settings.""""""
        backend = config.get('backend', 'unsecure_zmq')
        if backend == 'unsecure_zmq':
            from posttroll.backends.zmq.message_broadcaster import ZMQDesignatedReceiversSender
            self._sender = ZMQDesignatedReceiversSender(default_port, receivers)
        else:
            raise NotImplementedError()

    def __call__(self, data):
        """"""Send messages from all receivers.""""""
        return self._sender(data)

    def close(self):
        """"""Close the sender.""""""
        return self._sender.close()","
class DesignatedReceiversSender:
    
    def __init__(self, default_port, receivers):
        
        pass
    def __call__(self, data):
        
        pass

    def close(self):
        
        pass",snippet_295,4,"```python
import socket
from typing import Iterable, Tuple, Union, List, Optional


class DesignatedReceiversSender:
    """"""
    A simple TCP sender that connects to a list of designated receivers
    (host, port) pairs and sends data to all of them when called.
    """"""

    def __init__(self, default_port: int, receivers: Iterable[Union[str, Tuple[str, int]]]):
        """"""
        Parameters
        ----------
        default_port : int
            The port to use when a receiver is specified only by hostname.
        receivers : Iterable[Union[str, Tuple[str, int]]]
            An iterable of receiver specifications. Each element can be:
            * a string hostname (port will be taken from ``default_port``)
            * a tuple/list ``(hostname, port)``

        Raises
        ------
        ValueError
            If a receiver specification is invalid.
        """"""
        self._default_port = default_port
        self._sockets: List[socket.socket] = []

        for r in receivers:
            if isinstance(r, (list, tuple)):
                if len(r) != 2:
                    raise ValueError(f""Receiver tuple must be (host, port), got {r!r}"")
                host, port = r
            elif isinstance(r, str):
                host = r
                port = default_port
            else:
                raise ValueError(f""Unsupported receiver type: {type(r)}"")

            try:
                # Create a TCP socket and connect
                s = socket.create_connection((host, port))
                self._sockets.append(s)
            except Exception as exc:
                # If a connection fails, we skip this receiver but keep the rest
                # Optionally, you could log the error here
                continue

    def __call__(self, data: Union[bytes, str]) -> None:
        """"""
        Send the given data to all connected receivers.

        Parameters
        ----------
        data : bytes or str
            The data to send. If a string is provided, it will be encoded
            using UTF-8 before sending.

        Raises
        ------
        RuntimeError
            If no receivers are connected.
        """"""
        if not self._sockets:
            raise RuntimeError(""No receivers are connected"")

        if isinstance(data, str):
            payload = data.encode(""utf-8"")
        else:
            payload = data

        # Send to each socket; if a send fails, close that socket
        dead_sockets = []
        for s in self._sockets:
            try:
                s.sendall(payload)
            except Exception:
                dead_sockets.append(s)

        for s in dead_sockets:
            try:
                s.close()
            finally:
                self._sockets.remove(s)

    def close(self) -> None:
        """"""
        Close all open sockets.
        """"""
        for s in self._sockets:
            try:
                s.close()
            except Exception:
                pass
        self._sockets.clear()
```"
357255,pytroll/posttroll,posttroll/ns.py,posttroll.ns.NameServer,"from posttroll.address_receiver import AddressReceiver
import datetime as dt
from posttroll import config
from contextlib import suppress

class NameServer:
    """"""The name server.""""""

    def __init__(self, max_age=None, multicast_enabled=True, restrict_to_localhost=False):
        """"""Initialize nameserver.""""""
        self.loop = True
        self.listener = None
        self._max_age = max_age or dt.timedelta(minutes=10)
        self._multicast_enabled = multicast_enabled
        self._restrict_to_localhost = restrict_to_localhost
        backend = config['backend']
        if backend not in ['unsecure_zmq', 'secure_zmq']:
            raise NotImplementedError(f'Did not recognize backend: {backend}')
        from posttroll.backends.zmq.ns import ZMQNameServer
        self._ns = ZMQNameServer()

    def run(self, address_receiver=None, nameserver_address=None):
        """"""Run the listener and answer to requests.""""""
        if address_receiver is None:
            address_receiver = AddressReceiver(max_age=self._max_age, multicast_enabled=self._multicast_enabled, restrict_to_localhost=self._restrict_to_localhost)
            address_receiver.start()
        try:
            return self._ns.run(address_receiver, nameserver_address)
        finally:
            with suppress(AttributeError):
                address_receiver.stop()

    def stop(self):
        """"""Stop the nameserver.""""""
        return self._ns.stop()","
class NameServer:
    
    def __init__(self, max_age=None, multicast_enabled=True, restrict_to_localhost=False):
        
        pass
    def run(self, address_receiver=None, nameserver_address=None):
        
        pass

    def stop(self):
        
        pass",snippet_296,4,"```python
import json
import socket
import struct
import threading
import time
from typing import Callable, Dict, Optional, Tuple


class NameServer:
    """"""
    A very small, UDP‑based name server that supports optional multicast,
    optional restriction to localhost, and optional TTL for registrations.
    """"""

    def __init__(
        self,
        max_age: Optional[float] = None,
        multicast_enabled: bool = True,
        restrict_to_localhost: bool = False,
    ):
        """"""
        Parameters
        ----------
        max_age : float | None
            Maximum age (in seconds) of a registration before it is considered
            expired. If None, registrations never expire.
        multicast_enabled : bool
            If True, the server will join the multicast group 224.0.0.251
            (mDNS) and listen for multicast packets.
        restrict_to_localhost : bool
            If True, the server will only accept packets from the local host.
        """"""
        self.max_age = max_age
        self.multicast_enabled = multicast_enabled
        self.restrict_to_localhost = restrict_to_localhost

        # Internal state
        self._registry: Dict[str, Tuple[str, float]] = {}  # name -> (address, timestamp)
        self._sock: Optional[socket.socket] = None
        self._thread: Optional[threading.Thread] = None
        self._stop_event = threading.Event()

    # --------------------------------------------------------------------- #
    # Public API
    # --------------------------------------------------------------------- #

    def run(
        self,
        address_receiver: Optional[Callable[[bytes, Tuple[str, int]], None]] = None,
        nameserver_address: Optional[Tuple[str, int]] = None,
    ) -> None:
        """"""
        Start the name server.

        Parameters
        ----------
        address_receiver : callable | None
            Optional callback that receives raw data and the sender address.
            It is called *after* the server has processed the packet.
        nameserver_address : tuple | None
            Address to bind to. If None, defaults to ('0.0.0.0', 5353).
        """"""
        if self._thread and self._thread.is_alive():
            raise RuntimeError(""NameServer is already running"")

        bind_addr = nameserver_address or (""0.0.0.0"", 5353)
        self._sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)
        self._sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        self._sock.bind(bind_addr)

        if self.multicast_enabled:
            # Join the mDNS multicast group
            mcast_group = socket.inet_aton(""224.0.0.251"")
            mreq = mcast_group + socket.inet_aton(""0.0.0.0"")
            self._sock.setsockopt(socket.IPPROTO_IP, socket.IP_ADD_MEMBERSHIP, mreq)

        self._stop_event.clear()
        self._thread = threading.Thread(
            target=self._serve_loop,
            args=(address_receiver,),
            daemon=True,
        )
        self._thread.start()

    def stop(self) -> None:
        """"""
        Stop the name server and close the socket.
        """"""
        if not self._thread:
            return
        self._stop_event.set()
        # Send an empty packet to unblock the socket if it's waiting
        try:
            dummy_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            dummy_sock.sendto(b"""", (""127.0.0.1"", self._sock.getsockname()[1]))
            dummy_sock.close()
        except Exception:
            pass
        self._thread.join()
        self._thread = None
        if self._sock:
            self._sock.close()
            self._sock = None

    # --------------------------------------------------------------------- #
    # Internal helpers
    # --------------------------------------------------------------------- #

    def _serve_loop(self, address_receiver: Optional[Callable[[bytes, Tuple[str, int]], None]]) -> None:
        sock = self._sock
        if not sock:
            return

        while not self._stop_event.is_set():
            try:
                data, addr = sock.recvfrom(4096)
            except OSError:
                break

            if self.restrict_to_localhost and addr[0] != ""127.0.0.1"":
                continue

            try:
                msg = json.loads(data.decode(""utf-8""))
            except Exception:
                # Malformed packet; ignore
                continue

            action = msg.get(""action"")
            if action == ""register"":
                name = msg.get(""name"")
                address = msg.get(""address"")
                if name and address:
                    self._registry[name] = (address, time.time())
                    response = {""status"": ""ok""}
                else:
                    response = {""status"": ""error"", ""message"": ""invalid register payload""}
            elif action == ""lookup"":
                name = msg.get(""name"")
                if name:
                    entry = self._registry.get(name)
                    if entry:
                        address, ts = entry
                        if self.max_age is None or (time.time() - ts) <= self.max_age:
                            response = {""status"": ""ok"", ""address"": address}
                        else:
                            # Expired
                            del self._registry[name]
                            response = {""status"": ""error"", ""message"": ""name expired""}
                    else:
                        response = {""status"": ""error"", ""message"": ""name not found""}
                else:
                    response = {""status"": ""error"", ""message"": ""invalid lookup payload""}
            else:
                response = {""status"": ""error"", ""message"": ""unknown action""}

            # Send response
            try:
                sock.sendto(json.dumps(response).encode(""utf-8""), addr)
            except Exception:
                pass

            # Call optional receiver callback
            if address_receiver:
                try:
                    address_receiver(data, addr)
                except Exception:
                    pass
```"
357257,pytroll/posttroll,posttroll/publisher.py,posttroll.publisher.Publish,"class Publish:
    """"""The publishing context.

    See :class:`Publisher` and :class:`NoisyPublisher` for more information on the arguments.

    The publisher is selected based on the arguments, see :func:`create_publisher_from_dict_config` for
    information how the selection is done.

    Example on how to use the :class:`Publish` context::

            from posttroll.publisher import Publish
            from posttroll.message import Message
            import time

            try:
                with Publish(""my_service"", port=9000) as pub:
                    counter = 0
                    while True:
                        counter += 1
                        message = Message(""/counter"", ""info"", str(counter))
                        print(""publishing"", message)
                        pub.send(message.encode())
                        time.sleep(3)
            except KeyboardInterrupt:
                print(""terminating publisher..."")

    """"""

    def __init__(self, name, port=0, aliases=None, broadcast_interval=2, nameservers=None, min_port=None, max_port=None):
        """"""Initialize the class.""""""
        settings = {'name': name, 'port': port, 'min_port': min_port, 'max_port': max_port, 'aliases': aliases, 'broadcast_interval': broadcast_interval, 'nameservers': nameservers}
        self.publisher = create_publisher_from_dict_config(settings)

    def __enter__(self):
        """"""Enter the context.""""""
        return self.publisher.start()

    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""Exit the context.""""""
        self.publisher.stop()","class Publish:
    '''The publishing context.
    See :class:`Publisher` and :class:`NoisyPublisher` for more information on the arguments.
    The publisher is selected based on the arguments, see :func:`create_publisher_from_dict_config` for
    information how the selection is done.
    Example on how to use the :class:`Publish` context::
            from posttroll.publisher import Publish
            from posttroll.message import Message
            import time
            try:
                with Publish(""my_service"", port=9000) as pub:
                    counter = 0
                    while True:
                        counter += 1
                        message = Message(""/counter"", ""info"", str(counter))
                        print(""publishing"", message)
                        pub.send(message.encode())
                        time.sleep(3)
            except KeyboardInterrupt:
                print(""terminating publisher..."")
    '''
    def __init__(self, name, port=0, aliases=None, broadcast_interval=2, nameservers=None, min_port=None, max_port=None):
        
        pass

    def __enter__(self):
        
        pass

    def __exit__(self, exc_type, exc_val, exc_tb):
        
        pass",snippet_297,4,"```python
from posttroll.publisher import create_publisher_from_dict_config

class Publish:
    '''The publishing context.
    See :class:`Publisher` and :class:`NoisyPublisher` for more information on the arguments.
    The publisher is selected based on the arguments, see :func:`create_publisher_from_dict_config` for
    information how the selection is done.
    Example on how to use the :class:`Publish` context::
            from posttroll.publisher import Publish
            from posttroll.message import Message
            import time
            try:
                with Publish(""my_service"", port=9000) as pub:
                    counter = 0
                    while True:
                        counter += 1
                        message = Message(""/counter"", ""info"", str(counter))
                        print(""publishing"", message)
                        pub.send(message.encode())
                        time.sleep(3)
            except KeyboardInterrupt:
                print(""terminating publisher..."")
    '''
    def __init__(self, name, port=0, aliases=None, broadcast_interval=2,
                 nameservers=None, min_port=None, max_port=None):
        # Build configuration dictionary for the publisher
        self._config = {
            'name': name,
            'port': port,
            'aliases': aliases,
            'broadcast_interval': broadcast_interval,
            'nameservers': nameservers,
            'min_port': min_port,
            'max_port': max_port
        }
        self.publisher = None

    def __enter__(self):
        # Create the publisher instance from the configuration
        self.publisher = create_publisher_from_dict_config(self._config)
        # Start the publisher thread
        self.publisher.start()
        return self.publisher

    def __exit__(self, exc_type, exc_val, exc_tb):
        # Stop the publisher if it was started
        if self.publisher:
            try:
                self.publisher.stop()
            except Exception:
                pass
            try:
                self.publisher.join()
            except Exception:
                pass
        # Propagate any exception that occurred
        return False
```"
357261,pytroll/posttroll,posttroll/subscriber.py,posttroll.subscriber.Subscribe,"from posttroll.message import _MAGICK

class Subscribe:
    """"""Subscriber context.

    See :class:`NSSubscriber` and :class:`Subscriber` for initialization parameters.

    The subscriber is selected based on the arguments, see :func:`create_subscriber_from_dict_config` for
    information how the selection is done.

    Example::
            del tmp

        from posttroll.subscriber import Subscribe

        with Subscribe(""a_service"", ""my_topic"",) as sub:
            for msg in sub.recv():
                print(msg)

    """"""

    def __init__(self, services='', topics=_MAGICK, addr_listener=False, addresses=None, timeout=10, translate=False, nameserver='localhost', message_filter=None):
        """"""Initialize the class.""""""
        settings = {'services': services, 'topics': topics, 'message_filter': message_filter, 'translate': translate, 'addr_listener': addr_listener, 'addresses': addresses, 'timeout': timeout, 'nameserver': nameserver}
        self.subscriber = create_subscriber_from_dict_config(settings)

    def __enter__(self):
        """"""Start the subscriber when used as a context manager.""""""
        return self.subscriber

    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""Stop the subscriber when used as a context manager.""""""
        return self.subscriber.stop()","
class Subscribe:
    
    def __init__(self, services='', topics=_MAGICK, addr_listener=False, addresses=None, timeout=10, translate=False, nameserver='localhost', message_filter=None):
        
        pass
    def __enter__(self):
        
        pass

    def __exit__(self, exc_type, exc_val, exc_tb):
        
        pass",snippet_298,4,"```python
import threading
import time

# Sentinel value used to indicate that no specific topics were supplied
_MAGICK = object()


class Subscribe:
    """"""
    A simple context‑manager based subscription helper.

    Parameters
    ----------
    services : str, optional
        Comma separated list of services to subscribe to.
    topics : str or list, optional
        Topics to filter on. If omitted, all topics are accepted.
    addr_listener : bool, optional
        If True, a background thread will be started that pretends to
        listen for address updates. This is a no‑op placeholder.
    addresses : list, optional
        List of addresses to connect to. If None, a default address is used.
    timeout : int, optional
        Timeout in seconds for the subscription to be considered alive.
    translate : bool, optional
        If True, messages will be passed through a dummy translation step.
    nameserver : str, optional
        Name of the nameserver to use for service discovery.
    message_filter : callable, optional
        A callable that receives a message and returns True if the message
        should be processed.
    """"""

    def __init__(
        self,
        services="""",
        topics=_MAGICK,
        addr_listener=False,
        addresses=None,
        timeout=10,
        translate=False,
        nameserver=""localhost"",
        message_filter=None,
    ):
        # Store raw parameters
        self.services = services
        self._raw_topics = topics
        self.addr_listener = addr_listener
        self.addresses = addresses or [""tcp://localhost:5555""]
        self.timeout = timeout
        self.translate = translate
        self.nameserver = nameserver
        self.message_filter = message_filter

        # Normalise services and topics
        self.service_list = [s.strip() for s in services.split("","") if s.strip()]
        if topics is _MAGICK:
            self.topics = [""*""]
        elif isinstance(topics, str):
            self.topics = [t.strip() for t in topics.split("","") if t.strip()]
        else:
            self.topics = list(topics)

        # Internal state
        self._subscribed = False
        self._listener_thread = None
        self._stop_event = threading.Event()

    def __enter__(self):
        """"""
        Enter the runtime context related to this object.
        Sets up the subscription and starts the address listener if requested.
        """"""
        # Simulate subscription setup
        self._subscribed = True
        if self.addr_listener:
            self._start_listener()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""
        Exit the runtime context and clean up resources.
        """"""
        # Stop listener thread if running
        if self.addr_listener:
            self._stop_listener()
        # Simulate unsubscription
        self._subscribed = False
        # Returning False propagates any exception that occurred
        return False

    # ------------------------------------------------------------------
    # Helper methods for the dummy address listener
    # ------------------------------------------------------------------
    def _start_listener(self):
        """"""Start a background thread that pretends to listen for address updates.""""""
        if self._listener_thread and self._listener_thread.is_alive():
            return
        self._stop_event.clear()
        self._listener_thread = threading.Thread(
            target=self._listener_loop, daemon=True
        )
        self._listener_thread.start()

    def _stop_listener(self):
        """"""Signal the listener thread to stop and wait for it.""""""
        if self._listener_thread and self._listener_thread.is_alive():
            self._stop_event.set()
            self._listener_thread.join(timeout=self.timeout)
            self._listener_thread = None

    def _listener_loop(self):
        """"""Dummy loop that periodically prints a message.""""""
        while not self._stop_event.is_set():
            # In a real implementation this would handle address updates.
            time.sleep(1)

    # ------------------------------------------------------------------
    # Public API for sending/receiving messages (dummy)
    # ------------------------------------------------------------------
    def receive(self):
        """"""
        Dummy method that simulates receiving a message.
        Returns a dictionary with service, topic, and payload.
        """"""
        if not self._subscribed:
            raise RuntimeError(""Not subscribed"")
        # Simulate a message
        msg = {""service"": self.service_list[0] if self.service_list else None,
               ""topic"": self.topics[0] if self.topics else None,
               ""payload"": ""dummy payload""}
        # Apply translation if requested
        if self.translate:
            msg = self._translate(msg)
        # Apply message filter if provided
        if self.message_filter and not self.message_filter(msg):
            return None
        return msg

    def _translate(self, msg):
        """"""Dummy translation step.""""""
        msg[""payload""] = f""translated:{msg['payload']}""
        return msg
```"
357263,pytroll/posttroll,posttroll/subscriber.py,posttroll.subscriber._AddressListener,"from posttroll.address_receiver import get_configured_address_port

class _AddressListener:
    """"""Listener for new addresses of interest.""""""

    def __init__(self, subscriber, services='', nameserver='localhost'):
        """"""Initialize address listener.""""""
        if isinstance(services, str):
            services = [services]
        self.services = services
        self.subscriber = subscriber
        address_publish_port = get_configured_address_port()
        self.subscriber.add_hook_sub('tcp://' + nameserver + ':' + str(address_publish_port), ['pytroll://address'], self.handle_msg)

    def handle_msg(self, msg):
        """"""Handle the message *msg*.""""""
        addr_ = msg.data['URI']
        status = msg.data.get('status', True)
        if status:
            msg_services = msg.data.get('service')
            for service in self.services:
                if not service or service in msg_services:
                    LOGGER.debug('Adding address %s %s', str(addr_), str(service))
                    self.subscriber.add(addr_)
                    break
        else:
            LOGGER.debug('Removing address %s', str(addr_))
            self.subscriber.remove(addr_)","
class _AddressListener:
    
    def __init__(self, subscriber, services='', nameserver='localhost'):
        
        pass
    def handle_msg(self, msg):
        
        pass",snippet_299,3,"```python
import socket
from typing import Callable, Iterable, Optional, Union


class _AddressListener:
    """"""
    A simple address listener that forwards incoming messages to a subscriber
    while optionally filtering by service names and resolving hostnames.
    """"""

    def __init__(
        self,
        subscriber: Union[Callable[[dict], None], object],
        services: str = """",
        nameserver: str = ""localhost"",
    ):
        """"""
        Parameters
        ----------
        subscriber
            A callable or an object that will receive processed messages.
            If an object, it must provide a ``handle_msg`` or ``on_address`` method.
        services
            Comma‑separated list of service names to filter on. If empty, all
            services are accepted.
        nameserver
            Hostname of the DNS server to use for hostname resolution. This
            value is passed to ``socket.gethostbyname_ex``; if resolution fails,
            the message is passed unchanged.
        """"""
        self.subscriber = subscriber
        self.services = (
            [s.strip() for s in services.split("","") if s.strip()] if services else None
        )
        self.nameserver = nameserver

    def _resolve_hostname(self, hostname: str) -> Optional[str]:
        """"""
        Resolve a hostname to an IP address using the configured nameserver.
        Returns the first IP address found or None if resolution fails.
        """"""
        try:
            # ``socket.gethostbyname_ex`` uses the system resolver; we cannot
            # directly specify a nameserver without external libraries, so we
            # simply use the default resolver. The ``nameserver`` attribute is
            # kept for compatibility with potential future extensions.
            _, _, addrs = socket.gethostbyname_ex(hostname)
            return addrs[0] if addrs else None
        except Exception:
            return None

    def _dispatch(self, msg: dict) -> None:
        """"""
        Dispatch the message to the subscriber. Supports both callables and
        objects with ``handle_msg`` or ``on_address`` methods.
        """"""
        if callable(self.subscriber):
            self.subscriber(msg)
        else:
            if hasattr(self.subscriber, ""handle_msg""):
                self.subscriber.handle_msg(msg)
            elif hasattr(self.subscriber, ""on_address""):
                self.subscriber.on_address(msg)
            else:
                raise AttributeError(
                    ""Subscriber must be callable or provide a ""
                    ""'handle_msg' or 'on_address' method.""
                )

    def handle_msg(self, msg: dict) -> None:
        """"""
        Process an incoming message.

        The message is expected to be a dictionary that may contain the keys
        ``service`` and ``hostname``. If a ``service`` filter is configured,
        the message is ignored unless its ``service`` value matches one of the
        configured services. If a ``hostname`` key is present, the listener
        attempts to resolve it to an IP address and adds an ``ip`` key to the
        message.

        Parameters
        ----------
        msg
            The incoming message dictionary.
        """"""
        if not isinstance(msg, dict):
            raise TypeError(""Message must be a dictionary"")

        # Service filtering
        if self.services is not None:
            service = msg.get(""service"")
            if service not in self.services:
                return  # ignore message

        # Hostname resolution
        hostname = msg.get(""hostname"")
        if isinstance(hostname, str):
            ip = self._resolve_hostname(hostname)
            if ip:
                msg[""ip""] = ip

        # Dispatch to subscriber
        self._dispatch(msg)
```"
360979,reanahub/reana-commons,reanahub_reana-commons/reana_commons/validation/compute_backends.py,reana_commons.validation.compute_backends.ComputeBackendValidatorBase,"from typing import Dict, List, Optional
from reana_commons.errors import REANAValidationError

class ComputeBackendValidatorBase:
    """"""REANA workflow compute backend validation base class.""""""

    def __init__(self, workflow_steps: Optional[List[Dict]]=None, supported_backends: Optional[List[str]]=[]):
        """"""Validate compute backends in REANA workflow steps.

        :param workflow_steps: list of dictionaries which represents different steps involved in workflow.
        :param supported_backends: a list of the supported compute backends.
        """"""
        self.workflow_steps = workflow_steps
        self.supported_backends = supported_backends

    def validate(self) -> None:
        """"""Validate compute backends in REANA workflow.""""""
        raise NotImplementedError

    def raise_error(self, compute_backend: str, step_name: str) -> None:
        """"""Raise validation error.""""""
        raise REANAValidationError(f'''Compute backend ""{compute_backend}"" found in step ""{step_name}"" is not supported. List of supported compute backends: ""{', '.join(self.supported_backends)}""''')","
class ComputeBackendValidatorBase:
    
    def __init__(self, workflow_steps: Optional[List[Dict]]=None, supported_backends: Optional[List[str]]=[]):
        
        pass
    def validate(self) -> None:
        
        pass

    def raise_error(self, compute_backend: str, step_name: str) -> None:
        
        pass",snippet_300,4,"```python
from typing import Optional, List, Dict


class ComputeBackendValidatorBase:
    """"""
    Base class for validating compute backends used in workflow steps.
    """"""

    def __init__(
        self,
        workflow_steps: Optional[List[Dict]] = None,
        supported_backends: Optional[List[str]] = None,
    ):
        """"""
        Parameters
        ----------
        workflow_steps : Optional[List[Dict]]
            A list of dictionaries, each representing a workflow step.
            Each step dictionary is expected to contain at least the keys
            'name' (or 'step_name') and 'compute_backend'.
        supported_backends : Optional[List[str]]
            A list of compute backend names that are considered valid.
        """"""
        self.workflow_steps: List[Dict] = workflow_steps or []
        self.supported_backends: List[str] = supported_backends or []

    def validate(self) -> None:
        """"""
        Validate that every workflow step uses a supported compute backend.
        Raises a ValueError if an unsupported backend is found.
        """"""
        for step in self.workflow_steps:
            # Determine step name
            step_name = step.get(""name"") or step.get(""step_name"") or ""<unknown>""
            # Determine compute backend
            compute_backend = step.get(""compute_backend"") or step.get(""backend"")
            if compute_backend is None:
                # If no backend specified, skip or treat as error?
                # Here we treat missing backend as an error.
                self.raise_error(""<missing>"", step_name)
                continue

            if compute_backend not in self.supported_backends:
                self.raise_error(compute_backend, step_name)

    def raise_error(self, compute_backend: str, step_name: str) -> None:
        """"""
        Raise a ValueError indicating that the given compute backend is not supported
        for the specified step.

        Parameters
        ----------
        compute_backend : str
            The compute backend that was found in the step.
        step_name : str
            The name of the step where the unsupported backend was found.
        """"""
        supported = "", "".join(self.supported_backends) or ""none""
        message = (
            f""Unsupported compute backend '{compute_backend}' for step '{step_name}'. ""
            f""Supported backends: {supported}.""
        )
        raise ValueError(message)
```"
361134,raymondEhlers/pachyderm,raymondEhlers_pachyderm/src/pachyderm/fit/base.py,pachyderm.fit.base.BaseFitResult,"import numpy as np
import numpy.typing as npt
from dataclasses import dataclass
from typing import TYPE_CHECKING, Any, TypeVar, cast

@dataclass
class BaseFitResult:
    """"""Base fit result.

    This represents the most basic fit result.

    Attributes:
        parameters: Names of the parameters used in the fit.
        free_parameters: Names of the free parameters used in the fit.
        fixed_parameters: Names of the fixed parameters used in the fit.
        values_at_minimum: Contains the values of the full RP fit function at the minimum. Keys are the
            names of parameters, while values are the numerical values at convergence.
        errors_on_parameters: Contains the values of the errors associated with the parameters
            determined via the fit.
        covariance_matrix: Contains the values of the covariance matrix. Keys are tuples
            with (param_name_a, param_name_b), and the values are covariance between the specified parameters.
            Note that fixed parameters are _not_ included in this matrix.
        errors: Store the errors associated with the component fit function.
    """"""
    parameters: list[str]
    free_parameters: list[str]
    fixed_parameters: list[str]
    values_at_minimum: dict[str, float]
    errors_on_parameters: dict[str, float]
    covariance_matrix: dict[tuple[str, str], float]
    errors: npt.NDArray[Any]

    @property
    def correlation_matrix(self) -> dict[tuple[str, str], float]:
        """"""The correlation matrix of the free parameters.

        These values are derived from the covariance matrix values stored in the fit.

        Note:
            This property caches the correlation matrix value so we don't have to calculate it every time.

        Args:
            None
        Returns:
            The correlation matrix of the fit result.
        """"""
        try:
            return self._correlation_matrix
        except AttributeError:

            def corr(i_name: str, j_name: str) -> float:
                """"""Calculate the correlation matrix (definition from iminuit) from the covariance matrix.""""""
                value = self.covariance_matrix[i_name, j_name] / (np.sqrt(self.covariance_matrix[i_name, i_name] * self.covariance_matrix[j_name, j_name]) + 1e-100)
                return float(value)
            matrix: dict[tuple[str, str], float] = {}
            for i_name in self.free_parameters:
                for j_name in self.free_parameters:
                    matrix[i_name, j_name] = corr(i_name, j_name)
            self._correlation_matrix = matrix
        return self._correlation_matrix","@dataclass
class BaseFitResult:
    
    @property
    def correlation_matrix(self) -> dict[tuple[str, str], float]:
        
        pass
    def correlation_matrix(self) -> dict[tuple[str, str], float]:
        
        pass",snippet_301,3,
361458,AtteqCom/zsl,AtteqCom_zsl/src/zsl/errors.py,zsl.errors.ErrorHandler,"from abc import ABCMeta, abstractmethod

class ErrorHandler:
    """"""
    Custom error handler providing a response on a particular error.
    """"""
    __metaclass__ = ABCMeta

    @abstractmethod
    def can_handle(self, e):
        """"""
        Indicator if the handler is able to handle the given exception `e`.

        :param e: The exception that shall be determined if can be handled by the handler.
        :return: `True` or `False` depending on whether the handler can/should handle the method.
        """"""
        pass

    @abstractmethod
    def handle(self, e):
        """"""
        Handle the exception.

        :param e: The handled exception.
        :return: The error response for the exception.
        """"""
        pass","
class ErrorHandler:
    
    @abstractmethod
    def can_handle(self, e):
        
        pass
    @abstractmethod
    def handle(self, e):
        
        pass",snippet_302,3,"```python
from abc import ABC, abstractmethod

class ErrorHandler(ABC):
    
    @abstractmethod
    def can_handle(self, e):
        """"""Return True if this handler can handle the exception `e`.""""""
        pass

    @abstractmethod
    def handle(self, e):
        """"""Handle the exception `e`.""""""
        pass
```"
361483,AtteqCom/zsl,AtteqCom_zsl/src/zsl/resource/model_resource.py,zsl.resource.model_resource.ReadOnlyResourceMixin,"class ReadOnlyResourceMixin:
    """"""
    The mixin to be used to forbid the update/delete and create operations.
    Remember the Python's MRO and place this mixin at the right place in the inheritance declaration.

    .. automethod:: create
    .. automethod:: update
    .. automethod:: delete
    """"""
    OPERATION_CREATE = 'create'
    OPERATION_UPDATE = 'update'
    OPERATION_DELETE = 'delete'

    @staticmethod
    def create(params, args, data):
        """"""Raises exception.

        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.

        :raises ReadOnlyResourceUpdateOperationException: when accessed
        """"""
        raise ReadOnlyResourceUpdateOperationException(ReadOnlyResourceMixin.OPERATION_CREATE)

    @staticmethod
    def update(params, args, data):
        """"""Raises exception.

        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.

        :raises ReadOnlyResourceUpdateOperationException: when accessed
        """"""
        raise ReadOnlyResourceUpdateOperationException(ReadOnlyResourceMixin.OPERATION_UPDATE)

    @staticmethod
    def delete(params, args, data):
        """"""Raises exception.

        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.

        :raises ReadOnlyResourceUpdateOperationException: when accessed
        """"""
        raise ReadOnlyResourceUpdateOperationException(ReadOnlyResourceMixin.OPERATION_DELETE)","class ReadOnlyResourceMixin:
    
    @staticmethod
    def create(params, args, data):
        
        pass
    @staticmethod
    def update(params, args, data):
        
        pass
    @staticmethod
    def delete(params, args, data):
        
        pass",snippet_303,4,"class ReadOnlyResourceMixin:
    
    @staticmethod
    def create(params, args, data):
        raise NotImplementedError(""Create operation is not allowed on read-only resources."")
    
    @staticmethod
    def update(params, args, data):
        raise NotImplementedError(""Update operation is not allowed on read-only resources."")
    
    @staticmethod
    def delete(params, args, data):
        raise NotImplementedError(""Delete operation is not allowed on read-only resources."")"
361534,AtteqCom/zsl,AtteqCom_zsl/src/zsl/utils/command_dispatcher.py,zsl.utils.command_dispatcher.CommandDispatcher,"import inspect

class CommandDispatcher:
    """"""
    A simple class for command dictionary. A command is a function
    which can take named parameters.
    """"""

    def __init__(self):
        """"""
        Create command dictionary
        """"""
        self.commands = {}

    def command(self, fn):
        """"""
        Add method or function to dispatcher. Can be use as a nice
        decorator.

        :param fn: function or method
        :type fn: function
        :return: the same function
        :rtype: function
        """"""
        self.commands[fn.__name__] = fn
        return fn
    'alias for ``CommandDispatcher.command``'
    add_function = command

    def execute_command(self, command, args=None):
        """"""
        Execute a command

        :param command: name of the command
        :type command: str
        :param args: optional named arguments for command
        :type args: dict
        :return: the result of command
        :raises KeyError: if command is not found
        """"""
        if args is None:
            args = {}
        command_fn = self.commands[command]
        return command_fn(**args)

    def bound(self, instance):
        """"""
        Return a new dispatcher, which will switch all command functions
        with bounded methods of given instance matched by name. It will
        match only regular methods.

        :param instance: object instance
        :type instance: object
        :return: new Dispatcher
        :rtype: CommandDispatcher
        """"""
        bounded_dispatcher = CommandDispatcher()
        bounded_dispatcher.commands = self.commands.copy()
        for name in self.commands:
            method = getattr(instance, name, None)
            if method and inspect.ismethod(method) and (method.__self__ == instance):
                bounded_dispatcher.commands[name] = method
        return bounded_dispatcher","
class CommandDispatcher:
    
    def __init__(self):
        
        pass
    def command(self, fn):
        
        pass

    def execute_command(self, command, args=None):
        
        pass

    def bound(self, instance):
        
        pass",snippet_304,5,"```python
import types
from collections.abc import Mapping, Sequence

class CommandDispatcher:
    def __init__(self):
        # Mapping of command name to callable
        self.commands = {}

    def command(self, fn):
        """"""
        Decorator to register a function as a command.
        The command name defaults to the function's __name__.
        """"""
        name = fn.__name__
        self.commands[name] = fn
        return fn

    def execute_command(self, command, args=None):
        """"""
        Execute a registered command by name or callable.
        `args` can be:
            - None: call with no arguments
            - Sequence (list/tuple): passed as positional arguments
            - Mapping (dict): passed as keyword arguments
        """"""
        # Resolve command to callable
        if isinstance(command, str):
            fn = self.commands.get(command)
            if fn is None:
                raise KeyError(f""Command '{command}' not found"")
        elif callable(command):
            fn = command
        else:
            raise TypeError(""command must be a string or callable"")

        # Call with appropriate arguments
        if args is None:
            return fn()
        if isinstance(args, Mapping):
            return fn(**args)
        if isinstance(args, Sequence) and not isinstance(args, (str, bytes)):
            return fn(*args)
        # Fallback: treat as single positional argument
        return fn(args)

    def bound(self, instance):
        """"""
        Return a new CommandDispatcher where each command is bound to the given instance.
        Useful for dispatching instance methods.
        """"""
        bound_dispatcher = CommandDispatcher()
        for name, fn in self.commands.items():
            # Bind the function to the instance
            bound_fn = types.MethodType(fn, instance)
            bound_dispatcher.commands[name] = bound_fn
        return bound_dispatcher
```"
362561,reanahub/reana-db,reanahub_reana-db/reana_db/models.py,reana_db.models.QuotaBase,"class QuotaBase:
    """"""Quota base functionality.""""""

    def _get_quota_by_type(self, resource_type):
        """"""Aggregate quota usage by resource type.""""""

        def _get_health_status(usage, limit):
            """"""Calculate quota health status.""""""
            health = QuotaHealth.healthy
            if limit:
                percentage = usage / limit * 100
                if percentage >= 80:
                    if percentage >= 100:
                        health = QuotaHealth.critical
                    else:
                        health = QuotaHealth.warning
            return health.name
        quota_usage = 0
        quota_limit = 0
        unit = None
        for resource in self.resources:
            if resource.resource.type_ == resource_type:
                if unit and unit != resource.resource.unit:
                    raise Exception('Error while calculating quota usage. Not all resources of resource type {} use the same units.'.format(resource_type))
                unit = resource.resource.unit
                quota_usage += resource.quota_used
                if hasattr(resource, 'quota_limit'):
                    quota_limit += resource.quota_limit
        usage_dict = {'usage': {'raw': quota_usage, 'human_readable': ResourceUnit.human_readable_unit(unit, quota_usage)}}
        if quota_limit:
            usage_dict['limit'] = {'raw': quota_limit, 'human_readable': ResourceUnit.human_readable_unit(unit, quota_limit)}
            usage_dict['health'] = _get_health_status(quota_usage, quota_limit)
        return usage_dict

    def get_quota_usage(self):
        """"""Get quota usage information.""""""
        used_resource_types = set((res.resource.type_ for res in self.resources))
        return {resource_type.name: self._get_quota_by_type(resource_type) for resource_type in used_resource_types}","class QuotaBase:
    
    def _get_quota_by_type(self, resource_type):
        
        pass
        def _get_health_status(usage, limit):
                
                pass
    def get_quota_usage(self):
        
        pass",snippet_305,4,"```python
class QuotaBase:
    def __init__(self, quota_data=None):
        """"""
        Initialize the QuotaBase with optional quota data.

        :param quota_data: A dictionary mapping resource types to a dict
                           containing 'usage' and 'limit' keys.
        """"""
        self.quota_data = quota_data or {}

    def _get_quota_by_type(self, resource_type):
        """"""
        Retrieve the quota information for a specific resource type.

        :param resource_type: The type of resource to look up.
        :return: A dictionary with 'usage' and 'limit' keys.
        """"""
        return self.quota_data.get(resource_type, {""usage"": 0, ""limit"": 0})

    def _get_health_status(self, usage, limit):
        """"""
        Determine the health status based on usage and limit.

        :param usage: Current usage value.
        :param limit: Maximum allowed value.
        :return: A string representing the health status ('green', 'yellow', 'red', or 'unknown').
        """"""
        if limit == 0:
            return ""unknown""
        usage_ratio = usage / limit
        if usage_ratio < 0.5:
            return ""green""
        if usage_ratio < 0.8:
            return ""yellow""
        return ""red""

    def get_quota_usage(self):
        """"""
        Compile a comprehensive report of quota usage for all resource types.

        :return: A dictionary mapping each resource type to its usage, limit, and health status.
        """"""
        report = {}
        for resource_type, data in self.quota_data.items():
            usage = data.get(""usage"", 0)
            limit = data.get(""limit"", 0)
            status = self._get_health_status(usage, limit)
            report[resource_type] = {
                ""usage"": usage,
                ""limit"": limit,
                ""status"": status,
            }
        return report
```"
