id,repository_name,file_path,class_name,human_written_code,class_skeleton,snippet_id,total_program_units,gpt-5_generated_code
17249,jbarlow83/OCRmyPDF,src/ocrmypdf/quality.py,ocrmypdf.quality.OcrQualityDictionary,"import re
from collections.abc import Iterable

class OcrQualityDictionary:
    """"""Manages a dictionary for simple OCR quality checks.""""""

    def __init__(self, *, wordlist: Iterable[str]):
        """"""Construct a dictionary from a list of words.

        Words for which capitalization is important should be capitalized in the
        dictionary. Words that contain spaces or other punctuation will never match.
        """"""
        self.dictionary = set(wordlist)

    def measure_words_matched(self, ocr_text: str) -> float:
        """"""Check how many unique words in the OCR text match a dictionary.

        Words with mixed capitalized are only considered a match if the test word
        matches that capitalization.

        Returns:
            number of words that match / number
        """"""
        text = re.sub('[0-9_]+', ' ', ocr_text)
        text = re.sub('\\W+', ' ', text)
        text_words_list = re.split('\\s+', text)
        text_words = {w for w in text_words_list if len(w) >= 3}
        matches = 0
        for w in text_words:
            if w in self.dictionary or (w != w.lower() and w.lower() in self.dictionary):
                matches += 1
        if matches > 0:
            hit_ratio = matches / len(text_words)
        else:
            hit_ratio = 0.0
        return hit_ratio","
class OcrQualityDictionary:
    
    def __init__(self, *, wordlist: Iterable[str]):
        
        pass
    def measure_words_matched(self, ocr_text: str) -> float:
        
        pass",snippet_2,3,"from typing import Iterable, Set
import re
import unicodedata


class OcrQualityDictionary:
    def __init__(self, *, wordlist: Iterable[str]):
        self._dict: Set[str] = set()
        for w in wordlist:
            if w is None:
                continue
            nw = self._normalize_word(str(w))
            if nw:
                self._dict.add(nw)

        # precompile regex for tokenization (unicode word chars + apostrophes)
        self._token_re = re.compile(r""\b[\w']+\b"", re.UNICODE)

    def _normalize_word(self, w: str) -> str:
        w = unicodedata.normalize(""NFKC"", w).casefold().strip()
        # strip leading/trailing apostrophes commonly appearing in OCR
        w = w.strip(""'’`"")
        return w

    def _tokenize(self, text: str):
        for m in self._token_re.finditer(text):
            tok = self._normalize_word(m.group(0))
            if not tok:
                continue
            # consider tokens that include at least one letter to avoid counting pure numbers/underscores
            if any(c.isalpha() for c in tok):
                yield tok

    def measure_words_matched(self, ocr_text: str) -> float:
        tokens = list(self._tokenize(ocr_text))
        if not tokens or not self._dict:
            return 0.0
        matched = sum(1 for t in tokens if t in self._dict)
        return matched / len(tokens)"
19627,slundberg/shap,slundberg_shap/shap/_serializable.py,shap._serializable.Serializable,"import inspect
import pickle

class Serializable:
    """"""This is the superclass of all serializable objects.""""""

    def save(self, out_file):
        """"""Save the model to the given file stream.""""""
        pickle.dump(type(self), out_file)

    @classmethod
    def load(cls, in_file, instantiate=True):
        """"""This is meant to be overridden by subclasses and called with super.

        We return constructor argument values when not being instantiated. Since there are no
        constructor arguments for the Serializable class we just return an empty dictionary.
        """"""
        if instantiate:
            return cls._instantiated_load(in_file)
        return {}

    @classmethod
    def _instantiated_load(cls, in_file, **kwargs):
        """"""This is meant to be overridden by subclasses and called with super.

        We return constructor argument values (we have no values to load in this abstract class).
        """"""
        obj_type = pickle.load(in_file)
        if obj_type is None:
            return None
        if not inspect.isclass(obj_type) or (not issubclass(obj_type, cls) and obj_type is not cls):
            raise Exception(f'Invalid object type loaded from file. {obj_type} is not a subclass of {cls}.')
        constructor_args = obj_type.load(in_file, instantiate=False, **kwargs)
        used_args = inspect.getfullargspec(obj_type.__init__)[0]
        return obj_type(**{k: constructor_args[k] for k in constructor_args if k in used_args})","
class Serializable:
    
    def save(self, out_file):
        
        pass
    @classmethod
    def load(cls, in_file, instantiate=True):
        
        pass
    @classmethod
    def _instantiated_load(cls, in_file, **kwargs):
        
        pass",snippet_3,4,"class Serializable:
    def save(self, out_file):
        import pickle
        import io

        data = {
            ""module"": self.__class__.__module__,
            ""class"": self.__class__.__name__,
            ""state"": self.__getstate__() if hasattr(self, ""__getstate__"") else dict(getattr(self, ""__dict__"", {})),
        }

        if isinstance(out_file, (str, bytes, bytearray)):
            # If it's a path (str) or buffer (bytes/bytearray)
            if isinstance(out_file, str):
                with open(out_file, ""wb"") as f:
                    pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
            else:
                # bytes/bytearray -> treat as buffer and return bytes
                bio = io.BytesIO()
                pickle.dump(data, bio, protocol=pickle.HIGHEST_PROTOCOL)
                if isinstance(out_file, bytearray):
                    out_file[:] = bio.getvalue()
                else:
                    # bytes are immutable; return bytes to caller by raising TypeError is unhelpful.
                    # Instead, do nothing; caller should provide a writable file-like for bytes.
                    pass
        elif hasattr(out_file, ""write""):
            pickle.dump(data, out_file, protocol=pickle.HIGHEST_PROTOCOL)
        else:
            raise TypeError(""out_file must be a filepath string or a writable binary file-like object"")

    @classmethod
    def load(cls, in_file, instantiate=True):
        import pickle
        import io

        def _read_data(source):
            if isinstance(source, (bytes, bytearray)):
                return pickle.loads(bytes(source))
            if isinstance(source, str):
                with open(source, ""rb"") as f:
                    return pickle.load(f)
            if hasattr(source, ""read""):
                return pickle.load(source)
            raise TypeError(""in_file must be a filepath string, bytes/bytearray, or a readable binary file-like object"")

        if not instantiate:
            return _read_data(in_file)
        return cls._instantiated_load(in_file)

    @classmethod
    def _instantiated_load(cls, in_file, **kwargs):
        import pickle
        import importlib
        import io

        def _read_data(source):
            if isinstance(source, (bytes, bytearray)):
                return pickle.loads(bytes(source))
            if isinstance(source, str):
                with open(source, ""rb"") as f:
                    return pickle.load(f)
            if hasattr(source, ""read""):
                return pickle.load(source)
            raise TypeError(""in_file must be a filepath string, bytes/bytearray, or a readable binary file-like object"")

        data = _read_data(in_file)
        state = kwargs.get(""state"", data.get(""state"", {}))
        module_name = data.get(""module"")
        class_name = data.get(""class"")

        target_cls = cls
        if module_name and class_name:
            try:
                mod = importlib.import_module(module_name)
                loaded_cls = getattr(mod, class_name, None)
                if isinstance(loaded_cls, type):
                    target_cls = loaded_cls
            except Exception:
                pass

        obj = target_cls.__new__(target_cls)
        if hasattr(obj, ""__setstate__""):
            obj.__setstate__(state)
        else:
            if hasattr(obj, ""__dict__"") and isinstance(state, dict):
                obj.__dict__.update(state)
        return obj"
19720,audreyr/cookiecutter,audreyr_cookiecutter/cookiecutter/environment.py,cookiecutter.environment.ExtensionLoaderMixin,"from cookiecutter.exceptions import UnknownExtension
from typing import Any

class ExtensionLoaderMixin:
    """"""Mixin providing sane loading of extensions specified in a given context.

    The context is being extracted from the keyword arguments before calling
    the next parent class in line of the child.
    """"""

    def __init__(self, *, context: dict[str, Any] | None=None, **kwargs: Any) -> None:
        """"""Initialize the Jinja2 Environment object while loading extensions.

        Does the following:

        1. Establishes default_extensions (currently just a Time feature)
        2. Reads extensions set in the cookiecutter.json _extensions key.
        3. Attempts to load the extensions. Provides useful error if fails.
        """"""
        context = context or {}
        default_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']
        extensions = default_extensions + self._read_extensions(context)
        try:
            super().__init__(extensions=extensions, **kwargs)
        except ImportError as err:
            msg = f'Unable to load extension: {err}'
            raise UnknownExtension(msg) from err

    def _read_extensions(self, context: dict[str, Any]) -> list[str]:
        """"""Return list of extensions as str to be passed on to the Jinja2 env.

        If context does not contain the relevant info, return an empty
        list instead.
        """"""
        try:
            extensions = context['cookiecutter']['_extensions']
        except KeyError:
            return []
        else:
            return [str(ext) for ext in extensions]","
class ExtensionLoaderMixin:
    
    def __init__(self, *, context: dict[str, Any] | None=None, **kwargs: Any) -> None:
        
        pass
    def _read_extensions(self, context: dict[str, Any]) -> list[str]:
        
        pass",snippet_5,3,"from typing import Any, Iterable

class ExtensionLoaderMixin:
    
    def __init__(self, *, context: dict[str, Any] | None = None, **kwargs: Any) -> None:
        # Be cooperative in multiple-inheritance scenarios
        try:
            super().__init__(**kwargs)  # type: ignore[misc]
        except TypeError:
            # Parent may not accept kwargs or have no __init__
            pass

        self._context: dict[str, Any] = dict(context) if context else {}
        self.extensions: list[str] = self._read_extensions(self._context)

    def _read_extensions(self, context: dict[str, Any]) -> list[str]:
        def to_list(value: Any) -> list[str]:
            if value is None:
                return []
            if isinstance(value, str):
                parts = [p.strip() for p in value.replace(""\n"", "" "").split("","")]
                flat: list[str] = []
                for part in parts:
                    if not part:
                        continue
                    flat.extend([x for x in part.split() if x])
                return flat
            if isinstance(value, dict):
                raise TypeError(""Extensions cannot be provided as a dict."")
            if isinstance(value, Iterable):
                out: list[str] = []
                for v in value:
                    s = str(v).strip()
                    if s:
                        out.append(s)
                return out
            return [str(value).strip()] if str(value).strip() else []

        # Prefer 'extensions', fall back to 'extension'
        raw = None
        if ""extensions"" in context:
            raw = context.get(""extensions"")
        elif ""extension"" in context:
            raw = context.get(""extension"")

        items = to_list(raw)
        # Deduplicate while preserving order
        seen: set[str] = set()
        result: list[str] = []
        for item in items:
            if item not in seen:
                seen.add(item)
                result.append(item)
        return result"
21340,Delgan/loguru,loguru/_recattrs.py,loguru._recattrs.RecordFile,"class RecordFile:
    """"""A class representing a file record with name and path.

    Attributes
    ----------
    name : str
        The name of the file
    path : str
        The path to the file
    """"""
    __slots__ = ('name', 'path')

    def __init__(self, name, path):
        """"""Initialize a RecordFile instance.

        Parameters
        ----------
        name : str
            The name of the file
        path : str
            The path to the file
        """"""
        self.name = name
        self.path = path

    def __repr__(self):
        """"""Return string representation of RecordFile.

        Returns
        -------
        str
            Formatted string with name and path
        """"""
        return '(name=%r, path=%r)' % (self.name, self.path)

    def __format__(self, spec):
        """"""Format the RecordFile instance.

        Parameters
        ----------
        spec : str
            Format specification

        Returns
        -------
        str
            Formatted name according to specification
        """"""
        return self.name.__format__(spec)","class RecordFile:
    
    def __init__(self, name, path):
        
        pass
    def __repr__(self):
        
        pass
    def __format__(self, spec):
        
        pass",snippet_6,4,"class RecordFile:
    
    def __init__(self, name, path):
        from pathlib import Path
        self.name = str(name)
        self.path = Path(path)

    def __repr__(self):
        return f""RecordFile(name={self.name!r}, path={str(self.path)!r})""

    def __format__(self, spec):
        s = spec.strip()
        if not s:
            return str(self)
        key = s.lower()
        if key in (""n"", ""name""):
            return self.name
        if key in (""p"", ""path""):
            return str(self.path)
        if key in (""f"", ""full"", ""fullpath"", ""filepath""):
            return str(self.path / self.name)
        return str(self)"
21341,Delgan/loguru,loguru/_recattrs.py,loguru._recattrs.RecordLevel,"class RecordLevel:
    """"""A class representing the logging level record with name, number and icon.

    Attributes
    ----------
    icon : str
        The icon representing the log level
    name : str
        The name of the log level
    no : int
        The numeric value of the log level
    """"""
    __slots__ = ('icon', 'name', 'no')

    def __init__(self, name, no, icon):
        """"""Initialize a RecordLevel instance.

        Parameters
        ----------
        name : str
            The name of the log level
        no : int
            The numeric value of the log level
        icon : str
            The icon representing the log level
        """"""
        self.name = name
        self.no = no
        self.icon = icon

    def __repr__(self):
        """"""Return string representation of RecordLevel.

        Returns
        -------
        str
            Formatted string with name, number and icon
        """"""
        return '(name=%r, no=%r, icon=%r)' % (self.name, self.no, self.icon)

    def __format__(self, spec):
        """"""Format the RecordLevel instance.

        Parameters
        ----------
        spec : str
            Format specification

        Returns
        -------
        str
            Formatted name according to specification
        """"""
        return self.name.__format__(spec)","class RecordLevel:
    
    def __init__(self, name, no, icon):
        
        pass
    def __repr__(self):
        
        pass
    def __format__(self, spec):
        
        pass",snippet_7,4,"class RecordLevel:
    
    def __init__(self, name, no, icon):
        self.name = name
        self.no = no
        self.icon = icon

    def __repr__(self):
        return f""RecordLevel(name={self.name!r}, no={self.no!r}, icon={self.icon!r})""

    def __format__(self, spec):
        if not spec:
            return repr(self)
        if spec in ('name', 'n'):
            return format(self.name, '')
        if spec in ('no', 'd'):
            return format(self.no, '')
        if spec in ('icon', 'i'):
            return format(self.icon, '')
        if '{' in spec and '}' in spec:
            try:
                return spec.format(name=self.name, no=self.no, icon=self.icon)
            except Exception:
                pass
        return format(repr(self), spec)"
21342,Delgan/loguru,loguru/_recattrs.py,loguru._recattrs.RecordProcess,"class RecordProcess:
    """"""A class representing a process record with ID and name.

    Attributes
    ----------
    id : int
        The process ID
    name : str
        The process name
    """"""
    __slots__ = ('id', 'name')

    def __init__(self, id_, name):
        """"""Initialize a RecordProcess instance.

        Parameters
        ----------
        id_ : int
            The process ID
        name : str
            The process name
        """"""
        self.id = id_
        self.name = name

    def __repr__(self):
        """"""Return string representation of RecordProcess.

        Returns
        -------
        str
            Formatted string with id and name
        """"""
        return '(id=%r, name=%r)' % (self.id, self.name)

    def __format__(self, spec):
        """"""Format the RecordProcess instance.

        Parameters
        ----------
        spec : str
            Format specification

        Returns
        -------
        str
            Formatted ID according to specification
        """"""
        return self.id.__format__(spec)","class RecordProcess:
    
    def __init__(self, id_, name):
        
        pass
    def __repr__(self):
        
        pass
    def __format__(self, spec):
        
        pass",snippet_8,4,"class RecordProcess:
    
    def __init__(self, id_, name):
        self.id_ = id_
        self.name = name

    def __repr__(self):
        return f""RecordProcess(id_={self.id_!r}, name={self.name!r})""

    def __format__(self, spec):
        if not spec:
            return f""{self.name}({self.id_})""
        if spec in (""r"", ""repr""):
            return repr(self)
        if spec.startswith(""name""):
            sub = spec[4:]
            if sub.startswith("":""):
                sub = sub[1:]
            return format(self.name, sub)
        if spec.startswith(""id""):
            sub = spec[2:]
            if sub.startswith("":""):
                sub = sub[1:]
            return format(self.id_, sub)
        return format(f""{self.name}({self.id_})"", spec)"
21343,Delgan/loguru,loguru/_recattrs.py,loguru._recattrs.RecordThread,"class RecordThread:
    """"""A class representing a thread record with ID and name.

    Attributes
    ----------
    id : int
        The thread ID
    name : str
        The thread name
    """"""
    __slots__ = ('id', 'name')

    def __init__(self, id_, name):
        """"""Initialize a RecordThread instance.

        Parameters
        ----------
        id_ : int
            The thread ID
        name : str
            The thread name
        """"""
        self.id = id_
        self.name = name

    def __repr__(self):
        """"""Return string representation of RecordThread.

        Returns
        -------
        str
            Formatted string with id and name
        """"""
        return '(id=%r, name=%r)' % (self.id, self.name)

    def __format__(self, spec):
        """"""Format the RecordThread instance.

        Parameters
        ----------
        spec : str
            Format specification

        Returns
        -------
        str
            Formatted ID according to specification
        """"""
        return self.id.__format__(spec)","class RecordThread:
    
    def __init__(self, id_, name):
        
        pass
    def __repr__(self):
        
        pass
    def __format__(self, spec):
        
        pass",snippet_9,4,"class RecordThread:
    
    def __init__(self, id_, name):
        self.id = id_
        self.name = name

    def __repr__(self):
        return f""RecordThread(id_={self.id!r}, name={self.name!r})""

    def __format__(self, spec):
        if spec is None:
            spec = """"
        if spec == """" or spec in (""r"", ""repr""):
            return str(self)
        if spec in (""n"", ""name""):
            return format(self.name, """")
        if spec.startswith(""n:"") or spec.startswith(""name:""):
            _, inner = spec.split("":"", 1)
            return format(self.name, inner)
        if spec in (""i"", ""id""):
            return format(self.id, """")
        if spec.startswith(""i:"") or spec.startswith(""id:""):
            _, inner = spec.split("":"", 1)
            return format(self.id, inner)
        return format(str(self), spec)"
33971,Miserlou/Zappa,Miserlou_Zappa/zappa/asynchronous.py,zappa.asynchronous.LambdaAsyncResponse,"import json
import uuid

class LambdaAsyncResponse:
    """"""
    Base Response Dispatcher class
    Can be used directly or subclassed if the method to send the message is changed.
    """"""

    def __init__(self, lambda_function_name=None, aws_region=None, capture_response=False, **kwargs):
        """""" """"""
        if kwargs.get('boto_session'):
            self.client = kwargs.get('boto_session').client('lambda')
        else:
            self.client = LAMBDA_CLIENT
        self.lambda_function_name = lambda_function_name
        self.aws_region = aws_region
        if capture_response:
            if ASYNC_RESPONSE_TABLE is None:
                print(""Warning! Attempted to capture a response without async_response_table configured in settings (you won't capture async responses)."")
                capture_response = False
                self.response_id = 'MISCONFIGURED'
            else:
                self.response_id = str(uuid.uuid4())
        else:
            self.response_id = None
        self.capture_response = capture_response

    def send(self, task_path, args, kwargs):
        """"""
        Create the message object and pass it to the actual sender.
        """"""
        message = {'task_path': task_path, 'capture_response': self.capture_response, 'response_id': self.response_id, 'args': args, 'kwargs': kwargs}
        self._send(message)
        return self

    def _send(self, message):
        """"""
        Given a message, directly invoke the lamdba function for this task.
        """"""
        message['command'] = 'zappa.asynchronous.route_lambda_task'
        payload = json.dumps(message).encode('utf-8')
        if len(payload) > LAMBDA_ASYNC_PAYLOAD_LIMIT:
            raise AsyncException('Payload too large for async Lambda call')
        self.response = self.client.invoke(FunctionName=self.lambda_function_name, InvocationType='Event', Payload=payload)
        self.sent = self.response.get('StatusCode', 0) == 202","
class LambdaAsyncResponse:
    
    def __init__(self, lambda_function_name=None, aws_region=None, capture_response=False, **kwargs):
        
        pass
    def send(self, task_path, args, kwargs):
        
        pass

    def _send(self, message):
        
        pass",snippet_10,4,"import json
from typing import Any, Dict, Optional

try:
    import boto3
    from botocore.exceptions import BotoCoreError, ClientError
except Exception:  # pragma: no cover
    boto3 = None
    BotoCoreError = Exception
    ClientError = Exception


class LambdaAsyncResponse:
    
    def __init__(self, lambda_function_name: Optional[str] = None, aws_region: Optional[str] = None, capture_response: bool = False, **kwargs):
        if not lambda_function_name:
            raise ValueError(""lambda_function_name is required"")
        if boto3 is None:
            raise RuntimeError(""boto3 is required to use LambdaAsyncResponse"")
        self.lambda_function_name = lambda_function_name
        self.aws_region = aws_region
        self.capture_response = capture_response
        self._invoke_extra: Dict[str, Any] = {}
        # Allow passing additional invoke parameters, e.g., Qualifier, ClientContext
        if kwargs:
            self._invoke_extra.update(kwargs)
        self._client = boto3.client(""lambda"", region_name=self.aws_region)

    def send(self, task_path, args, kwargs):
        message = {
            ""task"": task_path,
            ""args"": args if args is not None else [],
            ""kwargs"": kwargs if kwargs is not None else {},
        }
        return self._send(message)

    def _send(self, message):
        try:
            payload_bytes = json.dumps(message, default=self._json_default).encode(""utf-8"")
        except (TypeError, ValueError) as exc:
            raise TypeError(f""Message is not JSON serializable: {exc}"") from exc

        params: Dict[str, Any] = {
            ""FunctionName"": self.lambda_function_name,
            ""InvocationType"": ""RequestResponse"" if self.capture_response else ""Event"",
            ""Payload"": payload_bytes,
        }
        # Merge any extra invoke parameters provided at init
        if self._invoke_extra:
            params.update(self._invoke_extra)

        try:
            response = self._client.invoke(**params)
        except (BotoCoreError, ClientError) as exc:
            raise RuntimeError(f""Failed to invoke Lambda '{self.lambda_function_name}': {exc}"") from exc

        status_code = response.get(""StatusCode"")
        request_id = response.get(""ResponseMetadata"", {}).get(""RequestId"")

        if not self.capture_response:
            return {
                ""StatusCode"": status_code,
                ""RequestId"": request_id,
            }

        # When capturing response, parse Payload
        payload_stream = response.get(""Payload"")
        raw = None
        parsed = None
        if payload_stream is not None:
            try:
                raw = payload_stream.read()
            finally:
                try:
                    payload_stream.close()
                except Exception:
                    pass
            if isinstance(raw, (bytes, bytearray)):
                text = raw.decode(""utf-8"") if raw else """"
            else:
                text = str(raw) if raw is not None else """"
            # Try to parse JSON; fall back to raw text
            try:
                parsed = json.loads(text) if text else None
            except json.JSONDecodeError:
                parsed = text

        result = {
            ""StatusCode"": status_code,
            ""RequestId"": request_id,
            ""FunctionError"": response.get(""FunctionError""),
            ""ExecutedVersion"": response.get(""ExecutedVersion""),
            ""Payload"": parsed,
        }
        return result

    @staticmethod
    def _json_default(obj):
        raise TypeError(f""Object of type {type(obj).__name__} is not JSON serializable"")"
44118,LonamiWebs/Telethon,LonamiWebs_Telethon/telethon/crypto/aesctr.py,telethon.crypto.aesctr.AESModeCTR,"import pyaes

class AESModeCTR:
    """"""Wrapper around pyaes.AESModeOfOperationCTR mode with custom IV""""""

    def __init__(self, key, iv):
        """"""
        Initializes the AES CTR mode with the given key/iv pair.

        :param key: the key to be used as bytes.
        :param iv: the bytes initialization vector. Must have a length of 16.
        """"""
        assert isinstance(key, bytes)
        self._aes = pyaes.AESModeOfOperationCTR(key)
        assert isinstance(iv, bytes)
        assert len(iv) == 16
        self._aes._counter._counter = list(iv)

    def encrypt(self, data):
        """"""
        Encrypts the given plain text through AES CTR.

        :param data: the plain text to be encrypted.
        :return: the encrypted cipher text.
        """"""
        return self._aes.encrypt(data)

    def decrypt(self, data):
        """"""
        Decrypts the given cipher text through AES CTR

        :param data: the cipher text to be decrypted.
        :return: the decrypted plain text.
        """"""
        return self._aes.decrypt(data)","
class AESModeCTR:
    
    def __init__(self, key, iv):
        
        pass
    def encrypt(self, data):
        
        pass

    def decrypt(self, data):
        
        pass",snippet_11,4,"class AESModeCTR:
    
    def __init__(self, key, iv):
        try:
            from Crypto.Cipher import AES  # noqa: F401
            from Crypto.Util import Counter  # noqa: F401
        except ImportError:
            raise ImportError(""PyCryptodome is required. Install with: pip install pycryptodome"")
        if not isinstance(key, (bytes, bytearray)):
            raise TypeError(""key must be bytes"")
        if len(key) not in (16, 24, 32):
            raise ValueError(""key must be 16, 24, or 32 bytes"")
        if not isinstance(iv, (bytes, bytearray)):
            raise TypeError(""iv must be bytes"")
        if len(iv) != 16:
            raise ValueError(""iv must be 16 bytes for AES-CTR initial counter value"")
        self._key = bytes(key)
        self._iv = bytes(iv)

    def _new_cipher(self):
        from Crypto.Cipher import AES
        from Crypto.Util import Counter
        initial_value = int.from_bytes(self._iv, byteorder=""big"", signed=False)
        ctr = Counter.new(128, initial_value=initial_value)
        return AES.new(self._key, AES.MODE_CTR, counter=ctr)

    def encrypt(self, data):
        if not isinstance(data, (bytes, bytearray)):
            raise TypeError(""data must be bytes"")
        cipher = self._new_cipher()
        return cipher.encrypt(bytes(data))

    def decrypt(self, data):
        if not isinstance(data, (bytes, bytearray)):
            raise TypeError(""data must be bytes"")
        cipher = self._new_cipher()
        return cipher.decrypt(bytes(data))"
44121,LonamiWebs/Telethon,LonamiWebs_Telethon/telethon/crypto/factorization.py,telethon.crypto.factorization.Factorization,"from random import randint

class Factorization:
    """"""
    Simple module to factorize large numbers really quickly.
    """"""

    @classmethod
    def factorize(cls, pq):
        """"""
        Factorizes the given large integer.

        Implementation from https://comeoncodeon.wordpress.com/2010/09/18/pollard-rho-brent-integer-factorization/.

        :param pq: the prime pair pq.
        :return: a tuple containing the two factors p and q.
        """"""
        if pq % 2 == 0:
            return (2, pq // 2)
        y, c, m = (randint(1, pq - 1), randint(1, pq - 1), randint(1, pq - 1))
        g = r = q = 1
        x = ys = 0
        while g == 1:
            x = y
            for i in range(r):
                y = (pow(y, 2, pq) + c) % pq
            k = 0
            while k < r and g == 1:
                ys = y
                for i in range(min(m, r - k)):
                    y = (pow(y, 2, pq) + c) % pq
                    q = q * abs(x - y) % pq
                g = cls.gcd(q, pq)
                k += m
            r *= 2
        if g == pq:
            while True:
                ys = (pow(ys, 2, pq) + c) % pq
                g = cls.gcd(abs(x - ys), pq)
                if g > 1:
                    break
        p, q = (g, pq // g)
        return (p, q) if p < q else (q, p)

    @staticmethod
    def gcd(a, b):
        """"""
        Calculates the Greatest Common Divisor.

        :param a: the first number.
        :param b: the second number.
        :return: GCD(a, b)
        """"""
        while b:
            a, b = (b, a % b)
        return a","
class Factorization:
    
    @classmethod
    def factorize(cls, pq):
        
        pass
    @staticmethod
    def gcd(a, b):
        
        pass",snippet_12,3,"class Factorization:
    
    @classmethod
    def factorize(cls, pq):
        n = abs(int(pq))
        if n < 2:
            return (n, 1)
        if n % 2 == 0:
            return (2, n // 2)
        i = 3
        while i * i <= n:
            if n % i == 0:
                return (i, n // i)
            i += 2
        return (n, 1)

    @staticmethod
    def gcd(a, b):
        a, b = abs(int(a)), abs(int(b))
        while b:
            a, b = b, a % b
        return a"
49235,blue-yonder/tsfresh,blue-yonder_tsfresh/tsfresh/examples/driftbif_simulation.py,tsfresh.examples.driftbif_simulation.velocity,"import numpy as np

class velocity:
    """"""
    Simulates the velocity of a dissipative soliton (kind of self organized particle) [6]_.
    The equilibrium velocity without noise R=0 for
    $	au>1.0/\\kappa_3$ is $\\kappa_3 \\sqrt{(tau - 1.0/\\kappa_3)/Q}.
    Before the drift-bifurcation $	au \\le 1.0/\\kappa_3$ the velocity is zero.

    References
    ----------

    .. [6] Andreas Kempa-Liehr (2013, p. 159-170)
        Dynamics of Dissipative Soliton
        Dissipative Solitons in Reaction Diffusion Systems.
        Springer: Berlin


    >>> ds = velocity(tau=3.5) # Dissipative soliton with equilibrium velocity 1.5e-3
    >>> print(ds.label) # Discriminating before or beyond Drift-Bifurcation
    1

    # Equilibrium velocity
    >>> print(ds.deterministic)
    0.0015191090506254991

    # Simulated velocity as a time series with 20000 time steps being disturbed by Gaussian white noise
    >>> v = ds.simulate(20000)
    """"""

    def __init__(self, tau=3.8, kappa_3=0.3, Q=1950.0, R=0.0003, delta_t=0.05, seed=None):
        """"""
        :param tau: Bifurcation parameter determining the intrinsic velocity of the dissipative soliton,
                    which is zero for tau<=1.0/kappa_3 and np.sqrt(kappa_3**3/Q * (tau - 1.0/kappa_3)) otherwise
        :type tau: float
        :param kappa_3: Inverse bifurcation point.
        :type kappa_3:
        :param Q: Shape parameter of dissipative soliton
        :type Q: float
        :param R: Noise amplitude
        :type R: float
        :param delta_t: temporal discretization
        :type delta_t: float
        """"""
        self.delta_t = delta_t
        self.kappa_3 = kappa_3
        self.Q = Q
        self.tau = tau
        self.a = self.delta_t * kappa_3 ** 2 * (tau - 1.0 / kappa_3)
        self.b = self.delta_t * Q / kappa_3
        self.label = int(tau > 1.0 / kappa_3)
        self.c = np.sqrt(self.delta_t) * R
        self.delta_t = self.delta_t
        if seed is not None:
            np.random.seed(seed)
        if tau <= 1.0 / kappa_3:
            self.deterministic = 0.0
        else:
            self.deterministic = kappa_3 ** 1.5 * np.sqrt((tau - 1.0 / kappa_3) / Q)

    def __call__(self, v):
        """"""
        returns deterministic dynamic = acceleration (without noise)

        :param v: initial velocity vector
        :rtype v: ndarray
        :return: velocity vector of next time step
        :return type: ndarray
        """"""
        return v * (1.0 + self.a - self.b * np.dot(v, v))

    def simulate(self, N, v0=np.zeros(2)):
        """"""

        :param N: number of time steps
        :type N: int
        :param v0: initial velocity vector
        :type v0: ndarray
        :return: time series of velocity vectors with shape (N, v0.shape[0])
        :rtype: ndarray
        """"""
        v = [v0]
        n = N - 1
        gamma = np.random.randn(n, v0.size)
        for i in range(n):
            next_v = self.__call__(v[i]) + self.c * gamma[i]
            v.append(next_v)
        v_vec = np.array(v)
        return v_vec","
class velocity:
    '''
    Simulates the velocity of a dissipative soliton (kind of self organized particle) [6]_.
    The equilibrium velocity without noise R=0 for
    $    au>1.0/\kappa_3$ is $\kappa_3 \sqrt{(tau - 1.0/\kappa_3)/Q}.
    Before the drift-bifurcation $    au \le 1.0/\kappa_3$ the velocity is zero.
    References
    ----------
    .. [6] Andreas Kempa-Liehr (2013, p. 159-170)
        Dynamics of Dissipative Soliton
        Dissipative Solitons in Reaction Diffusion Systems.
        Springer: Berlin
    >>> ds = velocity(tau=3.5) # Dissipative soliton with equilibrium velocity 1.5e-3
    >>> print(ds.label) # Discriminating before or beyond Drift-Bifurcation
    1
    # Equilibrium velocity
    >>> print(ds.deterministic)
    0.0015191090506254991
    # Simulated velocity as a time series with 20000 time steps being disturbed by Gaussian white noise
    >>> v = ds.simulate(20000)
    '''
    def __init__(self, tau=3.8, kappa_3=0.3, Q=1950.0, R=0.0003, delta_t=0.05, seed=None):
        
        pass

    def __call__(self, v):
        '''
        returns deterministic dynamic = acceleration (without noise)
        :param v: initial velocity vector
        :rtype v: ndarray
        :return: velocity vector of next time step
        :return type: ndarray
        '''
        pass

    def simulate(self, N, v0=np.zeros(2)):
        
        pass",snippet_13,4,"import numpy as np

class velocity:
    '''
    Simulates the velocity of a dissipative soliton (kind of self organized particle) [6]_.
    The equilibrium velocity without noise R=0 for
    $    au>1.0/\kappa_3$ is $\kappa_3 \sqrt{(tau - 1.0/\kappa_3)/Q}.
    Before the drift-bifurcation $    au \le 1.0/\kappa_3$ the velocity is zero.
    References
    ----------
    .. [6] Andreas Kempa-Liehr (2013, p. 159-170)
        Dynamics of Dissipative Soliton
        Dissipative Solitons in Reaction Diffusion Systems.
        Springer: Berlin
    >>> ds = velocity(tau=3.5) # Dissipative soliton with equilibrium velocity 1.5e-3
    >>> print(ds.label) # Discriminating before or beyond Drift-Bifurcation
    1
    # Equilibrium velocity
    >>> print(ds.deterministic)
    0.0015191090506254991
    # Simulated velocity as a time series with 20000 time steps being disturbed by Gaussian white noise
    >>> v = ds.simulate(20000)
    '''
    def __init__(self, tau=3.8, kappa_3=0.3, Q=1950.0, R=0.0003, delta_t=0.05, seed=None):
        self.tau = float(tau)
        self.kappa_3 = float(kappa_3)
        self.Q = float(Q)
        self.R = float(R)
        self.delta_t = float(delta_t)
        self.rng = np.random.default_rng(seed)

        self._inv_k3 = 1.0 / self.kappa_3
        self.label = 1 if self.tau > self._inv_k3 else 0

        # Parameters for deterministic dynamics: dv/dt = (a - b |v|^2) v
        self.a = (self.kappa_3 ** 2) * (self.tau - self._inv_k3)
        self.b = self.Q / self.kappa_3

        # Equilibrium speed (deterministic, without noise)
        if self.label == 1:
            self.deterministic = self.kappa_3 * np.sqrt((self.tau - self._inv_k3) / self.Q)
        else:
            self.deterministic = 0.0

    def __call__(self, v):
        '''
        returns deterministic dynamic = acceleration (without noise)
        :param v: initial velocity vector
        :rtype v: ndarray
        :return: velocity vector of next time step
        :return type: ndarray
        '''
        v = np.asarray(v, dtype=float)
        if v.ndim == 1:
            norm2 = np.dot(v, v)
            dv = (self.a - self.b * norm2) * v
        else:
            norm2 = np.sum(v * v, axis=-1, keepdims=True)
            dv = (self.a - self.b * norm2) * v
        return v + self.delta_t * dv

    def simulate(self, N, v0=np.zeros(2)):
        N = int(N)
        v = np.array(v0, dtype=float).reshape(2)
        out = np.empty((N, 2), dtype=float)
        sigma = np.sqrt(2.0 * self.R * self.delta_t)

        for i in range(N):
            # Deterministic update
            norm2 = np.dot(v, v)
            dv = (self.a - self.b * norm2) * v
            # Stochastic term (Gaussian white noise)
            eta = sigma * self.rng.normal(size=2)
            v = v + self.delta_t * dv + eta
            out[i] = v

        return out"
98891,peterbrittain/asciimatics,peterbrittain_asciimatics/asciimatics/effects.py,asciimatics.effects._Flake,"from random import randint, random, choice
from asciimatics.screen import Screen

class _Flake:
    """"""
    Track a single snow flake.
    """"""
    _snow_chars = '.+*'
    _drift_chars = ' ,;#@'

    def __init__(self, screen: Screen):
        """"""
        :param screen: The Screen being used for the Scene.
        """"""
        self._screen = screen
        self._x = 0
        self._y = 0
        self._rate = 0
        self._char = ''
        self._reseed()

    def _reseed(self):
        """"""
        Randomly create a new snowflake once this one is finished.
        """"""
        self._char = choice(self._snow_chars)
        self._rate = randint(1, 3)
        self._x = randint(0, self._screen.width - 1)
        self._y = self._screen.start_line + randint(0, self._rate)

    def update(self, reseed: bool):
        """"""
        Update that snowflake!

        :param reseed: Whether we are in the normal reseed cycle or not.
        """"""
        self._screen.print_at(' ', self._x, self._y)
        cell = None
        for _ in range(self._rate):
            self._y += 1
            cell = self._screen.get_from(self._x, self._y)
            if cell is None or cell[0] != 32:
                break
        if (cell is not None and cell[0] in [ord(x) for x in self._snow_chars + ' ']) and self._y < self._screen.start_line + self._screen.height:
            self._screen.print_at(self._char, self._x, self._y)
        else:
            self._y = min(self._y, self._screen.start_line + self._screen.height)
            drift_index = -1
            if cell:
                drift_index = self._drift_chars.find(chr(cell[0]))
            if 0 <= drift_index < len(self._drift_chars) - 1:
                drift_char = self._drift_chars[drift_index + 1]
                self._screen.print_at(drift_char, self._x, self._y)
            else:
                self._screen.print_at(',', self._x, self._y - 1)
            if reseed:
                self._reseed()","
class _Flake:
    
    def __init__(self, screen: Screen):
        
        pass
    def _reseed(self):
        
        pass

    def update(self, reseed: bool):
        
        pass",snippet_14,4,"import math
import random
from typing import Any


class _Flake:
    def __init__(self, screen: Any):
        self._screen = screen
        self.x = 0.0
        self.y = 0.0
        self.vy = 0.0
        self._phase = 0.0
        self._phase_step = 0.0
        self.char = ""*""
        self._reseed()

    def _reseed(self):
        width = getattr(self._screen, ""width"", getattr(self._screen, ""columns"", 0)) or 0
        height = getattr(self._screen, ""height"", getattr(self._screen, ""rows"", 0)) or 0

        if width <= 0:
            self.x = 0.0
        else:
            self.x = float(random.randint(0, max(0, width - 1)))

        # Start above the top of the screen
        start_span = max(1, height) if height else 1
        self.y = float(-random.randint(1, start_span))
        self.vy = random.uniform(0.35, 1.25)

        self.char = random.choice((""*"", ""•"", ""·"", ""❄""))
        self._phase = random.uniform(0.0, 2.0 * math.pi)
        self._phase_step = random.uniform(0.03, 0.12)

    def update(self, reseed: bool):
        width = getattr(self._screen, ""width"", getattr(self._screen, ""columns"", 0)) or 0
        height = getattr(self._screen, ""height"", getattr(self._screen, ""rows"", 0)) or 0

        # Horizontal drift (sway)
        if width > 0:
            drift_amplitude = max(0.0, min(1.5, width * 0.02))
            self.x += math.sin(self._phase) * 0.25 * drift_amplitude
            self._phase += self._phase_step
            if self._phase > 2.0 * math.pi:
                self._phase -= 2.0 * math.pi

            # Clamp to screen bounds
            if self.x < 0.0:
                self.x = 0.0
            elif self.x > width - 1:
                self.x = float(width - 1)

        # Fall
        self.y += self.vy

        # If out of bounds at bottom, reseed or pin to bottom
        if height > 0 and self.y >= height:
            if reseed:
                self._reseed()
            else:
                self.y = float(height - 1)
                self.vy = 0.0"
98892,peterbrittain/asciimatics,peterbrittain_asciimatics/asciimatics/effects.py,asciimatics.effects._Star,"from random import randint, random, choice
from asciimatics.screen import Screen

class _Star:
    """"""
    Simple class to represent a single star for the Stars special effect.
    """"""

    def __init__(self, screen: Screen, pattern: str):
        """"""
        :param screen: The Screen being used for the Scene.
        :param pattern: The pattern to loop through
        """"""
        self._screen = screen
        self._star_chars = pattern
        self._cycle = 0
        self._old_char = ''
        self._respawn()

    def _respawn(self):
        """"""
        Pick a random location for the star making sure it does
        not overwrite an existing piece of text.
        """"""
        self._cycle = randint(0, len(self._star_chars))
        height, width = self._screen.dimensions
        while True:
            self._x = randint(0, width - 1)
            self._y = self._screen.start_line + randint(0, height - 1)
            c = self._screen.get_from(self._x, self._y)
            if c is not None and c[0] == 32:
                break
        self._old_char = ' '

    def update(self):
        """"""
        Draw the star.
        """"""
        if not self._screen.is_visible(self._x, self._y):
            self._respawn()
        c = self._screen.get_from(self._x, self._y)
        if c is not None and c[0] not in (ord(self._old_char), 32):
            self._respawn()
        self._cycle += 1
        if self._cycle >= len(self._star_chars):
            self._cycle = 0
        new_char = self._star_chars[self._cycle]
        if new_char == self._old_char:
            return
        self._screen.print_at(new_char, self._x, self._y)
        self._old_char = new_char","
class _Star:
    
    def __init__(self, screen: Screen, pattern: str):
        
        pass
    def _respawn(self):
        
        pass

    def update(self):
        
        pass",snippet_15,4,"import random
from typing import Optional

class _Star:
    
    def __init__(self, screen, pattern: str):
        self._screen = screen
        self._pattern = pattern or ""*""
        self._width = getattr(screen, ""width"", 0)
        self._height = getattr(screen, ""height"", 0)
        self._prev_x: Optional[int] = None
        self._prev_y: Optional[int] = None
        self._x: float = 0.0
        self._y: float = 0.0
        self._vy: float = 0.0
        self._char: str = ""*""
        self._colour: int = 7
        self._respawn()
    
    def _respawn(self):
        if self._width <= 0 or self._height <= 0:
            self._x = 0
            self._y = 0
            self._vy = 0.0
            self._char = ""*""
            self._colour = 7
            return
        self._x = random.randint(0, self._width - 1)
        self._y = -1.0
        self._vy = random.uniform(0.2, 1.5)
        self._char = random.choice(self._pattern)
        max_colour = getattr(self._screen, ""colours"", 8) - 1
        max_colour = max(1, max_colour)
        self._colour = random.randint(1, max_colour)

    def update(self):
        # Erase previous position
        if self._prev_x is not None and self._prev_y is not None:
            if 0 <= self._prev_x < self._width and 0 <= self._prev_y < self._height:
                try:
                    self._screen.print_at("" "", self._prev_x, self._prev_y)
                except Exception:
                    pass

        self._y += self._vy

        if self._y >= self._height:
            self._prev_x = None
            self._prev_y = None
            self._respawn()
            return

        ix = int(self._x)
        iy = int(self._y)

        if 0 <= ix < self._width and 0 <= iy < self._height:
            try:
                self._screen.print_at(self._char, ix, iy, self._colour)
            except Exception:
                pass

        self._prev_x = ix
        self._prev_y = iy"
98893,peterbrittain/asciimatics,peterbrittain_asciimatics/asciimatics/effects.py,asciimatics.effects._Trail,"from random import randint, random, choice
from asciimatics.screen import Screen

class _Trail:
    """"""
    Track a single trail  for a falling character effect (a la Matrix).
    """"""

    def __init__(self, screen: Screen, x: int):
        """"""
        :param screen: The Screen being used for the Scene.
        :param x: The column (y coordinate) for this trail to use.
        """"""
        self._screen = screen
        self._x = x
        self._y = 0
        self._life = 0
        self._rate = 0
        self._clear = True
        self._maybe_reseed(True)

    def _maybe_reseed(self, normal: bool):
        """"""
        Randomly create a new column once this one is finished.
        """"""
        self._y += self._rate
        self._life -= 1
        if self._life <= 0:
            self._clear = not self._clear if normal else True
            self._rate = randint(1, 2)
            if self._clear:
                self._y = 0
                self._life = self._screen.height // self._rate
            else:
                self._y = randint(0, self._screen.height // 2) - self._screen.height // 4
                self._life = randint(1, self._screen.height - self._y) // self._rate

    def update(self, reseed: bool):
        """"""
        Update that trail!

        :param reseed: Whether we are in the normal reseed cycle or not.
        """"""
        if self._clear:
            for i in range(0, 3):
                self._screen.print_at(' ', self._x, self._screen.start_line + self._y + i)
            self._maybe_reseed(reseed)
        else:
            for i in range(0, 3):
                self._screen.print_at(chr(randint(32, 126)), self._x, self._screen.start_line + self._y + i, Screen.COLOUR_GREEN)
            for i in range(4, 6):
                self._screen.print_at(chr(randint(32, 126)), self._x, self._screen.start_line + self._y + i, Screen.COLOUR_GREEN, Screen.A_BOLD)
            self._maybe_reseed(reseed)","
class _Trail:
    
    def __init__(self, screen: Screen, x: int):
        
        pass
    def _maybe_reseed(self, normal: bool):
        
        pass

    def update(self, reseed: bool):
        
        pass",snippet_16,4,"class _Trail:
    
    def __init__(self, screen, x: int):
        import random
        import time

        self.screen = screen
        self.x = x

        # Determine screen height as best-effort
        self.height = (
            getattr(screen, ""height"", None)
            or getattr(screen, ""rows"", None)
            or getattr(screen, ""nrows"", None)
        )
        if self.height is None:
            get_h = getattr(screen, ""get_height"", None)
            if callable(get_h):
                try:
                    self.height = int(get_h())
                except Exception:
                    self.height = 24
            else:
                self.height = 24

        # Random generator and initial state
        self._rnd = random.Random()
        self._rnd.seed((int(time.time() * 1000) ^ (x << 8)) & 0xFFFFFFFF)

        self._min_len = 4
        self._max_len = max(6, self.height // 2)
        self._chars = ""0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz@#$%^&*+=-""
        self._tick = 0
        self._speed = 1  # steps per move (lower = faster)
        self._move_counter = 0

        # Trail state: y positions (head at end), and current head y
        self._length = self._rnd.randint(self._min_len, self._max_len)
        # Start above the screen so it drops in
        self._head_y = -self._rnd.randint(0, self.height)
        self._positions = []  # list of y positions making up the trail
        self._last_ops = []   # last draw operations emitted

        # Initial random speed
        self._speed = self._rnd.randint(1, 3)

    def _maybe_reseed(self, normal: bool):
        # Reseed if forced or with a small probability when normal
        force = not normal
        do_reseed = force or (self._rnd.random() < 0.02)

        # Also reseed if the entire trail has moved well past the screen
        if not do_reseed:
            if self._head_y - self._length > self.height + 5:
                do_reseed = True

        if not do_reseed:
            return

        # Reset trail parameters
        self._length = self._rnd.randint(self._min_len, self._max_len)
        self._speed = self._rnd.randint(1, 3)
        self._head_y = -self._rnd.randint(0, self.height)
        self._positions = []
        self._tick = 0
        self._move_counter = 0

    def update(self, reseed: bool):
        # Optionally reseed before update
        self._maybe_reseed(normal=not reseed)

        self._tick += 1
        moved = False
        if self._tick >= self._speed:
            self._tick = 0
            moved = True

        ops = []  # (y, x, char, brightness) where brightness in {'head','body','fade','clear'}

        if moved:
            # Move head down by one
            self._head_y += 1
            # Add new head
            self._positions.append(self._head_y)
            # Trim tail
            while len(self._positions) > self._length:
                tail_y = self._positions.pop(0)
                if 0 <= tail_y < self.height:
                    ops.append((tail_y, self.x, "" "", ""clear""))

        # Emit draw ops for visible positions
        for idx, y in enumerate(self._positions):
            if 0 <= y < self.height:
                # Determine brightness: head brighter
                if idx == len(self._positions) - 1:
                    brightness = ""head""
                elif idx >= len(self._positions) - 3:
                    brightness = ""body""
                else:
                    brightness = ""fade""
                ch = self._rnd.choice(self._chars)
                ops.append((y, self.x, ch, brightness))

        self._last_ops = ops
        return ops"
99108,ConsenSys/mythril-classic,ConsenSys_mythril-classic/mythril/analysis/ops.py,mythril.analysis.ops.Variable,"class Variable:
    """"""The representation of a variable with value and type.""""""

    def __init__(self, val, _type):
        """"""

        :param val:
        :param _type:
        """"""
        self.val = val
        self.type = _type

    def __str__(self):
        """"""

        :return:
        """"""
        return str(self.val)","class Variable:
    
    def __init__(self, val, _type):
        
        pass
    def __str__(self):
        
        pass",snippet_17,3,"class Variable:
    
    def __init__(self, val, _type):
        type_map = {
            'int': int,
            'float': float,
            'str': str,
            'bool': bool,
            'list': list,
            'dict': dict,
            'tuple': tuple,
            'set': set,
        }

        if isinstance(_type, str):
            if _type not in type_map:
                raise TypeError(f""Unsupported type string: {_type}"")
            target_type = type_map[_type]
        elif isinstance(_type, type):
            target_type = _type
        else:
            raise TypeError(""Type must be a type object or a supported type string"")

        if target_type is bool:
            if isinstance(val, str):
                v = val.strip().lower()
                if v in ('true', '1', 'yes', 'y', 't'):
                    self.value = True
                elif v in ('false', '0', 'no', 'n', 'f'):
                    self.value = False
                else:
                    raise ValueError(f""Cannot convert string '{val}' to bool"")
            else:
                self.value = bool(val)
        else:
            self.value = target_type(val)

        self.type = target_type

    def __str__(self):
        return str(self.value)"
100863,bigchaindb/bigchaindb,bigchaindb_bigchaindb/bigchaindb/validation.py,bigchaindb.validation.BaseValidationRules,"class BaseValidationRules:
    """"""Base validation rules for BigchainDB.

    A validation plugin must expose a class inheriting from this one via an entry_point.

    All methods listed below must be implemented.
    """"""

    @staticmethod
    def validate_transaction(bigchaindb, transaction):
        """"""See :meth:`bigchaindb.models.Transaction.validate`
        for documentation.
        """"""
        return transaction.validate(bigchaindb)

    @staticmethod
    def validate_block(bigchaindb, block):
        """"""See :meth:`bigchaindb.models.Block.validate` for documentation.""""""
        return block.validate(bigchaindb)","class BaseValidationRules:
    
    @staticmethod
    def validate_transaction(bigchaindb, transaction):
        
        pass
    @staticmethod
    def validate_block(bigchaindb, block):
        
        pass",snippet_22,3,"class BaseValidationRules:
    @staticmethod
    def validate_transaction(bigchaindb, transaction):
        # Basic structural validation for a transaction-like mapping
        if not isinstance(transaction, dict):
            return False

        required_keys = {'id', 'operation', 'inputs', 'outputs', 'version'}
        if not required_keys.issubset(transaction.keys()):
            return False

        # Validate types of some standard fields
        if not isinstance(transaction['id'], str) or not transaction['id']:
            return False

        if not isinstance(transaction['operation'], str) or not transaction['operation']:
            return False

        if not isinstance(transaction['version'], (str, int)):
            return False

        if not isinstance(transaction['inputs'], list):
            return False

        if not isinstance(transaction['outputs'], list) or not transaction['outputs']:
            return False

        return True

    @staticmethod
    def validate_block(bigchaindb, block):
        # Basic structural validation for a block-like mapping
        if not isinstance(block, dict):
            return False

        # Try different common shapes for blocks
        txs = None
        if 'block' in block and isinstance(block['block'], dict):
            inner = block['block']
            txs = inner.get('transactions', None)
        elif 'transactions' in block:
            txs = block.get('transactions', None)

        if not isinstance(txs, list) or len(txs) == 0:
            return False

        # Validate each transaction in the block
        for tx in txs:
            if not BaseValidationRules.validate_transaction(bigchaindb, tx):
                return False

        return True"
100865,bigchaindb/bigchaindb,bigchaindb_bigchaindb/bigchaindb/web/strip_content_type_middleware.py,bigchaindb.web.strip_content_type_middleware.StripContentTypeMiddleware,"class StripContentTypeMiddleware:
    """"""WSGI middleware to strip Content-Type header for GETs.""""""

    def __init__(self, app):
        """"""Create the new middleware.

        Args:
            app: a flask application
        """"""
        self.app = app

    def __call__(self, environ, start_response):
        """"""Run the middleware and then call the original WSGI application.""""""
        if environ['REQUEST_METHOD'] == 'GET':
            try:
                del environ['CONTENT_TYPE']
            except KeyError:
                pass
            else:
                logger.debug('Remove header ""Content-Type"" from GET request')
        return self.app(environ, start_response)","class StripContentTypeMiddleware:
    
    def __init__(self, app):
        
        pass
    def __call__(self, environ, start_response):
        
        pass",snippet_23,3,"class StripContentTypeMiddleware:
    
    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        cl = environ.get('CONTENT_LENGTH')
        has_length = False
        if cl is not None and cl != '':
            try:
                has_length = int(cl) > 0
            except (ValueError, TypeError):
                has_length = False

        if not has_length:
            environ.pop('CONTENT_TYPE', None)
            environ.pop('HTTP_CONTENT_TYPE', None)

        return self.app(environ, start_response)"
100895,ansible/molecule,src/molecule/app.py,molecule.app.App,"from pathlib import Path
from subprocess import CalledProcessError, CompletedProcess
from molecule.console import original_stderr
from ansible_compat.runtime import Runtime
from molecule.ansi_output import CommandBorders
from molecule.util import print_environment_vars

class App:
    """"""App class that keep runtime status.""""""

    def __init__(self, path: Path) -> None:
        """"""Create a new app instance.

        Args:
            path: The path to the project.
        """"""
        self.runtime = Runtime(project_dir=path, isolated=False)

    def run_command(self, cmd: str | list[str], env: dict[str, str] | None=None, cwd: Path | None=None, *, debug: bool=False, echo: bool=False, quiet: bool=False, check: bool=False, command_borders: bool=False) -> CompletedProcess[str]:
        """"""Execute the given command and returns None.

        Args:
            cmd: A list of strings containing the command to run.
            env: A dict containing the shell's environment.
            cwd: An optional Path to the working directory.
            debug: An optional bool to toggle debug output.
            echo: An optional bool to toggle command echo.
            quiet: An optional bool to toggle command output.
            check: An optional bool to toggle command error checking.
            command_borders: An optional bool to enable borders around command output.

        Returns:
            A completed process object.

        Raises:
            CalledProcessError: If return code is nonzero and check is True.
        """"""
        if debug:
            print_environment_vars(env)
        borders = None
        if command_borders:
            borders = CommandBorders(cmd=cmd, original_stderr=original_stderr)
        result = self.runtime.run(args=cmd, env=env, cwd=cwd, tee=True, set_acp=False)
        if borders:
            borders.finalize(result.returncode)
        if result.returncode != 0 and check:
            raise CalledProcessError(returncode=result.returncode, cmd=result.args, output=result.stdout, stderr=result.stderr)
        return result","
class App:
    
    def __init__(self, path: Path) -> None:
        
        pass
    def run_command(self, cmd: str | list[str], env: dict[str, str] | None=None, cwd: Path | None=None, *, debug: bool=False, echo: bool=False, quiet: bool=False, check: bool=False, command_borders: bool=False) -> CompletedProcess[str]:
        
        pass",snippet_24,3,"from __future__ import annotations

import os
import shlex
import subprocess
from pathlib import Path
from subprocess import CompletedProcess
from typing import Iterable


class App:
    def __init__(self, path: Path) -> None:
        self.path = Path(path)

    def run_command(
        self,
        cmd: str | list[str],
        env: dict[str, str] | None = None,
        cwd: Path | None = None,
        *,
        debug: bool = False,
        echo: bool = False,
        quiet: bool = False,
        check: bool = False,
        command_borders: bool = False,
    ) -> CompletedProcess[str]:
        use_shell = isinstance(cmd, str)
        run_cwd = Path(cwd) if cwd is not None else self.path

        base_env = os.environ.copy()
        if env:
            base_env.update(env)

        display_cmd: str
        if isinstance(cmd, str):
            display_cmd = cmd
        else:
            # best-effort shell-like representation
            try:
                display_cmd = shlex.join(cmd)
            except Exception:
                display_cmd = "" "".join(cmd)

        def _print(line: str = """") -> None:
            if not quiet:
                print(line)

        if command_borders and not quiet:
            _print(""="" * 80)
            _print(f""RUN: {display_cmd}"")
            _print(""- cwd: "" + str(run_cwd))
            if debug:
                _print(""- env overrides: "" + ("", "".join(sorted(env.keys())) if env else ""(none)""))
            _print(""- shell: "" + str(use_shell))
            _print(""- check: "" + str(check))
            _print(""- echo: "" + str(echo))
            _print(""="" * 80)

        if echo and not quiet and not command_borders:
            _print(f""$ {display_cmd}"")

        result = subprocess.run(
            cmd,
            shell=use_shell,
            cwd=str(run_cwd) if run_cwd is not None else None,
            env=base_env,
            text=True,
            capture_output=True,
            check=False,  # we'll handle check after printing
        )

        if not quiet:
            if result.stdout:
                print(result.stdout, end="""")
            if result.stderr:
                print(result.stderr, end="""", file=os.sys.stderr)

        if command_borders and not quiet:
            _print("""")
            _print(""="" * 80)
            _print(f""END (returncode={result.returncode})"")
            _print(""="" * 80)

        if check and result.returncode != 0:
            # Re-raise as CalledProcessError with captured output
            raise subprocess.CalledProcessError(
                result.returncode, cmd, output=result.stdout, stderr=result.stderr
            )

        return result"
104788,autokey/autokey,autokey_autokey/lib/autokey/scripting/system.py,autokey.scripting.system.System,"import subprocess

class System:
    """"""
    Simplified access to some system commands.
    """"""

    @staticmethod
    def exec_command(command, getOutput=True):
        """"""
        Execute a shell command

        Usage: C{system.exec_command(command, getOutput=True)}

        Set getOutput to False if the command does not exit and return immediately. Otherwise
        AutoKey will not respond to any hotkeys/abbreviations etc until the process started
        by the command exits.

        @param command: command to be executed (including any arguments) - e.g. ""ls -l""
        @param getOutput: whether to capture the (stdout) output of the command
        @raise subprocess.CalledProcessError: if the command returns a non-zero exit code
        """"""
        if getOutput:
            with subprocess.Popen(command, shell=True, bufsize=-1, stdout=subprocess.PIPE, universal_newlines=True) as p:
                output = p.communicate()[0]
                output = output.rstrip('\n')
                if p.returncode:
                    raise subprocess.CalledProcessError(p.returncode, output)
                return output
        else:
            subprocess.Popen(command, shell=True, bufsize=-1)

    @staticmethod
    def create_file(file_name, contents=''):
        """"""
        Create a file with contents

        Usage: C{system.create_file(fileName, contents="""")}

        @param fileName: full path to the file to be created
        @param contents: contents to insert into the file
        """"""
        with open(file_name, 'w') as written_file:
            written_file.write(contents)","
class System:
    
    @staticmethod
    def exec_command(command, getOutput=True):
        
        pass
    @staticmethod
    def create_file(file_name, contents=''):
        
        pass",snippet_26,3,"```python
import os
import subprocess

class System:
    
    @staticmethod
    def exec_command(command, getOutput=True):
        try:
            use_shell = isinstance(command, str)
            result = subprocess.run(
                command,
                shell=use_shell,
                capture_output=True,
                text=True
            )
            if getOutput:
                return result.stdout.strip()
            return result.returncode
        except Exception:
            if getOutput:
                return ''
            return -1

    @staticmethod
    def create_file(file_name, contents=''):
        try:
            directory = os.path.dirname(os.path.abspath(file_name))
            if directory and not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
            with open(file_name, 'w', encoding='utf-8') as f:
                f.write(contents)
            return True
        except Exception:
            return False
```"
106092,mikedh/trimesh,trimesh/collision.py,trimesh.collision.DistanceData,"class DistanceData:
    """"""
    Data structure for holding information about a distance query.
    """"""

    def __init__(self, names, result):
        """"""
        Initialize a DistanceData.

        Parameters
        ----------
        names : list of str
          The names of the two objects in order.
        contact : fcl.DistanceResult
          The distance query result.
        """"""
        self.names = set(names)
        self._inds = {names[0]: result.b1, names[1]: result.b2}
        self._points = {names[0]: result.nearest_points[0], names[1]: result.nearest_points[1]}
        self._distance = result.min_distance

    @property
    def distance(self):
        """"""
        Returns the distance between the two objects.

        Returns
        -------
        distance : float
          The euclidean distance between the objects.
        """"""
        return self._distance

    def index(self, name):
        """"""
        Returns the index of the closest face for the mesh with
        the given name.

        Parameters
        ----------
        name : str
          The name of the target object.

        Returns
        -------
        index : int
          The index of the face in collisoin.
        """"""
        return self._inds[name]

    def point(self, name):
        """"""
        The 3D point of closest distance on the mesh with the given name.

        Parameters
        ----------
        name : str
          The name of the target object.

        Returns
        -------
        point : (3,) float
          The closest point.
        """"""
        return self._points[name]","class DistanceData:
    
    def __init__(self, names, result):
        
        pass
    @property
    def distance(self):
        
        pass

    def index(self, name):
        
        pass

    def point(self, name):
        
        pass",snippet_28,5,"class DistanceData:
    
    def __init__(self, names, result):
        if not isinstance(names, (list, tuple)):
            raise TypeError(""names must be a list or tuple"")
        self.names = list(names)
        self._name_to_index = {name: i for i, name in enumerate(self.names)}
        n = len(self.names)
        # Initialize matrix
        self._matrix = [[0.0 if i == j else float('inf') for j in range(n)] for i in range(n)]
        if result is None:
            return
        # Helper to resolve index from name or index
        def idx(x):
            if isinstance(x, int):
                if 0 <= x < n:
                    return x
                raise IndexError(""index out of range"")
            try:
                return self._name_to_index[x]
            except KeyError as e:
                raise KeyError(f""unknown name: {x}"") from e
        # Load from 2D sequence
        if isinstance(result, (list, tuple)):
            if len(result) != n:
                raise ValueError(""result size does not match names length"")
            for i, row in enumerate(result):
                if not isinstance(row, (list, tuple)) or len(row) != n:
                    raise ValueError(""result must be a square 2D sequence matching names length"")
                for j, v in enumerate(row):
                    self._matrix[i][j] = float(v)
            return
        # Load from dict-of-dicts
        if isinstance(result, dict):
            # dict with tuple keys or nested dicts
            tuple_keyed = any(isinstance(k, tuple) and len(k) == 2 for k in result.keys())
            if tuple_keyed:
                for (a, b), v in result.items():
                    i, j = idx(a), idx(b)
                    self._matrix[i][j] = float(v)
            else:
                for a, inner in result.items():
                    ia = idx(a)
                    if not isinstance(inner, dict):
                        raise ValueError(""dict values must be dicts when not using tuple keys"")
                    for b, v in inner.items():
                        ib = idx(b)
                        self._matrix[ia][ib] = float(v)
            return
        raise TypeError(""Unsupported result type"")

    @property
    def distance(self):
        def _d(a, b):
            i = self.index(a) if not isinstance(a, int) else a
            j = self.index(b) if not isinstance(b, int) else b
            if not (0 <= i < len(self.names) and 0 <= j < len(self.names)):
                raise IndexError(""index out of range"")
            return self._matrix[i][j]
        return _d

    def index(self, name):
        if isinstance(name, int):
            if 0 <= name < len(self.names):
                return name
            raise IndexError(""index out of range"")
        try:
            return self._name_to_index[name]
        except KeyError as e:
            raise KeyError(f""unknown name: {name}"") from e

    def point(self, name):
        i = self.index(name)
        return list(self._matrix[i])"
106111,mikedh/trimesh,trimesh/path/packing.py,trimesh.path.packing.RectangleBin,"import numpy as np

class RectangleBin:
    """"""
    An N-dimensional binary space partition tree for packing
    hyper-rectangles. Split logic is pure `numpy` but behaves
    similarly to `scipy.spatial.Rectangle`.

    Mostly useful for packing 2D textures and 3D boxes and
    has not been tested outside of 2 and 3 dimensions.

    Original article about using this for packing textures:
    http://www.blackpawn.com/texts/lightmaps/
    """"""

    def __init__(self, bounds):
        """"""
        Create a rectangular bin.

        Parameters
        ------------
        bounds : (2, dimension *) float
          Bounds array are `[mins, maxes]`
        """"""
        self.child = []
        self.occupied = False
        self.bounds = np.array(bounds, dtype=np.float64)

    @property
    def extents(self):
        """"""
        Bounding box size.

        Returns
        ----------
        extents : (dimension,) float
          Edge lengths of bounding box
        """"""
        bounds = self.bounds
        return bounds[1] - bounds[0]

    def insert(self, size, rotate=True):
        """"""
        Insert a rectangle into the bin.

        Parameters
        -------------
        size : (dimension,) float
          Size of rectangle to insert/

        Returns
        ----------
        inserted : (2,) float or None
          Position of insertion in the tree or None
          if the insertion was unsuccessful.
        """"""
        for child in self.child:
            attempt = child.insert(size=size, rotate=rotate)
            if attempt is not None:
                return attempt
        if self.occupied:
            return None
        bounds = self.bounds.copy()
        extents = bounds[1] - bounds[0]
        if rotate:
            for roll in range(len(size)):
                size_test = extents - _roll(size, roll)
                fits = (size_test > -_TOL_ZERO).all()
                if fits:
                    size = _roll(size, roll)
                    break
            if not fits:
                return None
        else:
            size_test = extents - size
            if (size_test < -_TOL_ZERO).any():
                return None
        self.occupied = True
        if (size_test < _TOL_ZERO).all():
            return bounds
        axis = size_test.argmax()
        splits = np.vstack((bounds, bounds))
        splits[1:3, axis] = bounds[0][axis] + size[axis]
        self.child[:] = (RectangleBin(splits[:2]), RectangleBin(splits[2:]))
        return self.child[0].insert(size, rotate=rotate)","
class RectangleBin:
    
    def __init__(self, bounds):
        
        pass
    @property
    def extents(self):
        
        pass

    def insert(self, size, rotate=True):
        
        pass",snippet_29,4,"class RectangleBin:
    
    def __init__(self, bounds):
        if len(bounds) != 2 or bounds[0] <= 0 or bounds[1] <= 0:
            raise ValueError(""bounds must be a tuple (width, height) with positive values"")
        self.width, self.height = int(bounds[0]), int(bounds[1])
        self._free = [(0, 0, self.width, self.height)]
        self._used = []
        self._max_x = 0
        self._max_y = 0

    @property
    def extents(self):
        return (self._max_x, self._max_y)

    def insert(self, size, rotate=True):
        if len(size) != 2 or size[0] <= 0 or size[1] <= 0:
            raise ValueError(""size must be a tuple (width, height) with positive values"")
        w_req, h_req = int(size[0]), int(size[1])

        candidates = []
        for idx, fr in enumerate(self._free):
            fx, fy, fw, fh = fr
            # Try without rotation
            if w_req <= fw and h_req <= fh:
                candidates.append((idx, False, fx, fy, w_req, h_req, fw * fh - w_req * h_req))
            # Try with rotation
            if rotate and h_req <= fw and w_req <= fh:
                candidates.append((idx, True, fx, fy, h_req, w_req, fw * fh - w_req * h_req))

        if not candidates:
            return None

        # Choose the candidate with smallest area waste, tie-break by y then x (top-left heuristic)
        candidates.sort(key=lambda c: (c[6], c[3], c[2]))
        idx, rotated, px, py, pw, ph, _ = candidates[0]

        placed = (px, py, pw, ph)
        self._place_and_update(placed)

        self._max_x = max(self._max_x, px + pw)
        self._max_y = max(self._max_y, py + ph)
        self._used.append(placed)

        return placed

    # Internal helpers
    def _place_and_update(self, rect):
        # Subtract the placed rect from all free rectangles and rebuild the free list
        new_free = []
        for fr in self._free:
            new_free.extend(self._subtract_rect(fr, rect))

        # Prune redundant rectangles (remove those fully contained in another)
        pruned = self._prune_contained(new_free)

        # Optionally, we could merge adjacent rectangles, but pruning is sufficient for correctness
        self._free = pruned

    @staticmethod
    def _intersect(a, b):
        ax, ay, aw, ah = a
        bx, by, bw, bh = b
        ix = max(ax, bx)
        iy = max(ay, by)
        ix2 = min(ax + aw, bx + bw)
        iy2 = min(ay + ah, by + bh)
        if ix < ix2 and iy < iy2:
            return (ix, iy, ix2 - ix, iy2 - iy)
        return None

    @staticmethod
    def _subtract_rect(free_rect, used_rect):
        # Returns a list of rectangles representing free_rect minus used_rect
        fx, fy, fw, fh = free_rect
        ux, uy, uw, uh = used_rect

        inter = RectangleBin._intersect(free_rect, used_rect)
        if inter is None:
            return [free_rect]

        ix, iy, iw, ih = inter
        result = []

        # Left side
        if ix > fx:
            result.append((fx, fy, ix - fx, fh))
        # Right side
        right_x = ix + iw
        fr_right = fx + fw
        if right_x < fr_right:
            result.append((right_x, fy, fr_right - right_x, fh))
        # Top side
        if iy > fy:
            top_h = iy - fy
            # The middle strip that remains above the intersection, constrained to intersection's horizontal span
            result.append((ix, fy, iw, top_h))
        # Bottom side
        bottom_y = iy + ih
        fr_bottom = fy + fh
        if bottom_y < fr_bottom:
            bottom_h = fr_bottom - bottom_y
            result.append((ix, bottom_y, iw, bottom_h))

        # Filter zero-area
        result = [r for r in result if r[2] > 0 and r[3] > 0]
        return result

    @staticmethod
    def _contains(a, b):
        ax, ay, aw, ah = a
        bx, by, bw, bh = b
        return ax <= bx and ay <= by and (ax + aw) >= (bx + bw) and (ay + ah) >= (by + bh)

    @staticmethod
    def _prune_contained(rects):
        # Remove rectangles that are fully contained within another
        pruned = []
        for i, r in enumerate(rects):
            contained = False
            for j, s in enumerate(rects):
                if i != j and RectangleBin._contains(s, r):
                    contained = True
                    break
            if not contained:
                pruned.append(r)
        return pruned"
107609,weld-project/weld,python/grizzly/grizzly/lazy_op.py,grizzly.lazy_op.LazyOpResult,"class LazyOpResult:
    """"""Wrapper class around as yet un-evaluated Weld computation results

    Attributes:
        dim (int): Dimensionality of the output
        expr (WeldObject / Numpy.ndarray): The expression that needs to be
            evaluated
        weld_type (WeldType): Type of the output object
    """"""

    def __init__(self, expr, weld_type, dim):
        """"""Summary

        Args:
            expr (TYPE): Description
            weld_type (TYPE): Description
            dim (TYPE): Description
        """"""
        self.expr = expr
        self.weld_type = weld_type
        self.dim = dim

    def evaluate(self, verbose=True, decode=True, passes=None, num_threads=1, apply_experimental_transforms=False):
        """"""Summary

        Args:
            verbose (bool, optional): Description
            decode (bool, optional): Description

        Returns:
            TYPE: Description
        """"""
        if isinstance(self.expr, WeldObject):
            return self.expr.evaluate(to_weld_type(self.weld_type, self.dim), verbose, decode, passes=passes, num_threads=num_threads, apply_experimental_transforms=apply_experimental_transforms)
        return self.expr","class LazyOpResult:
    
    def __init__(self, expr, weld_type, dim):
        
        pass
    def evaluate(self, verbose=True, decode=True, pass
es=None, num_threads=1, apply_experimental_transforms=False):
        
        pass",snippet_30,3,"class LazyOpResult:
    def __init__(self, expr, weld_type=None, dim=None):
        self.expr = expr
        self.weld_type = weld_type
        if dim is None or isinstance(dim, int):
            self.dim = dim
        elif isinstance(dim, (list, tuple)):
            self.dim = tuple(dim)
        else:
            raise TypeError(""dim must be None, int, list, or tuple"")

    def evaluate(self, verbose=True, decode=True, passes=None, num_threads=1, apply_experimental_transforms=False):
        def _call_callable(fn):
            try:
                return fn()
            except TypeError:
                try:
                    return fn(self)
                except TypeError:
                    return fn

        def _eval(expr):
            if hasattr(expr, ""evaluate"") and callable(getattr(expr, ""evaluate"")):
                try:
                    return expr.evaluate(
                        verbose=verbose,
                        decode=decode,
                        passes=passes,
                        num_threads=num_threads,
                        apply_experimental_transforms=apply_experimental_transforms,
                    )
                except TypeError:
                    return expr.evaluate()
            if callable(expr):
                return _call_callable(expr)
            if isinstance(expr, tuple) and expr and callable(expr[0]):
                fn = expr[0]
                args = expr[1] if len(expr) > 1 else ()
                kwargs = expr[2] if len(expr) > 2 and isinstance(expr[2], dict) else {}
                try:
                    return fn(*args, **kwargs)
                except TypeError:
                    return fn()
            return expr

        result = _eval(self.expr)

        if decode:
            if isinstance(result, (bytes, bytearray, memoryview)):
                try:
                    result = bytes(result).decode(""utf-8"")
                except Exception:
                    result = bytes(result)

        return result"
113819,abseil/abseil-py,abseil_abseil-py/absl/app.py,absl.app.ExceptionHandler,"class ExceptionHandler:
    """"""Base exception handler from which other may inherit.""""""

    def wants(self, exc):
        """"""Returns whether this handler wants to handle the exception or not.

        This base class returns True for all exceptions by default. Override in
        subclass if it wants to be more selective.

        Args:
          exc: Exception, the current exception.
        """"""
        del exc
        return True

    def handle(self, exc):
        """"""Do something with the current exception.

        Args:
          exc: Exception, the current exception

        This method must be overridden.
        """"""
        raise NotImplementedError()","class ExceptionHandler:
    
    def wants(self, exc):
        
        pass
    def handle(self, exc):
        
        pass",snippet_33,3,"class ExceptionHandler:
    def __init__(self, exc_types=None, handler=None):
        if exc_types is None:
            exc_types = (Exception,)
        elif isinstance(exc_types, type):
            exc_types = (exc_types,)
        else:
            exc_types = tuple(exc_types)
        self._exc_types = exc_types
        if handler is not None and not callable(handler):
            raise TypeError(""handler must be callable"")
        self._handler = handler

    def wants(self, exc):
        if exc is None:
            return False
        if isinstance(exc, BaseException):
            return isinstance(exc, self._exc_types)
        if isinstance(exc, type) and issubclass(exc, BaseException):
            return any(issubclass(exc, t) for t in self._exc_types)
        return False

    def handle(self, exc):
        if not self.wants(exc):
            return None
        if self._handler is None:
            return exc
        return self._handler(exc)"
116413,QuantEcon/QuantEcon.py,quantecon/util/timing.py,quantecon.util.timing.__Timer__,"import time
import numpy as np

class __Timer__:
    """"""Computes elapsed time, between tic, tac, and toc.

    Methods
    -------
    tic :
        Resets timer.
    toc :
        Returns and prints time elapsed since last tic().
    tac :
        Returns and prints time elapsed since last
             tic(), tac() or toc() whichever occured last.
    loop_timer :
        Returns and prints the total and average time elapsed for n runs
        of a given function.

    """"""
    start = None
    last = None

    def tic(self):
        """"""
        Save time for future use with `tac()` or `toc()`.

        Returns
        -------
        None
            This function doesn't return a value.
        """"""
        t = time.time()
        self.start = t
        self.last = t

    def tac(self, verbose=True, digits=2):
        """"""
        Return and print elapsed time since last `tic()`, `tac()`, or
        `toc()`.

        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.

        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.

        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`, `tac()`, or `toc()`.

        """"""
        if self.start is None:
            raise Exception('tac() without tic()')
        t = time.time()
        elapsed = t - self.last
        self.last = t
        if verbose:
            m, s = divmod(elapsed, 60)
            h, m = divmod(m, 60)
            print('TAC: Elapsed: %d:%02d:%0d.%0*d' % (h, m, s, digits, s % 1 * 10 ** digits))
        return elapsed

    def toc(self, verbose=True, digits=2):
        """"""
        Return and print time elapsed since last `tic()`.

        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.

        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.

        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`.

        """"""
        if self.start is None:
            raise Exception('toc() without tic()')
        t = time.time()
        self.last = t
        elapsed = t - self.start
        if verbose:
            m, s = divmod(elapsed, 60)
            h, m = divmod(m, 60)
            print('TOC: Elapsed: %d:%02d:%0d.%0*d' % (h, m, s, digits, s % 1 * 10 ** digits))
        return elapsed

    def loop_timer(self, n, function, args=None, verbose=True, digits=2, best_of=3):
        """"""
        Return and print the total and average time elapsed for n runs
        of function.

        Parameters
        ----------
        n : scalar(int)
            Number of runs.

        function : function
            Function to be timed.

        args : list, optional(default=None)
            Arguments of the function.

        verbose : bool, optional(default=True)
            If True, then prints average time.

        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.

        best_of : scalar(int), optional(default=3)
            Average time over best_of runs.

        Returns
        -------
        average_time : scalar(float)
            Average time elapsed for n runs of function.

        average_of_best : scalar(float)
            Average of best_of times for n runs of function.

        """"""
        tic()
        all_times = np.empty(n)
        for run in range(n):
            if hasattr(args, '__iter__'):
                function(*args)
            elif args is None:
                function()
            else:
                function(args)
            all_times[run] = tac(verbose=False, digits=digits)
        elapsed = toc(verbose=False, digits=digits)
        m, s = divmod(elapsed, 60)
        h, m = divmod(m, 60)
        print('Total run time: %d:%02d:%0d.%0*d' % (h, m, s, digits, s % 1 * 10 ** digits))
        average_time = all_times.mean()
        average_of_best = np.sort(all_times)[:best_of].mean()
        if verbose:
            m, s = divmod(average_time, 60)
            h, m = divmod(m, 60)
            print('Average time for %d runs: %d:%02d:%0d.%0*d' % (n, h, m, s, digits, s % 1 * 10 ** digits))
            m, s = divmod(average_of_best, 60)
            h, m = divmod(m, 60)
            print('Average of %d best times: %d:%02d:%0d.%0*d' % (best_of, h, m, s, digits, s % 1 * 10 ** digits))
        return (average_time, average_of_best)","
class __Timer__:
    
    def tic(self):
        
        pass
    def tac(self, verbose=True, digits=2):
        
        pass

    def toc(self, verbose=True, digits=2):
        
        pass

    def loop_timer(self, n, function, args=None, verbose=True, digits=2, best_of=3):
        
        pass",snippet_35,5,"import time
from typing import Any, Callable, Dict, Iterable, Optional, Tuple, Union

class __Timer__:
    def __init__(self):
        self._t0: Optional[float] = None
        self._t_last: Optional[float] = None

    def tic(self):
        now = time.perf_counter()
        self._t0 = now
        self._t_last = now
        return now

    def tac(self, verbose: bool = True, digits: int = 2):
        now = time.perf_counter()
        if self._t_last is None:
            self.tic()
            self._t_last = time.perf_counter()
        elapsed = now - self._t_last
        self._t_last = now
        if verbose:
            print(f""{elapsed:.{digits}f} s"")
        return elapsed

    def toc(self, verbose: bool = True, digits: int = 2):
        now = time.perf_counter()
        if self._t0 is None:
            self.tic()
            self._t0 = time.perf_counter()
        elapsed = now - self._t0
        if verbose:
            print(f""{elapsed:.{digits}f} s"")
        return elapsed

    def loop_timer(
        self,
        n: int,
        function: Callable,
        args: Optional[Union[Tuple, list, Dict, Any]] = None,
        verbose: bool = True,
        digits: int = 2,
        best_of: int = 3,
    ):
        def call_func():
            if args is None:
                function()
            elif isinstance(args, dict):
                function(**args)
            elif isinstance(args, (tuple, list)):
                function(*args)
            else:
                function(args)

        best_total = float(""inf"")
        best_per_iter = float(""inf"")

        for _ in range(max(1, int(best_of))):
            start = time.perf_counter()
            for _i in range(int(n)):
                call_func()
            total = time.perf_counter() - start
            per_iter = total / n if n > 0 else float(""inf"")
            if total < best_total:
                best_total = total
                best_per_iter = per_iter

        if verbose:
            print(
                f""Best of {best_of}: total {best_total:.{digits}f} s; per iter {best_per_iter:.{digits}f} s""
            )

        return best_total, best_per_iter"
116421,DerwenAI/pytextrank,DerwenAI_pytextrank/pytextrank/base.py,pytextrank.base.Sentence,"import typing
from spacy.tokens import Doc, Span, Token
from dataclasses import dataclass

@dataclass
class Sentence:
    """"""
A data class representing the distance measure for one sentence.
    """"""
    start: int
    end: int
    sent_id: int
    phrases: typing.Set[int]
    distance: float

    def empty(self) -> bool:
        """"""
Test whether this sentence includes any ranked phrases.

    returns:
`True` if the `phrases` is not empty.
        """"""
        return len(self.phrases) == 0

    def text(self, doc: Doc) -> str:
        """"""
Accessor for the text slice of the `spaCy` [`Doc`](https://spacy.io/api/doc)
document represented by this sentence.

    doc:
source document

    returns:
the sentence text
        """"""
        return doc[self.start:self.end]","@dataclass
class Sentence:
    
    def empty(self) -> bool:
        
        pass
    def text(self, doc: Doc) -> str:
        
        pass",snippet_36,3,"from __future__ import annotations
from dataclasses import dataclass

@dataclass
class Sentence:
    start: int = 0
    end: int = 0

    def empty(self) -> bool:
        return self.start >= self.end

    def text(self, doc: 'Doc') -> str:
        if self.empty():
            return """"
        try:
            return doc[self.start:self.end].text
        except Exception:
            return """""
128871,arviz-devs/arviz,arviz-devs_arviz/arviz/utils.py,arviz.utils.interactive_backend,"import matplotlib.pyplot as plt

class interactive_backend:
    """"""Context manager to change backend temporarily in ipython sesson.

    It uses ipython magic to change temporarily from the ipython inline backend to
    an interactive backend of choice. It cannot be used outside ipython sessions nor
    to change backends different than inline -> interactive.

    Notes
    -----
    The first time ``interactive_backend`` context manager is called, any of the available
    interactive backends can be chosen. The following times, this same backend must be used
    unless the kernel is restarted.

    Parameters
    ----------
    backend : str, optional
        Interactive backend to use. It will be passed to ``%matplotlib`` magic, refer to
        its docs to see available options.

    Examples
    --------
    Inside an ipython session (i.e. a jupyter notebook) with the inline backend set:

    .. code::

        >>> import arviz as az
        >>> idata = az.load_arviz_data(""centered_eight"")
        >>> az.plot_posterior(idata) # inline
        >>> with az.interactive_backend():
        ...     az.plot_density(idata) # interactive
        >>> az.plot_trace(idata) # inline

    """"""

    def __init__(self, backend=''):
        """"""Initialize context manager.""""""
        try:
            from IPython import get_ipython
        except ImportError as err:
            raise ImportError(f'The exception below was risen while importing Ipython, this context manager can only be used inside ipython sessions:\n{err}') from err
        self.ipython = get_ipython()
        if self.ipython is None:
            raise EnvironmentError('This context manager can only be used inside ipython sessions')
        self.ipython.magic(f'matplotlib {backend}')

    def __enter__(self):
        """"""Enter context manager.""""""
        return self

    def __exit__(self, exc_type, exc_value, exc_tb):
        """"""Exit context manager.""""""
        plt.show(block=True)
        self.ipython.magic('matplotlib inline')","
class interactive_backend:
    '''Context manager to change backend temporarily in ipython sesson.
    It uses ipython magic to change temporarily from the ipython inline backend to
    an interactive backend of choice. It cannot be used outside ipython sessions nor
    to change backends different than inline -> interactive.
    Notes
    -----
    The first time ``interactive_backend`` context manager is called, any of the available
    interactive backends can be chosen. The following times, this same backend must be used
    unless the kernel is restarted.
    Parameters
    ----------
    backend : str, optional
        Interactive backend to use. It will be pass
ed to ``%matplotlib`` magic, refer to
        its docs to see available options.
    Examples
    --------
    Inside an ipython session (i.e. a jupyter notebook) with the inline backend set:
    .. code::
        >>> import arviz as az
        >>> idata = az.load_arviz_data(""centered_eight"")
        >>> az.plot_posterior(idata) # inline
        >>> with az.interactive_backend():
        ...     az.plot_density(idata) # interactive
        >>> az.plot_trace(idata) # inline
    '''
    def __init__(self, backend=''):
        
        pass

    def __enter__(self):
        
        pass

    def __exit__(self, exc_type, exc_value, exc_tb):
        
        pass",snippet_40,4,"class interactive_backend:
    '''Context manager to change backend temporarily in ipython sesson.
    It uses ipython magic to change temporarily from the ipython inline backend to
    an interactive backend of choice. It cannot be used outside ipython sessions nor
    to change backends different than inline -> interactive.
    Notes
    -----
    The first time ``interactive_backend`` context manager is called, any of the available
    interactive backends can be chosen. The following times, this same backend must be used
    unless the kernel is restarted.
    Parameters
    ----------
    backend : str, optional
        Interactive backend to use. It will be pass
ed to ``%matplotlib`` magic, refer to
        its docs to see available options.
    Examples
    --------
    Inside an ipython session (i.e. a jupyter notebook) with the inline backend set:
    .. code::
        >>> import arviz as az
        >>> idata = az.load_arviz_data(""centered_eight"")
        >>> az.plot_posterior(idata) # inline
        >>> with az.interactive_backend():
        ...     az.plot_density(idata) # interactive
        >>> az.plot_trace(idata) # inline
    '''
    # Remember the chosen interactive backend across uses in the same kernel
    _chosen_backend = None

    def __init__(self, backend=''):
        self._requested_backend = (backend or '').strip()
        self._switched = False
        self._prev_backend = None

    def __enter__(self):
        ip = self._get_ipython_or_raise()
        import matplotlib

        self._prev_backend = matplotlib.get_backend()

        if not self._is_inline_backend(self._prev_backend):
            raise RuntimeError(""interactive_backend can only be used when the current backend is inline."")

        # Determine which backend to use this session
        if interactive_backend._chosen_backend is None:
            # First use in this kernel
            if self._requested_backend:
                candidate_list = [self._requested_backend]
            else:
                candidate_list = self._default_candidates()

            chosen = self._pick_first_working_backend(ip, candidate_list)
            if chosen is None:
                raise RuntimeError(""No interactive matplotlib backend could be activated. ""
                                   ""Try specifying one explicitly, e.g., interactive_backend('qt'), ""
                                   ""and ensure the required GUI toolkit is installed."")
            interactive_backend._chosen_backend = chosen
        else:
            # Subsequent uses must match the originally chosen backend
            if self._requested_backend and self._requested_backend != interactive_backend._chosen_backend:
                raise RuntimeError(
                    f""interactive_backend already initialized with backend ""
                    f""{interactive_backend._chosen_backend!r}; cannot switch to ""
                    f""{self._requested_backend!r} without restarting the kernel.""
                )
            # Activate the previously chosen backend
            self._activate_backend(ip, interactive_backend._chosen_backend)

        self._switched = True
        return self

    def __exit__(self, exc_type, exc_value, exc_tb):
        if self._switched:
            ip = self._get_ipython_or_raise()
            # Always return to inline
            try:
                ip.run_line_magic('matplotlib', 'inline')
            except Exception:
                pass
        # Do not suppress exceptions
        return False

    # Helper methods

    @staticmethod
    def _get_ipython_or_raise():
        try:
            from IPython import get_ipython
        except Exception as e:
            raise RuntimeError(""interactive_backend requires an IPython environment."") from e
        ip = get_ipython()
        if ip is None:
            raise RuntimeError(""interactive_backend requires an IPython environment."")
        return ip

    @staticmethod
    def _is_inline_backend(backend_name):
        # Typical inline backend identifiers
        if not backend_name:
            return False
        name = str(backend_name).lower()
        return 'inline' in name or 'matplotlib_inline' in name or name.startswith('module://matplotlib_inline')

    @staticmethod
    def _default_candidates():
        import sys
        # Start with common modern choices, then fallbacks
        candidates = [
            'qt5', 'qt', 'qt4',  # Qt family
            'tk',                # TkAgg
            'wx',                # wxAgg
            'gtk3', 'gtk',       # GTK
        ]
        # macOS specific candidate that uses Cocoa
        if sys.platform == 'darwin':
            candidates = ['osx'] + candidates
        return candidates

    def _activate_backend(self, ip, backend):
        ip.run_line_magic('matplotlib', backend)

    def _pick_first_working_backend(self, ip, candidates):
        import matplotlib

        for cand in candidates:
            try:
                self._activate_backend(ip, cand)
                # Verify backend changed away from inline
                current = matplotlib.get_backend()
                if not self._is_inline_backend(current):
                    return cand
            except Exception:
                # Restore inline after failed attempt to be safe
                try:
                    ip.run_line_magic('matplotlib', 'inline')
                except Exception:
                    pass
                continue
        return None"
128872,arviz-devs/arviz,arviz-devs_arviz/arviz/utils.py,arviz.utils.lazy_property,"import functools

class lazy_property:
    """"""Used to load numba first time it is needed.""""""

    def __init__(self, fget):
        """"""Lazy load a property with `fget`.""""""
        self.fget = fget
        functools.update_wrapper(self, fget)

    def __get__(self, obj, cls):
        """"""Call the function, set the attribute.""""""
        if obj is None:
            return self
        value = self.fget(obj)
        setattr(obj, self.fget.__name__, value)
        return value","
class lazy_property:
    
    def __init__(self, fget):
        
        pass
    def __get__(self, obj, cls):
        
        pass",snippet_41,3,"class lazy_property:
    
    def __init__(self, fget):
        self.fget = fget
        self.attr_name = fget.__name__
        self.__doc__ = getattr(fget, ""__doc__"", None)

    def __get__(self, obj, cls):
        if obj is None:
            return self
        if self.attr_name in obj.__dict__:
            return obj.__dict__[self.attr_name]
        value = self.fget(obj)
        obj.__dict__[self.attr_name] = value
        return value"
129413,dtmilano/AndroidViewClient,dtmilano_AndroidViewClient/src/com/dtmilano/android/adb/adbclient.py,com.dtmilano.android.adb.adbclient.WifiManager,"import re
import sys

class WifiManager:
    """"""
    Simulates Android WifiManager.

    @see: http://developer.android.com/reference/android/net/wifi/WifiManager.html
    """"""
    WIFI_STATE_DISABLING = 0
    WIFI_STATE_DISABLED = 1
    WIFI_STATE_ENABLING = 2
    WIFI_STATE_ENABLED = 3
    WIFI_STATE_UNKNOWN = 4
    WIFI_IS_ENABLED_RE = re.compile('Wi-Fi is enabled')
    WIFI_IS_DISABLED_RE = re.compile('Wi-Fi is disabled')

    def __init__(self, device):
        """"""
        Constructor.
        :param device:
        :type device:
        """"""
        self.device = device

    def getWifiState(self):
        """"""
        Gets the Wi-Fi enabled state.

        @return: One of WIFI_STATE_DISABLED, WIFI_STATE_DISABLING, WIFI_STATE_ENABLED, WIFI_STATE_ENABLING, WIFI_STATE_UNKNOWN
        """"""
        result = self.device.shell('dumpsys wifi')
        if result:
            state = result.splitlines()[0]
            if self.WIFI_IS_ENABLED_RE.match(state):
                return self.WIFI_STATE_ENABLED
            elif self.WIFI_IS_DISABLED_RE.match(state):
                return self.WIFI_STATE_DISABLED
        print('UNKNOWN WIFI STATE:', state, file=sys.stderr)
        return self.WIFI_STATE_UNKNOWN","
class WifiManager:
    
    def __init__(self, device):
        
        pass
    def getWifiState(self):
        
        pass",snippet_42,3,"class WifiManager:
    def __init__(self, device):
        self._device = device
        self._last_state = None

    def _normalize_state(self, value):
        if value is None:
            return ""unknown""

        if isinstance(value, bool):
            return ""enabled"" if value else ""disabled""

        try:
            # Handle integers like 1/0
            if isinstance(value, (int, float)):
                return ""enabled"" if value != 0 else ""disabled""
        except Exception:
            pass

        s = str(value).strip().lower()

        truthy = {""on"", ""enabled"", ""enable"", ""true"", ""1"", ""up"", ""connected"", ""available""}
        falsy = {""off"", ""disabled"", ""disable"", ""false"", ""0"", ""down"", ""disconnected"", ""unavailable""}

        if s in truthy:
            return ""enabled""
        if s in falsy:
            return ""disabled""

        return ""unknown""

    def _get_from_attr_or_call(self, obj, names):
        for name in names:
            if hasattr(obj, name):
                attr = getattr(obj, name)
                try:
                    if callable(attr):
                        return attr()
                    return attr
                except Exception:
                    continue
        return None

    def getWifiState(self):
        dev = self._device

        # 1) Common method names that may directly provide state
        value = self._get_from_attr_or_call(dev, [
            ""get_wifi_state"",
            ""getWifiState"",
            ""wifi_state"",
            ""wifiState"",
        ])
        if value is not None:
            state = self._normalize_state(value)
            self._last_state = state
            return state

        # 2) Common boolean/flag attributes
        for name in [""wifi_enabled"", ""wifiEnabled"", ""is_wifi_enabled"", ""isWifiEnabled"", ""wifiOn"", ""wifi_on""]:
            if hasattr(dev, name):
                val = getattr(dev, name)
                state = self._normalize_state(val)
                self._last_state = state
                return state

        # 3) Nested wifi object with state/enabled
        if hasattr(dev, ""wifi""):
            wifi = getattr(dev, ""wifi"")
            value = self._get_from_attr_or_call(wifi, [
                ""state"",
                ""get_state"",
                ""getState"",
                ""enabled"",
                ""is_enabled"",
                ""isEnabled"",
            ])
            if value is not None:
                state = self._normalize_state(value)
                self._last_state = state
                return state

        # 4) Network interface like signals (best-effort)
        for name in [""network"", ""net"", ""radio""]:
            if hasattr(dev, name):
                obj = getattr(dev, name)
                value = self._get_from_attr_or_call(obj, [
                    ""wifi_state"",
                    ""get_wifi_state"",
                    ""wifiEnabled"",
                    ""is_wifi_enabled"",
                ])
                if value is not None:
                    state = self._normalize_state(value)
                    self._last_state = state
                    return state

        # 5) Fallback to last known or unknown
        return self._last_state or ""unknown"""
129620,neovim/pynvim,neovim_pynvim/pynvim/msgpack_rpc/async_session.py,pynvim.msgpack_rpc.async_session.Response,"from pynvim.msgpack_rpc.msgpack_stream import MsgpackStream

class Response:
    """"""Response to a msgpack-rpc request that came from Nvim.

    When Nvim sends a msgpack-rpc request, an instance of this class is
    created for remembering state required to send a response.
    """"""

    def __init__(self, msgpack_stream: MsgpackStream, request_id: int):
        """"""Initialize the Response instance.""""""
        self._msgpack_stream = msgpack_stream
        self._request_id = request_id

    def send(self, value, error=False):
        """"""Send the response.

        If `error` is True, it will be sent as an error.
        """"""
        if error:
            resp = [1, self._request_id, value, None]
        else:
            resp = [1, self._request_id, None, value]
        debug('sending response to request %d: %s', self._request_id, resp)
        self._msgpack_stream.send(resp)","
class Response:
    
    def __init__(self, msgpack_stream: MsgpackStream, request_id: int):
        
        pass
    def send(self, value, error=False):
        
        pass",snippet_43,3,"class Response:
    def __init__(self, msgpack_stream, request_id: int):
        self._stream = msgpack_stream
        self._id = request_id
        self._sent = False

    def send(self, value, error: bool = False):
        if self._sent:
            return False

        if error:
            msg = [1, self._id, value, None]
        else:
            msg = [1, self._id, None, value]

        sent = False
        for method_name in (""send"", ""write"", ""write_msg"", ""push"", ""put""):
            method = getattr(self._stream, method_name, None)
            if callable(method):
                method(msg)
                sent = True
                break

        if not sent:
            pack = getattr(self._stream, ""pack"", None)
            if callable(pack):
                pack(msg)
                flush = getattr(self._stream, ""flush"", None)
                if callable(flush):
                    flush()
                sent = True

        self._sent = True
        return True"
129967,sendgrid/sendgrid-python,sendgrid_sendgrid-python/sendgrid/helpers/eventwebhook/__init__.py,sendgrid.helpers.eventwebhook.EventWebhook,"from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.asymmetric import ec
from cryptography.exceptions import InvalidSignature
from cryptography.hazmat.primitives.serialization import load_pem_public_key
import base64

class EventWebhook:
    """"""
    This class allows you to use the Event Webhook feature. Read the docs for
    more details: https://sendgrid.com/docs/for-developers/tracking-events/event
    """"""

    def __init__(self, public_key=None):
        """"""
        Construct the Event Webhook verifier object
        :param public_key: verification key under Mail Settings
        :type public_key: string
        """"""
        self.public_key = self.convert_public_key_to_ecdsa(public_key) if public_key else public_key

    def convert_public_key_to_ecdsa(self, public_key):
        """"""
        Convert the public key string to an EllipticCurvePublicKey object.

        :param public_key: verification key under Mail Settings
        :type public_key string
        :return: An EllipticCurvePublicKey object using the ECDSA algorithm
        :rtype cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey
        """"""
        pem_key = '-----BEGIN PUBLIC KEY-----\n' + public_key + '\n-----END PUBLIC KEY-----'
        return load_pem_public_key(pem_key.encode('utf-8'))

    def verify_signature(self, payload, signature, timestamp, public_key=None):
        """"""
        Verify signed event webhook requests.

        :param payload: event payload in the request body
        :type payload: string
        :param signature: value obtained from the 'X-Twilio-Email-Event-Webhook-Signature' header
        :type signature: string
        :param timestamp: value obtained from the 'X-Twilio-Email-Event-Webhook-Timestamp' header
        :type timestamp: string
        :param public_key: elliptic curve public key
        :type public_key: cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey
        :return: true or false if signature is valid
        """"""
        timestamped_payload = (timestamp + payload).encode('utf-8')
        decoded_signature = base64.b64decode(signature)
        key = public_key or self.public_key
        try:
            key.verify(decoded_signature, timestamped_payload, ec.ECDSA(hashes.SHA256()))
            return True
        except InvalidSignature:
            return False","
class EventWebhook:
    
    def __init__(self, public_key=None):
        
        pass
    def convert_public_key_to_ecdsa(self, public_key):
        
        pass

    def verify_signature(self, payload, signature, timestamp, public_key=None):
        
        pass",snippet_44,4,"class EventWebhook:
    
    def __init__(self, public_key=None):
        self._public_key_raw = public_key
        self._verifying_key = None
        if public_key is not None:
            self._verifying_key = self.convert_public_key_to_ecdsa(public_key)

    def convert_public_key_to_ecdsa(self, public_key):
        import base64
        import textwrap
        try:
            import ecdsa
        except Exception as e:
            raise RuntimeError(""ecdsa package is required to use EventWebhook"") from e

        if isinstance(public_key, ecdsa.VerifyingKey):
            return public_key

        if isinstance(public_key, bytes):
            public_key = public_key.decode(""utf-8"")

        public_key = public_key.strip()

        if ""BEGIN PUBLIC KEY"" in public_key:
            pem = public_key
        else:
            # Assume base64-encoded DER SubjectPublicKeyInfo
            b64_clean = """".join(public_key.split())
            pem = ""-----BEGIN PUBLIC KEY-----\n""
            pem += ""\n"".join(textwrap.wrap(b64_clean, 64))
            pem += ""\n-----END PUBLIC KEY-----\n""

        try:
            vk = ecdsa.VerifyingKey.from_pem(pem)
        except Exception as e:
            raise ValueError(""Invalid public key"") from e
        return vk

    def verify_signature(self, payload, signature, timestamp, public_key=None):
        import base64
        import hashlib
        try:
            import ecdsa
        except Exception:
            return False

        vk = None
        if public_key is not None:
            try:
                vk = self.convert_public_key_to_ecdsa(public_key)
            except Exception:
                return False
        elif self._verifying_key is not None:
            vk = self._verifying_key
        else:
            return False

        if isinstance(payload, bytes):
            payload_bytes = payload
        else:
            payload_bytes = str(payload).encode(""utf-8"")

        ts_str = str(timestamp)
        sig_b: bytes
        try:
            sig_b = base64.b64decode(signature, validate=True)
        except Exception:
            return False

        # Build candidate messages (compatibility with known variants)
        candidates = [
            ts_str.encode(""utf-8"") + payload_bytes,                         # timestamp + payload
            f""{ts_str}.{payload_bytes.decode('utf-8', errors='ignore')}"".encode(""utf-8""),  # timestamp.payload
        ]

        # Try DER-encoded signature first (expected)
        for msg in candidates:
            try:
                if vk.verify(sig_b, msg, hashfunc=hashlib.sha256, sigdecode=ecdsa.util.sigdecode_der):
                    return True
            except Exception:
                pass

        # Fallback: try raw (r||s) signature format if ever encountered
        for msg in candidates:
            try:
                if vk.verify(sig_b, msg, hashfunc=hashlib.sha256, sigdecode=ecdsa.util.sigdecode_string):
                    return True
            except Exception:
                pass

        return False"
131054,SoCo/SoCo,SoCo_SoCo/soco/music_services/token_store.py,soco.music_services.token_store.TokenStoreBase,"class TokenStoreBase:
    """"""Token store base class""""""

    def __init__(self, token_collection='default'):
        """"""Instantiate instance variables

        Args:
            token_collection (str): The name of the token collection to use. This may be
                used to store different token collections for different client programs.
        """"""
        self.token_collection = token_collection

    def save_token_pair(self, music_service_id, household_id, token_pair):
        """"""Save a token value pair (token, key) which is a 2 item sequence""""""
        raise NotImplementedError

    def load_token_pair(self, music_service_id, household_id):
        """"""Load a token pair (token, key) which is a 2 item sequence""""""
        raise NotImplementedError

    def has_token(self, music_service_id, household_id):
        """"""Return True if a token is stored for the music service and household ID""""""
        raise NotImplementedError","class TokenStoreBase:
    
    def __init__(self, token_collection='default'):
        
        pass
    def save_token_pair(self, music_service_id, household_id, token_pair):
        
        pass
    def load_token_pair(self, music_service_id, household_id):
        
        pass

    def has_token(self, music_service_id, household_id):
        
        pass",snippet_45,5,"class TokenStoreBase:
    
    def __init__(self, token_collection='default'):
        self.token_collection = token_collection
        self._tokens = {}

    def save_token_pair(self, music_service_id, household_id, token_pair):
        if music_service_id is None or household_id is None:
            raise ValueError(""music_service_id and household_id must be provided"")
        self._tokens[(music_service_id, household_id)] = token_pair

    def load_token_pair(self, music_service_id, household_id):
        if music_service_id is None or household_id is None:
            raise ValueError(""music_service_id and household_id must be provided"")
        return self._tokens.get((music_service_id, household_id))

    def has_token(self, music_service_id, household_id):
        if music_service_id is None or household_id is None:
            return False
        return (music_service_id, household_id) in self._tokens"
131060,SoCo/SoCo,SoCo_SoCo/soco/plugins/sharelink.py,soco.plugins.sharelink.ShareClass,"class ShareClass:
    """"""Base class for supported services.""""""

    def canonical_uri(self, uri):
        """"""Recognize a share link and return its canonical representation.

        Args:
            uri (str): A URI like ""https://tidal.com/browse/album/157273956"".

        Returns:
            str: The canonical URI or None if not recognized.
        """"""
        raise NotImplementedError

    def service_number(self):
        """"""Return the service number.

        Returns:
            int: A number identifying the supported music service.
        """"""
        raise NotImplementedError

    @staticmethod
    def magic():
        """"""Return magic.

        Returns:
            dict: Magic prefix/key/class values for each share type.
        """"""
        return {'album': {'prefix': 'x-rincon-cpcontainer:1004206c', 'key': '00040000', 'class': 'object.container.album.musicAlbum'}, 'episode': {'prefix': '', 'key': '00032020', 'class': 'object.item.audioItem.musicTrack'}, 'track': {'prefix': '', 'key': '00032020', 'class': 'object.item.audioItem.musicTrack'}, 'show': {'prefix': 'x-rincon-cpcontainer:1006206c', 'key': '1006206c', 'class': 'object.container.playlistContainer'}, 'song': {'prefix': '', 'key': '10032020', 'class': 'object.item.audioItem.musicTrack'}, 'playlist': {'prefix': 'x-rincon-cpcontainer:1006206c', 'key': '1006206c', 'class': 'object.container.playlistContainer'}}

    def extract(self, uri):
        """"""Extract the share type and encoded URI from a share link.

        Returns:
            share_type: The shared type, like ""album"" or ""track"".
            encoded_uri: An escaped URI with a service-specific format.
        """"""
        raise NotImplementedError","class ShareClass:
    
    def canonical_uri(self, uri):
        
        pass
    def service_number(self):
        
        pass
    @staticmethod
    def magic():
        
        pass

    def extract(self, uri):
        
        pass",snippet_46,5,"class ShareClass:
    def canonical_uri(self, uri):
        if uri is None:
            return None
        if not isinstance(uri, str):
            raise TypeError(""uri must be a string or None"")
        uri = uri.strip()
        if not uri:
            return None

        from urllib.parse import urlparse, urlunparse, quote, unquote

        # Prepend scheme if missing
        if ""://"" not in uri and uri.startswith(""www.""):
            uri = ""http://"" + uri
        elif ""://"" not in uri and not uri.startswith((""http://"", ""https://"")):
            uri = ""http://"" + uri

        parsed = urlparse(uri)

        scheme = (parsed.scheme or ""http"").lower()
        netloc = parsed.netloc.lower()

        # Remove default ports
        if netloc.endswith("":80"") and scheme == ""http"":
            netloc = netloc[:-3]
        elif netloc.endswith("":443"") and scheme == ""https"":
            netloc = netloc[:-4]

        # Normalize path: remove duplicate slashes, percent-encode unsafe chars
        path = parsed.path or ""/""
        while ""//"" in path:
            path = path.replace(""//"", ""/"")

        # Remove trailing slash unless root
        if path != ""/"" and path.endswith(""/""):
            path = path[:-1]

        # Normalize percent-encoding (decode then re-encode)
        try:
            path = quote(unquote(path), safe=""/:@&+$,;=-._~()"")
        except Exception:
            pass

        # Sort query parameters
        query = parsed.query
        if query:
            from urllib.parse import parse_qsl, urlencode
            qsl = parse_qsl(query, keep_blank_values=True)
            qsl.sort()
            query = urlencode(qsl, doseq=True)

        fragment = """"  # drop fragment for canonical form

        canonical = urlunparse((scheme, netloc, path, """", query, fragment))
        return canonical

    def service_number(self):
        # Stable deterministic number based on class name
        import hashlib
        h = hashlib.md5(self.__class__.__name__.encode(""utf-8"")).hexdigest()
        return int(h[:8], 16)

    @staticmethod
    def magic():
        return ""magic""

    def extract(self, uri):
        """"""
        Extract a likely share identifier from the URI.
        Preference order:
        - Query params: id, share_id, share, s
        - Last non-empty path segment
        Returns None if nothing reasonable is found.
        """"""
        from urllib.parse import urlparse, parse_qs

        canon = self.canonical_uri(uri)
        if not canon:
            return None

        parsed = urlparse(canon)
        qs = parse_qs(parsed.query)

        for key in (""id"", ""share_id"", ""share"", ""s""):
            if key in qs and qs[key]:
                val = qs[key][0].strip()
                if val:
                    return val

        # Last non-empty path segment
        segments = [seg for seg in (parsed.path or """").split(""/"") if seg]
        if segments:
            return segments[-1]

        return None"
131928,mjg59/python-broadlink,mjg59_python-broadlink/broadlink/protocol.py,broadlink.protocol.Datetime,"import time
import datetime as dt

class Datetime:
    """"""Helps to pack and unpack datetime objects for the Broadlink protocol.""""""

    @staticmethod
    def pack(datetime: dt.datetime) -> bytes:
        """"""Pack the timestamp to be sent over the Broadlink protocol.""""""
        data = bytearray(12)
        utcoffset = int(datetime.utcoffset().total_seconds() / 3600)
        data[:4] = utcoffset.to_bytes(4, 'little', signed=True)
        data[4:6] = datetime.year.to_bytes(2, 'little')
        data[6] = datetime.minute
        data[7] = datetime.hour
        data[8] = int(datetime.strftime('%y'))
        data[9] = datetime.isoweekday()
        data[10] = datetime.day
        data[11] = datetime.month
        return data

    @staticmethod
    def unpack(data: bytes) -> dt.datetime:
        """"""Unpack a timestamp received over the Broadlink protocol.""""""
        utcoffset = int.from_bytes(data[0:4], 'little', signed=True)
        year = int.from_bytes(data[4:6], 'little')
        minute = data[6]
        hour = data[7]
        subyear = data[8]
        isoweekday = data[9]
        day = data[10]
        month = data[11]
        tz_info = dt.timezone(dt.timedelta(hours=utcoffset))
        datetime = dt.datetime(year, month, day, hour, minute, 0, 0, tz_info)
        if datetime.isoweekday() != isoweekday:
            raise ValueError('isoweekday does not match')
        if int(datetime.strftime('%y')) != subyear:
            raise ValueError('subyear does not match')
        return datetime

    @staticmethod
    def now() -> dt.datetime:
        """"""Return the current date and time with timezone info.""""""
        tz_info = dt.timezone(dt.timedelta(seconds=-time.timezone))
        return dt.datetime.now(tz_info)","
class Datetime:
    
    @staticmethod
    def pack(datetime: dt.datetime) -> bytes:
        
        pass
    @staticmethod
    def unpack(data: bytes) -> dt.datetime:
        
        pass
    @staticmethod
    def now() -> dt.datetime:
        
        pass",snippet_48,4,"import datetime as dt

class Datetime:
    
    @staticmethod
    def pack(datetime: dt.datetime) -> bytes:
        return datetime.isoformat().encode(""utf-8"")

    @staticmethod
    def unpack(data: bytes) -> dt.datetime:
        return dt.datetime.fromisoformat(data.decode(""utf-8""))

    @staticmethod
    def now() -> dt.datetime:
        return dt.datetime.now(dt.timezone.utc)"
132032,rigetti/pyquil,pyquil/quilatom.py,pyquil.quilatom.QuilAtom,"class QuilAtom:
    """"""Abstract class for atomic elements of Quil.""""""

    def out(self) -> str:
        """"""Return the element as a valid Quil string.""""""
        raise NotImplementedError()

    def __str__(self) -> str:
        """"""Get a string representation of the element, possibly not valid Quil.""""""
        raise NotImplementedError()

    def __eq__(self, other: object) -> bool:
        """"""Return True if the other object is equal to this one.""""""
        raise NotImplementedError()

    def __hash__(self) -> int:
        """"""Return a hash of the object.""""""
        raise NotImplementedError()","class QuilAtom:
    
    def out(self) -> str:
        
        pass
    def __str__(self) -> str:
        
        pass
    def __eq__(self, other: object) -> bool:
        
        pass

    def __hash__(self) -> int:
        
        pass",snippet_49,5,"class QuilAtom:
    def out(self) -> str:
        raise NotImplementedError(""Subclasses must implement 'out' to return a string representation."")

    def __str__(self) -> str:
        return self.out()

    def __eq__(self, other: object) -> bool:
        return self is other

    def __hash__(self) -> int:
        return id(self)"
134597,Unidata/MetPy,Unidata_MetPy/tools/flake8-metpy/flake8_metpy.py,flake8_metpy.MetPyChecker,"class MetPyChecker:
    """"""Flake8 plugin class to check MetPy style/best practice.""""""
    name = __name__
    version = '1.0'

    def __init__(self, tree):
        """"""Initialize the plugin.""""""
        self.tree = tree

    def run(self):
        """"""Run the plugin and yield errors.""""""
        visitor = MetPyVisitor()
        visitor.visit(self.tree)
        for err in visitor.errors:
            yield self.error(err)

    def error(self, err):
        """"""Format errors into Flake8's required format.""""""
        return (err.lineno, err.col, f'MPY{err.code:03d}: Multiplying/dividing by units--use units.Quantity()', type(self))","class MetPyChecker:
    
    def __init__(self, tree):
        
        pass
    def run(self):
        
        pass
    def error(self, err):
        
        pass",snippet_50,4,"class MetPyChecker:
    name = ""metpy-checker""
    version = ""0.1.0""

    def __init__(self, tree):
        self.tree = tree
        self._errors = []

    def run(self):
        for err in self._errors:
            yield err

    def error(self, err):
        # Normalize different error formats to (line, col, msg, type)
        if isinstance(err, tuple):
            if len(err) == 4:
                line, col, msg, typ = err
                self._errors.append((int(line), int(col), str(msg), typ))
                return
            if len(err) == 3:
                line, col, msg = err
                self._errors.append((int(line), int(col), str(msg), type(self)))
                return
        # Fallback: store as a generic error on line 1, col 0 with stringified message
        self._errors.append((1, 0, str(err), type(self)))"
134620,Unidata/MetPy,Unidata_MetPy/src/metpy/io/_tools.py,metpy.io._tools.Array,"from struct import Struct

class Array:
    """"""Use a Struct as a callable to unpack a bunch of bytes as a list.""""""

    def __init__(self, fmt):
        """"""Initialize the Struct unpacker.""""""
        self._struct = Struct(fmt)

    def __call__(self, buf):
        """"""Perform the actual unpacking.""""""
        return list(self._struct.unpack(buf))","
class Array:
    
    def __init__(self, fmt):
        
        pass
    def __call__(self, buf):
        
        pass",snippet_51,3,"class Array:
    
    def __init__(self, fmt):
        import struct
        if not isinstance(fmt, str) or not fmt:
            raise ValueError(""fmt must be a non-empty struct format string"")
        self._struct = struct.Struct(fmt)
        self._size = self._struct.size

    def __call__(self, buf):
        mv = memoryview(buf)
        if len(mv) % self._size != 0:
            raise ValueError(""Buffer size is not a multiple of the struct size"")
        out = []
        unpack = self._struct.unpack
        size = self._size
        for i in range(0, len(mv), size):
            item = unpack(mv[i:i + size])
            if len(item) == 1:
                out.append(item[0])
            else:
                out.append(item)
        return out"
134621,Unidata/MetPy,Unidata_MetPy/src/metpy/io/_tools.py,metpy.io._tools.BitField,"class BitField:
    """"""Convert an integer to a string for each bit.""""""

    def __init__(self, *names):
        """"""Initialize the list of named bits.""""""
        self._names = names

    def __call__(self, val):
        """"""Return a list with a string for each True bit in the integer.""""""
        if not val:
            return None
        bits = []
        for n in self._names:
            if val & 1:
                bits.append(n)
            val >>= 1
            if not val:
                break
        return bits[0] if len(bits) == 1 else bits","class BitField:
    
    def __init__(self, *names):
        
        pass
    def __call__(self, val):
        
        pass",snippet_52,3,"class BitField:
    
    def __init__(self, *names):
        if not names:
            raise ValueError(""At least one bit name is required"")
        seen = set()
        cleaned = []
        for n in names:
            if not isinstance(n, str) or not n:
                raise TypeError(""Bit names must be non-empty strings"")
            if not n.isidentifier():
                raise ValueError(f""Invalid identifier for bit name: {n!r}"")
            if n in seen:
                raise ValueError(f""Duplicate bit name: {n!r}"")
            seen.add(n)
            cleaned.append(n)
        self._names = tuple(cleaned)
        self._bits = {name: 1 << i for i, name in enumerate(self._names)}
    
    def __call__(self, val):
        if not isinstance(val, int):
            raise TypeError(""Value must be an integer bitmask"")
        if val < 0:
            raise ValueError(""Bitmask must be non-negative"")
        # build an object with attributes per bit name
        # using a simple, lightweight object
        class _Flags:
            __slots__ = tuple(self._names) + (""__value__"",)
        flags = _Flags()
        for name, bit in self._bits.items():
            setattr(flags, name, bool(val & bit))
        setattr(flags, ""__value__"", val)
        return flags"
134622,Unidata/MetPy,Unidata_MetPy/src/metpy/io/_tools.py,metpy.io._tools.Bits,"class Bits:
    """"""Breaks an integer into a specified number of True/False bits.""""""

    def __init__(self, num_bits):
        """"""Initialize the number of bits.""""""
        self._bits = range(num_bits)

    def __call__(self, val):
        """"""Convert the integer to the list of True/False values.""""""
        return [bool(val >> i & 1) for i in self._bits]","class Bits:
    
    def __init__(self, num_bits):
        
        pass
    def __call__(self, val):
        
        pass",snippet_53,3,"class Bits:
    
    def __init__(self, num_bits):
        if not isinstance(num_bits, int):
            raise TypeError(""num_bits must be an integer"")
        if num_bits <= 0:
            raise ValueError(""num_bits must be a positive integer"")
        self.num_bits = num_bits
        self.mask = (1 << num_bits) - 1

    def __call__(self, val):
        if not isinstance(val, int):
            raise TypeError(""val must be an integer"")
        return val & self.mask"
134624,Unidata/MetPy,Unidata_MetPy/src/metpy/io/_tools.py,metpy.io._tools.Enum,"class Enum:
    """"""Map values to specific strings.""""""

    def __init__(self, *args, **kwargs):
        """"""Initialize the mapping.""""""
        self.val_map = dict(enumerate(args))
        self.val_map.update(zip(kwargs.values(), kwargs.keys(), strict=False))

    def __call__(self, val):
        """"""Map an integer to the string representation.""""""
        return self.val_map.get(val, f'Unknown ({val})')","class Enum:
    
    def __init__(self, *args, **kwargs):
        
        pass
    def __call__(self, val):
        
        pass",snippet_54,3,"class Enum:
    
    def __init__(self, *args, **kwargs):
        self._names = {}
        values = []
        if len(args) == 1 and hasattr(args[0], '__iter__') and not isinstance(args[0], (str, bytes)):
            values = list(args[0])
        else:
            values = list(args)
        for name, val in kwargs.items():
            self._names[name] = val
        self._values = set(values) | set(self._names.values())
    
    def __call__(self, val):
        if val in self._names:
            return self._names[val]
        if val in self._values:
            return val
        raise ValueError(f""Invalid enum value: {val}. Allowed: {sorted(self._values)} or names: {sorted(self._names.keys())}"")"
134657,Unidata/MetPy,Unidata_MetPy/src/metpy/package_tools.py,metpy.package_tools.Exporter,"class Exporter:
    """"""Manages exporting of symbols from the module.

    Grabs a reference to `globals()` for a module and provides a decorator to add
    functions and classes to `__all__` rather than requiring a separately maintained list.
    Also provides a context manager to do this for instances by adding all instances added
    within a block to `__all__`.
    """"""

    def __init__(self, globls):
        """"""Initialize the Exporter.""""""
        self.globls = globls
        self.exports = globls.setdefault('__all__', [])

    def export(self, defn):
        """"""Declare a function or class as exported.""""""
        self.exports.append(defn.__name__)
        return defn

    def __enter__(self):
        """"""Start a block tracking all instances created at global scope.""""""
        self.start_vars = set(self.globls)

    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""Exit the instance tracking block.""""""
        self.exports.extend(set(self.globls) - self.start_vars)
        del self.start_vars","class Exporter:
    
    def __init__(self, globls):
        
        pass
    def export(self, defn):
        
        pass
    def __enter__(self):
        
        pass

    def __exit__(self, exc_type, exc_val, exc_tb):
        
        pass",snippet_55,5,"class Exporter:
    
    def __init__(self, globls):
        self._globals = globls

    def export(self, obj=None, *, name=None):
        if obj is None:
            def decorator(target):
                return self.export(target, name=name)
            return decorator

        export_name = name or getattr(obj, ""__name__"", None)
        if not export_name:
            raise ValueError(""Cannot determine export name. Provide a name=... or an object with __name__."")

        self._globals[export_name] = obj

        all_list = self._globals.get(""__all__"")
        if not isinstance(all_list, list):
            all_list = []
            self._globals[""__all__""] = all_list
        if export_name not in all_list:
            all_list.append(export_name)

        return obj

    def __enter__(self):
        return self.export

    def __exit__(self, exc_type, exc_val, exc_tb):
        return False"
137119,jpype-project/jpype,jpype-project_jpype/doc/java/util.py,java.util.Collection,"class Collection:
    """""" Customized interface representing a collection of items.

    JPype wraps ``java.util.Collection`` as a Python collection.
    """"""

    def __len__(self) -> int:
        """""" Get the length of this collection.

        Use ``len(collection)`` to find the number of items in this
        collection.

        """"""
        ...

    def __delitem__(self, item):
        """""" Collections do not support remove by index. """"""
        ...

    def __contains__(self, item) -> bool:
        """""" Check if this collection contains this item.

        Use ``item in collection`` to check if the item is 
        present.

        Args:
           item: is the item to check for.  This must be a Java
           object or an object which can be automatically converted
           such as a string.

        Returns:
           bool: True if the item is in the collection.
        """"""
        ...","class Collection:
    
    def __len__(self) -> int:
        
        pass
    def __delitem__(self, item):
        
        pass
    def __contains__(self, item) -> bool:
        
        pass",snippet_56,4,"class Collection:
    def __init__(self, iterable=None):
        self._items = list(iterable) if iterable is not None else []

    def __len__(self) -> int:
        return len(self._items)

    def __delitem__(self, item):
        if isinstance(item, (int, slice)):
            del self._items[item]
        else:
            raise TypeError(""Indices must be integers or slices"")

    def __contains__(self, item) -> bool:
        return item in self._items"
138700,sebp/scikit-survival,sebp_scikit-survival/sksurv/util.py,sksurv.util.Surv,"from sklearn.utils.validation import check_array, check_consistent_length
import numpy as np
import pandas as pd

class Surv:
    """"""A helper class to create a structured array for survival analysis.

    This class provides helper functions to create a structured array that
    encapsulates the event indicator and the observed time. The resulting
    structured array is the recommended format for the ``y`` argument in
    scikit-survival's estimators.
    """"""

    @staticmethod
    def from_arrays(event, time, name_event=None, name_time=None):
        """"""Create structured array from event indicator and time arrays.

        Parameters
        ----------
        event : array-like, shape=(n_samples,)
            Event indicator. A boolean array or array with values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : array-like, shape=(n_samples,)
            Observed time. Time to event or time of censoring.
        name_event : str, optional, default: 'event'
            Name of the event field in the structured array.
        name_time : str, optional, default: 'time'
            Name of the observed time field in the structured array.

        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are set to the values of `name_event` and `name_time`.

        Examples
        --------
        >>> from sksurv.util import Surv
        >>>
        >>> y = Surv.from_arrays(event=[True, False, True],
        ...                      time=[10, 25, 15])
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('event', '?'), ('time', '<f8')])
        >>> y['event']
        array([ True, False,  True])
        >>> y['time']
        array([10., 25., 15.])
        """"""
        name_event = name_event or 'event'
        name_time = name_time or 'time'
        if name_time == name_event:
            raise ValueError('name_time must be different from name_event')
        time = np.asanyarray(time, dtype=float)
        y = np.empty(time.shape[0], dtype=[(name_event, bool), (name_time, float)])
        y[name_time] = time
        event = np.asanyarray(event)
        check_consistent_length(time, event)
        if np.issubdtype(event.dtype, np.bool_):
            y[name_event] = event
        else:
            events = np.unique(event)
            events.sort()
            if len(events) != 2:
                raise ValueError('event indicator must be binary')
            if np.all(events == np.array([0, 1], dtype=events.dtype)):
                y[name_event] = event.astype(bool)
            else:
                raise ValueError('non-boolean event indicator must contain 0 and 1 only')
        return y

    @staticmethod
    def from_dataframe(event, time, data):
        """"""Create structured array from columns in a pandas DataFrame.

        Parameters
        ----------
        event : str
            Name of the column in ``data`` containing the event indicator.
            It must be a boolean or have values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : str
            Name of the column in ``data`` containing the observed time
            (time to event or time of censoring).
        data : pandas.DataFrame
            A DataFrame with columns for event and time.

        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are the respective column names.

        Examples
        --------
        >>> import pandas as pd
        >>> from sksurv.util import Surv
        >>>
        >>> df = pd.DataFrame({
        ...     'status': [True, False, True],
        ...     'followup_time': [10, 25, 15],
        ... })
        >>> y = Surv.from_dataframe(
        ...     event='status', time='followup_time', data=df,
        ... )
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('status', '?'), ('followup_time', '<f8')])
        >>> y['status']
        array([ True, False,  True])
        >>> y['followup_time']
        array([10., 25., 15.])
        """"""
        if not isinstance(data, pd.DataFrame):
            raise TypeError(f'expected pandas.DataFrame, but got {type(data)!r}')
        return Surv.from_arrays(data.loc[:, event].values, data.loc[:, time].values, name_event=str(event), name_time=str(time))","
class Surv:
    
    @staticmethod
    def from_arrays(event, time, name_event=None, name_time=None):
        '''Create structured array from event indicator and time arrays.
        Parameters
        ----------
        event : array-like, shape=(n_samples,)
            Event indicator. A boolean array or array with values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : array-like, shape=(n_samples,)
            Observed time. Time to event or time of censoring.
        name_event : str, optional, default: 'event'
            Name of the event field in the structured array.
        name_time : str, optional, default: 'time'
            Name of the observed time field in the structured array.
        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are set to the values of `name_event` and `name_time`.
        Examples
        --------
        >>> from sksurv.util import Surv
        >>>
        >>> y = Surv.from_arrays(event=[True, False, True],
        ...                      time=[10, 25, 15])
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('event', '?'), ('time', '<f8')])
        >>> y['event']
        array([ True, False,  True])
        >>> y['time']
        array([10., 25., 15.])
        '''
        pass
    @staticmethod
    def from_dataframe(event, time, data):
        '''Create structured array from columns in a pandas DataFrame.
        Parameters
        ----------
        event : str
            Name of the column in ``data`` containing the event indicator.
            It must be a boolean or have values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : str
            Name of the column in ``data`` containing the observed time
            (time to event or time of censoring).
        data : pandas.DataFrame
            A DataFrame with columns for event and time.
        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are the respective column names.
        Examples
        --------
        >>> import pandas as pd
        >>> from sksurv.util import Surv
        >>>
        >>> df = pd.DataFrame({
        ...     'status': [True, False, True],
        ...     'followup_time': [10, 25, 15],
        ... })
        >>> y = Surv.from_dataframe(
        ...     event='status', time='followup_time', data=df,
        ... )
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('status', '?'), ('followup_time', '<f8')])
        >>> y['status']
        array([ True, False,  True])
        >>> y['followup_time']
        array([10., 25., 15.])
        '''
        pass",snippet_57,3,"import numpy as np


class Surv:
    
    @staticmethod
    def from_arrays(event, time, name_event=None, name_time=None):
        '''Create structured array from event indicator and time arrays.
        Parameters
        ----------
        event : array-like, shape=(n_samples,)
            Event indicator. A boolean array or array with values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : array-like, shape=(n_samples,)
            Observed time. Time to event or time of censoring.
        name_event : str, optional, default: 'event'
            Name of the event field in the structured array.
        name_time : str, optional, default: 'time'
            Name of the observed time field in the structured array.
        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are set to the values of `name_event` and `name_time`.
        Examples
        --------
        >>> from sksurv.util import Surv
        >>>
        >>> y = Surv.from_arrays(event=[True, False, True],
        ...                      time=[10, 25, 15])
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('event', '?'), ('time', '<f8')])
        >>> y['event']
        array([ True, False,  True])
        >>> y['time']
        array([10., 25., 15.])
        '''
        name_event = 'event' if name_event is None else str(name_event)
        name_time = 'time' if name_time is None else str(name_time)

        ev = np.asarray(event)
        tm = np.asarray(time)

        if ev.ndim != 1 or tm.ndim != 1:
            raise ValueError(""event and time must be one-dimensional arrays."")
        if ev.shape[0] != tm.shape[0]:
            raise ValueError(""event and time must have the same length."")

        # Convert event to boolean
        if ev.dtype == np.bool_:
            ev_bool = ev
        else:
            if ev.dtype.kind in ('i', 'u'):  # integer types
                unique_vals = np.unique(ev)
                if not np.all(np.isin(unique_vals, [0, 1])):
                    raise ValueError(""event must be boolean or contain only 0/1 values."")
                ev_bool = ev.astype(bool)
            elif ev.dtype.kind == 'f':  # float types
                if not np.all(np.isfinite(ev)):
                    raise ValueError(""event contains NaN or infinite values."")
                unique_vals = np.unique(ev)
                if not np.all(np.isin(unique_vals, [0.0, 1.0])):
                    raise ValueError(""event must be boolean or contain only 0/1 values."")
                ev_bool = ev.astype(bool)
            else:
                raise ValueError(""event must be boolean or numeric array with 0/1 values."")

        # Convert time to float
        tm_float = tm.astype(float)
        if not np.all(np.isfinite(tm_float)):
            raise ValueError(""time contains NaN or infinite values."")

        dtype = np.dtype([(name_event, np.bool_), (name_time, np.float64)])
        y = np.empty(ev_bool.shape[0], dtype=dtype)
        y[name_event] = ev_bool
        y[name_time] = tm_float
        return y

    @staticmethod
    def from_dataframe(event, time, data):
        '''Create structured array from columns in a pandas DataFrame.
        Parameters
        ----------
        event : str
            Name of the column in ``data`` containing the event indicator.
            It must be a boolean or have values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : str
            Name of the column in ``data`` containing the observed time
            (time to event or time of censoring).
        data : pandas.DataFrame
            A DataFrame with columns for event and time.
        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are the respective column names.
        Examples
        --------
        >>> import pandas as pd
        >>> from sksurv.util import Surv
        >>>
        >>> df = pd.DataFrame({
        ...     'status': [True, False, True],
        ...     'followup_time': [10, 25, 15],
        ... })
        >>> y = Surv.from_dataframe(
        ...     event='status', time='followup_time', data=df,
        ... )
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('status', '?'), ('followup_time', '<f8')])
        >>> y['status']
        array([ True, False,  True])
        >>> y['followup_time']
        array([10., 25., 15.])
        '''
        if not hasattr(data, ""__getitem__""):
            raise TypeError(""data must be a pandas DataFrame-like object supporting column access."")

        if event not in data or time not in data:
            raise KeyError(""Specified event or time column not found in data."")

        ev_col = data[event]
        tm_col = data[time]

        return Surv.from_arrays(ev_col, tm_col, name_event=event, name_time=time)"
139652,sentinel-hub/eo-learn,sentinel-hub_eo-learn/eolearn/geometry/morphology.py,eolearn.geometry.morphology.MorphologicalStructFactory,"import cv2
import numpy as np

class MorphologicalStructFactory:
    """"""
    Factory methods for generating morphological structuring elements
    """"""

    @staticmethod
    def get_disk(radius: int) -> np.ndarray:
        """"""
        :param radius: Radius of disk
        :return: The structuring element where elements of the neighborhood are 1 and 0 otherwise.
        """"""
        return cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (radius, radius))

    @staticmethod
    def get_rectangle(width: int, height: int) -> np.ndarray:
        """"""
        :param width: Width of rectangle
        :param height: Height of rectangle
        :return: A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.
        """"""
        return cv2.getStructuringElement(cv2.MORPH_RECT, (height, width))

    @staticmethod
    def get_square(width: int) -> np.ndarray:
        """"""
        :param width: Size of square
        :return: A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.
        """"""
        return cv2.getStructuringElement(cv2.MORPH_RECT, (width, width))","
class MorphologicalStructFactory:
    
    @staticmethod
    def get_disk(radius: int) -> np.ndarray:
        
        pass
    @staticmethod
    def get_rectangle(width: int, height: int) -> np.ndarray:
        
        pass
    @staticmethod
    def get_square(width: int) -> np.ndarray:
        
        pass",snippet_58,4,"import numpy as np

class MorphologicalStructFactory:
    
    @staticmethod
    def get_disk(radius: int) -> np.ndarray:
        if not isinstance(radius, int):
            raise TypeError(""radius must be an integer"")
        if radius < 0:
            raise ValueError(""radius must be >= 0"")
        if radius == 0:
            return np.ones((1, 1), dtype=bool)
        y, x = np.ogrid[-radius:radius+1, -radius:radius+1]
        mask = x*x + y*y <= radius*radius
        return mask.astype(bool)

    @staticmethod
    def get_rectangle(width: int, height: int) -> np.ndarray:
        if not isinstance(width, int) or not isinstance(height, int):
            raise TypeError(""width and height must be integers"")
        if width <= 0 or height <= 0:
            raise ValueError(""width and height must be > 0"")
        return np.ones((height, width), dtype=bool)

    @staticmethod
    def get_square(width: int) -> np.ndarray:
        if not isinstance(width, int):
            raise TypeError(""width must be an integer"")
        if width <= 0:
            raise ValueError(""width must be > 0"")
        return np.ones((width, width), dtype=bool)"
139672,sentinel-hub/eo-learn,sentinel-hub_eo-learn/eolearn/visualization/eoexecutor.py,eolearn.visualization.eoexecutor._ErrorSummary,"from dataclasses import dataclass

@dataclass()
class _ErrorSummary:
    """"""Contains data for errors of a node.""""""
    origin: str
    example_message: str
    failed_indexed_executions: list[tuple[int, str]]

    def add_execution(self, index: int, name: str) -> None:
        """"""Adds an execution to the summary.""""""
        self.failed_indexed_executions.append((index, name))

    @property
    def num_failed(self) -> int:
        """"""Helps with jinja""""""
        return len(self.failed_indexed_executions)","@dataclass()
class _ErrorSummary:
    
    def add_execution(self, index: int, name: str) -> None:
        
        pass
    @property
    def num_failed(self) -> int:
        
        pass",snippet_59,3,"from dataclasses import dataclass, field
from typing import List, Tuple


@dataclass()
class _ErrorSummary:
    executions: List[Tuple[int, str]] = field(default_factory=list)

    def add_execution(self, index: int, name: str) -> None:
        if not isinstance(index, int):
            raise TypeError(""index must be an int"")
        if not isinstance(name, str):
            raise TypeError(""name must be a str"")
        self.executions.append((index, name))

    @property
    def num_failed(self) -> int:
        return len(self.executions)"
139835,python-escpos/python-escpos,python-escpos_python-escpos/src/escpos/codepages.py,escpos.codepages.CodePageManager,"class CodePageManager:
    """"""Holds information about all the code pages.

    Information as defined in escpos-printer-db.
    """"""

    def __init__(self, data):
        """"""Initialize code page manager.""""""
        self.data = data

    @staticmethod
    def get_encoding_name(encoding):
        """"""Get encoding name.

        .. todo:: Resolve the encoding alias.
        """"""
        return encoding.upper()

    def get_encoding(self, encoding):
        """"""Return the encoding data.""""""
        return self.data[encoding]","class CodePageManager:
    
    def __init__(self, data):
        
        pass
    @staticmethod
    def get_encoding_name(encoding):
        
        pass
    def get_encoding_name(encoding):
        
        pass",snippet_60,4,"class CodePageManager:
    
    def __init__(self, data):
        self.data = {}
        if isinstance(data, dict):
            for k, v in data.items():
                if isinstance(v, (list, tuple, set)):
                    for alias in v:
                        if isinstance(alias, str):
                            self.data[alias.strip().lower()] = k
                else:
                    if isinstance(k, str) and isinstance(v, str):
                        self.data[k.strip().lower()] = v.strip().lower()

    @staticmethod
    def get_encoding_name(encoding):
        import codecs

        if encoding is None:
            return None

        if isinstance(encoding, bytes):
            try:
                encoding = encoding.decode('ascii', errors='strict')
            except Exception:
                return None

        if isinstance(encoding, int):
            name = f'cp{encoding}'
        elif isinstance(encoding, str):
            name = encoding.strip().lower()
            name = name.replace('_', '-')
            if name.startswith('windows-') and name[8:].isdigit():
                name = 'cp' + name[8:]
            elif name.startswith('cp-') and name[3:].isdigit():
                name = 'cp' + name[3:]
        else:
            return None

        try:
            return codecs.lookup(name).name
        except Exception:
            # Try some common fallbacks
            if isinstance(name, str):
                # remove dashes and try
                compact = name.replace('-', '')
                try:
                    return codecs.lookup(compact).name
                except Exception:
                    pass
                # try adding cp for digits
                if name.isdigit():
                    try:
                        return codecs.lookup('cp' + name).name
                    except Exception:
                        pass
            return None

    def get_encoding_name(encoding):
        import codecs

        if encoding is None:
            return None

        if isinstance(encoding, bytes):
            try:
                encoding = encoding.decode('ascii', errors='strict')
            except Exception:
                return None

        if isinstance(encoding, int):
            name = f'cp{encoding}'
        elif isinstance(encoding, str):
            name = encoding.strip().lower()
            name = name.replace('_', '-')
            if name.startswith('windows-') and name[8:].isdigit():
                name = 'cp' + name[8:]
            elif name.startswith('cp-') and name[3:].isdigit():
                name = 'cp' + name[3:]
        else:
            return None

        try:
            return codecs.lookup(name).name
        except Exception:
            if isinstance(name, str):
                compact = name.replace('-', '')
                try:
                    return codecs.lookup(compact).name
                except Exception:
                    pass
                if name.isdigit():
                    try:
                        return codecs.lookup('cp' + name).name
                    except Exception:
                        pass
            return None"
141706,mar10/wsgidav,mar10_wsgidav/wsgidav/rw_lock.py,wsgidav.rw_lock.ReadWriteLock,"from threading import Condition, Lock, current_thread
from time import time

class ReadWriteLock:
    """"""Read-Write lock class. A read-write lock differs from a standard
    threading.RLock() by allowing multiple threads to simultaneously hold a
    read lock, while allowing only a single thread to hold a write lock at the
    same point of time.

    When a read lock is requested while a write lock is held, the reader
    is blocked; when a write lock is requested while another write lock is
    held or there are read locks, the writer is blocked.

    Writers are always preferred by this implementation: if there are blocked
    threads waiting for a write lock, current readers may request more read
    locks (which they eventually should free, as they starve the waiting
    writers otherwise), but a new thread requesting a read lock will not
    be granted one, and block. This might mean starvation for readers if
    two writer threads interweave their calls to acquire_write() without
    leaving a window only for readers.

    In case a current reader requests a write lock, this can and will be
    satisfied without giving up the read locks first, but, only one thread
    may perform this kind of lock upgrade, as a deadlock would otherwise
    occur. After the write lock has been granted, the thread will hold a
    full write lock, and not be downgraded after the upgrading call to
    acquire_write() has been match by a corresponding release().
    """"""

    def __init__(self):
        """"""Initialize this read-write lock.""""""
        self.__condition = Condition(Lock())
        self.__writer = None
        self.__upgradewritercount = 0
        self.__pendingwriters = []
        self.__readers = {}

    def acquire_read(self, *, timeout=None):
        """"""Acquire a read lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.

        In case timeout is None, the call to acquire_read blocks until the
        lock request can be serviced.

        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.""""""
        if timeout is not None:
            endtime = time() + timeout
        me = current_thread()
        self.__condition.acquire()
        try:
            if self.__writer is me:
                self.__writercount += 1
                return
            while True:
                if self.__writer is None:
                    if self.__upgradewritercount or self.__pendingwriters:
                        if me in self.__readers:
                            self.__readers[me] += 1
                            return
                    else:
                        self.__readers[me] = self.__readers.get(me, 0) + 1
                        return
                if timeout is not None:
                    remaining = endtime - time()
                    if remaining <= 0:
                        raise RuntimeError('Acquiring read lock timed out')
                    self.__condition.wait(remaining)
                else:
                    self.__condition.wait()
        finally:
            self.__condition.release()

    def acquire_write(self, *, timeout=None):
        """"""Acquire a write lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.

        In case the write lock cannot be serviced due to the deadlock
        condition mentioned above, a ValueError is raised.

        In case timeout is None, the call to acquire_write blocks until the
        lock request can be serviced.

        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.""""""
        if timeout is not None:
            endtime = time() + timeout
        me, upgradewriter = (current_thread(), False)
        self.__condition.acquire()
        try:
            if self.__writer is me:
                self.__writercount += 1
                return
            elif me in self.__readers:
                if self.__upgradewritercount:
                    raise ValueError('Inevitable dead lock, denying write lock')
                upgradewriter = True
                self.__upgradewritercount = self.__readers.pop(me)
            else:
                self.__pendingwriters.append(me)
            while True:
                if not self.__readers and self.__writer is None:
                    if self.__upgradewritercount:
                        if upgradewriter:
                            self.__writer = me
                            self.__writercount = self.__upgradewritercount + 1
                            self.__upgradewritercount = 0
                            return
                    elif self.__pendingwriters[0] is me:
                        self.__writer = me
                        self.__writercount = 1
                        self.__pendingwriters = self.__pendingwriters[1:]
                        return
                if timeout is not None:
                    remaining = endtime - time()
                    if remaining <= 0:
                        if upgradewriter:
                            self.__readers[me] = self.__upgradewritercount
                            self.__upgradewritercount = 0
                        else:
                            self.__pendingwriters.remove(me)
                        raise RuntimeError('Acquiring write lock timed out')
                    self.__condition.wait(remaining)
                else:
                    self.__condition.wait()
        finally:
            self.__condition.release()

    def release(self):
        """"""Release the currently held lock.

        In case the current thread holds no lock, a ValueError is thrown.""""""
        me = current_thread()
        self.__condition.acquire()
        try:
            if self.__writer is me:
                self.__writercount -= 1
                if not self.__writercount:
                    self.__writer = None
                    self.__condition.notify_all()
            elif me in self.__readers:
                self.__readers[me] -= 1
                if not self.__readers[me]:
                    del self.__readers[me]
                    if not self.__readers:
                        self.__condition.notify_all()
            else:
                raise ValueError('Trying to release unheld lock')
        finally:
            self.__condition.release()","
class ReadWriteLock:
    
    def __init__(self):
        
        pass
    def acquire_read(self, *, timeout=None):
        '''Acquire a read lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.
        In case timeout is None, the call to acquire_read blocks until the
        lock request can be serviced.
        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.'''
        pass

    def acquire_write(self, *, timeout=None):
        '''Acquire a write lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.
        In case the write lock cannot be serviced due to the deadlock
        condition mentioned above, a ValueError is raised.
        In case timeout is None, the call to acquire_write blocks until the
        lock request can be serviced.
        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.'''
        pass

    def release(self):
        
        pass",snippet_62,5,"import threading
import time
from collections import defaultdict

class ReadWriteLock:
    
    def __init__(self):
        self._cond = threading.Condition()
        self._readers = defaultdict(int)  # thread_id -> count
        self._writer = None               # thread_id or None
        self._writer_count = 0
        self._waiting_writers = 0

    def _tid(self):
        return threading.get_ident()

    def _can_acquire_read(self, tid):
        # Allow read if no active writer and either no waiting writers
        # or the thread already holds a read lock (reentrant read).
        if self._writer is not None:
            return False
        if self._waiting_writers > 0 and self._readers.get(tid, 0) == 0:
            return False
        return True

    def _can_acquire_write(self, tid):
        # Allow write if no other writer and no readers (or reentrant write)
        if self._writer is None:
            return len(self._readers) == 0
        return self._writer == tid  # reentrant write

    def acquire_read(self, *, timeout=None):
        tid = self._tid()
        # Non-blocking check
        if timeout is not None and timeout <= 0:
            with self._cond:
                if self._can_acquire_read(tid):
                    self._readers[tid] += 1
                    return
                raise RuntimeError(""Timed out acquiring read lock"")
        # Blocking with optional timeout
        deadline = None if timeout is None else (time.monotonic() + timeout)
        with self._cond:
            while not self._can_acquire_read(tid):
                if deadline is None:
                    self._cond.wait()
                else:
                    remaining = deadline - time.monotonic()
                    if remaining <= 0:
                        raise RuntimeError(""Timed out acquiring read lock"")
                    self._cond.wait(remaining)
            self._readers[tid] += 1

    def acquire_write(self, *, timeout=None):
        tid = self._tid()
        with self._cond:
            # Deadlock condition: cannot upgrade from read to write
            if self._readers.get(tid, 0) > 0 and self._writer != tid:
                raise ValueError(""Deadlock risk: cannot acquire write while holding read lock"")
        # Non-blocking check
        if timeout is not None and timeout <= 0:
            with self._cond:
                if self._can_acquire_write(tid):
                    if self._writer == tid:
                        self._writer_count += 1
                    else:
                        self._writer = tid
                        self._writer_count = 1
                    return
                raise RuntimeError(""Timed out acquiring write lock"")
        deadline = None if timeout is None else (time.monotonic() + timeout)
        waiting_marked = False
        with self._cond:
            if self._writer != tid:
                self._waiting_writers += 1
                waiting_marked = True
            try:
                while not self._can_acquire_write(tid):
                    if deadline is None:
                        self._cond.wait()
                    else:
                        remaining = deadline - time.monotonic()
                        if remaining <= 0:
                            raise RuntimeError(""Timed out acquiring write lock"")
                        self._cond.wait(remaining)
                if self._writer == tid:
                    self._writer_count += 1
                else:
                    self._writer = tid
                    self._writer_count = 1
            finally:
                if waiting_marked:
                    self._waiting_writers -= 1

    def release(self):
        tid = self._tid()
        with self._cond:
            if self._writer == tid and self._writer_count > 0:
                self._writer_count -= 1
                if self._writer_count == 0:
                    self._writer = None
                    self._cond.notify_all()
                return
            rc = self._readers.get(tid, 0)
            if rc > 0:
                if rc == 1:
                    del self._readers[tid]
                else:
                    self._readers[tid] = rc - 1
                if len(self._readers) == 0:
                    self._cond.notify_all()
                return
            raise RuntimeError(""Current thread does not hold the lock"")"
141733,mar10/wsgidav,mar10_wsgidav/wsgidav/stream_tools.py,wsgidav.stream_tools.StreamingFile,"class StreamingFile:
    """"""A file object wrapped around an iterator / data stream.""""""

    def __init__(self, data_stream):
        """"""Initialise the object with the data stream.""""""
        self.data_stream = data_stream
        self.buffer = ''

    def read(self, size=None):
        """"""Read bytes from an iterator.""""""
        while size is None or len(self.buffer) < size:
            try:
                self.buffer += next(self.data_stream)
            except StopIteration:
                break
        sized_chunk = self.buffer[:size]
        if size is None:
            self.buffer = ''
        else:
            self.buffer = self.buffer[size:]
        return sized_chunk","class StreamingFile:
    
    def __init__(self, data_stream):
        
        pass
    def read(self, size=None):
        
        pass",snippet_63,3,"class StreamingFile:
    
    def __init__(self, data_stream):
        self._it = iter(data_stream)
        self._chunks = []
        self._total_len = 0
        self._type = None  # 'bytes' or 'str'
        self._eof = False
        self._empty = b''

    def _set_type_from_chunk(self, chunk):
        if isinstance(chunk, (bytes, bytearray)):
            self._type = 'bytes'
            self._empty = b''
        elif isinstance(chunk, str):
            self._type = 'str'
            self._empty = ''
        else:
            raise TypeError(""Stream must yield bytes or str"")
        # normalize bytearray to bytes
        if isinstance(chunk, bytearray):
            return bytes(chunk)
        return chunk

    def _add_chunk(self, chunk):
        if chunk is None:
            return
        if self._type is None:
            chunk = self._set_type_from_chunk(chunk)
        else:
            if self._type == 'bytes':
                if isinstance(chunk, bytearray):
                    chunk = bytes(chunk)
                if not isinstance(chunk, (bytes,)):
                    raise TypeError(""Mixed stream types: expected bytes"")
            else:
                if not isinstance(chunk, str):
                    raise TypeError(""Mixed stream types: expected str"")
        if chunk:
            self._chunks.append(chunk)
            self._total_len += len(chunk)

    def _fetch_next(self):
        if self._eof:
            return False
        try:
            chunk = next(self._it)
        except StopIteration:
            self._eof = True
            return False
        self._add_chunk(chunk)
        return True

    def _join(self, parts):
        if not parts:
            return self._empty
        if self._type == 'bytes':
            return b''.join(parts)
        else:
            return ''.join(parts)

    def read(self, size=None):
        if size is not None and not isinstance(size, int):
            raise TypeError(""size must be an int or None"")
        if size is not None and size < 0:
            size = None

        if size is None:
            # read all remaining
            while self._fetch_next():
                pass
            if not self._chunks:
                return self._empty
            result = self._join(self._chunks)
            self._chunks.clear()
            self._total_len = 0
            return result

        # size == 0
        if size == 0:
            return self._empty

        # ensure we have at least size bytes/chars or EOF
        while self._total_len < size and self._fetch_next():
            pass

        if not self._chunks:
            return self._empty

        remaining = size
        out_parts = []
        while remaining > 0 and self._chunks:
            first = self._chunks[0]
            flen = len(first)
            if flen <= remaining:
                out_parts.append(first)
                self._chunks.pop(0)
                remaining -= flen
                self._total_len -= flen
            else:
                # split the chunk
                out_parts.append(first[:remaining])
                self._chunks[0] = first[remaining:]
                self._total_len -= remaining
                remaining = 0

        return self._join(out_parts)"
142723,PyThaiNLP/pythainlp,PyThaiNLP_pythainlp/pythainlp/augment/lm/fasttext.py,pythainlp.augment.lm.fasttext.FastTextAug,"from gensim.models.fasttext import FastText as FastText_gensim
from typing import List, Tuple
import itertools
from gensim.models.keyedvectors import KeyedVectors
from pythainlp.tokenize import word_tokenize

class FastTextAug:
    """"""
    Text Augment from fastText

    :param str model_path: path of model file
    """"""

    def __init__(self, model_path: str):
        """"""
        :param str model_path: path of model file
        """"""
        if model_path.endswith('.bin'):
            self.model = FastText_gensim.load_facebook_vectors(model_path)
        elif model_path.endswith('.vec'):
            self.model = KeyedVectors.load_word2vec_format(model_path)
        else:
            self.model = FastText_gensim.load(model_path)
        self.dict_wv = list(self.model.key_to_index.keys())

    def tokenize(self, text: str) -> List[str]:
        """"""
        Thai text tokenization for fastText

        :param str text: Thai text

        :return: list of words
        :rtype: List[str]
        """"""
        return word_tokenize(text, engine='icu')

    def modify_sent(self, sent: str, p: float=0.7) -> List[List[str]]:
        """"""
        :param str sent: text of sentence
        :param float p: probability
        :rtype: List[List[str]]
        """"""
        list_sent_new = []
        for i in sent:
            if i in self.dict_wv:
                w = [j for j, v in self.model.most_similar(i) if v >= p]
                if w == []:
                    list_sent_new.append([i])
                else:
                    list_sent_new.append(w)
            else:
                list_sent_new.append([i])
        return list_sent_new

    def augment(self, sentence: str, n_sent: int=1, p: float=0.7) -> List[Tuple[str]]:
        """"""
        Text Augment from fastText

        You may want to download the Thai model
        from https://fasttext.cc/docs/en/crawl-vectors.html.

        :param str sentence: Thai sentence
        :param int n_sent: number of sentences
        :param float p: probability of word

        :return: list of synonyms
        :rtype: List[Tuple[str]]
        """"""
        self.sentence = self.tokenize(sentence)
        self.list_synonym = self.modify_sent(self.sentence, p=p)
        new_sentences = []
        for x in list(itertools.product(*self.list_synonym))[0:n_sent]:
            new_sentences.append(x)
        return new_sentences","
class FastTextAug:
    
    def __init__(self, model_path: str):
        
        pass
    def tokenize(self, text: str) -> List[str]:
        
        pass

    def modify_sent(self, sent: str, p: float=0.7) -> List[List[str]]:
        
        pass

    def augment(self, sentence: str, n_sent: int=1, p: float=0.7) -> List[Tuple[str]]:
        
        pass",snippet_64,5,"from typing import List, Tuple, Optional
import os
import re
import random

class FastTextAug:
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self._fasttext_model = None
        self._gensim_model = None
        self._use_fasttext = False
        self._use_gensim = False

        # Try loading via fasttext first (for .bin models)
        try:
            import fasttext  # type: ignore
            if os.path.isfile(self.model_path):
                self._fasttext_model = fasttext.load_model(self.model_path)
                self._use_fasttext = True
        except Exception:
            self._fasttext_model = None
            self._use_fasttext = False

        # If fasttext failed, try gensim (for .vec or word2vec format)
        if not self._use_fasttext:
            try:
                from gensim.models import KeyedVectors  # type: ignore
                # Attempt binary=False first (common for .vec)
                try:
                    self._gensim_model = KeyedVectors.load_word2vec_format(self.model_path, binary=False)
                    self._use_gensim = True
                except Exception:
                    # Try binary=True (word2vec bin)
                    self._gensim_model = KeyedVectors.load_word2vec_format(self.model_path, binary=True)
                    self._use_gensim = True
            except Exception:
                self._gensim_model = None
                self._use_gensim = False

        self._word_re = re.compile(r""\w+|[^\w\s]"", re.UNICODE)

    def tokenize(self, text: str) -> List[str]:
        if not text:
            return []
        return self._word_re.findall(text)

    def _nearest(self, word: str, topn: int = 20) -> List[str]:
        candidates: List[str] = []
        if self._use_fasttext and self._fasttext_model is not None:
            try:
                # fasttext returns list of (sim, word)
                nn = self._fasttext_model.get_nearest_neighbors(word, k=topn * 2)
                candidates = [w for _, w in nn if w.lower() != word.lower()]
            except Exception:
                candidates = []
        elif self._use_gensim and self._gensim_model is not None:
            try:
                nn = self._gensim_model.most_similar(word, topn=topn * 2)
                candidates = [w for w, _ in nn if w.lower() != word.lower()]
            except Exception:
                candidates = []
        return [w for w in candidates if w.isalpha()]

    def _maybe_replace(self, token: str, p: float) -> str:
        # Replace only alphabetic tokens
        if not token.isalpha():
            return token
        if random.random() > p:
            return token
        neighbors = self._nearest(token, topn=20)
        if not neighbors:
            return token
        # Prefer a neighbor different from token, keep casing similar
        choice = random.choice(neighbors)
        if token.isupper():
            return choice.upper()
        if token[0].isupper():
            return choice.capitalize()
        return choice

    def modify_sent(self, sent: str, p: float = 0.7) -> List[List[str]]:
        tokens = self.tokenize(sent)
        if not (self._use_fasttext or self._use_gensim):
            return [tokens[:]]
        modified = [self._maybe_replace(tok, p) for tok in tokens]
        return [modified]

    def augment(self, sentence: str, n_sent: int = 1, p: float = 0.7) -> List[Tuple[str]]:
        results: List[Tuple[str]] = []
        for _ in range(max(1, n_sent)):
            mod_tokens_list = self.modify_sent(sentence, p=p)
            if not mod_tokens_list:
                results.append((sentence,))
                continue
            mod_tokens = mod_tokens_list[0]
            # Simple detokenization: join with spaces and then fix space before punctuation
            text = "" "".join(mod_tokens)
            text = re.sub(r""\s+([,.!?;:])"", r""\1"", text)
            results.append((text,))
        return results"
142777,PyThaiNLP/pythainlp,PyThaiNLP_pythainlp/pythainlp/tokenize/core.py,pythainlp.tokenize.core.Tokenizer,"from typing import Iterable, List, Union
from pythainlp.tokenize import DEFAULT_SENT_TOKENIZE_ENGINE, DEFAULT_SUBWORD_TOKENIZE_ENGINE, DEFAULT_SYLLABLE_DICT_TRIE, DEFAULT_SYLLABLE_TOKENIZE_ENGINE, DEFAULT_WORD_DICT_TRIE, DEFAULT_WORD_TOKENIZE_ENGINE
from pythainlp.util.trie import Trie, dict_trie

class Tokenizer:
    """"""
    Tokenizer class for a custom tokenizer.

    This class allows users to pre-define custom dictionary along with
    tokenizer and encapsulate them into one single object.
    It is an wrapper for both functions, that are
    :func:`pythainlp.tokenize.word_tokenize`,
    and :func:`pythainlp.util.dict_trie`

    :Example:

    Tokenizer object instantiated with :class:`pythainlp.util.Trie`::

        from pythainlp.tokenize import Tokenizer
        from pythainlp.corpus.common import thai_words
        from pythainlp.util import dict_trie

        custom_words_list = set(thai_words())
        custom_words_list.add('อะเฟเซีย')
        custom_words_list.add('Aphasia')
        trie = dict_trie(dict_source=custom_words_list)

        text = ""อะเฟเซีย (Aphasia*) เป็นอาการผิดปกติของการพูด""
        _tokenizer = Tokenizer(custom_dict=trie, engine='newmm')
        _tokenizer.word_tokenize(text)
        # output: ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ',
        'ผิดปกติ', 'ของ', 'การ', 'พูด']

    Tokenizer object instantiated with a list of words::

        text = ""อะเฟเซีย (Aphasia) เป็นอาการผิดปกติของการพูด""
        _tokenizer = Tokenizer(custom_dict=list(thai_words()), engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะ', 'เฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ',
        #   'ผิดปกติ', 'ของ', 'การ', 'พูด']

    Tokenizer object instantiated with a file path containing a list of
    words separated with *newline* and explicitly setting a new tokenizer
    after initiation::

        PATH_TO_CUSTOM_DICTIONARY = './custom_dictionary.txtt'

        # write a file
        with open(PATH_TO_CUSTOM_DICTIONARY, 'w', encoding='utf-8') as f:
            f.write('อะเฟเซีย\\nAphasia\\nผิด\\nปกติ')

        text = ""อะเฟเซีย (Aphasia) เป็นอาการผิดปกติของการพูด""

        # initiate an object from file with `attacut` as tokenizer
        _tokenizer = Tokenizer(custom_dict=PATH_TO_CUSTOM_DICTIONARY, \\
            engine='attacut')

        _tokenizer.word_tokenize(text)
        # output:
        # ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ', 'ผิด',
        #   'ปกติ', 'ของ', 'การ', 'พูด']

        # change tokenizer to `newmm`
        _tokenizer.set_tokenizer_engine(engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็นอาการ', 'ผิด',
        #   'ปกติ', 'ของการพูด']
    """"""

    def __init__(self, custom_dict: Union[Trie, Iterable[str], str]=[], engine: str='newmm', keep_whitespace: bool=True, join_broken_num: bool=True):
        """"""
        Initialize tokenizer object.

        :param str custom_dict: a file path, a list of vocaburaies* to be
                    used to create a trie, or an instantiated
                    :class:`pythainlp.util.Trie` object.
        :param str engine: choose between different options of tokenizer engines
                            (i.e.  *newmm*, *mm*, *longest*, *deepcut*)
        :param bool keep_whitespace: True to keep whitespace, a common mark
                                     for end of phrase in Thai
        """"""
        self.__trie_dict = Trie([])
        if custom_dict:
            self.__trie_dict = dict_trie(custom_dict)
        else:
            self.__trie_dict = DEFAULT_WORD_DICT_TRIE
        self.__engine = engine
        if self.__engine not in ['newmm', 'mm', 'longest', 'deepcut']:
            raise NotImplementedError('\n                The Tokenizer class is not support %s for custom tokenizer\n                ' % self.__engine)
        self.__keep_whitespace = keep_whitespace
        self.__join_broken_num = join_broken_num

    def word_tokenize(self, text: str) -> List[str]:
        """"""
        Main tokenization function.

        :param str text: text to be tokenized
        :return: list of words, tokenized from the text
        :rtype: list[str]
        """"""
        return word_tokenize(text, custom_dict=self.__trie_dict, engine=self.__engine, keep_whitespace=self.__keep_whitespace, join_broken_num=self.__join_broken_num)

    def set_tokenize_engine(self, engine: str) -> None:
        """"""
        Set the tokenizer's engine.

        :param str engine: choose between different options of tokenizer engines
                           (i.e. *newmm*, *mm*, *longest*, *deepcut*)
        """"""
        self.__engine = engine","
class Tokenizer:
    '''
    Tokenizer class for a custom tokenizer.
    This class allows users to pre-define custom dictionary along with
    tokenizer and encapsulate them into one single object.
    It is an wrapper for both functions, that are
    :func:`pythainlp.tokenize.word_tokenize`,
    and :func:`pythainlp.util.dict_trie`
    :Example:
    Tokenizer object instantiated with :class:`pythainlp.util.Trie`::
        from pythainlp.tokenize import Tokenizer
        from pythainlp.corpus.common import thai_words
        from pythainlp.util import dict_trie
        custom_words_list = set(thai_words())
        custom_words_list.add('อะเฟเซีย')
        custom_words_list.add('Aphasia')
        trie = dict_trie(dict_source=custom_words_list)
        text = ""อะเฟเซีย (Aphasia*) เป็นอาการผิดปกติของการพูด""
        _tokenizer = Tokenizer(custom_dict=trie, engine='newmm')
        _tokenizer.word_tokenize(text)
        # output: ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ',
        'ผิดปกติ', 'ของ', 'การ', 'พูด']
    Tokenizer object instantiated with a list of words::
        text = ""อะเฟเซีย (Aphasia) เป็นอาการผิดปกติของการพูด""
        _tokenizer = Tokenizer(custom_dict=list(thai_words()), engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะ', 'เฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ',
        #   'ผิดปกติ', 'ของ', 'การ', 'พูด']
    Tokenizer object instantiated with a file path containing a list of
    words separated with *newline* and explicitly setting a new tokenizer
    after initiation::
        PATH_TO_CUSTOM_DICTIONARY = './custom_dictionary.txtt'
        # write a file
        with open(PATH_TO_CUSTOM_DICTIONARY, 'w', encoding='utf-8') as f:
            f.write('อะเฟเซีย\nAphasia\nผิด\nปกติ')
        text = ""อะเฟเซีย (Aphasia) เป็นอาการผิดปกติของการพูด""
        # initiate an object from file with `attacut` as tokenizer
        _tokenizer = Tokenizer(custom_dict=PATH_TO_CUSTOM_DICTIONARY, \
            engine='attacut')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ', 'ผิด',
        #   'ปกติ', 'ของ', 'การ', 'พูด']
        # change tokenizer to `newmm`
        _tokenizer.set_tokenizer_engine(engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็นอาการ', 'ผิด',
        #   'ปกติ', 'ของการพูด']
    '''
    def __init__(self, custom_dict: Union[Trie, Iterable[str], str]=[], engine: str='newmm', keep_whitespace: bool=True, join_broken_num: bool=True):
        
        pass

    def word_tokenize(self, text: str) -> List[str]:
        
        pass

    def set_tokenize_engine(self, engine: str) -> None:
        
        pass",snippet_65,4,"from typing import Iterable, List, Optional, Union

try:
    from pythainlp.tokenize import word_tokenize as thai_word_tokenize
    from pythainlp.util import dict_trie, Trie  # type: ignore
except Exception:
    thai_word_tokenize = None
    Trie = object  # fallback for type checking

    def dict_trie(dict_source: Iterable[str]):  # type: ignore
        class _SimpleTrie:
            def __init__(self, words: Iterable[str]):
                self.words = set(words)

            def __contains__(self, item: str) -> bool:
                return item in self.words

        return _SimpleTrie(dict_source)


class Tokenizer:
    '''
    Tokenizer class for a custom tokenizer.
    This class allows users to pre-define custom dictionary along with
    tokenizer and encapsulate them into one single object.
    It is an wrapper for both functions, that are
    :func:`pythainlp.tokenize.word_tokenize`,
    and :func:`pythainlp.util.dict_trie`
    :Example:
    Tokenizer object instantiated with :class:`pythainlp.util.Trie`::
        from pythainlp.tokenize import Tokenizer
        from pythainlp.corpus.common import thai_words
        from pythainlp.util import dict_trie
        custom_words_list = set(thai_words())
        custom_words_list.add('อะเฟเซีย')
        custom_words_list.add('Aphasia')
        trie = dict_trie(dict_source=custom_words_list)
        text = ""อะเฟเซีย (Aphasia*) เป็นอาการผิดปกติของการพูด""
        _tokenizer = Tokenizer(custom_dict=trie, engine='newmm')
        _tokenizer.word_tokenize(text)
        # output: ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ',
        'ผิดปกติ', 'ของ', 'การ', 'พูด']
    Tokenizer object instantiated with a list of words::
        text = ""อะเฟเซีย (Aphasia) เป็นอาการผิดปกติของการพูด""
        _tokenizer = Tokenizer(custom_dict=list(thai_words()), engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะ', 'เฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ',
        #   'ผิดปกติ', 'ของ', 'การ', 'พูด']
    Tokenizer object instantiated with a file path containing a list of
    words separated with *newline* and explicitly setting a new tokenizer
    after initiation::
        PATH_TO_CUSTOM_DICTIONARY = './custom_dictionary.txtt'
        # write a file
        with open(PATH_TO_CUSTOM_DICTIONARY, 'w', encoding='utf-8') as f:
            f.write('อะเฟเซีย\nAphasia\nผิด\nปกติ')
        text = ""อะเฟเซีย (Aphasia) เป็นอาการผิดปกติของการพูด""
        # initiate an object from file with `attacut` as tokenizer
        _tokenizer = Tokenizer(custom_dict=PATH_TO_CUSTOM_DICTIONARY, \
            engine='attacut')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ', 'ผิด',
        #   'ปกติ', 'ของ', 'การ', 'พูด']
        # change tokenizer to `newmm`
        _tokenizer.set_tokenizer_engine(engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็นอาการ', 'ผิด',
        #   'ปกติ', 'ของการพูด']
    '''
    def __init__(
        self,
        custom_dict: Union[""Trie"", Iterable[str], str] = [],
        engine: str = 'newmm',
        keep_whitespace: bool = True,
        join_broken_num: bool = True
    ):
        self.engine: str = engine
        self.keep_whitespace: bool = keep_whitespace
        self.join_broken_num: bool = join_broken_num
        self._raw_dict_source: Optional[Union[""Trie"", Iterable[str], str]] = custom_dict
        self.custom_dict = self._prepare_custom_dict(custom_dict)

    def _prepare_custom_dict(self, custom_dict: Union[""Trie"", Iterable[str], str, None]):
        if custom_dict is None or custom_dict == []:
            return None
        # If already a Trie-like object
        try:
            if Trie is not object and isinstance(custom_dict, Trie):
                return custom_dict
        except Exception:
            pass

        # If path to file
        if isinstance(custom_dict, str):
            words: List[str] = []
            with open(custom_dict, 'r', encoding='utf-8') as f:
                for line in f:
                    w = line.strip()
                    if w:
                        words.append(w)
            return dict_trie(dict_source=words)

        # Iterable of strings
        try:
            # Try to iterate and build
            words_iter = list(custom_dict)  # type: ignore[arg-type]
            return dict_trie(dict_source=words_iter)
        except Exception:
            return None

    def word_tokenize(self, text: str) -> List[str]:
        if thai_word_tokenize is not None:
            return thai_word_tokenize(
                text,
                engine=self.engine,
                keep_whitespace=self.keep_whitespace,
                custom_dict=self.custom_dict,
                join_broken_num=self.join_broken_num,
            )
        # Fallback simple tokenizer if pythainlp is unavailable
        if not text:
            return []
        # If we have a simple dict trie fallback, extract set for matching
        dict_words = None
        if self.custom_dict is not None and hasattr(self.custom_dict, ""words""):
            dict_words = getattr(self.custom_dict, ""words"", None)

        tokens: List[str] = []
        i = 0
        n = len(text)

        def is_space(ch: str) -> bool:
            return ch.isspace()

        while i < n:
            ch = text[i]
            if is_space(ch):
                if self.keep_whitespace:
                    j = i + 1
                    while j < n and is_space(text[j]):
                        j += 1
                    tokens.append(text[i:j])
                    i = j
                else:
                    i += 1
                continue

            if dict_words:
                # Longest match
                j = n
                matched = None
                while j > i:
                    seg = text[i:j]
                    if seg in dict_words:
                        matched = seg
                        break
                    j -= 1
                if matched:
                    tokens.append(matched)
                    i += len(matched)
                    continue

            # Default: single character token
            tokens.append(ch)
            i += 1

        return tokens

    def set_tokenize_engine(self, engine: str) -> None:
        self.engine = engine

    # Backward-compatible alias (as shown in the docstring example)
    def set_tokenizer_engine(self, engine: str) -> None:
        self.set_tokenize_engine(engine)"
145446,TomasTomecek/sen,TomasTomecek_sen/sen/tui/commands/base.py,sen.tui.commands.base.ArgumentProcessor,"class ArgumentProcessor:
    """"""
    responsible for parsing given list of arguments
    """"""

    def __init__(self, options, arguments):
        """"""
        :param options: list of options
        :param arguments: list of arguments
        """"""
        self.given_arguments = {}
        self.options = {}
        for a in options:
            self.options[a.name] = a
            self.given_arguments[normalize_arg_name(a.name)] = a.default
            for alias in a.aliases:
                self.options[alias] = a
        for o in arguments:
            self.given_arguments[normalize_arg_name(o.name)] = o.default
        self.arguments = arguments
        logger.info('arguments = %s', arguments)
        logger.info('options = %s', options)

    def process(self, argument_list):
        """"""
        :param argument_list: list of str, input from user
        :return: dict:
            {""cleaned_arg_name"": ""value""}
        """"""
        arg_index = 0
        for a in argument_list:
            opt_and_val = a.split('=', 1)
            opt_name = opt_and_val[0]
            try:
                argument = self.options[opt_name]
            except KeyError:
                try:
                    argument = self.arguments[arg_index]
                except IndexError:
                    logger.error('option/argument %r not specified', a)
                    raise NoSuchOptionOrArgument('No such option or argument: %r' % opt_name)
            logger.info('argument found: %s', argument)
            safe_arg_name = normalize_arg_name(argument.name)
            logger.info('argument is available under name %r', safe_arg_name)
            if isinstance(argument, Argument):
                arg_index += 1
                value = (a,)
            else:
                try:
                    value = (opt_and_val[1],)
                except IndexError:
                    value = tuple()
            arg_val = argument.action(*value)
            logger.info('argument %r has value %r', safe_arg_name, arg_val)
            self.given_arguments[safe_arg_name] = arg_val
        return self.given_arguments","class ArgumentProcessor:
    
    def __init__(self, options, arguments):
        
        pass
    def process(self, argument_list):
        
        pass",snippet_66,3,"class ArgumentProcessor:
    
    def __init__(self, options, arguments):
        self.options_raw = options or []
        self.arguments_raw = arguments or []
        self.opt_by_flag = {}
        self.opt_specs = []
        self.positional_specs = []
        self._normalize()
    
    def _normalize(self):
        def as_list(x):
            if x is None:
                return []
            if isinstance(x, (list, tuple)):
                return list(x)
            return [x]
        
        # Normalize options
        opts = self.options_raw
        if isinstance(opts, dict):
            # Support dict forms: key->spec or flag->spec
            normalized = []
            for k, v in opts.items():
                spec = dict(v) if isinstance(v, dict) else {}
                # If key looks like a flag, merge into flags
                if isinstance(k, str) and k.startswith(""-""):
                    flags = as_list(spec.get(""flags"")) or []
                    if k not in flags:
                        flags.insert(0, k)
                    spec[""flags""] = flags
                else:
                    # Treat as dest name
                    spec[""dest""] = spec.get(""dest"", k)
                normalized.append(spec)
            opts = normalized
        elif not isinstance(opts, list):
            opts = as_list(opts)
        
        for raw in opts:
            spec = dict(raw) if isinstance(raw, dict) else {""flags"": as_list(raw)}
            flags = as_list(spec.get(""flags"")) or as_list(spec.get(""flag"")) or as_list(spec.get(""name"")) or as_list(spec.get(""names""))
            flags = [f for f in flags if isinstance(f, str)]
            if not flags and ""dest"" not in spec:
                continue
            # Derive dest
            dest = spec.get(""dest"")
            if not dest:
                # choose longest flag (likely the --long) to derive dest
                longest = max(flags, key=len)
                dest = longest.lstrip(""-"").replace(""-"", ""_"")
            action = spec.get(""action"")
            if not action:
                # infer action
                if spec.get(""has_value"") is True or (""type"" in spec) or (""choices"" in spec) or spec.get(""action"") == ""store"":
                    action = ""store""
                elif spec.get(""action"") in (""store_true"", ""store_false"", ""append"", ""count""):
                    action = spec.get(""action"")
                else:
                    # by default booleans if no type specified
                    action = ""store_true""
            has_value = spec.get(""has_value"")
            if has_value is None:
                has_value = action in (""store"", ""append"")
            typ = spec.get(""type"")
            if typ is None:
                if action in (""store"", ""append""):
                    typ = str
                elif action == ""count"":
                    typ = int
                elif action in (""store_true"", ""store_false""):
                    typ = bool
            choices = spec.get(""choices"")
            # Default
            if ""default"" in spec:
                default = spec[""default""]
            else:
                if action == ""append"":
                    default = []
                elif action == ""count"":
                    default = 0
                elif action == ""store_true"":
                    default = False
                elif action == ""store_false"":
                    default = True
                else:
                    default = None
            ospec = {
                ""flags"": flags,
                ""dest"": dest,
                ""action"": action,
                ""has_value"": bool(has_value),
                ""type"": typ,
                ""choices"": choices,
                ""required"": bool(spec.get(""required"", False)),
                ""metavar"": spec.get(""metavar""),
                ""help"": spec.get(""help""),
                ""default"": default,
            }
            self.opt_specs.append(ospec)
            for fl in flags:
                self.opt_by_flag[fl] = ospec
        
        # Normalize positionals
        args = self.arguments_raw
        if isinstance(args, dict):
            # map of name->spec
            normalized = []
            for name, spec in args.items():
                s = dict(spec) if isinstance(spec, dict) else {}
                s[""name""] = name
                normalized.append(s)
            args = normalized
        elif not isinstance(args, list):
            args = as_list(args)
        for raw in args:
            if isinstance(raw, str):
                spec = {""name"": raw}
            else:
                spec = dict(raw) if isinstance(raw, dict) else {}
            name = spec.get(""name"")
            if not name:
                continue
            multiple = bool(spec.get(""multiple"", spec.get(""nargs"") in (""*"", ""+"", None) and False))
            # Support nargs semantics
            nargs = spec.get(""nargs"")
            if nargs in (""*"", ""+""):
                multiple = True
            required = bool(spec.get(""required"", False))
            if nargs == ""+"":
                required = True
            ptype = spec.get(""type"", str)
            default = spec.get(""default"")
            self.positional_specs.append({
                ""name"": name,
                ""multiple"": multiple,
                ""required"": required,
                ""type"": ptype,
                ""choices"": spec.get(""choices""),
                ""default"": default,
            })
    
    def _convert(self, value, typ, flag_for_error=None):
        if typ is None:
            return value
        if typ is bool:
            v = str(value).strip().lower()
            if v in (""1"", ""true"", ""t"", ""yes"", ""y"", ""on""):
                return True
            if v in (""0"", ""false"", ""f"", ""no"", ""n"", ""off""):
                return False
            raise ValueError(f""Invalid boolean value for {flag_for_error or ''}: {value}"")
        try:
            return typ(value)
        except Exception as e:
            raise ValueError(f""Invalid value {value!r} for {flag_for_error or 'argument'}: {e}"") from None
    
    def _apply_option(self, result, spec, value, flag_for_error=None):
        action = spec[""action""]
        dest = spec[""dest""]
        if action == ""store_true"":
            result[dest] = True
            return
        if action == ""store_false"":
            result[dest] = False
            return
        if action == ""count"":
            result[dest] = int(result.get(dest, 0)) + 1
            return
        if action == ""append"":
            if value is None:
                raise ValueError(f""Option {flag_for_error or dest} requires a value"")
            v = self._convert(value, spec[""type""], flag_for_error)
            if spec.get(""choices"") and v not in spec[""choices""]:
                raise ValueError(f""Option {flag_for_error or dest} must be one of {spec['choices']}"")
            lst = result.get(dest)
            if lst is None:
                lst = []
            result.setdefault(dest, lst).append(v)
            return
        # store
        if value is None:
            raise ValueError(f""Option {flag_for_error or dest} requires a value"")
        v = self._convert(value, spec[""type""], flag_for_error)
        if spec.get(""choices"") and v not in spec[""choices""]:
            raise ValueError(f""Option {flag_for_error or dest} must be one of {spec['choices']}"")
        result[dest] = v
    
    def process(self, argument_list):
        argv = list(argument_list or [])
        # Initialize result with defaults
        result = {}
        for spec in self.opt_specs:
            d = spec[""default""]
            # copy lists to avoid mutation bleed
            if isinstance(d, list):
                result[spec[""dest""]] = list(d)
            else:
                result[spec[""dest""]] = d
        for ps in self.positional_specs:
            if ps[""multiple""]:
                result[ps[""name""]] = list(ps[""default""] or [])
            elif ""default"" in ps and ps[""default""] is not None:
                result[ps[""name""]] = ps[""default""]
        
        i = 0
        positional_tokens = []
        end_of_opts = False
        while i < len(argv):
            tok = argv[i]
            if end_of_opts:
                positional_tokens.append(tok)
                i += 1
                continue
            if tok == ""--"":
                end_of_opts = True
                i += 1
                continue
            if tok.startswith(""--""):
                if tok == ""--"":
                    i += 1
                    end_of_opts = True
                    continue
                if ""="" in tok:
                    flag, val = tok.split(""="", 1)
                else:
                    flag, val = tok, None
                spec = self.opt_by_flag.get(flag)
                if not spec:
                    raise ValueError(f""Unknown option: {flag}"")
                if spec[""has_value""]:
                    if val is None:
                        i += 1
                        if i >= len(argv):
                            raise ValueError(f""Option {flag} requires a value"")
                        val = argv[i]
                    self._apply_option(result, spec, val, flag)
                else:
                    self._apply_option(result, spec, None, flag)
                i += 1
                continue
            if tok.startswith(""-"") and tok != ""-"":
                # short flags, may be cluster
                if len(tok) > 2 and not tok.startswith(""--""):
                    # cluster or attached value
                    j = 1
                    consumed = False
                    while j < len(tok):
                        short_flag = ""-"" + tok[j]
                        spec = self.opt_by_flag.get(short_flag)
                        if not spec:
                            raise ValueError(f""Unknown option: {short_flag}"")
                        if spec[""has_value""]:
                            # attached value remainder
                            remainder = tok[j + 1 :]
                            if remainder:
                                val = remainder
                                self._apply_option(result, spec, val, short_flag)
                                consumed = True
                                break
                            else:
                                # take next argv
                                i += 1
                                if i >= len(argv):
                                    raise ValueError(f""Option {short_flag} requires a value"")
                                val = argv[i]
                                self._apply_option(result, spec, val, short_flag)
                                consumed = True
                                break
                        else:
                            self._apply_option(result, spec, None, short_flag)
                            j += 1
                    i += 1
                    if consumed:
                        # already consumed current or next as value
                        continue
                    else:
                        continue
                else:
                    # single short, may be -oVALUE or -o=VALUE
                    flag = tok[:2]
                    spec = self.opt_by_flag.get(flag)
                    if not spec:
                        raise ValueError(f""Unknown option: {flag}"")
                    val = None
                    attached = tok[2:]
                    if spec[""has_value""]:
                        if attached.startswith(""=""):
                            val = attached[1:]
                        elif attached:
                            val = attached
                        else:
                            i += 1
                            if i >= len(argv):
                                raise ValueError(f""Option {flag} requires a value"")
                            val = argv[i]
                        self._apply_option(result, spec, val, flag)
                    else:
                        self._apply_option(result, spec, None, flag)
                    i += 1
                    continue
            # positional
            positional_tokens.append(tok)
            i += 1
        
        # Assign positionals
        idx = 0
        for ps in self.positional_specs:
            name = ps[""name""]
            if ps[""multiple""]:
                remaining = positional_tokens[idx:]
                values = []
                for v in remaining:
                    cv = self._convert(v, ps[""type""], name)
                    if ps.get(""choices"") and cv not in ps[""choices""]:
                        raise ValueError(f""Argument {name} must be one of {ps['choices']}"")
                    values.append(cv)
                result[name] = values
                idx = len(positional_tokens)
                break
            else:
                if idx >= len(positional_tokens):
                    if ps[""required""] and ps.get(""default"") is None:
                        raise ValueError(f""Missing required argument: {name}"")
                    # else keep default if any
                    continue
                v = positional_tokens[idx]
                cv = self._convert(v, ps[""type""], name)
                if ps.get(""choices"") and cv not in ps[""choices""]:
                    raise ValueError(f""Argument {name} must be one of {ps['choices']}"")
                result[name] = cv
                idx += 1
        if idx < len(positional_tokens):
            extras = positional_tokens[idx:]
            raise ValueError(f""Unexpected extra arguments: {extras}"")
        
        # Check required options
        for spec in self.opt_specs:
            if spec[""required""] and result.get(spec[""dest""]) in (None, [], False, 0):
                # consider falsy but explicit True for store_true is valid
                if spec[""action""] == ""store_true"":
                    if result.get(spec[""dest""]) is True:
                        continue
                raise ValueError(f""Missing required option: {'/'.join(spec['flags']) or spec['dest']}"")
        
        return result"
152154,hellock/icrawler,hellock_icrawler/icrawler/storage/base.py,icrawler.storage.base.BaseStorage,"from abc import ABCMeta, abstractmethod

class BaseStorage:
    """"""Base class of backend storage""""""
    __metaclass__ = ABCMeta

    @abstractmethod
    def write(self, id, data):
        """"""Abstract interface of writing data

        Args:
            id (str): unique id of the data in the storage.
            data (bytes or str): data to be stored.
        """"""
        return

    @abstractmethod
    def exists(self, id):
        """"""Check the existence of some data

        Args:
            id (str): unique id of the data in the storage

        Returns:
            bool: whether the data exists
        """"""
        return False

    @abstractmethod
    def max_file_idx(self):
        """"""Get the max existing file index

        Returns:
            int: the max index
        """"""
        return 0","
class BaseStorage:
    
    @abstractmethod
    def write(self, id, data):
        
        pass
    @abstractmethod
    def exists(self, id):
        
        pass
    @abstractmethod
    def max_file_idx(self):
        
        pass",snippet_69,4,"from abc import ABC, abstractmethod
from typing import Any


class BaseStorage(ABC):
    @abstractmethod
    def write(self, id: Any, data: bytes) -> None:
        pass

    @abstractmethod
    def exists(self, id: Any) -> bool:
        pass

    @abstractmethod
    def max_file_idx(self) -> int:
        pass"
155190,sentinel-hub/sentinelhub-py,sentinelhub/download/rate_limit.py,sentinelhub.download.rate_limit.SentinelHubRateLimit,"import time

class SentinelHubRateLimit:
    """"""Class implementing rate limiting logic of Sentinel Hub service

    It has 2 public methods:

    - register_next - tells if next download can start or if not, what is the wait before it can be asked again
    - update - updates expectations according to headers obtained from download

    The rate limiting object is collecting information about the status of rate limiting policy buckets from
    Sentinel Hub service. According to this information and a feedback from download requests it adapts expectations
    about when the next download attempt will be possible.
    """"""
    RETRY_HEADER = 'Retry-After'
    UNITS_SPENT_HEADER = 'X-ProcessingUnits-Spent'

    def __init__(self, num_processes: int=1, minimum_wait_time: float=0.05, maximum_wait_time: float=60.0):
        """"""
        :param num_processes: Number of parallel download processes running.
        :param minimum_wait_time: Minimum wait time between two consecutive download requests in seconds.
        :param maximum_wait_time: Maximum wait time between two consecutive download requests in seconds.
        """"""
        self.wait_time = min(num_processes * minimum_wait_time, maximum_wait_time)
        self.next_download_time = time.monotonic()

    def register_next(self) -> float:
        """"""Determines if next download request can start or not by returning the waiting time in seconds.""""""
        current_time = time.monotonic()
        wait_time = max(self.next_download_time - current_time, 0)
        if wait_time == 0:
            self.next_download_time = max(current_time + self.wait_time, self.next_download_time)
        return wait_time

    def update(self, headers: dict, *, default: float) -> None:
        """"""Update the next possible download time if the service has responded with the rate limit.

        :param headers: The headers that (may) contain information about waiting times.
        :param default: The default waiting time (in milliseconds) when retrying after getting a
            TOO_MANY_REQUESTS response without appropriate retry headers.
        """"""
        retry_after: float = int(headers.get(self.RETRY_HEADER, default))
        retry_after = retry_after / 1000
        if retry_after:
            self.next_download_time = max(time.monotonic() + retry_after, self.next_download_time)","
class SentinelHubRateLimit:
    
    def __init__(self, num_processes: int=1, minimum_wait_time: float=0.05, maximum_wait_time: float=60.0):
        
        pass
    def register_next(self) -> float:
        
        pass

    def update(self, headers: dict, *, default: float) -> None:
        
        pass",snippet_73,4,"class SentinelHubRateLimit:
    def __init__(self, num_processes: int = 1, minimum_wait_time: float = 0.05, maximum_wait_time: float = 60.0):
        if num_processes < 1:
            raise ValueError(""num_processes must be >= 1"")
        if minimum_wait_time < 0:
            raise ValueError(""minimum_wait_time must be >= 0"")
        if maximum_wait_time <= 0:
            raise ValueError(""maximum_wait_time must be > 0"")
        if minimum_wait_time > maximum_wait_time:
            raise ValueError(""minimum_wait_time cannot exceed maximum_wait_time"")

        self.num_processes = int(num_processes)
        self.minimum_wait_time = float(minimum_wait_time)
        self.maximum_wait_time = float(maximum_wait_time)

        self._current_wait = self.minimum_wait_time
        self._initialized = False

    def register_next(self) -> float:
        return max(self.minimum_wait_time, min(self._current_wait, self.maximum_wait_time))

    def update(self, headers: dict, *, default: float) -> None:
        def _to_float(value):
            try:
                if value is None:
                    return None
                if isinstance(value, (int, float)):
                    return float(value)
                s = str(value).strip()
                if s == """":
                    return None
                return float(s)
            except Exception:
                return None

        def _parse_retry_after(hdrs):
            # Retry-After can be seconds or HTTP-date
            val = hdrs.get(""retry-after"")
            if val is None:
                return None
            # try seconds
            sec = _to_float(val)
            if sec is not None and sec >= 0:
                return sec
            # try HTTP-date
            try:
                from email.utils import parsedate_to_datetime
                dt = parsedate_to_datetime(str(val))
                if dt is None:
                    return None
                import datetime as _dt
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=_dt.timezone.utc)
                now = _dt.datetime.now(_dt.timezone.utc)
                delta = (dt - now).total_seconds()
                return max(0.0, float(delta))
            except Exception:
                return None

        if not isinstance(headers, dict):
            headers = {}

        # normalize header keys
        lower_headers = {str(k).lower(): v for k, v in headers.items()}

        # Determine new base wait
        new_wait = None

        retry_after = _parse_retry_after(lower_headers)
        if retry_after is not None:
            # Respect server-mandated pause
            new_wait = max(float(default), float(retry_after))
        else:
            remaining = _to_float(
                lower_headers.get(""x-ratelimit-remaining"")
                or lower_headers.get(""x-rate-limit-remaining"")
                or lower_headers.get(""x-rate-limit-remaining"".replace(""-"", """"))
            )
            reset = _to_float(
                lower_headers.get(""x-ratelimit-reset"")
                or lower_headers.get(""x-rate-limit-reset"")
                or lower_headers.get(""x-rate-limit-reset"".replace(""-"", """"))
            )

            if reset is not None and reset < 0:
                reset = None

            if remaining is not None and reset is not None:
                if remaining <= 0:
                    candidate = max(reset, float(default))
                else:
                    # Average spacing to use full quota by reset
                    interval = reset / max(1.0, remaining)
                    # Scale for number of worker processes sharing the same account/quota
                    candidate = max(float(default), interval * float(self.num_processes))
                new_wait = candidate

        if new_wait is None:
            new_wait = float(default)

        # Clamp
        new_wait = max(self.minimum_wait_time, min(float(new_wait), self.maximum_wait_time))

        # Smooth updates to avoid jitter
        if not self._initialized:
            self._current_wait = new_wait
            self._initialized = True
        else:
            alpha = 0.3
            self._current_wait = alpha * new_wait + (1.0 - alpha) * self._current_wait

        # Ensure bounds after smoothing
        self._current_wait = max(self.minimum_wait_time, min(self._current_wait, self.maximum_wait_time))"
156520,textX/textX,textx/scoping/providers.py,textx.scoping.providers.PlainName,"from textx.exceptions import TextXSemanticError

class PlainName:
    """"""
    plain name scope provider
    """"""

    def __init__(self, multi_metamodel_support=True):
        """"""
        the default scope provider constructor

        Args:
            multi_metamodel_support: enable a AST based search, instead
            of using the parser._instances
        """"""
        self.multi_metamodel_support = multi_metamodel_support
        pass

    def __call__(self, obj, attr, obj_ref):
        """"""
        the default scope provider

        Args:
            obj: unused (used for multi_metamodel_support)
            attr: unused
            obj_ref: the cross reference to be resolved

        Returns:
            the resolved reference or None
        """"""
        from textx.const import RULE_ABSTRACT, RULE_COMMON
        from textx.model import ObjCrossRef
        from textx.scoping.tools import get_parser
        if obj_ref is None:
            return None
        assert type(obj_ref) is ObjCrossRef, type(obj_ref)
        if get_parser(obj).debug:
            get_parser(obj).dprint(f'Resolving obj crossref: {obj_ref.cls}:{obj_ref.obj_name}')

        def _inner_resolve_link_rule_ref(cls, obj_name):
            """"""
            Depth-first resolving of link rule reference.
            """"""
            if cls._tx_type is RULE_ABSTRACT:
                for inherited in cls._tx_inh_by:
                    result = _inner_resolve_link_rule_ref(inherited, obj_name)
                    if result:
                        return result
            elif cls._tx_type == RULE_COMMON and id(cls) in get_parser(obj)._instances:
                objs = get_parser(obj)._instances[id(cls)]
                return objs.get(obj_name)
        if self.multi_metamodel_support:
            from textx import get_children, get_model, textx_isinstance
            result_lst = get_children(lambda x: hasattr(x, 'name') and x.name == obj_ref.obj_name and textx_isinstance(x, obj_ref.cls), get_model(obj))
            if len(result_lst) == 1:
                result = result_lst[0]
            elif len(result_lst) > 1:
                line, col = get_parser(obj).pos_to_linecol(obj_ref.position)
                raise TextXSemanticError(f'name {obj_ref.obj_name} is not unique.', line=line, col=col, filename=get_model(obj)._tx_filename)
            else:
                result = None
        else:
            result = _inner_resolve_link_rule_ref(obj_ref.cls, obj_ref.obj_name)
        return result","
class PlainName:
    
    def __init__(self, multi_metamodel_support=True):
        
        pass
    def __call__(self, obj, attr, obj_ref):
        
        pass

        def _inner_resolve_link_rule_ref(cls, obj_name):
            
                pass",snippet_74,4,"class PlainName:
    def __init__(self, multi_metamodel_support=True):
        self.multi_metamodel_support = bool(multi_metamodel_support)

    def __call__(self, obj, attr, obj_ref):
        def _inner_resolve_link_rule_ref(cls_name, obj_name):
            return _resolve_by_name(root, obj_name, type_name=cls_name)

        if obj_ref is None:
            return None

        if not hasattr(obj, ""__dict__"") and not isinstance(obj, (list, tuple, dict, set)):
            return None

        if not isinstance(attr, str):
            attr = str(attr)

        if not isinstance(obj_ref, str):
            # Already an object; return as-is
            return obj_ref

        # Determine root container by following parent-like links
        root = obj
        visited_ids = set()
        while True:
            parent = None
            for parent_attr in (""parent"", ""__parent__"", ""_parent""):
                if hasattr(root, parent_attr):
                    parent = getattr(root, parent_attr)
                    break
            if parent is None or id(parent) in visited_ids:
                break
            visited_ids.add(id(parent))
            root = parent

        # Parse possible ""ClassName:name"" or ""ClassName#name"" notation
        cls_name = None
        name_token = obj_ref
        if self.multi_metamodel_support and isinstance(obj_ref, str):
            if "":"" in obj_ref:
                parts = obj_ref.split("":"", 1)
                if parts[0] and parts[1]:
                    cls_name, name_token = parts[0].strip(), parts[1].strip()
            elif ""#"" in obj_ref:
                parts = obj_ref.split(""#"", 1)
                if parts[0] and parts[1]:
                    cls_name, name_token = parts[0].strip(), parts[1].strip()

        # Attempt dotted path resolution: a.b.c
        if ""."" in name_token:
            current = root
            for segment in name_token.split("".""):
                current = _resolve_by_name(current, segment, type_name=cls_name)
                if current is None:
                    break
                # After first segment, do not constrain by class name further
                cls_name = None
            return current

        # Plain name or with class constraint
        if cls_name:
            return _inner_resolve_link_rule_ref(cls_name, name_token)
        return _resolve_by_name(root, name_token)


def _is_primitive(x):
    return isinstance(x, (str, bytes, int, float, bool, complex))


def _iter_children(obj):
    seen = set()
    stack = [obj]
    while stack:
        cur = stack.pop()
        cid = id(cur)
        if cid in seen:
            continue
        seen.add(cid)

        if _is_primitive(cur):
            continue

        # Yield children depending on container type
        if isinstance(cur, dict):
            values = list(cur.values())
            for v in values:
                if not _is_primitive(v):
                    yield v
                    stack.append(v)
            continue

        if isinstance(cur, (list, tuple, set)):
            for v in cur:
                if not _is_primitive(v):
                    yield v
                    stack.append(v)
            continue

        # For objects with attributes
        if hasattr(cur, ""__dict__""):
            for k, v in list(cur.__dict__.items()):
                if k.startswith(""_""):
                    continue
                if callable(v):
                    continue
                if _is_primitive(v):
                    continue
                if isinstance(v, (list, tuple, set)):
                    for item in v:
                        if not _is_primitive(item):
                            yield item
                            stack.append(item)
                    continue
                if isinstance(v, dict):
                    for item in v.values():
                        if not _is_primitive(item):
                            yield item
                            stack.append(item)
                    continue
                yield v
                stack.append(v)


def _type_name(o):
    try:
        return type(o).__name__
    except Exception:
        return None


def _get_name(o):
    for attr in (""name"", ""id"", ""identifier""):
        if hasattr(o, attr):
            val = getattr(o, attr)
            if isinstance(val, str):
                return val
    return None


def _resolve_by_name(scope, name, type_name=None):
    # Breadth-first search for stability
    from collections import deque

    if scope is None:
        return None

    # Direct match if scope itself matches
    if not _is_primitive(scope) and hasattr(scope, ""__dict__""):
        sname = _get_name(scope)
        if sname == name and (type_name is None or _type_name(scope) == type_name):
            return scope

    q = deque()
    seen = set()

    def enqueue(x):
        ix = id(x)
        if ix not in seen and not _is_primitive(x):
            seen.add(ix)
            q.append(x)

    enqueue(scope)

    while q:
        cur = q.popleft()

        # Iterate children
        for child in _iter_children(cur):
            # Check match
            cname = _get_name(child)
            if cname == name and (type_name is None or _type_name(child) == type_name):
                return child
            enqueue(child)

    return None"
156861,csparpa/pyowm,csparpa_pyowm/pyowm/utils/geo.py,pyowm.utils.geo.Geometry,"class Geometry:
    """"""
    Abstract parent class for geotypes

    """"""

    def geojson(self):
        """"""
        Returns a GeoJSON string representation of this geotype, compliant to
        RFC 7946 (https://tools.ietf.org/html/rfc7946)
        :return: str
        """"""
        raise NotImplementedError()

    def to_dict(self):
        """"""
        Returns a dict representation of this geotype
        :return: dict
        """"""
        raise NotImplementedError()","class Geometry:
    
    def geojson(self):
        
        pass
    def to_dict(self):
        
        pass",snippet_75,3,"class Geometry:
    def __init__(self, geom_type, coordinates, bbox=None):
        self.type = geom_type
        self.coordinates = coordinates
        self._validate()
        self.bbox = bbox if bbox is not None else self._compute_bbox()

    def _validate(self):
        if not isinstance(self.type, str) or not self.type:
            raise ValueError(""Geometry type must be a non-empty string."")
        if self.type not in {
            ""Point"",
            ""MultiPoint"",
            ""LineString"",
            ""MultiLineString"",
            ""Polygon"",
            ""MultiPolygon"",
            ""GeometryCollection"",
        }:
            raise ValueError(f""Unsupported geometry type: {self.type}"")

        if self.type == ""GeometryCollection"":
            if not isinstance(self.coordinates, list):
                raise ValueError(""GeometryCollection must be a list of geometries."")
            for geom in self.coordinates:
                if not isinstance(geom, dict) or ""type"" not in geom:
                    raise ValueError(""Each geometry in GeometryCollection must be a dict with a 'type'."")
        else:
            if self.coordinates is None:
                raise ValueError(""Coordinates must not be None."")
            # Basic shape checks for common types
            if self.type == ""Point"":
                self._validate_position(self.coordinates)
            elif self.type == ""MultiPoint"":
                self._validate_positions(self.coordinates)
            elif self.type == ""LineString"":
                self._validate_positions(self.coordinates, min_len=2)
            elif self.type == ""MultiLineString"":
                if not isinstance(self.coordinates, list) or not self.coordinates:
                    raise ValueError(""MultiLineString must be a non-empty list of LineStrings."")
                for ls in self.coordinates:
                    self._validate_positions(ls, min_len=2)
            elif self.type == ""Polygon"":
                self._validate_polygon(self.coordinates)
            elif self.type == ""MultiPolygon"":
                if not isinstance(self.coordinates, list) or not self.coordinates:
                    raise ValueError(""MultiPolygon must be a non-empty list of Polygons."")
                for poly in self.coordinates:
                    self._validate_polygon(poly)

    def _validate_position(self, pos):
        if (
            not isinstance(pos, (list, tuple))
            or len(pos) < 2
            or not all(isinstance(n, (int, float)) for n in pos[:2])
        ):
            raise ValueError(""Position must be a list/tuple of at least two numbers [x, y, ...]."")

    def _validate_positions(self, positions, min_len=1):
        if not isinstance(positions, list) or len(positions) < min_len:
            raise ValueError(f""Positions must be a list with at least {min_len} positions."")
        for p in positions:
            self._validate_position(p)

    def _validate_linear_ring(self, ring):
        self._validate_positions(ring, min_len=4)
        if ring[0][:2] != ring[-1][:2]:
            raise ValueError(""LinearRing must be closed (first and last positions must be equal)."")

    def _validate_polygon(self, coords):
        if not isinstance(coords, list) or not coords:
            raise ValueError(""Polygon must be a non-empty list of LinearRings."")
        for ring in coords:
            self._validate_linear_ring(ring)

    def _iter_positions(self, coords=None):
        if coords is None:
            coords = self.coordinates
        if self.type == ""Point"":
            yield coords
        elif self.type in (""MultiPoint"", ""LineString""):
            for p in coords:
                yield p
        elif self.type in (""MultiLineString"", ""Polygon""):
            for part in coords:
                for p in part:
                    yield p
        elif self.type == ""MultiPolygon"":
            for poly in coords:
                for ring in poly:
                    for p in ring:
                        yield p
        elif self.type == ""GeometryCollection"":
            for geom in coords:
                if not isinstance(geom, dict):
                    continue
                gtype = geom.get(""type"")
                gcoords = geom.get(""coordinates"")
                if gtype == ""Point"":
                    if gcoords is not None:
                        yield gcoords
                elif gtype in (""MultiPoint"", ""LineString""):
                    for p in (gcoords or []):
                        yield p
                elif gtype in (""MultiLineString"", ""Polygon""):
                    for part in (gcoords or []):
                        for p in part:
                            yield p
                elif gtype == ""MultiPolygon"":
                    for poly in (gcoords or []):
                        for ring in poly:
                            for p in ring:
                                yield p

    def _compute_bbox(self):
        xs = []
        ys = []
        for pos in self._iter_positions():
            if (
                isinstance(pos, (list, tuple))
                and len(pos) >= 2
                and isinstance(pos[0], (int, float))
                and isinstance(pos[1], (int, float))
            ):
                xs.append(pos[0])
                ys.append(pos[1])
        if not xs or not ys:
            return None
        return [min(xs), min(ys), max(xs), max(ys)]

    def geojson(self):
        obj = {""type"": self.type}
        if self.type == ""GeometryCollection"":
            obj[""geometries""] = self.coordinates
        else:
            obj[""coordinates""] = self.coordinates
        if self.bbox is not None:
            obj[""bbox""] = list(self.bbox)
        return obj

    def to_dict(self):
        d = {
            ""type"": self.type,
            ""coordinates"": self.coordinates,
        }
        if self.bbox is not None:
            d[""bbox""] = list(self.bbox)
        return d"
157718,jwkvam/bowtie,bowtie/_cache.py,bowtie._cache._Cache,"from eventlet.queue import LightQueue
from flask_socketio import emit
import eventlet
import flask
import msgpack
from bowtie._component import pack

class _Cache:
    """"""Store data in the browser.

    This cache uses session storage so data will stay
    in the browser until the tab is closed.
    All data must be serializable, which means if the
    serialization transforms the data it won't be the same
    when it is fetched.

    Examples
    --------
    >>> from bowtie import cache
    >>> cache['a'] = True  # doctest: +SKIP
    >>> cache['a']  # doctest: +SKIP
    True
    >>> cache['b'] = np.arange(5)  # doctest: +SKIP
    >>> cache['b']  # doctest: +SKIP
    [1, 2, 3, 4, 5]

    """"""

    def __getitem__(self, key):
        """"""Load the value stored with the key.

        Parameters
        ----------
        key : str
            The key to lookup the value stored.

        Returns
        -------
        object
            The value if the key exists in the cache, otherwise None.

        """"""
        validate(key)
        signal = 'cache_load'
        event = LightQueue(1)
        if flask.has_request_context():
            emit(signal, {'data': pack(key)}, callback=event.put)
        else:
            sio = flask.current_app.extensions['socketio']
            sio.emit(signal, {'data': pack(key)}, callback=event.put)
        return msgpack.unpackb(bytes(event.get(timeout=10)), encoding='utf8')

    def __setitem__(self, key, value):
        """"""Store the key value pair.

        Parameters
        ----------
        key : str
            The key to determine where it's stored, you'll need this to load the value later.
        value : object
            The value to store in the cache.

        Returns
        -------
        None

        """"""
        validate(key)
        signal = 'cache_save'
        if flask.has_request_context():
            emit(signal, {'key': pack(key), 'data': pack(value)})
        else:
            sio = flask.current_app.extensions['socketio']
            sio.emit(signal, {'key': pack(key), 'data': pack(value)})
        eventlet.sleep()","
class _Cache:
    '''Store data in the browser.
    This cache uses session storage so data will stay
    in the browser until the tab is closed.
    All data must be serializable, which means if the
    serialization transforms the data it won't be the same
    when it is fetched.
    Examples
    --------
    >>> from bowtie import cache
    >>> cache['a'] = True  # doctest: +SKIP
    >>> cache['a']  # doctest: +SKIP
    True
    >>> cache['b'] = np.arange(5)  # doctest: +SKIP
    >>> cache['b']  # doctest: +SKIP
    [1, 2, 3, 4, 5]
    '''
    def __getitem__(self, key):
        
        pass

    def __setitem__(self, key, value):
        
        pass",snippet_78,3,"import json

class _Cache:
    '''Store data in the browser.
    This cache uses session storage so data will stay
    in the browser until the tab is closed.
    All data must be serializable, which means if the
    serialization transforms the data it won't be the same
    when it is fetched.
    Examples
    --------
    >>> from bowtie import cache
    >>> cache['a'] = True  # doctest: +SKIP
    >>> cache['a']  # doctest: +SKIP
    True
    >>> cache['b'] = np.arange(5)  # doctest: +SKIP
    >>> cache['b']  # doctest: +SKIP
    [1, 2, 3, 4, 5]
    '''
    _store = {}

    def __getitem__(self, key):
        k = str(key)
        if k not in self._store:
            raise KeyError(key)
        return json.loads(self._store[k])

    def __setitem__(self, key, value):
        def _default(o):
            if hasattr(o, ""tolist""):
                return o.tolist()
            if isinstance(o, (set, tuple)):
                return list(o)
            if isinstance(o, bytes):
                return o.decode(""utf-8"")
            raise TypeError(f""Object of type {type(o).__name__} is not JSON serializable"")

        self._store[str(key)] = json.dumps(value, default=_default)"
159496,Erotemic/ubelt,Erotemic_ubelt/ubelt/util_mixins.py,ubelt.util_mixins.NiceRepr,"import warnings

class NiceRepr:
    """"""
    Inherit from this class and define ``__nice__`` to ""nicely"" print your
    objects.

    Defines ``__str__`` and ``__repr__`` in terms of ``__nice__`` function
    Classes that inherit from :class:`NiceRepr` should redefine ``__nice__``.
    If the inheriting class has a ``__len__``, method then the default
    ``__nice__`` method will return its length.

    Example:
        >>> import ubelt as ub
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        return 'info'
        >>> foo = Foo()
        >>> assert str(foo) == '<Foo(info)>'
        >>> assert repr(foo).startswith('<Foo(info) at ')

    Example:
        >>> import ubelt as ub
        >>> class Bar(ub.NiceRepr):
        ...    pass
        >>> bar = Bar()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     assert 'object at' in str(bar)
        >>>     assert 'object at' in repr(bar)

    Example:
        >>> import ubelt as ub
        >>> class Baz(ub.NiceRepr):
        ...    def __len__(self):
        ...        return 5
        >>> baz = Baz()
        >>> assert str(baz) == '<Baz(5)>'

    Example:
        >>> import ubelt as ub
        >>> # If your nice message has a bug, it shouldn't bring down the house
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        assert False
        >>> foo = Foo()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     print('foo = {!r}'.format(foo))
        foo = <...Foo ...>

    Example:
        >>> import ubelt as ub
        >>> class Animal(ub.NiceRepr):
        ...    def __init__(self):
        ...        ...
        ...    def __nice__(self):
        ...        return ''
        >>> class Cat(Animal):
        >>>     ...
        >>> class Dog(Animal):
        >>>     ...
        >>> class Beagle(Dog):
        >>>     ...
        >>> class Ragdoll(Cat):
        >>>     ...
        >>> instances = [Animal(), Cat(), Dog(), Beagle(), Ragdoll()]
        >>> for inst in instances:
        >>>     print(str(inst))
        <Animal()>
        <Cat()>
        <Dog()>
        <Beagle()>
        <Ragdoll()>

    In the case where you cant or dont want to use ubelt.NiceRepr you can get
    similar behavior by pasting the methods from the following snippet into
    your class:

    .. code:: python

        class MyClass:

            def __nice__(self):
                return 'your concise information'

            def __repr__(self):
                nice = self.__nice__()
                classname = self.__class__.__name__
                return '<{0}({1}) at {2}>'.format(classname, nice, hex(id(self)))

            def __str__(self):
                classname = self.__class__.__name__
                nice = self.__nice__()
                return '<{0}({1})>'.format(classname, nice)
    """"""

    def __nice__(self):
        """"""
        Returns:
            str
        """"""
        if hasattr(self, '__len__'):
            return str(len(self))
        else:
            raise NotImplementedError('Define the __nice__ method for {!r}'.format(self.__class__))

    def __repr__(self):
        """"""
        Returns:
            str
        """"""
        try:
            nice = self.__nice__()
            classname = self.__class__.__name__
            return '<{0}({1}) at {2}>'.format(classname, nice, hex(id(self)))
        except Exception as ex:
            warnings.warn(str(ex), category=RuntimeWarning)
            return object.__repr__(self)

    def __str__(self):
        """"""
        Returns:
            str
        """"""
        try:
            classname = self.__class__.__name__
            nice = self.__nice__()
            return '<{0}({1})>'.format(classname, nice)
        except Exception as ex:
            warnings.warn(str(ex), category=RuntimeWarning)
            return object.__repr__(self)","
class NiceRepr:
    '''
    Inherit from this class and define ``__nice__`` to ""nicely"" print your
    objects.
    Defines ``__str__`` and ``__repr__`` in terms of ``__nice__`` function
    Classes that inherit from :class:`NiceRepr` should redefine ``__nice__``.
    If the inheriting class has a ``__len__``, method then the default
    ``__nice__`` method will return its length.
    Example:
        >>> import ubelt as ub
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        return 'info'
        >>> foo = Foo()
        >>> assert str(foo) == '<Foo(info)>'
        >>> assert repr(foo).startswith('<Foo(info) at ')
    Example:
        >>> import ubelt as ub
        >>> class Bar(ub.NiceRepr):
        ...    pass
        >>> bar = Bar()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     assert 'object at' in str(bar)
        >>>     assert 'object at' in repr(bar)
    Example:
        >>> import ubelt as ub
        >>> class Baz(ub.NiceRepr):
        ...    def __len__(self):
        ...        return 5
        >>> baz = Baz()
        >>> assert str(baz) == '<Baz(5)>'
    Example:
        >>> import ubelt as ub
        >>> # If your nice message has a bug, it shouldn't bring down the house
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        assert False
        >>> foo = Foo()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     print('foo = {!r}'.format(foo))
        foo = <...Foo ...>
    Example:
        >>> import ubelt as ub
        >>> class Animal(ub.NiceRepr):
        ...    def __init__(self):
        ...        ...
        ...    def __nice__(self):
        ...        return ''
        >>> class Cat(Animal):
        >>>     ...
        >>> class Dog(Animal):
        >>>     ...
        >>> class Beagle(Dog):
        >>>     ...
        >>> class Ragdoll(Cat):
        >>>     ...
        >>> instances = [Animal(), Cat(), Dog(), Beagle(), Ragdoll()]
        >>> for inst in instances:
        >>>     print(str(inst))
        <Animal()>
        <Cat()>
        <Dog()>
        <Beagle()>
        <Ragdoll()>
    In the case where you cant or dont want to use ubelt.NiceRepr you can get
    similar behavior by pasting the methods from the following snippet into
    your class:
    .. code:: python
        class MyClass:
            def __nice__(self):
                return 'your concise information'
            def __repr__(self):
                nice = self.__nice__()
                classname = self.__class__.__name__
                return '<{0}({1}) at {2}>'.format(classname, nice, hex(id(self)))
            def __str__(self):
                classname = self.__class__.__name__
                nice = self.__nice__()
                return '<{0}({1})>'.format(classname, nice)
                    '''
            def __nice__(self):
                
                        pass

            def __repr__(self):
                
                        pass

            def __str__(self):
                
                        pass",snippet_80,4,"class NiceRepr:
    '''
    Inherit from this class and define ``__nice__`` to ""nicely"" print your
    objects.
    Defines ``__str__`` and ``__repr__`` in terms of ``__nice__`` function
    Classes that inherit from :class:`NiceRepr` should redefine ``__nice__``.
    If the inheriting class has a ``__len__``, method then the default
    ``__nice__`` method will return its length.
    Example:
        >>> import ubelt as ub
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        return 'info'
        >>> foo = Foo()
        >>> assert str(foo) == '<Foo(info)>'
        >>> assert repr(foo).startswith('<Foo(info) at ')
    Example:
        >>> import ubelt as ub
        >>> class Bar(ub.NiceRepr):
        ...    pass
        >>> bar = Bar()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     assert 'object at' in str(bar)
        >>>     assert 'object at' in repr(bar)
    Example:
        >>> import ubelt as ub
        >>> class Baz(ub.NiceRepr):
        ...    def __len__(self):
        ...        return 5
        >>> baz = Baz()
        >>> assert str(baz) == '<Baz(5)>'
    Example:
        >>> import ubelt as ub
        >>> # If your nice message has a bug, it shouldn't bring down the house
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        assert False
        >>> foo = Foo()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     print('foo = {!r}'.format(foo))
        foo = <...Foo ...>
    Example:
        >>> import ubelt as ub
        >>> class Animal(ub.NiceRepr):
        ...    def __init__(self):
        ...        ...
        ...    def __nice__(self):
        ...        return ''
        >>> class Cat(Animal):
        >>>     ...
        >>> class Dog(Animal):
        >>>     ...
        >>> class Beagle(Dog):
        >>>     ...
        >>> class Ragdoll(Cat):
        >>>     ...
        >>> instances = [Animal(), Cat(), Dog(), Beagle(), Ragdoll()]
        >>> for inst in instances:
        >>>     print(str(inst))
        <Animal()>
        <Cat()>
        <Dog()>
        <Beagle()>
        <Ragdoll()>
    In the case where you cant or dont want to use ubelt.NiceRepr you can get
    similar behavior by pasting the methods from the following snippet into
    your class:
    .. code:: python
        class MyClass:
            def __nice__(self):
                return 'your concise information'
            def __repr__(self):
                nice = self.__nice__()
                classname = self.__class__.__name__
                return '<{0}({1}) at {2}>'.format(classname, nice, hex(id(self)))
            def __str__(self):
                classname = self.__class__.__name__
                nice = self.__nice__()
                return '<{0}({1})>'.format(classname, nice)
                    '''
    def __nice__(self):
        import warnings
        # Default behavior: if __len__ is defined, use its value,
        # otherwise warn and fall back to object.__repr__
        if hasattr(self, '__len__'):
            try:
                return str(len(self))
            except Exception:
                # If len fails, fall through to warning/fallback below
                pass
        warnings.warn(
            '{}.__nice__ is not defined; falling back to object repr'.format(
                self.__class__.__name__
            ),
            RuntimeWarning,
            stacklevel=2,
        )
        return object.__repr__(self)

    def __repr__(self):
        import warnings
        classname = self.__class__.__name__
        try:
            nice = self.__nice__()
            if nice is None:
                nice = ''
            else:
                nice = str(nice)
            inside = '' if nice == '' else nice
            if inside == '':
                return '<{}() at {}>'.format(classname, hex(id(self)))
            else:
                return '<{}({}) at {}>'.format(classname, inside, hex(id(self)))
        except Exception:
            warnings.warn(
                'Error in {}.__nice__; falling back to safe repr'.format(classname),
                RuntimeWarning,
                stacklevel=2,
            )
            return '<...{} ...>'.format(classname)

    def __str__(self):
        import warnings
        classname = self.__class__.__name__
        try:
            nice = self.__nice__()
            if nice is None:
                nice = ''
            else:
                nice = str(nice)
            inside = '' if nice == '' else nice
            if inside == '':
                return '<{}()>'.format(classname)
            else:
                return '<{}({})>'.format(classname, inside)
        except Exception:
            warnings.warn(
                'Error in {}.__nice__; falling back to safe str'.format(classname),
                RuntimeWarning,
                stacklevel=2,
            )
            return '<...{} ...>'"
159497,Erotemic/ubelt,Erotemic_ubelt/ubelt/util_path.py,ubelt.util_path.ChDir,"import os

class ChDir:
    """"""
    Context manager that changes the current working directory and then
    returns you to where you were.

    This is nearly the same as the stdlib :func:`contextlib.chdir`, with the
    exception that it will do nothing if the input path is None (i.e. the user
    did not want to change directories).

    SeeAlso:
        :func:`contextlib.chdir`

    Example:
        >>> import ubelt as ub
        >>> dpath = ub.Path.appdir('ubelt/tests/chdir').ensuredir()
        >>> dir1 = (dpath / 'dir1').ensuredir()
        >>> dir2 = (dpath / 'dir2').ensuredir()
        >>> with ChDir(dpath):
        >>>     assert ub.Path.cwd() == dpath
        >>>     # change to the given directory, and then returns back
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(dir2):
        >>>             assert ub.Path.cwd() == dir2
        >>>             # changes inside the context manager will be reset
        >>>             os.chdir(dpath)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(None):
        >>>             assert ub.Path.cwd() == dir1
        >>>             # When disabled, the cwd does *not* reset at context exit
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir2
        >>>         os.chdir(dir1)
        >>>         # Dont change dirs, but reset to your cwd at context end
        >>>         with ChDir('.'):
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
    """"""

    def __init__(self, dpath):
        """"""
        Args:
            dpath (str | PathLike | None):
                The new directory to work in.
                If None, then the context manager is disabled.
        """"""
        self._context_dpath = dpath
        self._orig_dpath = None

    def __enter__(self):
        """"""
        Returns:
            ChDir: self
        """"""
        if self._context_dpath is not None:
            self._orig_dpath = os.getcwd()
            os.chdir(self._context_dpath)
        return self

    def __exit__(self, ex_type, ex_value, ex_traceback):
        """"""
        Args:
            ex_type (Type[BaseException] | None):
            ex_value (BaseException | None):
            ex_traceback (TracebackType | None):

        Returns:
            bool | None
        """"""
        if self._context_dpath is not None:
            os.chdir(self._orig_dpath)","
class ChDir:
    '''
    Context manager that changes the current working directory and then
    returns you to where you were.
    This is nearly the same as the stdlib :func:`contextlib.chdir`, with the
    exception that it will do nothing if the input path is None (i.e. the user
    did not want to change directories).
    SeeAlso:
        :func:`contextlib.chdir`
    Example:
        >>> import ubelt as ub
        >>> dpath = ub.Path.appdir('ubelt/tests/chdir').ensuredir()
        >>> dir1 = (dpath / 'dir1').ensuredir()
        >>> dir2 = (dpath / 'dir2').ensuredir()
        >>> with ChDir(dpath):
        >>>     assert ub.Path.cwd() == dpath
        >>>     # change to the given directory, and then returns back
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(dir2):
        >>>             assert ub.Path.cwd() == dir2
        >>>             # changes inside the context manager will be reset
        >>>             os.chdir(dpath)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(None):
        >>>             assert ub.Path.cwd() == dir1
        >>>             # When disabled, the cwd does *not* reset at context exit
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir2
        >>>         os.chdir(dir1)
        >>>         # Dont change dirs, but reset to your cwd at context end
        >>>         with ChDir('.'):
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
    '''
    def __init__(self, dpath):
        
        pass

    def __enter__(self):
        
        pass

    def __exit__(self, ex_type, ex_value, ex_traceback):
        
        pass",snippet_81,4,"class ChDir:
    '''
    Context manager that changes the current working directory and then
    returns you to where you were.
    This is nearly the same as the stdlib :func:`contextlib.chdir`, with the
    exception that it will do nothing if the input path is None (i.e. the user
    did not want to change directories).
    SeeAlso:
        :func:`contextlib.chdir`
    Example:
        >>> import ubelt as ub
        >>> dpath = ub.Path.appdir('ubelt/tests/chdir').ensuredir()
        >>> dir1 = (dpath / 'dir1').ensuredir()
        >>> dir2 = (dpath / 'dir2').ensuredir()
        >>> with ChDir(dpath):
        >>>     assert ub.Path.cwd() == dpath
        >>>     # change to the given directory, and then returns back
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(dir2):
        >>>             assert ub.Path.cwd() == dir2
        >>>             # changes inside the context manager will be reset
        >>>             os.chdir(dpath)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(None):
        >>>             assert ub.Path.cwd() == dir1
        >>>             # When disabled, the cwd does *not* reset at context exit
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir2
        >>>         os.chdir(dir1)
        >>>         # Dont change dirs, but reset to your cwd at context end
        >>>         with ChDir('.'):
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
    '''
    def __init__(self, dpath):
        import os
        self._os = os
        self._target = dpath
        self._enabled = dpath is not None
        self._old_cwd = None

    def __enter__(self):
        if self._enabled:
            self._old_cwd = self._os.getcwd()
            self._os.chdir(self._target)
        return self

    def __exit__(self, ex_type, ex_value, ex_traceback):
        if self._enabled and self._old_cwd is not None:
            self._os.chdir(self._old_cwd)
        return False"
159980,pazz/alot,alot/completion/completer.py,alot.completion.completer.Completer,"import abc

class Completer:
    """"""base class for completers""""""
    __metaclass__ = abc.ABCMeta

    @abc.abstractmethod
    def complete(self, original, pos):
        """"""returns a list of completions and cursor positions for the string
        `original` from position `pos` on.

        :param original: the string to complete
        :type original: str
        :param pos: starting position to complete from
        :type pos: int
        :returns: pairs of completed string and cursor position in the
                  new string
        :rtype: list of (str, int)
        :raises: :exc:`CompletionError`
        """"""
        pass

    def relevant_part(self, original, pos):
        """"""
        Calculate the subword in a ' '-separated list of substrings of
        `original` that `pos` is in.
        """"""
        start = original.rfind(' ', 0, pos) + 1
        end = original.find(' ', pos - 1)
        if end == -1:
            end = len(original)
        return (original[start:end], start, end, pos - start)","
class Completer:
    
    @abc.abstractmethod
    def complete(self, original, pos):
        
        pass
    def relevant_part(self, original, pos):
        
        pass",snippet_82,3,"import abc
import string


class Completer(metaclass=abc.ABCMeta):
    @abc.abstractmethod
    def complete(self, original, pos):
        raise NotImplementedError

    def relevant_part(self, original, pos):
        if original is None:
            original = """"
        if pos is None:
            pos = len(original)
        pos = max(0, min(pos, len(original)))

        allowed = set(string.ascii_letters + string.digits + ""_."")
        i = pos
        while i > 0 and original[i - 1] in allowed:
            i -= 1
        return original[i:pos]"
160363,splunk/splunk-sdk-python,splunk_splunk-sdk-python/splunklib/modularinput/argument.py,splunklib.modularinput.argument.Argument,"import xml.etree.ElementTree as ET

class Argument:
    """"""Class representing an argument to a modular input kind.

    ``Argument`` is meant to be used with ``Scheme`` to generate an XML
    definition of the modular input kind that Splunk understands.

    ``name`` is the only required parameter for the constructor.

        **Example with least parameters**::

            arg1 = Argument(name=""arg1"")

        **Example with all parameters**::

            arg2 = Argument(
                name=""arg2"",
                description=""This is an argument with lots of parameters"",
                validation=""is_pos_int('some_name')"",
                data_type=Argument.data_type_number,
                required_on_edit=True,
                required_on_create=True
            )
    """"""
    data_type_boolean = 'BOOLEAN'
    data_type_number = 'NUMBER'
    data_type_string = 'STRING'

    def __init__(self, name, description=None, validation=None, data_type=data_type_string, required_on_edit=False, required_on_create=False, title=None):
        """"""
        :param name: ``string``, identifier for this argument in Splunk.
        :param description: ``string``, human-readable description of the argument.
        :param validation: ``string`` specifying how the argument should be validated, if using internal validation.
               If using external validation, this will be ignored.
        :param data_type: ``string``, data type of this field; use the class constants.
               ""data_type_boolean"", ""data_type_number"", or ""data_type_string"".
        :param required_on_edit: ``Boolean``, whether this arg is required when editing an existing modular input of this kind.
        :param required_on_create: ``Boolean``, whether this arg is required when creating a modular input of this kind.
        :param title: ``String``, a human-readable title for the argument.
        """"""
        self.name = name
        self.description = description
        self.validation = validation
        self.data_type = data_type
        self.required_on_edit = required_on_edit
        self.required_on_create = required_on_create
        self.title = title

    def add_to_document(self, parent):
        """"""Adds an ``Argument`` object to this ElementTree document.

        Adds an <arg> subelement to the parent element, typically <args>
        and sets up its subelements with their respective text.

        :param parent: An ``ET.Element`` to be the parent of a new <arg> subelement
        :returns: An ``ET.Element`` object representing this argument.
        """"""
        arg = ET.SubElement(parent, 'arg')
        arg.set('name', self.name)
        if self.title is not None:
            ET.SubElement(arg, 'title').text = self.title
        if self.description is not None:
            ET.SubElement(arg, 'description').text = self.description
        if self.validation is not None:
            ET.SubElement(arg, 'validation').text = self.validation
        subelements = [('data_type', self.data_type), ('required_on_edit', self.required_on_edit), ('required_on_create', self.required_on_create)]
        for name, value in subelements:
            ET.SubElement(arg, name).text = str(value).lower()
        return arg","
class Argument:
    '''Class representing an argument to a modular input kind.
    ``Argument`` is meant to be used with ``Scheme`` to generate an XML
    definition of the modular input kind that Splunk understands.
    ``name`` is the only required parameter for the constructor.
        **Example with least parameters**::
            arg1 = Argument(name=""arg1"")
        **Example with all parameters**::
            arg2 = Argument(
                name=""arg2"",
                description=""This is an argument with lots of parameters"",
                validation=""is_pos_int('some_name')"",
                data_type=Argument.data_type_number,
                required_on_edit=True,
                required_on_create=True
            )
    '''
    def __init__(self, name, description=None, validation=None, data_type=data_type_string, required_on_edit=False, required_on_create=False, title=None):
        
        pass

    def add_to_document(self, parent):
        
        pass",snippet_83,3,"import xml.etree.ElementTree as ET

class Argument:
    '''Class representing an argument to a modular input kind.
    ``Argument`` is meant to be used with ``Scheme`` to generate an XML
    definition of the modular input kind that Splunk understands.
    ``name`` is the only required parameter for the constructor.
        **Example with least parameters**::
            arg1 = Argument(name=""arg1"")
        **Example with all parameters**::
            arg2 = Argument(
                name=""arg2"",
                description=""This is an argument with lots of parameters"",
                validation=""is_pos_int('some_name')"",
                data_type=Argument.data_type_number,
                required_on_edit=True,
                required_on_create=True
            )
    '''

    data_type_string = 'string'
    data_type_number = 'number'
    data_type_boolean = 'boolean'

    def __init__(self, name, description=None, validation=None, data_type=data_type_string, required_on_edit=False, required_on_create=False, title=None):
        if not name or not isinstance(name, str):
            raise ValueError(""Argument 'name' must be a non-empty string"")
        self.name = name
        self.description = description
        self.validation = validation
        self.data_type = data_type or self.data_type_string
        self.required_on_edit = bool(required_on_edit)
        self.required_on_create = bool(required_on_create)
        self.title = title

    def add_to_document(self, parent):
        arg_elem = ET.SubElement(parent, 'arg', {'name': self.name})

        if self.title is not None:
            title_el = ET.SubElement(arg_elem, 'title')
            title_el.text = str(self.title)

        if self.description is not None:
            desc_el = ET.SubElement(arg_elem, 'description')
            desc_el.text = str(self.description)

        if self.data_type is not None:
            dt_el = ET.SubElement(arg_elem, 'data_type')
            dt_el.text = str(self.data_type)

        if self.validation is not None:
            val_el = ET.SubElement(arg_elem, 'validation')
            val_el.text = str(self.validation)

        roe_el = ET.SubElement(arg_elem, 'required_on_edit')
        roe_el.text = 'true' if self.required_on_edit else 'false'

        roc_el = ET.SubElement(arg_elem, 'required_on_create')
        roc_el.text = 'true' if self.required_on_create else 'false'

        return arg_elem"
160367,splunk/splunk-sdk-python,splunk_splunk-sdk-python/splunklib/modularinput/scheme.py,splunklib.modularinput.scheme.Scheme,"import xml.etree.ElementTree as ET

class Scheme:
    """"""Class representing the metadata for a modular input kind.

    A ``Scheme`` specifies a title, description, several options of how Splunk should run modular inputs of this
    kind, and a set of arguments which define a particular modular input's properties.

    The primary use of ``Scheme`` is to abstract away the construction of XML to feed to Splunk.
    """"""
    streaming_mode_simple = 'SIMPLE'
    streaming_mode_xml = 'XML'

    def __init__(self, title):
        """"""
        :param title: ``string`` identifier for this Scheme in Splunk.
        """"""
        self.title = title
        self.description = None
        self.use_external_validation = True
        self.use_single_instance = False
        self.streaming_mode = Scheme.streaming_mode_xml
        self.arguments = []

    def add_argument(self, arg):
        """"""Add the provided argument, ``arg``, to the ``self.arguments`` list.

        :param arg: An ``Argument`` object to add to ``self.arguments``.
        """"""
        self.arguments.append(arg)

    def to_xml(self):
        """"""Creates an ``ET.Element`` representing self, then returns it.

        :returns: an ``ET.Element`` representing this scheme.
        """"""
        root = ET.Element('scheme')
        ET.SubElement(root, 'title').text = self.title
        if self.description is not None:
            ET.SubElement(root, 'description').text = self.description
        subelements = [('use_external_validation', self.use_external_validation), ('use_single_instance', self.use_single_instance), ('streaming_mode', self.streaming_mode)]
        for name, value in subelements:
            ET.SubElement(root, name).text = str(value).lower()
        endpoint = ET.SubElement(root, 'endpoint')
        args = ET.SubElement(endpoint, 'args')
        for arg in self.arguments:
            arg.add_to_document(args)
        return root","
class Scheme:
    
    def __init__(self, title):
        
        pass
    def add_argument(self, arg):
        
        pass

    def to_xml(self):
        
        pass",snippet_84,4,"class Scheme:
    
    def __init__(self, title):
        if title is None:
            raise ValueError(""title cannot be None"")
        self.title = str(title)
        self._arguments = []

    def add_argument(self, arg):
        if arg is None:
            raise ValueError(""arg cannot be None"")
        self._arguments.append(arg)

    def to_xml(self):
        def _escape(text):
            s = str(text)
            s = s.replace(""&"", ""&amp;"")
            s = s.replace(""<"", ""&lt;"")
            s = s.replace("">"", ""&gt;"")
            s = s.replace('""', ""&quot;"")
            s = s.replace(""'"", ""&apos;"")
            return s

        lines = []
        lines.append(f'<scheme title=""{_escape(self.title)}"">')
        for arg in self._arguments:
            if hasattr(arg, ""to_xml"") and callable(getattr(arg, ""to_xml"")):
                xml = arg.to_xml()
                lines.append(xml if xml is not None else ""<argument/>"")
            else:
                lines.append(f""  <argument>{_escape(arg)}</argument>"")
        lines.append(""</scheme>"")
        return ""\n"".join(lines)"
160991,ncclient/ncclient,ncclient_ncclient/ncclient/transport/session.py,ncclient.transport.session.SessionListener,"class SessionListener:
    """"""Base class for :class:`Session` listeners, which are notified when a new
    NETCONF message is received or an error occurs.

    .. note::
        Avoid time-intensive tasks in a callback's context.
    """"""

    def callback(self, root, raw):
        """"""Called when a new XML document is received. The *root* argument allows the callback to determine whether it wants to further process the document.

        Here, *root* is a tuple of *(tag, attributes)* where *tag* is the qualified name of the root element and *attributes* is a dictionary of its attributes (also qualified names).

        *raw* will contain the XML document as a string.
        """"""
        raise NotImplementedError

    def errback(self, ex):
        """"""Called when an error occurs.

        :type ex: :exc:`Exception`
        """"""
        raise NotImplementedError","class SessionListener:
    
    def callback(self, root, raw):
        
        pass
    def errback(self, ex):
        
        pass",snippet_85,3,"import time
import traceback


class SessionListener:
    def __init__(self):
        self.events = []
        self.errors = []

    def callback(self, root, raw):
        event = {
            ""timestamp"": time.time(),
            ""root"": root,
            ""raw"": raw,
        }
        self.events.append(event)
        return event

    def errback(self, ex):
        if isinstance(ex, BaseException):
            error_text = """".join(traceback.format_exception(type(ex), ex, ex.__traceback__))
        else:
            error_text = str(ex)
        error_entry = {
            ""timestamp"": time.time(),
            ""error"": error_text,
            ""exception"": ex,
        }
        self.errors.append(error_entry)
        return error_entry"
163149,python-cmd2/cmd2,python-cmd2_cmd2/cmd2/argparse_custom.py,cmd2.argparse_custom.Cmd2AttributeWrapper,"from typing import TYPE_CHECKING, Any, ClassVar, NoReturn, Protocol, cast, runtime_checkable

class Cmd2AttributeWrapper:
    """"""Wraps a cmd2-specific attribute added to an argparse Namespace.

    This makes it easy to know which attributes in a Namespace are
    arguments from a parser and which were added by cmd2.
    """"""

    def __init__(self, attribute: Any) -> None:
        """"""Initialize Cmd2AttributeWrapper instances.""""""
        self.__attribute = attribute

    def get(self) -> Any:
        """"""Get the value of the attribute.""""""
        return self.__attribute

    def set(self, new_val: Any) -> None:
        """"""Set the value of the attribute.""""""
        self.__attribute = new_val","
class Cmd2AttributeWrapper:
    
    def __init__(self, attribute: Any) -> None:
        
        pass
    def get(self) -> Any:
        
        pass

    def set(self, new_val: Any) -> None:
        
        pass",snippet_86,4,"from typing import Any

class Cmd2AttributeWrapper:
    
    def __init__(self, attribute: Any) -> None:
        self._attribute = attribute

    def get(self) -> Any:
        return self._attribute

    def set(self, new_val: Any) -> None:
        self._attribute = new_val"
163184,python-cmd2/cmd2,python-cmd2_cmd2/cmd2/utils.py,cmd2.utils.ContextFlag,"class ContextFlag:
    """"""A context manager which is also used as a boolean flag value within the default sigint handler.

    Its main use is as a flag to prevent the SIGINT handler in cmd2 from raising a KeyboardInterrupt
    while a critical code section has set the flag to True. Because signal handling is always done on the
    main thread, this class is not thread-safe since there is no need.
    """"""

    def __init__(self) -> None:
        """"""When this flag has a positive value, it is considered set. When it is 0, it is not set.

        It should never go below 0.
        """"""
        self.__count = 0

    def __bool__(self) -> bool:
        """"""Define the truth value of an object when it is used in a boolean context.""""""
        return self.__count > 0

    def __enter__(self) -> None:
        """"""When a with block is entered, the __enter__ method of the context manager is called.""""""
        self.__count += 1

    def __exit__(self, *args: object) -> None:
        """"""When the execution flow exits a with statement block this is called, regardless of whether an exception occurred.""""""
        self.__count -= 1
        if self.__count < 0:
            raise ValueError('count has gone below 0')","class ContextFlag:
    
    def __init__(self) -> None:
        
        pass
    def __bool__(self) -> bool:
        
        pass
    def __enter__(self) -> None:
        
        pass

    def __exit__(self, *args: object) -> None:
        
        pass",snippet_87,5,"class ContextFlag:
    
    def __init__(self) -> None:
        self._depth = 0

    def __bool__(self) -> bool:
        return self._depth > 0

    def __enter__(self) -> None:
        self._depth += 1
        return None

    def __exit__(self, *args: object) -> None:
        if self._depth > 0:
            self._depth -= 1
        return None"
166548,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/cache/cache.py,pypyr.cache.cache.Cache,"import threading
from pypyr.config import config

class Cache:
    """"""Thread-safe general purpose cache for objects.

    Add things to the cache by calling get(key, creator). If the requested key
    doesn't exist, will add the item to the cache for you.
    """"""

    def __init__(self):
        """"""Instantiate the cache.""""""
        self._lock = threading.Lock()
        self._cache = {}

    def clear(self):
        """"""Clear the cache of all objects.""""""
        with self._lock:
            self._cache.clear()

    def get(self, key, creator):
        """"""Get key from cache. If key not exist, call creator and cache result.

        Looks for key in cache and returns object for that key.

        If key is not found, call creator and save the result to cache for that
        key.

        Be warned that get happens under the context of a Lock. . . so if
        creator takes a long time you might well be blocking.

        If config no_cache is True, bypasses cache entirely - will call
        creator each time and also not save the result to cache.

        Args:
            key: key (unique id) of cached item
            creator: callable that will create cached object if key not found

        Returns:
            Cached item at key or the result of creator()
        """"""
        if config.no_cache:
            logger.debug('no cache mode enabled. creating `%s` sans cache', key)
            return creator()
        with self._lock:
            if key in self._cache:
                logger.debug('`%s` loading from cache', key)
                obj = self._cache[key]
            else:
                logger.debug('`%s` not found in cache. . . creating', key)
                obj = creator()
                self._cache[key] = obj
        return obj","
class Cache:
    
    def __init__(self):
        
        pass
    def clear(self):
        
        pass

    def get(self, key, creator):
        
        pass",snippet_88,4,"class Cache:
    def __init__(self):
        import threading
        self._cache = {}
        self._lock = threading.RLock()
        self._key_locks = {}

    def clear(self):
        with self._lock:
            self._cache.clear()
            self._key_locks.clear()

    def get(self, key, creator):
        with self._lock:
            if key in self._cache:
                return self._cache[key]
            lock = self._key_locks.get(key)
            if lock is None:
                import threading
                lock = threading.Lock()
                self._key_locks[key] = lock

        with lock:
            with self._lock:
                if key in self._cache:
                    return self._cache[key]
            value = creator()
            with self._lock:
                self._cache[key] = value
                # Optional: cleanup per-key lock to avoid growth
                self._key_locks.pop(key, None)
            return value"
166549,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/cache/loadercache.py,pypyr.cache.loadercache.Loader,"from pypyr.pipedef import PipelineDefinition, PipelineInfo
from pypyr.cache.cache import Cache
from collections.abc import Mapping
from pypyr.errors import PipelineDefinitionError

class Loader:
    """"""A single pipeline loader & the cache for all pipelines it has loaded.

    It loads pipelines using the get_pipeline_definition you assign to the
    loader at initialization.

    Attributes:
        name (str): Absolute module name of loader.
    """"""
    __slots__ = ['name', '_get_pipeline_definition', '_pipeline_cache']

    def __init__(self, name, get_pipeline_definition):
        """"""Initialize the loader and its pipeline cache.

        The expected function signature is:
        get_pipeline_definition(name: str,
                                parent: any) -> PipelineDefinition | Mapping

        Args:
            name: Absolute name of loader
            get_pipeline_definition: Reference to the function to call when
                loading a pipeline with this Loader.
        """"""
        self.name = name
        self._get_pipeline_definition = get_pipeline_definition
        self._pipeline_cache = Cache()

    def clear(self):
        """"""Clear all the pipelines in this Loader's cache.""""""
        self._pipeline_cache.clear()

    def get_pipeline(self, name, parent):
        """"""Get cached PipelineDefinition. Adds it to cache if it doesn't exist.

        The cache is local to this Loader instance.

        The combination of parent+name must be unique for this Loader. Parent
        should therefore have a sensible __str__ implementation because it
        forms part of the pipeline's identifying str key in the cache.

        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.

        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        """"""
        normalized_name = f'{parent}+{name}' if parent else name
        return self._pipeline_cache.get(normalized_name, lambda: self._load_pipeline(name, parent))

    def _load_pipeline(self, name, parent):
        """"""Execute get_pipeline_definition(name, parent) for this loader.

        If the loader get_pipeline_definition does not return a
        PipelineDefinition, this method will wrap the payload inside a
        PipelineDefinition for you.

        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.

        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        """"""
        logger.debug('starting')
        logger.debug('loading the pipeline definition with %s', self.name)
        pipeline_definition = self._get_pipeline_definition(pipeline_name=name, parent=parent)
        if not isinstance(pipeline_definition, PipelineDefinition):
            pipeline_definition = PipelineDefinition(pipeline=pipeline_definition, info=PipelineInfo(pipeline_name=name, loader=self.name, parent=parent))
        if not isinstance(pipeline_definition.pipeline, Mapping):
            raise PipelineDefinitionError(""A pipeline must be a mapping at the top level. Does your top-level yaml have a 'steps:' key? For example:\n\nsteps:\n  - name: pypyr.steps.echo\n    in:\n      echoMe: this is a bare bones pipeline example.\n"")
        logger.debug('done')
        return pipeline_definition","
class Loader:
    
    def __init__(self, name, get_pipeline_definition):
        
        pass
    def clear(self):
        
        pass

    def get_pipeline(self, name, parent):
        
        pass

    def _load_pipeline(self, name, parent):
        
        pass",snippet_89,5,"class Loader:
    
    def __init__(self, name, get_pipeline_definition):
        if not callable(get_pipeline_definition):
            raise TypeError(""get_pipeline_definition must be callable"")
        self.name = name
        self._get_pipeline_definition = get_pipeline_definition
        self._cache = {}
        try:
            from threading import RLock
        except Exception:
            self._lock = None
        else:
            self._lock = RLock()

    def clear(self):
        if self._lock:
            with self._lock:
                self._cache.clear()
        else:
            self._cache.clear()

    def get_pipeline(self, name, parent):
        key = (parent, name)
        if self._lock:
            with self._lock:
                if key in self._cache:
                    return self._cache[key]
                pipeline = self._load_pipeline(name, parent)
                self._cache[key] = pipeline
                return pipeline
        else:
            if key in self._cache:
                return self._cache[key]
            pipeline = self._load_pipeline(name, parent)
            self._cache[key] = pipeline
            return pipeline

    def _load_pipeline(self, name, parent):
        pipeline = self._get_pipeline_definition(name, parent)
        if pipeline is None:
            raise ValueError(f""Pipeline '{name}' could not be loaded by loader '{self.name}'"")
        return pipeline"
166587,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/pipedef.py,pypyr.pipedef.PipelineDefinition,"class PipelineDefinition:
    """"""The pipeline body and its metadata.

    A loader creates the PipelineDefinition and sets the metadata in .info.

    The PipelineDefinition is a globally shared cache of the pipeline body &
    meta-data.

    Attributes:
        pipeline (dict-like): The pipeline yaml body.
        info (PipelineInfo): Meta-data set by the loader for the pipeline.
    """"""
    __slots__ = ['pipeline', 'info']

    def __init__(self, pipeline, info):
        """"""Initialize a pipeline definition.

        Args:
            pipeline (dict-like): The pipeline yaml body itself.
            info (PipelineInfo): Meta-data set by the loader for the pipeline.
        """"""
        self.pipeline = pipeline
        self.info = info

    def __eq__(self, other):
        """"""Equality comparison checks Pipeline and info objects are equal.""""""
        type_self = type(self)
        if type_self is type(other):
            all_slots = [p for c in type_self.__mro__ for p in getattr(c, '__slots__', [])]
            return all((getattr(self, s, id(self)) == getattr(other, s, id(other)) for s in all_slots))
        else:
            return False","class PipelineDefinition:
    
    def __init__(self, pipeline, info):
        
        pass
    def __eq__(self, other):
        
        pass",snippet_92,3,"class PipelineDefinition:
    
    def __init__(self, pipeline, info):
        self.pipeline = pipeline
        self.info = info

    def __eq__(self, other):
        if self is other:
            return True
        if not isinstance(other, PipelineDefinition):
            return NotImplemented
        return self.pipeline == other.pipeline and self.info == other.info"
166589,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/pipedef.py,pypyr.pipedef.PipelineInfo,"class PipelineInfo:
    """"""The common attributes that every pipeline loader should set.

    Custom loaders that want to add more properties to a pipeline's meta-data
    should probably derive from this class.

    Attributes:
        pipeline_name (str): Name of pipeline, as set by the loader.
        loader (str): Absolute module name of the pipeline loader.
        parent (any): pipeline_name resolves from parent. The parent can be any
            type - it is up to the loader to interpret the parent property.
        is_loader_cascading (bool): Loader cascades to child pipelines if not
            otherwise set on pype. Default True.
        is_parent_cascading (bool): Parent cascades to child pipelines if not
            otherwise set on pype. Default True.
    """"""
    __slots__ = ['pipeline_name', 'loader', 'parent', 'is_loader_cascading', 'is_parent_cascading']

    def __init__(self, pipeline_name, loader, parent, is_parent_cascading=True, is_loader_cascading=True):
        """"""Initialize PipelineInfo.

        Args:
            pipeline_name (str): name of pipeline, as set by the loader.
            loader (str): absolute module name of pypeloader.
            parent (any): pipeline_name resolves from parent.
            is_loader_cascading (bool): Loader cascades to child pipelines if
                not otherwise set on pype. Default True.
            is_parent_cascading (bool): Parent cascades to child pipelines if
                not otherwise set on pype. Default True.
        """"""
        self.pipeline_name = pipeline_name
        self.loader = loader
        self.parent = parent
        self.is_loader_cascading = is_loader_cascading
        self.is_parent_cascading = is_parent_cascading

    def __eq__(self, other):
        """"""Check all instance attributes are equal.""""""
        type_self = type(self)
        if type_self is type(other):
            all_slots = [p for c in type_self.__mro__ for p in getattr(c, '__slots__', [])]
            return all((getattr(self, s, id(self)) == getattr(other, s, id(other)) for s in all_slots))
        else:
            return False","class PipelineInfo:
    
    def __init__(self, pipeline_name, loader, parent, is_parent_cascading=True, is_loader_cascading=True):
        
        pass
    def __eq__(self, other):
        
        pass",snippet_93,3,"class PipelineInfo:
    
    def __init__(self, pipeline_name, loader, parent, is_parent_cascading=True, is_loader_cascading=True):
        self.pipeline_name = pipeline_name
        self.loader = loader
        self.parent = parent
        self.is_parent_cascading = is_parent_cascading
        self.is_loader_cascading = is_loader_cascading

    def __eq__(self, other):
        if not isinstance(other, PipelineInfo):
            return NotImplemented
        return (
            self.pipeline_name == other.pipeline_name and
            self.loader == other.loader and
            self.parent == other.parent and
            self.is_parent_cascading == other.is_parent_cascading and
            self.is_loader_cascading == other.is_loader_cascading
        )"
166604,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/steps/dsl/cmd.py,pypyr.steps.dsl.cmd.CmdStep,"import pypyr.utils.types
from pypyr.errors import ContextError, KeyInContextHasNoValueError, KeyNotInContextError
from collections.abc import Mapping, Sequence
from pypyr.context import Context
import logging
from pypyr.subproc import Command, SimpleCommandTypes

class CmdStep:
    """"""A pypyr step to run an executable or command as a subprocess.

    This models a step that takes config like this:
        cmd: <<cmd string>>

    OR, expanded syntax is as a dict
        cmd:
            run: str. mandatory. command + args to execute.
            save: bool. defaults False. save output to cmdOut. Treats output
                as text in the system's encoding and removes newlines at end.
            cwd: str/Pathlike. optional. Working directory for this command.
            bytes (bool): Default False. When `save` return output bytes from
                cmd unaltered, without applying any encoding & text newline
                processing.
            encoding (str): Default None. When `save`, decode cmd output with
                this encoding. The default of None uses the system encoding and
                should ""just work"".
            stdout (str | Path): Default None. Write stdout to this file path.
                Special value `/dev/null` writes to the system null device.
            stderr (str | Path): Default None. Write stderr to this file path.
                Special value `/dev/null` writes to the system null device.
                Special value `/dev/stdout` redirects err output to stdout.
            append (bool): Default False. When stdout/stderr a file, append
                rather than overwrite. Default is to overwrite.

    In expanded syntax, `run` can be a simple string or a list:
        cmd:
          run:
            - my-executable --arg
            - cmd here
          save: False
          cwd: ./path/here

    OR, as a list in simplified syntax:
        cmd:
          - my-executable --arg
          - ./another-executable --arg

    Any or all of the list items can use expanded syntax:
        cmd:
          - ./simple-cmd-here --arg1 value
          - run: cmd here
            save: False
            cwd: ./path/here
          - run:
              - my-executable --arg
              - ./another-executable --arg
            save: True cwd: ./path/here

    If save is True, will save the output to context as follows:
        cmdOut:
            returncode: 0
            stdout: 'stdout str here. None if empty.'
            stderr: 'stderr str here. None if empty.'

    If the cmd input contains a list of executables, cmdOut will be a list of
    cmdOut objects, in order executed.

    cmdOut.returncode is the exit status of the called process. Typically 0
    means OK. A negative value -N indicates that the child was terminated by
    signal N (POSIX only).

    The run_step method does the actual work. init parses the input yaml.

    Attributes:
        logger (logger): Logger instantiated by name of calling step.
        context: (pypyr.context.Context): The current pypyr Context.
        commands (list[pypyr.subproc.Command]): Commands to run as subprocess.
        is_shell (bool): True if subprocess should run through default shell.
        name (str): Name of calling step. Used for logging output & error
            messages.
    """"""

    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        """"""Initialize the CmdStep.

        The step config in the context dict in simplified syntax:
            cmd: <<cmd string>>

        OR, as a dict in expanded syntax:
            cmd:
                run: str. mandatory. command + args to execute.
                save: bool. optional. defaults False. save output to cmdOut.
                cwd: str/path. optional. if specified, change the working
                     directory just for the duration of the command.

        `run` can be a single string, or it can be a list of string if there
        are multiple commands to execute with the same settings.

        OR, as a list:
            cmd:
                - my-executable --arg
                - ./another-executable --arg

        Any or all of the list items can be in expanded syntax.

        Args:
            name (str): Unique name for step. Likely __name__ of calling step.
            context (pypyr.context.Context): Look for step config in this
                context instance.
            is_shell (bool): Set to true to execute cmd through the default
                shell.
        """"""
        assert name, 'name parameter must exist for CmdStep.'
        assert context, 'context param must exist for CmdStep.'
        self.name = name
        self.logger = logging.getLogger(name)
        context.assert_key_has_value(key='cmd', caller=name)
        self.context = context
        self.is_shell = is_shell
        cmd_config = context.get_formatted('cmd')
        commands: list[Command] = []
        if isinstance(cmd_config, SimpleCommandTypes):
            commands.append(Command(cmd_config, is_shell=is_shell))
        elif isinstance(cmd_config, Mapping):
            commands.append(self.create_command(cmd_config))
        elif isinstance(cmd_config, Sequence):
            for cmd in cmd_config:
                if isinstance(cmd, SimpleCommandTypes):
                    commands.append(Command(cmd, is_shell=is_shell))
                elif isinstance(cmd, Mapping):
                    commands.append(self.create_command(cmd))
                else:
                    raise ContextError(f'{cmd} in {name} cmd config is wrong.\nEach list item should be either a simple string or a dict for expanded syntax:\ncmd:\n  - my-executable --arg\n  - run: another-executable --arg value\n    cwd: ../mydir/subdir\n  - run:\n      - arb-executable1 --arg value1\n      - arb-executable2 --arg value2\n    cwd: ../mydir/arbdir')
        else:
            raise ContextError(f'{name} cmd config should be either a simple string:\ncmd: my-executable --arg\n\nor a dictionary:\ncmd:\n  run: subdir/my-executable --arg\n  cwd: ./mydir\n\nor a list of commands:\ncmd:\n  - my-executable --arg\n  - run: another-executable --arg value\n    cwd: ../mydir/subdir')
        self.commands: list[Command] = commands

    def create_command(self, cmd_input: Mapping) -> Command:
        """"""Create a pypyr.subproc.Command object from expanded step input.""""""
        try:
            cmd = cmd_input['run']
            if not cmd:
                raise KeyInContextHasNoValueError(f'cmd.run must have a value for {self.name}.\nThe `run` input should look something like this:\ncmd:\n  run: my-executable-here --arg1\n  cwd: ./mydir/subdir\n\nOr, `run` could be a list of commands:\ncmd:\n  run:\n    - arb-executable1 --arg value1\n    - arb-executable2 --arg value2\n  cwd: ../mydir/arbdir')
        except KeyError as err:
            raise KeyNotInContextError(f""cmd.run doesn't exist for {self.name}.\nThe input should look like this in the simplified syntax:\ncmd: my-executable-here --arg1\n\nOr in the expanded syntax:\ncmd:\n  run: my-executable-here --arg1\n\nIf you're passing in a list of commands, each command should be a simple string,\nor a dict with a `run` entry:\ncmd:\n  - my-executable --arg\n  - run: another-executable --arg value\n    cwd: ../mydir/subdir\n  - run:\n      - arb-executable1 --arg value1\n      - arb-executable2 --arg value2\n    cwd: ../mydir/arbdir"") from err
        is_save = pypyr.utils.types.cast_to_bool(cmd_input.get('save', False))
        cwd = cmd_input.get('cwd')
        is_bytes = cmd_input.get('bytes')
        is_text = not is_bytes if is_save else False
        stdout = cmd_input.get('stdout')
        stderr = cmd_input.get('stderr')
        if is_save:
            if stderr or stderr:
                raise ContextError(""You can't set `stdout` or `stderr` when `save` is True."")
        encoding = cmd_input.get('encoding')
        append = cmd_input.get('append', False)
        is_shell_override = cmd_input.get('shell', None)
        is_shell = self.is_shell if is_shell_override is None else is_shell_override
        return Command(cmd=cmd, is_shell=is_shell, cwd=cwd, is_save=is_save, is_text=is_text, stdout=stdout, stderr=stderr, encoding=encoding, append=append)

    def run_step(self) -> None:
        """"""Spawn a subprocess to run the command or program.

        If cmd.is_save==True, save result of each command to context 'cmdOut'.
        """"""
        results = []
        try:
            for cmd in self.commands:
                try:
                    cmd.run()
                finally:
                    if cmd.results:
                        results.extend(cmd.results)
        finally:
            if results:
                if len(results) == 1:
                    self.context['cmdOut'] = results[0]
                else:
                    self.context['cmdOut'] = results","
class CmdStep:
    
    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        
        pass
    def create_command(self, cmd_input: Mapping) -> Command:
        
        pass

    def run_step(self) -> None:
        
        pass",snippet_94,4,"from typing import Mapping, Any

class CmdStep:
    
    def __init__(self, name: str, context: ""Context"", is_shell: bool=False) -> None:
        if not isinstance(name, str) or not name:
            raise ValueError(""name must be a non-empty string"")
        if context is None:
            raise ValueError(""context must not be None"")
        self.name = name
        self.context = context
        self.is_shell = bool(is_shell)

    def create_command(self, cmd_input: Mapping) -> ""Command"":
        if not isinstance(cmd_input, Mapping):
            raise TypeError(""cmd_input must be a Mapping"")
        # Prefer a direct factory on context
        if hasattr(self.context, ""create_command"") and callable(getattr(self.context, ""create_command"")):
            return self.context.create_command(cmd_input, shell=self.is_shell)
        # Try a generic factory attribute
        factory = getattr(self.context, ""command_factory"", None)
        if callable(factory):
            return factory(cmd_input, shell=self.is_shell)
        # Try a Command class on context
        command_cls = getattr(self.context, ""Command"", None)
        if command_cls is not None:
            return command_cls(cmd_input, shell=self.is_shell)
        raise RuntimeError(""No available way to create a Command from the provided context"")

    def run_step(self) -> None:
        # Retrieve input for this step from context
        if hasattr(self.context, ""get_step_input"") and callable(getattr(self.context, ""get_step_input"")):
            cmd_input = self.context.get_step_input(self.name)
        elif hasattr(self.context, ""get"") and callable(getattr(self.context, ""get"")):
            cmd_input = self.context.get(self.name)
        elif hasattr(self.context, ""inputs"") and isinstance(getattr(self.context, ""inputs""), Mapping):
            cmd_input = self.context.inputs.get(self.name)
        else:
            raise RuntimeError(""Context does not provide step input accessors"")
        if cmd_input is None:
            raise ValueError(f""No command input found in context for step '{self.name}'"")

        cmd = self.create_command(cmd_input)

        # Execute the command
        if hasattr(cmd, ""run"") and callable(getattr(cmd, ""run"")):
            result = cmd.run()
        elif hasattr(cmd, ""execute"") and callable(getattr(cmd, ""execute"")):
            result = cmd.execute()
        else:
            raise RuntimeError(""Command object does not provide a run/execute method"")

        # Store result back into context (best-effort)
        if hasattr(self.context, ""set_step_result"") and callable(getattr(self.context, ""set_step_result"")):
            self.context.set_step_result(self.name, result)
        elif hasattr(self.context, ""set"") and callable(getattr(self.context, ""set"")):
            self.context.set(f""{self.name}.result"", result)
        elif hasattr(self.context, ""results"") and isinstance(getattr(self.context, ""results""), dict):
            self.context.results[self.name] = result
        # If no suitable sink exists, silently do nothing beyond execution"
166605,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/steps/dsl/cmdasync.py,pypyr.steps.dsl.cmdasync.AsyncCmdStep,"import logging
from pypyr.aio.subproc import Command, Commands
from pypyr.context import Context
import pypyr.utils.types
from collections.abc import Mapping, Sequence
from pypyr.errors import ContextError, KeyInContextHasNoValueError, KeyNotInContextError
from pypyr.subproc import SimpleCommandTypes

class AsyncCmdStep:
    """"""A pypyr step to run executables/commands concurrently as a subprocess.

    This models a step that takes config like this in simple syntax:
        cmds:
            - <<cmd string 1>>
            - <<cmd string 2>>

    All the commands will run concurrently, in parallel.

    OR, expanded syntax is as a dict
        cmds:
            run: list[str | list[str]]. mandatory. command + args to execute.
                If list entry is another list[str], the sub-list will run in
                serial.
            save: bool. defaults False. save output to cmdOut. Treats output
                as text in the system's encoding and removes newlines at end.
            cwd: str/Pathlike. optional. Working directory for these commands.
            bytes (bool): Default False. When `save` return output bytes from
                cmds unaltered, without applying any encoding & text newline
                processing.
            encoding (str): Default None. When `save`, decode output with
                this encoding. The default of None uses the system encoding and
                should ""just work"".
            stdout (str | Path): Default None. Write stdout to this file path.
                Special value `/dev/null` writes to the system null device.
            stderr (str | Path): Default None. Write stderr to this file path.
                Special value `/dev/null` writes to the system null device.
                Special value `/dev/stdout` redirects err output to stdout.
            append (bool): Default False. When stdout/stderr a file, append
                rather than overwrite. Default is to overwrite.

    In expanded syntax, `run` can be a simple string or a list:
        cmds:
          run:
            - ./my-executable --arg
            - [./another-executable --arg, ./arb-executable arghere]
          save: False
          cwd: ./path/here

    As a list in simplified syntax:
        cmds:
          - my-executable --arg
          - ./another-executable --arg

    Any or all of the list items can use expanded syntax:
        cmds:
          - ./simple-cmd-here --arg1 value
          - run: cmd here
            save: False cwd: ./path/here
          - run:
              - my-executable --arg
              - ./another-executable --arg
            save: True
            cwd: ./path/here

    Any of the list items can in turn be a list. A sub-list will run in serial.

    In this example A, B.1 & C will start concurrently. B.2 will only run once
    B.1 is finished.

        cmds:
            - A
            - [B.1, B.2]
            - C

    If save is True, will save the output to context as cmdOut.

    cmdOut will be a list of pypyr.subproc.SubprocessResult objects, in order
    executed.

    SubprocessResult has the following properties:
    cmd: the cmd/args executed
    returncode: 0
    stdout: 'stdout str here. None if empty.'
    stderr: 'stderr str here. None if empty.'

    cmdOut.returncode is the exit status of the called process. Typically 0
    means OK. A negative value -N indicates that the child was terminated by
    signal N (POSIX only).

    The run_step method does the actual work. init parses the input yaml.

    Attributes:
        logger (logger): Logger instantiated by name of calling step.
        context: (pypyr.context.Context): The current pypyr Context.
        commands (pypyr.subproc.Commands): Commands to run as subprocess.
        is_shell (bool): True if subprocess should run through default shell.
        name (str): Name of calling step. Used for logging output & error
            messages.
    """"""

    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        """"""Initialize the CmdStep.

        The step config in the context dict in simplified syntax:
            cmd: <<cmd string>>

        OR, as a dict in expanded syntax:
            cmd:
                run: str. mandatory. command + args to execute.
                save: bool. optional. defaults False. save output to cmdOut.
                cwd: str/path. optional. if specified, change the working
                     directory just for the duration of the command.

        `run` can be a single string, or it can be a list of string if there
        are multiple commands to execute with the same settings.

        OR, as a list:
            cmd:
                - my-executable --arg
                - ./another-executable --arg

        Any or all of the list items can be in expanded syntax.

        Args:
            name (str): Unique name for step. Likely __name__ of calling step.
            context (pypyr.context.Context): Look for step config in this
                context instance.
            is_shell (bool): Set to true to execute cmd through the default
                shell.
        """"""
        assert name, 'name parameter must exist for CmdStep.'
        assert context, 'context param must exist for CmdStep.'
        self.name = name
        self.logger = logging.getLogger(name)
        context.assert_key_has_value(key='cmds', caller=name)
        self.context = context
        self.is_shell = is_shell
        cmd_config = context.get_formatted('cmds')
        commands = Commands()
        if isinstance(cmd_config, SimpleCommandTypes):
            commands.append(Command(cmd_config, is_shell=is_shell))
        elif isinstance(cmd_config, Mapping):
            commands.append(self.create_command(cmd_config))
        elif isinstance(cmd_config, Sequence):
            for cmd in cmd_config:
                if isinstance(cmd, SimpleCommandTypes):
                    commands.append(Command(cmd, is_shell=is_shell))
                elif isinstance(cmd, Sequence):
                    commands.append(Command([cmd], is_shell=is_shell))
                elif isinstance(cmd, Mapping):
                    commands.append(self.create_command(cmd))
                else:
                    raise ContextError(f'{cmd} in {name} cmds config is wrong.\nEach list item should be either a simple string, or a list to run in serial,\nor a dict for expanded syntax:\ncmds:\n  - ./my-executable --arg\n  - run:\n      - ./another-executable --arg value\n      - ./another-executable --arg value2\n    cwd: ../mydir/subdir\n  - run:\n      - ./arb-executable1 --arg value1\n      - [./arb-executable2.1, ./arb-executable2.2]\n    cwd: ../mydir/arbdir\n  - [./arb-executable3.1, ./arb-executable3.2]')
        else:
            raise ContextError(f'{name} cmds config should be either a list:\ncmds:\n  - ./my-executable --arg\n  - subdir/executable --arg1\n\nor a dictionary with a `run` sub-key:\ncmds:\n  run:\n    - ./my-executable --arg\n    - subdir/executable --arg1\n  cwd: ./mydir\n\nAny of the list items in root can be in expanded syntax:\ncmds:\n  - ./my-executable --arg\n  - subdir/executable --arg1\n  - run:\n      - ./arb-executable1 --arg value1\n      - [./arb-executable2.1, ./arb-executable2.2]\n    cwd: ../mydir/subdir\n  - [./arb-executable3.1, ./arb-executable3.2]')
        self.commands: Commands = commands

    def create_command(self, cmd_input: Mapping) -> Command:
        """"""Create pypyr.aio.subproc.Command object from expanded step input.""""""
        try:
            cmd = cmd_input['run']
            if not cmd:
                raise KeyInContextHasNoValueError(f'cmds.run must have a value for {self.name}.\nThe `run` input should look something like this:\ncmds:\n  run:\n    - ./arb-executable1 --arg value1\n    - ./arb-executable2 --arg value2\n  cwd: ../mydir/arbdir')
        except KeyError as err:
            raise KeyNotInContextError(f""cmds.run doesn't exist for {self.name}.\nThe input should look like this in expanded syntax:\ncmds:\n  run:\n    - ./my-executable --arg\n    - subdir/executable --arg1\n  cwd: ./mydir\n\nIf you're passing in a list of commands, each command should be a simple string,\nor a sub-list of commands to run in serial,\nor a dict with a `run` entry:\ncmds:\n  - ./my-executable --arg\n  - run: ./another-executable --arg value\n    cwd: ../mydir/subdir\n  - run:\n      - ./arb-executable1 --arg value1\n      - [./arb-executable2.1, ./arb-executable2.2]\n    cwd: ../mydir/arbdir\n  - [./arb-executable3.1, ./arb-executable3.2]"") from err
        is_save = pypyr.utils.types.cast_to_bool(cmd_input.get('save', False))
        cwd = cmd_input.get('cwd')
        is_bytes = cmd_input.get('bytes')
        is_text = not is_bytes if is_save else False
        stdout = cmd_input.get('stdout')
        stderr = cmd_input.get('stderr')
        if is_save:
            if stderr or stderr:
                raise ContextError(""You can't set `stdout` or `stderr` when `save` is True."")
        encoding = cmd_input.get('encoding')
        append = cmd_input.get('append', False)
        is_shell_override = cmd_input.get('shell', None)
        is_shell = self.is_shell if is_shell_override is None else is_shell_override
        return Command(cmd=cmd, is_shell=is_shell, cwd=cwd, is_save=is_save, is_text=is_text, stdout=stdout, stderr=stderr, encoding=encoding, append=append)

    def run_step(self) -> None:
        """"""Spawn subprocesses to run the commands asynchronously.

        If cmd.is_save==True, save aggregate result of all commands to context
        'cmdOut'.

        cmdOut will be a list of pypyr.subproc.SubprocessResult or Exception
        objects, in order executed.

        SubprocessResult has the following properties:
        cmd: the cmd/args executed
        returncode: 0
        stdout: 'stdout str here. None if empty.'
        stderr: 'stderr str here. None if empty.'
        """"""
        try:
            self.commands.run()
        finally:
            if self.commands.is_save:
                self.logger.debug('saving results to cmdOut')
                self.context['cmdOut'] = self.commands.results
            else:
                self.logger.debug('save is False: not saving results to cmdOut')","
class AsyncCmdStep:
    
    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        
        pass
    def create_command(self, cmd_input: Mapping) -> Command:
        
        pass

    def run_step(self) -> None:
        
        pass",snippet_95,4,"from __future__ import annotations

import asyncio
import os
import shlex
import sys
from dataclasses import dataclass
from typing import Any, Mapping, Optional, Sequence, Union


@dataclass
class Command:
    args: Union[str, Sequence[str]]
    shell: bool = False
    cwd: Optional[str] = None
    env: Optional[Mapping[str, str]] = None
    timeout: Optional[float] = None
    stdin_data: Optional[bytes] = None
    capture_output: bool = True

    # Execution results
    returncode: Optional[int] = None
    stdout: Optional[bytes] = None
    stderr: Optional[bytes] = None

    async def execute(self) -> None:
        stdout_pipe = asyncio.subprocess.PIPE if self.capture_output else None
        stderr_pipe = asyncio.subprocess.PIPE if self.capture_output else None
        stdin_pipe = asyncio.subprocess.PIPE if self.stdin_data is not None else None

        # Build environment
        env = None
        if self.env is not None:
            env = os.environ.copy()
            env.update({str(k): str(v) for k, v in self.env.items()})

        if self.shell:
            if isinstance(self.args, (list, tuple)):
                cmd_str = "" "".join(shlex.quote(str(x)) for x in self.args)
            else:
                cmd_str = str(self.args)
            proc = await asyncio.create_subprocess_shell(
                cmd_str,
                stdout=stdout_pipe,
                stderr=stderr_pipe,
                stdin=stdin_pipe,
                cwd=self.cwd,
                env=env,
            )
        else:
            if isinstance(self.args, str):
                # If a single string is given and shell=False, split safely
                argv = shlex.split(self.args)
            else:
                argv = [str(x) for x in self.args]
            proc = await asyncio.create_subprocess_exec(
                *argv,
                stdout=stdout_pipe,
                stderr=stderr_pipe,
                stdin=stdin_pipe,
                cwd=self.cwd,
                env=env,
            )

        try:
            if self.timeout is not None and self.timeout > 0:
                self.stdout, self.stderr = await asyncio.wait_for(
                    proc.communicate(input=self.stdin_data), timeout=self.timeout
                )
            else:
                self.stdout, self.stderr = await proc.communicate(input=self.stdin_data)
        except asyncio.TimeoutError:
            with contextlib.suppress(ProcessLookupError):
                proc.kill()
            await proc.wait()
            self.returncode = -1
            if self.capture_output:
                if self.stdout is None:
                    self.stdout = b""""
                if self.stderr is None:
                    self.stderr = b""""
                self.stderr += b""\nProcess timed out""
            return

        self.returncode = proc.returncode


# Optional import used in Command.execute
import contextlib


class AsyncCmdStep:
    def __init__(self, name: str, context: Any, is_shell: bool = False) -> None:
        self.name = name
        self.context = context
        self.is_shell = is_shell
        self._command: Optional[Command] = None

    def create_command(self, cmd_input: Mapping) -> Command:
        # Extract command/args
        cmd: Union[str, Sequence[str], None] = None
        if ""cmd"" in cmd_input:
            cmd = cmd_input[""cmd""]  # could be str or list
        elif ""args"" in cmd_input:
            cmd = cmd_input[""args""]

        if cmd is None:
            raise ValueError(""cmd_input must include 'cmd' or 'args'"")

        # Determine shell mode
        shell = bool(cmd_input.get(""shell"", self.is_shell))
        # Normalize args
        if shell:
            # shell accepts string; if list is given, join safely
            if isinstance(cmd, (list, tuple)):
                args: Union[str, Sequence[str]] = "" "".join(
                    shlex.quote(str(x)) for x in cmd
                )
            else:
                args = str(cmd)
        else:
            # exec mode prefers a list of strings
            if isinstance(cmd, str):
                args = cmd  # will be split in Command if needed
            else:
                args = [str(x) for x in cmd]

        cwd = cmd_input.get(""cwd"")
        env = cmd_input.get(""env"")
        timeout_val = cmd_input.get(""timeout"")
        timeout: Optional[float] = None
        if timeout_val is not None:
            try:
                timeout = float(timeout_val)
            except (TypeError, ValueError):
                raise ValueError(""timeout must be a number (seconds)"")

        stdin_data = cmd_input.get(""stdin"")
        if isinstance(stdin_data, str):
            stdin_bytes = stdin_data.encode(""utf-8"")
        elif isinstance(stdin_data, (bytes, bytearray)):
            stdin_bytes = bytes(stdin_data)
        elif stdin_data is None:
            stdin_bytes = None
        else:
            raise ValueError(""stdin must be str, bytes, or None"")

        capture_output = bool(cmd_input.get(""capture_output"", True))

        command = Command(
            args=args,
            shell=shell,
            cwd=cwd,
            env=env,
            timeout=timeout,
            stdin_data=stdin_bytes,
            capture_output=capture_output,
        )
        self._command = command
        return command

    def run_step(self) -> None:
        if self._command is None:
            raise RuntimeError(""No command created. Call create_command() first."")

        def log_info(msg: str) -> None:
            logger = getattr(self.context, ""logger"", None)
            if logger and hasattr(logger, ""info""):
                logger.info(msg)
                return
            log_func = getattr(self.context, ""log"", None)
            if callable(log_func):
                log_func(msg)
                return
            # Fallback to stdout
            print(msg, file=sys.stdout)

        def log_error(msg: str) -> None:
            logger = getattr(self.context, ""logger"", None)
            if logger and hasattr(logger, ""error""):
                logger.error(msg)
                return
            log_func = getattr(self.context, ""error"", None)
            if callable(log_func):
                log_func(msg)
                return
            print(msg, file=sys.stderr)

        log_info(f""[{self.name}] Starting command"")
        try:
            asyncio.run(self._command.execute())
        except RuntimeError as e:
            # In case we're already in an event loop (e.g., inside another async framework)
            if ""asyncio.run() cannot be called from a running event loop"" in str(e):
                async def _run():
                    await self._command.execute()
                # Use nested loop via asyncio.run in a thread-safe way
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    # Create a new task and wait for it
                    fut = asyncio.run_coroutine_threadsafe(_run(), loop)
                    fut.result()
                else:
                    loop.run_until_complete(_run())
            else:
                raise

        rc = self._command.returncode
        stdout = (self._command.stdout or b"""").decode(""utf-8"", errors=""replace"")
        stderr = (self._command.stderr or b"""").decode(""utf-8"", errors=""replace"")

        if stdout:
            log_info(f""[{self.name}] stdout:\n{stdout.rstrip()}"")
        if stderr:
            log_error(f""[{self.name}] stderr:\n{stderr.rstrip()}"")

        if rc != 0:
            raise RuntimeError(f""[{self.name}] Command failed with exit code {rc}"")"
167547,automl/HpBandSter,automl_HpBandSter/hpbandster/optimizers/learning_curve_models/base.py,hpbandster.optimizers.learning_curve_models.base.LCModel,"class LCModel:
    """"""
        base class for simple learning curve models
    """"""

    def fit(self, times, losses, configs=None):
        """"""
            function to train the model on the observed data

            Parameters:
            -----------

            times: list
                list of numpy arrays of the timesteps for each curve
            losses: list
                list of numpy arrays of the loss (the actual learning curve)
            configs: list or None
                list of the configurations for each sample. Each element
                has to be a numpy array. Set to None, if no configuration
                information is available.
        """"""
        raise NotImplementedError()

    def predict_unseen(self, times, config):
        """"""
            predict the loss of an unseen configuration

            Parameters:
            -----------

            times: numpy array
                times where to predict the loss
            config: numpy array
                the numerical representation of the config

            Returns:
            --------

            mean and variance prediction at input times for the given config
        """"""
        raise NotImplementedError()

    def extend_partial(self, times, obs_times, obs_losses, config=None):
        """"""
            extends a partially observed curve

            Parameters:
            -----------

            times: numpy array
                times where to predict the loss
            obs_times: numpy array
                times where the curve has already been observed
            obs_losses: numpy array
                corresponding observed losses
            config: numpy array
                numerical reperesentation of the config; None if no config
                information is available

            Returns:
            --------

            mean and variance prediction at input times


        """"""","class LCModel:
    
    def fit(self, times, losses, configs=None):
        
        pass
    def predict_unseen(self, times, config):
        
        pass
    def extend_partial(self, times, obs_times, obs_losses, config=None):
        
        pass",snippet_98,4,"import numpy as np

class LCModel:
    def __init__(self):
        self.times_ = None
        self.mean_curve_ = None
        self.params_ = {}  # config -> (a, b)

    def _check_fitted(self):
        if self.times_ is None or self.mean_curve_ is None:
            raise RuntimeError(""Model is not fitted. Call fit() first."")

    @staticmethod
    def _to_array(x):
        return np.asarray(x, dtype=float)

    @staticmethod
    def _affine_fit(x, y):
        x = np.asarray(x, dtype=float)
        y = np.asarray(y, dtype=float)
        if x.ndim != 1 or y.ndim != 1:
            raise ValueError(""x and y must be 1-D arrays."")
        if len(x) != len(y):
            raise ValueError(""x and y must have the same length."")
        n = len(x)
        if n == 0:
            return 1.0, 0.0
        if n == 1:
            xi, yi = float(x[0]), float(y[0])
            a = 0.0
            b = yi - a * xi
            return a, b
        X = np.vstack([x, np.ones(n)]).T
        # Solve min ||a*x + b - y||_2
        sol, _, _, _ = np.linalg.lstsq(X, y, rcond=None)
        a, b = float(sol[0]), float(sol[1])
        return a, b

    def fit(self, times, losses, configs=None):
        times = self._to_array(times)
        if times.ndim != 1:
            raise ValueError(""times must be a 1-D array."")
        # Build loss matrix and config list
        if isinstance(losses, dict):
            cfgs = list(losses.keys())
            mat = []
            for c in cfgs:
                v = self._to_array(losses[c])
                if v.shape != times.shape:
                    raise ValueError(""Each loss sequence must match times in length."")
                mat.append(v)
            L = np.vstack(mat)
            configs_list = cfgs
        else:
            L = self._to_array(losses)
            if L.ndim == 1:
                L = L[None, :]
            if L.ndim != 2:
                raise ValueError(""losses must be a 2-D array if not a dict."")
            if L.shape[1] != times.shape[0]:
                raise ValueError(""losses shape must be (n_configs, n_times)."")
            if configs is None:
                configs_list = list(range(L.shape[0]))
            else:
                if len(configs) != L.shape[0]:
                    raise ValueError(""configs length must match number of rows in losses."")
                configs_list = list(configs)

        self.times_ = times.copy()
        self.mean_curve_ = np.mean(L, axis=0)

        self.params_.clear()
        mc = self.mean_curve_
        for i, cfg in enumerate(configs_list):
            a, b = self._affine_fit(mc, L[i])
            self.params_[cfg] = (a, b)
        return self

    def predict_unseen(self, times, config):
        self._check_fitted()
        times = self._to_array(times)
        if times.ndim != 1:
            raise ValueError(""times must be a 1-D array."")
        base = np.interp(times, self.times_, self.mean_curve_, left=self.mean_curve_[0], right=self.mean_curve_[-1])
        if config in self.params_:
            a, b = self.params_[config]
        else:
            a, b = 1.0, 0.0
        return a * base + b

    def extend_partial(self, times, obs_times, obs_losses, config=None):
        self._check_fitted()
        times = self._to_array(times)
        obs_times = self._to_array(obs_times)
        obs_losses = self._to_array(obs_losses)

        if obs_times.ndim != 1 or obs_losses.ndim != 1:
            raise ValueError(""obs_times and obs_losses must be 1-D arrays."")
        if len(obs_times) != len(obs_losses):
            raise ValueError(""obs_times and obs_losses must have the same length."")
        if times.ndim != 1:
            raise ValueError(""times must be a 1-D array."")

        # Base mean curve aligned to times
        base_full = np.interp(times, self.times_, self.mean_curve_, left=self.mean_curve_[0], right=self.mean_curve_[-1])

        # For fitting parameters, use mean curve evaluated at observed times
        base_obs = np.interp(obs_times, self.times_, self.mean_curve_, left=self.mean_curve_[0], right=self.mean_curve_[-1])

        if config is not None and config in self.params_ and len(obs_times) == 0:
            a, b = self.params_[config]
        else:
            # Fit affine transform to observed points
            a, b = self._affine_fit(base_obs, obs_losses)

        pred_full = a * base_full + b

        # Ensure we respect observed points exactly
        if len(obs_times) > 0:
            # Map observed times to nearest indices in 'times' for replacement
            # If 'times' not covering obs_times exactly, interpolate positions
            # Find indices where times match obs_times exactly
            # Replace values at exact matches; if no exact match, we can append or ignore
            # We'll replace at indices where |times - obs_time| is minimal if within tolerance
            assigned = np.zeros_like(pred_full, dtype=bool)
            for t_obs, y_obs in zip(obs_times, obs_losses):
                idx = int(np.argmin(np.abs(times - t_obs)))
                if np.isclose(times[idx], t_obs, rtol=0.0, atol=1e-12):
                    pred_full[idx] = y_obs
                    assigned[idx] = True
        return pred_full"
168574,romanz/trezor-agent,romanz_trezor-agent/libagent/age/client.py,libagent.age.client.Client,"from cryptography.hazmat.primitives.kdf.hkdf import HKDF
from cryptography.hazmat.primitives import hashes

class Client:
    """"""Sign messages and get public keys from a hardware device.""""""

    def __init__(self, device):
        """"""C-tor.""""""
        self.device = device

    def pubkey(self, identity, ecdh=False):
        """"""Return public key as VerifyingKey object.""""""
        with self.device:
            pubkey = bytes(self.device.pubkey(ecdh=ecdh, identity=identity))
            assert len(pubkey) == 32
            return pubkey

    def ecdh(self, identity, peer_pubkey):
        """"""Derive shared secret using ECDH from peer public key.""""""
        log.info('please confirm AGE decryption on %s for ""%s""...', self.device, identity.to_string())
        with self.device:
            assert len(peer_pubkey) == 32
            result, self_pubkey = self.device.ecdh_with_pubkey(pubkey=b'@' + peer_pubkey, identity=identity)
            assert result[:1] == b'\x04'
            hkdf = HKDF(algorithm=hashes.SHA256(), length=32, salt=peer_pubkey + self_pubkey, info=b'age-encryption.org/v1/X25519')
            return hkdf.derive(result[1:])","
class Client:
    
    def __init__(self, device):
        
        pass
    def pubkey(self, identity, ecdh=False):
        
        pass

    def ecdh(self, identity, peer_pubkey):
        
        pass",snippet_100,4,"class Client:
    def __init__(self, device):
        if device is None:
            raise ValueError(""device must not be None"")
        self._device = device

    def pubkey(self, identity, ecdh=False):
        # Preferred: device.pubkey(identity, ecdh=False)
        if hasattr(self._device, ""pubkey""):
            try:
                return self._device.pubkey(identity, ecdh=ecdh)
            except TypeError:
                # Some implementations may not use keyword
                return self._device.pubkey(identity, ecdh)
        # Alternative names
        for method_name in (""get_pubkey"", ""public_key"", ""get_public_key""):
            if hasattr(self._device, method_name):
                meth = getattr(self._device, method_name)
                try:
                    return meth(identity, ecdh=ecdh)
                except TypeError:
                    try:
                        return meth(identity, ecdh)
                    except TypeError:
                        return meth(identity)
        raise NotImplementedError(""Underlying device does not expose a public key retrieval method"")

    def ecdh(self, identity, peer_pubkey):
        # Preferred: device.ecdh(identity, peer_pubkey)
        if hasattr(self._device, ""ecdh""):
            return self._device.ecdh(identity, peer_pubkey)
        # Alternative names
        for method_name in (
            ""derive"",
            ""derive_shared"",
            ""shared_secret"",
            ""compute_shared"",
            ""compute_shared_secret"",
            ""derive_key"",
        ):
            if hasattr(self._device, method_name):
                return getattr(self._device, method_name)(identity, peer_pubkey)
        raise NotImplementedError(""Underlying device does not expose an ECDH method"")"
168596,romanz/trezor-agent,romanz_trezor-agent/libagent/ssh/__init__.py,libagent.ssh.ClosableNamedTemporaryFile,"import tempfile
import os

class ClosableNamedTemporaryFile:
    """"""Creates a temporary file that is not deleted when the file is closed.

    This allows the file to be opened with an exclusive lock, but used by other programs before
    it is deleted
    """"""

    def __init__(self):
        """"""Create a temporary file.""""""
        self.file = tempfile.NamedTemporaryFile(prefix='trezor-ssh-pubkey-', mode='w', delete=False)
        self.name = self.file.name

    def write(self, buf):
        """"""Write `buf` to the file.""""""
        self.file.write(buf)

    def close(self):
        """"""Closes the file, allowing it to be opened by other programs. Does not delete the file.""""""
        self.file.close()

    def __del__(self):
        """"""Deletes the temporary file.""""""
        try:
            os.unlink(self.file.name)
        except OSError:
            log.warning('Failed to delete temporary file: %s', self.file.name)","
class ClosableNamedTemporaryFile:
    
    def __init__(self):
        
        pass
    def write(self, buf):
        
        pass

    def close(self):
        
        pass

    def __del__(self):
        
        pass",snippet_101,5,"import os
import tempfile

class ClosableNamedTemporaryFile:
    
    def __init__(self):
        self._fp = tempfile.NamedTemporaryFile(mode='w+b', delete=False)
        self.name = self._fp.name
        self._closed = False

    def write(self, buf):
        if self._closed:
            raise ValueError(""I/O operation on closed file."")
        if isinstance(buf, str):
            buf = buf.encode('utf-8')
        written = self._fp.write(buf)
        self._fp.flush()
        os.fsync(self._fp.fileno())
        return written

    def close(self):
        if not self._closed:
            try:
                self._fp.flush()
            except Exception:
                pass
            try:
                self._fp.close()
            finally:
                self._closed = True

    def __del__(self):
        try:
            self.close()
        except Exception:
            pass
        try:
            if self.name and os.path.exists(self.name):
                os.remove(self.name)
        except Exception:
            pass"
168603,romanz/trezor-agent,romanz_trezor-agent/libagent/util.py,libagent.util.ExpiringCache,"import time

class ExpiringCache:
    """"""Simple cache with a deadline.""""""

    def __init__(self, seconds, timer=time.time):
        """"""C-tor.""""""
        self.duration = seconds
        self.timer = timer
        self.value = None
        self.set(None)

    def get(self):
        """"""Returns existing value, or None if deadline has expired.""""""
        if self.timer() > self.deadline:
            self.value = None
        return self.value

    def set(self, value):
        """"""Set new value and reset the deadline for expiration.""""""
        self.deadline = self.timer() + self.duration
        self.value = value","
class ExpiringCache:
    
    def __init__(self, seconds, timer=time.time):
        
        pass
    def get(self):
        
        pass

    def set(self, value):
        
        pass",snippet_102,4,"import time

class ExpiringCache:
    _MISSING = object()
    
    def __init__(self, seconds, timer=time.time):
        self._seconds = seconds
        self._timer = timer
        self._value = self._MISSING
        self._expires_at = None

    def get(self):
        if self._value is self._MISSING:
            return None
        if self._expires_at is not None and self._timer() >= self._expires_at:
            self._value = self._MISSING
            self._expires_at = None
            return None
        return self._value

    def set(self, value):
        self._value = value
        if self._seconds is None:
            self._expires_at = None
        else:
            self._expires_at = self._timer() + float(self._seconds)"
168604,romanz/trezor-agent,romanz_trezor-agent/libagent/util.py,libagent.util.Reader,"import struct
import contextlib

class Reader:
    """"""Read basic type objects out of given stream.""""""

    def __init__(self, stream):
        """"""Create a non-capturing reader.""""""
        self.s = stream
        self._captured = None

    def readfmt(self, fmt):
        """"""Read a specified object, using a struct format string.""""""
        size = struct.calcsize(fmt)
        blob = self.read(size)
        obj, = struct.unpack(fmt, blob)
        return obj

    def read(self, size=None):
        """"""Read `size` bytes from stream.""""""
        blob = self.s.read(size)
        if size is not None and len(blob) < size:
            raise EOFError
        if self._captured:
            self._captured.write(blob)
        return blob

    @contextlib.contextmanager
    def capture(self, stream):
        """"""Capture all data read during this context.""""""
        self._captured = stream
        try:
            yield
        finally:
            self._captured = None","
class Reader:
    
    def __init__(self, stream):
        
        pass
    def readfmt(self, fmt):
        
        pass

    def readfmt(self, fmt):
        
        pass
    @contextlib.contextmanager
    def capture(self, stream):
        
        pass",snippet_103,5,"import contextlib
import struct

class Reader:
    
    def __init__(self, stream):
        self.stream = stream

    def _read_exact(self, n):
        if n < 0:
            raise ValueError(""size must be non-negative"")
        data = bytearray()
        read = getattr(self.stream, ""read"", None)
        if read is None:
            raise TypeError(""stream must have a read() method"")
        while len(data) < n:
            chunk = read(n - len(data))
            if chunk is None:
                raise IOError(""stream.read() returned None"")
            if chunk == b"""" or chunk == """":
                raise EOFError(""unexpected end of stream"")
            if isinstance(chunk, str):
                # Fallback if the stream is text-mode; best-effort 1:1 mapping
                chunk = chunk.encode(""latin-1"")
            data.extend(chunk)
        return bytes(data)

    def _readfmt_impl(self, fmt):
        s = struct.Struct(fmt)
        data = self._read_exact(s.size)
        out = s.unpack(data)
        return out[0] if len(out) == 1 else out
        
    def readfmt(self, fmt):
        return self._readfmt_impl(fmt)

    def readfmt(self, fmt):
        return self._readfmt_impl(fmt)

    @contextlib.contextmanager
    def capture(self, stream):
        old = self.stream
        self.stream = stream
        try:
            yield self
        finally:
            self.stream = old"
171012,indygreg/python-zstandard,indygreg_python-zstandard/zstandard/backend_cffi.py,zstandard.backend_cffi.BufferSegment,"class BufferSegment:
    """"""Represents a segment within a ``BufferWithSegments``.

    This type is essentially a reference to N bytes within a
    ``BufferWithSegments``.

    The object conforms to the buffer protocol.
    """"""

    @property
    def offset(self):
        """"""The byte offset of this segment within its parent buffer.""""""
        raise NotImplementedError()

    def __len__(self):
        """"""Obtain the length of the segment, in bytes.""""""
        raise NotImplementedError()

    def tobytes(self):
        """"""Obtain bytes copy of this segment.""""""
        raise NotImplementedError()","class BufferSegment:
    
    @property
    def offset(self):
        
        pass
    def __len__(self):
        
        pass

    def tobytes(self):
        
        pass",snippet_104,4,"class BufferSegment:
    def __init__(self, buffer=b"""", offset=0, length=None):
        mv = memoryview(buffer)
        n = len(mv)
        if not isinstance(offset, int):
            raise TypeError(""offset must be an int"")
        if offset < 0 or offset > n:
            raise ValueError(""offset out of range"")
        if length is None:
            length = n - offset
        if not isinstance(length, int):
            raise TypeError(""length must be an int or None"")
        if length < 0 or offset + length > n:
            raise ValueError(""length out of range"")
        self._base = mv
        self._offset = offset
        self._view = mv[offset:offset + length]

    @property
    def offset(self):
        return self._offset

    def __len__(self):
        return len(self._view)

    def tobytes(self):
        return self._view.tobytes()"
171015,indygreg/python-zstandard,indygreg_python-zstandard/zstandard/backend_cffi.py,zstandard.backend_cffi.BufferWithSegmentsCollection,"class BufferWithSegmentsCollection:
    """"""A virtual spanning view over multiple BufferWithSegments.

    Instances are constructed from 1 or more :py:class:`BufferWithSegments`
    instances. The resulting object behaves like an ordered sequence whose
    members are the segments within each ``BufferWithSegments``.

    If the object is composed of 2 ``BufferWithSegments`` instances with the
    first having 2 segments and the second have 3 segments, then ``b[0]``
    and ``b[1]`` access segments in the first object and ``b[2]``, ``b[3]``,
    and ``b[4]`` access segments from the second.
    """"""

    def __len__(self):
        """"""The number of segments within all ``BufferWithSegments``.""""""
        raise NotImplementedError()

    def __getitem__(self, i):
        """"""Obtain the ``BufferSegment`` at an offset.""""""
        raise NotImplementedError()","class BufferWithSegmentsCollection:
    
    def __len__(self):
        
        pass
    def __getitem__(self, i):
        
        pass",snippet_105,3,"class BufferWithSegmentsCollection:
    def __len__(self):
        return len(self.segments)

    def __getitem__(self, i):
        n = len(self)
        if isinstance(i, slice):
            start, stop, step = i.indices(n)
            return [self[j] for j in range(start, stop, step)]

        if not isinstance(i, int):
            raise TypeError(""Index must be int or slice"")

        if i < 0:
            i += n
        if i < 0 or i >= n:
            raise IndexError(""Index out of range"")

        seg = self.segments[i]

        # Normalize segment to a slice
        if isinstance(seg, slice):
            s = seg
        elif isinstance(seg, tuple) and len(seg) == 2:
            s = slice(seg[0], seg[1])
        elif hasattr(seg, ""start"") and hasattr(seg, ""stop""):
            s = slice(getattr(seg, ""start""), getattr(seg, ""stop""), getattr(seg, ""step"", None))
        else:
            raise TypeError(""Unsupported segment type"")

        return self.buffer[s]"
171185,ssalentin/plip,ssalentin_plip/plip/exchange/xml.py,plip.exchange.xml.XMLStorage,"class XMLStorage:
    """"""Generic class for storing XML data from PLIP XML files.""""""

    @staticmethod
    def getdata(tree, location, force_string=False):
        """"""Gets XML data from a specific element and handles types.""""""
        found = tree.xpath('%s/text()' % location)
        if not found:
            return None
        else:
            data = found[0]
        if force_string:
            return data
        if data == 'True':
            return True
        elif data == 'False':
            return False
        else:
            try:
                return int(data)
            except ValueError:
                try:
                    return float(data)
                except ValueError:
                    return data

    @staticmethod
    def getcoordinates(tree, location):
        """"""Gets coordinates from a specific element in PLIP XML""""""
        return tuple((float(x) for x in tree.xpath('.//%s/*/text()' % location)))","class XMLStorage:
    
    @staticmethod
    def getdata(tree, location, force_string=False):
        
        pass
    @staticmethod
    def getcoordinates(tree, location):
        
        pass",snippet_106,3,"class XMLStorage:
    @staticmethod
    def _get_root(tree):
        try:
            # ElementTree
            return tree.getroot()
        except AttributeError:
            # Already an Element
            return tree

    @staticmethod
    def _convert_value(value, force_string=False):
        if value is None:
            return None
        s = value.strip()
        if force_string:
            return s
        # booleans
        low = s.lower()
        if low in (""true"", ""yes"", ""on""):
            return True
        if low in (""false"", ""no"", ""off""):
            return False
        # int
        try:
            if s.isdigit() or (s.startswith(('+', '-')) and s[1:].isdigit()):
                return int(s)
        except Exception:
            pass
        # float
        try:
            return float(s)
        except Exception:
            pass
        return s

    @staticmethod
    def _split_path_attr(location):
        # support path ending with /@attr or .../@attr
        if not location:
            return location, None
        if ""/@"" in location:
            idx = location.rfind(""/@"")
            return location[:idx], location[idx + 2 :]
        # also support last-segment @attr like ""node@id""
        if ""@"" in location and ""/@"" not in location:
            # only treat as attribute if it's in the last segment
            parts = location.split(""/"")
            last = parts[-1]
            if last.startswith(""@""):
                # path like ""path/@attr""
                return ""/"".join(parts[:-1]), last[1:]
            if ""@"" in last:
                seg, attr = last.split(""@"", 1)
                parts[-1] = seg
                return ""/"".join([p for p in parts if p]), attr
        return location, None

    @staticmethod
    def getdata(tree, location, force_string=False):
        root = XMLStorage._get_root(tree)
        if not location:
            return None

        path, attr = XMLStorage._split_path_attr(location)

        try:
            elems = root.findall(path) if path else [root]
        except SyntaxError:
            # fallback: try direct children lookup simple tags
            elems = []
            cur = [root]
            for seg in [s for s in path.split(""/"") if s]:
                nxt = []
                for e in cur:
                    nxt.extend(list(e.findall(seg)))
                cur = nxt
            elems = cur

        results = []
        for e in elems:
            if attr:
                val = e.get(attr)
            else:
                # if element has text, use it; otherwise, if it has a single child with text, try that
                val = e.text if e.text is not None else None
                if (val is None or not str(val).strip()) and len(list(e)) == 1:
                    child = list(e)[0]
                    if child is not None and child.text:
                        val = child.text
            results.append(XMLStorage._convert_value(val, force_string))

        if not results:
            return None
        # If any list elements are None and others exist, keep them as None to preserve positions
        # If single result, return scalar
        if len(results) == 1:
            return results[0]
        return results

    @staticmethod
    def getcoordinates(tree, location):
        root = XMLStorage._get_root(tree)
        if not location:
            return None

        try:
            elems = root.findall(location)
        except SyntaxError:
            elems = []
            cur = [root]
            for seg in [s for s in location.split(""/"") if s]:
                nxt = []
                for e in cur:
                    nxt.extend(list(e.findall(seg)))
                cur = nxt
            elems = cur

        coords = []

        def parse_pair(e):
            # Try attributes first
            lat = e.get(""lat"") or e.get(""latitude"") or e.get(""y"")
            lon = e.get(""lon"") or e.get(""long"") or e.get(""longitude"") or e.get(""x"")
            # If not attributes, try child elements
            if lat is None:
                lat_elem = e.find(""lat"") or e.find(""latitude"") or e.find(""y"")
                lat = lat_elem.text if lat_elem is not None else None
            if lon is None:
                lon_elem = e.find(""lon"") or e.find(""long"") or e.find(""longitude"") or e.find(""x"")
                lon = lon_elem.text if lon_elem is not None else None
            # Some formats store coordinates as single text ""lat,lon"" or ""lon,lat""
            if (lat is None or lon is None) and e.text:
                txt = e.text.strip()
                if "","" in txt:
                    a, b = [t.strip() for t in txt.split("","", 1)]
                    # heuristics: if absolute value of first > 90, it's lon,lat
                    try:
                        fa = float(a)
                        fb = float(b)
                        if abs(fa) > 90 and abs(fb) <= 90:
                            lon, lat = a, b
                        elif abs(fb) > 90 and abs(fa) <= 90:
                            lat, lon = a, b
                        else:
                            # default to lat,lon
                            lat, lon = a, b
                    except Exception:
                        pass

            if lat is None or lon is None:
                return None
            try:
                return (float(lat), float(lon))
            except Exception:
                return None

        for e in elems:
            pair = parse_pair(e)
            if pair is not None:
                coords.append(pair)

        if not coords:
            return None
        if len(coords) == 1:
            return coords[0]
        return coords"
172857,cogeotiff/rio-tiler,rio_tiler/colormap.py,rio_tiler.colormap.ColorMaps,"import attr
from rio_tiler.errors import ColorMapAlreadyRegistered, InvalidColorFormat, InvalidColorMapName, InvalidFormat
import json
import numpy
from typing import Dict, List, Sequence, Tuple, Union
import pathlib
from rio_tiler.types import ColorMapType, DataMaskType, DiscreteColorMapType, GDALColorMapType, IntervalColorMapType

@attr.s(frozen=True)
class ColorMaps:
    """"""Default Colormaps holder.

    Attributes:
        data (dict): colormaps. Defaults to `rio_tiler.colormap.DEFAULTS_CMAPS`.

    """"""
    data: Dict[str, Union[str, pathlib.Path, ColorMapType]] = attr.ib(default=attr.Factory(lambda: DEFAULT_CMAPS_FILES))

    def get(self, name: str) -> ColorMapType:
        """"""Fetch a colormap.

        Args:
            name (str): colormap name.

        Returns
            dict: colormap dictionary.

        """"""
        cmap = self.data.get(name, None)
        if cmap is None:
            raise InvalidColorMapName(f'Invalid colormap name: {name}')
        if isinstance(cmap, (pathlib.Path, str)):
            if isinstance(cmap, str):
                cmap = pathlib.Path(cmap)
            if cmap.suffix == '.npy':
                colormap = numpy.load(cmap)
                assert colormap.shape == (256, 4)
                assert colormap.dtype == numpy.uint8
                cmap_data = {idx: tuple(value) for idx, value in enumerate(colormap)}
            elif cmap.suffix == '.json':
                with cmap.open() as f:
                    cmap_data = json.load(f, object_hook=lambda x: {int(k): parse_color(v) for k, v in x.items()})
                if isinstance(cmap_data, Sequence):
                    cmap_data = [(tuple(inter), parse_color(v)) for inter, v in cmap_data]
            else:
                raise ValueError(f'Not supported {cmap.suffix} extension for ColorMap')
            self.data[name] = cmap_data
            return cmap_data
        return cmap

    def list(self) -> List[str]:
        """"""List registered Colormaps.

        Returns
            list: list of colormap names.

        """"""
        return list(self.data)

    def register(self, custom_cmap: Dict[str, Union[str, pathlib.Path, ColorMapType]], overwrite: bool=False) -> 'ColorMaps':
        """"""Register a custom colormap.

        Args:
            custom_cmap (dict): custom colormap(s) to register.
            overwrite (bool): Overwrite existing colormap with same key. Defaults to False.

        Examples:
            >>> cmap = cmap.register({""acmap"": {0: (0, 0, 0, 0), ...}})

            >>> cmap = cmap.register({""acmap"": ""acmap.npy""})

        """"""
        for name, _ in custom_cmap.items():
            if not overwrite and name in self.data:
                raise ColorMapAlreadyRegistered(f'{name} is already registered. Use force=True to overwrite.')
        return ColorMaps({**self.data, **custom_cmap})","@attr.s(frozen=True)
class ColorMaps:
    
    def get(self, name: str) -> ColorMapType:
        
        pass
    def list(self) -> List[str]:
        
        pass

    def register(self, custom_cmap: Dict[str, Union[str, pathlib.Path, ColorMapType]], overwrite: bool=False) -> 'ColorMaps':
        '''Register a custom colormap.
        Args:
            custom_cmap (dict): custom colormap(s) to register.
            overwrite (bool): Overwrite existing colormap with same key. Defaults to False.
        Examples:
            >>> cmap = cmap.register({""acmap"": {0: (0, 0, 0, 0), ...}})
            >>> cmap = cmap.register({""acmap"": ""acmap.npy""})
        '''
        pass",snippet_107,4,"import pathlib
from typing import Dict, Union, List, Tuple, Any
import numpy as np
import attr

ColorMapType = Union[np.ndarray, Dict[int, Tuple[float, float, float, float]], List[Tuple[float, float, float, float]]]


def _to_array(cmap: ColorMapType) -> np.ndarray:
    if isinstance(cmap, np.ndarray):
        arr = np.array(cmap, dtype=float)
    elif isinstance(cmap, dict):
        if not cmap:
            raise ValueError(""Colormap dict cannot be empty."")
        # Sort by numeric keys
        try:
            items = sorted(cmap.items(), key=lambda kv: float(kv[0]))
        except Exception as e:
            raise ValueError(f""Invalid colormap dict keys; must be numeric-sortable. {e}"")
        arr = np.array([kv[1] for kv in items], dtype=float)
    else:
        # Assume list-like of tuples
        arr = np.array(cmap, dtype=float)

    if arr.ndim != 2 or arr.shape[1] not in (3, 4):
        raise ValueError(""Colormap array must be of shape (N, 3) or (N, 4)."")
    if arr.shape[1] == 3:
        alpha = np.ones((arr.shape[0], 1), dtype=float)
        arr = np.concatenate([arr, alpha], axis=1)
    # Clip to [0,1]
    arr = np.clip(arr, 0.0, 1.0)
    return arr


def _load_from_path(p: Union[str, pathlib.Path]) -> np.ndarray:
    path = pathlib.Path(p)
    if not path.exists():
        raise FileNotFoundError(f""Colormap file not found: {path}"")
    if path.suffix.lower() == "".npy"":
        data = np.load(str(path), allow_pickle=True)
        # If saved dict or array
        if isinstance(data, np.ndarray) and data.dtype == object:
            # Try to unwrap single object (common when saving dict with np.save)
            if data.shape == ():
                data = data.item()
        return _to_array(data)
    elif path.suffix.lower() == "".npz"":
        data = np.load(str(path), allow_pickle=True)
        # Try common keys
        for key in (""cmap"", ""colormap"", ""arr"", ""data""):
            if key in data:
                return _to_array(data[key])
        # Fallback: first array
        if len(list(data.files)) > 0:
            return _to_array(data[data.files[0]])
        raise ValueError(f""No array-like data found in {path}"")
    else:
        raise ValueError(f""Unsupported colormap file extension: {path.suffix}"")


@attr.s(frozen=True)
class ColorMaps:
    _maps: Dict[str, np.ndarray] = attr.ib(factory=dict)

    def get(self, name: str) -> ColorMapType:
        if not isinstance(name, str) or not name:
            raise ValueError(""Name must be a non-empty string."")
        try:
            arr = self._maps[name]
        except KeyError:
            raise KeyError(f""Colormap '{name}' is not registered."")
        return np.array(arr, copy=True)

    def list(self) -> List[str]:
        return sorted(self._maps.keys())

    def register(self, custom_cmap: Dict[str, Union[str, pathlib.Path, ColorMapType]], overwrite: bool = False) -> 'ColorMaps':
        if not isinstance(custom_cmap, dict) or not custom_cmap:
            raise ValueError(""custom_cmap must be a non-empty dict of name -> colormap."")
        new_maps = dict(self._maps)
        for name, spec in custom_cmap.items():
            if not isinstance(name, str) or not name:
                raise ValueError(""All colormap names must be non-empty strings."")
            if (not overwrite) and (name in new_maps):
                raise KeyError(f""Colormap '{name}' already exists. Use overwrite=True to replace it."")
            if isinstance(spec, (str, pathlib.Path)):
                arr = _load_from_path(spec)
            else:
                arr = _to_array(spec)
            new_maps[name] = arr
        return attr.evolve(self, _maps=new_maps)"
174678,ipinfo/python,ipinfo_python/ipinfo/details.py,ipinfo.details.Details,"class Details:
    """"""Encapsulates data for single IP address.""""""

    def __init__(self, details):
        """"""Initialize by settings `details` attribute.""""""
        self.details = details

    def __getattr__(self, attr):
        """"""Return attribute if it exists in details array, else return error.""""""
        if attr not in self.details:
            raise AttributeError(f'{attr} is not a valid attribute of Details')
        return self.details[attr]

    @property
    def all(self):
        """"""Return all details as dict.""""""
        return self.details","class Details:
    
    def __init__(self, details):
        
        pass
    def __getattr__(self, attr):
        
        pass
    @property
    def all(self):
        
        pass",snippet_108,4,"class Details:
    def __init__(self, details):
        from collections.abc import Mapping

        def wrap(value):
            if isinstance(value, Mapping):
                return Details(value)
            if isinstance(value, list):
                return [wrap(v) for v in value]
            if isinstance(value, tuple):
                return tuple(wrap(v) for v in value)
            if isinstance(value, set):
                return {wrap(v) for v in value}
            return value

        if details is None:
            details = {}
        elif isinstance(details, Details):
            details = details.all
        elif not isinstance(details, Mapping):
            raise TypeError(""details must be a mapping or Details instance"")

        wrapped = {k: wrap(v) for k, v in details.items()}
        object.__setattr__(self, ""_details"", wrapped)

    def __getattr__(self, attr):
        d = object.__getattribute__(self, ""_details"")
        try:
            return d[attr]
        except KeyError:
            raise AttributeError(f""{type(self).__name__!s} has no attribute '{attr}'"")

    @property
    def all(self):
        def unwrap(value):
            if isinstance(value, Details):
                return value.all
            if isinstance(value, list):
                return [unwrap(v) for v in value]
            if isinstance(value, tuple):
                return tuple(unwrap(v) for v in value)
            if isinstance(value, set):
                return {unwrap(v) for v in value}
            return value

        return {k: unwrap(v) for k, v in self._details.items()}"
176091,opencobra/cobrapy,opencobra_cobrapy/src/cobra/util/context.py,cobra.util.context.HistoryManager,"from typing import TYPE_CHECKING, Any, Callable, Optional

class HistoryManager:
    """"""
    Define a base context manager.

    It records a list of actions to be taken at a later time.
    This is used to implement context managers that allow temporary
    changes to a `cobra.core.Model`.

    """"""

    def __init__(self, **kwargs) -> None:
        """"""Initialize the class.""""""
        super().__init__(**kwargs)
        self._history = []

    def __call__(self, operation: Callable[[Any], Any]) -> None:
        """"""Add the corresponding operation to the history stack.

        Parameters
        ----------
        operation : callable
            A function to be called at a later time.

        """"""
        self._history.append(operation)

    def reset(self) -> None:
        """"""Trigger executions for all items in the stack in reverse order.""""""
        while self._history:
            entry = self._history.pop()
            entry()

    def size(self) -> int:
        """"""Calculate number of operations on the stack.""""""
        return len(self._history)","
class HistoryManager:
    
    def __init__(self, **kwargs) -> None:
        
        pass
    def __call__(self, operation: Callable[[Any], Any]) -> None:
        
        pass

    def reset(self) -> None:
        
        pass

    def size(self) -> int:
        
        pass",snippet_109,5,"from typing import Any, Callable, List


class HistoryManager:
    def __init__(self, **kwargs) -> None:
        self._history: List[Callable[[Any], Any]] = []
        self._capacity: int | None = kwargs.get(""capacity"")

    def __call__(self, operation: Callable[[Any], Any]) -> None:
        if not callable(operation):
            raise TypeError(""operation must be callable"")
        if self._capacity is not None and self._capacity <= 0:
            return
        if self._capacity is not None and len(self._history) >= self._capacity:
            # Maintain a fixed-size history by dropping the oldest entry
            self._history.pop(0)
        self._history.append(operation)

    def reset(self) -> None:
        self._history.clear()

    def size(self) -> int:
        return len(self._history)"
176713,adafruit/Adafruit_Blinka,adafruit_Adafruit_Blinka/src/adafruit_blinka/__init__.py,adafruit_blinka.Enum,"class Enum:
    """"""
    Object supporting CircuitPython-style of static symbols
    as seen with Direction.OUTPUT, Pull.UP
    """"""

    def __repr__(self):
        """"""
        Assumes instance will be found as attribute of own class.
        Returns dot-subscripted path to instance
        (assuming absolute import of containing package)
        """"""
        cls = type(self)
        for key in dir(cls):
            if getattr(cls, key) is self:
                return '{}.{}.{}'.format(cls.__module__, cls.__qualname__, key)
        return repr(self)

    @classmethod
    def iteritems(cls):
        """"""
        Inspects attributes of the class for instances of the class
        and returns as key,value pairs mirroring dict#iteritems
        """"""
        for key in dir(cls):
            val = getattr(cls, key)
            if isinstance(cls, val):
                yield (key, val)","class Enum:
    
    def __repr__(self):
        
        pass
    @classmethod
    def iteritems(cls):
        
        pass",snippet_110,3,"class Enum:
    
    def __repr__(self):
        cls = type(self)
        items = "", "".join(f""{k}={v!r}"" for k, v in cls.iteritems())
        return f""<{cls.__name__} {items}>""

    @classmethod
    def iteritems(cls):
        for k, v in cls.__dict__.items():
            if k.startswith('_'):
                continue
            if isinstance(v, (staticmethod, classmethod)):
                continue
            if callable(v):
                continue
            yield k, v"
176756,adafruit/Adafruit_Blinka,adafruit_Adafruit_Blinka/src/adafruit_blinka/microcontroller/nova/__init__.py,adafruit_blinka.microcontroller.nova.Connection,"class Connection:
    """"""Connection class""""""
    __instance = None

    @staticmethod
    def getInstance():
        """"""Static access method.""""""
        if Connection.__instance is None:
            Connection()
        return Connection.__instance

    def __init__(self):
        """"""Virtually private constructor.""""""
        if Connection.__instance is not None:
            raise Exception('This class is a singleton!')
        from binhoHostAdapter import binhoHostAdapter
        from binhoHostAdapter import binhoUtilities
        devices = binhoUtilities.listAvailableDevices()
        if len(devices) > 0:
            Connection.__instance = binhoHostAdapter.binhoHostAdapter(devices[0])
        else:
            raise RuntimeError('No Binho Nova found!')","class Connection:
    
    @staticmethod
    def getInstance():
        
        pass
    def __init__(self):
        
        pass",snippet_111,3,"import threading

class Connection:
    _instance = None
    _lock = threading.Lock()
    
    @staticmethod
    def getInstance():
        if Connection._instance is None:
            with Connection._lock:
                if Connection._instance is None:
                    Connection._instance = Connection()
        return Connection._instance

    def __init__(self):
        if getattr(self, ""_initialized"", False):
            return
        self._initialized = True"
177578,ottogroup/palladium,ottogroup_palladium/palladium/persistence.py,palladium.persistence.FileLikeIO,"from abc import abstractmethod

class FileLikeIO:
    """"""Used by :class:`FileLike` to access low level file handle
    operations.
    """"""

    @abstractmethod
    def open(self, path, mode='r'):
        """"""Return a file handle

        For normal files, the implementation is:

        ```python
        return open(path, mode)
        ```
        """"""

    @abstractmethod
    def exists(self, path):
        """"""Test whether a path exists

        For normal files, the implementation is:

        ```python
        return os.path.exists(path)
        ```
        """"""

    @abstractmethod
    def remove(self, path):
        """"""Remove a file

        For normal files, the implementation is:

        ```python
        os.remove(path)
        ```
        """"""","
class FileLikeIO:
    
    @abstractmethod
    def open(self, path, mode='r'):
        
        pass
    @abstractmethod
    def exists(self, path):
        
        pass
    @abstractmethod
    def remove(self, path):
        
        pass",snippet_113,4,"from pathlib import Path
import builtins
from typing import Union, IO, AnyStr, Optional


class FileLikeIO:
    def open(self, path: Union[str, Path], mode: str = 'r') -> IO[AnyStr]:
        p = Path(path)
        if any(flag in mode for flag in ('w', 'a', 'x', '+')):
            parent = p.parent
            if parent and not parent.exists():
                parent.mkdir(parents=True, exist_ok=True)
        if 'b' in mode:
            return builtins.open(p, mode)
        return builtins.open(p, mode, encoding='utf-8', newline='')

    def exists(self, path: Union[str, Path]) -> bool:
        return Path(path).exists()

    def remove(self, path: Union[str, Path]) -> None:
        p = Path(path)
        if not p.exists():
            raise FileNotFoundError(str(p))
        if p.is_dir():
            raise IsADirectoryError(str(p))
        p.unlink()"
179419,python-constraint/python-constraint,python-constraint_python-constraint/constraint/domain.py,constraint.domain.Variable,"class Variable:
    """"""Helper class for variable definition.

    Using this class is optional, since any hashable object,
    including plain strings and integers, may be used as variables.
    """"""

    def __init__(self, name):
        """"""Initialization method.

        Args:
            name (string): Generic variable name for problem-specific
                purposes
        """"""
        self.name = name

    def __repr__(self):
        """"""Represents itself with the name attribute.""""""
        return self.name","class Variable:
    
    def __init__(self, name):
        
        pass
    def __repr__(self):
        
        pass",snippet_115,3,"class Variable:
    
    def __init__(self, name):
        if not isinstance(name, str):
            raise TypeError(""name must be a string"")
        if not name:
            raise ValueError(""name must not be empty"")
        self.name = name

    def __repr__(self):
        return self.name"
179425,python-constraint/python-constraint,python-constraint_python-constraint/constraint/solvers.py,constraint.solvers.Solver,"class Solver:
    """"""Abstract base class for solvers.""""""
    requires_pickling = False

    def getSolution(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Return one solution for the given problem.

        Args:
            domains (dict): Dictionary mapping variables to their domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        """"""
        msg = f'{self.__class__.__name__} is an abstract class'
        raise NotImplementedError(msg)

    def getSolutions(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Return all solutions for the given problem.

        Args:
            domains (dict): Dictionary mapping variables to domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        """"""
        msg = f'{self.__class__.__name__} provides only a single solution'
        raise NotImplementedError(msg)

    def getSolutionIter(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Return an iterator for the solutions of the given problem.

        Args:
            domains (dict): Dictionary mapping variables to domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        """"""
        msg = f""{self.__class__.__name__} doesn't provide iteration""
        raise NotImplementedError(msg)","class Solver:
    
    def getSolution(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        
        pass
    def getSolutions(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        
        pass
    def getSolutionIter(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        
        pass",snippet_116,4,"class Solver:
    def _normalize_constraints(self, domains: dict, constraints: list[tuple]):
        norm = []
        for c in constraints:
            # Accept formats:
            # - {'vars': [...], 'func': callable}
            # - (vars, func)
            # - (func, vars)
            if isinstance(c, dict):
                vars_ = c.get('vars')
                func = c.get('func')
            elif isinstance(c, tuple) and len(c) == 2:
                a, b = c
                if callable(a) and (isinstance(b, (list, tuple)) or b is None):
                    func, vars_ = a, b
                elif callable(b) and (isinstance(a, (list, tuple)) or a is None):
                    vars_, func = a, b
                else:
                    raise ValueError(""Constraint tuple must be (vars, func) or (func, vars)."")
            else:
                raise ValueError(""Unsupported constraint format."")
            if vars_ is None:
                # Global constraint: will be evaluated when all variables are assigned
                vars_ = tuple(domains.keys())
            if not callable(func):
                raise ValueError(""Constraint func must be callable."")
            vars_ = tuple(vars_)
            # Validate variables exist
            for v in vars_:
                if v not in domains:
                    raise KeyError(f""Constraint references unknown variable: {v}"")
            norm.append((vars_, func))
        return norm

    def _build_vconstraints(self, variables, constraints):
        vmap = {v: [] for v in variables}
        for idx, (vars_, _) in enumerate(constraints):
            for v in vars_:
                vmap[v].append(idx)
        return vmap

    def _is_consistent(self, assignment, constraint, scope):
        # Only evaluate when all vars in scope are assigned
        if any(v not in assignment for v in scope):
            return True
        func_args = [assignment[v] for v in scope]
        return bool(constraint(*func_args))

    def _backtrack(self, vars_order, domains, constraints, vconstraints, assignment):
        if len(assignment) == len(vars_order):
            yield dict(assignment)
            return

        # Choose next unassigned variable (simple order)
        for var in vars_order:
            if var not in assignment:
                next_var = var
                break

        for value in domains[next_var]:
            assignment[next_var] = value
            ok = True
            # Check only constraints that involve next_var; evaluate those whose scope fully assigned
            for cidx in vconstraints.get(next_var, []):
                scope, func = constraints[cidx]
                if not self._is_consistent(assignment, func, scope):
                    ok = False
                    break
            if ok:
                yield from self._backtrack(vars_order, domains, constraints, vconstraints, assignment)
            del assignment[next_var]

    def getSolution(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        solutions = self.getSolutionIter(domains, constraints, vconstraints)
        try:
            return next(solutions)
        except StopIteration:
            return None

    def getSolutions(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        return list(self.getSolutionIter(domains, constraints, vconstraints))

    def getSolutionIter(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        if not isinstance(domains, dict) or not domains:
            return iter(())  # empty iterator

        # Normalize domains to lists
        doms = {v: list(vals) for v, vals in domains.items()}
        norm_constraints = self._normalize_constraints(doms, constraints or [])

        if not vconstraints:
            vmap = self._build_vconstraints(doms.keys(), norm_constraints)
        else:
            # Ensure indices are valid
            vmap = {k: list(vconstraints.get(k, [])) for k in doms.keys()}
        vars_order = list(doms.keys())

        def generator():
            yield from self._backtrack(vars_order, doms, norm_constraints, vmap, {})

        return generator()"
179445,maroba/findiff,findiff/pde.py,findiff.pde.PDE,"import numpy as np
from scipy.sparse.linalg import spsolve
import scipy.sparse as sparse

class PDE:
    """"""
    Representation of a partial differential equation.
    """"""

    def __init__(self, lhs, rhs, bcs):
        """"""
        Initializes the PDE.

        You need to specify the left hand side (lhs) in terms of derivatives
        as well as the right hand side in terms of an array.

        Parameters
        ----------
        lhs: FinDiff object or combination of FinDiff objects
            the left hand side of the PDE
        rhs: numpy.ndarray
            the right hand side of the PDE
        bcs: BoundaryConditions
            the boundary conditions for the PDE

        """"""
        self.lhs = lhs
        self.rhs = rhs
        self.bcs = bcs
        self._L = None

    def solve(self):
        """"""
        Solves the PDE.

        Returns
        -------
        out: numpy.ndarray
            Array with the solution of the PDE.
        """"""
        shape = self.bcs.shape
        if self._L is None:
            self._L = self.lhs.matrix(shape)
        L = sparse.lil_matrix(self._L)
        f = self.rhs.reshape(-1, 1)
        nz = list(self.bcs.row_inds())
        L[nz, :] = self.bcs.lhs[nz, :]
        f[nz] = np.array(self.bcs.rhs[nz].toarray()).reshape(-1, 1)
        L = sparse.csr_matrix(L)
        return spsolve(L, f).reshape(shape)","
class PDE:
    
    def __init__(self, lhs, rhs, bcs):
        
        pass
    def solve(self):
        
        pass",snippet_117,3,"class PDE:
    
    def __init__(self, lhs, rhs, bcs):
        self.lhs = lhs
        self.rhs = rhs
        self.bcs = bcs
        self.n = 100
        self.domain = (0.0, 1.0)

    def _thomas_tridiagonal(self, a, b, c, d):
        n = len(d)
        cp = c.copy()
        bp = b.copy()
        dp = d.copy()

        for i in range(1, n):
            m = a[i - 1] / bp[i - 1]
            bp[i] = bp[i] - m * cp[i - 1]
            dp[i] = dp[i] - m * dp[i - 1]

        x = dp
        x[-1] = dp[-1] / bp[-1]
        for i in range(n - 2, -1, -1):
            x[i] = (dp[i] - cp[i] * x[i + 1]) / bp[i]
        return x

    def solve(self):
        import numpy as np

        # Case 1: user provides a full discrete operator matrix
        if hasattr(self.lhs, ""shape"") and len(getattr(self.lhs, ""shape"", ())) == 2:
            A = np.asarray(self.lhs, dtype=float)
            b = self.rhs
            if callable(b):
                raise ValueError(""rhs cannot be callable when lhs is a matrix."")
            b = np.asarray(b, dtype=float)
            if A.shape[0] != A.shape[1] or A.shape[0] != b.shape[0]:
                raise ValueError(""Incompatible shapes between lhs matrix and rhs vector."")
            u = np.linalg.solve(A, b)
            return u

        # Case 2: user provides a callable to construct the discrete operator
        if callable(self.lhs):
            A_x = self.lhs
            a, bdom = self.domain
            A, x = A_x(self.n, a, bdom)
            bvec = self.rhs
            if callable(bvec):
                bvec = np.asarray([bvec(xi) for xi in x], dtype=float)
            else:
                bvec = np.asarray(bvec, dtype=float)
                if bvec.shape[0] != A.shape[0]:
                    raise ValueError(""rhs length does not match operator size."")
            u = np.linalg.solve(A, bvec)
            return u

        # Default: 1D Poisson problem on [a,b] with Dirichlet BCs
        if not isinstance(self.bcs, (tuple, list)) or len(self.bcs) != 2:
            raise ValueError(""For default 1D Poisson solver, bcs must be a tuple (u_a, u_b)."")
        ua, ub = float(self.bcs[0]), float(self.bcs[1])

        a, bdom = self.domain
        n = int(self.n)
        if n < 1:
            raise ValueError(""n must be >= 1."")
        h = (bdom - a) / (n + 1)
        x_interior = np.linspace(a + h, bdom - h, n)

        if callable(self.rhs):
            f = np.asarray([self.rhs(xi) for xi in x_interior], dtype=float)
        else:
            f = np.asarray(self.rhs, dtype=float)
            if f.shape[0] != n:
                raise ValueError(""rhs length must match the number of interior points."")
        d = (h * h) * f
        if n >= 1:
            d[0] += ua
            d[-1] += ub

        lower = -np.ones(n - 1)
        main = 2.0 * np.ones(n)
        upper = -np.ones(n - 1)

        u_interior = self._thomas_tridiagonal(lower, main, upper, d)
        x_all = np.concatenate(([a], x_interior, [bdom]))
        u_all = np.concatenate(([ua], u_interior, [ub]))
        return x_all, u_all"
186773,sphinx-gallery/sphinx-gallery,sphinx_gallery/_dummy/__init__.py,sphinx_gallery._dummy.DummyClass,"class DummyClass:
    """"""Dummy class for testing method resolution.""""""

    def run(self):
        """"""Do nothing.""""""
        pass

    @property
    def prop(self):
        """"""Property.""""""
        return 'Property'","class DummyClass:
    
    def run(self):
        
        pass
    @property
    def prop(self):
        
        pass",snippet_119,3,"class DummyClass:
    def __init__(self, start: int = 0):
        self._value = int(start)

    def run(self):
        self._value += 1
        return self._value

    @property
    def prop(self):
        return self._value"
186774,sphinx-gallery/sphinx-gallery,sphinx_gallery/_dummy/nested.py,sphinx_gallery._dummy.nested.NestedDummyClass,"class NestedDummyClass:
    """"""Nested dummy class for testing method resolution.""""""

    def run(self):
        """"""Do nothing.""""""
        pass

    @property
    def prop(self):
        """"""Property.""""""
        return 'Property'","class NestedDummyClass:
    
    def run(self):
        
        pass
    @property
    def prop(self):
        
        pass",snippet_120,3,"class NestedDummyClass:
    class _Counter:
        def __init__(self, start=0):
            self.value = int(start)

        def inc(self, n=1):
            self.value += int(n)
            return self.value

        def reset(self):
            self.value = 0

        @property
        def even(self):
            return self.value % 2 == 0

    def __init__(self, start=0):
        self._counter = self._Counter(start)

    def run(self):
        return self._counter.inc()

    @property
    def prop(self):
        return {""value"": self._counter.value, ""even"": self._counter.even}"
190420,JamesPHoughton/pysd,JamesPHoughton_pysd/pysd/py_backend/external.py,pysd.py_backend.external.Excels,"import numpy as np
from openpyxl import load_workbook
import pandas as pd

class Excels:
    """"""
    Class to save the read Excel files and thus avoid double reading
    """"""
    _Excels, _Excels_opyxl = ({}, {})

    @classmethod
    def read(cls, file_name, tab):
        """"""
        Read the Excel file or return the previously read one
        """"""
        if file_name.joinpath(tab) in cls._Excels:
            return cls._Excels[file_name.joinpath(tab)]
        else:
            read_kwargs = {}
            ext = file_name.suffix.lower()
            if ext in _SPREADSHEET_EXTS:
                read_func = pd.read_excel
                read_kwargs['sheet_name'] = tab
            elif ext == '.csv':
                read_func = pd.read_csv
                if tab and (not tab[0].isalnum()):
                    read_kwargs['sep'] = tab
            else:
                read_func = pd.read_table
                if tab and (not tab[0].isalnum()):
                    read_kwargs['sep'] = tab
            excel = np.array([pd.to_numeric(ex, errors='coerce') for ex in read_func(file_name, header=None, **read_kwargs).values])
            cls._Excels[file_name.joinpath(tab)] = excel
            return excel

    @classmethod
    def read_opyxl(cls, file_name):
        """"""
        Read the Excel file using OpenPyXL or return the previously read one
        """"""
        if file_name in cls._Excels_opyxl:
            return cls._Excels_opyxl[file_name]
        else:
            excel = load_workbook(file_name, read_only=True, data_only=True)
            cls._Excels_opyxl[file_name] = excel
            return excel

    @classmethod
    def clean(cls):
        """"""
        Clean the dictionary of read files
        """"""
        for file in cls._Excels_opyxl.values():
            file.close()
        cls._Excels, cls._Excels_opyxl = ({}, {})","
class Excels:
    
    @classmethod
    def read(cls, file_name, tab):
        
        pass
    @classmethod
    def read_opyxl(cls, file_name):
        
        pass
    @classmethod
    def clean(cls):
        
        pass",snippet_121,4,"class Excels:
    _cache = {}

    @classmethod
    def _ensure_path(cls, file_name):
        import os
        if not isinstance(file_name, str) or not file_name.strip():
            raise ValueError(""file_name must be a non-empty string"")
        if not os.path.exists(file_name):
            raise FileNotFoundError(f""File not found: {file_name}"")

    @classmethod
    def _strip_str(cls, v):
        if isinstance(v, str):
            s = v.strip()
            return s
        return v

    @classmethod
    def _row_is_empty(cls, row):
        for v in row:
            if v is None or (isinstance(v, float) and v != v):
                continue
            if isinstance(v, str) and v.strip() == """":
                continue
            return False
        return True

    @classmethod
    def _to_records(cls, headers, rows):
        clean_headers = []
        seen = {}
        for h in headers:
            key = str(h).strip() if h is not None else """"
            if key == """":
                key = ""column""
            base = key
            if key in seen:
                seen[key] += 1
                key = f""{base}_{seen[base]}""
            else:
                seen[key] = 0
            clean_headers.append(key)
        records = []
        for r in rows:
            if cls._row_is_empty(r):
                continue
            rec = {}
            for i, h in enumerate(clean_headers):
                v = r[i] if i < len(r) else None
                if v is None:
                    rec[h] = None
                else:
                    if isinstance(v, float) and v != v:
                        rec[h] = None
                    else:
                        rec[h] = cls._strip_str(v)
            records.append(rec)
        return records

    @classmethod
    def read(cls, file_name, tab):
        cls._ensure_path(file_name)
        cache_key = (""pandas"", file_name, tab)
        if cache_key in cls._cache:
            return cls._cache[cache_key]
        try:
            import pandas as pd
            df = pd.read_excel(file_name, sheet_name=tab, dtype=object)
            # Normalize column names
            df.columns = [str(c).strip() if c is not None else """" for c in df.columns]
            # Replace NaN with None
            df = df.where(pd.notnull(df), None)
            # Strip strings
            for c in df.columns:
                df[c] = df[c].map(cls._strip_str)
            records = df.to_dict(orient=""records"")
            cls._cache[cache_key] = records
            return records
        except ImportError:
            # Fallback to openpyxl-only read
            wb_data = cls.read_opyxl(file_name)
            # tab can be index or name
            if isinstance(tab, int):
                try:
                    sheet_name = list(wb_data.keys())[tab]
                except IndexError:
                    raise ValueError(f""Sheet index out of range: {tab}"")
            else:
                sheet_name = tab
            if sheet_name not in wb_data:
                raise ValueError(f""Sheet not found: {sheet_name}"")
            records = wb_data[sheet_name]
            cls._cache[cache_key] = records
            return records

    @classmethod
    def read_opyxl(cls, file_name):
        cls._ensure_path(file_name)
        cache_key = (""openpyxl"", file_name)
        if cache_key in cls._cache:
            return cls._cache[cache_key]
        try:
            from openpyxl import load_workbook
        except ImportError:
            raise ImportError(""openpyxl is required to read Excel files without pandas"")
        wb = load_workbook(filename=file_name, read_only=True, data_only=True)
        result = {}
        try:
            for ws in wb.worksheets:
                rows_iter = ws.iter_rows(values_only=True)
                try:
                    header_row = next(rows_iter)
                except StopIteration:
                    result[ws.title] = []
                    continue
                headers = [h if h is not None else """" for h in header_row]
                data_rows = []
                for r in rows_iter:
                    data_rows.append(list(r))
                records = cls._to_records(headers, data_rows)
                result[ws.title] = records
        finally:
            wb.close()
        cls._cache[cache_key] = result
        return result

    @classmethod
    def clean(cls):
        cls._cache.clear()"
222265,box/flaky,box_flaky/flaky/multiprocess_string_io.py,flaky.multiprocess_string_io.MultiprocessingStringIO,"import multiprocessing

class MultiprocessingStringIO:
    """"""
    Provide a StringIO-like interface to the multiprocessing ListProxy. The
    multiprocessing ListProxy needs to be instantiated before the flaky plugin
    is configured, so the list is created as a class variable.
    """"""
    _manager = multiprocessing.Manager()
    proxy = _manager.list()

    def getvalue(self):
        """"""
        Shadow the StringIO.getvalue method.
        """"""
        return ''.join((i for i in self.proxy))

    def writelines(self, content_list):
        """"""
        Shadow the StringIO.writelines method. Ingests a list and
        translates that to a string
        """"""
        for item in content_list:
            self.write(item)

    def write(self, content):
        """"""
        Shadow the StringIO.write method.
        """"""
        content.strip('\n')
        self.proxy.append(content)","
class MultiprocessingStringIO:
    
    def getvalue(self):
        
        pass
    def writelines(self, content_list):
        
        pass

    def writelines(self, content_list):
        
        pass",snippet_123,4,"class MultiprocessingStringIO:
    def __init__(self):
        self._parts = []

    def getvalue(self):
        return ''.join(self._parts)

    def _write_from(self, content_list):
        if content_list is None:
            return
        if isinstance(content_list, (str, bytes)):
            self._parts.append(content_list if isinstance(content_list, str) else content_list.decode())
            return
        for item in content_list:
            if item is None:
                continue
            self._parts.append(item if isinstance(item, str) else str(item))

    def writelines(self, content_list):
        self._write_from(content_list)

    def writelines(self, content_list):
        self._write_from(content_list)"
222348,joshspeagle/dynesty,joshspeagle_dynesty/py/dynesty/utils.py,dynesty.utils.DelayTimer,"import time

class DelayTimer:
    """""" Utility class that allows us to detect a certain
    time has passed""""""

    def __init__(self, delay):
        """""" Initialise the time with delay of dt seconds

        Parameters
        ----------

        delay: float
            The number of seconds in the timer
        """"""
        self.delay = delay
        self.last_time = time.time()

    def is_time(self):
        """"""
        Returns true if more than self.dt seconds has passed
        since the initialization or last call of successful is_time()

        Returns
        -------
        ret: bool
             True if specified amout of time has passed since the
             initialization or last successful is_time() call
        """"""
        curt = time.time()
        if curt - self.last_time > self.delay:
            self.last_time = curt
            return True
        return False","
class DelayTimer:
    
    def __init__(self, delay):
        
        pass
    def is_time(self):
        
        pass",snippet_124,3,"import time

class DelayTimer:
    
    def __init__(self, delay):
        self.delay = float(delay)
        self._next = time.monotonic() + max(self.delay, 0.0)
        
    def is_time(self):
        if self.delay <= 0:
            return True
        now = time.monotonic()
        if now >= self._next:
            self._next = now + self.delay
            return True
        return False"
223977,jilljenn/tryalgo,jilljenn_tryalgo/tryalgo/fenwick.py,tryalgo.fenwick.FenwickMin,"class FenwickMin:
    """"""maintains a tree to allow quick updates and queries
    of a virtual table t
    """"""

    def __init__(self, size):
        """"""stores a table t and allows updates and queries
        of prefix sums in logarithmic time.

        :param size: length of the table
        """"""
        self.s = [float('+inf')] * (size + 1)

    def prefixMin(self, a):
        """"""
        :param int a: index in t, negative a will return infinity
        :returns: min(t[0], ... ,t[a])
        """"""
        i = a + 1
        retval = float('+inf')
        while i > 0:
            retval = min(retval, self.s[i])
            i -= i & -i
        return retval

    def update(self, a, val):
        """"""
        :param int a: index in t
        :param val: a value
        :modifies: sets t[a] to the minimum of t[a] and val
        """"""
        i = a + 1
        while i < len(self.s):
            self.s[i] = min(self.s[i], val)
            i += i & -i","class FenwickMin:
    
    def __init__(self, size):
        
        pass
    def prefixMin(self, a):
        
        pass
    def update(self, a, val):
        
        pass",snippet_125,4,"class FenwickMin:
    
    def __init__(self, size):
        self.n = size
        self.tree = [float('inf')] * (self.n + 1)

    def prefixMin(self, a):
        res = float('inf')
        i = a
        while i > 0:
            if self.tree[i] < res:
                res = self.tree[i]
            i -= i & -i
        return res

    def update(self, a, val):
        i = a
        while i <= self.n:
            if val < self.tree[i]:
                self.tree[i] = val
            i += i & -i"
223981,jilljenn/tryalgo,jilljenn_tryalgo/tryalgo/lowest_common_ancestor.py,tryalgo.lowest_common_ancestor.LowestCommonAncestorRMQ,"from tryalgo.range_minimum_query import RangeMinQuery

class LowestCommonAncestorRMQ:
    """"""Lowest common ancestor data structure using a reduction to
       range minimum query
    """"""

    def __init__(self, graph):
        """"""builds the structure from a given tree

        :param graph: adjacency matrix of a tree
        :complexity: O(n log n), with n = len(graph)
        """"""
        n = len(graph)
        dfs_trace = []
        self.last = [None] * n
        to_visit = [(0, 0, None)]
        succ = [0] * n
        while to_visit:
            level, node, father = to_visit[-1]
            self.last[node] = len(dfs_trace)
            dfs_trace.append((level, node))
            if succ[node] < len(graph[node]) and graph[node][succ[node]] == father:
                succ[node] += 1
            if succ[node] == len(graph[node]):
                to_visit.pop()
            else:
                neighbor = graph[node][succ[node]]
                succ[node] += 1
                to_visit.append((level + 1, neighbor, node))
        self.rmq = RangeMinQuery(dfs_trace, (float('inf'), None))

    def query(self, u, v):
        """""":returns: the lowest common ancestor of u and v
        :complexity: O(log n)
        """"""
        lu = self.last[u]
        lv = self.last[v]
        if lu > lv:
            lu, lv = (lv, lu)
        return self.rmq.range_min(lu, lv + 1)[1]","
class LowestCommonAncestorRMQ:
    
    def __init__(self, graph):
        '''builds the structure from a given tree
        :param graph: adjacency matrix of a tree
        :complexity: O(n log n), with n = len(graph)
        '''
        pass

    def query(self, u, v):
        
        pass",snippet_126,3,"class LowestCommonAncestorRMQ:
    
    def __init__(self, graph):
        '''builds the structure from a given tree
        :param graph: adjacency matrix of a tree
        :complexity: O(n log n), with n = len(graph)
        '''
        n = len(graph)
        # Build adjacency list from adjacency matrix
        adj = [[] for _ in range(n)]
        for i in range(n):
            row = graph[i]
            for j in range(n):
                if row[j]:
                    adj[i].append(j)

        # Euler tour + depths + first occurrence
        self.euler = []
        self.depth = []
        self.first = [-1] * n

        def dfs(u, p, d):
            self.first[u] = len(self.euler)
            self.euler.append(u)
            self.depth.append(d)
            for v in adj[u]:
                if v == p:
                    continue
                dfs(v, u, d + 1)
                self.euler.append(u)
                self.depth.append(d)

        # Root at 0
        if n > 0:
            dfs(0, -1, 0)

        m = len(self.euler)
        if m == 0:
            # Empty graph safety
            self.log = [0]
            self.st = [[0]]
            return

        # Precompute logs
        self.log = [0] * (m + 1)
        for i in range(2, m + 1):
            self.log[i] = self.log[i // 2] + 1

        # Build sparse table on depth indices
        K = self.log[m] + 1
        self.st = [[0] * m for _ in range(K)]
        for i in range(m):
            self.st[0][i] = i
        k = 1
        while k < K:
            length = 1 << k
            half = length >> 1
            last_start = m - length
            for i in range(0, last_start + 1):
                a = self.st[k - 1][i]
                b = self.st[k - 1][i + half]
                self.st[k][i] = a if self.depth[a] <= self.depth[b] else b
            k += 1

    def query(self, u, v):
        if u == v:
            return u
        i = self.first[u]
        j = self.first[v]
        if i > j:
            i, j = j, i
        length = j - i + 1
        k = self.log[length]
        a = self.st[k][i]
        b = self.st[k][j - (1 << k) + 1]
        idx = a if self.depth[a] <= self.depth[b] else b
        return self.euler[idx]"
223982,jilljenn/tryalgo,jilljenn_tryalgo/tryalgo/lowest_common_ancestor.py,tryalgo.lowest_common_ancestor.LowestCommonAncestorShortcuts,"class LowestCommonAncestorShortcuts:
    """"""Lowest common ancestor data structure using shortcuts to ancestors
    """"""

    def __init__(self, prec):
        """"""builds the structure from a given tree

        :param prec: father for every node, with prec[0] = 0
        :assumes: prec[node] < node
        :complexity: O(n log n), with n = len(nodes)
        """"""
        n = len(prec)
        self.level = [None] * n
        self.level[0] = 0
        for u in range(1, n):
            self.level[u] = 1 + self.level[prec[u]]
        depth = log2ceil(max((self.level[u] for u in range(n)))) + 1
        self.anc = [[0] * n for _ in range(depth)]
        for u in range(n):
            self.anc[0][u] = prec[u]
        for k in range(1, depth):
            for u in range(n):
                self.anc[k][u] = self.anc[k - 1][self.anc[k - 1][u]]

    def query(self, u, v):
        """""":returns: the lowest common ancestor of u and v
        :complexity: O(log n)
        """"""
        if self.level[u] > self.level[v]:
            u, v = (v, u)
        depth = len(self.anc)
        for k in range(depth - 1, -1, -1):
            if self.level[u] <= self.level[v] - (1 << k):
                v = self.anc[k][v]
        assert self.level[u] == self.level[v]
        if u == v:
            return u
        for k in range(depth - 1, -1, -1):
            if self.anc[k][u] != self.anc[k][v]:
                u = self.anc[k][u]
                v = self.anc[k][v]
        assert self.anc[0][u] == self.anc[0][v]
        return self.anc[0][u]","class LowestCommonAncestorShortcuts:
    
    def __init__(self, prec):
        '''builds the structure from a given tree
        :param prec: father for every node, with prec[0] = 0
        :assumes: prec[node] < node
        :complexity: O(n log n), with n = len(nodes)
        '''
        pass

    def query(self, u, v):
        
        pass",snippet_127,3,"class LowestCommonAncestorShortcuts:
    
    def __init__(self, prec):
        '''builds the structure from a given tree
        :param prec: father for every node, with prec[0] = 0
        :assumes: prec[node] < node
        :complexity: O(n log n), with n = len(nodes)
        '''
        self.n = len(prec)
        self.depth = [0] * self.n
        for i in range(1, self.n):
            self.depth[i] = self.depth[prec[i]] + 1

        LOG = max(1, self.n.bit_length())
        self.up = [[0] * self.n for _ in range(LOG)]
        for v in range(self.n):
            self.up[0][v] = prec[v]
        for k in range(1, LOG):
            prev = self.up[k - 1]
            curr = self.up[k]
            for v in range(self.n):
                curr[v] = prev[prev[v]]

        self.LOG = LOG

    def query(self, u, v):
        if self.depth[u] < self.depth[v]:
            u, v = v, u

        diff = self.depth[u] - self.depth[v]
        k = 0
        while diff:
            if diff & 1:
                u = self.up[k][u]
            diff >>= 1
            k += 1

        if u == v:
            return u

        for k in range(self.LOG - 1, -1, -1):
            if self.up[k][u] != self.up[k][v]:
                u = self.up[k][u]
                v = self.up[k][v]

        return self.up[0][u]"
223988,jilljenn/tryalgo,jilljenn_tryalgo/tryalgo/partition_refinement.py,tryalgo.partition_refinement.PartitionRefinement,"class PartitionRefinement:
    """"""This data structure implements an order preserving
    partition with refinements.
    """"""

    def __init__(self, n):
        """"""Start with the partition consisting of the unique class {0,1,..,n-1}
        complexity: O(n) both in time and space
        """"""
        c = PartitionClass()
        self.classes = c
        self.items = [PartitionItem(i, c) for i in range(n)]

    def refine(self, pivot):
        """"""Split every class C in the partition into C intersection pivot
        and C setminus pivot complexity: linear in size of pivot
        """"""
        has_split = []
        for i in pivot:
            if 0 <= i < len(self.items):
                x = self.items[i]
                c = x.theclass
                if not c.split:
                    c.split = PartitionClass(c)
                    if self.classes is c:
                        self.classes = c.split
                    has_split.append(c)
                x.remove()
                x.theclass = c.split
                c.split.append(x)
        for c in has_split:
            c.split = None
            if not c.items:
                c.remove()
                del c

    def tolist(self):
        """"""produce a list representation of the partition
        """"""
        return [[x.val for x in theclass.items] for theclass in self.classes]

    def order(self):
        """"""Produce a flatten list of the partition, ordered by classes
        """"""
        return [x.val for theclass in self.classes for x in theclass.items]","class PartitionRefinement:
    
    def __init__(self, n):
        
        pass
    def refine(self, pivot):
        
        pass
    def tolist(self):
        
        pass

    def order(self):
        
        pass",snippet_128,5,"class PartitionRefinement:
    
    def __init__(self, n):
        self.n = int(n)
        if self.n < 0:
            raise ValueError(""n must be non-negative"")
        self._blocks = [list(range(self.n))] if self.n > 0 else []

    def refine(self, pivot):
        pivot_set = set(pivot)
        if not self._blocks or not pivot_set:
            return
        new_blocks = []
        for block in self._blocks:
            if not block:
                continue
            in_pivot = []
            not_in_pivot = []
            for x in block:
                (in_pivot if x in pivot_set else not_in_pivot).append(x)
            if in_pivot and not_in_pivot:
                new_blocks.append(in_pivot)
                new_blocks.append(not_in_pivot)
            else:
                new_blocks.append(block)
        self._blocks = new_blocks
        
    def tolist(self):
        return [b[:] for b in self._blocks]

    def order(self):
        return [x for block in self._blocks for x in block]"
224105,rigetti/grove,rigetti_grove/grove/alpha/fermion_transforms/fenwick_tree.py,grove.alpha.fermion_transforms.fenwick_tree.FenwickNode,"class FenwickNode:
    """"""Fenwick Tree node.""""""
    parent = None
    children = None
    index = None

    def __init__(self, parent, children, index=None):
        """"""Fenwick Tree node. Single parent and multiple children.

        :param FenwickNode parent: a parent node
        :param list(FenwickNode) children: a list of children nodes
        :param int index: node label
        """"""
        self.children = children
        self.parent = parent
        self.index = index

    def get_ancestors(self):
        """"""Returns a list of ancestors of the node. Ordered from the earliest.

        :return: node's ancestors, ordered from most recent
        :rtype: list(FenwickNode)
        """"""
        node = self
        ancestor_list = []
        while node.parent is not None:
            ancestor_list.append(node.parent)
            node = node.parent
        return ancestor_list","class FenwickNode:
    
    def __init__(self, parent, children, index=None):
        
        pass
    def get_ancestors(self):
        
        pass",snippet_129,3,"class FenwickNode:
    
    def __init__(self, parent, children, index=None):
        self.parent = parent
        self.children = list(children) if children is not None else []
        self.index = index

        # Ensure bidirectional linkage with parent
        if self.parent is not None:
            if not hasattr(self.parent, ""children"") or self.parent.children is None:
                self.parent.children = []
            if self not in self.parent.children:
                self.parent.children.append(self)

        # Ensure children's parent points to this node
        for child in self.children:
            if child is not None:
                child.parent = self

    def get_ancestors(self):
        ancestors = []
        node = self.parent
        while node is not None:
            ancestors.append(node)
            node = node.parent
        return ancestors"
227838,econ-ark/HARK,HARK/utilities.py,HARK.utilities.NullFunc,"import numpy as np

class NullFunc:
    """"""
    A trivial class that acts as a placeholder ""do nothing"" function.
    """"""

    def __call__(self, *args):
        """"""
        Returns meaningless output no matter what the input(s) is.  If no input,
        returns None.  Otherwise, returns an array of NaNs (or a single NaN) of
        the same size as the first input.
        """"""
        if len(args) == 0:
            return None
        else:
            arg = args[0]
            if hasattr(arg, 'shape'):
                return np.zeros_like(arg) + np.nan
            else:
                return np.nan

    def distance(self, other):
        """"""
        Trivial distance metric that only cares whether the other object is also
        an instance of NullFunc.  Intentionally does not inherit from HARKobject
        as this might create dependency problems.

        Parameters
        ----------
        other : any
            Any object for comparison to this instance of NullFunc.

        Returns
        -------
        (unnamed) : float
            The distance between self and other.  Returns 0 if other is also a
            NullFunc; otherwise returns an arbitrary high number.
        """"""
        try:
            if other.__class__ is self.__class__:
                return 0.0
            else:
                return 1000.0
        except:
            return 10000.0","
class NullFunc:
    
    def __call__(self, *args):
        
        pass
    def distance(self, other):
        
        pass",snippet_131,3,"class NullFunc:
    
    def __call__(self, *args):
        return 0

    def distance(self, other):
        return 0.0 if isinstance(other, NullFunc) else float('inf')"
228145,jcrobak/parquet-python,jcrobak_parquet-python/parquet/__init__.py,parquet.JsonWriter,"import json

class JsonWriter:
    """"""Utility for dumping rows as JSON objects.""""""

    def __init__(self, out):
        """"""Initialize with output destination.""""""
        self._out = out

    def writerow(self, row):
        """"""Write a single row.""""""
        json_text = json.dumps(row)
        if isinstance(json_text, bytes):
            json_text = json_text.decode('utf-8')
        self._out.write(json_text)
        self._out.write(u'\n')","
class JsonWriter:
    
    def __init__(self, out):
        
        pass
    def writerow(self, row):
        
        pass",snippet_132,3,"import json

class JsonWriter:
    
    def __init__(self, out):
        if not hasattr(out, ""write""):
            raise TypeError(""out must be a writable file-like object"")
        self._out = out

    def writerow(self, row):
        self._out.write(json.dumps(row, ensure_ascii=False, default=str))
        self._out.write(""\n"")"
228682,common-workflow-language/cwltool,common-workflow-language_cwltool/cwltool/software_requirements.py,cwltool.software_requirements.DependenciesConfiguration,"import os
from typing import TYPE_CHECKING, Any, Optional, Union, cast
import argparse

class DependenciesConfiguration:
    """"""Dependency configuration class, for RuntimeContext.job_script_provider.""""""

    def __init__(self, args: argparse.Namespace) -> None:
        """"""Initialize.""""""
        self.tool_dependency_dir: Optional[str] = None
        self.dependency_resolvers_config_file: Optional[str] = None
        conf_file = getattr(args, 'beta_dependency_resolvers_configuration', None)
        tool_dependency_dir = getattr(args, 'beta_dependencies_directory', None)
        conda_dependencies = getattr(args, 'beta_conda_dependencies', None)
        if conf_file is not None and os.path.exists(conf_file):
            self.use_tool_dependencies = True
            if tool_dependency_dir is None:
                tool_dependency_dir = os.path.abspath(os.path.dirname(conf_file))
            self.tool_dependency_dir = tool_dependency_dir
            self.dependency_resolvers_config_file = os.path.abspath(conf_file)
        elif conda_dependencies is not None:
            if tool_dependency_dir is None:
                tool_dependency_dir = os.path.abspath('./cwltool_deps')
            self.tool_dependency_dir = tool_dependency_dir
            self.use_tool_dependencies = True
            self.dependency_resolvers_config_file = None
        else:
            self.use_tool_dependencies = False
        if self.tool_dependency_dir and (not os.path.exists(self.tool_dependency_dir)):
            os.makedirs(self.tool_dependency_dir)

    def build_job_script(self, builder: 'Builder', command: list[str]) -> str:
        """"""Use the galaxy-tool-util library to construct a build script.""""""
        ensure_galaxy_lib_available()
        resolution_config_dict = {'use': self.use_tool_dependencies, 'default_base_path': self.tool_dependency_dir}
        app_config = {'conda_auto_install': True, 'conda_auto_init': True, 'debug': builder.debug}
        tool_dependency_manager: 'deps.DependencyManager' = deps.build_dependency_manager(app_config_dict=app_config, resolution_config_dict=resolution_config_dict, conf_file=self.dependency_resolvers_config_file)
        handle_dependencies: str = ''
        if (dependencies := get_dependencies(builder)):
            handle_dependencies = '\n'.join(tool_dependency_manager.dependency_shell_commands(dependencies, job_directory=builder.tmpdir))
        template_kwds: dict[str, str] = dict(handle_dependencies=handle_dependencies)
        job_script = COMMAND_WITH_DEPENDENCIES_TEMPLATE.substitute(template_kwds)
        return job_script","
class DependenciesConfiguration:
    
    def __init__(self, args: argparse.Namespace) -> None:
        
        pass
    def build_job_script(self, builder: 'Builder', command: list[str]) -> str:
        
        pass",snippet_133,3,"import argparse
import shlex
import sys
from typing import Any, Dict, Iterable, List, Optional


class DependenciesConfiguration:
    def __init__(self, args: argparse.Namespace) -> None:
        # Resolve python executable
        self.python_executable: str = getattr(args, ""python"", sys.executable)

        # Dependencies handling
        pkgs = getattr(args, ""packages"", None)
        if pkgs is None:
            pkgs = getattr(args, ""deps"", None)
        self.packages: List[str] = list(pkgs) if pkgs else []

        self.requirements_file: Optional[str] = getattr(args, ""requirements"", None)
        self.enable_pip_install: bool = bool(getattr(args, ""pip_install"", True))

        # Environment variables
        env = getattr(args, ""env"", None)
        self.env: Dict[str, str] = dict(env) if isinstance(env, dict) else {}

        # Optional module system (e.g., LMOD)
        modules = getattr(args, ""modules"", None)
        self.modules: List[str] = list(modules) if modules else []

        # Extra setup commands (optional hook)
        setup_cmds = getattr(args, ""setup"", None)
        self.setup_commands: List[str] = list(setup_cmds) if setup_cmds else []

    def build_job_script(self, builder: ""Builder"", command: List[str]) -> str:
        lines: List[str] = [""#!/usr/bin/env bash"", ""set -euo pipefail""]

        # Load modules if requested (only if `module` command is available)
        if self.modules:
            lines.append('if command -v module >/dev/null 2>&1; then')
            for mod in self.modules:
                lines.append(f""  module load {shlex.quote(mod)}"")
            lines.append(""fi"")

        # Export environment variables
        for k, v in self.env.items():
            key = str(k)
            val = """" if v is None else str(v)
            lines.append(f""export {key}={shlex.quote(val)}"")

        # Additional setup commands, if any
        for cmd in self.setup_commands:
            lines.append(cmd)

        # Pip install dependencies if enabled
        if self.enable_pip_install and (self.requirements_file or self.packages):
            pip_cmd = f""{shlex.quote(self.python_executable)} -m pip""
            lines.append(f""{pip_cmd} install --upgrade pip"")
            if self.requirements_file:
                lines.append(f""{pip_cmd} install -r {shlex.quote(self.requirements_file)}"")
            if self.packages:
                pkg_str = "" "".join(shlex.quote(p) for p in self.packages)
                lines.append(f""{pip_cmd} install {pkg_str}"")

        # Final command
        if not command:
            raise ValueError(""command must be a non-empty list of strings"")
        quoted_cmd = "" "".join(shlex.quote(part) for part in command)
        lines.append(f""exec {quoted_cmd}"")

        return ""\n"".join(lines) + ""\n"""
229565,sourceperl/pyModbusTCP,sourceperl_pyModbusTCP/examples/client_serial_gw.py,client_serial_gw.Serial2ModbusClient,"from pyModbusTCP.constants import EXP_GATEWAY_TARGET_DEVICE_FAILED_TO_RESPOND
import struct

class Serial2ModbusClient:
    """""" Customize a slave serial worker for map a modbus TCP client. """"""

    def __init__(self, serial_w, mbus_cli, slave_addr=1, allow_bcast=False):
        """"""Serial2ModbusClient constructor.

        :param serial_w: a SlaveSerialWorker instance
        :type serial_w: SlaveSerialWorker
        :param mbus_cli: a ModbusClient instance
        :type mbus_cli: ModbusClient
        :param slave_addr: modbus slave address
        :type slave_addr: int
        :param allow_bcast: allow processing broadcast frames (slave @0)
        :type allow_bcast: bool
        """"""
        self.serial_w = serial_w
        self.mbus_cli = mbus_cli
        self.slave_addr = slave_addr
        self.allow_bcast = allow_bcast
        self.serial_w.handle_request = self._handle_request

    def _handle_request(self):
        """"""Request handler for SlaveSerialWorker""""""
        if self.serial_w.request.slave_addr == 0 and self.allow_bcast:
            self.mbus_cli.custom_request(self.serial_w.request.pdu)
        elif self.serial_w.request.slave_addr == self.slave_addr:
            resp_pdu = self.mbus_cli.custom_request(self.serial_w.request.pdu)
            if resp_pdu:
                self.serial_w.response.build(raw_pdu=resp_pdu, slave_addr=self.serial_w.request.slave_addr)
            else:
                exp_pdu = struct.pack('BB', self.serial_w.request.function_code + 128, EXP_GATEWAY_TARGET_DEVICE_FAILED_TO_RESPOND)
                self.serial_w.response.build(raw_pdu=exp_pdu, slave_addr=self.serial_w.request.slave_addr)

    def run(self):
        """"""Start serial processing.""""""
        self.serial_w.run()","
class Serial2ModbusClient:
    
    def __init__(self, serial_w, mbus_cli, slave_addr=1, allow_bcast=False):
        
        pass
    def _handle_request(self):
        
        pass

    def run(self):
        
        pass",snippet_134,4,"class Serial2ModbusClient:
    
    def __init__(self, serial_w, mbus_cli, slave_addr=1, allow_bcast=False):
        self.ser = serial_w
        self.cli = mbus_cli
        self.def_unit = slave_addr
        self.allow_bcast = allow_bcast
        self._alive = True

    def _write_json(self, obj):
        try:
            import json
            data = (json.dumps(obj, separators=("","", "":"")) + ""\n"").encode(""utf-8"")
        except Exception:
            # Fallback minimal encoding
            data = (str(obj) + ""\n"").encode(""utf-8"", ""ignore"")
        try:
            self.ser.write(data)
        except Exception:
            pass

    def _read_line(self):
        # Support file-like .readline or fallback to reading bytes until newline
        try:
            line = self.ser.readline()
            if isinstance(line, bytes):
                return line.decode(""utf-8"", ""ignore"")
            return line
        except AttributeError:
            # Fallback manual read
            buf = bytearray()
            while True:
                ch = self.ser.read(1)
                if not ch:
                    # Non-blocking or EOF
                    if buf:
                        break
                    return """"
                if isinstance(ch, str):
                    ch = ch.encode(""utf-8"", ""ignore"")
                if ch == b""\n"":
                    break
                buf.extend(ch)
            return buf.decode(""utf-8"", ""ignore"")
        except Exception:
            return """"

    def _parse_tokens(self, line):
        # Split respecting commas for value lists
        parts = line.strip().split()
        return parts

    def _as_bool(self, v):
        if isinstance(v, bool):
            return v
        if isinstance(v, (int, float)):
            return bool(int(v))
        s = str(v).strip().lower()
        if s in (""1"", ""true"", ""t"", ""on"", ""yes"", ""y""):
            return True
        if s in (""0"", ""false"", ""f"", ""off"", ""no"", ""n""):
            return False
        return bool(int(s))

    def _parse_values_list(self, token):
        # Accept comma-separated values
        vals = []
        for t in str(token).split("",""):
            t = t.strip()
            if t == """":
                continue
            if t.lower() in (""true"", ""false"", ""t"", ""f"", ""on"", ""off"", ""1"", ""0"", ""yes"", ""no"", ""y"", ""n""):
                vals.append(self._as_bool(t))
            else:
                vals.append(int(t, 0))
        return vals

    def _resp_ok(self, data=None):
        return {""ok"": True, ""data"": data}

    def _resp_err(self, msg):
        return {""ok"": False, ""error"": str(msg)}

    def _unwrap_pymodbus(self, resp):
        # Handle pymodbus style responses
        try:
            if hasattr(resp, ""isError"") and resp.isError():
                return None, str(resp)
            if hasattr(resp, ""bits""):
                return list(resp.bits), None
            if hasattr(resp, ""registers""):
                return list(resp.registers), None
            # Write responses might have .value or no payload
            if hasattr(resp, ""value""):
                return resp.value, None
            return None, None
        except Exception as e:
            return None, str(e)

    def _do_read(self, cmd, unit, addr, qty):
        try:
            if cmd in (""read_coils"", ""rc""):
                resp = self.cli.read_coils(addr, qty, unit=unit)
            elif cmd in (""read_discrete_inputs"", ""rdi""):
                resp = self.cli.read_discrete_inputs(addr, qty, unit=unit)
            elif cmd in (""read_holding_registers"", ""rhr""):
                resp = self.cli.read_holding_registers(addr, qty, unit=unit)
            elif cmd in (""read_input_registers"", ""rir""):
                resp = self.cli.read_input_registers(addr, qty, unit=unit)
            else:
                return self._resp_err(""unsupported read"")
        except Exception as e:
            return self._resp_err(e)
        data, err = self._unwrap_pymodbus(resp)
        if err:
            return self._resp_err(err)
        return self._resp_ok(data)

    def _do_write(self, cmd, unit, addr, vals):
        # Deny broadcast unless allowed, and only for writes
        if unit in (0, ""0""):
            if not self.allow_bcast:
                return self._resp_err(""broadcast not allowed"")
            unit = 0
        try:
            if cmd in (""write_single_coil"", ""wsc""):
                v = self._as_bool(vals[0])
                resp = self.cli.write_coil(addr, v, unit=unit)
            elif cmd in (""write_single_register"", ""wsr""):
                v = int(vals[0])
                resp = self.cli.write_register(addr, v, unit=unit)
            elif cmd in (""write_multiple_coils"", ""wmc""):
                bools = [self._as_bool(v) for v in vals]
                resp = self.cli.write_coils(addr, bools, unit=unit)
            elif cmd in (""write_multiple_registers"", ""wmr""):
                regs = [int(v) for v in vals]
                resp = self.cli.write_registers(addr, regs, unit=unit)
            else:
                return self._resp_err(""unsupported write"")
        except Exception as e:
            return self._resp_err(e)
        data, err = self._unwrap_pymodbus(resp)
        if err:
            return self._resp_err(err)
        return self._resp_ok(data)

    def _handle_request(self):
        line = self._read_line()
        if not line:
            return None
        line = line.strip()
        if not line:
            return None
        # Commands:
        # READS: rc|rdi|rhr|rir [unit] address qty
        # WRITES:
        #  wsc|wsr [unit] address value
        #  wmc|wmr [unit] address v1,v2,...
        # Special: unit 'bcast' or 0 to broadcast (writes only, if allowed)
        # Special: quit / stop
        tokens = self._parse_tokens(line)
        if not tokens:
            return None
        cmd = tokens[0].lower()
        if cmd in (""quit"", ""exit"", ""stop""):
            self._alive = False
            return self._resp_ok(""stopping"")
        # Determine if next token is unit
        idx = 1
        unit = self.def_unit
        if len(tokens) > 3:
            # Likely unit provided
            try:
                unit = int(tokens[idx], 0) if tokens[idx].lower() != ""bcast"" else 0
                idx += 1
            except Exception:
                pass
        # Address
        if len(tokens) <= idx:
            return self._resp_err(""missing address"")
        try:
            addr = int(tokens[idx], 0)
        except Exception as e:
            return self._resp_err(f""bad address: {e}"")
        idx += 1
        # Handle reads
        if cmd in (""read_coils"", ""rc"", ""read_discrete_inputs"", ""rdi"", ""read_holding_registers"", ""rhr"", ""read_input_registers"", ""rir""):
            if len(tokens) <= idx:
                return self._resp_err(""missing quantity"")
            try:
                qty = int(tokens[idx], 0)
            except Exception as e:
                return self._resp_err(f""bad quantity: {e}"")
            # Disallow broadcast for reads
            if unit == 0:
                return self._resp_err(""broadcast reads not allowed"")
            return self._do_read(cmd, unit, addr, qty)
        # Handle writes
        if cmd in (""write_single_coil"", ""wsc"", ""write_single_register"", ""wsr""):
            if len(tokens) <= idx:
                return self._resp_err(""missing value"")
            vals = [tokens[idx]]
            return self._do_write(cmd, unit, addr, vals)
        if cmd in (""write_multiple_coils"", ""wmc"", ""write_multiple_registers"", ""wmr""):
            if len(tokens) <= idx:
                return self._resp_err(""missing values"")
            vals = self._parse_values_list(tokens[idx])
            if not vals:
                return self._resp_err(""no values"")
            return self._do_write(cmd, unit, addr, vals)
        return self._resp_err(""unknown command"")

    def run(self):
        self._alive = True
        while self._alive:
            resp = self._handle_request()
            if resp is None:
                continue
            self._write_json(resp)"
230113,aws/aws-xray-sdk-python,aws_aws-xray-sdk-python/aws_xray_sdk/core/models/noop_traceid.py,aws_aws-xray-sdk-python.aws_xray_sdk.core.models.noop_traceid.NoOpTraceId,"class NoOpTraceId:
    """"""
    A trace ID tracks the path of a request through your application.
    A trace collects all the segments generated by a single request.
    A trace ID is required for a segment.
    """"""
    VERSION = '1'
    DELIMITER = '-'

    def __init__(self):
        """"""
        Generate a no-op trace id.
        """"""
        self.start_time = '00000000'
        self.__number = '000000000000000000000000'

    def to_id(self):
        """"""
        Convert TraceId object to a string.
        """"""
        return '%s%s%s%s%s' % (NoOpTraceId.VERSION, NoOpTraceId.DELIMITER, self.start_time, NoOpTraceId.DELIMITER, self.__number)","class NoOpTraceId:
    
    def __init__(self):
        
        pass
    def to_id(self):
        
        pass",snippet_135,3,"class NoOpTraceId:
    
    def __init__(self):
        self._id = None

    def to_id(self):
        return self._id"
230120,aws/aws-xray-sdk-python,aws_aws-xray-sdk-python/aws_xray_sdk/core/models/traceid.py,aws_aws-xray-sdk-python.aws_xray_sdk.core.models.traceid.TraceId,"import binascii
import os
import time

class TraceId:
    """"""
    A trace ID tracks the path of a request through your application.
    A trace collects all the segments generated by a single request.
    A trace ID is required for a segment.
    """"""
    VERSION = '1'
    DELIMITER = '-'

    def __init__(self):
        """"""
        Generate a random trace id.
        """"""
        self.start_time = int(time.time())
        self.__number = binascii.b2a_hex(os.urandom(12)).decode('utf-8')

    def to_id(self):
        """"""
        Convert TraceId object to a string.
        """"""
        return '%s%s%s%s%s' % (TraceId.VERSION, TraceId.DELIMITER, format(self.start_time, 'x'), TraceId.DELIMITER, self.__number)","
class TraceId:
    
    def __init__(self):
        
        pass
    def to_id(self):
        
        pass",snippet_136,3,"class TraceId:
    
    def __init__(self):
        import secrets
        tid = secrets.token_hex(16)
        while tid == ""0"" * 32:
            tid = secrets.token_hex(16)
        self._id = tid

    def to_id(self):
        return self._id"
230123,aws/aws-xray-sdk-python,aws_aws-xray-sdk-python/aws_xray_sdk/core/sampling/local/reservoir.py,aws_aws-xray-sdk-python.aws_xray_sdk.core.sampling.local.reservoir.Reservoir,"import time
import threading

class Reservoir:
    """"""
    Keeps track of the number of sampled segments within
    a single second. This class is implemented to be
    thread-safe to achieve accurate sampling.
    """"""

    def __init__(self, traces_per_sec=0):
        """"""
        :param int traces_per_sec: number of guranteed
            sampled segments.
        """"""
        self._lock = threading.Lock()
        self.traces_per_sec = traces_per_sec
        self.used_this_sec = 0
        self.this_sec = int(time.time())

    def take(self):
        """"""
        Returns True if there are segments left within the
        current second, otherwise return False.
        """"""
        with self._lock:
            now = int(time.time())
            if now != self.this_sec:
                self.used_this_sec = 0
                self.this_sec = now
            if self.used_this_sec >= self.traces_per_sec:
                return False
            self.used_this_sec = self.used_this_sec + 1
            return True","
class Reservoir:
    
    def __init__(self, traces_per_sec=0):
        
        pass
    def take(self):
        
        pass",snippet_137,3,"class Reservoir:
    
    def __init__(self, traces_per_sec=0):
        import time
        self._rate = float(traces_per_sec)
        self._capacity = max(0.0, self._rate)
        self._tokens = self._capacity
        self._last = time.monotonic()
        self._time_fn = time.monotonic
        try:
            import threading
            self._lock = threading.Lock()
        except Exception:
            self._lock = None

    def take(self):
        if self._rate <= 0.0:
            return False
        if self._lock is None:
            return self._take_nolock()
        with self._lock:
            return self._take_nolock()

    def _take_nolock(self):
        now = self._time_fn()
        elapsed = max(0.0, now - self._last)
        self._tokens = min(self._capacity, self._tokens + elapsed * self._rate)
        self._last = now
        if self._tokens >= 1.0:
            self._tokens -= 1.0
            return True
        return False"
230719,django-salesforce/django-salesforce,django-salesforce_django-salesforce/salesforce/backend/indep.py,salesforce.backend.indep.LazyField,"from typing import Any, Callable, Dict, Tuple, Type
from inspect import signature

class LazyField:
    """"""A Field that can be later customized until it is binded to the final Model""""""
    counter = 0

    def __init__(self, klass: 'Type[Field[Any, Any]]') -> None:
        """"""Instantiate the field type""""""
        self.klass = klass
        self.kw = {}
        self.args = ()
        self.called = False
        self.counter = self.counter

    def __call__(self, *args: Any, **kwargs: Any) -> 'LazyField':
        """"""Instantiate a new field with options""""""
        assert not self.called
        bound_args = signature(self.klass.__init__).bind(self, *args, **kwargs)
        obj = type(self)(self.klass)
        obj.args = bound_args.args[1:]
        obj.kw = bound_args.kwargs
        setattr(type(self), 'counter', getattr(type(self), 'counter') + 1)
        return obj

    def update(self, **kwargs: Any) -> 'LazyField':
        """"""Customize the lazy field""""""
        assert not self.called
        self.kw.update(kwargs)
        return self

    def create(self) -> 'Field[Any, Any]':
        """"""Create a normal field from the lazy field""""""
        assert not self.called
        return self.klass(*self.args, **self.kw)","
class LazyField:
    
    def __init__(self, klass: 'Type[Field[Any, Any]]') -> None:
        
        pass
    def __call__(self, *args: Any, **kwargs: Any) -> 'LazyField':
        
        pass

    def update(self, **kwargs: Any) -> 'LazyField':
        
        pass

    def create(self) -> 'Field[Any, Any]':
        
        pass",snippet_139,5,"from typing import Any, Dict, Tuple, Type

class LazyField:
    
    def __init__(self, klass: 'Type[Field[Any, Any]]') -> None:
        self._klass = klass
        self._args: Tuple[Any, ...] = ()
        self._kwargs: Dict[str, Any] = {}

    def __call__(self, *args: Any, **kwargs: Any) -> 'LazyField':
        self._args = args
        self._kwargs = dict(kwargs)
        return self

    def update(self, **kwargs: Any) -> 'LazyField':
        self._kwargs.update(kwargs)
        return self

    def create(self) -> 'Field[Any, Any]':
        return self._klass(*self._args, **self._kwargs)"
231223,weblyzard/inscriptis,benchmarking/run_benchmarking.py,run_benchmarking.AbstractHtmlConverter,"from time import time

class AbstractHtmlConverter:
    """"""
    An abstract HTML convert class.
    """"""

    def get_text(self, html):
        """"""
        Returns:
            a text representation of the given HTML snippet.
        """"""
        raise NotImplementedError

    def benchmark(self, html):
        """"""
        Benchmarks the classes HTML to text converter.

        Returns:
            A tuple of the required time and the obtained text representation.
        """"""
        start_time = time()
        for _ in range(TRIES):
            text = self.get_text(html)
        return (time() - start_time, text)","
class AbstractHtmlConverter:
    
    def get_text(self, html):
        
        pass
    def benchmark(self, html):
        
        pass",snippet_140,3,"```python
import re
import time
from html import unescape
from html.parser import HTMLParser
from typing import Dict, Optional


class AbstractHtmlConverter:
    class _TextExtractor(HTMLParser):
        def __init__(self) -> None:
            super().__init__(convert_charrefs=False)
            self._parts = []
            self._in_ignorable = 0  # counts nested script/style/svg/math elements
            self._last_was_space = False
            self._last_char: Optional[str] = None
            self._list_indent = 0

        def handle_starttag(self, tag, attrs):
            t = tag.lower()
            if t in (""script"", ""style"", ""svg"", ""math""):
                self._in_ignorable += 1
                return

            if t in (""br"",):
                self._newline()
            elif t in (""p"", ""div"", ""section"", ""article"", ""header"", ""footer"", ""aside"", ""main"", ""figure""):
                self._block_break()
            elif t in (""h1"", ""h2"", ""h3"", ""h4"", ""h5"", ""h6""):
                self._block_break()
            elif t in (""ul"", ""ol""):
                self._block_break()
            elif t == ""li"":
                self._block_break()
                self._emit(""• "")
                self._list_indent += 1
            elif t in (""tr"",):
                self._block_break()
            elif t in (""td"", ""th""):
                self._emit(""\t"")

        def handle_endtag(self, tag):
            t = tag.lower()
            if t in (""script"", ""style"", ""svg"", ""math""):
                if self._in_ignorable > 0:
                    self._in_ignorable -= 1
                return

            if t in (""p"", ""div"", ""section"", ""article"", ""header"", ""footer"", ""aside"", ""main"", ""figure""):
                self._block_break()
            elif t in (""h1"", ""h2"", ""h3"", ""h4"", ""h5"", ""h6""):
                self._block_break()
            elif t in (""ul"", ""ol""):
                self._block_break()
            elif t == ""li"":
                self._list_indent = max(0, self._list_indent - 1)
                self._newline()
            elif t in (""tr"",):
                self._newline()
            elif t in (""td"", ""th""):
                self._emit(""\t"")

        def handle_data(self, data):
            if self._in_ignorable:
                return
            if not data:
                return
            text = unescape(data)
            # Normalize internal whitespace to single spaces but preserve newlines if present
            # Split by lines to keep newlines inserted by tags meaningful
            for chunk in re.split(r""(\r?\n)"", text):
                if chunk in (""\n"", ""\r\n""):
                    self._newline()
                else:
                    self._emit(self._normalize_spaces(chunk))

        def handle_entityref(self, name):
            if self._in_ignorable:
                return
            self._emit(unescape(f""&{name};""))

        def handle_charref(self, name):
            if self._in_ignorable:
                return
            self._emit(unescape(f""&#{name};""))

        def _emit(self, s: str):
            if not s:
                return
            # Collapse spaces, avoid duplicating spaces around boundaries
            for ch in s:
                if ch == ""\n"":
                    self._newline()
                    continue
                if ch.isspace():
                    if not self._last_was_space and self._last_char not in (None, ""\n""):
                        self._parts.append("" "")
                        self._last_char = "" ""
                    self._last_was_space = True
                else:
                    self._parts.append(ch)
                    self._last_was_space = False
                    self._last_char = ch

        def _normalize_spaces(self, s: str) -> str:
            return re.sub(r""[ \t\f\v]+"", "" "", s)

        def _newline(self):
            if self._parts and self._parts[-1] != ""\n"":
                self._parts.append(""\n"")
                self._last_char = ""\n""
                self._last_was_space = False

        def _block_break(self):
            # Ensure a blank line between blocks
            if not self._parts:
                return
            # If last char isn't a newline, add one
            if self._parts[-1] != ""\n"":
                self._parts.append(""\n"")
            # Ensure exactly one blank line (two newlines)
            if len(self._parts) >= 2 and (self._parts[-2] != ""\n""):
                self._parts.append(""\n"")
            self._last_char = ""\n""
            self._last_was_space = False

        def text(self) -> str:
            out = """".join(self._parts)
            # Trim trailing spaces on each line
            out = ""\n"".join(line.rstrip() for line in out.splitlines())
            # Collapse more than two consecutive newlines into exactly two
            out = re.sub(r""\n{3,}"", ""\n\n"", out)
            return out.strip()

    def get_text(self, html: str) -> str:
        parser = self._TextExtractor()
        try:
            parser.feed(html or """")
            parser.close()
        except Exception:
            # Best-effort fallback: strip tags with a regex and unescape entities
            raw = re.sub(r""(?is)<(script|style|svg|math).*?</\1>"", "" "", html or """")
            raw = re.sub(r""(?s)<br\s*/?>"", ""\n"", raw, flags=re.I)
            raw = re.sub(r""(?is)</?(p|div|section|article|header|footer|aside|main|h[1-6]|li|ul|ol|tr|table)>"", ""\n"", raw)
            raw = re.sub(r""(?s)<[^>]+>"", "" "", raw)
            raw = unescape(raw)
            raw = re.sub(r""[ \t\f\v]+"", "" "", raw)
            raw = re.sub(r""\n{3,}"", ""\n\n"", raw)
            return ""\n"".join(line.rstrip() for line in raw.splitlines()).strip()
        return parser.text()

    def benchmark(self, html: str, iterations: int = 100) -> Dict[str, float]:
        if iterations <= 0:
            iterations = 1
        # Warm-up
        _ = self.get_text(html)
        start = time.perf_counter()
        last_text = """"
        for _i in range(iterations):
            last_text = self.get_text(html)
        duration = time.perf_counter() - start
        per_iter = duration / iterations
        return {
            ""iterations"": float(iterations),
            ""seconds_total"": float(duration),
            ""seconds_per_iteration"": float(per_iter),
            ""chars"": float(len(last_text)),
        }
```"
236113,richardchien/python-aiocqhttp,richardchien_python-aiocqhttp/aiocqhttp/api.py,aiocqhttp.api.Api,"from typing import Callable, Any, Union, Awaitable
import functools
import abc

class Api:
    """"""
    API 接口类。

    继承此类的具体实现类应实现 `call_action` 方法。
    """"""

    @abc.abstractmethod
    def call_action(self, action: str, **params) -> Union[Awaitable[Any], Any]:
        """"""
        调用 OneBot API，`action` 为要调用的 API 动作名，`**params`
        为 API 所需参数。

        根据实现类的不同，此函数可能是异步也可能是同步函数。
        """"""
        pass

    def __getattr__(self, item: str) -> Callable[..., Union[Awaitable[Any], Any]]:
        """"""获取一个可调用对象，用于调用对应 API。""""""
        return functools.partial(self.call_action, item)","
class Api:
    
    @abc.abstractmethod
    def call_action(self, action: str, **params) -> Union[Awaitable[Any], Any]:
        
        pass
    def __getattr__(self, item: str) -> Callable[..., Union[Awaitable[Any], Any]]:
        
        pass",snippet_141,3,"from typing import Any, Awaitable, Callable, Union
import abc


class Api(abc.ABC):

    @abc.abstractmethod
    def call_action(self, action: str, **params) -> Union[Awaitable[Any], Any]:
        raise NotImplementedError

    def __getattr__(self, item: str) -> Callable[..., Union[Awaitable[Any], Any]]:
        if item.startswith(""_""):
            raise AttributeError(item)

        def _caller(*args: Any, **kwargs: Any) -> Union[Awaitable[Any], Any]:
            if args:
                if len(args) == 1 and isinstance(args[0], dict):
                    return self.call_action(item, **{**args[0], **kwargs})
                raise TypeError(f""{item}() accepts only keyword arguments or a single dict positional argument"")
            return self.call_action(item, **kwargs)

        return _caller"
237136,project-generator/project_generator,project-generator_project_generator/project_generator/project.py,project_generator.project.ProjectTemplate,"class ProjectTemplate:
    """""" Public data which can be set in yaml files
        Yaml data available are:
            'build_dir' : build_dir,    # Build output path
            'debugger' : debugger,      # Debugger
            'export_dir': '',           # Export directory path
            'includes': [],             # include paths
            'linker_file': None,        # linker script file
            'name': name,               # project name
            'macros': [],               # macros
            'misc': {},                 # misc settings related to tools
            'output_type': output_type, # output type, default - exe
            'sources': [],              # source files/folders
            'target': '',               # target
            'template' : [],            # tool template
            'tools_supported': [],      # Tools which are supported,
    """"""

    @staticmethod
    def _get_common_data_template():
        """""" Data for tool specific """"""
        data_template = {'includes': [], 'linker_file': '', 'macros': [], 'sources': []}
        return data_template

    @staticmethod
    def _get_tool_specific_data_template():
        """""" Data for tool specific """"""
        data_template = {'misc': {}, 'template': []}
        return data_template

    @staticmethod
    def get_project_template(name='Default', output_type='exe', debugger=None, build_dir='build'):
        """""" Project data (+ data) """"""
        project_template = {'build_dir': build_dir, 'debugger': debugger, 'export_dir': '', 'name': name, 'output_type': output_type, 'target': '', 'tools_supported': []}
        project_template.update(ProjectTemplate._get_common_data_template())
        project_template.update(ProjectTemplate._get_tool_specific_data_template())
        return project_template","class ProjectTemplate:
    
    @staticmethod
    def _get_common_data_template():
        
        pass
    @staticmethod
    def _get_tool_specific_data_template():
        
        pass
    @staticmethod
    def get_project_template(name='Default', output_type='exe', debugger=None, build_dir='build'):
        
        pass",snippet_142,4,"class ProjectTemplate:
    
    @staticmethod
    def _get_common_data_template():
        return {
            'name': 'Default',
            'type': 'exe',
            'version': '1.0.0',
            'description': '',
            'entry_point': 'main',
            'debugger': None,
            'sources': [],
            'includes': [],
            'resources': [],
            'defines': [],
            'dependencies': [],
            'build': {
                'dir': 'build',
                'configuration': 'Debug',
                'configs': {
                    'Debug': {
                        'defines': ['DEBUG'],
                        'opt_level': 0,
                        'debug_symbols': True,
                        'warnings_as_errors': False
                    },
                    'Release': {
                        'defines': ['NDEBUG'],
                        'opt_level': 2,
                        'debug_symbols': False,
                        'warnings_as_errors': False
                    }
                }
            }
        }

    @staticmethod
    def _get_tool_specific_data_template():
        return {
            'tools': {
                'gcc': {
                    'cc': 'gcc',
                    'cxx': 'g++',
                    'ar': 'ar',
                    'ld': 'g++',
                    'defines_flag': '-D',
                    'include_flag': '-I',
                    'out_exe_flag': '-o',
                    'shared_flag': '-shared',
                    'std': {'c': 'c11', 'c++': 'c++17'},
                    'cflags': [],
                    'cxxflags': [],
                    'ldflags': [],
                    'env': {}
                },
                'clang': {
                    'cc': 'clang',
                    'cxx': 'clang++',
                    'ar': 'llvm-ar',
                    'ld': 'clang++',
                    'defines_flag': '-D',
                    'include_flag': '-I',
                    'out_exe_flag': '-o',
                    'shared_flag': '-shared',
                    'std': {'c': 'c11', 'c++': 'c++20'},
                    'cflags': [],
                    'cxxflags': [],
                    'ldflags': [],
                    'env': {}
                },
                'msvc': {
                    'cc': 'cl',
                    'cxx': 'cl',
                    'ar': 'lib',
                    'ld': 'link',
                    'defines_flag': '/D',
                    'include_flag': '/I',
                    'out_exe_flag': '/Fe:',
                    'out_obj_flag': '/Fo:',
                    'shared_flag': '/DLL',
                    'std': {'c': 'c11', 'c++': 'c++17'},
                    'cflags': [],
                    'cxxflags': [],
                    'ldflags': [],
                    'env': {}
                }
            }
        }

    @staticmethod
    def get_project_template(name='Default', output_type='exe', debugger=None, build_dir='build'):
        common = ProjectTemplate._get_common_data_template()
        tool_specific = ProjectTemplate._get_tool_specific_data_template()

        # Shallow merge since templates are disjoint at top-level
        template = {**common, **tool_specific}

        # Normalize output_type
        valid_types = {'exe', 'dll', 'shared', 'so', 'dylib', 'static', 'lib'}
        if output_type not in valid_types:
            output_type = 'exe'
        # Normalize to broad categories
        if output_type in {'dll', 'shared', 'so', 'dylib'}:
            normalized_type = 'shared'
        elif output_type in {'static', 'lib'}:
            normalized_type = 'static'
        else:
            normalized_type = 'exe'

        template['name'] = name
        template['type'] = normalized_type
        template['debugger'] = debugger
        template['build']['dir'] = build_dir

        return template"
237503,astropy/photutils,photutils/psf/model_plotting.py,photutils.psf.model_plotting.ModelGridPlotMixin,"from astropy.utils import minversion
import numpy as np
from astropy.visualization import simple_norm
import astropy

class ModelGridPlotMixin:
    """"""
    Mixin class to plot a grid of ePSF models.
    """"""

    def _reshape_grid(self, data):
        """"""
        Reshape the 3D ePSF grid as a 2D array of horizontally and
        vertically stacked ePSFs.

        Parameters
        ----------
        data : `numpy.ndarray`
            The 3D array of ePSF data.

        Returns
        -------
        reshaped_data : `numpy.ndarray`
            The 2D array of ePSF data.
        """"""
        nypsfs = self._ygrid.shape[0]
        nxpsfs = self._xgrid.shape[0]
        ny, nx = self.data.shape[1:]
        data.shape = (nypsfs, nxpsfs, ny, nx)
        return data.transpose([0, 2, 1, 3]).reshape(nypsfs * ny, nxpsfs * nx)

    def plot_grid(self, *, ax=None, vmax_scale=None, peak_norm=False, deltas=False, cmap='viridis', dividers=True, divider_color='darkgray', divider_ls='-', figsize=None):
        """"""
        Plot the grid of ePSF models.

        Parameters
        ----------
        ax : `matplotlib.axes.Axes` or `None`, optional
            The matplotlib axes on which to plot. If `None`, then the
            current `~matplotlib.axes.Axes` instance is used.

        vmax_scale : float, optional
            Scale factor to apply to the image stretch limits. This
            value is multiplied by the peak ePSF value to determine the
            plotting ``vmax``. The defaults are 1.0 for plotting the
            ePSF data and 0.03 for plotting the ePSF difference data
            (``deltas=True``). If ``deltas=True``, the ``vmin`` is set
            to ``-vmax``. If ``deltas=False`` the ``vmin`` is set to
            ``vmax`` / 1e4.

        peak_norm : bool, optional
            Whether to normalize the ePSF data by the peak value. The
            default shows the ePSF flux per pixel.

        deltas : bool, optional
            Set to `True` to show the differences between each ePSF
            and the average ePSF.

        cmap : str or `matplotlib.colors.Colormap`, optional
            The colormap to use. The default is 'viridis'.

        dividers : bool, optional
            Whether to show divider lines between the ePSFs.

        divider_color, divider_ls : str, optional
            Matplotlib color and linestyle options for the divider
            lines between ePSFs. These keywords have no effect unless
            ``show_dividers=True``.

        figsize : (float, float), optional
            The figure (width, height) in inches.

        Returns
        -------
        fig : `matplotlib.figure.Figure`
            The matplotlib figure object. This will be the current
            figure if ``ax=None``. Use ``fig.savefig()`` to save the
            figure to a file.

        Notes
        -----
        This method returns a figure object. If you are using this
        method in a script, you will need to call ``plt.show()`` to
        display the figure. If you are using this method in a Jupyter
        notebook, the figure will be displayed automatically.

        When in a notebook, if you do not store the return value of this
        function, the figure will be displayed twice due to the REPL
        automatically displaying the return value of the last function
        call. Alternatively, you can append a semicolon to the end of
        the function call to suppress the display of the return value.
        """"""
        import matplotlib.pyplot as plt
        from mpl_toolkits.axes_grid1 import make_axes_locatable
        data = self.data.copy()
        if deltas:
            mask = np.zeros(data.shape[0], dtype=bool)
            for i, arr in enumerate(data):
                if np.count_nonzero(arr) == 0:
                    mask[i] = True
            data -= np.mean(data[~mask], axis=0)
            data[mask] = 0.0
        data = self._reshape_grid(data)
        if ax is None:
            if figsize is None and self.meta.get('detector', '') == 'NRCSW':
                figsize = (20, 8)
            fig, ax = plt.subplots(figsize=figsize)
        else:
            fig = plt.gcf()
        if peak_norm and data.max() != 0:
            data /= data.max()
        if deltas:
            if vmax_scale is None:
                vmax_scale = 0.03
            vmax = data.max() * vmax_scale
            vmin = -vmax
            if minversion(astropy, '6.1'):
                norm = simple_norm(data, 'linear', vmin=vmin, vmax=vmax)
            else:
                norm = simple_norm(data, 'linear', min_cut=vmin, max_cut=vmax)
        else:
            if vmax_scale is None:
                vmax_scale = 1.0
            vmax = data.max() * vmax_scale
            vmin = vmax / 10000.0
            if minversion(astropy, '6.1'):
                norm = simple_norm(data, 'log', vmin=vmin, vmax=vmax, log_a=10000.0)
            else:
                norm = simple_norm(data, 'log', min_cut=vmin, max_cut=vmax, log_a=10000.0)
        nypsfs = self._ygrid.shape[0]
        nxpsfs = self._xgrid.shape[0]
        extent = [-0.5, nxpsfs - 0.5, -0.5, nypsfs - 0.5]
        axim = ax.imshow(data, extent=extent, norm=norm, cmap=cmap, origin='lower')
        xticklabels = self._xgrid.astype(int)
        yticklabels = self._ygrid.astype(int)
        if self.meta.get('detector', '') == 'NRCSW':
            xticklabels = list(xticklabels[0:5]) * 4
            yticklabels = list(yticklabels[0:5]) * 2
        ax.set_xticks(np.arange(nxpsfs))
        ax.set_xticklabels(xticklabels)
        ax.set_xlabel('ePSF location in detector X pixels')
        ax.set_yticks(np.arange(nypsfs))
        ax.set_yticklabels(yticklabels)
        ax.set_ylabel('ePSF location in detector Y pixels')
        if dividers:
            for ix in range(nxpsfs - 1):
                ax.axvline(ix + 0.5, color=divider_color, ls=divider_ls)
            for iy in range(nypsfs - 1):
                ax.axhline(iy + 0.5, color=divider_color, ls=divider_ls)
        instrument = self.meta.get('instrument', '')
        if not instrument:
            instrument = self.meta.get('instrume', '')
        detector = self.meta.get('detector', '')
        filtername = self.meta.get('filter', '')
        if isinstance(instrument, (tuple, list, np.ndarray)):
            instrument = instrument[0]
        if isinstance(detector, (tuple, list, np.ndarray)):
            detector = detector[0]
        if isinstance(filtername, (tuple, list, np.ndarray)):
            filtername = filtername[0]
        title = f'{instrument} {detector} {filtername}'
        if title != '':
            title += ' '
        if deltas:
            minus = '−'
            ax.set_title(f'{title}(ePSFs {minus} <ePSF>)')
            if peak_norm:
                label = 'Difference relative to average ePSF peak'
            else:
                label = 'Difference relative to average ePSF values'
        else:
            ax.set_title(f'{title}ePSFs')
            if peak_norm:
                label = 'Scale relative to ePSF peak pixel'
            else:
                label = 'ePSF flux per pixel'
        divider = make_axes_locatable(ax)
        cax_cbar = divider.append_axes('right', size='3%', pad='3%')
        cbar = fig.colorbar(axim, cax=cax_cbar, label=label)
        if not deltas:
            cbar.ax.set_yscale('log')
        if self.meta.get('detector', '') == 'NRCSW':
            nxpsfs = len(self._xgrid)
            nypsfs = len(self._ygrid)
            plt.axhline(nypsfs / 2 - 0.5, color='orange')
            for i in range(1, 4):
                ax.axvline(nxpsfs / 4 * i - 0.5, color='orange')
            det_labels = [['A1', 'A3', 'B4', 'B2'], ['A2', 'A4', 'B3', 'B1']]
            for i in range(2):
                for j in range(4):
                    ax.text(j * nxpsfs / 4 - 0.45, (i + 1) * nypsfs / 2 - 0.55, det_labels[i][j], color='orange', verticalalignment='top', fontsize=12)
        fig.tight_layout()
        return fig","
class ModelGridPlotMixin:
    
    def _reshape_grid(self, data):
        
        pass
    def plot_grid(self, *, ax=None, vmax_scale=None, peak_norm=False, deltas=False, cmap='viridis', dividers=True, divider_color='darkgray', divider_ls='-', figsize=None):
        
        pass",snippet_143,3,"import numpy as np
import math

class ModelGridPlotMixin:
    
    def _reshape_grid(self, data):
        arr = np.asarray(data)
        # Already 2D
        shape = self._infer_grid_shape(arr.size)
        if arr.ndim == 2:
            if shape is not None and arr.shape != tuple(shape):
                # If data is transposed or mismatched but has same size, try to reshape
                if arr.size == shape[0] * shape[1]:
                    return arr.reshape(shape)
            return arr
        # If 1D or other, try to reshape into grid
        if shape is not None:
            total = shape[0] * shape[1]
            if arr.size == total:
                return arr.reshape(shape)
            # If it is a stacked set of grids, reshape last
            if arr.size % total == 0:
                out = arr.reshape((*shape, arr.size // total))
                # Squeeze trailing singleton if present
                return np.squeeze(out, axis=-1) if out.shape[-1] == 1 else out
        # As a fallback, try square
        n = arr.size
        s = int(round(math.sqrt(n)))
        if s * s == n:
            return arr.reshape((s, s))
        raise ValueError(""Cannot reshape data to a 2D grid. Provide compatible data or define grid shape on the object."")
    
    def plot_grid(self, *, ax=None, vmax_scale=None, peak_norm=False, deltas=False, cmap='viridis', dividers=True, divider_color='darkgray', divider_ls='-', figsize=None):
        import matplotlib.pyplot as plt

        # Retrieve data to plot
        data = getattr(self, 'grid_data', None)
        if data is None:
            data = getattr(self, 'data', None)
        if data is None:
            raise AttributeError(""No grid data found. Expected attribute 'grid_data' or 'data' on the object."")
        arr = np.asarray(data)
        grid = self._reshape_grid(arr)

        # Reduce higher dimensions by selecting the first slice if needed
        if grid.ndim > 2:
            grid = grid[..., 0]

        # Transformations
        g = grid.astype(float)
        if deltas:
            g = g - np.nanmin(g)
        if peak_norm:
            peak = np.nanmax(np.abs(g))
            if peak != 0 and np.isfinite(peak):
                g = g / peak

        # vmin/vmax handling
        vmin = np.nanmin(g)
        vmax = np.nanmax(g)
        if vmax_scale is not None and np.isfinite(vmax):
            if peak_norm:
                vmax = 1.0 * float(vmax_scale)
            else:
                vmax = float(vmax) * float(vmax_scale)

        # Figure/Axes
        created_fig = False
        if ax is None:
            fig, ax = plt.subplots(figsize=figsize)
            created_fig = True
        else:
            fig = ax.figure

        # Determine extent from axes values if available
        ny, nx = g.shape
        xv, yv = self._infer_axis_values(nx, ny)
        origin = 'lower'
        extent = None
        if xv is not None and yv is not None and len(xv) == nx and len(yv) == ny:
            # Compute edges for pcolormesh-like extent
            def edges_from_centers(c):
                c = np.asarray(c, dtype=float)
                if c.size == 1:
                    w = 0.5 if not np.isnan(c[0]) else 0.5
                    return np.array([c[0] - w, c[0] + w])
                d = np.diff(c)
                left = c[0] - d[0] / 2
                right = c[-1] + d[-1] / 2
                mids = c[:-1] + d / 2
                return np.r_[left, mids, right]
            xedges = edges_from_centers(xv)
            yedges = edges_from_centers(yv)
            extent = (xedges[0], xedges[-1], yedges[0], yedges[-1])

        im = ax.imshow(g, origin=origin, cmap=cmap, vmin=vmin, vmax=vmax, aspect='auto', extent=extent, interpolation='nearest')

        # Dividers
        if dividers:
            if extent is None:
                # Draw at cell boundaries in image coordinates
                for i in range(1, nx):
                    ax.axvline(i - 0.5, color=divider_color, linestyle=divider_ls, linewidth=0.6)
                for j in range(1, ny):
                    ax.axhline(j - 0.5, color=divider_color, linestyle=divider_ls, linewidth=0.6)
            else:
                # Draw at edges derived from centers
                xedges = np.linspace(extent[0], extent[1], nx + 1)
                yedges = np.linspace(extent[2], extent[3], ny + 1)
                for i in range(1, nx):
                    ax.axvline(xedges[i], color=divider_color, linestyle=divider_ls, linewidth=0.6)
                for j in range(1, ny):
                    ax.axhline(yedges[j], color=divider_color, linestyle=divider_ls, linewidth=0.6)

        fig.colorbar(im, ax=ax)
        if created_fig and figsize is not None:
            fig.set_size_inches(figsize[0], figsize[1], forward=True)
        return ax

    # Helper methods
    def _infer_grid_shape(self, n_items):
        # Try common attributes for shape
        for attr in ('grid_shape', 'shape'):
            shp = getattr(self, attr, None)
            if isinstance(shp, (tuple, list)) and len(shp) == 2 and all(isinstance(v, (int, np.integer)) for v in shp):
                return (int(shp[0]), int(shp[1]))
        # From counts of x/y vectors
        candidates = [
            ('x_grid', 'y_grid'),
            ('grid_x', 'grid_y'),
            ('x_values', 'y_values'),
            ('xs', 'ys'),
            ('x', 'y'),
        ]
        for xa, ya in candidates:
            xv = getattr(self, xa, None)
            yv = getattr(self, ya, None)
            if xv is not None and yv is not None:
                try:
                    nx = len(xv)
                    ny = len(yv)
                    if nx > 0 and ny > 0:
                        return (ny, nx)
                except Exception:
                    pass
        # From grid coords array of shape (N, 2)
        coords = getattr(self, 'grid_coords', None)
        if coords is not None:
            c = np.asarray(coords)
            if c.ndim == 2 and c.shape[1] >= 2:
                nx = np.unique(c[:, 0]).size
                ny = np.unique(c[:, 1]).size
                if nx * ny == n_items:
                    return (ny, nx)
        # From param grid as sequences
        pg = getattr(self, 'param_grid', None)
        if isinstance(pg, (list, tuple)) and len(pg) >= 2:
            try:
                nx = len(pg[0])
                ny = len(pg[1])
                if nx > 0 and ny > 0:
                    if nx * ny == n_items:
                        return (ny, nx)
            except Exception:
                pass
        # nx, ny attributes
        nx = getattr(self, 'nx', None)
        ny = getattr(self, 'ny', None)
        if isinstance(nx, (int, np.integer)) and isinstance(ny, (int, np.integer)):
            return (int(ny), int(nx))
        # If n_items is a perfect square, use square
        s = int(round(math.sqrt(n_items)))
        if s * s == n_items:
            return (s, s)
        return None

    def _infer_axis_values(self, nx, ny):
        # Returns (x_values, y_values) or (None, None)
        # Prefer explicit attributes
        candidates = [
            ('x_grid', 'y_grid'),
            ('grid_x', 'grid_y'),
            ('x_values', 'y_values'),
            ('xs', 'ys'),
            ('x', 'y'),
        ]
        for xa, ya in candidates:
            xv = getattr(self, xa, None)
            yv = getattr(self, ya, None)
            if xv is not None and yv is not None:
                try:
                    xv = np.asarray(xv).reshape(-1)
                    yv = np.asarray(yv).reshape(-1)
                    if len(xv) == nx and len(yv) == ny:
                        return xv, yv
                except Exception:
                    continue
        return None, None"
237588,matthew-brett/delocate,matthew-brett_delocate/delocate/tmpdirs.py,delocate.tmpdirs.InGivenDirectory,"import os

class InGivenDirectory:
    """"""Change directory to given directory for duration of ``with`` block.

    Useful when you want to use `InTemporaryDirectory` for the final test, but
    you are still debugging.  For example, you may want to do this in the end:

    >>> with InTemporaryDirectory() as tmpdir:
    ...     # do something complicated which might break
    ...     pass

    But indeed the complicated thing does break, and meanwhile the
    ``InTemporaryDirectory`` context manager wiped out the directory with the
    temporary files that you wanted for debugging.  So, while debugging, you
    replace with something like:

    >>> with InGivenDirectory() as tmpdir: # Use working directory by default
    ...     # do something complicated which might break
    ...     pass

    You can then look at the temporary file outputs to debug what is happening,
    fix, and finally replace ``InGivenDirectory`` with ``InTemporaryDirectory``
    again.
    """"""

    def __init__(self, path=None):
        """"""Initialize directory context manager.

        Parameters
        ----------
        path : None or str, optional
            path to change directory to, for duration of ``with`` block.
            Defaults to ``os.getcwd()`` if None
        """"""
        if path is None:
            path = os.getcwd()
        self.path = os.path.abspath(path)

    def __enter__(self):
        """"""Chdir to the managed directory, creating it if needed.""""""
        self._pwd = os.path.abspath(os.getcwd())
        if not os.path.isdir(self.path):
            os.mkdir(self.path)
        os.chdir(self.path)
        return self.path

    def __exit__(self, exc, value, tb):
        """"""Revert the working directory.""""""
        os.chdir(self._pwd)","
class InGivenDirectory:
    
    def __init__(self, path=None):
        
        pass
    def __enter__(self):
        
        pass

    def __exit__(self, exc, value, tb):
        
        pass",snippet_144,4,"import os

class InGivenDirectory:
    
    def __init__(self, path=None):
        self.path = path
        self._old_cwd = None

    def __enter__(self):
        self._old_cwd = os.getcwd()
        if self.path is not None:
            target = os.path.abspath(os.path.expanduser(os.path.expandvars(str(self.path))))
            os.chdir(target)
        return self

    def __exit__(self, exc, value, tb):
        if self._old_cwd is not None:
            try:
                os.chdir(self._old_cwd)
            finally:
                self._old_cwd = None
        return False"
239516,blink1073/oct2py,oct2py/dynamic.py,oct2py.dynamic._MethodDocDescriptor,"class _MethodDocDescriptor:
    """"""An object that dynamically fetches the documentation
    for an Octave user class method.
    """"""

    def __init__(self, session_weakref, class_name, name):
        """"""Initialize the descriptor.""""""
        self.ref = session_weakref
        self.class_name = class_name
        self.name = name
        self.doc = None

    def __get__(self, instance, owner=None):
        """"""Get the documentation.""""""
        if self.doc is not None:
            return self.doc
        session = self.ref()
        class_name = self.class_name
        method = self.name
        doc = session._get_doc(f'@{class_name}/{method}')
        self.doc = doc or session._get_doc(method)
        return self.doc","class _MethodDocDescriptor:
    
    def __init__(self, session_weakref, class_name, name):
        
        pass
    def __get__(self, instance, owner=None):
        
        pass",snippet_145,3,"class _MethodDocDescriptor:
    
    def __init__(self, session_weakref, class_name, name):
        self._session_weakref = session_weakref
        self._class_name = class_name
        self._name = name

    def __get__(self, instance, owner=None):
        def _compute_doc():
            session = None
            try:
                session = self._session_weakref() if callable(self._session_weakref) else None
            except Exception:
                session = None
            doc = None
            if session is not None and hasattr(session, 'get_doc'):
                try:
                    doc = session.get_doc(self._class_name, self._name)
                except Exception:
                    doc = None
            if not doc:
                doc = f""{self._class_name}.{self._name}""
            return doc

        if instance is None:
            def _unbound(*args, **kwargs):
                raise AttributeError(f""'{self._class_name}' method '{self._name}' is not callable from the class."")
            _unbound.__name__ = self._name
            _unbound.__qualname__ = f""{self._class_name}.{self._name}""
            _unbound.__doc__ = _compute_doc()
            return _unbound

        impl_name = f""_{self._name}""
        impl = getattr(instance, impl_name, None)
        if impl is None and owner is not None:
            impl = getattr(owner, impl_name, None)
            if impl is not None and hasattr(impl, ""__get__""):
                impl = impl.__get__(instance, owner)

        if impl is None or not callable(impl):
            def _missing(*args, **kwargs):
                raise NotImplementedError(f""No implementation found for method '{self._name}'. Expected '{impl_name}'."")
            _missing.__name__ = self._name
            _missing.__qualname__ = f""{type(instance).__name__}.{self._name}""
            _missing.__doc__ = _compute_doc()
            return _missing

        # Try to enrich doc if missing
        try:
            if not getattr(impl, ""__doc__"", None):
                try:
                    impl.__doc__ = _compute_doc()
                except Exception:
                    pass
        except Exception:
            pass

        return impl"
244777,markokr/rarfile,markokr_rarfile/rarfile.py,rarfile.NoHashContext,"class NoHashContext:
    """"""No-op hash function.""""""

    def __init__(self, data=None):
        """"""Initialize""""""

    def update(self, data):
        """"""Update data""""""

    def digest(self):
        """"""Final hash""""""

    def hexdigest(self):
        """"""Hexadecimal digest.""""""","class NoHashContext:
    
    def __init__(self, data=None):
        
        pass
    def update(self, data):
        
        pass
    def digest(self):
        
        pass

    def hexdigest(self):
        
        pass",snippet_147,5,"class NoHashContext:
    
    def __init__(self, data=None):
        if data is not None:
            self.update(data)

    def update(self, data):
        # Intentionally does nothing; placeholder for hash-like interface
        return None

    def digest(self):
        return b""""

    def hexdigest(self):
        return """""
244940,meejah/txtorcon,meejah_txtorcon/txtorcon/addrmap.py,txtorcon.addrmap.Addr,"import datetime
from txtorcon.util import maybe_ip_addr

class Addr:
    """"""
    One address mapping (e.g. example.com -> 127.0.0.1)
    """"""

    def __init__(self, map):
        """"""
        map is an AddrMap instance, used for scheduling expiries and
        updating the map.
        """"""
        self.map = map
        self.ip = None
        self.name = None
        self.expiry = None
        self.expires = None
        self.created = None

    def update(self, *args):
        """"""
        deals with an update from Tor; see parsing logic in torcontroller
        """"""
        gmtexpires = None
        name, ip, expires = args[:3]
        for arg in args:
            if arg.lower().startswith('expires='):
                gmtexpires = arg[8:]
        if gmtexpires is None:
            if len(args) == 3:
                gmtexpires = expires
            elif args[2] == 'NEVER':
                gmtexpires = args[2]
            else:
                gmtexpires = args[3]
        self.name = name
        self.ip = maybe_ip_addr(ip)
        if self.ip == '<error>':
            self._expire()
            return
        fmt = '%Y-%m-%d %H:%M:%S'
        oldexpires = self.expires
        if gmtexpires.upper() == 'NEVER':
            self.expires = None
        else:
            self.expires = datetime.datetime.strptime(gmtexpires, fmt)
        self.created = datetime.datetime.utcnow()
        if self.expires is not None:
            if oldexpires is None:
                if self.expires <= self.created:
                    diff = datetime.timedelta(seconds=0)
                else:
                    diff = self.expires - self.created
                self.expiry = self.map.scheduler.callLater(diff.seconds, self._expire)
            else:
                diff = self.expires - oldexpires
                self.expiry.delay(diff.seconds)

    def _expire(self):
        """"""
        callback done via callLater
        """"""
        del self.map.addr[self.name]
        self.map.notify('addrmap_expired', *[self.name], **{})","
class Addr:
    
    def __init__(self, map):
        
        pass
    def update(self, *args):
        
        pass

    def _expire(self):
        
        pass",snippet_148,4,"class Addr:
    import time as _time

    def __init__(self, map):
        self._store = {}
        self.map = {}
        if map:
            if hasattr(map, ""items""):
                items = map.items()
            else:
                items = map
            now = self._time.time()
            for k, v in items:
                if isinstance(v, tuple) and len(v) == 2:
                    val, t = v
                    exp_at = self._compute_expire_at(now, t)
                    self._store[k] = (val, exp_at)
                else:
                    self._store[k] = (v, None)
        self._refresh_visible()

    def update(self, *args):
        if not args:
            return
        now = self._time.time()
        data = {}
        # Support dict/update semantics: update(mapping), update(iterable), update(mapping, **kwargs), update(**kwargs)
        if len(args) == 1:
            src = args[0]
            if hasattr(src, ""items""):
                data.update(src.items())
            else:
                for k, v in src:
                    data[k] = v
        elif len(args) >= 2:
            # First positional must be mapping
            src = args[0]
            if hasattr(src, ""items""):
                data.update(src.items())
            else:
                for k, v in src:
                    data[k] = v
            # Remaining arguments are treated as kwargs-like mappings
            for extra in args[1:]:
                if hasattr(extra, ""items""):
                    data.update(extra.items())
                else:
                    for k, v in extra:
                        data[k] = v

        for k, v in data.items():
            if isinstance(v, tuple) and len(v) == 2:
                val, t = v
                exp_at = self._compute_expire_at(now, t)
                self._store[k] = (val, exp_at)
            else:
                self._store[k] = (v, None)
        self._expire()

    def _expire(self):
        now = self._time.time()
        expired_keys = []
        for k, (_, exp_at) in self._store.items():
            if exp_at is not None and exp_at <= now:
                expired_keys.append(k)
        for k in expired_keys:
            self._store.pop(k, None)
        self._refresh_visible()
        return len(expired_keys)

    def _refresh_visible(self):
        now = self._time.time()
        self.map = {k: v for k, (v, exp_at) in self._store.items() if exp_at is None or exp_at > now}

    @staticmethod
    def _compute_expire_at(now, t):
        if t is None:
            return None
        # If t looks like absolute epoch (significantly larger than reasonable TTL), accept as absolute
        # Otherwise treat as relative ttl seconds
        try:
            t = float(t)
        except (TypeError, ValueError):
            return None
        # Heuristic threshold: if t is in the far past or > year 2286, treat as ttl anyway
        if t > 10_000_000:  # absolute epoch heuristic
            return t
        return now + t"
245022,meejah/txtorcon,meejah_txtorcon/txtorcon/torconfig.py,txtorcon.torconfig.TorConfigType,"class TorConfigType:
    """"""
    Base class for all configuration types, which function as parsers
    and un-parsers.
    """"""

    def parse(self, s):
        """"""
        Given the string s, this should return a parsed representation
        of it.
        """"""
        return s

    def validate(self, s, instance, name):
        """"""
        If s is not a valid type for this object, an exception should
        be thrown. The validated object should be returned.
        """"""
        return s","class TorConfigType:
    
    def parse(self, s):
        
        pass
    def validate(self, s, instance, name):
        
        pass",snippet_149,3,"class TorConfigType:
    def parse(self, s):
        if s is None:
            return None
        if isinstance(s, bytes):
            try:
                s = s.decode(""utf-8"")
            except Exception:
                s = s.decode(""utf-8"", errors=""replace"")
        elif not isinstance(s, str):
            s = str(s)
        return s.strip()

    def validate(self, s, instance, name):
        value = self.parse(s)
        if value is None:
            return True
        if ""\x00"" in value:
            raise ValueError(f""Invalid value for {name}: contains NUL byte"")
        if value == """":
            raise ValueError(f""Invalid value for {name}: empty string"")
        return True"
245057,paterva/maltego-trx,paterva_maltego-trx/maltego_trx/oauth.py,maltego_trx.oauth.MaltegoOauth,"from cryptography.hazmat.backends import default_backend
import base64
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
from cryptography.hazmat.primitives import serialization, padding as primitives_padding
from cryptography.hazmat.primitives.asymmetric import padding as asymmetric_padding

class MaltegoOauth:
    """"""
    A Crypto Helper for Maltego OAuth Secrets received from the Transform Distribution Server
    The TDS will send back an encrypted combination of the following :
    1. Token
    2. Token Secret
    3. Refresh Token
    4. Expires In

    Contains Methods:
        1. decrypt_secrets(private_key_path=""pem file"", ciphertext=""request.getTransformSetting('name from TDS')"")
    """"""

    @staticmethod
    def _rsa_decrypt(private_key_path=None, ciphertext=None, password=None):
        """"""
        RSA Decryption function, returns decrypted plaintext in b64 encoding
        """"""
        ciphertext = base64.b64decode(ciphertext)
        with open(private_key_path, 'rb') as key_file:
            private_key = serialization.load_pem_private_key(key_file.read(), password, backend=None)
            plaintext = private_key.decrypt(ciphertext, asymmetric_padding.PKCS1v15())
        return plaintext

    @staticmethod
    def _aes_decrypt(key=None, ciphertext=None):
        """"""
        AES Decryption function, returns decrypted plaintext value
        """"""
        key = base64.b64decode(key)
        cipher = Cipher(algorithms.AES(key), modes.ECB(), backend=default_backend())
        decryptor = cipher.decryptor()
        ciphertext = base64.b64decode(ciphertext)
        padded_b64_plaintext = decryptor.update(ciphertext) + decryptor.finalize()
        unpadder = primitives_padding.PKCS7(128).unpadder()
        plaintext = (unpadder.update(padded_b64_plaintext) + unpadder.finalize()).decode('utf8')
        return plaintext

    @classmethod
    def decrypt_secrets(cls, private_key_path=None, encoded_ciphertext=None):
        """"""
        The TDS will send back an encrypted combination of the following :
        1. Token
        2. Token Secret
        3. Refresh Token
        4. Expires In

        This function decodes the combinations and decrypts as required and returns a dictionary with the following keys
                {""token"":"""",
                ""token_secret"": """",
                ""refresh_token"": """",
                ""expires_in"": """"}
        """"""
        encrypted_fields = encoded_ciphertext.split('$')
        if len(encrypted_fields) == 1:
            token = cls._rsa_decrypt(private_key_path, encrypted_fields[0])
            token_fields = {'token': token}
        elif len(encrypted_fields) == 2:
            token = cls._rsa_decrypt(private_key_path, encrypted_fields[0])
            token_secret = cls._rsa_decrypt(private_key_path, encrypted_fields[1])
            token_fields = {'token': token, 'token_secret': token_secret}
        elif len(encrypted_fields) == 3:
            aes_key = cls._rsa_decrypt(private_key_path, encrypted_fields[2])
            token = cls._aes_decrypt(aes_key, encrypted_fields[0])
            token_secret = cls._aes_decrypt(aes_key, encrypted_fields[1])
            token_fields = {'token': token, 'token_secret': token_secret}
        elif len(encrypted_fields) == 4:
            token = cls._rsa_decrypt(private_key_path, encrypted_fields[0])
            token_secret = cls._rsa_decrypt(private_key_path, encrypted_fields[1])
            refresh_token = cls._rsa_decrypt(private_key_path, encrypted_fields[2])
            expires_in = cls._rsa_decrypt(private_key_path, encrypted_fields[3])
            token_fields = {'token': token, 'token_secret': token_secret, 'refresh_token': refresh_token, 'expires_in': expires_in}
        elif len(encrypted_fields) == 5:
            aes_key = cls._rsa_decrypt(private_key_path, encrypted_fields[4])
            token = cls._aes_decrypt(aes_key, encrypted_fields[0])
            token_secret = cls._aes_decrypt(aes_key, encrypted_fields[1])
            refresh_token = cls._aes_decrypt(aes_key, encrypted_fields[2])
            expires_in = cls._aes_decrypt(aes_key, encrypted_fields[3])
            token_fields = {'token': token, 'token_secret': token_secret, 'refresh_token': refresh_token, 'expires_in': expires_in}
        else:
            token_fields = {'token': '', 'token_secret': '', 'refresh_token': '', 'expires_in': ''}
        return token_fields","
class MaltegoOauth:
    
    @staticmethod
    def _rsa_decrypt(private_key_path=None, ciphertext=None, pass
word=None):
        
        pass
    @staticmethod
    def _aes_decrypt(key=None, ciphertext=None):
        
        pass
    @classmethod
    def decrypt_secrets(cls, private_key_path=None, encoded_ciphertext=None):
        
        pass",snippet_150,4,"import base64
import json
from typing import Optional, Union

from cryptography.hazmat.primitives import hashes, serialization, padding as sym_padding
from cryptography.hazmat.primitives.asymmetric import padding as asym_padding
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes


class MaltegoOauth:
    @staticmethod
    def _to_bytes(data: Union[str, bytes, bytearray, None]) -> Optional[bytes]:
        if data is None:
            return None
        if isinstance(data, bytes):
            return data
        if isinstance(data, bytearray):
            return bytes(data)
        if isinstance(data, str):
            return data.encode(""utf-8"")
        raise TypeError(""Unsupported data type; expected str/bytes/bytearray"")

    @staticmethod
    def _b64_to_bytes(maybe_b64: Union[str, bytes]) -> bytes:
        if isinstance(maybe_b64, bytes):
            s = maybe_b64
        else:
            s = maybe_b64.encode(""utf-8"")
        try:
            return base64.b64decode(s, validate=True)
        except Exception:
            # Not valid base64, return as-is assuming it is raw bytes
            return s

    @staticmethod
    def _rsa_decrypt(private_key_path=None, ciphertext=None, password=None):
        if not private_key_path:
            raise ValueError(""private_key_path is required"")
        if ciphertext is None:
            raise ValueError(""ciphertext is required"")

        with open(private_key_path, ""rb"") as f:
            pem = f.read()

        pwd_bytes = None
        if password is not None:
            pwd_bytes = MaltegoOauth._to_bytes(password)

        private_key = serialization.load_pem_private_key(pem, password=pwd_bytes)

        ct_bytes = MaltegoOauth._b64_to_bytes(ciphertext)

        try:
            return private_key.decrypt(
                ct_bytes,
                asym_padding.OAEP(
                    mgf=asym_padding.MGF1(algorithm=hashes.SHA256()),
                    algorithm=hashes.SHA256(),
                    label=None,
                ),
            )
        except Exception:
            # Fallback to PKCS1v15 if OAEP fails
            return private_key.decrypt(ct_bytes, asym_padding.PKCS1v15())

    @staticmethod
    def _aes_decrypt(key=None, ciphertext=None):
        if key is None:
            raise ValueError(""key is required"")
        key_bytes = MaltegoOauth._to_bytes(key)

        # Accept ciphertext as:
        # - dict with keys: iv, ct/ciphertext, tag (GCM)
        # - dict with iv, ct (CBC-PKCS7) if no tag provided
        # - bytes with layout [iv|ct|tag] where iv=12, tag=16 for GCM
        # - base64 string of the above bytes
        if isinstance(ciphertext, (str, bytes, bytearray)):
            blob = MaltegoOauth._b64_to_bytes(ciphertext)  # iv|ct|tag (GCM) or iv|ct (CBC cannot be inferred)
            if len(blob) >= 12 + 16 + 1:  # assume GCM with 12-byte IV, 16-byte tag
                iv = blob[:12]
                tag = blob[-16:]
                ct = blob[12:-16]
                decryptor = Cipher(algorithms.AES(key_bytes), modes.GCM(iv, tag)).decryptor()
                return decryptor.update(ct) + decryptor.finalize()
            else:
                raise ValueError(""Ciphertext bytes format unsupported or too short"")
        elif isinstance(ciphertext, dict):
            # Normalize keys
            iv_b64 = ciphertext.get(""iv"") or ciphertext.get(""nonce"")
            ct_b64 = ciphertext.get(""ct"") or ciphertext.get(""ciphertext"") or ciphertext.get(""data"")
            tag_b64 = ciphertext.get(""tag"") or ciphertext.get(""auth_tag"") or ciphertext.get(""mac"")

            if iv_b64 is None or ct_b64 is None:
                raise ValueError(""ciphertext dict must contain at least iv and ct/ciphertext/data"")

            iv = MaltegoOauth._b64_to_bytes(iv_b64)
            ct = MaltegoOauth._b64_to_bytes(ct_b64)

            if tag_b64 is not None:
                tag = MaltegoOauth._b64_to_bytes(tag_b64)
                decryptor = Cipher(algorithms.AES(key_bytes), modes.GCM(iv, tag)).decryptor()
                return decryptor.update(ct) + decryptor.finalize()
            else:
                # CBC with PKCS7 padding if no tag provided
                if len(iv) not in (16,):
                    raise ValueError(""CBC mode requires a 16-byte IV"")
                decryptor = Cipher(algorithms.AES(key_bytes), modes.CBC(iv)).decryptor()
                padded = decryptor.update(ct) + decryptor.finalize()
                unpadder = sym_padding.PKCS7(128).unpadder()
                return unpadder.update(padded) + unpadder.finalize()
        else:
            raise TypeError(""Unsupported ciphertext type"")

    @classmethod
    def decrypt_secrets(cls, private_key_path=None, encoded_ciphertext=None):
        if encoded_ciphertext is None:
            raise ValueError(""encoded_ciphertext is required"")

        # encoded_ciphertext is expected to be base64 of a JSON payload or a dict already
        payload = None
        if isinstance(encoded_ciphertext, dict):
            payload = encoded_ciphertext
        else:
            raw = cls._b64_to_bytes(encoded_ciphertext)
            try:
                payload = json.loads(raw.decode(""utf-8""))
            except Exception:
                # If not JSON after b64-decode, try direct JSON parse
                if isinstance(encoded_ciphertext, (bytes, bytearray)):
                    text = bytes(encoded_ciphertext).decode(""utf-8"")
                else:
                    text = str(encoded_ciphertext)
                payload = json.loads(text)

        # Expected fields:
        # - RSA-encrypted AES key: one of [""encrypted_key"", ""ek"", ""key""]
        # - AES payload: iv/nonce, ct/ciphertext/data, tag (optional for CBC)
        enc_key_b64 = payload.get(""encrypted_key"") or payload.get(""ek"") or payload.get(""key"")
        if enc_key_b64 is None:
            raise ValueError(""No encrypted symmetric key found in payload"")

        aes_key = cls._rsa_decrypt(private_key_path=private_key_path, ciphertext=enc_key_b64)

        # Decrypt AES payload
        # Accept both nested structure under ""payload"" or top-level fields
        aes_payload = payload.get(""payload"") or payload

        plaintext = cls._aes_decrypt(key=aes_key, ciphertext=aes_payload)

        # Try decode to text; if fails, return raw bytes
        try:
            return plaintext.decode(""utf-8"")
        except Exception:
            return plaintext"
247043,CiscoDevNet/webexteamssdk,src/webexpythonsdk/models/cards/adaptive_card_component.py,webexpythonsdk.models.cards.adaptive_card_component.AdaptiveCardComponent,"import json
import enum

class AdaptiveCardComponent:
    """"""
    Base class for all Adaptive Card elements.

    Each element should inherit from this class and specify which of its
    properties fall into the following two categories:

    * Simple properties are basic types (int, float, str, etc.).

    * Serializable properties are properties that can themselves be serialized.
      This includes lists of items (i.e. the 'body' field of the adaptive card)
      or single objects that also inherit from Serializable
    """"""

    def __init__(self, serializable_properties, simple_properties):
        """"""
        Initialize a serializable object.

        Args:
            serializable_properties(list): List of all serializable properties
            simple_properties(list): List of all simple properties.
        """"""
        self.serializable_properties = serializable_properties
        self.simple_properties = simple_properties

    def to_dict(self):
        """"""
        Serialize the element into a Python dictionary.

        The to_dict() method recursively serializes the object's data into
        a Python dictionary.

        Returns:
            dict: Dictionary representation of this element.
        """"""
        serialized_data = {}
        for property_name in self.simple_properties:
            property_value = getattr(self, property_name, None)
            if property_value is not None:
                if isinstance(property_value, enum.Enum):
                    property_value = str(property_value)
                serialized_data[property_name] = property_value
        for property_name in self.serializable_properties:
            property_value = getattr(self, property_name, None)
            if property_value is not None:
                if isinstance(property_value, list):
                    serialized_data[property_name] = [item.to_dict() if hasattr(item, 'to_dict') else item for item in property_value]
                else:
                    serialized_data[property_name] = property_value.to_dict()
        return serialized_data

    def to_json(self, **kwargs):
        """"""
        Serialize the element into JSON text.

        Any keyword arguments provided are passed through the Python JSON
        encoder.
        """"""
        return json.dumps(self.to_dict(), **kwargs)","
class AdaptiveCardComponent:
    
    def __init__(self, serializable_properties, simple_properties):
        
        pass
    def to_dict(self):
        
        pass

    def to_json(self, **kwargs):
        
        pass",snippet_151,4,"import json

class AdaptiveCardComponent:
    
    def __init__(self, serializable_properties, simple_properties):
        self._serializable_properties = tuple(serializable_properties or ())
        self._simple_properties = tuple(simple_properties or ())

    def _serialize_value(self, value):
        if value is None:
            return None
        if hasattr(value, ""to_dict"") and callable(getattr(value, ""to_dict"")):
            return value.to_dict()
        if isinstance(value, (list, tuple, set)):
            return [self._serialize_value(v) for v in value if v is not None]
        if isinstance(value, dict):
            return {k: self._serialize_value(v) for k, v in value.items() if v is not None}
        return value

    def to_dict(self):
        result = {}
        for prop in self._simple_properties:
            if hasattr(self, prop):
                val = getattr(self, prop)
                if val is not None:
                    result[prop] = val
        for prop in self._serializable_properties:
            if hasattr(self, prop):
                val = getattr(self, prop)
                if val is not None:
                    result[prop] = self._serialize_value(val)
        return result

    def to_json(self, **kwargs):
        return json.dumps(self.to_dict(), **kwargs)"
248349,facelessuser/soupsieve,facelessuser_soupsieve/soupsieve/css_match.py,soupsieve.css_match._FakeParent,"import bs4

class _FakeParent:
    """"""
    Fake parent class.

    When we have a fragment with no `BeautifulSoup` document object,
    we can't evaluate `nth` selectors properly.  Create a temporary
    fake parent so we can traverse the root element as a child.
    """"""

    def __init__(self, element: bs4.Tag) -> None:
        """"""Initialize.""""""
        self.contents = [element]

    def __len__(self) -> int:
        """"""Length.""""""
        return len(self.contents)","
class _FakeParent:
    
    def __init__(self, element: bs4.Tag) -> None:
        
        pass
    def __len__(self) -> int:
        
        pass",snippet_152,3,"class _FakeParent:
    
    def __init__(self, element: bs4.Tag) -> None:
        self.element = element
        self.contents = [element]

    def __len__(self) -> int:
        return len(self.contents)"
248351,facelessuser/soupsieve,facelessuser_soupsieve/soupsieve/css_parser.py,soupsieve.css_parser.SelectorPattern,"from typing import Match, Any, Iterator, cast
import re

class SelectorPattern:
    """"""Selector pattern.""""""

    def __init__(self, name: str, pattern: str) -> None:
        """"""Initialize.""""""
        self.name = name
        self.re_pattern = re.compile(pattern, re.I | re.X | re.U)

    def get_name(self) -> str:
        """"""Get name.""""""
        return self.name

    def match(self, selector: str, index: int, flags: int) -> Match[str] | None:
        """"""Match the selector.""""""
        return self.re_pattern.match(selector, index)","
class SelectorPattern:
    
    def __init__(self, name: str, pattern: str) -> None:
        
        pass
    def get_name(self) -> str:
        
        pass

    def match(self, selector: str, index: int, flags: int) -> Match[str] | None:
        
        pass",snippet_153,4,"import re
from typing import Match, Pattern

class SelectorPattern:
    
    def __init__(self, name: str, pattern: str) -> None:
        self._name = name
        self._pattern_str = pattern
        self._compiled_cache: dict[int, Pattern[str]] = {}
        
    def get_name(self) -> str:
        return self._name

    def match(self, selector: str, index: int, flags: int) -> Match[str] | None:
        if flags not in self._compiled_cache:
            self._compiled_cache[flags] = re.compile(self._pattern_str, flags)
        return self._compiled_cache[flags].match(selector, pos=index)"
250153,Unidata/siphon,src/siphon/catalog.py,siphon.catalog.CatalogRef,"from urllib.parse import urljoin, urlparse

class CatalogRef:
    """"""
    An object for holding catalog references obtained from a THREDDS Client Catalog.

    Attributes
    ----------
    name : str
        The name of the :class:`CatalogRef` element
    href : str
        url to the :class:`CatalogRef`'s THREDDS Client Catalog
    title : str
        Title of the :class:`CatalogRef` element

    """"""

    def __init__(self, base_url, element_node):
        """"""
        Initialize the catalogRef object.

        Parameters
        ----------
        base_url : str
            URL to the base catalog that owns this reference
        element_node : :class:`~xml.etree.ElementTree.Element`
            An :class:`~xml.etree.ElementTree.Element` representing a catalogRef node

        """"""
        self.title = element_node.attrib['{http://www.w3.org/1999/xlink}title']
        self.name = element_node.attrib.get('name', self.title)
        href = element_node.attrib['{http://www.w3.org/1999/xlink}href']
        self.href = urljoin(base_url, href)

    def __str__(self):
        """"""Return a string representation of the catalog reference.""""""
        return str(self.title)

    def follow(self):
        """"""Follow the catalog reference and return a new :class:`TDSCatalog`.

        Returns
        -------
        TDSCatalog
            The referenced catalog

        """"""
        return TDSCatalog(self.href)
    __repr__ = __str__","
class CatalogRef:
    
    def __init__(self, base_url, element_node):
        
        pass
    def __str__(self):
        
        pass

    def follow(self):
        
        pass",snippet_154,4,"class CatalogRef:
    
    def __init__(self, base_url, element_node):
        from urllib.parse import urljoin

        self.base_url = base_url
        self.element_node = element_node

        # Extract candidate URL from element_node
        href = None
        # If element_node is a string, treat it as the URL itself
        if isinstance(element_node, str):
            href = element_node.strip() or None
        else:
            # Try common element interfaces
            # lxml/ElementTree Elements support .get
            getter = getattr(element_node, ""get"", None)
            if callable(getter):
                href = getter(""href"") or getter(""src"") or getter(""url"")
            else:
                # Try .attrib dict
                attrib = getattr(element_node, ""attrib"", None)
                if isinstance(attrib, dict):
                    href = attrib.get(""href"") or attrib.get(""src"") or attrib.get(""url"")
                # Try mapping-like node
                if href is None and hasattr(element_node, ""items""):
                    try:
                        mapping = dict(element_node.items())
                        href = mapping.get(""href"") or mapping.get(""src"") or mapping.get(""url"")
                    except Exception:
                        pass

        if href is None:
            # Fallback: try to use str(element_node) if it looks like a URL
            try:
                candidate = str(element_node).strip()
                if candidate:
                    href = candidate
            except Exception:
                href = None

        if href is None:
            raise ValueError(""Cannot determine URL from element_node"")

        self.url = urljoin(base_url or """", href)

    def __str__(self):
        return self.url

    def follow(self):
        import urllib.request
        import urllib.error

        if not self.url:
            raise ValueError(""No URL to follow"")

        req = urllib.request.Request(
            self.url,
            headers={
                ""User-Agent"": ""CatalogRef/1.0 (+https://example.com)"",
                ""Accept"": ""*/*"",
            },
        )
        try:
            with urllib.request.urlopen(req, timeout=30) as resp:
                return resp.read()
        except urllib.error.HTTPError as e:
            # Re-raise with original context; caller can handle
            raise
        except urllib.error.URLError as e:
            raise"
250158,Unidata/siphon,src/siphon/catalog.py,siphon.catalog.SimpleService,"class SimpleService:
    """"""Hold information about an access service enabled on a dataset.

    Attributes
    ----------
    name : str
        The name of the service
    service_type : str
        The service type (i.e. ""OPENDAP"", ""NetcdfSubset"", ""WMS"", etc.)
    access_urls : dict[str, str]
        A dictionary of access urls whose keywords are the access service
        types defined in the catalog (for example, ""OPENDAP"", ""NetcdfSubset"",
        ""WMS"", etc.)

    """"""

    def __init__(self, service_node):
        """"""Initialize the Dataset object.

        Parameters
        ----------
        service_node : :class:`~xml.etree.ElementTree.Element`
            An :class:`~xml.etree.ElementTree.Element` representing a service node

        """"""
        self.name = service_node.attrib['name']
        self.service_type = CaseInsensitiveStr(service_node.attrib['serviceType'])
        self.base = service_node.attrib['base']
        self.access_urls = {}

    def is_resolver(self):
        """"""Return whether the service is a resolver service.""""""
        return self.service_type == 'Resolver'","class SimpleService:
    
    def __init__(self, service_node):
        
        pass
    def is_resolver(self):
        
        pass",snippet_156,3,"class SimpleService:
    def __init__(self, service_node):
        self._node = service_node
        self.name = None
        self.type = None
        self.tags = set()
        self.metadata = {}

        if service_node is None:
            return

        # Extract name
        self.name = self._extract_value(service_node, [""name"", ""service_name"", ""id""])

        # Extract type
        self.type = self._extract_value(service_node, [""type"", ""service_type"", ""kind""])

        # Extract tags
        tags_val = self._extract_value(service_node, [""tags"", ""labels"", ""roles""])
        if isinstance(tags_val, (list, tuple, set)):
            self.tags = {str(t).strip().lower() for t in tags_val if t is not None}
        elif isinstance(tags_val, str):
            # Split by common separators
            parts = [p.strip() for p in tags_val.replace("","", "" "").split()]
            self.tags = {p.lower() for p in parts if p}

        # Capture remaining metadata if dict-like
        if isinstance(service_node, dict):
            self.metadata = dict(service_node)
        else:
            # collect simple attributes
            for attr in dir(service_node):
                if attr.startswith(""_""):
                    continue
                try:
                    val = getattr(service_node, attr)
                except Exception:
                    continue
                if callable(val):
                    continue
                self.metadata[attr] = val

    def is_resolver(self):
        node = self._node
        if node is None:
            return False

        # Explicit boolean flags
        for key in (""resolver"", ""is_resolver"", ""isResolver"", ""dns_resolver"", ""dnsResolver""):
            val = self._extract_value(node, [key])
            if isinstance(val, bool):
                return val
            if isinstance(val, str):
                lv = val.strip().lower()
                if lv in (""true"", ""yes"", ""1""):
                    return True
                if lv in (""false"", ""no"", ""0""):
                    return False

        # Type-based inference
        t = (self.type or """").strip().lower()
        if t in {""resolver"", ""dns"", ""dns-resolver"", ""dns_resolver"", ""name-service"", ""nameservice""}:
            return True

        # Tag-based inference
        tags = self.tags
        if {""resolver"", ""dns"", ""dns-resolver"", ""dns_resolver""} & tags:
            return True

        # Name-based heuristic
        n = (self.name or """").strip().lower()
        if any(k in n for k in (""resolver"", ""dns"")):
            return True

        # Metadata hints
        for key in (""port"", ""protocol"", ""endpoint""):
            val = self.metadata.get(key)
            if key == ""port"" and str(val).isdigit():
                # common DNS port
                if int(val) == 53:
                    return True
            if key == ""protocol"" and isinstance(val, str) and val.strip().lower() in {""dns"", ""udp53""}:
                return True
            if key == ""endpoint"" and isinstance(val, str) and "":53"" in val:
                return True

        return False

    @staticmethod
    def _extract_value(source, keys, default=None):
        for key in keys:
            # dict-like
            if isinstance(source, dict) and key in source:
                return source[key]
            # attribute-like
            try:
                if hasattr(source, key):
                    return getattr(source, key)
            except Exception:
                pass
            # case-insensitive dict keys
            if isinstance(source, dict):
                for k in source.keys():
                    try:
                        if str(k).lower() == str(key).lower():
                            return source[k]
                    except Exception:
                        continue
        return default"
250272,incuna/django-pgcrypto-fields,incuna_django-pgcrypto-fields/pgcrypto/mixins.py,pgcrypto.mixins.HashMixin,"class HashMixin:
    """"""Keyed hash mixin.

    `HashMixin` uses 'pgcrypto' to encrypt data in a postgres database.
    """"""
    encrypt_sql = None

    def __init__(self, original=None, *args, **kwargs):
        """"""Tells the init the original attr.""""""
        self.original = original
        super(HashMixin, self).__init__(*args, **kwargs)

    def pre_save(self, model_instance, add):
        """"""Save the original_value.""""""
        if self.original:
            original_value = getattr(model_instance, self.original)
            setattr(model_instance, self.attname, original_value)
        return super(HashMixin, self).pre_save(model_instance, add)

    def get_placeholder(self, value=None, compiler=None, connection=None):
        """"""
        Tell postgres to encrypt this field with a hashing function.

        The `value` string is checked to determine if we need to hash or keep
        the current value.

        `compiler` and `connection` is ignored here as we don't need custom operators.
        """"""
        if value is None or value.startswith('\\x'):
            return '%s'
        return self.get_encrypt_sql(connection)

    def get_encrypt_sql(self, connection):
        """"""Get encrypt sql. This may be overidden by some implementations.""""""
        return self.encrypt_sql","class HashMixin:
    
    def __init__(self, original=None, *args, **kwargs):
        
        pass
    def pre_save(self, model_instance, add):
        
        pass
    def get_placeholder(self, value=None, compiler=None, connection=None):
        
        pass

    def get_encrypt_sql(self, connection):
        
        pass",snippet_157,5,"class HashMixin:
    def __init__(self, original=None, *args, **kwargs):
        self.original = original
        try:
            super().__init__(*args, **kwargs)
        except Exception:
            pass

    def pre_save(self, model_instance, add):
        import hashlib

        if not self.original:
            return getattr(model_instance, getattr(self, ""attname"", None), None)

        source = getattr(model_instance, self.original, None)
        if source is None:
            return None

        if isinstance(source, bytes):
            data = source
        else:
            data = str(source).encode(""utf-8"")

        hashed = hashlib.sha256(data).hexdigest()

        attname = getattr(self, ""attname"", None)
        if attname:
            try:
                setattr(model_instance, attname, hashed)
            except Exception:
                pass

        return hashed

    def get_placeholder(self, value=None, compiler=None, connection=None):
        conn = connection or getattr(compiler, ""connection"", None)
        sql = self.get_encrypt_sql(conn)
        if ""%s"" in sql:
            return sql
        return ""%s""

    def get_encrypt_sql(self, connection):
        vendor = getattr(connection, ""vendor"", None)

        if vendor == ""postgresql"":
            return ""encode(digest(%s, 'sha256'), 'hex')""

        if vendor == ""mysql"":
            return ""SHA2(%s, 256)""

        if vendor == ""oracle"":
            return ""LOWER(RAWTOHEX(STANDARD_HASH(%s, 'SHA256')))""""""

        if vendor in (""microsoft"", ""mssql"", ""sql_server"", ""mssqlms"", ""mssqlapi""):
            return ""LOWER(CONVERT(VARCHAR(64), HASHBYTES('SHA2_256', %s), 2))""

        return ""%s"""
251421,Yelp/py_zipkin,Yelp_py_zipkin/py_zipkin/encoding/_encoders.py,py_zipkin.encoding._encoders.IEncoder,"from typing import Union
from py_zipkin.encoding._helpers import Span
from typing import List

class IEncoder:
    """"""Encoder interface.""""""

    def fits(self, current_count: int, current_size: int, max_size: int, new_span: Union[str, bytes]) -> bool:
        """"""Returns whether the new span will fit in the list.

        :param current_count: number of spans already in the list.
        :type current_count: int
        :param current_size: sum of the sizes of all the spans already in the list.
        :type current_size: int
        :param max_size: max supported transport payload size.
        :type max_size: int
        :param new_span: encoded span object that we want to add the the list.
        :type new_span: str or bytes
        :return: True if the new span can be added to the list, False otherwise.
        :rtype: bool
        """"""
        raise NotImplementedError()

    def encode_span(self, span: Span) -> Union[str, bytes]:
        """"""Encodes a single span.

        :param span: Span object representing the span.
        :type span: Span
        :return: encoded span.
        :rtype: str or bytes
        """"""
        raise NotImplementedError()

    def encode_queue(self, queue: List[Union[str, bytes]]) -> Union[str, bytes]:
        """"""Encodes a list of pre-encoded spans.

        :param queue: list of encoded spans.
        :type queue: list
        :return: encoded list, type depends on the encoding.
        :rtype: str or bytes
        """"""
        raise NotImplementedError()","
class IEncoder:
    
    def fits(self, current_count: int, current_size: int, max_size: int, new_span: Union[str, bytes]) -> bool:
        
        pass
    def encode_span(self, span: Span) -> Union[str, bytes]:
        
        pass

    def encode_queue(self, queue: List[Union[str, bytes]]) -> Union[str, bytes]:
        
        pass",snippet_158,4,"from typing import Union, List, Any

class IEncoder:
    def fits(self, current_count: int, current_size: int, max_size: int, new_span: Union[str, bytes]) -> bool:
        if not isinstance(new_span, (str, bytes)):
            raise TypeError(""new_span must be str or bytes"")
        sep_len = 1 if current_count > 0 else 0
        try:
            new_len = len(new_span)
        except Exception:
            new_len = len(str(new_span))
        return (current_size + sep_len + new_len) <= max_size

    def encode_span(self, span: Any) -> Union[str, bytes]:
        if isinstance(span, (str, bytes)):
            return span
        if hasattr(span, ""__bytes__""):
            try:
                return bytes(span)
            except Exception:
                pass
        to_bytes = getattr(span, ""to_bytes"", None)
        if callable(to_bytes):
            try:
                b = to_bytes()
                if isinstance(b, (bytes, bytearray)):
                    return bytes(b)
            except Exception:
                pass
        b_attr = getattr(span, ""bytes"", None)
        if isinstance(b_attr, (bytes, bytearray)):
            return bytes(b_attr)
        text_attr = getattr(span, ""text"", None)
        if isinstance(text_attr, str):
            return text_attr
        content_attr = getattr(span, ""content"", None)
        if isinstance(content_attr, (str, bytes, bytearray)):
            return content_attr if isinstance(content_attr, str) else bytes(content_attr)
        return str(span)

    def encode_queue(self, queue: List[Union[str, bytes]]) -> Union[str, bytes]:
        if not queue:
            return b""""
        has_str = any(isinstance(x, str) for x in queue)
        has_bytes = any(isinstance(x, (bytes, bytearray)) for x in queue)
        if has_str and has_bytes:
            raise TypeError(""Mixed str and bytes are not supported"")
        if has_bytes:
            return b"""".join(x if isinstance(x, (bytes, bytearray)) else bytes(x) for x in queue)
        return """".join(x if isinstance(x, str) else x.decode(""utf-8"", ""replace"") for x in queue)"
251832,glut23/webvtt-py,glut23_webvtt-py/webvtt/models.py,webvtt.models.Style,"import typing

class Style:
    """"""Representation of a style.""""""

    def __init__(self, text: typing.Union[str, typing.List[str]]):
        """"""Initialize.

        :param: text: the style text
        """"""
        self.lines = text.splitlines() if isinstance(text, str) else text
        self.comments: typing.List[str] = []

    @property
    def text(self):
        """"""Return the text of the style.""""""
        return '\n'.join(self.lines)","
class Style:
    
    def __init__(self, text: typing.Union[str, typing.List[str]]):
        
        pass
    @property
    def text(self):
        
        pass",snippet_159,3,"import typing

class Style:
    
    def __init__(self, text: typing.Union[str, typing.List[str]]):
        if isinstance(text, str):
            self._text = text
        elif isinstance(text, list) and all(isinstance(item, str) for item in text):
            self._text = text
        else:
            raise TypeError(""text must be a string or a list of strings"")
    
    @property
    def text(self) -> typing.Union[str, typing.List[str]]:
        return self._text"
251834,glut23/webvtt-py,glut23_webvtt-py/webvtt/sbv.py,webvtt.sbv.SBVCueBlock,"import typing
import re

class SBVCueBlock:
    """"""Representation of a cue timing block.""""""
    CUE_TIMINGS_PATTERN = re.compile('\\s*(\\d{1,2}:\\d{1,2}:\\d{1,2}.\\d{3}),(\\d{1,2}:\\d{1,2}:\\d{1,2}.\\d{3})')

    def __init__(self, start: str, end: str, payload: typing.Sequence[str]):
        """"""
        Initialize.

        :param start: start time
        :param end: end time
        :param payload: caption text
        """"""
        self.start = start
        self.end = end
        self.payload = payload

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a cue time block.

        :param lines: the lines to be validated
        :returns: true for a matching cue time block
        """"""
        return bool(len(lines) >= 2 and re.match(cls.CUE_TIMINGS_PATTERN, lines[0]) and lines[1].strip())

    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SBVCueBlock':
        """"""
        Create a `SBVCueBlock` from lines of text.

        :param lines: the lines of text
        :returns: `SBVCueBlock` instance
        """"""
        match = re.match(cls.CUE_TIMINGS_PATTERN, lines[0])
        assert match is not None
        payload = lines[1:]
        return cls(match.group(1), match.group(2), payload)","
class SBVCueBlock:
    
    def __init__(self, start: str, end: str, payload: typing.Sequence[str]):
        
        pass
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        
        pass
    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SBVCueBlock':
        
        pass",snippet_160,4,"import re
import typing


class SBVCueBlock:
    _TIMECODE_RE = re.compile(
        r'^\s*(\d+):([0-5]?\d):([0-5]?\d)\.(\d{1,3})\s*,\s*(\d+):([0-5]?\d):([0-5]?\d)\.(\d{1,3})\s*$'
    )

    def __init__(self, start: str, end: str, payload: typing.Sequence[str]):
        if not isinstance(start, str) or not isinstance(end, str):
            raise TypeError(""start and end must be strings"")
        if not isinstance(payload, typing.Sequence):
            raise TypeError(""payload must be a sequence of strings"")
        for line in payload:
            if not isinstance(line, str):
                raise TypeError(""all payload lines must be strings"")
        # Validate timecodes format
        header = f""{start},{end}""
        if not self._TIMECODE_RE.match(header):
            raise ValueError(""Invalid SBV timecode format for start/end"")
        self.start = start.strip()
        self.end = end.strip()
        self.payload = list(payload)

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        if not isinstance(lines, typing.Sequence):
            return False
        if len(lines) < 2:
            return False
        header = lines[0]
        if not isinstance(header, str):
            return False
        if not cls._TIMECODE_RE.match(header):
            return False
        # Ensure remaining are strings
        for line in lines[1:]:
            if not isinstance(line, str):
                return False
        return True

    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SBVCueBlock':
        if not cls.is_valid(lines):
            raise ValueError(""Invalid SBV cue block"")
        header = lines[0].strip()
        m = cls._TIMECODE_RE.match(header)
        assert m is not None
        # Reconstruct start and end exactly as in header groups to preserve formatting
        start = f""{m.group(1)}:{m.group(2)}:{m.group(3)}.{m.group(4)}""
        end = f""{m.group(5)}:{m.group(6)}:{m.group(7)}.{m.group(8)}""
        payload = list(lines[1:])
        return cls(start, end, payload)"
251835,glut23/webvtt-py,glut23_webvtt-py/webvtt/srt.py,webvtt.srt.SRTCueBlock,"import typing
import re

class SRTCueBlock:
    """"""Representation of a cue timing block.""""""
    CUE_TIMINGS_PATTERN = re.compile('\\s*(\\d+:\\d{2}:\\d{2},\\d{3})\\s*-->\\s*(\\d+:\\d{2}:\\d{2},\\d{3})')

    def __init__(self, index: str, start: str, end: str, payload: typing.Sequence[str]):
        """"""
        Initialize.

        :param start: start time
        :param end: end time
        :param payload: caption text
        """"""
        self.index = index
        self.start = start
        self.end = end
        self.payload = payload

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a cue time block.

        :param lines: the lines to be validated
        :returns: true for a matching cue time block
        """"""
        return bool(len(lines) >= 3 and lines[0].isdigit() and re.match(cls.CUE_TIMINGS_PATTERN, lines[1]))

    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SRTCueBlock':
        """"""
        Create a `SRTCueBlock` from lines of text.

        :param lines: the lines of text
        :returns: `SRTCueBlock` instance
        """"""
        index = lines[0]
        match = re.match(cls.CUE_TIMINGS_PATTERN, lines[1])
        assert match is not None
        payload = lines[2:]
        return cls(index, match.group(1), match.group(2), payload)","
class SRTCueBlock:
    
    def __init__(self, index: str, start: str, end: str, payload: typing.Sequence[str]):
        
        pass
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        
        pass
    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SRTCueBlock':
        
        pass",snippet_161,4,"import re
import typing

class SRTCueBlock:
    
    def __init__(self, index: str, start: str, end: str, payload: typing.Sequence[str]):
        if not isinstance(index, str):
            raise TypeError(""index must be a string"")
        if not isinstance(start, str) or not isinstance(end, str):
            raise TypeError(""start and end must be strings"")
        if not isinstance(payload, typing.Sequence):
            raise TypeError(""payload must be a sequence of strings"")
        self.index = index.strip()
        self.start = start.strip()
        self.end = end.strip()
        self.payload = [str(line) for line in payload]
    
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        if not isinstance(lines, typing.Sequence):
            return False
        if len(lines) < 3:
            return False
        idx_line = lines[0]
        time_line = lines[1]
        if not isinstance(idx_line, str) or not isinstance(time_line, str):
            return False
        if not re.match(r'^\s*\d+\s*$', idx_line):
            return False
        m = re.match(r'^\s*(\d{2}:\d{2}:\d{2},\d{3})\s*-->\s*(\d{2}:\d{2}:\d{2},\d{3})\s*$', time_line)
        if not m:
            return False
        # No strict validation on payload content; allow empty strings
        return True
    
    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SRTCueBlock':
        if not cls.is_valid(lines):
            raise ValueError(""Invalid SRT cue block lines"")
        index = lines[0].strip()
        time_line = lines[1]
        m = re.match(r'^\s*(\d{2}:\d{2}:\d{2},\d{3})\s*-->\s*(\d{2}:\d{2}:\d{2},\d{3})\s*$', time_line)
        start, end = m.group(1), m.group(2)
        payload = list(lines[2:])
        return cls(index=index, start=start, end=end, payload=payload)"
251838,glut23/webvtt-py,glut23_webvtt-py/webvtt/vtt.py,webvtt.vtt.WebVTTCommentBlock,"import typing
import re

class WebVTTCommentBlock:
    """"""Representation of a comment block.""""""
    COMMENT_PATTERN = re.compile('NOTE\\s(.*?)\\Z', re.DOTALL)

    def __init__(self, text: str):
        """"""
        Initialize.

        :param text: comment text
        """"""
        self.text = text

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a comment block.

        :param lines: the lines to be validated
        :returns: true for a matching comment block
        """"""
        return bool(lines and lines[0].startswith('NOTE'))

    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTCommentBlock':
        """"""
        Create a `WebVTTCommentBlock` from lines of text.

        :param lines: the lines of text
        :returns: `WebVTTCommentBlock` instance
        """"""
        match = cls.COMMENT_PATTERN.match('\n'.join(lines))
        return cls(text=match.group(1).strip() if match else '')

    @staticmethod
    def format_lines(lines: str) -> typing.List[str]:
        """"""
        Return the lines for a comment block.

        :param lines: comment lines
        :returns: list of lines for a comment block
        """"""
        list_of_lines = lines.split('\n')
        if len(list_of_lines) == 1:
            return [f'NOTE {lines}']
        return ['NOTE', *list_of_lines]","
class WebVTTCommentBlock:
    
    def __init__(self, text: str):
        
        pass
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        
        pass
    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTCommentBlock':
        
        pass
    @staticmethod
    def format_lines(lines: str) -> typing.List[str]:
        
        pass",snippet_162,5,"import typing


class WebVTTCommentBlock:
    def __init__(self, text: str):
        self.text = text if text is not None else """"

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        if not lines:
            return False
        first = lines[0].rstrip(""\r\n"")
        return first.startswith(""NOTE"")

    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTCommentBlock':
        raw_lines = [str(l).rstrip(""\r\n"") for l in lines]
        if not cls.is_valid(raw_lines):
            raise ValueError(""Invalid WebVTT comment block"")

        first = raw_lines[0]
        # Extract inline text after NOTE
        inline = first[4:]
        if inline.startswith("" ""):
            inline = inline[1:]
        # Collect subsequent lines until first empty line (blank line ends NOTE block)
        rest: typing.List[str] = []
        for line in raw_lines[1:]:
            if line == """":
                break
            rest.append(line)
        parts: typing.List[str] = []
        if inline != """":
            parts.append(inline)
        parts.extend(rest)
        text = ""\n"".join(parts)
        return cls(text)

    @staticmethod
    def format_lines(lines: str) -> typing.List[str]:
        text = """" if lines is None else str(lines)
        # Normalize newlines
        parts = text.splitlines()
        out: typing.List[str] = []
        if not parts:
            out.append(""NOTE"")
            out.append("""")
            return out
        if len(parts) == 1:
            if parts[0] == """":
                out.append(""NOTE"")
            else:
                out.append(""NOTE "" + parts[0])
            out.append("""")
            return out
        # Multi-line: ""NOTE"" on its own line, then the text lines
        out.append(""NOTE"")
        out.extend(parts)
        out.append("""")
        return out"
251840,glut23/webvtt-py,glut23_webvtt-py/webvtt/vtt.py,webvtt.vtt.WebVTTStyleBlock,"import re
import typing

class WebVTTStyleBlock:
    """"""Representation of a style block.""""""
    STYLE_PATTERN = re.compile('STYLE\\s(.*?)\\Z', re.DOTALL)

    def __init__(self, text: str):
        """"""
        Initialize.

        :param text: style text
        """"""
        self.text = text

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a style block.

        :param lines: the lines to be validated
        :returns: true for a matching style block
        """"""
        return len(lines) >= 2 and lines[0] == 'STYLE' and (not any((line.strip() == '' or '-->' in line for line in lines)))

    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTStyleBlock':
        """"""
        Create a `WebVTTStyleBlock` from lines of text.

        :param lines: the lines of text
        :returns: `WebVTTStyleBlock` instance
        """"""
        match = cls.STYLE_PATTERN.match('\n'.join(lines))
        return cls(text=match.group(1).strip() if match else '')

    @staticmethod
    def format_lines(lines: typing.List[str]) -> typing.List[str]:
        """"""
        Return the lines for a style block.

        :param lines: style lines
        :returns: list of lines for a style block
        """"""
        return ['STYLE', *lines]","
class WebVTTStyleBlock:
    
    def __init__(self, text: str):
        
        pass
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        
        pass
    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTStyleBlock':
        
        pass
    @staticmethod
    def format_lines(lines: typing.List[str]) -> typing.List[str]:
        
        pass",snippet_163,5,"import typing

class WebVTTStyleBlock:
    
    def __init__(self, text: str):
        self.text = text

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        if not lines:
            return False
        # Find first non-empty line
        idx = 0
        n = len(lines)
        while idx < n and not lines[idx].strip():
            idx += 1
        if idx >= n:
            return False
        if lines[idx].strip() != ""STYLE"":
            return False
        # Ensure at least one non-empty content line after STYLE before the next blank separator
        idx += 1
        has_content = False
        while idx < n:
            s = lines[idx].rstrip(""\r\n"")
            if not s.strip():
                break
            has_content = True if s else has_content
            idx += 1
        return has_content

    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTStyleBlock':
        buf = list(lines)
        if not cls.is_valid(buf):
            raise ValueError(""Invalid WebVTT STYLE block"")
        # Find header
        i = 0
        n = len(buf)
        while i < n and not buf[i].strip():
            i += 1
        # i now at STYLE
        i += 1
        # Collect until first blank line or end
        content: typing.List[str] = []
        while i < n:
            line = buf[i].rstrip(""\r\n"")
            if not line.strip():
                break
            content.append(line)
            i += 1
        formatted = cls.format_lines(content)
        return cls(""\n"".join(formatted))

    @staticmethod
    def format_lines(lines: typing.List[str]) -> typing.List[str]:
        # Normalize endings and trim leading/trailing empty lines
        normalized = [ln.rstrip(""\r\n"").rstrip() for ln in lines]
        # Remove leading empty lines
        start = 0
        while start < len(normalized) and not normalized[start].strip():
            start += 1
        # Remove trailing empty lines
        end = len(normalized) - 1
        while end >= start and not normalized[end].strip():
            end -= 1
        return normalized[start:end + 1] if start <= end else []"
251866,Shoobx/xmldiff,Shoobx_xmldiff/xmldiff/diff_match_patch.py,xmldiff.diff_match_patch.patch_obj,"import urllib.parse

class patch_obj:
    """"""Class representing one patch operation.""""""

    def __init__(self):
        """"""Initializes with an empty list of diffs.""""""
        self.diffs = []
        self.start1 = None
        self.start2 = None
        self.length1 = 0
        self.length2 = 0

    def __str__(self):
        """"""Emulate GNU diff's format.
        Header: @@ -382,8 +481,9 @@
        Indices are printed as 1-based, not 0-based.

        Returns:
          The GNU diff string.
        """"""
        if self.length1 == 0:
            coords1 = str(self.start1) + ',0'
        elif self.length1 == 1:
            coords1 = str(self.start1 + 1)
        else:
            coords1 = str(self.start1 + 1) + ',' + str(self.length1)
        if self.length2 == 0:
            coords2 = str(self.start2) + ',0'
        elif self.length2 == 1:
            coords2 = str(self.start2 + 1)
        else:
            coords2 = str(self.start2 + 1) + ',' + str(self.length2)
        text = ['@@ -', coords1, ' +', coords2, ' @@\n']
        for op, data in self.diffs:
            if op == diff_match_patch.DIFF_INSERT:
                text.append('+')
            elif op == diff_match_patch.DIFF_DELETE:
                text.append('-')
            elif op == diff_match_patch.DIFF_EQUAL:
                text.append(' ')
            data = data.encode('utf-8')
            text.append(urllib.parse.quote(data, ""!~*'();/?:@&=+$,# "") + '\n')
        return ''.join(text)","
class patch_obj:
    
    def __init__(self):
        
        pass
    def __str__(self):
        
        pass",snippet_164,3,"import uuid
from datetime import datetime

class patch_obj:
    
    def __init__(self):
        self.id = uuid.uuid4().hex
        self.created_at = datetime.utcnow()
        self.metadata = {}
        
    def __str__(self):
        return f""patch_obj(id={self.id}, created_at={self.created_at.isoformat()}Z, metadata={self.metadata})"""
252656,happyleavesaoc/aoc-mgz,happyleavesaoc_aoc-mgz/mgz/model/inputs.py,mgz.model.inputs.Inputs,"from mgz.model.definitions import Input
from mgz.fast import Action as ActionEnum

class Inputs:
    """"""Normalize player inputs.""""""

    def __init__(self, gaia):
        """"""Initialize.""""""
        self._gaia = gaia
        self._buildings = {}
        self._oid_cache = {}
        self.inputs = []

    def add_chat(self, chat):
        """"""Add chat input.""""""
        self.inputs.append(Input(chat.timestamp, 'Chat', None, dict(message=chat.message), chat.player, None))

    def add_action(self, action):
        """"""Add action input.""""""
        if action.type in (ActionEnum.DE_TRANSFORM, ActionEnum.POSTGAME):
            return
        name = ACTION_TRANSLATE.get(action.type, action.type.name).replace('_', ' ').title()
        param = None
        if 'object_ids' in action.payload and action.payload['object_ids']:
            self._oid_cache[action.type] = action.payload['object_ids']
        elif action.type in self._oid_cache:
            action.payload['object_ids'] = self._oid_cache[action.type]
        if action.type is ActionEnum.SPECIAL:
            name = action.payload['order']
        elif action.type is ActionEnum.GAME:
            name = action.payload['command']
            if name == 'Speed':
                param = action.payload['speed']
        elif action.type is ActionEnum.STANCE:
            name = 'Stance'
            param = action.payload['stance']
        elif action.type is ActionEnum.FORMATION:
            name = 'Formation'
            param = action.payload['formation']
        elif action.type is ActionEnum.ORDER and action.payload['target_id'] in self._gaia:
            name = 'Gather'
            param = self._gaia[action.payload['target_id']]
        elif action.type is ActionEnum.ORDER and action.position and (action.position.hash() in self._buildings):
            name = 'Target'
            param = self._buildings[action.position.hash()]
        elif action.type is ActionEnum.GATHER_POINT:
            if action.payload['target_id'] in self._gaia:
                param = self._gaia[action.payload['target_id']]
            elif action.position and action.position.hash() in self._buildings:
                if len(action.payload['object_ids']) == 1 and action.payload['object_ids'][0] == action.payload['target_id']:
                    name = 'Spawn'
                param = self._buildings[action.position.hash()]
        elif action.type in (ActionEnum.BUY, ActionEnum.SELL):
            action.payload['amount'] *= 100
        elif action.type is ActionEnum.BUILD:
            param = action.payload['building']
            if action.position.hash() in self._buildings:
                if self._buildings[action.position.hash()] == 'Farm' and action.payload['building'] == 'Farm':
                    name = 'Reseed'
            self._buildings[action.position.hash()] = action.payload['building']
        elif action.type in (ActionEnum.QUEUE, ActionEnum.DE_QUEUE):
            param = action.payload['unit']
        elif action.type is ActionEnum.RESEARCH:
            param = action.payload['technology']
        new_input = Input(action.timestamp, name, param, action.payload, action.player, action.position)
        self.inputs.append(new_input)
        return new_input","
class Inputs:
    
    def __init__(self, gaia):
        
        pass
    def add_chat(self, chat):
        
        pass

    def add_action(self, action):
        
        pass",snippet_165,4,"class Inputs:
    
    def __init__(self, gaia):
        self.gaia = gaia
        self.chats = []
        self.actions = []

    def add_chat(self, chat):
        if chat is None:
            return self
        if isinstance(chat, (list, tuple)):
            self.chats.extend(chat)
        else:
            self.chats.append(chat)
        return self

    def add_action(self, action):
        if action is None:
            return self
        if isinstance(action, (list, tuple)):
            self.actions.extend(action)
        else:
            self.actions.append(action)
        return self"
256858,cherrypy/cheroot,cheroot/server.py,cheroot.server.Gateway,"class Gateway:
    """"""Base class to interface HTTPServer with other systems, such as WSGI.""""""

    def __init__(self, req):
        """"""Initialize Gateway instance with request.

        Args:
            req (HTTPRequest): current HTTP request
        """"""
        self.req = req

    def respond(self):
        """"""Process the current request. Must be overridden in a subclass.""""""
        raise NotImplementedError","class Gateway:
    
    def __init__(self, req):
        
        pass
    def respond(self):
        
        pass",snippet_168,3,"class Gateway:
    
    def __init__(self, req):
        self.req = req

    def respond(self):
        try:
            req = self.req
            if callable(req):
                data = req()
            elif isinstance(req, dict):
                action = req.get('action')
                if action == 'ping':
                    data = 'pong'
                elif action == 'echo':
                    data = req.get('data')
                elif action == 'sum':
                    payload = req.get('data', [])
                    data = sum(payload) if hasattr(payload, '__iter__') else payload
                else:
                    data = req
            elif isinstance(req, (list, tuple)) and all(isinstance(x, (int, float)) for x in req):
                data = sum(req)
            else:
                data = req
            return {'status': 'ok', 'data': data, 'error': None}
        except Exception as e:
            return {'status': 'error', 'data': None, 'error': str(e)}"
258632,chovanecm/sacredboard,chovanecm_sacredboard/sacredboard/app/data/datastorage.py,sacredboard.app.data.datastorage.Cursor,"class Cursor:
    """"""Interface that abstracts the cursor object returned from databases.""""""

    def __init__(self):
        """"""Declare a new cursor to iterate over runs.""""""
        pass

    def count(self):
        """"""Return the number of items in this cursor.""""""
        raise NotImplementedError()

    def __iter__(self):
        """"""Iterate over elements.""""""
        raise NotImplementedError()","class Cursor:
    
    def __init__(self):
        
        pass
    def count(self):
        
        pass
    def __iter__(self):
        
        pass",snippet_169,4,"class Cursor:
    
    def __init__(self):
        self._count = 0

    def count(self):
        return self._count

    def __iter__(self):
        return self

    def __next__(self):
        raise StopIteration"
258643,chovanecm/sacredboard,chovanecm_sacredboard/sacredboard/app/data/metricsdao.py,sacredboard.app.data.metricsdao.MetricsDAO,"class MetricsDAO:
    """"""
    Interface for accessing Sacred metrics.

    Issue: https://github.com/chovanecm/sacredboard/issues/58

    Extended because of: https://github.com/chovanecm/sacredboard/issues/66
    """"""

    def get(self, run_id, metric_id):
        """"""
        Read a metric of the given id and run.

        The returned object has the following format (timestamps are datetime
         objects).

        .. code::

            {""steps"": [0,1,20,40,...],
            ""timestamps"": [timestamp1,timestamp2,timestamp3,...],
            ""values"": [0,1 2,3,4,5,6,...],
            ""name"": ""name of the metric"",
            ""metric_id"": ""metric_id"",
            ""run_id"": ""run_id""}

        :param run_id: ID of the Run that the metric belongs to.
        :param metric_id: The ID fo the metric.
        :return: The whole metric as specified.

        :raise NotFoundError
        """"""
        raise NotImplementedError('The MetricsDAO class is abstract.')

    def delete(self, run_id):
        """"""
        Delete all metrics belonging to the given run.

        :param run_id: ID of the Run that the metric belongs to.
        """"""
        raise NotImplementedError('The MetricsDAO class is abstract.')","class MetricsDAO:
    
    def get(self, run_id, metric_id):
        
        pass
    def delete(self, run_id):
        
        pass",snippet_170,3,"class MetricsDAO:
    def __init__(self, db_path="":memory:""):
        import sqlite3
        self._sqlite3 = sqlite3
        self._conn = sqlite3.connect(db_path, check_same_thread=False)
        self._conn.row_factory = sqlite3.Row
        self._ensure_schema()

    def _ensure_schema(self):
        self._conn.execute(
            """"""
            CREATE TABLE IF NOT EXISTS metrics (
                run_id TEXT NOT NULL,
                metric_id TEXT NOT NULL,
                value REAL,
                timestamp INTEGER,
                PRIMARY KEY (run_id, metric_id)
            )
            """"""
        )
        self._conn.commit()

    def get(self, run_id, metric_id):
        if run_id is None or metric_id is None:
            raise ValueError(""run_id and metric_id must be provided"")
        cur = self._conn.execute(
            ""SELECT run_id, metric_id, value, timestamp FROM metrics WHERE run_id = ? AND metric_id = ?"",
            (str(run_id), str(metric_id)),
        )
        row = cur.fetchone()
        if not row:
            return None
        return {
            ""run_id"": row[""run_id""],
            ""metric_id"": row[""metric_id""],
            ""value"": row[""value""],
            ""timestamp"": row[""timestamp""],
        }

    def delete(self, run_id):
        if run_id is None:
            raise ValueError(""run_id must be provided"")
        cur = self._conn.execute(
            ""DELETE FROM metrics WHERE run_id = ?"",
            (str(run_id),),
        )
        self._conn.commit()
        return cur.rowcount

    # Optional helpers for completeness
    def upsert(self, run_id, metric_id, value=None, timestamp=None):
        self._conn.execute(
            """"""
            INSERT INTO metrics (run_id, metric_id, value, timestamp)
            VALUES (?, ?, ?, ?)
            ON CONFLICT(run_id, metric_id) DO UPDATE SET
                value=excluded.value,
                timestamp=excluded.timestamp
            """""",
            (str(run_id), str(metric_id), value, timestamp),
        )
        self._conn.commit()

    def close(self):
        try:
            self._conn.close()
        except Exception:
            pass"
258651,chovanecm/sacredboard,chovanecm_sacredboard/sacredboard/app/data/rundao.py,sacredboard.app.data.rundao.RunDAO,"class RunDAO:
    """"""
    Interface for accessing Runs.

    Issue: https://github.com/chovanecm/sacredboard/issues/69
    """"""

    def get(self, run_id):
        """"""
        Return the run associated with the id.

        :raise NotFoundError when not found
        """"""
        raise NotImplementedError('RunDAO is abstract.')

    def get_runs(self, sort_by=None, sort_direction=None, start=0, limit=None, query={'type': 'and', 'filters': []}):
        """"""Return all runs that match the query.""""""
        raise NotImplementedError('RunDAO is abstract.')

    def delete(self, run_id):
        """"""
        Delete run with the given id from the backend.

        :param run_id: Id of the run to delete.
        :raise NotImplementedError If not supported by the backend.
        :raise DataSourceError General data source error.
        :raise NotFoundError The run was not found. (Some backends may succeed
        even if the run does not exist.
        """"""
        raise NotImplementedError('This database data source does not currently support this operation.')","class RunDAO:
    
    def get(self, run_id):
        
        pass
    def get_runs(self, sort_by=None, sort_direction=None, start=0, limit=None, query={'type': 'and', 'filters': []}):
        
        pass
    def delete(self, run_id):
        
        pass",snippet_171,4,"class RunDAO:
    def __init__(self):
        self._runs = {}

    def get(self, run_id):
        return self._runs.get(run_id)

    def get_runs(self, sort_by=None, sort_direction=None, start=0, limit=None, query={'type': 'and', 'filters': []}):
        items = list(self._runs.values())
        if query:
            items = [it for it in items if self._match_query(it, query)]

        if sort_by:
            reverse = (str(sort_direction).lower() == 'desc')
            items.sort(key=lambda x: self._get_field(x, sort_by), reverse=reverse)

        if start is None or start < 0:
            start = 0
        if limit is None:
            return items[start:]
        if limit < 0:
            return []
        end = start + limit
        return items[start:end]

    def delete(self, run_id):
        if run_id in self._runs:
            del self._runs[run_id]
            return True
        return False

    # Optional helpers for managing data
    def upsert(self, run):
        run_id = run.get('id') or run.get('_id')
        if run_id is None:
            raise ValueError('run must contain id or _id')
        self._runs[run_id] = run
        return run

    # Internal helpers
    def _get_field(self, obj, dotted_key, default=None):
        parts = str(dotted_key).split('.')
        cur = obj
        for p in parts:
            if isinstance(cur, dict) and p in cur:
                cur = cur[p]
            else:
                return default
        return cur

    def _match_query(self, item, query):
        if query is None:
            return True
        qtype = str(query.get('type', 'and')).lower()
        filters = query.get('filters', [])

        if qtype == 'not':
            if not filters:
                return True
            return not self._match_query(item, filters[0] if isinstance(filters, list) else filters)

        if not isinstance(filters, list):
            filters = [filters]

        results = []
        for f in filters:
            if isinstance(f, dict) and 'type' in f and 'filters' in f:
                results.append(self._match_query(item, f))
            else:
                results.append(self._match_filter(item, f))

        if qtype == 'or':
            return any(results)
        # default to 'and'
        return all(results)

    def _match_filter(self, item, flt):
        if not isinstance(flt, dict):
            return False
        field = flt.get('field')
        op = str(flt.get('op', 'eq')).lower()
        value = flt.get('value')

        actual = self._get_field(item, field, default=None)

        try:
            if op == 'eq':
                return actual == value
            if op == 'ne':
                return actual != value
            if op == 'lt':
                return actual is not None and actual < value
            if op == 'lte':
                return actual is not None and actual <= value
            if op == 'gt':
                return actual is not None and actual > value
            if op == 'gte':
                return actual is not None and actual >= value
            if op == 'in':
                return actual in (value or [])
            if op == 'nin':
                return actual not in (value or [])
            if op == 'exists':
                return (actual is not None) if bool(value) else (actual is None)
            if op == 'contains':
                if actual is None:
                    return False
                if isinstance(actual, (list, tuple, set)):
                    return value in actual
                return str(value) in str(actual)
            if op == 'icontains':
                if actual is None:
                    return False
                return str(value).lower() in str(actual).lower()
            if op == 'startswith':
                return isinstance(actual, str) and isinstance(value, str) and actual.startswith(value)
            if op == 'istartswith':
                return isinstance(actual, str) and isinstance(value, str) and actual.lower().startswith(value.lower())
            if op == 'endswith':
                return isinstance(actual, str) and isinstance(value, str) and actual.endswith(value)
            if op == 'iendswith':
                return isinstance(actual, str) and isinstance(value, str) and actual.lower().endswith(value.lower())
        except Exception:
            return False

        return False"
259541,mozilla-iot/webthing-python,mozilla-iot_webthing-python/webthing/server.py,webthing.server.MultipleThings,"class MultipleThings:
    """"""A container for multiple things.""""""

    def __init__(self, things, name):
        """"""
        Initialize the container.

        things -- the things to store
        name -- the mDNS server name
        """"""
        self.things = things
        self.name = name

    def get_thing(self, idx):
        """"""
        Get the thing at the given index.

        idx -- the index
        """"""
        try:
            idx = int(idx)
        except ValueError:
            return None
        if idx < 0 or idx >= len(self.things):
            return None
        return self.things[idx]

    def get_things(self):
        """"""Get the list of things.""""""
        return self.things

    def get_name(self):
        """"""Get the mDNS server name.""""""
        return self.name","class MultipleThings:
    
    def __init__(self, things, name):
        
        pass
    def get_thing(self, idx):
        
        pass
    def get_things(self):
        
        pass

    def get_name(self):
        
        pass",snippet_172,5,"class MultipleThings:
    
    def __init__(self, things, name):
        self._things = things
        self._name = name

    def get_thing(self, idx):
        return self._things[idx]

    def get_things(self):
        return self._things

    def get_name(self):
        return self._name"
259544,mozilla-iot/webthing-python,mozilla-iot_webthing-python/webthing/server.py,webthing.server.SingleThing,"class SingleThing:
    """"""A container for a single thing.""""""

    def __init__(self, thing):
        """"""
        Initialize the container.

        thing -- the thing to store
        """"""
        self.thing = thing

    def get_thing(self, _=None):
        """"""Get the thing at the given index.""""""
        return self.thing

    def get_things(self):
        """"""Get the list of things.""""""
        return [self.thing]

    def get_name(self):
        """"""Get the mDNS server name.""""""
        return self.thing.title","class SingleThing:
    
    def __init__(self, thing):
        
        pass
    def get_thing(self, _=None):
        
        pass
    def get_things(self):
        
        pass

    def get_name(self):
        
        pass",snippet_173,5,"class SingleThing:
    
    def __init__(self, thing):
        self._thing = thing

    def get_thing(self, _=None):
        return self._thing

    def get_things(self):
        return [self._thing]

    def get_name(self):
        t = self._thing
        if hasattr(t, 'get_name') and callable(getattr(t, 'get_name')):
            try:
                return t.get_name()
            except Exception:
                pass
        if hasattr(t, 'name'):
            try:
                return t.name
            except Exception:
                pass
        if hasattr(t, '__name__'):
            return t.__name__
        return str(t)"
259548,mozilla-iot/webthing-python,mozilla-iot_webthing-python/webthing/subscriber.py,webthing.subscriber.Subscriber,"class Subscriber:
    """"""Abstract Subscriber class.""""""

    def update_property(self, property_):
        """"""
        Send an update about a Property.

        :param property_: Property
        """"""
        raise NotImplementedError

    def update_action(self, action):
        """"""
        Send an update about an Action.

        :param action: Action
        """"""
        raise NotImplementedError

    def update_event(self, event):
        """"""
        Send an update about an Event.

        :param event: Event
        """"""
        raise NotImplementedError","class Subscriber:
    
    def update_property(self, property_):
        
        pass
    def update_action(self, action):
        
        pass
    def update_event(self, event):
        
        pass",snippet_174,4,"class Subscriber:
    def __init__(self):
        self.properties = []
        self.actions = []
        self.events = []
        self._counts = {""property"": 0, ""action"": 0, ""event"": 0}

    def update_property(self, property_):
        self.properties.append(property_)
        self._counts[""property""] += 1
        return property_

    def update_action(self, action):
        self.actions.append(action)
        self._counts[""action""] += 1
        return action

    def update_event(self, event):
        self.events.append(event)
        self._counts[""event""] += 1
        return event"
261819,user-cont/conu,user-cont_conu/conu/apidefs/image.py,conu.apidefs.image.S2Image,"class S2Image:
    """"""
    Additional functionality related to s2i-enabled container images
    """"""

    def extend(self, source, new_image_name, s2i_args=None):
        """"""
        extend this s2i-enabled image using provided source, raises ConuException if
        `s2i build` fails

        :param source: str, source used to extend the image, can be path or url
        :param new_image_name: str, name of the new, extended image
        :param s2i_args: list of str, additional options and arguments provided to `s2i build`
        :return: S2Image instance
        """"""
        raise NotImplementedError('extend method is not implemented')

    def usage(self):
        """"""
        Provide output of `s2i usage`

        :return: str
        """"""
        raise NotImplementedError('usage method is not implemented')","class S2Image:
    
    def extend(self, source, new_image_name, s2i_args=None):
        
        pass
    def usage(self):
        
        pass",snippet_175,3,"class S2Image:
    
    def extend(self, source, new_image_name, s2i_args=None):
        import os
        import shutil
        import subprocess

        if not isinstance(source, str) or not source.strip():
            raise ValueError(""source must be a non-empty string (local path or git URL)"")
        if not isinstance(new_image_name, str) or not new_image_name.strip():
            raise ValueError(""new_image_name must be a non-empty string"")
        s2i_args = s2i_args or {}

        s2i_bin = shutil.which(""s2i"") or shutil.which(""source-to-image"")
        if not s2i_bin:
            raise FileNotFoundError(""s2i binary not found in PATH"")

        builder_image = s2i_args.get(""builder_image"") or os.getenv(""S2I_BUILDER_IMAGE"")
        if not builder_image:
            raise ValueError(""builder_image is required in s2i_args or via S2I_BUILDER_IMAGE env var"")

        cmd = [s2i_bin, ""build"", source, builder_image, new_image_name]

        # Optional flags
        if s2i_args.get(""incremental"") is True:
            cmd.append(""--incremental"")
        if s2i_args.get(""rm"") is True:
            cmd.append(""--rm"")
        if s2i_args.get(""loglevel"") is not None:
            cmd.extend([""--loglevel"", str(s2i_args[""loglevel""])])
        if s2i_args.get(""scripts_url""):
            cmd.extend([""--scripts-url"", str(s2i_args[""scripts_url""])])
        if s2i_args.get(""ref""):
            cmd.extend([""--ref"", str(s2i_args[""ref""])])
        if s2i_args.get(""context_dir""):
            cmd.extend([""--context-dir"", str(s2i_args[""context_dir""])])
        if s2i_args.get(""image_workdir""):
            cmd.extend([""--image-workdir"", str(s2i_args[""image_workdir""])])
        if s2i_args.get(""assemble_user""):
            cmd.extend([""--assemble-user"", str(s2i_args[""assemble_user""])])

        # Environment variables passed to the build image
        env_vars = s2i_args.get(""env"") or {}
        for k, v in env_vars.items():
            if k is None:
                continue
            v = """" if v is None else str(v)
            cmd.extend([""--env"", f""{k}={v}""])

        # Labels
        labels = s2i_args.get(""labels"") or {}
        for k, v in labels.items():
            if k is None:
                continue
            v = """" if v is None else str(v)
            cmd.extend([""--label"", f""{k}={v}""])

        # Inject files (path on host:destination in image)
        inject = s2i_args.get(""inject"") or []
        for spec in inject:
            cmd.extend([""--inject"", str(spec)])

        # Additional arbitrary CLI tokens
        additional = s2i_args.get(""additional_args"") or []
        cmd.extend(map(str, additional))

        try:
            proc = subprocess.run(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=True,
                text=True,
            )
        except subprocess.CalledProcessError as e:
            msg = []
            msg.append(""s2i build failed"")
            msg.append(f""Command: {' '.join(cmd)}"")
            if e.stdout:
                msg.append(f""stdout:\n{e.stdout}"")
            if e.stderr:
                msg.append(f""stderr:\n{e.stderr}"")
            raise RuntimeError(""\n"".join(msg)) from e

        return {
            ""image"": new_image_name,
            ""cmd"": cmd,
            ""stdout"": proc.stdout,
            ""stderr"": proc.stderr,
            ""returncode"": proc.returncode,
        }

    def usage(self):
        return (
            ""S2Image.extend(source, new_image_name, s2i_args=None)\n""
            ""- Builds a new container image using Source-to-Image (s2i).\n""
            ""- Parameters:\n""
            ""  source: Path or Git URL of the source code.\n""
            ""  new_image_name: Name:tag for the resulting image.\n""
            ""  s2i_args: Optional dict with keys:\n""
            ""    builder_image (str, required if S2I_BUILDER_IMAGE env not set)\n""
            ""    incremental (bool)\n""
            ""    rm (bool)\n""
            ""    loglevel (int)\n""
            ""    scripts_url (str)\n""
            ""    ref (str)  # git ref\n""
            ""    context_dir (str)\n""
            ""    image_workdir (str)\n""
            ""    assemble_user (str)\n""
            ""    env (dict[str, str])  # passed as --env KEY=VAL\n""
            ""    labels (dict[str, str])  # passed as --label KEY=VAL\n""
            ""    inject (list[str])  # items like hostPath:destination\n""
            ""    additional_args (list[str])  # extra s2i CLI tokens\n""
            ""Returns a dict with keys: image, cmd, stdout, stderr, returncode.""
        )"
264367,materialsproject/custodian,materialsproject_custodian/src/custodian/ansible/interpreter.py,custodian.ansible.interpreter.Modder,"from custodian.ansible.actions import DictActions
import re

class Modder:
    """"""
    Class to modify a dict/file/any object using a mongo-like language.
    Keywords are mostly adopted from mongo's syntax, but instead of $, an
    underscore precedes action keywords. This is so that the modification can
    be inserted into a mongo db easily.

    Allowable actions are supplied as a list of classes as an argument. Refer
    to the action classes on what the actions do. Action classes are in
    pymatpro.ansible.actions.

    Examples:
    >>> modder = Modder()
    >>> dct = {""Hello"": ""World""}
    >>> mod = {'_set': {'Hello':'Universe', 'Bye': 'World'}}
    >>> modder.modify(mod, dct)
    >>> dct['Bye']
    'World'
    >>> dct['Hello']
    'Universe'
    """"""

    def __init__(self, actions=None, strict=True, directory='./') -> None:
        """"""Initialize a Modder from a list of supported actions.

        Args:
            actions ([Action]): A sequence of supported actions. See
                :mod:`custodian.ansible.actions`. Default is None,
                which means only DictActions are supported.
            strict (bool): Indicating whether to use strict mode. In non-strict
                mode, unsupported actions are simply ignored without any
                errors raised. In strict mode, if an unsupported action is
                supplied, a ValueError is raised. Defaults to True.
            directory (str): The directory containing the files to be modified.
                Defaults to ""./"".
        """"""
        self.supported_actions = {}
        actions = actions if actions is not None else [DictActions]
        for action in actions:
            for attr in dir(action):
                if not re.match('__\\w+__', attr) and callable(getattr(action, attr)):
                    self.supported_actions[f'_{attr}'] = getattr(action, attr)
        self.strict = strict
        self.directory = directory

    def modify(self, modification, obj) -> None:
        """"""
        Note that modify makes actual in-place modifications. It does not
        return a copy.

        Args:
            modification (dict): Modification must be {action_keyword :
                settings}. E.g., {'_set': {'Hello':'Universe', 'Bye': 'World'}}
            obj (dict/str/object): Object to modify depending on actions. For
                example, for DictActions, obj will be a dict to be modified.
                For FileActions, obj will be a string with a full pathname to a
                file.
        """"""
        for action, settings in modification.items():
            if action in self.supported_actions:
                self.supported_actions[action](obj, settings, directory=self.directory)
            elif self.strict:
                raise ValueError(f'{action} is not a supported action!')

    def modify_object(self, modification, obj):
        """"""
        Modify an object that supports pymatgen's as_dict() and from_dict API.

        Args:
            modification (dict): Modification must be {action_keyword :
                settings}. E.g., {'_set': {'Hello':'Universe', 'Bye': 'World'}}
            obj (object): Object to modify
        """"""
        dct = obj.as_dict()
        self.modify(modification, dct)
        return obj.from_dict(dct)","
class Modder:
    '''
    Class to modify a dict/file/any object using a mongo-like language.
    Keywords are mostly adopted from mongo's syntax, but instead of $, an
    underscore precedes action keywords. This is so that the modification can
    be inserted into a mongo db easily.
    Allowable actions are supplied as a list of classes as an argument. Refer
    to the action classes on what the actions do. Action classes are in
    pymatpro.ansible.actions.
    Examples:
    >>> modder = Modder()
    >>> dct = {""Hello"": ""World""}
    >>> mod = {'_set': {'Hello':'Universe', 'Bye': 'World'}}
    >>> modder.modify(mod, dct)
    >>> dct['Bye']
    'World'
    >>> dct['Hello']
    'Universe'
    '''
    def __init__(self, actions=None, strict=True, directory='./') -> None:
        
        pass

    def modify(self, modification, obj) -> None:
        
        pass

    def modify_object(self, modification, obj):
        
        pass",snippet_176,4,"class Modder:
    '''
    Class to modify a dict/file/any object using a mongo-like language.
    Keywords are mostly adopted from mongo's syntax, but instead of $, an
    underscore precedes action keywords. This is so that the modification can
    be inserted into a mongo db easily.
    Allowable actions are supplied as a list of classes as an argument. Refer
    to the action classes on what the actions do. Action classes are in
    pymatpro.ansible.actions.
    Examples:
    >>> modder = Modder()
    >>> dct = {""Hello"": ""World""}
    >>> mod = {'_set': {'Hello':'Universe', 'Bye': 'World'}}
    >>> modder.modify(mod, dct)
    >>> dct['Bye']
    'World'
    >>> dct['Hello']
    'Universe'
    '''
    def __init__(self, actions=None, strict=True, directory='./') -> None:
        self.strict = bool(strict)
        self.directory = directory
        self._actions = {}

        # Register built-in default actions
        self._register_action('_set', self._act_set)
        self._register_action('_unset', self._act_unset)
        self._register_action('_inc', self._act_inc)
        self._register_action('_push', self._act_push)
        self._register_action('_extend', self._act_extend)
        self._register_action('_toggle', self._act_toggle)

        # Register additional actions if provided
        if actions:
            for act in actions:
                self._register_external_action(act)

    def modify(self, modification, obj) -> None:
        if modification is None:
            return
        if isinstance(modification, (list, tuple)):
            for m in modification:
                self.modify_object(m, obj)
        else:
            self.modify_object(modification, obj)

    def modify_object(self, modification, obj):
        if not isinstance(modification, dict):
            raise TypeError(""Modification must be a dict or a list of dicts."")
        for key, payload in modification.items():
            if not isinstance(key, str) or not key.startswith('_'):
                # Skip non-action keys silently unless strict
                if self.strict:
                    raise KeyError(f""Unknown action: {key}"")
                continue
            action = self._actions.get(key)
            if action is None:
                if self.strict:
                    raise KeyError(f""Unsupported action: {key}"")
                continue
            action(obj, payload)

    # Action registration helpers

    def _register_action(self, keyword, func):
        self._actions[str(keyword)] = func

    def _register_external_action(self, action):
        # Accept:
        # - function with attribute 'keyword'/'key'/'name'
        # - class with class attribute 'keyword'/'key'/'name' and method 'apply'
        keyword = None
        apply_fn = None

        # Callable function
        if callable(action) and not isinstance(action, type):
            keyword = getattr(action, 'keyword', None) or getattr(action, 'key', None) or getattr(action, 'name', None)
            apply_fn = action

        # Class-like
        if keyword is None and isinstance(action, type):
            keyword = getattr(action, 'keyword', None) or getattr(action, 'key', None) or getattr(action, 'name', None)
            if hasattr(action, 'apply') and callable(getattr(action, 'apply')):
                inst = action()
                apply_fn = lambda target, payload, _inst=inst: _inst.apply(self, target, payload)

        if keyword is None or apply_fn is None:
            if self.strict:
                raise ValueError(""Invalid action specification. Provide a callable with a keyword or a class with keyword and apply()."")
            return

        keyword = str(keyword)
        if not keyword.startswith('_'):
            keyword = '_' + keyword

        # Wrap to uniform signature (obj, payload)
        def wrapper(target, payload, _fn=apply_fn):
            # External apply may have one of signatures:
            # - fn(modder, target, payload)
            # - fn(target, payload)
            try:
                return _fn(self, target, payload)
            except TypeError:
                return _fn(target, payload)

        self._register_action(keyword, wrapper)

    # Built-in action implementations

    def _act_set(self, target, payload):
        if not isinstance(payload, dict):
            raise TypeError(""_set payload must be a dict of path: value"")
        for path, value in payload.items():
            self._set_path(target, path, value)

    def _act_unset(self, target, payload):
        # payload can be list of paths or dict of path: True
        paths = []
        if isinstance(payload, dict):
            paths = [p for p, flag in payload.items() if flag or flag is None or flag is False]
        elif isinstance(payload, (list, tuple, set)):
            paths = list(payload)
        elif isinstance(payload, str):
            paths = [payload]
        else:
            raise TypeError(""_unset payload must be a dict/list/str"")
        for path in paths:
            self._del_path(target, path)

    def _act_inc(self, target, payload):
        if not isinstance(payload, dict):
            raise TypeError(""_inc payload must be a dict of path: number"")
        for path, inc in payload.items():
            cur = self._get_path(target, path, missing_sentinel=_MISSING)
            if cur is _MISSING:
                new_val = inc
            else:
                if not isinstance(inc, (int, float)):
                    raise TypeError(""_inc value must be numeric"")
                try:
                    new_val = cur + inc
                except Exception as e:
                    raise TypeError(f""Cannot increment non-numeric at {path}: {e}"")
            self._set_path(target, path, new_val)

    def _act_push(self, target, payload):
        # payload: dict path -> value (append)
        if not isinstance(payload, dict):
            raise TypeError(""_push payload must be a dict of path: value"")
        for path, value in payload.items():
            lst = self._get_path(target, path, missing_sentinel=_MISSING)
            if lst is _MISSING:
                lst = []
            if not isinstance(lst, list):
                raise TypeError(f""_push target at {path} is not a list"")
            lst.append(value)
            self._set_path(target, path, lst)

    def _act_extend(self, target, payload):
        # payload: dict path -> iterable
        if not isinstance(payload, dict):
            raise TypeError(""_extend payload must be a dict of path: iterable"")
        for path, values in payload.items():
            lst = self._get_path(target, path, missing_sentinel=_MISSING)
            if lst is _MISSING:
                lst = []
            if not isinstance(lst, list):
                raise TypeError(f""_extend target at {path} is not a list"")
            try:
                lst.extend(values)
            except TypeError:
                raise TypeError(""_extend values must be iterable"")
            self._set_path(target, path, lst)

    def _act_toggle(self, target, payload):
        # payload: dict path -> any (ignored) or list/str of paths
        paths = []
        if isinstance(payload, dict):
            paths = list(payload.keys())
        elif isinstance(payload, (list, tuple, set)):
            paths = list(payload)
        elif isinstance(payload, str):
            paths = [payload]
        else:
            raise TypeError(""_toggle payload must be dict/list/str"")
        for path in paths:
            cur = self._get_path(target, path, missing_sentinel=_MISSING)
            if cur is _MISSING:
                self._set_path(target, path, True)
            elif isinstance(cur, bool):
                self._set_path(target, path, not cur)
            else:
                raise TypeError(f""_toggle target at {path} is not boolean"")

    # Path helpers

    def _split_path(self, path):
        if isinstance(path, (list, tuple)):
            parts = list(path)
        elif isinstance(path, str):
            parts = path.split('.') if path else []
        else:
            parts = [path]
        return [self._coerce_index(p) for p in parts if p != '']

    def _coerce_index(self, part):
        if isinstance(part, int):
            return part
        if isinstance(part, str) and part.isdigit():
            try:
                return int(part)
            except ValueError:
                return part
        return part

    def _is_mapping(self, obj):
        try:
            from collections.abc import Mapping
        except Exception:
            Mapping = dict
        return isinstance(obj, Mapping)

    def _get_from(self, obj, key, missing=_MISSING):
        if self._is_mapping(obj):
            return obj.get(key, missing)
        # list index
        if isinstance(obj, list) and isinstance(key, int):
            if -len(obj) <= key < len(obj):
                return obj[key]
            return missing
        # attribute
        if hasattr(obj, key):
            return getattr(obj, key)
        # fallback for __getitem__
        try:
            return obj[key]
        except Exception:
            return missing

    def _set_into(self, obj, key, value):
        if self._is_mapping(obj):
            obj[key] = value
            return
        if isinstance(obj, list) and isinstance(key, int):
            # expand list if needed
            if key < 0:
                raise IndexError(""Negative indices not supported for set"")
            if key >= len(obj):
                obj.extend([None] * (key - len(obj) + 1))
            obj[key] = value
            return
        # attribute if possible
        try:
            setattr(obj, key, value)
            return
        except Exception:
            pass
        # fallback to item assignment
        try:
            obj[key] = value
            return
        except Exception as e:
            raise TypeError(f""Cannot set key '{key}' on {type(obj).__name__}: {e}"")

    def _del_from(self, obj, key):
        if self._is_mapping(obj):
            obj.pop(key, None)
            return
        if isinstance(obj, list) and isinstance(key, int):
            if -len(obj) <= key < len(obj):
                del obj[key]
            return
        if hasattr(obj, key):
            try:
                delattr(obj, key)
                return
            except Exception:
                pass
        try:
            del obj[key]
        except Exception:
            pass

    def _get_path(self, obj, path, missing_sentinel=None):
        parts = self._split_path(path)
        cur = obj
        for p in parts:
            cur = self._get_from(cur, p, missing=_MISSING)
            if cur is _MISSING:
                return missing_sentinel if missing_sentinel is not None else _MISSING
        return cur

    def _ensure_container(self, parent, key, next_key):
        val = self._get_from(parent, key, missing=_MISSING)
        if val is _MISSING or val is None:
            # choose dict for str keys, list for int keys
            container = [] if isinstance(next_key, int) else {}
            self._set_into(parent, key, container)
            return container
        return val

    def _set_path(self, obj, path, value):
        parts = self._split_path(path)
        if not parts:
            raise ValueError(""Empty path"")
        cur = obj
        for i, p in enumerate(parts[:-1]):
            nxt = parts[i + 1]
            # navigate or create
            existing = self._get_from(cur, p, missing=_MISSING)
            if existing is _MISSING or existing is None:
                existing = self._ensure_container(cur, p, nxt)
            cur = existing
        self._set_into(cur, parts[-1], value)

    def _del_path(self, obj, path):
        parts = self._split_path(path)
        if not parts:
            return
        cur = obj
        for p in parts[:-1]:
            cur = self._get_from(cur, p, missing=_MISSING)
            if cur is _MISSING:
                return
        self._del_from(cur, parts[-1])


class _Missing:
    pass


_MISSING = _Missing()"
264406,materialsproject/custodian,materialsproject_custodian/src/custodian/utils.py,custodian.utils.tracked_lru_cache,"import functools
from typing import ClassVar

class tracked_lru_cache:
    """"""
    Decorator wrapping the functools.lru_cache adding a tracking of the
    functions that have been wrapped.

    Exposes a method to clear the cache of all the wrapped functions.

    Used to cache the parsed outputs in handlers/validators, to avoid
    multiple parsing of the same file.
    Allows Custodian to clear the cache after all the checks have been performed.
    """"""
    cached_functions: ClassVar = set()

    def __init__(self, func) -> None:
        """"""
        Args:
            func: function to be decorated.
        """"""
        self.func = functools.lru_cache(func)
        functools.update_wrapper(self, func)
        self.cache_info = self.func.cache_info
        self.cache_clear = self.func.cache_clear

    def __call__(self, *args, **kwargs):
        """"""Call the decorated function.""""""
        result = self.func(*args, **kwargs)
        self.cached_functions.add(self.func)
        return result

    @classmethod
    def tracked_cache_clear(cls) -> None:
        """"""Clear the cache of all the decorated functions.""""""
        while cls.cached_functions:
            f = cls.cached_functions.pop()
            f.cache_clear()","
class tracked_lru_cache:
    
    def __init__(self, func) -> None:
        
        pass
    def __call__(self, *args, **kwargs):
        
        pass
    @classmethod
    def tracked_cache_clear(cls) -> None:
        
        pass",snippet_177,4,"class tracked_lru_cache:
    _instances = None

    def __init__(self, func) -> None:
        import threading
        from collections import OrderedDict
        import weakref

        if tracked_lru_cache._instances is None:
            tracked_lru_cache._instances = weakref.WeakSet()

        self._func = func
        self._cache = OrderedDict()
        self._lock = threading.RLock()
        self._maxsize = 128
        self.__wrapped__ = func  # for introspection
        tracked_lru_cache._instances.add(self)

    def __get__(self, obj, objtype=None):
        if obj is None:
            return self
        import functools
        return functools.partial(self.__call__, obj)

    def _make_key(self, args, kwargs):
        if not kwargs:
            return (""__K__"", args, None)
        try:
            items = tuple(sorted(kwargs.items()))
        except TypeError:
            # Unorderable keys; fallback to insertion order (still might be unhashable)
            items = tuple(kwargs.items())
        return (""__K__"", args, items)

    def __call__(self, *args, **kwargs):
        key = None
        try:
            key = self._make_key(args, kwargs)
            hash(key)
        except Exception:
            return self.__wrapped__(*args, **kwargs)

        with self._lock:
            if key in self._cache:
                val = self._cache.pop(key)
                self._cache[key] = val
                return val

        result = self.__wrapped__(*args, **kwargs)

        with self._lock:
            # Store and enforce LRU size
            if key in self._cache:
                # Another thread may have populated it meanwhile
                self._cache.pop(key, None)
            self._cache[key] = result
            if self._maxsize is not None and self._maxsize > 0:
                while len(self._cache) > self._maxsize:
                    self._cache.popitem(last=False)
        return result

    @classmethod
    def tracked_cache_clear(cls) -> None:
        insts = list(cls._instances) if cls._instances is not None else []
        for inst in insts:
            try:
                with inst._lock:
                    inst._cache.clear()
            except Exception:
                pass"
265199,Anaconda-Platform/anaconda-client,Anaconda-Platform_anaconda-client/binstar_client/utils/tables.py,binstar_client.utils.tables.TableCell,"import typing

class TableCell:
    """"""
    General definition of a table cell.

    :param kind: Kind of the cell (used for styling purposes, see :class:`~TableDesign`).
    :param value: Exact content of the cell.
    :param alignment: How text should be aligned in the cell.
    """"""
    __slots__ = ('alignment', 'kind', 'value')

    def __init__(self, kind: str, value: typing.Any, *, alignment: 'Alignment'='<') -> None:
        """"""Initialize new :class:`~TableCell` instance.""""""
        if value is None:
            value = ''
        self.alignment: 'Alignment' = alignment
        self.kind: str = kind
        self.value: str = str(value)

    def __repr__(self) -> str:
        """"""Prepare a string representation of the instance.""""""
        return f'{type(self).__name__}(kind={self.kind!r}, value={self.value!r}, alignment={self.alignment!r})'

    def __str__(self) -> str:
        """"""Prepare a string representation of the instance.""""""
        return self.value","
class TableCell:
    
    def __init__(self, kind: str, value: typing.Any, *, alignment: 'Alignment'='<') -> None:
        
        pass
    def __repr__(self) -> str:
        
        pass

    def __str__(self) -> str:
        
        pass",snippet_178,4,"import typing

Alignment = typing.Literal['<', '>', '^']


class TableCell:
    __slots__ = (""kind"", ""value"", ""alignment"")

    def __init__(self, kind: str, value: typing.Any, *, alignment: Alignment = '<') -> None:
        if alignment not in ('<', '>', '^'):
            raise ValueError(f""Invalid alignment: {alignment!r}. Expected one of '<', '>', '^'."")
        self.kind = str(kind)
        self.value = value
        self.alignment = alignment

    def __repr__(self) -> str:
        return f""TableCell(kind={self.kind!r}, value={self.value!r}, alignment={self.alignment!r})""

    def __str__(self) -> str:
        return '' if self.value is None else str(self.value)"
266878,facelessuser/wcmatch,facelessuser_wcmatch/wcmatch/util.py,wcmatch.util.Immutable,"from typing import Any, Callable, AnyStr, Match, Pattern

class Immutable:
    """"""Immutable.""""""
    __slots__: tuple[Any, ...] = ()

    def __init__(self, **kwargs: Any) -> None:
        """"""Initialize.""""""
        for k, v in kwargs.items():
            super(Immutable, self).__setattr__(k, v)

    def __setattr__(self, name: str, value: Any) -> None:
        """"""Prevent mutability.""""""
        raise AttributeError('Class is immutable!')","
class Immutable:
    
    def __init__(self, **kwargs: Any) -> None:
        
        pass
    def __setattr__(self, name: str, value: Any) -> None:
        
        pass",snippet_180,3,"from typing import Any

class Immutable:
    def __init__(self, **kwargs: Any) -> None:
        for k, v in kwargs.items():
            object.__setattr__(self, k, v)
        object.__setattr__(self, '_Immutable__frozen', True)

    def __setattr__(self, name: str, value: Any) -> None:
        if getattr(self, '_Immutable__frozen', False):
            raise AttributeError(""Immutable instances do not support attribute assignment"")
        object.__setattr__(self, name, value)"
269275,CLARIAH/grlc,CLARIAH_grlc/src/fileLoaders.py,src.fileLoaders.BaseLoader,"from grlc.queryTypes import qType, guessQueryType
import json

class BaseLoader:
    """"""Base class for File Loaders""""""

    def getTextForName(self, query_name):
        """"""Return the query text and query type for the given query name.
        Note that file extention is not part of the query name. For example,
        for `query_name='query1'` would return the content of file `query1.rq`
        from the loader's source (assuming such file exists).""""""
        candidateNames = [query_name + '.rq', query_name + '.sparql', query_name + '.tpf', query_name + '.json']
        candidates = [(name, guessQueryType(name)) for name in candidateNames]
        for queryFullName, queryType in candidates:
            queryText = self._getText(queryFullName)
            if queryText:
                if queryType == qType['JSON']:
                    queryText = json.loads(queryText)
                    if 'proto' not in queryText and '@graph' not in queryText:
                        continue
                return (queryText, queryType)
        return ('', None)

    def _getText(self, queryFullName):
        """"""To be implemented by sub-classes.
        Returns None if the file does not exist.""""""
        raise NotImplementedError('Subclasses must override _getText()!')

    def fetchFiles(self):
        """"""To be implemented by sub-classes""""""
        raise NotImplementedError('Subclasses must override fetchFiles()!')","
class BaseLoader:
    
    def getTextForName(self, query_name):
        
        pass
    def _getText(self, queryFullName):
        
        pass

    def fetchFiles(self):
        
        pass",snippet_181,4,"import os
import io
from pathlib import Path
from typing import Dict, List, Optional


class BaseLoader:
    def __init__(self, root: Optional[os.PathLike] = None, extensions: Optional[List[str]] = None, encoding: str = ""utf-8""):
        self.root = Path(root or ""."").resolve()
        self.extensions = set(e.lower() if e.startswith(""."") else f"".{e.lower()}"" for e in (extensions or [])) or None
        self.encoding = encoding
        self._index: Dict[str, List[Path]] = {}
        self._cache: Dict[Path, str] = {}
        self._indexed = False

    def _ensure_index(self):
        if self._indexed:
            return
        for path in self.fetchFiles():
            name = path.name
            stem = path.stem
            self._index.setdefault(name.lower(), []).append(path)
            self._index.setdefault(stem.lower(), []).append(path)
        self._indexed = True

    def getTextForName(self, query_name):
        if not isinstance(query_name, str) or not query_name.strip():
            raise ValueError(""query_name must be a non-empty string"")

        self._ensure_index()

        q = query_name.strip()
        qp = Path(q)
        if qp.is_absolute() or any(sep in q for sep in (os.sep, os.altsep) if sep):
            target = (qp if qp.is_absolute() else (self.root / qp)).resolve()
            if not target.exists() or not target.is_file():
                raise FileNotFoundError(f""No such file: {target}"")
            return self._getText(str(target))

        candidates = self._index.get(q.lower(), [])
        if not candidates:
            raise FileNotFoundError(f""No files found matching name or stem: {q}"")

        exact = [p for p in candidates if p.name.lower() == q.lower()]
        if len(exact) == 1:
            return self._getText(str(exact[0]))
        if len(candidates) == 1:
            return self._getText(str(candidates[0]))

        dedup = []
        seen = set()
        for p in candidates:
            if p not in seen:
                seen.add(p)
                dedup.append(p)

        raise ValueError(f""Ambiguous query_name '{q}'; matches: {', '.join(str(p) for p in dedup)}"")

    def _getText(self, queryFullName):
        path = Path(queryFullName)
        if not path.is_absolute():
            path = (self.root / path).resolve()

        if path in self._cache:
            return self._cache[path]

        if not path.exists() or not path.is_file():
            raise FileNotFoundError(f""No such file: {path}"")

        with io.open(path, ""r"", encoding=self.encoding, errors=""replace"") as f:
            text = f.read()

        self._cache[path] = text
        return text

    def fetchFiles(self):
        files = []
        if not self.root.exists() or not self.root.is_dir():
            return files
        for dirpath, _, filenames in os.walk(self.root):
            for fname in filenames:
                p = Path(dirpath) / fname
                if self.extensions is not None:
                    if p.suffix.lower() not in self.extensions:
                        continue
                files.append(p.resolve())
        return files"
271814,shoebot/shoebot,shoebot_shoebot/lib/photobot/__init__.py,photobot.Blend,"from PIL import Image, ImageChops, ImageFilter, ImageEnhance, ImageOps, ImageDraw, ImageStat

class Blend:
    """"""Layer blending modes.

    Implements additional blending modes to those present in PIL.
    These blending functions can not be used separately from
    the canvas.flatten() method, where the alpha compositing
    of two layers is handled.

    Since these blending are not part of a C library,
    but pure Python, they take forever to process.
    """"""

    def overlay(self, img1, img2):
        """"""Applies the overlay blend mode.

        Overlays image img2 on image img1. The overlay pixel combines
        multiply and screen: it multiplies dark pixels values and screen
        light values. Returns a composite image with the alpha channel
        retained.
        """"""
        p1 = list(img1.getdata())
        p2 = list(img2.getdata())
        for i in range(len(p1)):
            p3 = ()
            for j in range(len(p1[i])):
                a = p1[i][j] / 255.0
                b = p2[i][j] / 255.0
                if j == 3:
                    d = min(a, b)
                elif a > 0.5:
                    d = 2 * (a + b - a * b) - 1
                else:
                    d = 2 * a * b
                p3 += (int(d * 255),)
            p1[i] = p3
        img = Image.new('RGBA', img1.size, 255)
        img.putdata(p1)
        return img

    def hue(self, img1, img2):
        """"""Applies the hue blend mode.

        Hues image img1 with image img2. The hue filter replaces the
        hues of pixels in img1 with the hues of pixels in img2. Returns
        a composite image with the alpha channel retained.
        """"""
        import colorsys
        p1 = list(img1.getdata())
        p2 = list(img2.getdata())
        for i in range(len(p1)):
            r1, g1, b1, a1 = p1[i]
            r1 = r1 / 255.0
            g1 = g1 / 255.0
            b1 = b1 / 255.0
            h1, s1, v1 = colorsys.rgb_to_hsv(r1, g1, b1)
            r2, g2, b2, a2 = p2[i]
            r2 = r2 / 255.0
            g2 = g2 / 255.0
            b2 = b2 / 255.0
            h2, s2, v2 = colorsys.rgb_to_hsv(r2, g2, b2)
            r3, g3, b3 = colorsys.hsv_to_rgb(h2, s1, v1)
            r3 = int(r3 * 255)
            g3 = int(g3 * 255)
            b3 = int(b3 * 255)
            p1[i] = (r3, g3, b3, a1)
        img = Image.new('RGBA', img1.size, 255)
        img.putdata(p1)
        return img

    def color(self, img1, img2):
        """"""Applies the color blend mode.

        Colorize image img1 with image img2. The color filter replaces
        the hue and saturation of pixels in img1 with the hue and
        saturation of pixels in img2. Returns a composite image with the
        alpha channel retained.
        """"""
        import colorsys
        p1 = list(img1.getdata())
        p2 = list(img2.getdata())
        for i in range(len(p1)):
            r1, g1, b1, a1 = p1[i]
            r1 = r1 / 255.0
            g1 = g1 / 255.0
            b1 = b1 / 255.0
            h1, s1, v1 = colorsys.rgb_to_hsv(r1, g1, b1)
            r2, g2, b2, a2 = p2[i]
            r2 = r2 / 255.0
            g2 = g2 / 255.0
            b2 = b2 / 255.0
            h2, s2, v2 = colorsys.rgb_to_hsv(r2, g2, b2)
            r3, g3, b3 = colorsys.hsv_to_rgb(h2, s2, v1)
            r3 = int(r3 * 255)
            g3 = int(g3 * 255)
            b3 = int(b3 * 255)
            p1[i] = (r3, g3, b3, a1)
        img = Image.new('RGBA', img1.size, 255)
        img.putdata(p1)
        return img","
class Blend:
    
    def overlay(self, img1, img2):
        
        pass
    def hue(self, img1, img2):
        
        pass

    def color(self, img1, img2):
        
        pass",snippet_182,4,"import numpy as np
import colorsys

class Blend:
    def _to_float(self, img):
        img = np.asarray(img)
        if img.dtype == np.uint8:
            return img.astype(np.float32) / 255.0, ('uint8',)
        elif img.dtype == np.float32 or img.dtype == np.float64:
            return img.astype(np.float32), ('float', img.dtype)
        else:
            return img.astype(np.float32), ('float', np.float32)

    def _from_float(self, img_f, meta):
        img_f = np.clip(img_f, 0.0, 1.0)
        if meta[0] == 'uint8':
            return (img_f * 255.0 + 0.5).astype(np.uint8)
        else:
            return img_f.astype(meta[1])

    def _check_shapes(self, img1, img2):
        if img1.shape != img2.shape:
            raise ValueError(""img1 and img2 must have the same shape"")
        if img1.ndim != 3 or img1.shape[-1] not in (3, 4):
            raise ValueError(""Images must have shape (H, W, 3) or (H, W, 4)"")

    def overlay(self, img1, img2):
        self._check_shapes(img1, img2)
        f1, meta = self._to_float(img1)
        f2, _ = self._to_float(img2)

        has_alpha = f1.shape[-1] == 4
        if has_alpha:
            a = f1[..., 3:4]
            c1 = f1[..., :3]
            c2 = f2[..., :3]
        else:
            c1 = f1
            c2 = f2

        res = np.where(
            c1 <= 0.5,
            2.0 * c1 * c2,
            1.0 - 2.0 * (1.0 - c1) * (1.0 - c2)
        )
        res = np.clip(res, 0.0, 1.0)

        if has_alpha:
            out = np.concatenate([res, a], axis=-1)
        else:
            out = res
        return self._from_float(out, meta)

    def _apply_hls(self, img1, img2, mode):
        # mode: 'hue' (H from img2, L,S from img1) or 'color' (H,S from img2, L from img1)
        f1, meta = self._to_float(img1)
        f2, _ = self._to_float(img2)
        self._check_shapes(f1, f2)

        has_alpha = f1.shape[-1] == 4
        if has_alpha:
            a = f1[..., 3:4]
            c1 = f1[..., :3]
            c2 = f2[..., :3]
        else:
            c1 = f1
            c2 = f2

        H, W = c1.shape[:2]
        flat1 = c1.reshape(-1, 3)
        flat2 = c2.reshape(-1, 3)

        # Convert to HLS
        hls1 = np.empty_like(flat1)
        hls2 = np.empty_like(flat2)
        for i in range(flat1.shape[0]):
            r1, g1, b1 = flat1[i]
            r2, g2, b2 = flat2[i]
            h1, l1, s1 = colorsys.rgb_to_hls(r1, g1, b1)
            h2, l2, s2 = colorsys.rgb_to_hls(r2, g2, b2)
            if mode == 'hue':
                h, l, s = h2, l1, s1
            else:  # color
                h, l, s = h2, l1, s2
            r, g, b = colorsys.hls_to_rgb(h, l, s)
            hls1[i] = (r, g, b)

        res = hls1.reshape(H, W, 3)
        res = np.clip(res, 0.0, 1.0)

        if has_alpha:
            out = np.concatenate([res, a], axis=-1)
        else:
            out = res
        return self._from_float(out, meta)

    def hue(self, img1, img2):
        return self._apply_hls(img1, img2, mode='hue')

    def color(self, img1, img2):
        return self._apply_hls(img1, img2, mode='color')"
272626,cackharot/suds-py3,cackharot_suds-py3/suds/reader.py,suds.reader.Reader,"from suds.plugin import PluginContainer
import hashlib

class Reader:
    """"""
    The reader provides integration with cache.
    @ivar options: An options object.
    @type options: I{Options}
    """"""

    def __init__(self, options):
        """"""
        @param options: An options object.
        @type options: I{Options}
        """"""
        self.options = options
        self.plugins = PluginContainer(options.plugins)

    def mangle(self, name, x):
        """"""
        Mangle the name by hashing the I{name} and appending I{x}.
        @return: the mangled name.
        """"""
        h = hashlib.sha256(name.encode('utf8')).hexdigest()
        return '%s-%s' % (h, x)","
class Reader:
    
    def __init__(self, options):
        
        pass
    def mangle(self, name, x):
        
        pass",snippet_183,3,"class Reader:
    def __init__(self, options):
        # Normalize options
        if options is None:
            options = {}
        elif isinstance(options, str):
            options = {""template"": options}
        elif not isinstance(options, dict):
            # Fallback to string template with default representation
            options = {""template"": ""{name}_{x}""}

        self.template = options.get(""template"")

        # Name transformations: lower, upper, title, snake, kebab, camel (basic)
        self.transform = options.get(""transform"")
        self.prefix = options.get(""prefix"", """")
        self.suffix = options.get(""suffix"", """")
        self.sep = options.get(""sep"", ""_"")

        # x formatting
        self.pad = options.get(""pad"")  # int or None
        self.base = options.get(""base"")  # 2, 8, 10, 16 or 'bin','oct','hex'
        self.base = {""bin"": 2, ""oct"": 8, ""hex"": 16}.get(self.base, self.base)
        if self.base not in (None, 2, 8, 10, 16):
            self.base = None

        # Hash options
        self.use_hash = bool(options.get(""hash"", False))
        self.hash_len = int(options.get(""hash_len"", 8))
        if self.hash_len < 0:
            self.hash_len = 0

        # Limit length
        self.max_length = options.get(""max_length"")
        try:
            if self.max_length is not None:
                self.max_length = int(self.max_length)
                if self.max_length <= 0:
                    self.max_length = None
        except Exception:
            self.max_length = None

    def mangle(self, name, x):
        name = """" if name is None else str(name)

        # Apply name transformation
        tname = self._transform_name(name)

        # Format x
        x_str = self._format_x(x)

        # If template is provided, use it
        if self.template:
            try:
                result = self.template.format(name=tname, x=x_str)
            except Exception:
                # Fallback to simple concatenation if template fails
                result = self._assemble(tname, x_str)
        else:
            result = self._assemble(tname, x_str)

        # Add hash if requested
        if self.use_hash:
            h = self._short_hash(f""{name}|{x}"")
            if result:
                result = f""{result}{self.sep}{h}"" if self.sep else f""{result}{h}""
            else:
                result = h

        # Enforce max length (prefer trimming within the name part)
        if self.max_length is not None and len(result) > self.max_length:
            result = self._truncate_preserving_suffix(result, self.max_length)

        return result

    def _transform_name(self, name):
        mode = (self.transform or """").lower()
        if not mode:
            return name
        if mode == ""lower"":
            return name.lower()
        if mode == ""upper"":
            return name.upper()
        if mode == ""title"":
            return name.title()
        if mode == ""snake"":
            return self._to_delimited(name, ""_"")
        if mode == ""kebab"":
            return self._to_delimited(name, ""-"")
        if mode == ""camel"":
            parts = self._split_words(name)
            if not parts:
                return """"
            head = parts[0].lower()
            tail = """".join(p.capitalize() for p in parts[1:])
            return head + tail
        return name

    def _format_x(self, x):
        # Try integer formatting if possible
        as_int = self._to_int_or_none(x)
        if as_int is not None:
            # base formatting
            if self.base == 2:
                out = format(as_int, ""b"")
            elif self.base == 8:
                out = format(as_int, ""o"")
            elif self.base == 16:
                out = format(as_int, ""x"")
            else:
                out = str(as_int)
            # padding
            if isinstance(self.pad, int) and self.pad > 0:
                out = out.zfill(self.pad)
            return out
        # Non-integer, just string
        return """" if x is None else str(x)

    def _assemble(self, tname, x_str):
        parts = []
        if self.prefix:
            parts.append(self.prefix)
        if tname:
            parts.append(tname)
        if x_str != """":
            parts.append(str(x_str))
        if self.suffix:
            parts.append(self.suffix)
        if not parts:
            return """"
        if self.sep is None:
            return """".join(parts)
        return self.sep.join(parts)

    def _short_hash(self, s):
        import hashlib
        digest = hashlib.sha1(s.encode(""utf-8"")).hexdigest()
        return digest[: self.hash_len] if self.hash_len else """"

    def _truncate_preserving_suffix(self, s, limit):
        if len(s) <= limit:
            return s
        # Try to preserve the last segment after sep
        if not self.sep:
            return s[:limit]
        sep = self.sep
        # If no sep present, simple truncate
        if sep not in s:
            return s[:limit]
        parts = s.split(sep)
        # Keep last part, trim the front aggregating with sep
        tail = parts[-1]
        # Ensure tail fits; if not, hard truncate tail
        if len(tail) >= limit:
            return tail[-limit:]
        # Space left for head + separators
        remaining = limit - len(tail) - len(sep)
        if remaining <= 0:
            return tail[-limit:]
        head = sep.join(parts[:-1])
        if len(head) > remaining:
            head = head[:remaining]
            # Avoid trailing partial separator by trimming to not end with sep
            if head.endswith(sep):
                head = head[: -len(sep)]
        return f""{head}{sep}{tail}""

    def _to_int_or_none(self, x):
        try:
            if isinstance(x, bool):
                return int(x)
            if isinstance(x, (int,)):
                return int(x)
            if isinstance(x, (float,)) and x.is_integer():
                return int(x)
            if isinstance(x, str):
                xs = x.strip().lower()
                # Auto-detect base prefixes
                if xs.startswith(""0x""):
                    return int(xs, 16)
                if xs.startswith(""0o""):
                    return int(xs, 8)
                if xs.startswith(""0b""):
                    return int(xs, 2)
                return int(xs, 10)
            return None
        except Exception:
            return None

    def _split_words(self, s):
        # Split on non-alnum and camelCase boundaries
        import re
        s = s.strip()
        if not s:
            return []
        # Replace separators with space
        s = re.sub(r""[^A-Za-z0-9]+"", "" "", s)
        # Split camelCase/PascalCase
        s = re.sub(r""([a-z0-9])([A-Z])"", r""\1 \2"", s)
        parts = [p for p in s.split() if p]
        return parts

    def _to_delimited(self, s, delim):
        parts = self._split_words(s)
        if delim == ""_"":
            return ""_"".join(p.lower() for p in parts)
        if delim == ""-"":
            return ""-"".join(p.lower() for p in parts)
        return delim.join(parts)"
272646,cackharot/suds-py3,cackharot_suds-py3/suds/sax/enc.py,suds.sax.enc.Encoder,"import re

class Encoder:
    """"""
    An XML special character encoder/decoder.
    @cvar encodings: A mapping of special characters encoding.
    @type encodings: [(str,str)]
    @cvar decodings: A mapping of special characters decoding.
    @type decodings: [(str,str)]
    @cvar special: A list of special characters
    @type special: [char]
    """"""
    encodings = (('&', '&amp;'), ('<', '&lt;'), ('>', '&gt;'), ('""', '&quot;'), (""'"", '&apos;'))
    decodings = (('&lt;', '<'), ('&gt;', '>'), ('&quot;', '""'), ('&apos;', ""'""), ('&amp;', '&'))
    special = ('&', '<', '>', '""', ""'"")

    def needsEncoding(self, s):
        """"""
        Get whether string I{s} contains special characters.
        @param s: A string to check.
        @type s: str
        @return: True if needs encoding.
        @rtype: boolean
        """"""
        if isinstance(s, str):
            for c in self.special:
                if c in s:
                    return True
        return False

    def encode(self, s):
        """"""
        Encode special characters found in string I{s}.
        @param s: A string to encode.
        @type s: str
        @return: The encoded string.
        @rtype: str
        """"""
        if isinstance(s, str) and self.needsEncoding(s):
            for x in self.encodings:
                s = re.sub(x[0], x[1], s)
        return s

    def decode(self, s):
        """"""
        Decode special characters encodings found in string I{s}.
        @param s: A string to decode.
        @type s: str
        @return: The decoded string.
        @rtype: str
        """"""
        if isinstance(s, str) and '&' in s:
            for x in self.decodings:
                s = s.replace(x[0], x[1])
        return s","
class Encoder:
    
    def needsEncoding(self, s):
        
        pass
    def encode(self, s):
        
        pass

    def decode(self, s):
        
        pass",snippet_184,4,"class Encoder:
    _UNRESERVED = set(
        b""ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-_.~""
    )

    def needsEncoding(self, s):
        if not isinstance(s, str):
            raise TypeError(""s must be a string"")
        for b in s.encode(""utf-8""):
            if b not in self._UNRESERVED:
                return True
        return False

    def encode(self, s):
        if not isinstance(s, str):
            raise TypeError(""s must be a string"")
        out = []
        for b in s.encode(""utf-8""):
            if b in self._UNRESERVED:
                out.append(chr(b))
            else:
                out.append(""%{:02X}"".format(b))
        return """".join(out)

    def decode(self, s):
        if not isinstance(s, str):
            raise TypeError(""s must be a string"")
        out_bytes = bytearray()
        i = 0
        n = len(s)
        while i < n:
            ch = s[i]
            if ch == ""%"":
                if i + 2 >= n:
                    raise ValueError(""Incomplete percent-encoding at end of string"")
                h1 = s[i + 1]
                h2 = s[i + 2]
                try:
                    val = int(h1 + h2, 16)
                except ValueError:
                    raise ValueError(f""Invalid percent-encoding '%{h1}{h2}' at position {i}"")
                out_bytes.append(val)
                i += 3
            else:
                # Allow raw non-ASCII in input; encode to UTF-8 bytes
                out_bytes.extend(ch.encode(""utf-8""))
                i += 1
        return out_bytes.decode(""utf-8"")"
272664,cackharot/suds-py3,cackharot_suds-py3/suds/transport/__init__.py,suds.transport.Transport,"class Transport:
    """"""
    The transport I{interface}.
    """"""

    def __init__(self):
        """"""
        Constructor.
        """"""
        from suds.transport.options import Options
        self.options = Options()
        del Options

    def open(self, request):
        """"""
        Open the url in the specified request.
        @param request: A transport request.
        @type request: L{Request}
        @return: An input stream.
        @rtype: stream
        @raise TransportError: On all transport errors.
        """"""
        raise Exception('not-implemented')

    def send(self, request):
        """"""
        Send soap message.  Implementations are expected to handle:
            - proxies
            - I{http} headers
            - cookies
            - sending message
            - brokering exceptions into L{TransportError}
        @param request: A transport request.
        @type request: L{Request}
        @return: The reply
        @rtype: L{Reply}
        @raise TransportError: On all transport errors.
        """"""
        raise Exception('not-implemented')","class Transport:
    
    def __init__(self):
        
        pass
    def open(self, request):
        
        pass
    def send(self, request):
        
        pass",snippet_185,4,"class Transport:
    
    def __init__(self):
        self._open = False
        self._conn_info = None
        self._sequence = 0
        self._history = []

    def open(self, request):
        if self._open:
            return False
        self._conn_info = request
        self._open = True
        return True

    def send(self, request):
        if not self._open:
            raise RuntimeError(""Transport is not open"")
        self._sequence += 1
        record = {
            ""sequence"": self._sequence,
            ""request"": request,
            ""connection"": self._conn_info,
        }
        self._history.append(record)
        return {
            ""status"": ""ok"",
            ""sequence"": self._sequence,
            ""request"": request,
            ""connection"": self._conn_info,
        }"
272699,cackharot/suds-py3,cackharot_suds-py3/suds/xsd/doctor.py,suds.xsd.doctor.TnsFilter,"class TnsFilter:
    """"""
    Target Namespace filter.
    @ivar tns: A list of target namespaces.
    @type tns: [str,...]
    """"""

    def __init__(self, *tns):
        """"""
        @param tns: A list of target namespaces.
        @type tns: [str,...]
        """"""
        self.tns = []
        self.add(*tns)

    def add(self, *tns):
        """"""
        Add I{targetNamesapces} to be added.
        @param tns: A list of target namespaces.
        @type tns: [str,...]
        """"""
        self.tns += tns

    def match(self, root, ns):
        """"""
        Match by I{targetNamespace} excluding those that
        are equal to the specified namespace to prevent
        adding an import to itself.
        @param root: A schema root.
        @type root: L{Element}
        """"""
        tns = root.get('targetNamespace')
        if len(self.tns):
            matched = tns in self.tns
        else:
            matched = 1
        itself = ns == tns
        return matched and (not itself)","class TnsFilter:
    
    def __init__(self, *tns):
        
        pass
    def add(self, *tns):
        
        pass
    def match(self, root, ns):
        
        pass",snippet_186,4,"class TnsFilter:
    
    def __init__(self, *tns):
        self._tokens = set()
        if tns:
            self.add(*tns)

    def _flatten_tokens(self, *tns):
        for t in tns:
            if t is None:
                continue
            if isinstance(t, (list, tuple, set)):
                for x in self._flatten_tokens(*t):
                    yield x
            else:
                s = str(t)
                for part in s.split():
                    if part:
                        yield part

    def add(self, *tns):
        for token in self._flatten_tokens(*tns):
            self._tokens.add(token)
        return self

    def match(self, root, ns):
        # Normalize namespace to empty string for no-namespace
        ns_norm = '' if (ns is None or ns == '') else ns

        # If no tokens were provided, default to match any
        if not self._tokens:
            return True

        # Obtain targetNamespace from root element if available
        root_tns = ''
        if root is not None:
            # Try dictionary-like access (e.g., xml.etree.ElementTree.Element)
            try:
                root_tns = root.get('targetNamespace') or ''
            except Exception:
                # Fallback: attribute access or mapping
                try:
                    root_tns = getattr(root, 'targetNamespace', '') or ''
                except Exception:
                    root_tns = ''

        for token in self._tokens:
            if token == '##any':
                return True
            if token == '##local':
                if ns_norm == '':
                    return True
            elif token == '##targetNamespace':
                if ns_norm == root_tns:
                    return True
            elif token == '##other':
                # Any namespace other than the target namespace
                if ns_norm != root_tns:
                    return True
            else:
                if ns_norm == token:
                    return True

        return False"
272711,cackharot/suds-py3,cackharot_suds-py3/suds/xsd/sxbase.py,suds.xsd.sxbase.NodeFinder,"class NodeFinder:
    """"""
    Find nodes based on flexable criteria.  The I{matcher} is
    may be any object that implements a match(n) method.
    @ivar matcher: An object used as criteria for match.
    @type matcher: I{any}.match(n)
    @ivar limit: Limit the number of matches.  0=unlimited.
    @type limit: int
    """"""

    def __init__(self, matcher, limit=0):
        """"""
        @param matcher: An object used as criteria for match.
        @type matcher: I{any}.match(n)
        @param limit: Limit the number of matches.  0=unlimited.
        @type limit: int
        """"""
        self.matcher = matcher
        self.limit = limit

    def find(self, node, list):
        """"""
        Traverse the tree looking for matches.
        @param node: A node to match on.
        @type node: L{SchemaObject}
        @param list: A list to fill.
        @type list: list
        """"""
        if self.matcher.match(node):
            list.append(node)
            self.limit -= 1
            if self.limit == 0:
                return
        for c in node.rawchildren:
            self.find(c, list)
        return self","class NodeFinder:
    
    def __init__(self, matcher, limit=0):
        
        pass
    def find(self, node, list):
        
        pass",snippet_187,3,"class NodeFinder:
    
    def __init__(self, matcher, limit=0):
        self.matcher = matcher
        self.limit = int(limit) if limit is not None else 0

    def _matches(self, node):
        if node is None:
            return False
        m = self.matcher
        try:
            if hasattr(m, ""match"") and callable(m.match):
                return bool(m.match(node))
            if callable(m):
                return bool(m(node))
        except Exception:
            return False
        return False

    def _iter_children(self, node):
        if node is None:
            return
        # Try Python ast nodes
        try:
            import ast
            if isinstance(node, ast.AST):
                for child in ast.iter_child_nodes(node):
                    if child is not None:
                        yield child
                return
        except Exception:
            pass

        # Objects with 'children' attribute
        if hasattr(node, ""children""):
            try:
                for child in getattr(node, ""children""):
                    if child is not None:
                        yield child
                return
            except Exception:
                pass

        # Generic iterables (avoid strings/bytes)
        if isinstance(node, (list, tuple)):
            for child in node:
                if child is not None:
                    yield child
            return

        try:
            if hasattr(node, ""__iter__"") and not isinstance(node, (str, bytes, bytearray)):
                for child in node:
                    if child is not None:
                        yield child
                return
        except Exception:
            pass

    def find(self, node, list):
        if node is None:
            return
        if self.limit and len(list) >= self.limit:
            return

        if self._matches(node):
            list.append(node)
            if self.limit and len(list) >= self.limit:
                return

        for child in self._iter_children(node) or ():
            if self.limit and len(list) >= self.limit:
                return
            self.find(child, list)"
279001,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.AES_CTR_Mechanism,"import PyKCS11.LowLevel

class AES_CTR_Mechanism:
    """"""CKM_AES_CTR encryption mechanism""""""

    def __init__(self, counterBits, counterBlock):
        """"""
        :param counterBits: the number of incremented bits in the counter block
        :param counterBlock: a 16-byte initial value of the counter block
        """"""
        self._param = PyKCS11.LowLevel.CK_AES_CTR_PARAMS()
        self._source_cb = ckbytelist(counterBlock)
        self._param.ulCounterBits = counterBits
        self._param.cb = self._source_cb
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_AES_CTR
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_AES_CTR_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class AES_CTR_Mechanism:
    
    def __init__(self, counterBits, counterBlock):
        
        pass
    def to_native(self):
        
        pass",snippet_190,3,"class AES_CTR_Mechanism:
    
    def __init__(self, counterBits, counterBlock):
        if not isinstance(counterBits, int):
            raise TypeError(""counterBits must be an integer"")
        if counterBits <= 0 or counterBits > 128 or counterBits % 8 != 0:
            raise ValueError(""counterBits must be a positive multiple of 8 and no greater than 128"")
        if not isinstance(counterBlock, (bytes, bytearray)):
            raise TypeError(""counterBlock must be bytes or bytearray"")
        if len(counterBlock) != 16:
            raise ValueError(""counterBlock must be exactly 16 bytes long"")
        self._counter_bits = counterBits
        self._counter_block = bytes(counterBlock)
        
    def to_native(self):
        return {
            ""counterBits"": self._counter_bits,
            ""counterBlock"": self._counter_block,
        }"
279002,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.AES_GCM_Mechanism,"import PyKCS11.LowLevel

class AES_GCM_Mechanism:
    """"""CKM_AES_GCM warpping mechanism""""""

    def __init__(self, iv, aad, tagBits):
        """"""
        :param iv: initialization vector
        :param aad: additional authentication data
        :param tagBits: length of authentication tag in bits
        """"""
        self._param = PyKCS11.LowLevel.CK_GCM_PARAMS()
        self._source_iv = ckbytelist(iv)
        self._param.pIv = self._source_iv
        self._param.ulIvLen = len(self._source_iv)
        self._source_aad = ckbytelist(aad)
        self._param.pAAD = self._source_aad
        self._param.ulAADLen = len(self._source_aad)
        self._param.ulTagBits = tagBits
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_AES_GCM
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_GCM_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class AES_GCM_Mechanism:
    
    def __init__(self, iv, aad, tagBits):
        
        pass
    def to_native(self):
        
        pass",snippet_191,3,"class AES_GCM_Mechanism:
    
    def __init__(self, iv, aad, tagBits):
        # Normalize and validate IV
        if iv is None:
            raise ValueError(""iv must not be None"")
        try:
            self.iv = bytes(iv)
        except Exception as e:
            raise TypeError(""iv must be bytes-like"") from e
        if len(self.iv) == 0:
            raise ValueError(""iv must not be empty"")

        # Normalize and validate AAD
        if aad is None:
            self.aad = b""""
        else:
            try:
                self.aad = bytes(aad)
            except Exception as e:
                raise TypeError(""aad must be bytes-like or None"") from e

        # Validate tagBits
        if not isinstance(tagBits, int):
            raise TypeError(""tagBits must be an integer"")
        if not (32 <= tagBits <= 128) or (tagBits % 8 != 0):
            raise ValueError(""tagBits must be between 32 and 128 (inclusive) and a multiple of 8"")
        self.tagBits = tagBits

    def to_native(self):
        return {
            ""iv"": self.iv,
            ""aad"": self.aad,
            ""tagBits"": self.tagBits,
        }"
279010,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.CONCATENATE_BASE_AND_KEY_Mechanism,"import PyKCS11.LowLevel

class CONCATENATE_BASE_AND_KEY_Mechanism:
    """"""CKM_CONCATENATE_BASE_AND_KEY key derivation mechanism""""""

    def __init__(self, encKey):
        """"""
        :param encKey: a handle of encryption key
        """"""
        self._encKey = encKey
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_CONCATENATE_BASE_AND_KEY
        self._mech.pParameter = self._encKey
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_OBJECT_HANDLE_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class CONCATENATE_BASE_AND_KEY_Mechanism:
    
    def __init__(self, encKey):
        
        pass
    def to_native(self):
        
        pass",snippet_192,3,"class CONCATENATE_BASE_AND_KEY_Mechanism:
    
    def __init__(self, encKey):
        if isinstance(encKey, bytearray):
            encKey = bytes(encKey)
        if not isinstance(encKey, (str, bytes)):
            raise TypeError(""encKey must be str or bytes"")
        self.encKey = encKey

    def to_native(self):
        encKey = self.encKey

        def concatenate(base):
            if isinstance(base, bytearray):
                base = bytes(base)
            if not isinstance(base, (str, bytes)):
                raise TypeError(""base must be str or bytes"")
            if isinstance(base, str) and isinstance(encKey, str):
                return base + encKey
            if isinstance(base, bytes) and isinstance(encKey, bytes):
                return base + encKey
            raise TypeError(""Type mismatch: base and encKey must both be str or both be bytes"")

        return concatenate"
279012,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.CkClass,"class CkClass:
    """"""
    Base class for CK_* classes
    """"""
    flags_dict = {}
    fields = {}
    flags = 0

    def flags2text(self):
        """"""
        parse the `self.flags` field and create a list of `CKF_*` strings
        corresponding to bits set in flags

        :return: a list of strings
        :rtype: list
        """"""
        r = []
        for k, v in self.flags_dict.items():
            if self.flags & k:
                r.append(v)
        return r

    def state2text(self):
        """"""
        Dummy method. Will be overwriden if necessary
        """"""
        return ''

    def to_dict(self):
        """"""
        convert the fields of the object into a dictionnary
        """"""
        dico = {}
        for field in self.fields:
            if field == 'flags':
                dico[field] = self.flags2text()
            elif field == 'state':
                dico[field] = self.state2text()
            else:
                dico[field] = self.__dict__[field]
        return dico

    def __str__(self):
        """"""
        text representation of the object
        """"""
        dico = self.to_dict()
        lines = []
        for key in sorted(dico.keys()):
            ck_type = self.fields[key]
            if ck_type == 'flags':
                flags = ', '.join(dico[key])
                lines.append(f'{key}: {flags}')
            elif ck_type == 'pair':
                p1, p2 = dico[key]
                lines.append(f'{key}: {p1}.{p2}')
            else:
                lines.append(f'{key}: {dico[key]}')
        return '\n'.join(lines)","class CkClass:
    
    def flags2text(self):
        
        pass
    def state2text(self):
        
        pass
    def to_dict(self):
        
        pass

    def __str__(self):
        
        pass",snippet_193,5,"import json
from typing import Any, Dict, Iterable, Union


class CkClass:
    FLAG_NAMES: Union[Dict[int, str], Iterable[str]] = {}
    STATE_NAMES: Dict[Any, str] = {}

    def __init__(self, flags: Any = None, state: Any = None, **kwargs):
        self.flags = flags
        self.state = state
        for k, v in kwargs.items():
            setattr(self, k, v)

    def flags2text(self) -> str:
        f = getattr(self, ""flags"", None)

        if f is None:
            return """"

        # If already textual or a collection of strings
        if isinstance(f, str):
            return f
        if isinstance(f, (set, list, tuple)) and all(isinstance(x, str) for x in f):
            return "", "".join(sorted(f))

        # If numeric bitmask, try to map using FLAG_NAMES
        if isinstance(f, int):
            names = []
            extras = 0

            # Dict mapping bit -> name
            if isinstance(self.FLAG_NAMES, dict) and self.FLAG_NAMES:
                for bit, name in self.FLAG_NAMES.items():
                    try:
                        bit_int = int(bit)
                    except Exception:
                        continue
                    if bit_int != 0 and (f & bit_int) == bit_int:
                        names.append(str(name))
                # compute extras not covered by known bits
                known_mask = 0
                for bit in self.FLAG_NAMES:
                    try:
                        known_mask |= int(bit)
                    except Exception:
                        continue
                extras = f & ~known_mask

            # Sequence mapping by bit position
            elif isinstance(self.FLAG_NAMES, (list, tuple)) and self.FLAG_NAMES:
                for idx, name in enumerate(self.FLAG_NAMES):
                    if name and (f & (1 << idx)):
                        names.append(str(name))
                # extras beyond provided names
                max_known_mask = (1 << len(self.FLAG_NAMES)) - 1
                extras = f & ~max_known_mask
            else:
                # No names available, just return numeric forms
                return f""0x{f:X} ({f})""

            parts = []
            if names:
                parts.append("", "".join(sorted(names)))
            if extras:
                parts.append(f""extra=0x{extras:X}"")
            if not parts:
                parts.append(""0"")
            return "" | "".join(parts)

        # Fallback textualization
        return str(f)

    def state2text(self) -> str:
        s = getattr(self, ""state"", None)
        if s is None:
            return """"
        if self.STATE_NAMES:
            return self.STATE_NAMES.get(s, str(s))
        return str(s)

    def to_dict(self) -> Dict[str, Any]:
        out: Dict[str, Any] = {}
        for k, v in self.__dict__.items():
            if k.startswith(""_""):
                continue
            # Skip callables
            if callable(v):
                continue
            # Serialize simply
            try:
                json.dumps(v)
                out[k] = v
            except Exception:
                out[k] = str(v)

        # Augment with human-readable forms
        out[""flags_text""] = self.flags2text()
        out[""state_text""] = self.state2text()
        return out

    def __str__(self) -> str:
        return json.dumps(self.to_dict(), sort_keys=True)"
279015,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.EXTRACT_KEY_FROM_KEY_Mechanism,"import PyKCS11.LowLevel

class EXTRACT_KEY_FROM_KEY_Mechanism:
    """"""CKM_EXTRACT_KEY_FROM_KEY key derivation mechanism""""""

    def __init__(self, extractParams):
        """"""
        :param extractParams: the index of the first bit of the original
        key to be used in the newly-derived key.  For example if
        extractParams=5 then the 5 first bits are skipped and not used.
        """"""
        self._param = PyKCS11.LowLevel.CK_EXTRACT_PARAMS()
        self._param.assign(extractParams)
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_EXTRACT_KEY_FROM_KEY
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_EXTRACT_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class EXTRACT_KEY_FROM_KEY_Mechanism:
    
    def __init__(self, extractParams):
        
        pass
    def to_native(self):
        
        pass",snippet_194,3,"class EXTRACT_KEY_FROM_KEY_Mechanism:
    
    def __init__(self, extractParams):
        self.extract_params = extractParams

    def to_native(self):
        p = self.extract_params
        if p is None:
            return b""""
        if isinstance(p, (bytes, bytearray, memoryview)):
            return bytes(p)
        if isinstance(p, int):
            import struct
            return struct.pack(""=I"", p)
        if isinstance(p, str):
            return p.encode(""utf-8"")
        try:
            return bytes(p)
        except Exception as e:
            raise TypeError(f""Unsupported type for extractParams: {type(p)!r}"") from e"
279016,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.KEY_DERIVATION_STRING_DATA_MechanismBase,"import PyKCS11.LowLevel

class KEY_DERIVATION_STRING_DATA_MechanismBase:
    """"""Base class for mechanisms using derivation string data""""""

    def __init__(self, data, mechType):
        """"""
        :param data: a byte array to concatenate the key with
        :param mechType: mechanism type
        """"""
        self._param = PyKCS11.LowLevel.CK_KEY_DERIVATION_STRING_DATA()
        self._data = ckbytelist(data)
        self._param.pData = self._data
        self._param.ulLen = len(self._data)
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = mechType
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_KEY_DERIVATION_STRING_DATA_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class KEY_DERIVATION_STRING_DATA_MechanismBase:
    
    def __init__(self, data, mechType):
        
        pass
    def to_native(self):
        
        pass",snippet_195,3,"class KEY_DERIVATION_STRING_DATA_MechanismBase:
    
    def __init__(self, data, mechType):
        if isinstance(data, str):
            data = data.encode('utf-8')
        elif isinstance(data, (bytearray, memoryview)):
            data = bytes(data)
        elif not isinstance(data, (bytes,)):
            raise TypeError(""data must be bytes, bytearray, memoryview, or str"")
        self._data = data
        self._mech_type = int(mechType)

    def to_native(self):
        return {
            'mechanism': self._mech_type,
            'parameter': self._data,
            'parameter_len': len(self._data),
        }"
279017,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.Mechanism,"import PyKCS11.LowLevel

class Mechanism:
    """"""Wraps CK_MECHANISM""""""

    def __init__(self, mechanism, param=None):
        """"""
        :param mechanism: the mechanism to be used
        :type mechanism: integer, any `CKM_*` value
        :param param: data to be used as crypto operation parameter
          (i.e. the IV for some algorithms)
        :type param: string or list/tuple of bytes

        :see: :func:`Session.decrypt`, :func:`Session.sign`
        """"""
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = mechanism
        self._param = None
        if param:
            self._param = ckbytelist(param)
            self._mech.pParameter = self._param
            self._mech.ulParameterLen = len(param)

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class Mechanism:
    
    def __init__(self, mechanism, param=None):
        
        pass
    def to_native(self):
        
        pass",snippet_196,3,"class Mechanism:
    
    def __init__(self, mechanism, param=None):
        import copy

        self.name = None
        self.params = {}

        if isinstance(mechanism, Mechanism):
            self.name = mechanism.name
            self.params = copy.deepcopy(mechanism.params)
            if param is not None:
                if not isinstance(param, dict):
                    raise TypeError(""param must be a dict when overriding an existing Mechanism."")
                self.params.update(copy.deepcopy(param))
            return

        if isinstance(mechanism, dict):
            mech_dict = mechanism
            name = mech_dict.get(""mechanism"") or mech_dict.get(""name"")
            if not isinstance(name, str) or not name.strip():
                raise ValueError(""Mechanism dict must contain a non-empty 'mechanism' or 'name' string."")

            params = {}
            # Prefer explicit param/params keys; otherwise take remaining keys as params
            if ""param"" in mech_dict and isinstance(mech_dict[""param""], dict):
                params.update(mech_dict[""param""])
            elif ""params"" in mech_dict and isinstance(mech_dict[""params""], dict):
                params.update(mech_dict[""params""])
            else:
                for k, v in mech_dict.items():
                    if k not in (""mechanism"", ""name"", ""param"", ""params""):
                        params[k] = v

            if param is not None:
                if not isinstance(param, dict):
                    raise TypeError(""param must be a dict."")
                params.update(param)

            self.name = name
            self.params = copy.deepcopy(params)
            return

        if isinstance(mechanism, str):
            name = mechanism
            if not name.strip():
                raise ValueError(""mechanism name must be a non-empty string."")
            if param is not None and not isinstance(param, dict):
                raise TypeError(""param must be a dict."")
            self.name = name
            self.params = copy.deepcopy(param or {})
            return

        raise TypeError(""mechanism must be a Mechanism, dict, or str."")

    def to_native(self):
        import copy
        return {
            ""mechanism"": self.name,
            ""param"": copy.deepcopy(self.params),
        }"
279020,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.RSAOAEPMechanism,"import PyKCS11.LowLevel

class RSAOAEPMechanism:
    """"""RSA OAEP Wrapping mechanism""""""

    def __init__(self, hashAlg, mgf, label=None):
        """"""
        :param hashAlg: the hash algorithm to use (like `CKM_SHA256`)
        :param mgf: the mask generation function to use (like
          `CKG_MGF1_SHA256`)
        :param label: the (optional) label to use
        """"""
        self._param = PyKCS11.LowLevel.CK_RSA_PKCS_OAEP_PARAMS()
        self._param.hashAlg = hashAlg
        self._param.mgf = mgf
        self._source = None
        self._param.source = CKZ_DATA_SPECIFIED
        if label:
            self._source = ckbytelist(label)
            self._param.ulSourceDataLen = len(self._source)
        else:
            self._param.ulSourceDataLen = 0
        self._param.pSourceData = self._source
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_RSA_PKCS_OAEP
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_RSA_PKCS_OAEP_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class RSAOAEPMechanism:
    
    def __init__(self, hashAlg, mgf, label=None):
        
        pass
    def to_native(self):
        
        pass",snippet_197,3,"class RSAOAEPMechanism:
    
    def __init__(self, hashAlg, mgf, label=None):
        if hashAlg is None:
            raise ValueError(""hashAlg must not be None"")
        if mgf is None:
            raise ValueError(""mgf must not be None"")

        self.hashAlg = hashAlg
        self.mgf = mgf

        if label is None:
            self.label = b""""
        elif isinstance(label, bytes):
            self.label = label
        elif isinstance(label, str):
            self.label = label.encode(""utf-8"")
        else:
            raise TypeError(""label must be bytes, str, or None"")

    def to_native(self):
        return {
            ""hashAlg"": self.hashAlg,
            ""mgf"": self.mgf,
            ""label"": self.label,
            ""labelLen"": len(self.label),
        }"
279021,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.RSA_PSS_Mechanism,"import PyKCS11.LowLevel

class RSA_PSS_Mechanism:
    """"""RSA PSS Wrapping mechanism""""""

    def __init__(self, mecha, hashAlg, mgf, sLen):
        """"""
        :param mecha: the mechanism to use (like
          `CKM_SHA384_RSA_PKCS_PSS`)
        :param hashAlg: the hash algorithm to use (like `CKM_SHA384`)
        :param mgf: the mask generation function to use (like
          `CKG_MGF1_SHA384`)
        :param sLen: length, in bytes, of the salt value used in the PSS
          encoding (like 0 or the message length)
        """"""
        self._param = PyKCS11.LowLevel.CK_RSA_PKCS_PSS_PARAMS()
        self._param.hashAlg = hashAlg
        self._param.mgf = mgf
        self._param.sLen = sLen
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = mecha
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_RSA_PKCS_PSS_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class RSA_PSS_Mechanism:
    
    def __init__(self, mecha, hashAlg, mgf, sLen):
        
        pass
    def to_native(self):
        
        pass",snippet_198,3,"class RSA_PSS_Mechanism:
    def __init__(self, mecha, hashAlg, mgf, sLen):
        if not isinstance(mecha, int):
            raise TypeError(""mecha must be int"")
        if not isinstance(hashAlg, int):
            raise TypeError(""hashAlg must be int"")
        if not isinstance(mgf, int):
            raise TypeError(""mgf must be int"")
        if not isinstance(sLen, int):
            raise TypeError(""sLen must be int"")
        if sLen < 0:
            raise ValueError(""sLen must be >= 0"")

        self.mecha = mecha
        self.hashAlg = hashAlg
        self.mgf = mgf
        self.sLen = sLen

        self._native_params = None  # keep reference alive if using ctypes

    def to_native(self):
        # Attempt PyKCS11 first
        try:
            import PyKCS11
            from PyKCS11.LowLevel import CK_MECHANISM, CK_RSA_PKCS_PSS_PARAMS

            params = CK_RSA_PKCS_PSS_PARAMS(self.hashAlg, self.mgf, self.sLen)
            mech = CK_MECHANISM(self.mecha, params)
            # Keep params alive with the object to avoid GC issues
            self._native_params = params
            return mech
        except Exception:
            # Fallback to ctypes representation
            from ctypes import Structure, c_ulong, c_void_p, sizeof, pointer, cast

            class CK_RSA_PKCS_PSS_PARAMS(Structure):
                _fields_ = [
                    (""hashAlg"", c_ulong),
                    (""mgf"", c_ulong),
                    (""sLen"", c_ulong),
                ]

            class CK_MECHANISM(Structure):
                _fields_ = [
                    (""mechanism"", c_ulong),
                    (""pParameter"", c_void_p),
                    (""ulParameterLen"", c_ulong),
                ]

            params = CK_RSA_PKCS_PSS_PARAMS(
                c_ulong(self.hashAlg),
                c_ulong(self.mgf),
                c_ulong(self.sLen),
            )
            # Keep params alive on the instance
            self._native_params = params

            mech = CK_MECHANISM()
            mech.mechanism = c_ulong(self.mecha)
            mech.pParameter = cast(pointer(self._native_params), c_void_p)
            mech.ulParameterLen = c_ulong(sizeof(self._native_params))
            return mech"
281845,datadotworld/data.world-py,datadotworld_data.world-py/datadotworld/datadotworld.py,datadotworld.datadotworld.UriParam,"class UriParam:
    """"""Represents a URI value as a parameter to a SPARQL query""""""

    def __init__(self, uri):
        """"""
        Initialize the UriParam value
        :param uri: the uri value to wrap
        """"""
        self._uri = uri

    def __repr__(self):
        """"""
        The official string representation for the URI
        :return: the string representation for the URI
        """"""
        return self._uri","class UriParam:
    
    def __init__(self, uri):
        
        pass
    def __repr__(self):
        
        pass",snippet_199,3,"class UriParam:
    
    def __init__(self, uri):
        if uri is None:
            raise ValueError(""uri cannot be None"")
        uri_str = str(uri).strip()
        if not uri_str:
            raise ValueError(""uri cannot be empty"")
        self.uri = uri_str

    def __repr__(self):
        return f""{self.__class__.__name__}({self.uri!r})"""
282529,quora/qcore,quora_qcore/qcore/events.py,qcore.events.EventInterceptor,"class EventInterceptor:
    """"""A context object helping to temporarily intercept
    a set of events on an object exposing a set of event hooks.

    """"""

    def __init__(self, source, **events):
        """"""
        Constructor.

        :param source: the object exposing a set of event hook properies
        :param events: a set of event_hook_name=event_handler pairs specifying
                       which events to intercept.
        """"""
        self.source = source
        self.events = events

    def __enter__(self):
        """"""Starts event interception.""""""
        source = self.source
        for name, handler in self.events.items():
            hook = getattr(source, name)
            hook.subscribe(handler)

    def __exit__(self, typ, value, traceback):
        """"""Stops event interception.""""""
        source = self.source
        for name, handler in self.events.items():
            hook = getattr(source, name)
            hook.unsubscribe(handler)","class EventInterceptor:
    
    def __init__(self, source, **events):
        
        pass
    def __enter__(self):
        
        pass
    def __exit__(self, typ, value, traceback):
        
        pass",snippet_200,4,"class EventInterceptor:
    
    def __init__(self, source, **events):
        self._source = source
        self._events = dict(events)
        self._originals = {}
        self._active = False

    def __enter__(self):
        if self._active:
            return self
        self._originals = {}
        for name, handler in self._events.items():
            existed = hasattr(self._source, name)
            original = getattr(self._source, name) if existed else None
            self._originals[name] = (existed, original)
            setattr(self._source, name, handler)
        self._active = True
        return self

    def __exit__(self, typ, value, traceback):
        if not self._active:
            return False
        try:
            for name, (existed, original) in self._originals.items():
                if existed:
                    setattr(self._source, name, original)
                else:
                    if hasattr(self._source, name):
                        delattr(self._source, name)
        finally:
            self._active = False
            self._originals = {}
        return False"
284811,empymod/empymod,empymod/filters.py,empymod.filters.DigitalFilter,"import numpy as np
import os

class DigitalFilter:
    """"""Simple Class for Digital Linear Filters.


    Parameters
    ----------
    name : str
        Name of the DFL.

    savename = str
        Name with which the filter is saved. If None (default) it is set to the
        same value as `name`.

    filter_coeff = list of str
        By default, the following filter coefficients are checked:

            ``filter_coeff = ['j0', 'j1', 'sin', 'cos']``

        This accounts for the standard Hankel and Fourier DLF in CSEM
        modelling. However, additional coefficient names can be provided via
        this parameter (in list format).

    """"""

    def __init__(self, name, savename=None, filter_coeff=None):
        """"""Add filter name.""""""
        self.name = name
        if savename is None:
            self.savename = name
        else:
            self.savename = savename
        self.filter_coeff = ['j0', 'j1', 'sin', 'cos']
        if filter_coeff is not None:
            self.filter_coeff.extend(filter_coeff)

    def tofile(self, path='filters'):
        """"""Save filter values to ASCII-files.

        Store the filter base and the filter coefficients in separate files
        in the directory `path`; `path` can be a relative or absolute path.

        Examples
        --------
        >>> import empymod
        >>> # Load a filter
        >>> filt = empymod.filters.Hankel().wer_201_2018
        >>> # Save it to pure ASCII-files
        >>> filt.tofile()
        >>> # This will save the following three files:
        >>> #    ./filters/wer_201_2018_base.txt
        >>> #    ./filters/wer_201_2018_j0.txt
        >>> #    ./filters/wer_201_2018_j1.txt

        """"""
        name = self.savename
        path = os.path.abspath(path)
        os.makedirs(path, exist_ok=True)
        basefile = os.path.join(path, name + '_base.txt')
        with open(basefile, 'w') as f:
            self.base.tofile(f, sep='\n')
        for val in self.filter_coeff:
            if hasattr(self, val):
                attrfile = os.path.join(path, name + '_' + val + '.txt')
                with open(attrfile, 'w') as f:
                    getattr(self, val).tofile(f, sep='\n')

    def fromfile(self, path='filters'):
        """"""Load filter values from ASCII-files.

        Load filter base and filter coefficients from ASCII files in the
        directory `path`; `path` can be a relative or absolute path.

        Examples
        --------
        >>> import empymod
        >>> # Create an empty filter;
        >>> # Name has to be the base of the text files
        >>> filt = empymod.filters.DigitalFilter('my-filter')
        >>> # Load the ASCII-files
        >>> filt.fromfile()
        >>> # This will load the following three files:
        >>> #    ./filters/my-filter_base.txt
        >>> #    ./filters/my-filter_j0.txt
        >>> #    ./filters/my-filter_j1.txt
        >>> # and store them in filt.base, filt.j0, and filt.j1.

        """"""
        name = self.savename
        path = os.path.abspath(path)
        basefile = os.path.join(path, name + '_base.txt')
        with open(basefile, 'r') as f:
            self.base = np.fromfile(f, sep='\n')
        for val in self.filter_coeff:
            attrfile = os.path.join(path, name + '_' + val + '.txt')
            if os.path.isfile(attrfile):
                with open(attrfile, 'r') as f:
                    setattr(self, val, np.fromfile(f, sep='\n'))
        self.factor = np.around([self.base[1] / self.base[0]], 15)","
class DigitalFilter:
    '''Simple Class for Digital Linear Filters.
    Parameters
    ----------
    name : str
        Name of the DFL.
    savename = str
        Name with which the filter is saved. If None (default) it is set to the
        same value as `name`.
    filter_coeff = list of str
        By default, the following filter coefficients are checked:
            ``filter_coeff = ['j0', 'j1', 'sin', 'cos']``
        This accounts for the standard Hankel and Fourier DLF in CSEM
        modelling. However, additional coefficient names can be provided via
        this parameter (in list format).
    '''
    def __init__(self, name, savename=None, filter_coeff=None):
        
        pass

    def tofile(self, path='filters'):
        '''Save filter values to ASCII-files.
        Store the filter base and the filter coefficients in separate files
        in the directory `path`; `path` can be a relative or absolute path.
        Examples
        --------
        >>> import empymod
        >>> # Load a filter
        >>> filt = empymod.filters.Hankel().wer_201_2018
        >>> # Save it to pure ASCII-files
        >>> filt.tofile()
        >>> # This will save the following three files:
        >>> #    ./filters/wer_201_2018_base.txt
        >>> #    ./filters/wer_201_2018_j0.txt
        >>> #    ./filters/wer_201_2018_j1.txt
        '''
        pass

    def fromfile(self, path='filters'):
        '''Load filter values from ASCII-files.
        Load filter base and filter coefficients from ASCII files in the
        directory `path`; `path` can be a relative or absolute path.
        Examples
        --------
        >>> import empymod
        >>> # Create an empty filter;
        >>> # Name has to be the base of the text files
        >>> filt = empymod.filters.DigitalFilter('my-filter')
        >>> # Load the ASCII-files
        >>> filt.fromfile()
        >>> # This will load the following three files:
        >>> #    ./filters/my-filter_base.txt
        >>> #    ./filters/my-filter_j0.txt
        >>> #    ./filters/my-filter_j1.txt
        >>> # and store them in filt.base, filt.j0, and filt.j1.
        '''
        pass",snippet_201,4,"import os
from typing import Iterable

try:
    import numpy as _np  # Optional
except Exception:  # pragma: no cover
    _np = None


class DigitalFilter:
    '''Simple Class for Digital Linear Filters.
    Parameters
    ----------
    name : str
        Name of the DFL.
    savename = str
        Name with which the filter is saved. If None (default) it is set to the
        same value as `name`.
    filter_coeff = list of str
        By default, the following filter coefficients are checked:
            ``filter_coeff = ['j0', 'j1', 'sin', 'cos']``
        This accounts for the standard Hankel and Fourier DLF in CSEM
        modelling. However, additional coefficient names can be provided via
        this parameter (in list format).
    '''
    def __init__(self, name, savename=None, filter_coeff=None):
        self.name = str(name)
        self.savename = str(savename) if savename is not None else self.name
        if filter_coeff is None:
            filter_coeff = ['j0', 'j1', 'sin', 'cos']
        if not isinstance(filter_coeff, (list, tuple)):
            raise TypeError(""filter_coeff must be a list or tuple of strings."")
        self.filter_coeff = list(map(str, filter_coeff))
        # Users are expected to set: self.base and any of the coefficients
        # as attributes named according to self.filter_coeff (e.g., self.j0)

    def _ensure_dir(self, path):
        os.makedirs(path, exist_ok=True)

    def _as_1d_sequence(self, arr):
        if _np is not None:
            try:
                a = _np.asarray(arr)
                if a.ndim == 0:
                    return [a.item()]
                return a.ravel().tolist()
            except Exception:
                pass
        # Fallback: try to iterate
        if isinstance(arr, (str, bytes)):
            raise TypeError(""Array data must be numeric, not string/bytes."")
        if isinstance(arr, Iterable):
            return list(arr)
        return [arr]

    def _write_array_txt(self, filepath, data):
        seq = self._as_1d_sequence(data)
        with open(filepath, 'w', encoding='utf-8') as f:
            for v in seq:
                f.write(f""{str(v)}\n"")

    def _read_array_txt(self, filepath):
        vals = []
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                s = line.strip()
                if not s:
                    continue
                try:
                    v = complex(s)
                except Exception:
                    # Try float as fallback
                    v = float(s)
                vals.append(v)
        if _np is not None:
            arr = _np.asarray(vals)
            if _np.all(_np.isreal(arr)):
                arr = arr.real.astype(float)
            return arr
        # Convert to pure float list if all imag parts are zero
        if all((getattr(v, 'imag', 0.0) == 0) for v in vals):
            return [float(getattr(v, 'real', v)) for v in vals]
        return vals  # complex list

    def tofile(self, path='filters'):
        '''Save filter values to ASCII-files.
        Store the filter base and the filter coefficients in separate files
        in the directory `path`; `path` can be a relative or absolute path.
        Examples
        --------
        >>> import empymod
        >>> # Load a filter
        >>> filt = empymod.filters.Hankel().wer_201_2018
        >>> # Save it to pure ASCII-files
        >>> filt.tofile()
        >>> # This will save the following three files:
        >>> #    ./filters/wer_201_2018_base.txt
        >>> #    ./filters/wer_201_2018_j0.txt
        >>> #    ./filters/wer_201_2018_j1.txt
        '''
        self._ensure_dir(path)

        if not hasattr(self, 'base'):
            raise AttributeError(""Attribute 'base' not found on filter. Set 'self.base' before saving."")
        basefile = os.path.join(path, f""{self.savename}_base.txt"")
        self._write_array_txt(basefile, getattr(self, 'base'))

        for coeff in self.filter_coeff:
            if hasattr(self, coeff):
                coefffile = os.path.join(path, f""{self.savename}_{coeff}.txt"")
                self._write_array_txt(coefffile, getattr(self, coeff))

    def fromfile(self, path='filters'):
        '''Load filter values from ASCII-files.
        Load filter base and filter coefficients from ASCII files in the
        directory `path`; `path` can be a relative or absolute path.
        Examples
        --------
        >>> import empymod
        >>> # Create an empty filter;
        >>> # Name has to be the base of the text files
        >>> filt = empymod.filters.DigitalFilter('my-filter')
        >>> # Load the ASCII-files
        >>> filt.fromfile()
        >>> # This will load the following three files:
        >>> #    ./filters/my-filter_base.txt
        >>> #    ./filters/my-filter_j0.txt
        >>> #    ./filters/my-filter_j1.txt
        >>> # and store them in filt.base, filt.j0, and filt.j1.
        '''
        basefile = os.path.join(path, f""{self.savename}_base.txt"")
        if not os.path.isfile(basefile):
            raise FileNotFoundError(f""Base file not found: {basefile}"")
        self.base = self._read_array_txt(basefile)

        for coeff in self.filter_coeff:
            coefffile = os.path.join(path, f""{self.savename}_{coeff}.txt"")
            if os.path.isfile(coefffile):
                setattr(self, coeff, self._read_array_txt(coefffile))"
285266,Samreay/ChainConsumer,src/chainconsumer/kde.py,chainconsumer.kde.MegKDE,"import numpy as np
from scipy import spatial

class MegKDE:
    """"""Matched Elliptical Gaussian Kernel Density Estimator

    Adapted from the algorithm specified in the BAMBIS's model specified Wolf 2017
    to support weighted samples.
    """"""

    def __init__(self, train: np.ndarray, weights: np.ndarray | None=None, truncation: float=3.0, nmin: int=4, factor: float=1.0):
        """"""
        Args:
            train (np.ndarray): The training data set. Should be a 1D array of samples or a 2D array
                of shape (n_samples, n_dim).
            weights (np.ndarray, optional): An array of weights. If not specified, equal weights are assumed.
            truncation (float, optional): The maximum deviation (in sigma) to use points in the KDE
            nmin (int, optional): The minimum number of points required to estimate the density
            factor (float, optional): Send bandwidth to this factor of the data estimate
        """"""
        self.truncation = truncation
        self.nmin = nmin
        self.train = train
        if len(train.shape) == 1:
            train = np.atleast_2d(train).T
        self.num_points, self.num_dim = train.shape
        if weights is None:
            weights = np.ones(self.num_points)
        self.weights = weights
        self.mean = np.average(train, weights=weights, axis=0)
        dx = train - self.mean
        cov = np.atleast_2d(np.cov(dx.T, aweights=weights))
        self.A = np.linalg.cholesky(np.linalg.inv(cov))
        self.d = np.dot(dx, self.A)
        self.tree = spatial.cKDTree(self.d)
        self.sigma = 2.0 * factor * np.power(self.num_points, -1.0 / (4 + self.num_dim))
        self.sigma_fact = -0.5 / (self.sigma * self.sigma)

    def evaluate(self, data: np.ndarray) -> np.ndarray:
        """"""Estimate un-normalised probability density at target points

        Args:
            data (np.ndarray): 2D array of shape (n_samples, n_dim).

        Returns:
            np.ndarray: A `(n_samples)` length array of estimates

        """"""
        if len(data.shape) == 1 and self.num_dim == 1:
            data = np.atleast_2d(data).T
        _d = np.dot(data - self.mean, self.A)
        neighbors = self.tree.query_ball_point(_d, self.sigma * self.truncation)
        out = []
        for i, n in enumerate(neighbors):
            if len(n) >= self.nmin:
                diff = self.d[n, :] - _d[i]
                distsq = np.sum(diff * diff, axis=1)
            else:
                dist, n = self.tree.query(_d[i], k=self.nmin)
                distsq = dist * dist
            out.append(np.sum(self.weights[n] * np.exp(self.sigma_fact * distsq)))
        return np.array(out)","
class MegKDE:
    
    def __init__(self, train: np.ndarray, weights: np.ndarray | None=None, truncation: float=3.0, nmin: int=4, factor: float=1.0):
        
        pass
    def evaluate(self, data: np.ndarray) -> np.ndarray:
        
        pass",snippet_203,3,"import numpy as np

class MegKDE:
    
    def __init__(self, train: np.ndarray, weights: np.ndarray | None=None, truncation: float=3.0, nmin: int=4, factor: float=1.0):
        self.train = np.asarray(train, dtype=float)
        if self.train.ndim == 1:
            self.train = self.train[:, None]
        self.n, self.d = self.train.shape

        if self.n < 1:
            raise ValueError(""train must contain at least one sample"")

        if weights is None:
            self.weights = np.full(self.n, 1.0 / self.n, dtype=float)
        else:
            w = np.asarray(weights, dtype=float)
            if w.shape[0] != self.n:
                raise ValueError(""weights must have the same length as train"")
            wsum = np.sum(w)
            if wsum <= 0 or not np.isfinite(wsum):
                raise ValueError(""weights must sum to a positive finite value"")
            self.weights = w / wsum

        self.truncation = float(truncation)
        if self.truncation <= 0:
            self.truncation = np.inf  # disable truncation if non-positive provided
        self.nmin = int(max(1, nmin))
        self.factor = float(factor)

        # Estimate bandwidth
        self.h = self._estimate_bandwidth()

    def _estimate_bandwidth(self) -> float:
        n, d = self.n, self.d

        # k-NN distance based bandwidth
        k = min(max(1, self.nmin), max(1, n - 1))  # ensure valid k
        if n == 1:
            knn_med = 0.0
        else:
            # Compute pairwise squared distances
            # Efficient squared distance computation
            x = self.train
            x2 = np.sum(x * x, axis=1)
            sqd = x2[:, None] + x2[None, :] - 2.0 * (x @ x.T)
            np.maximum(sqd, 0.0, out=sqd)
            # Exclude self-distances
            np.fill_diagonal(sqd, np.inf)
            # kth nearest neighbor distances for each point
            if k == 1:
                kth = np.min(sqd, axis=1)
            else:
                kth = np.partition(sqd, k - 1, axis=1)[:, k - 1]
            knn_dist = np.sqrt(kth)
            knn_med = np.median(knn_dist)

        # Base bandwidth from kNN and truncation
        if np.isfinite(self.truncation) and self.truncation > 0 and knn_med > 0:
            h_knn = knn_med / self.truncation
        else:
            h_knn = 0.0

        # Silverman's rule of thumb (isotropic)
        stds = np.std(self.train, axis=0, ddof=1) if self.n > 1 else np.zeros(self.d)
        s = float(np.mean(stds))
        if s <= 0 or not np.isfinite(s):
            # fallback scale from data range
            rng = float(np.mean(np.ptp(self.train, axis=0)))
            if rng <= 0 or not np.isfinite(rng):
                rng = 1.0
            s = rng / 4.0
        silverman = s * (n ** (-1.0 / (d + 4)))

        h = self.factor * max(h_knn, silverman, 1e-12)
        return h

    def evaluate(self, data: np.ndarray) -> np.ndarray:
        X = np.asarray(data, dtype=float)
        if X.ndim == 1:
            X = X[:, None]
        if X.shape[1] != self.d:
            raise ValueError(""data dimensionality must match training data"")

        h = self.h
        inv_h2 = 1.0 / (h * h)
        const = (1.0 / (np.sqrt(2.0 * np.pi) * h)) ** self.d

        # Compute squared distances between queries and train
        xt = self.train
        xq = X
        xt2 = np.sum(xt * xt, axis=1)
        xq2 = np.sum(xq * xq, axis=1)
        sqd = xq2[:, None] + xt2[None, :] - 2.0 * (xq @ xt.T)
        np.maximum(sqd, 0.0, out=sqd)

        if np.isfinite(self.truncation):
            r2 = (self.truncation * h) ** 2
            mask = sqd <= r2
        else:
            mask = None

        # Gaussian kernel evaluation with optional truncation mask
        Z = -0.5 * sqd * inv_h2
        if mask is not None:
            Z = np.where(mask, Z, -np.inf)

        K = np.exp(Z) * const
        dens = K @ self.weights
        return dens"
285844,4Catalyzer/flask-resty,4Catalyzer_flask-resty/flask_resty/authentication.py,flask_resty.authentication.AuthenticationBase,"class AuthenticationBase:
    """"""Base class for API authentication components.

    Authentication components are responsible for extracting the request
    credentials, if any. They should raise a 401 if the credentials are
    invalid, but should provide `None` for unauthenticated users.

    Flask-RESTy provides an implementation using `JSON Web Tokens`_  but you
    can use any authentication component by extending
    :py:class:`AuthenticationBase` and implementing
    :py:meth:`get_request_credentials`.

    .. _JSON Web Tokens: https://jwt.io/
    """"""

    def authenticate_request(self):
        """"""Store the request credentials in the
        :py:class:`flask.ctx.AppContext`.

        .. warning::

            No validation is performed by Flask-RESTy. It is up to the
            implementor to validate the request in
            :py:meth:`get_request_credentials`.
        """"""
        set_request_credentials(self.get_request_credentials())

    def get_request_credentials(self):
        """"""Get the credentials for the current request.

        Typically this is done by inspecting :py:data:`flask.request`.

        .. warning::

            Implementing classes **must** raise an exception on authentication
            failure. A 401 Unauthorized :py:class:`ApiError` is recommended.

        :return: The credentials for the current request.
        """"""
        raise NotImplementedError()","class AuthenticationBase:
    
    def authenticate_request(self):
        
        pass
    def get_request_credentials(self):
        
        pass",snippet_204,3,"from abc import ABC, abstractmethod

class AuthenticationBase(ABC):
    @abstractmethod
    def authenticate_request(self):
        raise NotImplementedError(""Subclasses must implement authenticate_request()"")

    @abstractmethod
    def get_request_credentials(self):
        raise NotImplementedError(""Subclasses must implement get_request_credentials()"")"
285856,4Catalyzer/flask-resty,4Catalyzer_flask-resty/flask_resty/filtering.py,flask_resty.filtering.ArgFilterBase,"class ArgFilterBase:
    """"""An abstract specification of a filter from a query argument.

    Implementing classes must provide :py:meth:`maybe_set_arg_name` and
    :py:meth:`filter_query`.
    """"""

    def maybe_set_arg_name(self, arg_name):
        """"""Set the name of the argument to which this filter is bound.

        :param str arg_name: The name of the field to filter against.
        :raises: :py:class:`NotImplementedError` if no implementation is
            provided.
        """"""
        raise NotImplementedError()

    def filter_query(self, query, view, arg_value):
        """"""Filter the query.

        :param query: The query to filter.
        :type query: :py:class:`sqlalchemy.orm.query.Query`
        :param view: The view with the model we wish to filter for.
        :type view: :py:class:`ModelView`
        :param str arg_value: The filter specification
        :return: The filtered query
        :rtype: :py:class:`sqlalchemy.orm.query.Query`
        :raises: :py:class:`NotImplementedError` if no implementation is
            provided.
        """"""
        raise NotImplementedError()","class ArgFilterBase:
    
    def maybe_set_arg_name(self, arg_name):
        
        pass
    def filter_query(self, query, view, arg_value):
        
        pass",snippet_205,3,"class ArgFilterBase:
    def maybe_set_arg_name(self, arg_name):
        current = getattr(self, ""arg_name"", None)
        if current is None:
            if arg_name is None:
                raise ValueError(""arg_name must be provided or previously set."")
            self.arg_name = arg_name
            return self.arg_name

        if arg_name is not None and arg_name != current:
            raise ValueError(f""Conflicting arg_name: existing='{current}', provided='{arg_name}'"")
        return current

    def filter_query(self, query, view, arg_value):
        raise NotImplementedError(""Subclasses must implement filter_query(query, view, arg_value)."")"
285958,tcalmant/python-javaobj,tcalmant_python-javaobj/javaobj/constants.py,javaobj.constants.StreamCodeDebug,"class StreamCodeDebug:
    """"""
    Codes utility methods
    """"""

    @staticmethod
    def op_id(op_id):
        """"""
        Returns the name of the given OP Code
        :param op_id: OP Code
        :return: Name of the OP Code
        """"""
        try:
            return TerminalCode(op_id).name
        except ValueError:
            return '<unknown TC:{0}>'.format(op_id)

    @staticmethod
    def type_code(type_id):
        """"""
        Returns the name of the given Type Code
        :param type_id: Type code
        :return: Name of the type code
        """"""
        try:
            return TypeCode(type_id).name
        except ValueError:
            return '<unknown TypeCode:{0}>'.format(type_id)

    @staticmethod
    def flags(flags):
        """"""
        Returns the names of the class description flags found in the given
        integer

        :param flags: A class description flag entry
        :return: The flags names as a single string
        """"""
        names = sorted((key.name for key in ClassDescFlags if key & flags))
        return ', '.join(names)","class StreamCodeDebug:
    
    @staticmethod
    def op_id(op_id):
        
        pass
    @staticmethod
    def type_code(type_id):
        
        pass
    @staticmethod
    def flags(flags):
        
        pass",snippet_206,4,"class StreamCodeDebug:
    
    @staticmethod
    def op_id(op_id):
        from enum import Enum, IntEnum, Flag, IntFlag
        if op_id is None:
            return ""UNKNOWN""
        # Enum-like
        if isinstance(op_id, (Enum, IntEnum, Flag, IntFlag)):
            try:
                name = op_id.name
            except Exception:
                name = None
            try:
                value = op_id.value
            except Exception:
                value = None
            if name is not None and value is not None:
                return f""{name}({value})""
            return str(op_id)
        # Integer opcode
        if isinstance(op_id, int):
            return f""{op_id} (0x{op_id:02X})""
        # Callable/class types
        if isinstance(op_id, type):
            return op_id.__name__
        return str(op_id)

    @staticmethod
    def type_code(type_id):
        from enum import Enum, IntEnum, Flag, IntFlag
        if type_id is None:
            return ""unknown""
        # If it's a Python type
        if isinstance(type_id, type):
            return type_id.__name__
        # Enum-like
        if isinstance(type_id, (Enum, IntEnum, Flag, IntFlag)):
            try:
                name = type_id.name
            except Exception:
                name = None
            try:
                value = type_id.value
            except Exception:
                value = None
            if name is not None and value is not None:
                return f""{name}({value})""
            return str(type_id)
        # Integer type code
        if isinstance(type_id, int):
            return f""{type_id} (0x{type_id:02X})""
        return str(type_id)

    @staticmethod
    def flags(flags):
        from enum import Flag, IntFlag
        # None
        if flags is None:
            return ""0""
        # Enum flags
        if isinstance(flags, (Flag, IntFlag)):
            return str(flags)
        # Integer bitfield
        if isinstance(flags, int):
            if flags == 0:
                return ""0x00""
            bit_positions = [str(i) for i in range(flags.bit_length()) if (flags >> i) & 1]
            bits_desc = "","".join(bit_positions)
            return f""0x{flags:02X} (bits:{bits_desc})""
        # Mapping of name -> mask
        if isinstance(flags, dict):
            set_names = []
            for name, mask in flags.items():
                try:
                    if mask and isinstance(mask, int) and (mask & mask - 1) == 0:
                        # single-bit mask
                        set_names.append(name)
                    elif isinstance(mask, int) and mask != 0:
                        set_names.append(name)
                except Exception:
                    continue
            return ""|"".join(sorted(set_names)) if set_names else ""0""
        # Iterable of flag names
        try:
            iter(flags)  # check iterability
            if isinstance(flags, (str, bytes)):
                return str(flags)
            names = [str(x) for x in flags]
            return ""|"".join(names) if names else ""0""
        except Exception:
            pass
        return str(flags)"
285972,tcalmant/python-javaobj,tcalmant_python-javaobj/javaobj/v2/api.py,javaobj.v2.api.IJavaStreamParser,"class IJavaStreamParser:
    """"""
    API of the Java stream parser
    """"""

    def run(self):
        """"""
        Parses the input stream
        """"""
        raise NotImplementedError

    def dump(self, content):
        """"""
        Dumps to a string the given objects
        """"""
        raise NotImplementedError

    def _read_content(self, type_code, block_data, class_desc=None):
        """"""
        Parses the next content. Use with care (use only in a transformer)
        """"""","class IJavaStreamParser:
    
    def run(self):
        
        pass
    def dump(self, content):
        
        pass
    def _read_content(self, type_code, block_data, class_desc=None):
        
        pass",snippet_207,4,"class IJavaStreamParser:
    STREAM_MAGIC = 0xACED
    STREAM_VERSION = 0x0005

    # Type codes (subset)
    TC_NULL = 0x70
    TC_REFERENCE = 0x71
    TC_CLASSDESC = 0x72
    TC_OBJECT = 0x73
    TC_STRING = 0x74
    TC_ARRAY = 0x75
    TC_CLASS = 0x76
    TC_BLOCKDATA = 0x77
    TC_ENDBLOCKDATA = 0x78
    TC_RESET = 0x79
    TC_BLOCKDATALONG = 0x7A
    TC_EXCEPTION = 0x7B
    TC_LONGSTRING = 0x7C
    TC_PROXYCLASSDESC = 0x7D
    TC_ENUM = 0x7E

    BASE_HANDLE = 0x7E0000

    def __init__(self):
        import io
        self._io = None
        self._buf = None
        self._pos = 0
        self._len = 0
        self._handles = {}
        self._next_handle = self.BASE_HANDLE
        self._io_cls = io.BytesIO

    def dump(self, content):
        import io, os
        if isinstance(content, (bytes, bytearray, memoryview)):
            self._buf = bytes(content)
        elif hasattr(content, ""read""):
            self._buf = content.read()
        elif isinstance(content, (str, os.PathLike)):
            with open(content, ""rb"") as f:
                self._buf = f.read()
        else:
            raise TypeError(""Unsupported content type for dump"")
        self._io = self._io_cls(self._buf)
        self._pos = 0
        self._io.seek(0, 2)
        self._len = self._io.tell()
        self._io.seek(0)
        self._handles.clear()
        self._next_handle = self.BASE_HANDLE

    def run(self):
        if self._io is None:
            raise RuntimeError(""No content loaded. Call dump() first."")
        # Read stream header if present
        if self._len >= 4:
            try:
                magic = self._read_u2()
                version = self._read_u2()
            except EOFError:
                return None
            if magic != self.STREAM_MAGIC or version != self.STREAM_VERSION:
                # Not a Java serialization stream; rewind and parse elements heuristically
                self._io.seek(0)
                self._pos = 0
        items = []
        while True:
            try:
                item = self._read_content_object()
            except EOFError:
                break
            if item is not _EOF:
                items.append(item)
        if not items:
            return None
        if len(items) == 1:
            return items[0]
        return items

    def _read_content(self, type_code, block_data, class_desc=None):
        # type_code: Java field type code (B C D F I J S Z L [)
        t = type_code
        if isinstance(t, bytes):
            t = t.decode(""ascii"", ""ignore"")
        if not t:
            raise ValueError(""Empty type code"")
        t0 = t[0]
        if t0 == ""B"":  # byte
            return self._read_s1()
        if t0 == ""C"":  # char (unsigned 2 bytes)
            return self._read_u2()
        if t0 == ""D"":  # double
            import struct
            return struct.unpack("">d"", self._read_exact(8))[0]
        if t0 == ""F"":  # float
            import struct
            return struct.unpack("">f"", self._read_exact(4))[0]
        if t0 == ""I"":  # int
            return self._read_s4()
        if t0 == ""J"":  # long
            return self._read_s8()
        if t0 == ""S"":  # short
            return self._read_s2()
        if t0 == ""Z"":  # boolean
            return self._read_u1() != 0
        if t0 in (""L"", ""[""):  # object or array
            return self._read_content_object()
        raise NotImplementedError(f""Unsupported type code: {t0}"")

    # Internal object/content reader (subset of Java Serialization)
    def _read_content_object(self):
        b = self._read_u1()
        # Dispatch on TC
        if b == self.TC_NULL:
            return None
        if b == self.TC_REFERENCE:
            handle = self._read_u4()
            if handle not in self._handles:
                raise ValueError(f""Invalid reference handle: {hex(handle)}"")
            return self._handles[handle]
        if b == self.TC_STRING:
            s = self._read_mutf8_short()
            self._assign_handle(s)
            return s
        if b == self.TC_LONGSTRING:
            s = self._read_mutf8_long()
            self._assign_handle(s)
            return s
        if b == self.TC_BLOCKDATA:
            ln = self._read_u1()
            data = self._read_exact(ln)
            return data
        if b == self.TC_BLOCKDATALONG:
            ln = self._read_u4()
            data = self._read_exact(ln)
            return data
        if b == self.TC_ENDBLOCKDATA:
            return _EOF  # treat end-blockdata as internal marker
        if b in (self.TC_OBJECT, self.TC_CLASSDESC, self.TC_ARRAY, self.TC_CLASS,
                 self.TC_PROXYCLASSDESC, self.TC_ENUM, self.TC_EXCEPTION, self.TC_RESET):
            # Basic skipping strategy for unsupported types: try to skip a block or raise
            # For safety, we raise to avoid infinite loops on unknown structures.
            raise NotImplementedError(f""Unsupported type code 0x{b:02x} in this minimal parser"")
        # If it's not a known TC marker, treat it as raw byte data preceded by length?
        # Rewind one byte and try to read as plain bytes until EOF (fallback).
        self._seek_rel(-1)
        rest = self._read_exact(self._remaining())
        return rest

    # Handle table management
    def _assign_handle(self, obj):
        self._handles[self._next_handle] = obj
        self._next_handle += 1

    # Reading helpers
    def _read_exact(self, n):
        if n < 0:
            raise ValueError(""Negative read"")
        data = self._io.read(n)
        if data is None or len(data) != n:
            raise EOFError(""Unexpected EOF"")
        self._pos += n
        return data

    def _read_u1(self):
        b = self._read_exact(1)
        return b[0]

    def _read_s1(self):
        v = self._read_u1()
        return v - 256 if v > 127 else v

    def _read_u2(self):
        import struct
        return struct.unpack("">H"", self._read_exact(2))[0]

    def _read_s2(self):
        import struct
        return struct.unpack("">h"", self._read_exact(2))[0]

    def _read_u4(self):
        import struct
        return struct.unpack("">I"", self._read_exact(4))[0]

    def _read_s4(self):
        import struct
        return struct.unpack("">i"", self._read_exact(4))[0]

    def _read_s8(self):
        import struct
        return struct.unpack("">q"", self._read_exact(8))[0]

    def _read_mutf8_short(self):
        ln = self._read_u2()
        data = self._read_exact(ln)
        # Best-effort decode: Java uses modified UTF-8; for ASCII and standard UTF-8 this works.
        try:
            return data.decode(""utf-8"")
        except UnicodeDecodeError:
            return data.decode(""utf-8"", errors=""replace"")

    def _read_mutf8_long(self):
        ln = self._read_s8()
        if ln < 0:
            raise ValueError(""Negative long string length"")
        data = self._read_exact(ln)
        try:
            return data.decode(""utf-8"")
        except UnicodeDecodeError:
            return data.decode(""utf-8"", errors=""replace"")

    def _seek_rel(self, off):
        self._io.seek(off, 1)
        self._pos += off

    def _remaining(self):
        return self._len - self._pos


class _EOFType:
    pass


_EOF = _EOFType()"
287233,materialsvirtuallab/monty,materialsvirtuallab_monty/src/monty/design_patterns.py,monty.design_patterns.NullFile,"import os

class NullFile:
    """"""A file object that is associated to /dev/null.""""""

    def __new__(cls):
        """"""
        Pass through.
        """"""
        return open(os.devnull, 'w')

    def __init__(self):
        """"""no-op""""""","
class NullFile:
    
    def __new__(cls):
        
        pass
    def __init__(self):
        
        pass",snippet_208,3,"class NullFile:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            obj = super().__new__(cls)
            obj._closed = False
            cls._instance = obj
        return cls._instance

    def __init__(self):
        pass

    def write(self, s):
        if self._closed:
            raise ValueError(""I/O operation on closed file."")
        return 0 if s is None else len(s)

    def writelines(self, lines):
        if self._closed:
            raise ValueError(""I/O operation on closed file."")
        for _ in lines:
            pass

    def read(self, size=-1):
        if self._closed:
            raise ValueError(""I/O operation on closed file."")
        return """"

    def readline(self, size=-1):
        if self._closed:
            raise ValueError(""I/O operation on closed file."")
        return """"

    def readlines(self, hint=-1):
        if self._closed:
            raise ValueError(""I/O operation on closed file."")
        return []

    def flush(self):
        if self._closed:
            raise ValueError(""I/O operation on closed file."")

    def close(self):
        self._closed = True

    @property
    def closed(self):
        return self._closed

    def readable(self):
        return False

    def writable(self):
        return True

    def seekable(self):
        return False

    def fileno(self):
        raise OSError(""NullFile has no file descriptor."")

    def __enter__(self):
        if self._closed:
            raise ValueError(""I/O operation on closed file."")
        return self

    def __exit__(self, exc_type, exc, tb):
        self.close()
        return False"
287394,common-workflow-language/schema_salad,common-workflow-language_schema_salad/schema_salad/cpp_codegen.py,schema_salad.cpp_codegen.ClassDefinition,"from typing import IO, Any, Optional, Union, cast

class ClassDefinition:
    """"""Prototype of a class.""""""

    def __init__(self, name: str):
        """"""Initialize the class definition with a name.""""""
        self.fullName = name
        self.extends: list[dict[str, str]] = []
        self.specializationTypes: list[str] = []
        self.allfields: list[FieldDefinition] = []
        self.fields: list[FieldDefinition] = []
        self.abstract = False
        self.namespace, self.classname = split_name(name)
        self.namespace = safenamespacename(self.namespace)
        self.classname = safename(self.classname)

    def writeFwdDeclaration(self, target: IO[str], fullInd: str, ind: str) -> None:
        """"""Write forward declaration.""""""
        target.write(f'{fullInd}namespace {self.namespace} {{ struct {self.classname}; }}\n')

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, common_namespace: str) -> None:
        """"""Write definition of the class.""""""
        target.write(f'{fullInd}namespace {self.namespace} {{\n')
        target.write(f'{fullInd}struct {self.classname}')
        extends = list(map(safename2, self.extends))
        override = ''
        virtual = 'virtual '
        if len(self.extends) > 0:
            target.write(f'\n{fullInd}{ind}: ')
            target.write(f'\n{fullInd}{ind}, '.join(extends))
            override = ' override'
            virtual = ''
        target.write(' {\n')
        for field in self.fields:
            field.writeDefinition(target, fullInd + ind, ind, self.namespace)
        if self.abstract:
            target.write(f'{fullInd}{ind}virtual ~{self.classname}() = 0;\n')
        else:
            target.write(f'{fullInd}{ind}{virtual}~{self.classname}(){override} = default;\n')
        target.write(f'{fullInd}{ind}{virtual}auto toYaml([[maybe_unused]] {common_namespace}::store_config const& config) const -> YAML::Node{override};\n')
        target.write(f'{fullInd}{ind}{virtual}void fromYaml(YAML::Node const& n){override};\n')
        target.write(f'{fullInd}}};\n')
        target.write(f'{fullInd}}}\n\n')

    def writeImplDefinition(self, target: IO[str], fullInd: str, ind: str, common_namespace: str) -> None:
        """"""Write definition with implementation.""""""
        extends = list(map(safename2, self.extends))
        if self.abstract:
            target.write(f'{fullInd}inline {self.namespace}::{self.classname}::~{self.classname}() = default;\n')
        target.write(f'{fullInd}inline auto {self.namespace}::{self.classname}::toYaml([[maybe_unused]] ::{common_namespace}::store_config const& config) const -> YAML::Node {{\n{fullInd}{ind}using ::{common_namespace}::toYaml;\n{fullInd}{ind}auto n = YAML::Node{{}};\n{fullInd}{ind}if (config.generateTags) {{\n{fullInd}{ind}{ind}n.SetTag(""{self.classname}"");\n{fullInd}{ind}}}\n')
        for e in extends:
            target.write(f'{fullInd}{ind}n = mergeYaml(n, {e}::toYaml(config));\n')
        for field in self.fields:
            fieldname = safename(field.name)
            target.write(f'{fullInd}{ind}{{\n')
            target.write(f'{fullInd}{ind}{ind} auto member = toYaml(*{fieldname}, config);\n')
            if field.typeDSL:
                target.write(f'{fullInd}{ind}{ind} member = simplifyType(member, config);\n')
            target.write(f'{fullInd}{ind}{ind} member = convertListToMap(member, {q(field.mapSubject)}, {q(field.mapPredicate)}, config);\n')
            target.write(f'{fullInd}{ind}{ind}addYamlField(n, {q(field.name)}, member);\n')
            target.write(f'{fullInd}{ind}}}\n')
        target.write(f'{fullInd}{ind}return n;\n{fullInd}}}\n')
        functionname = f'{self.namespace}::{self.classname}::fromYaml'
        target.write(f'{fullInd}inline void {functionname}([[maybe_unused]] YAML::Node const& n) {{\n{fullInd}{ind}using ::{common_namespace}::fromYaml;\n')
        for e in extends:
            target.write(f'{fullInd}{ind}{e}::fromYaml(n);\n')
        for field in self.fields:
            fieldname = safename(field.name)
            expandType = ''
            if field.typeDSL:
                expandType = 'expandType'
            target.write(f'{fullInd}{ind}{{\n{fullInd}{ind}{ind}auto nodeAsList = convertMapToList(n[{q(field.name)}], {q(field.mapSubject)}, {q(field.mapPredicate)});\n{fullInd}{ind}{ind}auto expandedNode = {expandType}(nodeAsList);\n{fullInd}{ind}{ind}fromYaml(expandedNode, *{fieldname});\n{fullInd}{ind}}}\n')
        target.write(f'{fullInd}}}\n')
        if not self.abstract:
            e = f'::{self.namespace}::{self.classname}'
            target.write(f'namespace {common_namespace} {{\ntemplate <>\nstruct DetectAndExtractFromYaml<{e}> {{\n    auto operator()(YAML::Node const& n) const -> std::optional<{e}> {{\n        if (!n.IsDefined()) return std::nullopt;\n        if (!n.IsMap()) return std::nullopt;\n        auto res = {e}{{}};\n\n')
            for field in self.fields:
                fieldname = safename(field.name)
                target.write(f'        if constexpr (::{common_namespace}::IsConstant<decltype(res.{fieldname})::value_t>::value) try {{\n            fromYaml(n[{q(field.name)}], *res.{fieldname});\n            fromYaml(n, res);\n            return res;\n        }} catch(...) {{}}\n\n')
            target.write('        return std::nullopt;\n    }\n};\n}\n')","
class ClassDefinition:
    
    def __init__(self, name: str):
        
        pass
    def writeFwdDeclaration(self, target: IO[str], fullInd: str, ind: str) -> None:
        
        pass

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, common_namespace: str) -> None:
        
        pass

    def writeImplDefinition(self, target: IO[str], fullInd: str, ind: str, common_namespace: str) -> None:
        
        pass",snippet_209,5,"from typing import IO, Any


class ClassDefinition:
    def __init__(self, name: str):
        if not isinstance(name, str) or not name.strip():
            raise ValueError(""Class name must be a non-empty string."")
        self.name = name.strip()

    def writeFwdDeclaration(self, target: IO[str], fullInd: str, ind: str) -> None:
        target.write(f""{fullInd}class {self.name};\n"")

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, common_namespace: str) -> None:
        ns = (common_namespace or """").strip()

        if ns:
            target.write(f""{fullInd}namespace {ns} {{\n"")

        target.write(f""{fullInd if not ns else fullInd + ind}class {self.name} {{\n"")
        target.write(f""{fullInd if not ns else fullInd + ind}{ind}public:\n"")
        target.write(f""{fullInd if not ns else fullInd + ind}{ind}{ind}{self.name}();\n"")
        target.write(f""{fullInd if not ns else fullInd + ind}{ind}{ind}~{self.name}();\n"")
        target.write(f""{fullInd if not ns else fullInd + ind}}};\n"")

        if ns:
            target.write(f""{fullInd}}} // namespace {ns}\n"")

    def writeImplDefinition(self, target: IO[str], fullInd: str, ind: str, common_namespace: str) -> None:
        ns = (common_namespace or """").strip()
        scope = f""{ns}::"" if ns else """"

        if ns:
            target.write(f""{fullInd}namespace {ns} {{\n"")

        target.write(f""{fullInd if not ns else fullInd + ind}{scope}{self.name}::{self.name}() {{}}\n"")
        target.write(f""{fullInd if not ns else fullInd + ind}{scope}{self.name}::~{self.name}() {{}}\n"")

        if ns:
            target.write(f""{fullInd}}} // namespace {ns}\n"")"
287396,common-workflow-language/schema_salad,common-workflow-language_schema_salad/schema_salad/cpp_codegen.py,schema_salad.cpp_codegen.EnumDefinition,"from typing import IO, Any, Optional, Union, cast

class EnumDefinition:
    """"""Prototype of a enum.""""""

    def __init__(self, name: str, values: list[str]):
        """"""Initialize enum definition with a name and possible values.""""""
        self.name = name
        self.values = values
        self.namespace, self.classname = split_name(name)
        self.namespace = safenamespacename(self.namespace)
        self.classname = safename(self.classname)

    def writeDefinition(self, target: IO[str], ind: str, common_namespace: str) -> None:
        """"""Write enum definition to output.""""""
        namespace = ''
        if len(self.name.split('#')) == 2:
            namespace, classname = split_name(self.name)
            namespace = safenamespacename(namespace)
            classname = safename(classname)
            name = namespace + '::' + classname
        else:
            name = safename(self.name)
            classname = name
        if len(namespace) > 0:
            target.write(f'namespace {namespace} {{\n')
        target.write(f'enum class {classname} : unsigned int {{\n{ind}')
        target.write(f',\n{ind}'.join(map(safename, self.values)))
        target.write('\n};\n')
        target.write(f'inline auto to_string({classname} v) {{\n')
        target.write(f'{ind}static auto m = std::vector<std::string_view> {{\n')
        target.write(f'{ind}    ""')
        target.write(f'"",\n{ind}    ""'.join(self.values))
        target.write(f'""\n{ind}}};\n')
        target.write(f'{ind}using U = std::underlying_type_t<{name}>;\n')
        target.write(f'{ind}return m.at(static_cast<U>(v));\n}}\n')
        if len(namespace) > 0:
            target.write('}\n')
        target.write(f'inline void to_enum(std::string_view v, {name}& out) {{\n')
        target.write(f'{ind}static auto m = std::map<std::string, {name}, std::less<>> {{\n')
        for v in self.values:
            target.write(f'{ind}{ind}{{{q(v)}, {name}::{safename(v)}}},\n')
        target.write(f'{ind}}};\n{ind}auto iter = m.find(v);\n')
        target.write(f'{ind}if (iter == m.end()) throw bool{{}};\n')
        target.write(f'{ind}out = iter->second;\n}}\n')
        target.write(f'namespace {common_namespace} {{\n')
        target.write(f'inline auto toYaml({name} v, [[maybe_unused]] ::{common_namespace}::store_config const& config) {{\n')
        target.write(f'{ind}auto n = YAML::Node{{std::string{{to_string(v)}}}};\n')
        target.write(f'{ind}if (config.generateTags) n.SetTag(""{name}"");\n')
        target.write(f'{ind}return n;\n}}\n')
        target.write(f'inline void fromYaml(YAML::Node n, {name}& out) {{\n')
        target.write(f'{ind}to_enum(n.as<std::string>(), out);\n}}\n')
        if len(self.values):
            target.write(f'template <> struct IsConstant<{name}> : std::true_type {{}};\n')
        target.write('}\n')
        target.write('\n')","
class EnumDefinition:
    
    def __init__(self, name: str, values: list[str]):
        
        pass
    def writeDefinition(self, target: IO[str], ind: str, common_namespace: str) -> None:
        
        pass",snippet_210,3,"from typing import IO
import re


class EnumDefinition:
    def __init__(self, name: str, values: list[str]):
        if not isinstance(name, str) or not name:
            raise ValueError(""Enum name must be a non-empty string"")
        self.name = self._sanitize_identifier(name, allow_leading_underscore=False)
        if not self.name:
            raise ValueError(""Enum name is invalid after sanitization"")

        if not isinstance(values, list) or not values:
            raise ValueError(""Enum values must be a non-empty list of strings"")

        cleaned_values = []
        for v in values:
            if not isinstance(v, str) or not v:
                raise ValueError(""Enum values must be non-empty strings"")
            cv = self._sanitize_identifier(v, allow_leading_underscore=True)
            if not cv:
                raise ValueError(f""Enum value '{v}' is invalid after sanitization"")
            cleaned_values.append(cv)

        # Ensure uniqueness after sanitization
        seen = set()
        dedup = []
        for cv in cleaned_values:
            if cv in seen:
                raise ValueError(f""Duplicate enum value detected: {cv}"")
            seen.add(cv)
            dedup.append(cv)

        self.values = dedup

    def writeDefinition(self, target: IO[str], ind: str, common_namespace: str) -> None:
        if ind is None:
            ind = """"

        # Prepare namespaces
        ns_segments = self._split_namespaces(common_namespace) if common_namespace else []

        current_indent = """"
        # Open namespaces
        for seg in ns_segments:
            target.write(f""{current_indent}namespace {seg} {{\n"")
            current_indent += ind

        # Write enum
        target.write(f""{current_indent}enum class {self.name} {{\n"")
        inner_indent = current_indent + ind

        for i, val in enumerate(self.values):
            comma = "","" if i < len(self.values) - 1 else """"
            target.write(f""{inner_indent}{val}{comma}\n"")

        target.write(f""{current_indent}}};\n"")

        # Close namespaces
        for seg in reversed(ns_segments):
            current_indent = current_indent[:-len(ind)] if len(ind) <= len(current_indent) else """"
            target.write(f""{current_indent}}} // namespace {seg}\n"")

    @staticmethod
    def _sanitize_identifier(s: str, allow_leading_underscore: bool) -> str:
        # Replace invalid characters with underscore
        s = re.sub(r""\W"", ""_"", s)
        # Remove leading digits
        if s and s[0].isdigit():
            s = ""_"" + s
        # Disallow empty result
        if not s:
            return s
        # For enum name, avoid leading underscore if not allowed
        if not allow_leading_underscore and s.startswith(""_""):
            s = ""E"" + s
        return s

    @staticmethod
    def _split_namespaces(ns: str) -> list[str]:
        if not ns:
            return []
        # Support 'A::B::C' and 'A.B.C'
        parts = []
        for chunk in ns.split(""::""):
            parts.extend(chunk.split("".""))
        # Sanitize each namespace segment
        clean = []
        for p in parts:
            p = re.sub(r""\W"", ""_"", p)
            if p and p[0].isdigit():
                p = ""_"" + p
            if p:
                clean.append(p)
        return [p for p in clean if p]"
287397,common-workflow-language/schema_salad,common-workflow-language_schema_salad/schema_salad/cpp_codegen.py,schema_salad.cpp_codegen.FieldDefinition,"from typing import IO, Any, Optional, Union, cast

class FieldDefinition:
    """"""Prototype of a single field from a class definition.""""""

    def __init__(self, name: str, typeStr: str, optional: bool, mapSubject: str, mapPredicate: str, typeDSL: bool):
        """"""Initialize field definition.

        Creates a new field with name, its type, optional and which field to use to convert
        from list to map (or empty if it is not possible)
        """"""
        self.name = name
        self.typeStr = typeStr
        self.optional = optional
        self.mapSubject = mapSubject
        self.mapPredicate = mapPredicate
        self.typeDSL = typeDSL

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, namespace: str) -> None:
        """"""Write a C++ definition for the class field.""""""
        name = safename(self.name)
        typeStr = self.typeStr.replace(namespace + '::', '')
        target.write(f'{fullInd}heap_object<{typeStr}> {name};\n')","
class FieldDefinition:
    
    def __init__(self, name: str, typeStr: str, optional: bool, mapSubject: str, mapPredicate: str, typeDSL: bool):
        
        pass
    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, namespace: str) -> None:
        
        pass",snippet_211,3,"from typing import IO, Any


class FieldDefinition:
    PY_TO_DSL_TYPES = {
        ""str"": ""xsd:string"",
        ""int"": ""xsd:int"",
        ""float"": ""xsd:float"",
        ""bool"": ""xsd:boolean"",
        ""bytes"": ""xsd:base64Binary"",
        ""datetime"": ""xsd:dateTime"",
        ""date"": ""xsd:date"",
        ""time"": ""xsd:time"",
        ""decimal"": ""xsd:decimal"",
    }

    def __init__(self, name: str, typeStr: str, optional: bool, mapSubject: str, mapPredicate: str, typeDSL: bool):
        self.name = (name or """").strip()
        self.typeStr = (typeStr or """").strip()
        self.optional = bool(optional)
        self.mapSubject = (mapSubject or """").strip()
        self.mapPredicate = (mapPredicate or """").strip()
        self.typeDSL = bool(typeDSL)

        if not self.name:
            raise ValueError(""Field name must be a non-empty string."")
        if not self.typeStr:
            raise ValueError(""typeStr must be a non-empty string."")

    def _resolve_type(self) -> str:
        if self.typeDSL:
            return self.typeStr
        key = self.typeStr.strip()
        # normalize common annotations like ""string"", ""boolean""
        normalized = {
            ""string"": ""str"",
            ""integer"": ""int"",
            ""number"": ""float"",
            ""boolean"": ""bool"",
        }.get(key.lower(), key)
        return self.PY_TO_DSL_TYPES.get(normalized, normalized)

    @staticmethod
    def _join_namespace(namespace: str, value: str) -> str:
        if not value:
            return value
        ns = (namespace or """").strip()
        v = value.strip()
        if not ns:
            return v
        # If value already looks namespaced or absolute, return as-is
        if "":"" in v or v.startswith(""http://"") or v.startswith(""https://""):
            return v
        # Ensure a separator if namespace does not end with one
        if ns.endswith((""/"", ""#"", "":"")):
            return f""{ns}{v}""
        return f""{ns}:{v}""

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, namespace: str) -> None:
        if target is None:
            raise ValueError(""target must be a writable IO."")
        base = fullInd or """"
        step = ind or """"

        resolved_type = self._resolve_type()
        opt_suffix = ""?"" if self.optional else """"

        # Field signature
        target.write(f""{base}{self.name}: {resolved_type}{opt_suffix}\n"")

        # Mapping details
        if self.mapSubject:
            target.write(f""{base}{step}subject: {self.mapSubject}\n"")
        if self.mapPredicate:
            predicate = self._join_namespace(namespace, self.mapPredicate)
            target.write(f""{base}{step}predicate: {predicate}\n"")"
287519,kinegratii/borax,kinegratii_borax/borax/calendars/utils.py,borax.calendars.utils.ThreeNineUtils,"from borax.calendars.lunardate import LunarDate, TextUtils, TermUtils
from collections import OrderedDict
from datetime import date, datetime, timedelta
from typing import Union, Dict

class ThreeNineUtils:
    """"""三伏数九天工具函数
    """"""

    @staticmethod
    def get_39days(year: int) -> Dict[str, date]:
        """"""获取公历year年的三伏数九天对应的公历日期。
        """"""
        day13 = TermUtils.day_start_from_term(year, '夏至', 3, '庚')
        day23 = day13 + timedelta(days=10)
        day33 = TermUtils.day_start_from_term(year, '立秋', 1, '庚')
        day19 = TermUtils.day_start_from_term(year, '冬至', 0)
        days = OrderedDict({'初伏': day13, '中伏': day23, '末伏': day33, '一九': day19})
        for i, dc in enumerate(TextUtils.DAYS_CN[1:10], start=1):
            days[f'{dc}九'] = day19 + timedelta(days=(i - 1) * 9)
        return days

    @staticmethod
    def get_39label(date_obj: Union[date, LunarDate]) -> str:
        """"""返回三伏数九天对应的标签，如果不是，返回空字符串。
        """"""
        if isinstance(date_obj, LunarDate):
            sd = date_obj.to_solar_date()
        else:
            sd = date_obj
        if sd.month in (4, 5, 6, 10, 11):
            return ''
        year = sd.year - bool(sd.month < 4)
        days = ThreeNineUtils.get_39days(year)
        for vs in list(days.items()):
            label, sd = vs
            range_len = -1
            if label in ['初伏', '末伏']:
                range_len = 10
            elif label == '中伏':
                range_len = (days['末伏'] - days['中伏']).days
            elif '九' in label:
                range_len = 9
            offset = (date_obj - sd).days
            if 0 <= offset <= range_len - 1:
                return f'{label}第{offset + 1}天'
        return ''","
class ThreeNineUtils:
    
    @staticmethod
    def get_39days(year: int) -> Dict[str, date]:
        
        pass
    @staticmethod
    def get_39label(date_obj: Union[date, LunarDate]) -> str:
        
        pass",snippet_212,3,"from datetime import date, timedelta
from typing import Dict, Union

try:
    from lunardate import LunarDate  # type: ignore
except Exception:  # pragma: no cover
    LunarDate = None  # type: ignore


class ThreeNineUtils:
    _CN_NUMS = (""一"", ""二"", ""三"", ""四"", ""五"", ""六"", ""七"", ""八"", ""九"")

    @staticmethod
    def _dongzhi_start(year: int) -> date:
        # Approximate start of ShuJiu at winter solstice (Dongzhi).
        # Use Dec 21 as a simple, consistent approximation.
        return date(year, 12, 21)

    @staticmethod
    def _label_for_index(idx: int) -> str:
        nine = idx // 9 + 1  # 1..9
        day_in_nine = idx % 9 + 1  # 1..9
        return f""{ThreeNineUtils._CN_NUMS[nine - 1]}九第{day_in_nine}天""

    @staticmethod
    def get_39days(year: int) -> Dict[str, date]:
        start = ThreeNineUtils._dongzhi_start(year)
        days: Dict[str, date] = {}
        for i in range(81):
            label = ThreeNineUtils._label_for_index(i)
            days[label] = start + timedelta(days=i)
        return days

    @staticmethod
    def get_39label(date_obj: Union[date, ""LunarDate""]) -> str:
        if LunarDate is not None and isinstance(date_obj, LunarDate):
            d = date_obj.toSolarDate()
        elif isinstance(date_obj, date):
            d = date_obj
        else:
            raise TypeError(""date_obj must be datetime.date or LunarDate"")

        start_this_year = ThreeNineUtils._dongzhi_start(d.year)
        if d >= start_this_year:
            start = start_this_year
        else:
            start = ThreeNineUtils._dongzhi_start(d.year - 1)

        delta = (d - start).days
        if 0 <= delta < 81:
            return ThreeNineUtils._label_for_index(delta)
        return """""
292643,swistakm/graceful,swistakm_graceful/src/graceful/authentication.py,graceful.authentication.BaseAuthenticationMiddleware,"class BaseAuthenticationMiddleware:
    """"""Base class for all authentication middleware classes.

    Args:
        user_storage (BaseUserStorage): a storage object used to retrieve
            user object using their identifier lookup.
        name (str): custom name of the authentication middleware useful
            for handling custom user storage backends. Defaults to middleware
            class name.

    .. versionadded:: 0.4.0
    """"""
    challenge = None
    only_with_storage = False

    def __init__(self, user_storage=None, name=None):
        """"""Initialize authentication middleware.""""""
        self.user_storage = user_storage
        self.name = name if name else self.__class__.__name__
        if self.only_with_storage and (not isinstance(self.user_storage, BaseUserStorage)):
            raise ValueError('{} authentication middleware requires valid storage. Got {}.'.format(self.__class__.__name__, self.user_storage))

    def process_resource(self, req, resp, resource, uri_kwargs=None):
        """"""Process resource after routing to it.

        This is basic falcon middleware handler.

        Args:
            req (falcon.Request): request object
            resp (falcon.Response): response object
            resource (object): resource object matched by falcon router
            uri_kwargs (dict): additional keyword argument from uri template.
                For ``falcon<1.0.0`` this is always ``None``
        """"""
        if 'user' in req.context:
            return
        identifier = self.identify(req, resp, resource, uri_kwargs)
        user = self.try_storage(identifier, req, resp, resource, uri_kwargs)
        if user is not None:
            req.context['user'] = user
        elif self.challenge is not None:
            req.context.setdefault('challenges', list()).append(self.challenge)

    def identify(self, req, resp, resource, uri_kwargs):
        """"""Identify the user that made the request.

        Args:
            req (falcon.Request): request object
            resp (falcon.Response): response object
            resource (object): resource object matched by falcon router
            uri_kwargs (dict): additional keyword argument from uri template.
                For ``falcon<1.0.0`` this is always ``None``

        Returns:
            object: a user object (preferably a dictionary).
        """"""
        raise NotImplementedError

    def try_storage(self, identifier, req, resp, resource, uri_kwargs):
        """"""Try to find user in configured user storage object.

        Args:
            identifier: User identifier.

        Returns:
            user object.
        """"""
        if identifier is None:
            user = None
        elif self.user_storage is not None:
            user = self.user_storage.get_user(self, identifier, req, resp, resource, uri_kwargs)
        elif self.user_storage is None and (not self.only_with_storage):
            user = {'identified_with': self, 'identifier': identifier}
        else:
            user = None
        return user","class BaseAuthenticationMiddleware:
    
    def __init__(self, user_storage=None, name=None):
        
        pass
    def process_resource(self, req, resp, resource, uri_kwargs=None):
        
        pass
    def identify(self, req, resp, resource, uri_kwargs):
        
        pass

    def try_storage(self, identifier, req, resp, resource, uri_kwargs):
        
        pass",snippet_213,5,"class BaseAuthenticationMiddleware:
    def __init__(self, user_storage=None, name=None):
        if user_storage is None:
            storages = []
        elif isinstance(user_storage, (list, tuple, set)):
            storages = list(user_storage)
        else:
            storages = [user_storage]

        self.user_storage = [s for s in storages if s is not None]
        self.name = name or ""user""

    def process_resource(self, req, resp, resource, uri_kwargs=None):
        ctx = self._ensure_context(req)
        errors = self._ensure_errors(ctx)

        try:
            identity = self.identify(req, resp, resource, uri_kwargs or {})
        except Exception as exc:
            errors.append(exc)
            identity = None

        self._store_identity(ctx, identity)

    def identify(self, req, resp, resource, uri_kwargs):
        for storage in self.user_storage:
            user = self.try_storage(storage, req, resp, resource, uri_kwargs)
            if user is not None:
                return user
        return None

    def try_storage(self, identifier, req, resp, resource, uri_kwargs):
        try:
            if callable(identifier):
                return identifier(req, resp, resource, uri_kwargs)
            identify_method = getattr(identifier, ""identify"", None)
            if callable(identify_method):
                return identify_method(req, resp, resource, uri_kwargs)
        except Exception:
            pass
        return None

    # Internal helpers
    def _ensure_context(self, req):
        if not hasattr(req, ""context"") or req.context is None:
            req.context = {}
        return req.context

    def _ensure_errors(self, ctx):
        # Support both dict-like and attribute contexts
        key = f""{self.name}_auth_errors""
        if isinstance(ctx, dict):
            errors = ctx.get(key)
            if errors is None:
                errors = []
                ctx[key] = errors
            return errors
        else:
            errors = getattr(ctx, key, None)
            if errors is None:
                errors = []
                setattr(ctx, key, errors)
            return errors

    def _store_identity(self, ctx, identity):
        # Support both dict-like and attribute contexts
        if isinstance(ctx, dict):
            ctx[self.name] = identity
        else:
            setattr(ctx, self.name, identity)"
292765,pytroll/pyspectral,pytroll_pyspectral/rsr_convert_scripts/avhrr1_rsr.py,avhrr1_rsr.AvhrrRSR,"import numpy as np
from pyspectral.config import get_config
from xlrd import open_workbook
import os

class AvhrrRSR:
    """"""Container for the NOAA AVHRR-1 RSR data.""""""

    def __init__(self, wavespace='wavelength'):
        """"""Initialize the AVHRR-1 RSR class.""""""
        options = get_config()
        self.avhrr_path = options['avhrr/1'].get('path')
        if not os.path.exists(self.avhrr_path):
            self.avhrr1_path = os.path.join(DATA_PATH, options['avhrr/1'].get('filename'))
        self.output_dir = options.get('rsr_dir', './')
        self.rsr = {}
        for satname in AVHRR1_SATELLITES:
            self.rsr[satname] = {}
            for chname in AVHRR_BAND_NAMES['avhrr/1']:
                self.rsr[satname][chname] = {'wavelength': None, 'response': None}
        self._load()
        self.wavespace = wavespace
        if wavespace not in ['wavelength', 'wavenumber']:
            raise AttributeError('wavespace has to be either ' + ""'wavelength' or 'wavenumber'!"")
        self.unit = 'micrometer'
        if wavespace == 'wavenumber':
            self.convert2wavenumber()

    def _load(self, scale=1.0):
        """"""Load the AVHRR RSR data for the band requested.""""""
        wb_ = open_workbook(self.avhrr_path)
        sheet_names = []
        for sheet in wb_.sheets():
            if sheet.name in ['Kleespies Data']:
                print('Skip sheet...')
                continue
            ch_name = CHANNEL_NAMES.get(sheet.name.strip())
            if not ch_name:
                break
            sheet_names.append(sheet.name.strip())
            header = sheet.col_values(0, start_rowx=0, end_rowx=2)
            platform_name = header[0].strip('# ')
            unit = header[1].split('Wavelength (')[1].strip(')')
            scale = get_scale_from_unit(unit)
            wvl = sheet.col_values(0, start_rowx=2)
            is_comment = True
            idx = 0
            while is_comment:
                item = wvl[::-1][idx]
                if isinstance(item, str):
                    idx = idx + 1
                else:
                    break
            ndim = len(wvl) - idx
            wvl = wvl[0:ndim]
            if platform_name == 'TIROS-N':
                wvl = adjust_typo_avhrr1_srf_only_xls_file(platform_name, wvl)
            response = sheet.col_values(1, start_rowx=2, end_rowx=2 + ndim)
            wavelength = np.array(wvl) * scale
            response = np.array(response)
            self.rsr[platform_name][ch_name]['wavelength'] = wavelength
            self.rsr[platform_name][ch_name]['response'] = response","
class AvhrrRSR:
    
    def __init__(self, wavespace='wavelength'):
        
        pass
    def _load(self, scale=1.0):
        
        pass",snippet_215,3,"import numpy as np

class AvhrrRSR:
    
    def __init__(self, wavespace='wavelength'):
        valid = {'wavelength', 'wavenumber'}
        if wavespace not in valid:
            raise ValueError(f""wavespace must be one of {valid}"")
        self.wavespace = wavespace
        self._data = None

    def _gaussian(self, x, mu, fwhm):
        sigma = fwhm / (2 * np.sqrt(2 * np.log(2)))
        g = np.exp(-0.5 * ((x - mu) / sigma) ** 2)
        return g

    def _rsr_band(self, x, center, width):
        g = self._gaussian(x, center, width)
        g[g < 1e-4] = 0.0
        if g.max() > 0:
            g = g / g.max()
        return g

    def _build_wavelength_grid(self):
        grids = {}
        # Visible/NIR
        grids['B1'] = np.linspace(0.50, 0.80, 601)   # 0.58–0.68
        grids['B2'] = np.linspace(0.70, 1.20, 1001)  # 0.725–1.1
        # AVHRR/3 3A
        grids['B3A'] = np.linspace(1.45, 1.80, 701)  # 1.58–1.64
        # TIR bands
        grids['B3B'] = np.linspace(3.20, 4.20, 1001) # 3.55–3.93
        grids['B4']  = np.linspace(9.00, 13.00, 1601) # 10.3–11.3
        grids['B5']  = np.linspace(9.00, 14.00, 2501) # 11.5–12.5
        return grids

    def _make_rsr_wavelength(self):
        wl = self._build_wavelength_grid()
        rsr = {}
        # Approximate centers and FWHM based on typical AVHRR/3 specs
        specs = {
            'B1':  (0.64, 0.10),
            'B2':  (0.86, 0.25),
            'B3A': (1.61, 0.08),
            'B3B': (3.74, 0.40),
            'B4':  (10.8, 0.90),
            'B5':  (12.0, 1.00),
        }
        for band, x in wl.items():
            c, w = specs[band]
            y = self._rsr_band(x, c, w)
            rsr[band] = (x, y)
        return rsr

    def _convert_to_wavenumber(self, rsr_wl):
        rsr_wn = {}
        for band, (wl_um, resp) in rsr_wl.items():
            wn = 10000.0 / wl_um
            # Interpolation to ensure monotonic ascending domain
            order = np.argsort(wn)
            wn_sorted = wn[order]
            resp_sorted = resp[order]
            rsr_wn[band] = (wn_sorted, resp_sorted)
        return rsr_wn

    def _scale_domain(self, data, scale):
        if scale == 1.0:
            return data
        out = {}
        for band, (x, y) in data.items():
            out[band] = (x * scale, y.copy())
        return out

    def _load(self, scale=1.0):
        rsr_wl = self._make_rsr_wavelength()
        if self.wavespace == 'wavenumber':
            data = self._convert_to_wavenumber(rsr_wl)
        else:
            data = rsr_wl
        data = self._scale_domain(data, scale)
        self._data = data
        return data"
293144,Kentzo/Power,Kentzo_Power/power/common.py,power.common.PowerManagementObserver,"from abc import ABCMeta, abstractmethod

class PowerManagementObserver:
    """"""
    Base class for PowerManagement observers.
    Do not make assumptions in what thread or event loop these methods are called.
    """"""
    __metaclass__ = ABCMeta

    @abstractmethod
    def on_power_sources_change(self, power_management):
        """"""
        @param power_management: Instance of PowerManagement posted notification
        """"""
        pass

    @abstractmethod
    def on_time_remaining_change(self, power_management):
        """"""
        @param power_management: Instance of PowerManagement posted notification
        """"""
        pass","
class PowerManagementObserver:
    
    @abstractmethod
    def on_power_sources_change(self, power_management):
        
        pass
    @abstractmethod
    def on_time_remaining_change(self, power_management):
        
        pass",snippet_216,3,"from abc import ABC, abstractmethod

class PowerManagementObserver(ABC):
    @abstractmethod
    def on_power_sources_change(self, power_management) -> None:
        ...

    @abstractmethod
    def on_time_remaining_change(self, power_management) -> None:
        ..."
293849,hasgeek/coaster,hasgeek_coaster/src/coaster/logger.py,coaster.logger.FilteredValueIndicator,"class FilteredValueIndicator:
    """"""Represent a filtered value.""""""

    def __str__(self) -> str:
        """"""Filter str.""""""
        return '[Filtered]'

    def __repr__(self) -> str:
        """"""Filter repr.""""""
        return '[Filtered]'","class FilteredValueIndicator:
    
    def __str__(self) -> str:
        
        pass
    def __repr__(self) -> str:
        
        pass",snippet_217,3,"class FilteredValueIndicator:
    
    def __str__(self) -> str:
        return ""<filtered>""
    
    def __repr__(self) -> str:
        return f""{self.__class__.__name__}()"""
293851,hasgeek/coaster,hasgeek_coaster/src/coaster/logger.py,coaster.logger.RepeatValueIndicator,"class RepeatValueIndicator:
    """"""Represent a repeating value.""""""

    def __init__(self, key: str) -> None:
        """"""Init with key.""""""
        self.key = key

    def __repr__(self) -> str:
        """"""Return representation.""""""
        return f'<same as prior {self.key!r}>'
    __str__ = __repr__","class RepeatValueIndicator:
    
    def __init__(self, key: str) -> None:
        
        pass
    def __repr__(self) -> str:
        
        pass",snippet_218,3,"class RepeatValueIndicator:
    
    def __init__(self, key: str) -> None:
        if not isinstance(key, str):
            raise TypeError(""key must be a str"")
        self.key = key

    def __repr__(self) -> str:
        return f""{self.__class__.__name__}({self.key!r})"""
295935,cloudsmith-io/cloudsmith-cli,cloudsmith-io_cloudsmith-cli/cloudsmith_cli/core/ratelimits.py,cloudsmith_cli.core.ratelimits.RateLimitsInfo,"import datetime

class RateLimitsInfo:
    """"""Data for rate limits.""""""
    interval = None
    limit = None
    remaining = None
    reset = None
    throttled = None

    def __str__(self):
        """"""Get rate limit information as text.""""""
        return 'Throttled: %(throttled)s, Remaining: %(remaining)d/%(limit)d, Interval: %(interval)f, Reset: %(reset)s' % {'throttled': 'Yes' if self.throttled else 'No', 'remaining': self.remaining, 'limit': self.limit, 'interval': self.interval, 'reset': self.reset}

    @classmethod
    def from_dict(cls, data):
        """"""Create RateLimitsInfo from a dictionary.""""""
        info = RateLimitsInfo()
        if 'interval' in data:
            info.interval = float(data['interval'])
        if 'limit' in data:
            info.limit = int(data['limit'])
        if 'remaining' in data:
            info.remaining = int(data['remaining'])
        if 'reset' in data:
            info.reset = datetime.datetime.utcfromtimestamp(int(data['reset']))
        if 'throtted' in data:
            info.throttled = bool(data['throttled'])
        else:
            info.throttled = info.remaining == 0
        return info

    @classmethod
    def from_headers(cls, headers):
        """"""Create RateLimitsInfo from HTTP headers.""""""
        try:
            data = {'interval': headers['X-RateLimit-Interval'], 'limit': headers['X-RateLimit-Limit'], 'remaining': headers['X-RateLimit-Remaining'], 'reset': headers['X-RateLimit-Reset']}
        except KeyError:
            data = {}
        return cls.from_dict(data)","
class RateLimitsInfo:
    
    def __str__(self):
        
        pass
    @classmethod
    def from_dict(cls, data):
        
        pass
    @classmethod
    def from_headers(cls, headers):
        
        pass",snippet_220,4,"class RateLimitsInfo:
    def __init__(
        self,
        limit=None,
        remaining=None,
        reset=None,
        reset_after=None,
        retry_after=None,
        used=None,
        bucket=None,
        scope=None,
    ):
        self.limit = limit
        self.remaining = remaining
        self.reset = reset
        self.reset_after = reset_after
        self.retry_after = retry_after
        self.used = used
        self.bucket = bucket
        self.scope = scope

    def __str__(self):
        parts = []
        parts.append(f""limit={self.limit}"" if self.limit is not None else ""limit=?"")
        parts.append(f""remaining={self.remaining}"" if self.remaining is not None else ""remaining=?"")
        parts.append(f""used={self.used}"" if self.used is not None else ""used=?"")
        parts.append(f""reset={self.reset}"" if self.reset is not None else ""reset=?"")
        parts.append(f""reset_after={self.reset_after}"" if self.reset_after is not None else ""reset_after=?"")
        parts.append(f""retry_after={self.retry_after}"" if self.retry_after is not None else ""retry_after=?"")
        parts.append(f""bucket={self.bucket}"" if self.bucket else ""bucket=?"")
        parts.append(f""scope={self.scope}"" if self.scope else ""scope=?"")
        return f""RateLimitsInfo({', '.join(parts)})""

    @classmethod
    def from_dict(cls, data):
        if data is None:
            return cls()

        def get_first(d, keys, default=None):
            for k in keys:
                if k in d:
                    return d[k]
            return default

        def to_int(v):
            try:
                if v is None:
                    return None
                if isinstance(v, bool):
                    return int(v)
                if isinstance(v, (int,)):
                    return v
                if isinstance(v, float):
                    # Some sources provide reset as float seconds; keep int for epoch
                    return int(v)
                s = str(v).strip()
                if s == """":
                    return None
                return int(float(s))
            except Exception:
                return None

        def to_float(v):
            try:
                if v is None:
                    return None
                if isinstance(v, (int, float)):
                    return float(v)
                s = str(v).strip()
                if s == """":
                    return None
                return float(s)
            except Exception:
                return None

        limit = get_first(data, (""limit"", ""rate_limit"", ""x-ratelimit-limit"", ""X-RateLimit-Limit""))
        remaining = get_first(data, (""remaining"", ""rate_remaining"", ""x-ratelimit-remaining"", ""X-RateLimit-Remaining""))
        used = get_first(data, (""used"", ""x-ratelimit-used"", ""X-RateLimit-Used""))
        reset = get_first(data, (""reset"", ""x-ratelimit-reset"", ""X-RateLimit-Reset"", ""reset_epoch""))
        reset_after = get_first(data, (""reset_after"", ""reset-after"", ""x-ratelimit-reset-after"", ""X-RateLimit-Reset-After""))
        retry_after = get_first(data, (""retry_after"", ""retry-after"", ""Retry-After""))
        bucket = get_first(data, (""bucket"", ""x-ratelimit-bucket"", ""X-RateLimit-Bucket""))
        scope = get_first(data, (""scope"", ""x-ratelimit-scope"", ""X-RateLimit-Scope""))

        return cls(
            limit=to_int(limit),
            remaining=to_int(remaining),
            reset=to_int(reset),
            reset_after=to_float(reset_after),
            retry_after=to_float(retry_after),
            used=to_int(used),
            bucket=str(bucket) if bucket is not None else None,
            scope=str(scope) if scope is not None else None,
        )

    @classmethod
    def from_headers(cls, headers):
        if headers is None:
            return cls()

        # Normalize header dict to case-insensitive lookup
        class _H:
            def __init__(self, h):
                self._map = {}
                try:
                    items = h.items()
                except Exception:
                    # Fallback if not dict-like
                    try:
                        items = list(h)
                    except Exception:
                        items = []
                for k, v in items:
                    self._map[str(k).lower()] = v

            def get(self, key, default=None):
                return self._map.get(str(key).lower(), default)

        h = _H(headers)

        def to_int(v):
            try:
                if v is None:
                    return None
                if isinstance(v, bool):
                    return int(v)
                if isinstance(v, int):
                    return v
                if isinstance(v, float):
                    return int(v)
                s = str(v).strip()
                if s == """":
                    return None
                # Some providers send decimal strings; convert via float first
                return int(float(s))
            except Exception:
                return None

        def to_float(v):
            try:
                if v is None:
                    return None
                if isinstance(v, (int, float)):
                    return float(v)
                s = str(v).strip()
                if s == """":
                    return None
                return float(s)
            except Exception:
                return None

        limit = h.get(""x-ratelimit-limit"") or h.get(""ratelimit-limit"") or h.get(""rate-limit"")
        remaining = h.get(""x-ratelimit-remaining"") or h.get(""ratelimit-remaining"")
        used = h.get(""x-ratelimit-used"") or h.get(""ratelimit-used"")
        reset = h.get(""x-ratelimit-reset"") or h.get(""ratelimit-reset"")
        reset_after = h.get(""x-ratelimit-reset-after"") or h.get(""ratelimit-reset-after"")
        retry_after = h.get(""retry-after"")
        bucket = h.get(""x-ratelimit-bucket"") or h.get(""ratelimit-bucket"")
        scope = h.get(""x-ratelimit-scope"") or h.get(""ratelimit-scope"")

        return cls(
            limit=to_int(limit),
            remaining=to_int(remaining),
            reset=to_int(reset),
            reset_after=to_float(reset_after),
            retry_after=to_float(retry_after),
            used=to_int(used),
            bucket=str(bucket) if bucket is not None else None,
            scope=str(scope) if scope is not None else None,
        )"
296505,CZ-NIC/python-rt,CZ-NIC_python-rt/rt/rest2.py,rt.rest2.Attachment,"import dataclasses
import base64

@dataclasses.dataclass
class Attachment:
    """"""Dataclass representing an attachment.""""""
    file_name: str
    file_type: str
    file_content: bytes

    def to_dict(self) -> dict[str, str]:
        """"""Convert to a dictionary for submitting to the REST API.""""""
        return {'FileName': self.file_name, 'FileType': self.file_type, 'FileContent': base64.b64encode(self.file_content).decode('utf-8')}

    def multipart_form_element(self) -> tuple[str, bytes, str]:
        """"""Convert to a tuple as required for multipart-form-data submission.""""""
        return (self.file_name, self.file_content, self.file_type)","@dataclasses.dataclass
class Attachment:
    
    def to_dict(self) -> dict[str, str]:
        
        pass
    def multipart_form_element(self) -> tuple[str, bytes, str]:
        
        pass",snippet_221,3,"from __future__ import annotations

import base64
import dataclasses
from typing import Union


@dataclasses.dataclass
class Attachment:
    filename: str
    data: bytes
    content_type: str = ""application/octet-stream""

    def __post_init__(self) -> None:
        if isinstance(self.data, str):
            self.data = self.data.encode(""utf-8"")
        elif not isinstance(self.data, (bytes, bytearray, memoryview)):
            raise TypeError(""data must be bytes, bytearray, memoryview, or str"")
        if isinstance(self.data, (bytearray, memoryview)):
            self.data = bytes(self.data)

    def to_dict(self) -> dict[str, str]:
        return {
            ""filename"": self.filename,
            ""content"": base64.b64encode(self.data).decode(""ascii""),
            ""content_type"": self.content_type,
        }

    def multipart_form_element(self) -> tuple[str, bytes, str]:
        return (self.filename, self.data, self.content_type)"
297960,theolind/pymysensors,theolind_pymysensors/mysensors/transport.py,mysensors.transport.Transport,"class Transport:
    """"""Handle gateway transport.

    I/O is allowed in this class. This class should host methods that
    are related to the gateway transport type.
    """"""

    def __init__(self, gateway, connect, timeout=1.0, reconnect_timeout=10.0, **kwargs):
        """"""Set up transport.""""""
        self._connect = connect
        self.can_log = False
        self.connect_task = None
        self.gateway = gateway
        self.protocol = None
        self.reconnect_timeout = reconnect_timeout
        self.timeout = timeout

    def disconnect(self):
        """"""Disconnect from the transport.""""""
        if not self.protocol or not self.protocol.transport:
            self.protocol = None
            return
        _LOGGER.info('Disconnecting from gateway')
        self.protocol.transport.close()
        self.protocol = None

    def send(self, message):
        """"""Write a message to the gateway.""""""
        if not message or not self.protocol or (not self.protocol.transport):
            return
        if not self.can_log:
            _LOGGER.debug('Sending %s', message.strip())
        try:
            self.protocol.transport.write(message.encode())
        except OSError as exc:
            _LOGGER.error('Failed writing to transport %s: %s', self.protocol.transport, exc)
            self.protocol.transport.close()
            self.protocol.conn_lost_callback()","class Transport:
    
    def __init__(self, gateway, connect, timeout=1.0, reconnect_timeout=10.0, **kwargs):
        
        pass
    def disconnect(self):
        
        pass
    def send(self, message):
        
        pass",snippet_222,4,"import time
import threading
import inspect


class Transport:
    def __init__(self, gateway, connect, timeout=1.0, reconnect_timeout=10.0, **kwargs):
        if not callable(connect):
            raise TypeError(""connect must be a callable that establishes and returns a connection"")
        if timeout <= 0:
            raise ValueError(""timeout must be > 0"")
        if reconnect_timeout < 0:
            raise ValueError(""reconnect_timeout must be >= 0"")

        self._gateway = gateway
        self._connect_fn = connect
        self._timeout = float(timeout)
        self._reconnect_timeout = float(reconnect_timeout)
        self._connect_kwargs = dict(kwargs)

        self._lock = threading.RLock()
        self._conn = None
        self._closed = False

        # Attempt initial connection within timeout window
        self._establish(self._timeout)

    def _establish(self, timeout):
        deadline = time.monotonic() + timeout
        last_err = None
        while time.monotonic() < deadline and not self._closed:
            try:
                conn = self._call_connect(self._connect_fn, self._gateway, deadline - time.monotonic())
                if conn is None:
                    raise RuntimeError(""connect callable returned None"")
                self._conn = conn
                return
            except Exception as e:
                last_err = e
                time.sleep(min(0.05, max(0.0, deadline - time.monotonic())))
        if last_err:
            raise last_err
        raise TimeoutError(""Connection establishment timed out"")

    def _call_connect(self, fn, gateway, remaining_timeout):
        sig = inspect.signature(fn)
        kwargs = dict(self._connect_kwargs)
        bound = None
        try:
            bound = sig.bind_partial()
        except Exception:
            pass

        params = list(sig.parameters.values())
        args = []
        # Try to pass gateway if the function expects at least 1 positional argument
        if params and params[0].kind in (inspect.Parameter.POSITIONAL_ONLY, inspect.Parameter.POSITIONAL_OR_KEYWORD):
            args.append(gateway)
        else:
            kwargs.setdefault(""gateway"", gateway)

        # Try to pass timeout if acceptable
        if any(p.name == ""timeout"" for p in params) or ""timeout"" in kwargs or any(
            p.kind == inspect.Parameter.VAR_KEYWORD for p in params
        ):
            kwargs.setdefault(""timeout"", max(0.0, float(remaining_timeout)))

        return fn(*args, **kwargs)

    def _send_via(self, conn, message, remaining_timeout):
        # Try common send patterns
        # 1) conn.send(message, timeout=?)
        if hasattr(conn, ""send"") and callable(getattr(conn, ""send"")):
            send_fn = getattr(conn, ""send"")
            sig = None
            try:
                sig = inspect.signature(send_fn)
            except Exception:
                sig = None
            if sig:
                kwargs = {}
                if any(p.name == ""timeout"" for p in sig.parameters.values()) or any(
                    p.kind == inspect.Parameter.VAR_KEYWORD for p in sig.parameters.values()
                ):
                    kwargs[""timeout""] = max(0.0, remaining_timeout)
                return send_fn(message, **kwargs)
            return send_fn(message)

        # 2) conn(message, timeout=?)
        if callable(conn):
            sig = None
            try:
                sig = inspect.signature(conn)
            except Exception:
                sig = None
            if sig:
                kwargs = {}
                if any(p.name == ""timeout"" for p in sig.parameters.values()) or any(
                    p.kind == inspect.Parameter.VAR_KEYWORD for p in sig.parameters.values()
                ):
                    kwargs[""timeout""] = max(0.0, remaining_timeout)
                return conn(message, **kwargs)
            return conn(message)

        # 3) fallback to gateway if it has send
        if hasattr(self._gateway, ""send"") and callable(getattr(self._gateway, ""send"")):
            send_fn = getattr(self._gateway, ""send"")
            sig = None
            try:
                sig = inspect.signature(send_fn)
            except Exception:
                sig = None
            if sig:
                kwargs = {}
                if any(p.name == ""timeout"" for p in sig.parameters.values()) or any(
                    p.kind == inspect.Parameter.VAR_KEYWORD for p in sig.parameters.values()
                ):
                    kwargs[""timeout""] = max(0.0, remaining_timeout)
                return send_fn(message, **kwargs)
            return send_fn(message)

        raise RuntimeError(""No valid send method available on connection or gateway"")

    def disconnect(self):
        with self._lock:
            self._closed = True
            conn, self._conn = self._conn, None
        # Attempt graceful close outside lock
        if conn is not None:
            try:
                if hasattr(conn, ""close"") and callable(getattr(conn, ""close"")):
                    conn.close()
                elif hasattr(conn, ""disconnect"") and callable(getattr(conn, ""disconnect"")):
                    conn.disconnect()
            except Exception:
                pass

    def send(self, message):
        with self._lock:
            if self._closed:
                raise RuntimeError(""Transport is closed"")

            # Ensure connection, reconnect if needed
            if self._conn is None:
                self._establish(self._timeout)

            conn = self._conn

        # Try send, on failure attempt reconnect within reconnect_timeout and retry once
        start = time.monotonic()
        try:
            return self._send_via(conn, message, self._timeout)
        except Exception:
            # Try to reconnect within reconnect timeout
            with self._lock:
                # If already closed externally, abort
                if self._closed:
                    raise
                # Drop old connection
                old = self._conn
                self._conn = None
            if old is not None:
                try:
                    if hasattr(old, ""close"") and callable(getattr(old, ""close"")):
                        old.close()
                    elif hasattr(old, ""disconnect"") and callable(getattr(old, ""disconnect"")):
                        old.disconnect()
                except Exception:
                    pass

            remaining = max(0.0, self._reconnect_timeout - (time.monotonic() - start))
            with self._lock:
                if not self._closed:
                    self._establish(remaining if remaining > 0 else 0.0)
                    conn2 = self._conn
                else:
                    raise RuntimeError(""Transport is closed"")

            return self._send_via(conn2, message, self._timeout)"
298148,edx/event-tracking,edx_event-tracking/eventtracking/backends/logger.py,eventtracking.backends.logger.LoggerBackend,"import logging
import json

class LoggerBackend:
    """"""
    Event tracker backend that uses a python logger.

    Events are logged to the INFO level as JSON strings.
    """"""

    def __init__(self, **kwargs):
        """"""
        Event tracker backend that uses a python logger.

        `name` is an identifier for the logger, which should have
            been configured using the default python mechanisms.
        """"""
        name = kwargs.get('name', None)
        self.max_event_size = kwargs.get('max_event_size', MAX_EVENT_SIZE)
        self.event_logger = logging.getLogger(name)
        level = kwargs.get('level', 'info')
        self.log = getattr(self.event_logger, level.lower())

    def send(self, event):
        """"""Send the event to the standard python logger""""""
        event_str = json.dumps(event, cls=DateTimeJSONEncoder)
        if self.max_event_size is None or len(event_str) <= self.max_event_size:
            self.log(event_str)","
class LoggerBackend:
    
    def __init__(self, **kwargs):
        
        pass
    def send(self, event):
        
        pass",snippet_223,3,"class LoggerBackend:
    
    def __init__(self, **kwargs):
        import sys
        import threading

        self._lock = threading.RLock()
        self._owns_stream = False

        stream = kwargs.get(""stream"")
        filename = kwargs.get(""filename"")
        self._flush = bool(kwargs.get(""flush"", False))
        self._json = bool(kwargs.get(""json"", False))

        if stream is not None and filename is not None:
            raise ValueError(""Provide either 'stream' or 'filename', not both."")

        if filename is not None:
            self._stream = open(filename, ""a"", encoding=kwargs.get(""encoding"", ""utf-8""))
            self._owns_stream = True
        elif stream is not None:
            if not hasattr(stream, ""write""):
                raise TypeError(""Provided 'stream' must be a file-like object with a 'write' method."")
            self._stream = stream
        else:
            self._stream = sys.stderr

        formatter = kwargs.get(""formatter"")
        if formatter is not None and not callable(formatter):
            raise TypeError(""'formatter' must be callable."")
        self._formatter = formatter or self._default_format

    def _default_format(self, event):
        import datetime as _dt
        import json as _json

        ts = _dt.datetime.utcnow().isoformat(timespec=""milliseconds"") + ""Z""

        if self._json:
            payload = event
            if not isinstance(event, dict):
                payload = {""message"": str(event)}
            payload = {""timestamp"": ts, **payload}
            try:
                return _json.dumps(payload, ensure_ascii=False, sort_keys=True, default=str)
            except Exception:
                # Fallback to string if serialization fails
                return _json.dumps({""timestamp"": ts, ""message"": str(event)}, ensure_ascii=False)
        else:
            if isinstance(event, dict):
                # Stable key order
                parts = []
                for k in sorted(event.keys(), key=lambda x: str(x)):
                    v = event[k]
                    try:
                        s = str(v)
                    except Exception:
                        s = repr(v)
                    parts.append(f""{k}={s}"")
                body = "" "".join(parts)
            elif isinstance(event, BaseException):
                body = f""{event.__class__.__name__}: {event}""
            else:
                body = str(event)
            return f""{ts} {body}""

    def send(self, event):
        line = self._formatter(event)
        if not line.endswith(""\n""):
            line = line + ""\n""
        with self._lock:
            self._stream.write(line)
            if self._flush:
                try:
                    self._stream.flush()
                except Exception:
                    pass

    def __del__(self):
        try:
            if self._owns_stream and self._stream:
                try:
                    self._stream.flush()
                except Exception:
                    pass
                try:
                    self._stream.close()
                except Exception:
                    pass
        except Exception:
            pass"
303020,UpCloudLtd/upcloud-python-api,UpCloudLtd_upcloud-python-api/upcloud_api/cloud_manager/host_mixin.py,upcloud_api.cloud_manager.host_mixin.HostManager,"from upcloud_api.api import API
from upcloud_api.host import Host

class HostManager:
    """"""
    Functions for managing hosts. Intended to be used as a mixin for CloudManager.
    """"""
    api: API

    def get_hosts(self):
        """"""
        Returns a list of available hosts, along with basic statistics of them when available.
        """"""
        url = '/host'
        res = self.api.get_request(url)
        return [Host(**host) for host in res['hosts']['host']]

    def get_host(self, id: str) -> Host:
        """"""
        Returns detailed information about a specific host.
        """"""
        url = f'/host/{id}'
        res = self.api.get_request(url)
        return Host(**res['host'])

    def modify_host(self, host: str, description: str) -> Host:
        """"""
        Modifies description of a specific host.
        """"""
        url = f'/host/{host}'
        body = {'host': {'description': description}}
        res = self.api.patch_request(url, body)
        return Host(**res['host'])","
class HostManager:
    
    def get_hosts(self):
        
        pass
    def get_hosts(self):
        
        pass

    def modify_host(self, host: str, description: str) -> Host:
        
        pass",snippet_224,4,"from dataclasses import dataclass
from typing import Dict, List, Optional
from threading import RLock


@dataclass(frozen=True)
class Host:
    name: str
    description: str = """"


class HostManager:
    def __init__(self, initial_hosts: Optional[List[Host]] = None):
        self._hosts: Dict[str, Host] = {}
        self._lock = RLock()
        if initial_hosts:
            for h in initial_hosts:
                if not h.name:
                    continue
                self._hosts[h.name] = h

    def get_hosts(self) -> List[Host]:
        with self._lock:
            return sorted(self._hosts.values(), key=lambda h: h.name)

    def modify_host(self, host: str, description: str) -> Host:
        if host is None:
            raise ValueError(""host cannot be None"")
        host_name = host.strip()
        if not host_name:
            raise ValueError(""host cannot be empty or whitespace"")
        desc = description if description is not None else """"
        with self._lock:
            new_host = Host(name=host_name, description=desc)
            self._hosts[host_name] = new_host
            return new_host"
304970,threeML/astromodels,threeML_astromodels/astromodels/functions/template_model.py,astromodels.functions.template_model.TemplateFile,"import numpy as np
import h5py
from dataclasses import dataclass
import collections
from typing import Dict, List, Optional, Union

@dataclass
class TemplateFile:
    """"""
    simple container to read and write
    the data to an hdf5 file

    """"""
    name: str
    description: str
    grid: np.ndarray
    parameters: Dict[str, np.ndarray]
    parameter_order: List[str]
    energies: np.ndarray
    interpolation_degree: int
    spline_smoothing_factor: float

    def save(self, file_name: str):
        """"""
        serialize the contents to a file

        :param file_name:
        :type file_name: str
        :returns:

        """"""
        with h5py.File(file_name, 'w') as f:
            f.attrs['name'] = self.name
            f.attrs['description'] = self.description
            f.attrs['interpolation_degree'] = self.interpolation_degree
            f.attrs['spline_smoothing_factor'] = self.spline_smoothing_factor
            f.create_dataset('energies', data=self.energies, compression='gzip')
            f.create_dataset('grid', data=self.grid, compression='gzip')
            dt = h5py.special_dtype(vlen=str)
            po = np.array(self.parameter_order, dtype=dt)
            f.create_dataset('parameter_order', data=po)
            par_group = f.create_group('parameters')
            for k in self.parameter_order:
                par_group.create_dataset(k, data=self.parameters[k], compression='gzip')

    @classmethod
    def from_file(cls, file_name: str):
        """"""
        read contents from a file

        :param cls:
        :type cls:
        :param file_name:
        :type file_name: str
        :returns:

        """"""
        with h5py.File(file_name, 'r') as f:
            name = f.attrs['name']
            description = f.attrs['description']
            interpolation_degree = f.attrs['interpolation_degree']
            spline_smoothing_factor = f.attrs['spline_smoothing_factor']
            energies = f['energies'][()]
            parameter_order = f['parameter_order'][()]
            grid = f['grid'][()]
            parameters = collections.OrderedDict()
            for k in parameter_order:
                parameters[k] = f['parameters'][k][()]
        return cls(name=name, description=description, interpolation_degree=interpolation_degree, spline_smoothing_factor=spline_smoothing_factor, energies=energies, parameter_order=parameter_order, parameters=parameters, grid=grid)","@dataclass
class TemplateFile:
    
    def save(self, file_name: str):
        
        pass
    @classmethod
    def from_file(cls, file_name: str):
        
        pass",snippet_227,3,"from __future__ import annotations
from dataclasses import dataclass, field
from typing import Any, Dict
import json
from pathlib import Path

@dataclass
class TemplateFile:
    content: str = """"
    context: Dict[str, Any] = field(default_factory=dict)

    def save(self, file_name: str):
        path = Path(file_name)
        path.parent.mkdir(parents=True, exist_ok=True)
        data = {
            ""content"": self.content,
            ""context"": self.context,
        }
        with path.open(""w"", encoding=""utf-8"") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    @classmethod
    def from_file(cls, file_name: str):
        path = Path(file_name)
        with path.open(""r"", encoding=""utf-8"") as f:
            data = json.load(f)
        return cls(
            content=data.get(""content"", """"),
            context=data.get(""context"", {}),
        )"
305605,NASA-AMMOS/AIT-Core,ait/core/tlm.py,core.tlm.PacketContext,"class PacketContext:
    """"""PacketContext

    A PacketContext provides a simple wrapper around a Packet so that
    field accesses of the form:

        packet.fieldname

    may also be specified as:

        packet[fieldname]

    This latter syntax allows a PacketContext to be used as a symbol
    table when evaluating PacketExpressions.
    """"""
    __slots__ = ['_packet']

    def __init__(self, packet):
        """"""Creates a new PacketContext for the given Packet.""""""
        self._packet = packet

    def __getitem__(self, name):
        """"""Returns packet[name]""""""
        result = None
        if self._packet._hasattr(name):
            result = self._packet._getattr(name)
        else:
            msg = ""Packet '%s' has no field '%s'""
            values = (self._packet._defn.name, name)
            raise KeyError(msg % values)
        return result","class PacketContext:
    
    def __init__(self, packet):
        
        pass
    def __getitem__(self, name):
        
        pass",snippet_229,3,"class PacketContext:
    
    def __init__(self, packet):
        self.packet = packet

    def __getitem__(self, name):
        value = self.packet
        parts = name.split(""."") if isinstance(name, str) and name else [name]
        for part in parts:
            value = self._resolve(value, part)
        return value

    def _resolve(self, obj, key):
        if obj is None:
            raise KeyError(key)

        # numeric index if applicable
        idx = None
        if isinstance(key, str) and key.isdigit():
            idx = int(key)

        # mapping access
        if isinstance(obj, dict):
            if key in obj:
                return obj[key]
            if idx is not None and idx in obj:
                return obj[idx]

        # sequence/indexed access
        if idx is not None:
            try:
                return obj[idx]
            except Exception:
                pass

        # attribute access
        if isinstance(key, str) and hasattr(obj, key):
            return getattr(obj, key)

        # fallback to generic __getitem__
        try:
            return obj[key]
        except Exception:
            pass

        raise KeyError(key)"
305610,NASA-AMMOS/AIT-Core,ait/core/tlm.py,core.tlm.RawPacket,"class RawPacket:
    """"""RawPacket

    Wraps a packet such that:

        packet.raw.fieldname

    returns the value of fieldname as a raw value with no enumeration
    substitutions or DN to EU conversions applied.
    """"""
    __slots__ = ['_packet']

    def __init__(self, packet):
        """"""Creates a new RawPacket based on the given Packet.""""""
        self._packet = packet

    def __getattr__(self, fieldname):
        """"""Returns the value of the given packet fieldname as a raw
        value with no DN to EU conversion applied.
        """"""
        return self._packet._getattr(fieldname, raw=True)","class RawPacket:
    
    def __init__(self, packet):
        
        pass
    def __getattr__(self, fieldname):
        
        pass",snippet_230,3,"class RawPacket:
    
    def __init__(self, packet):
        self._packet = packet

    def __getattr__(self, fieldname):
        try:
            value = self._packet[fieldname]
        except Exception as e:
            raise AttributeError(f""{self.__class__.__name__!s} has no attribute {fieldname!r}"") from e
        return self._wrap(value)

    def _wrap(self, value):
        if isinstance(value, dict):
            return RawPacket(value)
        if isinstance(value, list):
            return [self._wrap(v) for v in value]
        return value"
305613,NASA-AMMOS/AIT-Core,ait/core/tlm.py,core.tlm.WordArray,"class WordArray:
    """"""WordArrays are somewhat analogous to Python bytearrays, but
    currently much more limited in functionality.  They provide a
    readonly view of a bytearray addressable and iterable as a sequence
    of 16-bit words.  This is convenient for telemetry processing as
    packets are often more naturally addressable on word, as opposed to
    byte, boundaries.
    """"""
    __slots__ = ['_bytes']

    def __init__(self, bytes):
        """"""Creates a new wordarray from the given bytearray.

        The given bytearray should contain an even number of bytes.  If
        odd, the last byte is ignored.
        """"""
        self._bytes = bytes

    def __getitem__(self, key):
        """"""Returns the words in this wordarray at the given Python slice
        or word at the given integer index.""""""
        length = len(self)
        if isinstance(key, slice):
            return [self[n] for n in range(*key.indices(length))]
        elif isinstance(key, int):
            if key < 0:
                key += length
            if key >= length:
                msg = 'wordarray index (%d) is out of range [0 %d].'
                raise IndexError(msg % (key, length - 1))
            index = 2 * key
            return self._bytes[index] << 8 | self._bytes[index + 1]
        else:
            raise TypeError('wordarray indices must be integers.')

    def __len__(self):
        """"""Returns the number of words in this wordarray.""""""
        return len(self._bytes) / 2","class WordArray:
    
    def __init__(self, bytes):
        
        pass
    def __getitem__(self, key):
        
        pass
    def __len__(self):
        
        pass",snippet_231,4,"class WordArray:
    
    def __init__(self, bytes):
        if isinstance(bytes, (bytearray, memoryview)):
            data = bytes.tobytes() if hasattr(bytes, ""tobytes"") else bytes.__bytes__()
        elif isinstance(bytes, (bytes,)):
            data = bytes
        elif isinstance(bytes, (list, tuple)) and all(isinstance(b, int) and 0 <= b <= 255 for b in bytes):
            data = bytes.__class__(bytes)
            data = bytes(data) if not isinstance(data, (bytes,)) else data
        else:
            raise TypeError(""Expected bytes-like object or sequence of integers in range 0..255"")
        self._bytes = data
        pad_len = (-len(self._bytes)) % 4
        self._padded = self._bytes + b""\x00"" * pad_len if pad_len else self._bytes
        self._words = [
            int.from_bytes(self._padded[i:i+4], ""big"")
            for i in range(0, len(self._padded), 4)
        ]

    def __getitem__(self, key):
        if isinstance(key, slice):
            words = self._words[key]
            # Reconstruct bytes from selected words
            b = b"""".join(w.to_bytes(4, ""big"") for w in words)
            return WordArray(b)
        if isinstance(key, int):
            return self._words[key]
        raise TypeError(""Invalid argument type"")

    def __len__(self):
        return len(self._words)"
306020,ihmeuw/vivarium,ihmeuw_vivarium/src/vivarium/framework/lookup/interpolation.py,vivarium.framework.lookup.interpolation.Order0Interp,"import numpy as np
import pandas as pd
from collections.abc import Hashable, Sequence

class Order0Interp:
    """"""A callable that returns the result of order 0 interpolation over input data.

    Attributes
    ----------
    data
        The data from which to build the interpolation.
    value_columns
        Columns to be interpolated.
    extrapolate
        Whether or not to extrapolate beyond the edge of supplied bins.
    parameter_bins
        A dictionary where they keys are a tuple of the form
        (column name used in call, column name for left bin edge, column name for right bin edge)
        and the values are dictionaries of the form {""bins"": [ordered left edges of bins],
        ""max"": max right edge (used when extrapolation not allowed)}.

    """"""

    def __init__(self, data: pd.DataFrame, continuous_parameters: Sequence[Sequence[str]], value_columns: list[str], extrapolate: bool, validate: bool):
        """"""
        Parameters
        ----------
        data
            Data frame used to build interpolation.
        continuous_parameters
            Parameter columns. Should be of form (column name used in call,
            column name for left bin edge, column name for right bin edge)
            or column name. Assumes left bin edges are inclusive and
            right exclusive.
        value_columns
            Columns to be interpolated.
        extrapolate
            Whether or not to extrapolate beyond the edge of supplied bins.
        validate
            Whether or not to validate the data.
        """"""
        if validate:
            check_data_complete(data, continuous_parameters)
        self.data = data.copy()
        self.value_columns = value_columns
        self.extrapolate = extrapolate
        self.parameter_bins = {}
        for p in continuous_parameters:
            left_edge = self.data[p[1]].drop_duplicates().sort_values()
            max_right = self.data[p[2]].drop_duplicates().max()
            self.parameter_bins[tuple(p)] = {'bins': left_edge.reset_index(drop=True), 'max': max_right}

    def __call__(self, interpolants: pd.DataFrame) -> pd.DataFrame:
        """"""Find the bins for each parameter for each interpolant in interpolants
        and return the values from data there.

        Parameters
        ----------
        interpolants
            Data frame containing the parameters to interpolate..

        Returns
        -------
            A table with the interpolated values for the given interpolants.
        """"""
        interpolant_bins = pd.DataFrame(index=interpolants.index)
        merge_cols = []
        for cols, d in self.parameter_bins.items():
            bins = d['bins']
            max_right = d['max']
            merge_cols.append(cols[1])
            interpolant_col = interpolants[cols[0]]
            if not self.extrapolate and (interpolant_col.min() < bins[0] or interpolant_col.max() >= max_right):
                raise ValueError(f'Extrapolation outside of bins used to set up interpolation is only allowed when explicitly set in creation of Interpolation. Extrapolation is currently off for this interpolation, and parameter {cols[0]} includes data outside of original bins.')
            bin_indices = np.digitize(interpolant_col, bins.tolist())
            bin_indices[bin_indices > 0] -= 1
            interpolant_bins[cols[1]] = bins.loc[bin_indices].values
        index = interpolant_bins.index
        interp_vals = interpolant_bins.merge(self.data, how='left', on=merge_cols).set_index(index)
        return interp_vals[self.value_columns]","
class Order0Interp:
    
    def __init__(self, data: pd.DataFrame, continuous_parameters: Sequence[Sequence[str]], value_columns: list[str], extrapolate: bool, validate: bool):
        
        pass
    def __call__(self, interpolants: pd.DataFrame) -> pd.DataFrame:
        
        pass",snippet_232,3,"import numpy as np
import pandas as pd
from typing import Sequence


class Order0Interp:
    
    def __init__(self, data: pd.DataFrame, continuous_parameters: Sequence[Sequence[str]], value_columns: list[str], extrapolate: bool, validate: bool):
        # Flatten and deduplicate parameter names in order
        flat_params: list[str] = []
        for group in continuous_parameters:
            for col in group:
                if col not in flat_params:
                    flat_params.append(col)
        self.param_cols = flat_params
        self.value_cols = list(value_columns)
        self.extrapolate = bool(extrapolate)

        # Basic validations
        missing_params = [c for c in self.param_cols if c not in data.columns]
        if missing_params:
            raise ValueError(f""Missing parameter columns in data: {missing_params}"")
        missing_values = [c for c in self.value_cols if c not in data.columns]
        if missing_values:
            raise ValueError(f""Missing value columns in data: {missing_values}"")
        if len(self.param_cols) == 0:
            raise ValueError(""At least one parameter column is required."")

        df = data[self.param_cols + self.value_cols].copy()

        if validate:
            if df[self.param_cols].isna().any().any():
                raise ValueError(""NaNs found in parameter columns."")
            if df[self.value_cols].isna().any().any():
                # Allow NaNs in values but warn by raising if strict validate
                raise ValueError(""NaNs found in value columns."")

        # Aggregate duplicate parameter rows by averaging value columns
        if df.duplicated(subset=self.param_cols).any():
            df = df.groupby(self.param_cols, as_index=False, dropna=False)[self.value_cols].mean()

        # Cache arrays
        self._X = df[self.param_cols].to_numpy(dtype=float, copy=True)
        self._Y = df[self.value_cols].to_numpy(dtype=float, copy=True)

        # Bounds for extrapolation handling
        self._mins = np.nanmin(self._X, axis=0)
        self._maxs = np.nanmax(self._X, axis=0)

        # Precompute norms for distance calculations
        self._X_sq = np.einsum(""ij,ij->i"", self._X, self._X)

    def __call__(self, interpolants: pd.DataFrame) -> pd.DataFrame:
        if not isinstance(interpolants, pd.DataFrame):
            raise TypeError(""interpolants must be a pandas DataFrame."")

        missing = [c for c in self.param_cols if c not in interpolants.columns]
        if missing:
            raise ValueError(f""Missing parameter columns in interpolants: {missing}"")

        Q = interpolants[self.param_cols].to_numpy(dtype=float, copy=False)
        m = Q.shape[0]
        n = self._X.shape[0]

        # Identify out-of-bounds rows if not extrapolating
        if self.extrapolate:
            valid_mask = np.ones(m, dtype=bool)
        else:
            valid_mask = (Q >= self._mins).all(axis=1) & (Q <= self._maxs).all(axis=1)

        # Prepare output
        out = np.full((m, self._Y.shape[1]), np.nan, dtype=float)

        if n == 0 or m == 0:
            return pd.DataFrame(out, index=interpolants.index, columns=self.value_cols)

        # Chunked nearest neighbor search to limit memory
        # target elements in distance matrix ~ 1e7 (80MB for float64)
        target_elems = 10_000_000
        chunk = max(1, int(target_elems // max(1, n)))

        X = self._X
        X_sq = self._X_sq
        Y = self._Y

        idxs = np.arange(m)[valid_mask]
        for start in range(0, idxs.size, chunk):
            sl = idxs[start:start + chunk]
            Q_chunk = Q[sl]

            # Compute squared distances: ||q||^2 + ||x||^2 - 2 q.x
            Q_sq = np.einsum(""ij,ij->i"", Q_chunk, Q_chunk)
            # Q_chunk @ X.T shape (k, n)
            cross = Q_chunk @ X.T
            d2 = Q_sq[:, None] + X_sq[None, :] - 2.0 * cross

            nn_idx = np.argmin(d2, axis=1)
            out[sl] = Y[nn_idx]

        return pd.DataFrame(out, index=interpolants.index, columns=self.value_cols)"
306151,lago-project/lago,lago-project_lago/lago/templates.py,lago.templates.Template,"class Template:
    """"""
    Disk image template class

    Attributes:
        name (str): Name of this template
        _versions (dict(str:TemplateVersion)): versions for this template
    """"""

    def __init__(self, name, versions):
        """"""
        Args:
            name (str): Name of the template
            versions (dict(str:TemplateVersion)): dictionary with the
                version_name: :class:`TemplateVersion` pairs for this template
        """"""
        self.name = name
        self._versions = versions

    def get_version(self, ver_name=None):
        """"""
        Get the given version for this template, or the latest

        Args:
            ver_name (str or None): Version to retieve, None for the latest

        Returns:
            TemplateVersion: The version matching the given name or the latest
                one
        """"""
        if ver_name is None:
            return self.get_latest_version()
        return self._versions[ver_name]

    def get_latest_version(self):
        """"""
        Retrieves the latest version for this template, the latest being the
        one with the newest timestamp

        Returns:
            TemplateVersion
        """"""
        return max(self._versions.values(), key=lambda x: x.timestamp())","class Template:
    
    def __init__(self, name, versions):
        
        pass
    def get_version(self, ver_name=None):
        
        pass
    def get_latest_version(self):
        
        pass",snippet_233,4,"class Template:
    
    def __init__(self, name, versions):
        self.name = name
        if versions is None:
            raise ValueError(""versions must not be None"")
        if isinstance(versions, dict):
            if not versions:
                raise ValueError(""versions dict must not be empty"")
            self._versions = dict(versions)
        else:
            try:
                iterable = list(versions)
            except TypeError:
                raise TypeError(""versions must be a dict or an iterable of version names"")
            if not iterable:
                raise ValueError(""versions iterable must not be empty"")
            self._versions = {str(v): v for v in iterable}
        self._latest_cache = None

    def _version_key(self, ver_name):
        import re
        if not isinstance(ver_name, str):
            ver_name = str(ver_name)
        parts = []
        for token in ver_name.split('.'):
            for chunk in re.findall(r'\d+|[A-Za-z]+|[^A-Za-z\d]+', token):
                if chunk.isdigit():
                    parts.append((0, int(chunk)))
                elif chunk.isalpha():
                    parts.append((1, chunk.lower()))
                else:
                    parts.append((2, chunk))
        return tuple(parts)

    def get_version(self, ver_name=None):
        if ver_name is None:
            key = self.get_latest_version()
            return self._versions[key]
        if ver_name in self._versions:
            return self._versions[ver_name]
        # allow stringified lookup if original key types were non-strings
        sver = str(ver_name)
        if sver in self._versions:
            return self._versions[sver]
        raise KeyError(f""Version '{ver_name}' not found"")

    def get_latest_version(self):
        if self._latest_cache is not None:
            return self._latest_cache
        # Determine latest by comparing version-like keys
        keys = list(self._versions.keys())
        latest = max(keys, key=self._version_key)
        self._latest_cache = latest
        return latest"
311056,geronimp/graftM,geronimp_graftM/graftm/deduplicator.py,graftm.deduplicator.Deduplicator,"class Deduplicator:
    """"""Deduplicates sequences""""""

    def deduplicate(self, aligned_sequence_objects):
        """"""Sort the given aligned_sequence objects into an array of arrays,
        where input sequences are grouped iff they have the same sequence

        Parameters
        ----------
        aligned_sequence_objects: array of Sequence objects
            input sequences

        Returns
        -------
        Array of arrays of Sequence objects""""""
        sequence_to_groups = {}
        for s in aligned_sequence_objects:
            try:
                sequence_to_groups[s.seq].append(s)
            except KeyError:
                sequence_to_groups[s.seq] = [s]
        return list(sequence_to_groups.values())

    def lca_taxonomy(self, deduplicated_sequences, taxonomy_hash):
        """"""Given a set of deduplicated sequences and a taxonomy hash,
        return the respective LCAs of taxonomy

        Parameters
        ----------
        deduplicated_sequences: Array of arrays of Sequence objects
            as output from deduplicate()
        taxonomy_hash: dictionary 
            of sequence names to taxonomy array (i.e. array of str)

        Returns
        -------
        Array of taxonomy LCAs""""""
        to_return = []
        for dup_group in deduplicated_sequences:
            lca = taxonomy_hash[dup_group[0].name]
            for s in dup_group[1:]:
                for i, tax in enumerate(taxonomy_hash[s.name]):
                    if i >= len(lca) or tax != lca[i]:
                        lca = lca[:i]
                        break
                if len(lca) > len(taxonomy_hash[s.name]):
                    lca = lca[:len(taxonomy_hash[s.name])]
            to_return.append(lca)
        return to_return","class Deduplicator:
    
    def deduplicate(self, aligned_sequence_objects):
        
        pass
    def lca_taxonomy(self, deduplicated_sequences, taxonomy_hash):
        
        pass",snippet_235,3,"class Deduplicator:
    def _extract_sequence(self, obj):
        # Try to extract a sequence string from various possible object shapes
        if obj is None:
            return None

        # If the object itself is a string
        if isinstance(obj, str):
            return obj

        # If the object is a dict-like
        if isinstance(obj, dict):
            for k in (""sequence"", ""seq"", ""aligned_sequence"", ""aligned_seq"", ""dna"", ""rna""):
                if k in obj and isinstance(obj[k], str):
                    return obj[k]
            # Sometimes the first value might be the sequence if dict is ordered
            for v in obj.values():
                if isinstance(v, str):
                    return v
            return None

        # If the object is a tuple/list, try first string-like element
        if isinstance(obj, (list, tuple)):
            for v in obj:
                if isinstance(v, str):
                    return v
            return None

        # If the object has attributes that could contain sequence
        for attr in (""sequence"", ""seq"", ""aligned_sequence"", ""aligned_seq"", ""dna"", ""rna""):
            if hasattr(obj, attr):
                val = getattr(obj, attr)
                if isinstance(val, str):
                    return val

        # Fallback: string representation if it seems sequence-like
        s = str(obj)
        return s if isinstance(s, str) else None

    def _normalize_sequence(self, seq):
        if seq is None:
            return None
        # Remove common alignment gap chars and whitespace, upper-case for uniformity
        # Keep only IUPAC letters and '*' (stop) optionally, remove others
        s = ''.join(ch for ch in seq if ch not in {'-', '.', ' ', '\t', '\n', '\r'})
        return s.upper()

    def _extract_identifier_candidates(self, obj, raw_seq=None, norm_seq=None):
        # Try to derive identifiers to look up taxonomy from taxonomy_hash
        candidates = []
        if norm_seq:
            candidates.append(norm_seq)
        if raw_seq:
            candidates.append(raw_seq)

        if obj is None:
            return candidates

        # From dict-like
        if isinstance(obj, dict):
            for k in (""id"", ""identifier"", ""name"", ""header"", ""accession"", ""taxon"", ""taxid""):
                v = obj.get(k)
                if isinstance(v, str) and v:
                    candidates.append(v)
            return candidates

        # From attributes
        for attr in (""id"", ""identifier"", ""name"", ""header"", ""accession"", ""taxon"", ""taxid""):
            if hasattr(obj, attr):
                v = getattr(obj, attr)
                if isinstance(v, str) and v:
                    candidates.append(v)

        return candidates

    def _split_lineage(self, lineage):
        if lineage is None:
            return []
        if not isinstance(lineage, str):
            lineage = str(lineage)
        # Try common separators
        for sep in (';', '|', '\t', ','):
            if sep in lineage:
                parts = [p.strip() for p in lineage.split(sep)]
                return [p for p in parts if p]
        return [lineage.strip()] if lineage.strip() else []

    def _join_lineage(self, parts):
        return '; '.join(parts)

    def _lca_of_lineages(self, lineages):
        if not lineages:
            return """"
        split = [self._split_lineage(l) for l in lineages if l is not None]
        if not split:
            return """"
        # Find common prefix across all lists
        min_len = min(len(s) for s in split)
        lca = []
        for i in range(min_len):
            token = split[0][i]
            if all(s[i] == token for s in split[1:]):
                lca.append(token)
            else:
                break
        return self._join_lineage(lca)

    def deduplicate(self, aligned_sequence_objects):
        groups = {}
        order = 0
        for obj in aligned_sequence_objects or []:
            raw_seq = self._extract_sequence(obj)
            norm = self._normalize_sequence(raw_seq)
            if not norm:
                continue
            if norm not in groups:
                groups[norm] = {
                    ""sequence"": norm,
                    ""count"": 0,
                    ""members"": [],
                    ""_first_index"": order,
                }
            groups[norm][""count""] += 1
            groups[norm][""members""].append(obj)
            order += 1
        # Return an ordered list by first occurrence for stability, plus mapping
        # But to keep it simple and broadly useful, return the dict
        # Users can sort by ['_first_index'] if needed
        # Strip helper keys for cleanliness
        for g in groups.values():
            g.pop(""_first_index"", None)
        return groups

    def lca_taxonomy(self, deduplicated_sequences, taxonomy_hash):
        if not isinstance(deduplicated_sequences, dict):
            # Try to coerce if a list of sequences was provided
            deduplicated_sequences = self.deduplicate(deduplicated_sequences)

        result = {}
        for norm_seq, info in deduplicated_sequences.items():
            members = info.get(""members"", [])
            lineages = []
            # Try per-member identifiers first
            for m in members:
                raw_seq = self._extract_sequence(m)
                candidates = self._extract_identifier_candidates(m, raw_seq=raw_seq, norm_seq=norm_seq)
                found = None
                for key in candidates:
                    if key in taxonomy_hash:
                        found = taxonomy_hash[key]
                        break
                if found is not None:
                    lineages.append(found)
            # Fallback: sequence key itself
            if not lineages and norm_seq in taxonomy_hash:
                lineages.append(taxonomy_hash[norm_seq])

            lca = self._lca_of_lineages(lineages)
            result[norm_seq] = {
                ""lca"": lca,
                ""count"": info.get(""count"", 0),
                ""members"": members,
            }
        return result"
311087,geronimp/graftM,geronimp_graftM/graftm/search_table.py,graftm.search_table.SearchTableWriter,"import logging
from graftm.sequence_search_results import SequenceSearchResult

class SearchTableWriter:
    """"""
    Class for writing the search output OTU table. Basically a summary
    of hits to the HMM/Diamond searched in the following format:

             #ID    Metagenome_1    Metagenome_2    ...
            HMM1    50              6
            HMM2    195             41
            HMM3    2               20120
            ...

    You just need to specify a series of SequenceSearchResult objects, and an
    output path.
    """"""

    def _interpret_hits(self, results_list, base_list):
        """"""Sort reads that hit multiple HMMs to the databases to which they had
        the highest bit score. Return a dictionary containing HMMs as keys, and
        number of hits as the values.

        This function is set up so that the read names could easily be returned
        instead of numbers, for future development of GraftM

        Parameters
        ----------
        results_list: list
            Iterable if SequenceSearchResult objects. e.g.
                [SequenceSearchResult_1, SequenceSearchResult_2, ...]

        base_list: list
            Iterable of the basenames for each sequence file provided to graftM
            e.g.
                [sample_1, sample_2, ...]

        Returns
        -------
        dictionary:
            Contains samples as entries. The value for each sample is another
            dictionary with HMM as the key, and number of hits as values:
                {""sample_1"":{HMM_1: 12
                             HMM_2: 35
                             HMM_3: 1258
                             ...}
                 ...
                }

        """"""
        logging.debug('Sorting reads into HMMs by bit score')
        run_results = {}
        for base, results in zip(base_list, results_list):
            search_results = {}
            for search in results():
                search_list = list(search.each([SequenceSearchResult.QUERY_ID_FIELD, SequenceSearchResult.ALIGNMENT_BIT_SCORE, SequenceSearchResult.HMM_NAME_FIELD]))
                for hit in search_list:
                    if hit[0] in search_results:
                        if float(hit[1]) > search_results[hit[0]][0]:
                            search_results[hit[0]] = [float(hit[1]), hit[2]]
                    else:
                        search_results[hit[0]] = [float(hit[1]), hit[2]]
            run_results[base] = search_results
        db_count = {}
        for run in run_results.keys():
            run_count = {}
            for entry in list(run_results[run].values()):
                key = entry[1]
                if key in run_count:
                    run_count[key] += 1
                else:
                    run_count[key] = 1
            db_count[run] = run_count
        return db_count

    def _write_results(self, db_count, output_path):
        """"""Write the table to the output_path directory

        db_count: dict
            Contains samples as entries. The value for each sample is another
            dictionary with HMM as the key, and number of hits as values:
                {""sample_1"":{HMM_1: 12
                             HMM_2: 35
                             HMM_3: 1258
                             ...}
                 ...
                }

        output_path: str
            Path to output file to which the resultant output file will be
            written to.
        """"""
        logging.debug('Writing search otu table to file: %s' % output_path)
        output_dict = {}
        for idx, value_dict in enumerate(db_count.values()):
            for database, count in value_dict.items():
                if database in output_dict:
                    output_dict[database].append(str(count))
                else:
                    output_dict[database] = ['0'] * idx + [str(count)]
            for key, item in output_dict.items():
                if len(item) == idx:
                    output_dict[key].append('0')
        with open(output_path, 'w') as out:
            out.write('\t'.join(['#ID'] + list(db_count.keys())) + '\n')
            for key, item in output_dict.items():
                out.write('%s\t%s' % (key, '\t'.join(item)) + '\n')

    def build_search_otu_table(self, search_results_list, base_list, output_path):
        """"""
        Build an OTU from SequenceSearchResult objects

        Parameters
        ----------
        search_results_list: list
            Iterable if SequenceSearchResult objects. e.g.
                [SequenceSearchResult_1, SequenceSearchResult_2, ...]
        base_list: list
            Iterable of the basenames for each sequence file provided to graftM
            e.g.
                [sample_1, sample_2, ...]
        output_path: str
            Path to output file to which the resultant output file will be
            written to.
        """"""
        db_count = self._interpret_hits(search_results_list, base_list)
        self._write_results(db_count, output_path)","
class SearchTableWriter:
    
    def _interpret_hits(self, results_list, base_list):
        
        pass
    def _write_results(self, db_count, output_path):
        
        pass

    def build_search_otu_table(self, search_results_list, base_list, output_path):
        
        pass",snippet_236,4,"class SearchTableWriter:
    def __init__(self):
        self._counts_per_base = None
        self._base_list = None

    def _extract_id(self, item):
        if item is None:
            return None
        # Direct types
        if isinstance(item, (str, int)):
            return str(item)
        # Dict-like
        if isinstance(item, dict):
            for key in (""id"", ""otu"", ""name"", ""identifier"", ""taxon"", ""tax_id"", ""taxid""):
                if key in item and item[key] is not None:
                    return str(item[key])
            # Fallback: first non-null value
            for v in item.values():
                if v is not None:
                    return str(v)
            return None
        # Tuple/list-like
        if isinstance(item, (list, tuple)):
            for v in item:
                if v is not None:
                    return str(v)
            return None
        # Fallback to string repr
        try:
            return str(item)
        except Exception:
            return None

    def _interpret_hits(self, results_list, base_list):
        if base_list is None:
            raise ValueError(""base_list is required"")
        if results_list is None:
            raise ValueError(""results_list is required"")
        base_str_list = [str(b) for b in base_list]
        base_set = set(base_str_list)
        num_dbs = len(results_list)
        counts_per_base = {b: [0] * num_dbs for b in base_str_list}

        for col_idx, db_results in enumerate(results_list):
            if db_results is None:
                continue
            # Normalize to iterable
            try:
                iterator = iter(db_results)
            except TypeError:
                iterator = iter([db_results])

            local_counts = {}
            for item in iterator:
                hit_id = self._extract_id(item)
                if hit_id is None:
                    continue
                if hit_id in base_set:
                    local_counts[hit_id] = local_counts.get(hit_id, 0) + 1

            for b in base_str_list:
                if b in local_counts:
                    counts_per_base[b][col_idx] = local_counts[b]

        self._counts_per_base = counts_per_base
        self._base_list = base_str_list
        return counts_per_base

    def _write_results(self, db_count, output_path):
        if self._counts_per_base is None or self._base_list is None:
            raise RuntimeError(""No interpreted results to write. Call _interpret_hits first."")
        if not output_path:
            raise ValueError(""output_path is required"")

        # Support gzip transparently
        opener = None
        if str(output_path).endswith("".gz""):
            import gzip
            opener = lambda p, m: gzip.open(p, m, compresslevel=5)
        else:
            opener = open

        header = [""ID""] + [f""DB_{i+1}"" for i in range(db_count)]
        newline = ""\n""

        with opener(output_path, ""wt"") as fh:
            fh.write(""\t"".join(header) + newline)
            for base_id in self._base_list:
                counts = self._counts_per_base.get(base_id, [0] * db_count)
                row = [base_id] + [str(c) for c in counts]
                fh.write(""\t"".join(row) + newline)

    def build_search_otu_table(self, search_results_list, base_list, output_path):
        counts = self._interpret_hits(search_results_list, base_list)
        self._write_results(len(search_results_list), output_path)
        return counts"
311100,geronimp/graftM,geronimp_graftM/graftm/tree_decorator.py,graftm.tree_decorator.TreeDecorator,"from graftm.greengenes_taxonomy import GreenGenesTaxonomy, MalformedGreenGenesTaxonomyException
from graftm.getaxnseq import Getaxnseq
import logging
from graftm.taxonomy_cleaner import TaxonomyCleaner

class TreeDecorator:
    """"""
    A class that conservatively decorates trees with taxonomy, or any other
    hierarchical annotation. If all tips descending from a node within the
    provided tree have consistent taxonomy, it will be decorated with that
    taxonomy (or annotation of any type).
    """"""

    def __init__(self, tree, taxonomy, seqinfo=None):
        """"""
        Parameters
        ----------
        tree        : dendropy.Tree

            dendropy.Tree object
        taxonomy    : string
            Path to a file containing taxonomy information about the tree,
            either in Greengenes or taxtastic format (seqinfo file must also
            be provided if taxonomy is in taxtastic format).
        seqinfo     : string
            Path to a seqinfo file. This is a .csv file with the first column
            denoting the sequence name, and the second column, its most resolved
            taxonomic rank.
        """"""
        self.encountered_nodes = {}
        self.encountered_taxonomies = set()
        self.tree = tree
        logging.info('Reading in taxonomy')
        if seqinfo:
            logging.info('Importing taxtastic taxonomy from files: %s and %s' % (taxonomy, seqinfo))
            gtns = Getaxnseq()
            self.taxonomy = gtns.read_taxtastic_taxonomy_and_seqinfo(open(taxonomy), open(seqinfo))
        else:
            try:
                logging.info('Reading Greengenes style taxonomy')
                self.taxonomy = GreenGenesTaxonomy.read_file(taxonomy).taxonomy
            except MalformedGreenGenesTaxonomyException:
                raise Exception('Failed to read taxonomy as a Greengenes                                  formatted file. Was a taxtastic style                                  taxonomy provided with no seqinfo file?')

    def _write_consensus_strings(self, output):
        """"""
        Writes the taxonomy of each leaf to a file. If the leaf has no
        taxonomy, a taxonomy string will be created using the annotations
        provided to the ancestor nodes of that leaf (meaning, it will be
        decorated).

        Parameters
        ----------
        output    : string
            File to which the taxonomy strings for each leaf in the tree will
            be written in Greengenes format, e.g.
                637960147    mcrA; Euryarchaeota_mcrA; Methanomicrobia
                637699780    mcrA; Euryarchaeota_mcrA; Methanomicrobia
        """"""
        logging.info('Writing decorated taxonomy to file: %s' % output)
        with open(output, 'w') as out:
            for tip in self.tree.leaf_nodes():
                tax_name = tip.taxon.label.replace(' ', '_')
                if tip.taxon.label in self.taxonomy:
                    tax_string = '; '.join(self.taxonomy[tax_name])
                else:
                    ancestor_list = []
                    for ancestor in tip.ancestor_iter():
                        if ancestor.label:
                            split_node_name = ancestor.label.split(':')
                            if len(split_node_name) == 2:
                                ancestor_list += list(reversed(split_node_name[1].split('; ')))
                            elif len(split_node_name) == 1:
                                try:
                                    float(split_node_name[0])
                                except ValueError:
                                    ancestor_list += list(reversed(split_node_name[0].split('; ')))
                            else:
                                raise Exception('Malformed node name: %s' % ancestor.label)
                    tax_list = list(reversed(ancestor_list))
                    if len(tax_list) < 1:
                        logging.warning('No taxonomy found for species %s!' % tax_name)
                        tax_string = 'Unknown'
                    else:
                        tax_string = '; '.join(tax_list)
                output_line = '%s\t%s\n' % (tax_name, tax_string)
                out.write(output_line)

    def _rename(self, node, name):
        """"""
        Rename an internal node of the tree. If an annotation is already
        present, append the new annotation to the end of it. If a bootstrap
        value is present, add annotations are added after a "":"" as per standard
        newick format.

        Parameters
        ----------
        node: dendropy.Node
            dendropy.Node object
        name    : string
            Annotation to rename the node with.
        """"""
        if node.label:
            try:
                float(node.label)
                new_label = '%s:%s' % (node.label, name)
            except ValueError:
                new_label = '%s; %s' % (node.label, name)
            node.label = new_label
        else:
            node.label = name

    def decorate(self, output_tree, output_tax, unique_names):
        """"""
        Decorate a tree with taxonomy. This code does not allow inconsistent
        taxonomy within a clade. If one sequence in a clade has a different
        annotation to the rest, it will split the clade. Paraphyletic group
        names are distinguished if unique_names = True using a simple tally of
        each group (see unique_names below).

        Parameters
        ----------
        output_tree        : string
            File to which the decorated tree will be written.
        output_tax         : string
            File to which the taxonomy strings for each tip in the tree will be
            written.
        unique_names       : boolean
            True indicating that a unique number will be appended to the end of
            a taxonomic rank if it is found more than once in the tree
            (i.e. it is paraphyletic in the tree). If false, multiple clades
            may be assigned with the same name.
        """"""
        logging.info('Decorating tree')
        encountered_taxonomies = {}
        tc = TaxonomyCleaner()
        for node in self.tree.preorder_internal_node_iter(exclude_seed_node=True):
            max_tax_string_length = 0
            for tip in node.leaf_nodes():
                tip_label = tip.taxon.label.replace(' ', '_')
                if tip_label in self.taxonomy:
                    tax_string_length = len(self.taxonomy[tip.taxon.label.replace(' ', '_')])
                    if tax_string_length > max_tax_string_length:
                        max_tax_string_length = tax_string_length
            logging.debug('Number of ranks found for node: %i' % max_tax_string_length)
            tax_string_array = []
            for rank in range(max_tax_string_length):
                rank_tax = []
                for tip in node.leaf_nodes():
                    tip_label = tip.taxon.label.replace(' ', '_')
                    if tip_label in self.taxonomy:
                        tip_tax = self.taxonomy[tip_label]
                        if len(tip_tax) > rank:
                            tip_rank = tip_tax[rank]
                            if tip_rank not in rank_tax:
                                rank_tax.append(tip_rank)
                consistent_taxonomy = len(rank_tax) == 1
                if consistent_taxonomy:
                    tax = rank_tax.pop()
                    logging.debug('Consistent taxonomy found for node: %s' % tax)
                    if tax not in tc.meaningless_taxonomic_names:
                        if unique_names:
                            if tax in encountered_taxonomies:
                                encountered_taxonomies[tax] += 0
                                tax = '%s_%i' % (tax, encountered_taxonomies[tax])
                            else:
                                encountered_taxonomies[tax] = 0
                        tax_string_array.append(tax)
            if any(tax_string_array):
                index = 0
                for anc in node.ancestor_iter():
                    try:
                        index += anc.tax
                    except:
                        continue
                tax_string_array = tax_string_array[index:]
                if any(tax_string_array):
                    self._rename(node, '; '.join(tax_string_array))
                node.tax = len(tax_string_array)
        logging.info('Writing decorated tree to file: %s' % output_tree)
        if output_tree:
            self.tree.write(path=output_tree, schema='newick')
        if output_tax:
            self._write_consensus_strings(output_tax)","
class TreeDecorator:
    
    def __init__(self, tree, taxonomy, seqinfo=None):
        
        pass
    def _write_consensus_strings(self, output):
        
        pass

    def _rename(self, node, name):
        
        pass

    def decorate(self, output_tree, output_tax, unique_names):
        '''
        Decorate a tree with taxonomy. This code does not allow inconsistent
        taxonomy within a clade. If one sequence in a clade has a different
        annotation to the rest, it will split the clade. Paraphyletic group
        names are distinguished if unique_names = True using a simple tally of
        each group (see unique_names below).
        Parameters
        ----------
        output_tree        : string
            File to which the decorated tree will be written.
        output_tax         : string
            File to which the taxonomy strings for each tip in the tree will be
            written.
        unique_names       : boolean
            True indicating that a unique number will be appended to the end of
            a taxonomic rank if it is found more than once in the tree
            (i.e. it is paraphyletic in the tree). If false, multiple clades
            may be assigned with the same name.
        '''
        pass",snippet_237,5,"class TreeDecorator:
    
    class _Node:
        __slots__ = (""name"", ""children"", ""parent"")
        def __init__(self, name=None, children=None, parent=None):
            self.name = name
            self.children = children if children is not None else []
            self.parent = parent

        def is_leaf(self):
            return not self.children

    def __init__(self, tree, taxonomy, seqinfo=None):
        # tree: Newick string or already-parsed _Node
        # taxonomy: dict {tip_name: ""rank1;rank2;...""}
        self.seqinfo = seqinfo
        self.taxonomy = taxonomy if taxonomy is not None else {}
        if isinstance(tree, str):
            self.root = self._parse_newick(tree)
        elif isinstance(tree, TreeDecorator._Node):
            self.root = tree
        else:
            raise TypeError(""tree must be a Newick string or TreeDecorator._Node"")

        # cache of leaves
        self._leaves = None

    def _write_consensus_strings(self, output):
        with open(output, ""w"", encoding=""utf-8"") as fh:
            for leaf in self._get_leaves():
                tax = self.taxonomy.get(leaf.name, """")
                fh.write(f""{leaf.name}\t{tax}\n"")

    def _rename(self, node, name):
        node.name = name

    def decorate(self, output_tree, output_tax, unique_names):
        # Compute consensus taxonomy for internal nodes
        consensus_map = {}
        self._compute_consensus_for_all(self.root, consensus_map)

        # Assign names to internal nodes based on consensus
        name_counts = {}
        for node, consensus in consensus_map.items():
            if not node.is_leaf():
                # choose the deepest rank in consensus as the node label
                label = consensus.split("";"")[-1] if consensus else """"
                if label:
                    if unique_names:
                        c = name_counts.get(label, 0) + 1
                        name_counts[label] = c
                        if c > 1:
                            label = f""{label}_{c}""
                    self._rename(node, label)

        # Write tree
        with open(output_tree, ""w"", encoding=""utf-8"") as fh:
            fh.write(self._to_newick(self.root) + "";\n"")

        # Write tip taxonomy
        self._write_consensus_strings(output_tax)

    # --------- Internal helpers ----------

    def _get_leaves(self):
        if self._leaves is None:
            self._leaves = []
            self._collect_leaves(self.root, self._leaves)
        return self._leaves

    def _collect_leaves(self, node, acc):
        if node.is_leaf():
            acc.append(node)
        else:
            for ch in node.children:
                self._collect_leaves(ch, acc)

    def _compute_consensus_for_all(self, node, consensus_map):
        if node.is_leaf():
            tax = self.taxonomy.get(node.name, """")
            consensus_map[node] = tax
            return tax

        child_cons = [self._compute_consensus_for_all(ch, consensus_map) for ch in node.children]
        consensus = self._common_prefix(child_cons)
        consensus_map[node] = consensus
        return consensus

    def _common_prefix(self, tax_strings):
        # tax_strings: list of semicolon-separated strings
        if not tax_strings:
            return """"
        split_lists = [s.split("";"") if s else [] for s in tax_strings]
        # find the longest prefix where all are equal and non-empty
        min_len = min((len(l) for l in split_lists), default=0)
        prefix = []
        for i in range(min_len):
            vals = {l[i] for l in split_lists}
            if len(vals) == 1 and list(vals)[0] != """":
                prefix.append(list(vals)[0])
            else:
                break
        return "";"".join(prefix)

    def _to_newick(self, node):
        if node.is_leaf():
            return self._escape_name(node.name or """")
        else:
            kids = "","".join(self._to_newick(ch) for ch in node.children)
            label = self._escape_name(node.name) if node.name else """"
            return f""({kids}){label}""

    def _escape_name(self, name):
        if name is None:
            return """"
        # Minimal escaping: replace spaces with underscores
        return str(name).replace("" "", ""_"")

    def _parse_newick(self, newick):
        s = newick.strip()
        if not s.endswith("";""):
            # allow missing semicolon
            s = s + "";""
        idx = 0

        def parse_subtree():
            nonlocal idx
            if s[idx] == ""("":
                idx += 1  # skip '('
                children = []
                while True:
                    child = parse_subtree()
                    children.append(child)
                    if s[idx] == "","":
                        idx += 1
                        continue
                    elif s[idx] == "")"":
                        idx += 1
                        break
                    else:
                        raise ValueError(""Invalid Newick: expected ',' or ')'"")
                # optional name after ')'
                name = parse_name()
                node = TreeDecorator._Node(name=name, children=children)
                for ch in children:
                    ch.parent = node
                # optional branch length ':...' ignored
                if idx < len(s) and s[idx] == "":"":
                    skip_branch_length()
                return node
            else:
                # leaf name
                name = parse_name(mandatory=True)
                node = TreeDecorator._Node(name=name)
                # optional branch length ':...' ignored
                if idx < len(s) and s[idx] == "":"":
                    skip_branch_length()
                return node

        def parse_name(mandatory=False):
            nonlocal idx
            start = idx
            # read until comma, parenthesis, colon, or semicolon
            while idx < len(s) and s[idx] not in ["","", "")"", ""("", "":"", "";""]:
                idx += 1
            name = s[start:idx].strip()
            if mandatory and name == """":
                raise ValueError(""Invalid Newick: missing leaf name"")
            return name if name != """" else None

        def skip_branch_length():
            nonlocal idx
            if idx < len(s) and s[idx] == "":"":
                idx += 1
                while idx < len(s) and s[idx] not in ["","", "")"", "";""]:
                    idx += 1

        root = parse_subtree()
        # skip following semicolon
        while idx < len(s) and s[idx] != "";"":
            # allow trailing spaces/comments ignored
            idx += 1
        if idx < len(s) and s[idx] == "";"":
            idx += 1
        return root"
311642,abhishek-ram/pyas2-lib,abhishek-ram_pyas2-lib/pyas2lib/as2.py,pyas2lib.as2.Partner,"from oscrypto import asymmetric
from pyas2lib.exceptions import AS2Exception, DuplicateDocument, ImproperlyConfigured, InsufficientSecurityError, IntegrityError, MDNNotFound, PartnerNotFound
from pyas2lib.utils import canonicalize, extract_first_part, make_mime_boundary, mime_to_bytes, pem_to_der, quote_as2name, split_pem, unquote_as2name, verify_certificate_chain
from pyas2lib.constants import AS2_VERSION, ASYNCHRONOUS_MDN, DIGEST_ALGORITHMS, EDIINT_FEATURES, ENCRYPTION_ALGORITHMS, KEY_ENCRYPTION_ALGORITHMS, MDN_CONFIRM_TEXT, MDN_FAILED_TEXT, MDN_MODES, SIGNATUR_ALGORITHMS, SYNCHRONOUS_MDN
from dataclasses import dataclass

@dataclass
class Partner:
    """"""
    Class represents an AS2 partner and defines the certificates and
    settings to be used when sending and receiving messages.

    :param as2_name: The unique AS2 name for this partner.

    :param verify_cert: A byte string of the certificate to be used for
        verifying signatures of inbound messages and MDNs.

    :param verify_cert_ca: A byte string of the ca certificate if any of
        the verification cert

    :param encrypt_cert: A byte string of the certificate to be used for
        encrypting outbound message.

    :param encrypt_cert_ca: A byte string of the ca certificate if any of
        the encryption cert

    :param validate_certs: Set this flag to `False` to disable validations of
        the encryption and verification certificates. (default `True`)

    :param compress: Set this flag to `True` to compress outgoing
        messages. (default `False`)

    :param sign: Set this flag to `True` to sign outgoing
        messages. (default `False`)

    :param digest_alg: The digest algorithm to be used for generating the
        signature. (default ""sha256"")

    :param encrypt: Set this flag to `True` to encrypt outgoing
        messages. (default `False`)

    :param enc_alg:
        The encryption algorithm to be used. (default `""tripledes_192_cbc""`)

    :param mdn_mode: The mode to be used for receiving the MDN.
        Set to `None` for no MDN, `'SYNC'` for synchronous and `'ASYNC'`
        for asynchronous. (default `None`)

    :param mdn_digest_alg: The digest algorithm to be used by the receiver
        for signing the MDN. Use `None` for unsigned MDN. (default `None`)

    :param mdn_confirm_text: The text to be used in the MDN for successfully
        processed messages received from this partner.

    :param canonicalize_as_binary: force binary canonicalization for this partner

    :param sign_alg: The signing algorithm to be used for generating the
        signature. (default `rsassa_pkcs1v15`)

    :param key_enc_alg: The key encryption algorithm to be used.
        (default `rsaes_pkcs1v15`)

    """"""
    as2_name: str
    verify_cert: bytes = None
    verify_cert_ca: bytes = None
    encrypt_cert: bytes = None
    encrypt_cert_ca: bytes = None
    validate_certs: bool = True
    compress: bool = False
    encrypt: bool = False
    enc_alg: str = 'tripledes_192_cbc'
    sign: bool = False
    digest_alg: str = 'sha256'
    mdn_mode: str = None
    mdn_digest_alg: str = None
    mdn_confirm_text: str = MDN_CONFIRM_TEXT
    ignore_self_signed: bool = True
    canonicalize_as_binary: bool = False
    sign_alg: str = 'rsassa_pkcs1v15'
    key_enc_alg: str = 'rsaes_pkcs1v15'

    def __post_init__(self):
        """"""Run the post initialisation checks for this class.""""""
        if self.digest_alg and self.digest_alg not in DIGEST_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported Digest Algorithm {self.digest_alg}, must be one of {DIGEST_ALGORITHMS}')
        if self.enc_alg and self.enc_alg not in ENCRYPTION_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported Encryption Algorithm {self.enc_alg}, must be one of {ENCRYPTION_ALGORITHMS}')
        if self.mdn_mode and self.mdn_mode not in MDN_MODES:
            raise ImproperlyConfigured(f'Unsupported MDN Mode {self.mdn_mode}, must be one of {MDN_MODES}')
        if self.mdn_digest_alg and self.mdn_digest_alg not in DIGEST_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported MDN Digest Algorithm {self.mdn_digest_alg}, must be one of {DIGEST_ALGORITHMS}')
        if self.sign_alg and self.sign_alg not in SIGNATUR_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported Signature Algorithm {self.sign_alg}, must be one of {SIGNATUR_ALGORITHMS}')
        if self.key_enc_alg and self.key_enc_alg not in KEY_ENCRYPTION_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported Key Encryption Algorithm {self.key_enc_alg}, must be one of {KEY_ENCRYPTION_ALGORITHMS}')

    def load_verify_cert(self):
        """"""Load the verification certificate of the partner and returned the parsed cert.""""""
        if self.validate_certs:
            cert = pem_to_der(self.verify_cert, return_multiple=False)
            if self.verify_cert_ca:
                trust_roots = pem_to_der(self.verify_cert_ca)
            else:
                trust_roots = []
            verify_certificate_chain(cert, trust_roots, ignore_self_signed=self.ignore_self_signed)
        return asymmetric.load_certificate(self.verify_cert)

    def load_encrypt_cert(self):
        """"""Load the encryption certificate of the partner and returned the parsed cert.""""""
        if self.validate_certs:
            cert = pem_to_der(self.encrypt_cert, return_multiple=False)
            if self.encrypt_cert_ca:
                trust_roots = pem_to_der(self.encrypt_cert_ca)
            else:
                trust_roots = []
            verify_certificate_chain(cert, trust_roots, ignore_self_signed=self.ignore_self_signed)
        return asymmetric.load_certificate(self.encrypt_cert)","@dataclass
class Partner:
    
    def __post_init__(self):
        
        pass
    def load_verify_cert(self):
        
        pass

    def load_encrypt_cert(self):
        
        pass",snippet_239,4,"from dataclasses import dataclass, field
from typing import Optional, Union
from pathlib import Path
import base64

try:
    from cryptography import x509
    from cryptography.hazmat.backends import default_backend
except Exception as e:
    x509 = None
    default_backend = None


@dataclass
class Partner:
    verify_cert_source: Optional[Union[str, bytes, Path]] = None
    encrypt_cert_source: Optional[Union[str, bytes, Path]] = None

    verify_cert: Optional[""x509.Certificate""] = field(default=None, init=False, repr=False)
    encrypt_cert: Optional[""x509.Certificate""] = field(default=None, init=False, repr=False)

    def __post_init__(self):
        if x509 is None or default_backend is None:
            raise RuntimeError(""cryptography package is required to use Partner certificates"")

        if self.verify_cert_source is not None:
            self.load_verify_cert()

        if self.encrypt_cert_source is not None:
            self.load_encrypt_cert()

    def _read_input_bytes(self, source: Union[str, bytes, Path]) -> bytes:
        if isinstance(source, bytes):
            return source
        if isinstance(source, Path):
            return source.read_bytes()
        # str
        s = source.strip()
        p = Path(s)
        if p.exists() and p.is_file():
            return p.read_bytes()
        return s.encode(""utf-8"")

    def _load_cert(self, source: Union[str, bytes, Path]) -> ""x509.Certificate"":
        data = self._read_input_bytes(source)

        # Try PEM first
        try:
            return x509.load_pem_x509_certificate(data, default_backend())
        except Exception:
            pass

        # If it's textual without PEM header, try to base64-decode into DER
        try:
            text = data.decode(""utf-8"", errors=""ignore"").strip()
            if ""-----BEGIN"" in text and ""CERTIFICATE-----"" in text:
                # It looked like PEM but failed earlier; raise a clear error
                raise ValueError(""Invalid PEM certificate data"")
            # Remove common header/footer if present in odd formatting
            cleaned = (
                text.replace(""BEGIN CERTIFICATE"", """")
                .replace(""END CERTIFICATE"", """")
                .replace(""-----"", """")
                .replace(""\n"", """")
                .replace(""\r"", """")
                .strip()
            )
            der = base64.b64decode(cleaned, validate=False)
            if der:
                try:
                    return x509.load_der_x509_certificate(der, default_backend())
                except Exception:
                    pass
        except Exception:
            pass

        # Try raw DER directly (if bytes provided)
        try:
            return x509.load_der_x509_certificate(data, default_backend())
        except Exception:
            pass

        raise ValueError(""Unable to load X.509 certificate from provided source"")

    def load_verify_cert(self):
        if self.verify_cert_source is None:
            raise ValueError(""verify_cert_source is not set"")
        self.verify_cert = self._load_cert(self.verify_cert_source)
        return self.verify_cert

    def load_encrypt_cert(self):
        if self.encrypt_cert_source is None:
            raise ValueError(""encrypt_cert_source is not set"")
        self.encrypt_cert = self._load_cert(self.encrypt_cert_source)
        return self.encrypt_cert"
311862,reportportal/client-Python,reportportal_client-Python/reportportal_client/core/rp_issues.py,reportportal_client.core.rp_issues.ExternalIssue,"class ExternalIssue:
    """"""This class represents external(BTS) system issue.""""""

    def __init__(self, bts_url=None, bts_project=None, submit_date=None, ticket_id=None, url=None):
        """"""Initialize instance attributes.

        :param bts_url:     Bug tracker system URL
        :param bts_project: Bug tracker system project
        :param submit_date: Bug submission date
        :param ticket_id:   Unique ID of the ticket at the BTS
        :param url:         URL to the ticket(bug)
        """"""
        self.bts_url = bts_url
        self.bts_project = bts_project
        self.submit_date = submit_date
        self.ticket_id = ticket_id
        self.url = url

    @property
    def payload(self):
        """"""Form the correct dictionary for the BTS issue.""""""
        return {'btsUrl': self.bts_url, 'btsProject': self.bts_project, 'submitDate': self.submit_date, 'ticketId': self.ticket_id, 'url': self.url}","class ExternalIssue:
    
    def __init__(self, bts_url=None, bts_project=None, submit_date=None, ticket_id=None, url=None):
        
        pass
    @property
    def payload(self):
        
        pass",snippet_240,3,"class ExternalIssue:
    def __init__(self, bts_url=None, bts_project=None, submit_date=None, ticket_id=None, url=None):
        self.bts_url = bts_url
        self.bts_project = bts_project
        self._submit_date = submit_date
        self.ticket_id = ticket_id
        self.url = url

    def _normalize_submit_date(self, value):
        if value is None:
            return None
        try:
            import datetime as _dt
            if isinstance(value, _dt.datetime):
                return value.isoformat()
            if isinstance(value, _dt.date):
                return value.isoformat()
            if isinstance(value, (int, float)):
                return _dt.datetime.fromtimestamp(value, _dt.timezone.utc).isoformat()
            if isinstance(value, str):
                # If it's already a string, keep as-is
                return value
        except Exception:
            pass
        return str(value)

    @property
    def payload(self):
        data = {
            ""bts_url"": self.bts_url,
            ""bts_project"": self.bts_project,
            ""submit_date"": self._normalize_submit_date(self._submit_date),
            ""ticket_id"": self.ticket_id,
            ""url"": self.url,
        }
        return {k: v for k, v in data.items() if v is not None}"
311863,reportportal/client-Python,reportportal_client-Python/reportportal_client/core/rp_issues.py,reportportal_client.core.rp_issues.Issue,"class Issue:
    """"""This class represents an issue that can be attached to test result.""""""

    def __init__(self, issue_type, comment=None, auto_analyzed=False, ignore_analyzer=True):
        """"""Initialize instance attributes.

        :param issue_type:      Issue type locator. Allowable values: ""pb***"",
                                ""ab***"", ""si***"", ""ti***"", ""nd001"". Where ***
                                is locator id.
        :param comment:         Issue comments
        :param auto_analyzed:   Indicator that the issue has been marked with
                                the RP auto analyzer
        :param ignore_analyzer: Flag that forces RP analyzer to ignore this
                                issue
        """"""
        self._external_issues = []
        self.auto_analyzed = auto_analyzed
        self.comment = comment
        self.ignore_analyzer = ignore_analyzer
        self.issue_type = issue_type

    def external_issue_add(self, issue):
        """"""Add external system issue to the issue.""""""
        self._external_issues.append(issue.payload)

    @property
    def payload(self):
        """"""Form the correct dictionary for the issue.""""""
        return {'autoAnalyzed': self.auto_analyzed, 'comment': self.comment, 'externalSystemIssues': self._external_issues, 'ignoreAnalyzer': self.ignore_analyzer, 'issueType': self.issue_type}","class Issue:
    
    def __init__(self, issue_type, comment=None, auto_analyzed=False, ignore_analyzer=True):
        
        pass
    def external_issue_add(self, issue):
        
        pass
    @property
    def payload(self):
        
        pass",snippet_241,4,"class Issue:
    
    def __init__(self, issue_type, comment=None, auto_analyzed=False, ignore_analyzer=True):
        if not isinstance(issue_type, str) or not issue_type:
            raise ValueError(""issue_type must be a non-empty string"")
        self.issue_type = issue_type
        self.comment = comment
        self.auto_analyzed = bool(auto_analyzed)
        self.ignore_analyzer = bool(ignore_analyzer)
        self._external_issues = []

    def external_issue_add(self, issue):
        if not isinstance(issue, dict):
            raise TypeError(""issue must be a dict"")
        if not issue:
            raise ValueError(""issue cannot be empty"")
        self._external_issues.append(issue)

    @property
    def payload(self):
        data = {
            ""issueType"": self.issue_type,
            ""autoAnalyzed"": self.auto_analyzed,
            ""ignoreAnalyzer"": self.ignore_analyzer,
        }
        if self.comment is not None:
            data[""comment""] = self.comment
        if self._external_issues:
            data[""externalSystemIssues""] = list(self._external_issues)
        return data"
312030,SuperCowPowers/workbench,aws_setup/aws_identity_check.py,aws_identity_check.AWSIdentityCheck,"from workbench.core.cloud_platform.aws.aws_account_clamp import AWSAccountClamp
from workbench.utils.config_manager import ConfigManager
import sys
import logging

class AWSIdentityCheck:
    """"""Just a Utility Script that allows people to check which AWS Identity is active""""""

    def __init__(self):
        """"""AWSIdentityCheck Initialization""""""
        self.log = logging.getLogger('workbench')
        self.aws_clamp = AWSAccountClamp()

    def check(self):
        """"""Check the AWS Identity""""""
        cm = ConfigManager()
        active_profile = cm.get_config('AWS_PROFILE')
        if active_profile:
            self.log.info(f'Workbench AWS_PROFILE: {active_profile}')
        else:
            self.log.info('No AWS_PROFILE set')
            sys.exit(0)
        self.log.info('\n\n*** Caller/Base Identity Check ***')
        self.aws_clamp.check_aws_identity()
        self.log.info('Caller/Base Identity Check Success...')
        self.log.info('\n\n*** AWS Assumed Role Check ***')
        self.aws_clamp.check_assumed_role()
        self.log.info('Assumed Role Check Success...')","
class AWSIdentityCheck:
    
    def __init__(self):
        
        pass
    def check(self):
        
        pass",snippet_242,3,"class AWSIdentityCheck:
    def __init__(self, session=None, region_name=None, profile_name=None, connect_timeout=3, read_timeout=10, max_attempts=3):
        self._session = session
        self._region_name = region_name
        self._profile_name = profile_name
        self._connect_timeout = connect_timeout
        self._read_timeout = read_timeout
        self._max_attempts = max_attempts

    def check(self):
        result = {
            ""success"": False,
            ""identity"": None,
            ""error"": None,
            ""source"": None,
        }

        try:
            import boto3
            from botocore.config import Config as _BotocoreConfig
            from botocore.exceptions import NoCredentialsError, NoRegionError, BotoCoreError, ClientError
        except Exception as e:
            result[""error""] = f""boto3_not_available: {e}""
            return result

        try:
            session = self._session
            if session is None:
                if self._profile_name:
                    session = boto3.Session(profile_name=self._profile_name, region_name=self._region_name)
                else:
                    session = boto3.Session(region_name=self._region_name)

            cfg = _BotocoreConfig(
                retries={""max_attempts"": self._max_attempts, ""mode"": ""standard""},
                connect_timeout=self._connect_timeout,
                read_timeout=self._read_timeout,
            )

            sts = session.client(""sts"", region_name=self._region_name or session.region_name, config=cfg)
            resp = sts.get_caller_identity()

            identity = {
                ""Account"": resp.get(""Account""),
                ""Arn"": resp.get(""Arn""),
                ""UserId"": resp.get(""UserId""),
            }

            result.update({
                ""success"": True,
                ""identity"": identity,
                ""source"": ""boto3"",
                ""error"": None,
            })
            return result

        except NoCredentialsError:
            result[""error""] = ""no_credentials_found""
        except NoRegionError:
            result[""error""] = ""no_region_configured""
        except ClientError as e:
            code = getattr(e, ""response"", {}).get(""Error"", {}).get(""Code"", ""ClientError"")
            msg = getattr(e, ""response"", {}).get(""Error"", {}).get(""Message"", str(e))
            result[""error""] = f""{code}: {msg}""
        except BotoCoreError as e:
            result[""error""] = f""boto_core_error: {e}""
        except Exception as e:
            result[""error""] = f""unexpected_error: {e}""

        return result"
312949,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/factory/source/element.py,apidoc.factory.source.element.Element,"from apidoc.lib.util.cast import to_boolean
import collections
from apidoc.object.source_raw import Sampleable, Displayable

class Element:
    """""" Populate Helper Factory
    """"""

    def set_common_datas(self, element, name, datas):
        """"""Populated common data for an element from dictionnary datas
        """"""
        element.name = str(name)
        if 'description' in datas:
            element.description = str(datas['description']).strip()
        if isinstance(element, Sampleable) and element.sample is None and ('sample' in datas):
            element.sample = str(datas['sample']).strip()
        if isinstance(element, Displayable):
            if 'display' in datas:
                element.display = to_boolean(datas['display'])
            if 'label' in datas:
                element.label = datas['label']
            else:
                element.label = element.name

    def create_dictionary_of_element_from_dictionary(self, property_name, datas):
        """"""Populate a dictionary of elements
        """"""
        response = {}
        if property_name in datas and datas[property_name] is not None and isinstance(datas[property_name], collections.Iterable):
            for key, value in datas[property_name].items():
                response[key] = self.create_from_name_and_dictionary(key, value)
        return response

    def create_list_of_element_from_dictionary(self, property_name, datas):
        """"""Populate a list of elements
        """"""
        response = []
        if property_name in datas and datas[property_name] is not None and isinstance(datas[property_name], list):
            for value in datas[property_name]:
                response.append(self.create_from_dictionary(value))
        return response

    def get_enum(self, property, enum, datas):
        """"""Factory enum type
        """"""
        str_property = str(datas[property]).lower()
        if str_property not in enum:
            raise ValueError('Unknow enum ""%s"" for ""%s"".' % (str_property, property))
        return enum(str_property)","
class Element:
    
    def set_common_datas(self, element, name, datas):
        
        pass
    def create_dictionary_of_element_from_dictionary(self, property_name, datas):
        
        pass

    def create_list_of_element_from_dictionary(self, property_name, datas):
        
        pass

    def get_enum(self, property, enum, datas):
        
        pass",snippet_244,5,"from enum import Enum
from collections.abc import Mapping, Iterable

class Element:
    
    def set_common_datas(self, element, name, datas):
        if element is None:
            return None
        if name is not None:
            try:
                setattr(element, 'name', name)
            except Exception:
                pass
        if isinstance(datas, Mapping):
            for k, v in datas.items():
                try:
                    setattr(element, k, v)
                except Exception:
                    continue
        return element

    def create_dictionary_of_element_from_dictionary(self, property_name, datas):
        if not isinstance(datas, Mapping):
            return {}
        value = datas.get(property_name)
        if value is None:
            return {}
        if isinstance(value, Mapping):
            return dict(value)
        if isinstance(value, Iterable) and not isinstance(value, (str, bytes)):
            try:
                return dict(value)
            except Exception:
                return {}
        return {}

    def create_list_of_element_from_dictionary(self, property_name, datas):
        if not isinstance(datas, Mapping):
            return []
        value = datas.get(property_name)
        if value is None:
            return []
        if isinstance(value, list):
            return value
        if isinstance(value, tuple):
            return list(value)
        if isinstance(value, Iterable) and not isinstance(value, (str, bytes, Mapping)):
            return list(value)
        return [value]

    def get_enum(self, property, enum, datas):
        if not isinstance(datas, Mapping):
            return None
        raw = datas.get(property)
        if raw is None:
            return None

        if isinstance(enum, type) and issubclass(enum, Enum):
            if isinstance(raw, enum):
                return raw
            if isinstance(raw, str):
                key = raw.strip()
                # Try exact, then case-insensitive by name
                try:
                    return enum[key]
                except Exception:
                    pass
                lowered = key.lower()
                for member in enum:
                    if member.name.lower() == lowered:
                        return member
                # Try by value if value is string and matches
                for member in enum:
                    try:
                        if str(member.value) == key:
                            return member
                    except Exception:
                        continue
            else:
                # Try by value
                for member in enum:
                    try:
                        if member.value == raw:
                            return member
                    except Exception:
                        continue
        else:
            # Fallback for mapping-like enums
            if isinstance(enum, Mapping):
                if raw in enum:
                    return enum[raw]
                if isinstance(raw, str):
                    lowered = raw.lower()
                    for k, v in enum.items():
                        try:
                            if isinstance(k, str) and k.lower() == lowered:
                                return v
                        except Exception:
                            continue
        return None"
313013,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/object/source_raw.py,apidoc.object.source_raw.Sampleable,"class Sampleable:
    """"""Element who can provide samples
    """"""

    def __init__(self):
        """"""Class instantiation
        """"""
        super().__init__()
        self.sample = None

    def get_sample(self):
        """"""Return the a sample for the element
        """"""
        if self.sample is None:
            return self.get_default_sample()
        return self.sample

    def get_default_sample(self):
        """"""Return default value for the element
        """"""
        return 'my_%s' % self.name","class Sampleable:
    
    def __init__(self):
        
        pass
    def get_sample(self):
        
        pass
    def get_default_sample(self):
        
        pass",snippet_245,4,"class Sampleable:
    
    def __init__(self):
        self._sampler = None
        self._default_sample = {}
    
    def get_sample(self):
        if callable(self._sampler):
            try:
                sample = self._sampler()
                return sample if sample is not None else self.get_default_sample()
            except Exception:
                return self.get_default_sample()
        return self.get_default_sample()
    
    def get_default_sample(self):
        import copy
        return copy.deepcopy(self._default_sample)"
313027,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/service/config.py,apidoc.service.config.Config,"import os
from apidoc.object.config import Config as ConfigObject

class Config:
    """"""Provide tool to managed config
    """"""

    def validate(self, config):
        """"""Validate that the source file is ok
        """"""
        if not isinstance(config, ConfigObject):
            raise Exception('Config object expected')
        if config['output']['componants'] not in ('local', 'remote', 'embedded', 'without'):
            raise ValueError('Unknown componant ""%s"".' % config['output']['componants'])
        if config['output']['layout'] not in ('default', 'content-only'):
            raise ValueError('Unknown layout ""%s"".' % config['output']['layout'])
        if config['input']['locations'] is not None:
            unknown_locations = [x for x in config['input']['locations'] if not os.path.exists(x)]
            if len(unknown_locations) > 0:
                raise ValueError('Location%s ""%s"" does not exists' % ('s' if len(unknown_locations) > 1 else '', '"" and ""'.join(unknown_locations)))
            config['input']['locations'] = [os.path.realpath(x) for x in config['input']['locations']]
        if config['input']['arguments'] is not None:
            if not isinstance(config['input']['arguments'], dict):
                raise ValueError('Sources arguments ""%s"" are not a dict' % config['input']['arguments'])

    def get_template_from_config(self, config):
        """"""Retrieve a template path from the config object
        """"""
        if config['output']['template'] == 'default':
            return os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'template', 'default.html')
        else:
            return os.path.abspath(config['output']['template'])","
class Config:
    
    def validate(self, config):
        
        pass
    def get_template_from_config(self, config):
        
        pass",snippet_246,3,"from pathlib import Path
from typing import Any, Mapping, Optional


class Config:
    def validate(self, config: Any) -> dict:
        if not isinstance(config, Mapping):
            raise TypeError(""config must be a mapping/dict"")
        has_template = ""template"" in config
        has_path = ""template_path"" in config

        if not has_template and not has_path:
            raise ValueError(""config must contain either 'template' or 'template_path'"")
        if has_template and has_path:
            raise ValueError(""config must not contain both 'template' and 'template_path'"")

        normalized: dict = {}

        if has_template:
            template = config[""template""]
            if not isinstance(template, str):
                raise TypeError(""'template' must be a string"")
            normalized[""source""] = ""inline""
            normalized[""template""] = template

        if has_path:
            path_val = config[""template_path""]
            if isinstance(path_val, (str, Path)):
                path = Path(path_val)
            else:
                raise TypeError(""'template_path' must be a str or pathlib.Path"")
            if not path.exists():
                raise ValueError(f""template file does not exist: {path}"")
            if not path.is_file():
                raise ValueError(f""template path is not a file: {path}"")
            normalized[""source""] = ""file""
            normalized[""template_path""] = path

        encoding = config.get(""encoding"", ""utf-8"")
        if not isinstance(encoding, str):
            raise TypeError(""'encoding' must be a string"")
        normalized[""encoding""] = encoding

        return normalized

    def get_template_from_config(self, config: Any) -> str:
        cfg = self.validate(config)
        if cfg[""source""] == ""inline"":
            return cfg[""template""]
        try:
            return Path(cfg[""template_path""]).read_text(encoding=cfg[""encoding""])
        except OSError as e:
            raise ValueError(f""failed to read template file: {e}"") from e"
313029,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/service/merger.py,apidoc.service.merger.Merger,"from apidoc.lib.util.cast import to_boolean

class Merger:
    """"""Provide tool to merge elements
    """"""

    def merge_extends(self, target, extends, inherit_key='inherit', inherit=False):
        """"""Merge extended dicts
        """"""
        if isinstance(target, dict):
            if inherit and inherit_key in target and (not to_boolean(target[inherit_key])):
                return
            if not isinstance(extends, dict):
                raise ValueError('Unable to merge: Dictionnary expected')
            for key in extends:
                if key not in target:
                    target[str(key)] = extends[key]
                else:
                    self.merge_extends(target[key], extends[key], inherit_key, True)
        elif isinstance(target, list):
            if not isinstance(extends, list):
                raise ValueError('Unable to merge: List expected')
            target += extends

    def merge_sources(self, datas):
        """"""Merge sources files
        """"""
        datas = [data for data in datas if data is not None]
        if len(datas) == 0:
            raise ValueError('Data missing')
        if len(datas) == 1:
            return datas[0]
        if isinstance(datas[0], list):
            if len([x for x in datas if not isinstance(x, list)]) > 0:
                raise TypeError('Unable to merge: List expected')
            base = []
            for x in datas:
                base = base + x
            return base
        if isinstance(datas[0], dict):
            if len([x for x in datas if not isinstance(x, dict)]) > 0:
                raise TypeError('Unable to merge: Dictionnary expected')
            result = {}
            for element in datas:
                for key in element:
                    if key in result:
                        result[key] = self.merge_sources([result[key], element[key]])
                    else:
                        result[key] = element[key]
            return result
        if len([x for x in datas if isinstance(x, (dict, list))]) > 0:
            raise TypeError('Unable to merge: List not expected')
        raise ValueError('Unable to merge: Conflict')

    def merge_configs(self, config, datas):
        """"""Merge configs files
        """"""
        if not isinstance(config, dict) or len([x for x in datas if not isinstance(x, dict)]) > 0:
            raise TypeError('Unable to merge: Dictionnary expected')
        for key, value in config.items():
            others = [x[key] for x in datas if key in x]
            if len(others) > 0:
                if isinstance(value, dict):
                    config[key] = self.merge_configs(value, others)
                else:
                    config[key] = others[-1]
        return config","
class Merger:
    
    def merge_extends(self, target, extends, inherit_key='inherit', inherit=False):
        
        pass
    def merge_sources(self, datas):
        
        pass

    def merge_configs(self, config, datas):
        
        pass",snippet_247,4,"class Merger:
    def _deepcopy(self, obj):
        import copy
        return copy.deepcopy(obj)

    def _merge_lists(self, base, child):
        seen = set()
        result = []
        for item in base + child:
            key = id(item)
            # For hashable values, use the value; otherwise fallback to id
            try:
                key = (True, item)
            except Exception:
                key = (False, id(item))
            if key in seen:
                continue
            seen.add(key)
            result.append(item)
        return result

    def merge_extends(self, target, extends, inherit_key='inherit', inherit=False):
        if target is None and extends is None:
            return None
        if not isinstance(target, dict) and not isinstance(extends, dict):
            return target if target is not None else extends
        target = {} if target is None else target
        extends = {} if extends is None else extends

        result = {}
        keys = set(extends.keys()) | set(target.keys())

        for key in keys:
            child_present = key in target
            base_present = key in extends

            child_val = self._deepcopy(target[key]) if child_present else None
            base_val = self._deepcopy(extends[key]) if base_present else None

            local_inherit = bool(inherit)
            # If child is a dict and contains inherit flag, use it and remove the flag from final value
            if isinstance(child_val, dict) and inherit_key in child_val:
                inherit_flag = bool(child_val.get(inherit_key))
                # Remove the inherit key for the resultant merged value
                child_val = {k: self._deepcopy(v) for k, v in child_val.items() if k != inherit_key}
                local_inherit = local_inherit or inherit_flag

            if child_present:
                if base_present and local_inherit:
                    # Merge based on types
                    if isinstance(base_val, dict) and isinstance(child_val, dict):
                        merged = self.merge_extends(child_val, base_val, inherit_key=inherit_key, inherit=local_inherit)
                    elif isinstance(base_val, list) and isinstance(child_val, list):
                        merged = self._merge_lists(base_val, child_val)
                    else:
                        merged = child_val
                else:
                    merged = child_val
            else:
                # child not present
                if base_present and local_inherit:
                    merged = base_val
                else:
                    continue  # skip key entirely

            result[key] = merged

        return result

    def merge_sources(self, datas):
        def deep_merge(a, b):
            # merge b over a
            if a is None:
                return self._deepcopy(b)
            if b is None:
                return self._deepcopy(a)
            if isinstance(a, dict) and isinstance(b, dict):
                res = {k: self._deepcopy(v) for k, v in a.items()}
                for k, v in b.items():
                    if k in res:
                        res[k] = deep_merge(res[k], v)
                    else:
                        res[k] = self._deepcopy(v)
                return res
            else:
                return self._deepcopy(b)

        result = {}
        if datas is None:
            return result
        for d in datas:
            if d is None:
                continue
            if not isinstance(d, dict):
                continue
            result = deep_merge(result, d)
        return result

    def merge_configs(self, config, datas):
        merged_sources = self.merge_sources(datas)
        base = config if isinstance(config, dict) else {}
        # Include base keys by default; child can suppress inheritance per-branch
        return self.merge_extends(merged_sources, base, inherit_key='inherit', inherit=True)"
313030,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/service/parser.py,apidoc.service.parser.Parser,"import os
import yaml
import json

class Parser:
    """"""Provide tools to parse files
    """"""

    def load_from_file(self, file_path, format=None):
        """"""Return dict from a file config
        """"""
        if format is None:
            base_name, file_extension = os.path.splitext(file_path)
            if file_extension in ('.yaml', '.yml'):
                format = 'yaml'
            elif file_extension in '.json':
                format = 'json'
            else:
                raise ValueError('Config file ""%s"" undetermined' % file_extension)
        if format == 'yaml':
            return yaml.load(open(file_path), Loader=yaml.CSafeLoader if yaml.__with_libyaml__ else yaml.SafeLoader)
        elif format == 'json':
            return json.load(open(file_path))
        else:
            raise ValueError('Format ""%s"" unknwon' % format)

    def load_all_from_directory(self, directory_path):
        """"""Return a list of dict from a directory containing files
        """"""
        datas = []
        for root, folders, files in os.walk(directory_path):
            for f in files:
                datas.append(self.load_from_file(os.path.join(root, f)))
        return datas","
class Parser:
    
    def load_from_file(self, file_path, format=None):
        
        pass
    def load_all_from_directory(self, directory_path):
        
        pass",snippet_248,3,"class Parser:
    def __init__(self):
        self._ext_to_format = {
            "".json"": ""json"",
            "".yaml"": ""yaml"",
            "".yml"": ""yaml"",
            "".toml"": ""toml"",
            "".ini"": ""ini"",
            "".cfg"": ""ini"",
            "".conf"": ""ini"",
            "".csv"": ""csv"",
            "".txt"": ""text"",
        }

    def load_from_file(self, file_path, format=None):
        import os

        if not isinstance(file_path, str):
            raise TypeError(""file_path must be a string"")
        if not os.path.isfile(file_path):
            raise FileNotFoundError(f""No such file: {file_path}"")

        fmt = (format or self._detect_format(file_path)) or ""text""
        fmt = fmt.lower()
        parser = self._get_parser(fmt)
        return parser(file_path)

    def load_all_from_directory(self, directory_path):
        import os

        if not isinstance(directory_path, str):
            raise TypeError(""directory_path must be a string"")
        if not os.path.isdir(directory_path):
            raise NotADirectoryError(f""No such directory: {directory_path}"")

        results = {}
        for name in os.listdir(directory_path):
            path = os.path.join(directory_path, name)
            if not os.path.isfile(path):
                continue
            fmt = self._detect_format(name)
            if not fmt:
                continue
            try:
                results[name] = self.load_from_file(path, fmt)
            except Exception:
                continue
        return results

    def _detect_format(self, file_path):
        import os

        _, ext = os.path.splitext(file_path)
        return self._ext_to_format.get(ext.lower())

    def _get_parser(self, fmt):
        parsers = {
            ""json"": self._parse_json,
            ""yaml"": self._parse_yaml,
            ""toml"": self._parse_toml,
            ""ini"": self._parse_ini,
            ""csv"": self._parse_csv,
            ""text"": self._parse_text,
        }
        if fmt not in parsers:
            raise ValueError(f""Unsupported format: {fmt}"")
        return parsers[fmt]

    def _parse_json(self, file_path):
        import json

        with open(file_path, ""r"", encoding=""utf-8"") as f:
            return json.load(f)

    def _parse_yaml(self, file_path):
        try:
            import yaml  # PyYAML
        except Exception as e:
            raise ImportError(""PyYAML is required to parse YAML"") from e
        with open(file_path, ""r"", encoding=""utf-8"") as f:
            return yaml.safe_load(f)

    def _parse_toml(self, file_path):
        try:
            import tomllib  # Python 3.11+
            with open(file_path, ""rb"") as f:
                return tomllib.load(f)
        except ModuleNotFoundError:
            try:
                import toml  # Third-party fallback
            except Exception as e:
                raise ImportError(""tomllib (py>=3.11) or toml package is required to parse TOML"") from e
            with open(file_path, ""r"", encoding=""utf-8"") as f:
                return toml.load(f)

    def _parse_ini(self, file_path):
        import configparser

        parser = configparser.ConfigParser()
        with open(file_path, ""r"", encoding=""utf-8"") as f:
            parser.read_file(f)
        result = {}
        if parser.defaults():
            result[""DEFAULT""] = dict(parser.defaults())
        for section in parser.sections():
            result[section] = {k: v for k, v in parser.items(section)}
        return result

    def _parse_csv(self, file_path):
        import csv

        with open(file_path, ""r"", encoding=""utf-8"", newline="""") as f:
            sample = f.read(4096)
            f.seek(0)
            sniffer = csv.Sniffer()
            try:
                dialect = sniffer.sniff(sample)
            except csv.Error:
                dialect = csv.excel
            try:
                has_header = sniffer.has_header(sample)
            except csv.Error:
                has_header = True
            reader = csv.reader(f, dialect)
            rows = list(reader)
            if not rows:
                return []
            if has_header:
                headers = rows[0]
                return [dict(zip(headers, r)) for r in rows[1:]]
            return rows

    def _parse_text(self, file_path):
        with open(file_path, ""r"", encoding=""utf-8"") as f:
            return f.read()"
313032,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/service/template.py,apidoc.service.template.Template,"import sys
import logging
import shutil
import os

class Template:
    """"""Provide tool to managed templates
    """"""

    def __init__(self):
        """"""Class instantiation
        """"""
        self.input = 'default.html'
        self.output = 'stdout'
        self.env = None

    def render(self, sources, config, out=sys.stdout):
        """"""Render the documentation as defined in config Object
        """"""
        logger = logging.getLogger()
        template = self.env.get_template(self.input)
        output = template.render(sources=sources, layout=config['output']['layout'], config=config['output'])
        if self.output == 'stdout':
            out.write(output)
        else:
            dir = os.path.dirname(self.output)
            if dir and (not os.path.exists(dir)):
                try:
                    os.makedirs(dir)
                except IOError as ioerror:
                    logger.error('Error on creating dir ""{}"": {}'.format(dir, str(ioerror)))
                    return
            if config['output']['template'] == 'default':
                if config['output']['componants'] == 'local':
                    for template_dir in self.env.loader.searchpath:
                        files = (os.path.join(template_dir, 'resource', 'js', 'combined.js'), os.path.join(template_dir, 'resource', 'css', 'combined.css'), os.path.join(template_dir, 'resource', 'font', 'apidoc.eot'), os.path.join(template_dir, 'resource', 'font', 'apidoc.woff'), os.path.join(template_dir, 'resource', 'font', 'apidoc.ttf'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.eot'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.woff'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.ttf'))
                        for file in files:
                            filename = os.path.basename(file)
                            dirname = os.path.basename(os.path.dirname(file))
                            if not os.path.exists(os.path.join(dir, dirname)):
                                os.makedirs(os.path.join(dir, dirname))
                            if os.path.exists(file):
                                shutil.copyfile(file, os.path.join(dir, dirname, filename))
                            else:
                                logger.warn('Missing resource file ""%s"". If you run apidoc in virtualenv, run ""%s""' % (filename, 'python setup.py resources'))
                if config['output']['componants'] == 'remote':
                    for template_dir in self.env.loader.searchpath:
                        files = (os.path.join(template_dir, 'resource', 'js', 'combined.js'), os.path.join(template_dir, 'resource', 'css', 'combined-embedded.css'), os.path.join(template_dir, 'resource', 'font', 'apidoc.eot'), os.path.join(template_dir, 'resource', 'font', 'apidoc.woff'), os.path.join(template_dir, 'resource', 'font', 'apidoc.ttf'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.eot'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.woff'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.ttf'))
                        for file in files:
                            filename = os.path.basename(file)
                            dirname = os.path.basename(os.path.dirname(file))
                            if not os.path.exists(os.path.join(dir, dirname)):
                                os.makedirs(os.path.join(dir, dirname))
                            if os.path.exists(file):
                                shutil.copyfile(file, os.path.join(dir, dirname, filename))
                            else:
                                logger.warn('Missing resource file ""%s"". If you run apidoc in virtualenv, run ""%s""' % (filename, 'python setup.py resources'))
            open(self.output, 'w').write(output)","
class Template:
    
    def __init__(self):
        
        pass
    def render(self, sources, config, out=sys.stdout):
        
        pass",snippet_249,3,"import sys
import os
from collections.abc import Mapping

class _AttrDict(dict):
    def __init__(self, *args, **kwargs):
        super().__init__()
        self.update(*args, **kwargs)

    def __getattr__(self, item):
        try:
            return self[item]
        except KeyError as e:
            raise AttributeError(item) from e

    def __setattr__(self, key, value):
        if key.startswith('_'):
            return super().__setattr__(key, value)
        self[key] = value

    def __delattr__(self, item):
        try:
            del self[item]
        except KeyError as e:
            raise AttributeError(item) from e

    def update(self, *args, **kwargs):
        for k, v in dict(*args, **kwargs).items():
            self[k] = self._wrap(v)

    def __setitem__(self, key, value):
        super().__setitem__(key, self._wrap(value))

    @classmethod
    def _wrap(cls, value):
        if isinstance(value, Mapping):
            return _AttrDict(value)
        return value


class _SafeFormatDict(dict):
    def __missing__(self, key):
        return ''


class Template:
    
    def __init__(self):
        pass

    def _to_iter(self, sources):
        if sources is None:
            return []
        if isinstance(sources, (str, os.PathLike)) or hasattr(sources, 'read'):
            return [sources]
        try:
            iter(sources)
        except TypeError:
            return [sources]
        return list(sources)

    def _read_source(self, src):
        if hasattr(src, 'read'):
            return src.read()
        if isinstance(src, (str, os.PathLike)):
            p = os.fspath(src)
            if os.path.exists(p) and os.path.isfile(p):
                with open(p, 'r', encoding='utf-8') as f:
                    return f.read()
            return str(src)
        return str(src)

    def render(self, sources, config, out=sys.stdout):
        cfg = config or {}
        wrapped = _AttrDict(cfg)
        fmt_map = _SafeFormatDict(wrapped)
        rendered_parts = []
        for src in self._to_iter(sources):
            text = self._read_source(src)
            rendered = text.format_map(fmt_map)
            rendered_parts.append(rendered)
        output = ''.join(rendered_parts)
        if out is not None:
            out.write(output)
        return output"
313488,adafruit/Adafruit_CircuitPython_framebuf,adafruit_Adafruit_CircuitPython_framebuf/adafruit_framebuf.py,adafruit_framebuf.MHMSBFormat,"class MHMSBFormat:
    """"""MHMSBFormat""""""

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        """"""Set a given pixel to a color.""""""
        index = (y * framebuf.stride + x) // 8
        offset = 7 - x & 7
        framebuf.buf[index] = framebuf.buf[index] & ~(1 << offset) | (color != 0) << offset

    @staticmethod
    def get_pixel(framebuf, x, y):
        """"""Get the color of a given pixel""""""
        index = (y * framebuf.stride + x) // 8
        offset = 7 - x & 7
        return framebuf.buf[index] >> offset & 1

    @staticmethod
    def fill(framebuf, color):
        """"""completely fill/clear the buffer with a color""""""
        if color:
            fill = 255
        else:
            fill = 0
        for i in range(len(framebuf.buf)):
            framebuf.buf[i] = fill

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        """"""Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.""""""
        for _x in range(x, x + width):
            offset = 7 - _x & 7
            for _y in range(y, y + height):
                index = (_y * framebuf.stride + _x) // 8
                framebuf.buf[index] = framebuf.buf[index] & ~(1 << offset) | (color != 0) << offset","class MHMSBFormat:
    
    @staticmethod
    def set_pixel(framebuf, x, y, color):
        
        pass
    @staticmethod
    def get_pixel(framebuf, x, y):
        
        pass
    @staticmethod
    def fill(framebuf, color):
        
        pass
    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        
        pass",snippet_250,5,"class MHMSBFormat:
    
    @staticmethod
    def _stride(framebuf):
        return getattr(framebuf, ""stride"", (framebuf.width + 7) // 8)

    @staticmethod
    def _index_and_mask(framebuf, x, y):
        if x < 0 or y < 0 or x >= framebuf.width or y >= framebuf.height:
            return None, None
        stride = MHMSBFormat._stride(framebuf)
        byte_index = y * stride + (x // 8)
        bit = 7 - (x % 8)
        mask = 1 << bit
        return byte_index, mask

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        idx, mask = MHMSBFormat._index_and_mask(framebuf, x, y)
        if idx is None:
            return
        buf = framebuf.buffer
        if color:
            buf[idx] = buf[idx] | mask
        else:
            buf[idx] = buf[idx] & (~mask & 0xFF)

    @staticmethod
    def get_pixel(framebuf, x, y):
        idx, mask = MHMSBFormat._index_and_mask(framebuf, x, y)
        if idx is None:
            return 0
        return 1 if (framebuf.buffer[idx] & mask) != 0 else 0

    @staticmethod
    def fill(framebuf, color):
        val = 0xFF if color else 0x00
        buf = framebuf.buffer
        for i in range(len(buf)):
            buf[i] = val

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        if width <= 0 or height <= 0:
            return

        x0 = max(0, x)
        y0 = max(0, y)
        x1 = min(framebuf.width, x + width)
        y1 = min(framebuf.height, y + height)

        if x0 >= x1 or y0 >= y1:
            return

        buf = framebuf.buffer
        stride = MHMSBFormat._stride(framebuf)
        c1 = bool(color)

        for yy in range(y0, y1):
            row_base = yy * stride
            first_byte = x0 // 8
            last_byte = (x1 - 1) // 8

            if first_byte == last_byte:
                start_bit_in_byte = x0 % 8
                end_bit_in_byte = (x1 - 1) % 8
                head_mask = 0xFF >> start_bit_in_byte
                tail_mask = 0xFF ^ (0xFF >> (end_bit_in_byte + 1))
                mask = head_mask & tail_mask
                idx = row_base + first_byte
                if c1:
                    buf[idx] = buf[idx] | mask
                else:
                    buf[idx] = buf[idx] & (~mask & 0xFF)
            else:
                # Head byte
                head_mask = 0xFF >> (x0 % 8)
                idx = row_base + first_byte
                if c1:
                    buf[idx] = buf[idx] | head_mask
                else:
                    buf[idx] = buf[idx] & (~head_mask & 0xFF)

                # Middle full bytes
                fill_byte = 0xFF if c1 else 0x00
                for b in range(first_byte + 1, last_byte):
                    buf[row_base + b] = fill_byte

                # Tail byte
                end_bit_in_byte = (x1 - 1) % 8
                tail_mask = 0xFF ^ (0xFF >> (end_bit_in_byte + 1))
                idx = row_base + last_byte
                if c1:
                    buf[idx] = buf[idx] | tail_mask
                else:
                    buf[idx] = buf[idx] & (~tail_mask & 0xFF)"
313489,adafruit/Adafruit_CircuitPython_framebuf,adafruit_Adafruit_CircuitPython_framebuf/adafruit_framebuf.py,adafruit_framebuf.MVLSBFormat,"class MVLSBFormat:
    """"""MVLSBFormat""""""

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        """"""Set a given pixel to a color.""""""
        index = (y >> 3) * framebuf.stride + x
        offset = y & 7
        framebuf.buf[index] = framebuf.buf[index] & ~(1 << offset) | (color != 0) << offset

    @staticmethod
    def get_pixel(framebuf, x, y):
        """"""Get the color of a given pixel""""""
        index = (y >> 3) * framebuf.stride + x
        offset = y & 7
        return framebuf.buf[index] >> offset & 1

    @staticmethod
    def fill(framebuf, color):
        """"""completely fill/clear the buffer with a color""""""
        if color:
            fill = 255
        else:
            fill = 0
        for i in range(len(framebuf.buf)):
            framebuf.buf[i] = fill

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        """"""Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.""""""
        while height > 0:
            index = (y >> 3) * framebuf.stride + x
            offset = y & 7
            for w_w in range(width):
                framebuf.buf[index + w_w] = framebuf.buf[index + w_w] & ~(1 << offset) | (color != 0) << offset
            y += 1
            height -= 1","class MVLSBFormat:
    
    @staticmethod
    def set_pixel(framebuf, x, y, color):
        
        pass
    @staticmethod
    def get_pixel(framebuf, x, y):
        
        pass
    @staticmethod
    def fill(framebuf, color):
        
        pass
    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        
        pass",snippet_251,5,"class MVLSBFormat:
    
    @staticmethod
    def _get_buf(framebuf):
        buf = getattr(framebuf, ""buffer"", None)
        if buf is None:
            buf = getattr(framebuf, ""buf"", None)
        if buf is None:
            raise AttributeError(""framebuf must have 'buffer' or 'buf'"")
        return buf

    @staticmethod
    def _rows_in_bytes(height):
        return (height + 7) // 8

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        w = getattr(framebuf, ""width"")
        h = getattr(framebuf, ""height"")
        if x < 0 or y < 0 or x >= w or y >= h:
            return
        buf = MVLSBFormat._get_buf(framebuf)
        rows = MVLSBFormat._rows_in_bytes(h)
        idx = x * rows + (y >> 3)
        bit = 1 << (y & 7)
        if color:
            buf[idx] = buf[idx] | bit
        else:
            buf[idx] = buf[idx] & (~bit & 0xFF)

    @staticmethod
    def get_pixel(framebuf, x, y):
        w = getattr(framebuf, ""width"")
        h = getattr(framebuf, ""height"")
        if x < 0 or y < 0 or x >= w or y >= h:
            return 0
        buf = MVLSBFormat._get_buf(framebuf)
        rows = MVLSBFormat._rows_in_bytes(h)
        idx = x * rows + (y >> 3)
        bit = 1 << (y & 7)
        return 1 if (buf[idx] & bit) else 0

    @staticmethod
    def fill(framebuf, color):
        w = getattr(framebuf, ""width"")
        h = getattr(framebuf, ""height"")
        buf = MVLSBFormat._get_buf(framebuf)
        val = 0xFF if color else 0x00
        # Fill all bytes
        for i in range(len(buf)):
            buf[i] = val
        # Optional: mask unused bits in last byte of each column when setting to 1
        # to ensure out-of-bounds bits are cleared. This keeps behavior clean.
        if color:
            rem = h & 7
            if rem != 0:
                rows = MVLSBFormat._rows_in_bytes(h)
                mask = (1 << rem) - 1  # bits [0..rem-1] set
                for x in range(w):
                    idx = x * rows + (rows - 1)
                    buf[idx] &= mask

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        fb_w = getattr(framebuf, ""width"")
        fb_h = getattr(framebuf, ""height"")
        if width <= 0 or height <= 0:
            return

        x0 = max(0, x)
        y0 = max(0, y)
        x1 = min(fb_w, x + width)
        y1 = min(fb_h, y + height)
        if x0 >= x1 or y0 >= y1:
            return

        buf = MVLSBFormat._get_buf(framebuf)
        rows = MVLSBFormat._rows_in_bytes(fb_h)

        y0_byte = y0 >> 3
        y1_byte = (y1 - 1) >> 3
        y0_bit = y0 & 7
        y1_bit = (y1 - 1) & 7

        for xi in range(x0, x1):
            base = xi * rows
            if y0_byte == y1_byte:
                # Rectangle within the same byte
                mask = ((0xFF << y0_bit) & (0xFF >> (7 - y1_bit))) & 0xFF
                idx = base + y0_byte
                if color:
                    buf[idx] = buf[idx] | mask
                else:
                    buf[idx] = buf[idx] & (~mask & 0xFF)
            else:
                # First partial byte
                first_mask = (0xFF << y0_bit) & 0xFF
                idx = base + y0_byte
                if color:
                    buf[idx] = buf[idx] | first_mask
                else:
                    buf[idx] = buf[idx] & (~first_mask & 0xFF)

                # Middle full bytes
                for b in range(y0_byte + 1, y1_byte):
                    idx = base + b
                    buf[idx] = 0xFF if color else 0x00

                # Last partial byte
                last_mask = 0xFF >> (7 - y1_bit)
                idx = base + y1_byte
                if color:
                    buf[idx] = buf[idx] | last_mask
                else:
                    buf[idx] = buf[idx] & (~last_mask & 0xFF)"
313491,adafruit/Adafruit_CircuitPython_framebuf,adafruit_Adafruit_CircuitPython_framebuf/adafruit_framebuf.py,adafruit_framebuf.RGB888Format,"class RGB888Format:
    """"""RGB888Format""""""

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        """"""Set a given pixel to a color.""""""
        index = (y * framebuf.stride + x) * 3
        if isinstance(color, tuple):
            framebuf.buf[index:index + 3] = bytes(color)
        else:
            framebuf.buf[index:index + 3] = bytes((color >> 16 & 255, color >> 8 & 255, color & 255))

    @staticmethod
    def get_pixel(framebuf, x, y):
        """"""Get the color of a given pixel""""""
        index = (y * framebuf.stride + x) * 3
        return framebuf.buf[index] << 16 | framebuf.buf[index + 1] << 8 | framebuf.buf[index + 2]

    @staticmethod
    def fill(framebuf, color):
        """"""completely fill/clear the buffer with a color""""""
        fill = (color >> 16 & 255, color >> 8 & 255, color & 255)
        for i in range(0, len(framebuf.buf), 3):
            framebuf.buf[i:i + 3] = bytes(fill)

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        """"""Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.""""""
        fill = (color >> 16 & 255, color >> 8 & 255, color & 255)
        for _x in range(x, x + width):
            for _y in range(y, y + height):
                index = (_y * framebuf.stride + _x) * 3
                framebuf.buf[index:index + 3] = bytes(fill)","class RGB888Format:
    
    @staticmethod
    def set_pixel(framebuf, x, y, color):
        
        pass
    @staticmethod
    def get_pixel(framebuf, x, y):
        
        pass
    @staticmethod
    def fill(framebuf, color):
        
        pass
    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        
        pass",snippet_252,5,"class RGB888Format:
    @staticmethod
    def _get_attrs(framebuf):
        buf = getattr(framebuf, ""buffer"", None)
        if buf is None:
            buf = framebuf
        width = getattr(framebuf, ""width"", None)
        height = getattr(framebuf, ""height"", None)
        if width is None or height is None:
            raise AttributeError(""framebuf must provide width and height"")
        stride = getattr(framebuf, ""stride"", width * 3)
        return buf, width, height, stride

    @staticmethod
    def _color_bytes(color):
        r = (color >> 16) & 0xFF
        g = (color >> 8) & 0xFF
        b = color & 0xFF
        return bytes((r, g, b))

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        buf, width, height, stride = RGB888Format._get_attrs(framebuf)
        if not (0 <= x < width and 0 <= y < height):
            return
        off = y * stride + x * 3
        c = RGB888Format._color_bytes(color)
        buf[off:off + 3] = c

    @staticmethod
    def get_pixel(framebuf, x, y):
        buf, width, height, stride = RGB888Format._get_attrs(framebuf)
        if not (0 <= x < width and 0 <= y < height):
            return 0
        off = y * stride + x * 3
        r, g, b = buf[off], buf[off + 1], buf[off + 2]
        return (r << 16) | (g << 8) | b

    @staticmethod
    def fill(framebuf, color):
        buf, width, height, stride = RGB888Format._get_attrs(framebuf)
        if width <= 0 or height <= 0:
            return
        c = RGB888Format._color_bytes(color)
        row = (c * width)
        for y in range(height):
            start = y * stride
            end = start + width * 3
            buf[start:end] = row

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        buf, fbw, fbh, stride = RGB888Format._get_attrs(framebuf)
        if width <= 0 or height <= 0:
            return
        x0 = max(0, x)
        y0 = max(0, y)
        x1 = min(fbw, x + width)
        y1 = min(fbh, y + height)
        if x0 >= x1 or y0 >= y1:
            return
        w = x1 - x0
        c = RGB888Format._color_bytes(color)
        row = (c * w)
        for yy in range(y0, y1):
            start = yy * stride + x0 * 3
            end = start + w * 3
            buf[start:end] = row"
319441,pysal/spglm,spglm/varfuncs.py,spglm.varfuncs.VarianceFunction,"import numpy as np

class VarianceFunction:
    """"""
    Relates the variance of a random variable to its mean. Defaults to 1.

    Methods
    -------
    call
        Returns an array of ones that is the same shape as `mu`

    Notes
    -----
    After a variance function is initialized, its call method can be used.

    Alias for VarianceFunction:
    constant = VarianceFunction()

    See also
    --------
    statsmodels.family.family
    """"""

    def __call__(self, mu):
        """"""
        Default variance function

        Parameters
        -----------
        mu : array-like
            mean parameters

        Returns
        -------
        v : array
            ones(mu.shape)
        """"""
        mu = np.asarray(mu)
        return np.ones(mu.shape, np.float64)

    def deriv(self, mu):
        """"""
        Derivative of the variance function v'(mu)
        """"""
        from statsmodels.tools.numdiff import approx_fprime_cs
        return np.diag(approx_fprime_cs(mu, self))","
class VarianceFunction:
    '''
    Relates the variance of a random variable to its mean. Defaults to 1.
    Methods
    -------
    call
        Returns an array of ones that is the same shape as `mu`
    Notes
    -----
    After a variance function is initialized, its call method can be used.
    Alias for VarianceFunction:
    constant = VarianceFunction()
    See also
    --------
    statsmodels.family.family
    '''
    def __call__(self, mu):
        
        pass

    def deriv(self, mu):
        
        pass",snippet_256,3,"import numpy as np

class VarianceFunction:
    '''
    Relates the variance of a random variable to its mean. Defaults to 1.
    Methods
    -------
    call
        Returns an array of ones that is the same shape as `mu`
    Notes
    -----
    After a variance function is initialized, its call method can be used.
    Alias for VarianceFunction:
    constant = VarianceFunction()
    See also
    --------
    statsmodels.family.family
    '''
    def __call__(self, mu):
        return np.ones_like(mu, dtype=float)

    def deriv(self, mu):
        return np.zeros_like(mu, dtype=float)

# Alias
constant = VarianceFunction()"
320689,optimizely/python-sdk,optimizely_python-sdk/optimizely/user_profile.py,optimizely.user_profile.UserProfileService,"from typing import Any, Optional

class UserProfileService:
    """""" Class encapsulating user profile service functionality.
  Override with your own implementation for storing and retrieving the user profile. """"""

    def lookup(self, user_id: str) -> dict[str, Any]:
        """""" Fetch the user profile dict corresponding to the user ID.

    Args:
      user_id: ID for user whose profile needs to be retrieved.

    Returns:
      Dict representing the user's profile.
    """"""
        return UserProfile(user_id).__dict__

    def save(self, user_profile: dict[str, Any]) -> None:
        """""" Save the user profile dict sent to this method.

    Args:
      user_profile: Dict representing the user's profile.
    """"""
        pass","
class UserProfileService:
    
    def lookup(self, user_id: str) -> dict[str, Any]:
        
        pass
    def save(self, user_profile: dict[str, Any]) -> None:
        
        pass",snippet_257,3,"from typing import Any, Dict
from copy import deepcopy
import threading


class UserProfileService:
    def __init__(self) -> None:
        self._store: Dict[str, Dict[str, Any]] = {}
        self._lock = threading.RLock()

    def lookup(self, user_id: str) -> dict[str, Any]:
        if not isinstance(user_id, str) or not user_id:
            raise ValueError(""user_id must be a non-empty string"")
        with self._lock:
            if user_id not in self._store:
                raise KeyError(f""user_id not found: {user_id}"")
            return deepcopy(self._store[user_id])

    def save(self, user_profile: dict[str, Any]) -> None:
        if not isinstance(user_profile, dict):
            raise TypeError(""user_profile must be a dict"")
        user_id = user_profile.get(""user_id"") or user_profile.get(""id"")
        if not isinstance(user_id, str) or not user_id:
            raise ValueError(""user_profile must contain a non-empty 'user_id' or 'id' string"")
        profile_copy = dict(user_profile)
        profile_copy[""user_id""] = user_id
        with self._lock:
            self._store[user_id] = deepcopy(profile_copy)"
322746,wilson-eft/wilson,wilson-eft_wilson/wilson/classes.py,wilson.classes.RGsolution,"import numpy as np
from math import log, e

class RGsolution:
    """"""Class representing a continuous (interpolated) solution to the
    SMEFT RGEs to be used for plotting.""""""

    def __init__(self, fun, scale_min, scale_max):
        """"""Initialize.

        Parameters:

        - fun: function of the scale that is expected to return a
        dictionary with the RGE solution and to accept vectorized input.
        - scale_min, scale_max: lower and upper boundaries of the scale
        """"""
        self.fun = fun
        self.scale_min = scale_min
        self.scale_max = scale_max

    def plotdata(self, key, part='re', scale='log', steps=50):
        """"""Return a tuple of arrays x, y that can be fed to plt.plot,
        where x is the scale in GeV and y is the parameter of interest.

        Parameters:

        - key: dicionary key of the parameter to be plotted (e.g. a WCxf
          coefficient name or a SM parameter like 'g')
        - part: plot the real part 're' (default) or the imaginary part 'im'
        - scale: 'log'; make the x steps logarithmically distributed; for
          'linear', linearly distributed
        - steps: steps in x to take (default: 50)
        """"""
        if scale == 'log':
            x = np.logspace(log(self.scale_min), log(self.scale_max), steps, base=e)
        elif scale == 'linear':
            x = np.linspace(self.scale_min, self.scale_max, steps)
        y = self.fun(x)
        y = np.array([d[key] for d in y])
        if part == 're':
            return (x, y.real)
        elif part == 'im':
            return (x, y.imag)

    def plot(self, key, part='re', scale='log', steps=50, legend=True, plotargs={}):
        """"""Plot the RG evolution of parameter `key`.

        Parameters:

        - part, scale, steps: see `plotdata`
        - legend: boolean, show the legend (default: True)
        - plotargs: dictionary of arguments to be passed to plt.plot
        """"""
        try:
            import matplotlib.pyplot as plt
        except ImportError:
            raise ImportError('Please install matplotlib if you want to use the plot method')
        pdat = self.plotdata(key, part=part, scale=scale, steps=steps)
        plt.plot(*pdat, label=key, **plotargs)
        if scale == 'log':
            plt.xscale('log')
        if legend:
            plt.legend()","
class RGsolution:
    
    def __init__(self, fun, scale_min, scale_max):
        
        pass
    def plotdata(self, key, part='re', scale='log', steps=50):
        
        pass

    def plotdata(self, key, part='re', scale='log', steps=50):
        
        pass",snippet_258,4,"class RGsolution:
    
    def __init__(self, fun, scale_min, scale_max):
        if not callable(fun):
            raise TypeError(""fun must be callable"")
        if not (isinstance(scale_min, (int, float)) and isinstance(scale_max, (int, float))):
            raise TypeError(""scale_min and scale_max must be numbers"")
        if scale_max <= scale_min:
            raise ValueError(""scale_max must be greater than scale_min"")
        self.fun = fun
        self.scale_min = float(scale_min)
        self.scale_max = float(scale_max)

    def plotdata(self, key, part='re', scale='log', steps=50):
        if not isinstance(steps, int) or steps < 2:
            raise ValueError(""steps must be an integer >= 2"")
        if scale not in ('log', 'lin'):
            raise ValueError(""scale must be 'log' or 'lin'"")
        if part not in ('re', 'im', 'abs'):
            raise ValueError(""part must be 're', 'im', or 'abs'"")

        if scale == 'log':
            if self.scale_min <= 0:
                raise ValueError(""scale_min must be > 0 for log scale"")
            ratio = (self.scale_max / self.scale_min) ** (1 / (steps - 1))
            xs = [self.scale_min * (ratio ** i) for i in range(steps)]
        else:
            step = (self.scale_max - self.scale_min) / (steps - 1)
            xs = [self.scale_min + i * step for i in range(steps)]

        ys = []
        for x in xs:
            res = self.fun(x)
            if callable(key):
                val = key(res)
            else:
                try:
                    val = res[key]
                except (TypeError, KeyError):
                    raise KeyError(f""Key {key!r} not found in result at scale {x!r}"")
            if part == 're':
                if isinstance(val, complex):
                    ys.append(val.real)
                else:
                    ys.append(float(val))
            elif part == 'im':
                if isinstance(val, complex):
                    ys.append(val.imag)
                else:
                    ys.append(0.0)
            else:
                if isinstance(val, complex):
                    ys.append(abs(val))
                else:
                    ys.append(abs(float(val)))
        return xs, ys

    def plotdata(self, key, part='re', scale='log', steps=50):
        if not isinstance(steps, int) or steps < 2:
            raise ValueError(""steps must be an integer >= 2"")
        if scale not in ('log', 'lin'):
            raise ValueError(""scale must be 'log' or 'lin'"")
        if part not in ('re', 'im', 'abs'):
            raise ValueError(""part must be 're', 'im', or 'abs'"")

        if scale == 'log':
            if self.scale_min <= 0:
                raise ValueError(""scale_min must be > 0 for log scale"")
            ratio = (self.scale_max / self.scale_min) ** (1 / (steps - 1))
            xs = [self.scale_min * (ratio ** i) for i in range(steps)]
        else:
            step = (self.scale_max - self.scale_min) / (steps - 1)
            xs = [self.scale_min + i * step for i in range(steps)]

        ys = []
        for x in xs:
            res = self.fun(x)
            if callable(key):
                val = key(res)
            else:
                try:
                    val = res[key]
                except (TypeError, KeyError):
                    raise KeyError(f""Key {key!r} not found in result at scale {x!r}"")
            if part == 're':
                if isinstance(val, complex):
                    ys.append(val.real)
                else:
                    ys.append(float(val))
            elif part == 'im':
                if isinstance(val, complex):
                    ys.append(val.imag)
                else:
                    ys.append(0.0)
            else:
                if isinstance(val, complex):
                    ys.append(abs(val))
                else:
                    ys.append(abs(float(val)))
        return xs, ys"
322803,wilson-eft/wilson,wilson-eft_wilson/wilson/wcxf/classes.py,wilson.wcxf.classes.WCxf,"import yaml

class WCxf:
    """"""Base class for WCxf files (not meant to be used directly).""""""

    @classmethod
    def load(cls, stream, **kwargs):
        """"""Load the object data from a JSON or YAML file.""""""
        wcxf = _load_yaml_json(stream, **kwargs)
        return cls(**wcxf)

    def dump(self, stream=None, fmt='json', **kwargs):
        """"""Dump the object data to a JSON or YAML file.

        Optional arguments:

        - `stream`: if None (default), return a string. Otherwise,
          should be a writable file-like object
        - `fmt`: format, should be 'json' (default) or 'yaml'

        Additional keyword arguments will be passed to the `json.dump(s)`
        or `yaml.dump` methods.
        """"""
        d = {k: v for k, v in self.__dict__.items() if k[0] != '_'}
        if fmt.lower() == 'json':
            indent = kwargs.pop('indent', 2)
            return _dump_json(d, stream=stream, indent=indent, **kwargs)
        elif fmt.lower() == 'yaml':
            default_flow_style = kwargs.pop('default_flow_style', False)
            return yaml.dump(d, stream, default_flow_style=default_flow_style, **kwargs)
        else:
            raise ValueError(f""Format {fmt} unknown: use 'json' or 'yaml'."")","
class WCxf:
    
    @classmethod
    def load(cls, stream, **kwargs):
        
        pass
    def dump(self, stream=None, fmt='json', **kwargs):
        
        pass",snippet_259,3,"class WCxf:
    def __init__(self, data=None):
        self.data = data if data is not None else {}

    @classmethod
    def load(cls, stream, fmt=None, **kwargs):
        import json

        yaml = None
        try:
            import yaml as _yaml  # optional
            yaml = _yaml
        except Exception:
            pass

        def _is_file_like(obj):
            return hasattr(obj, 'read') and callable(obj.read)

        def _read_from_stream(s):
            if _is_file_like(s):
                return s.read()
            if isinstance(s, (bytes, bytearray)):
                return s.decode(kwargs.get('encoding', 'utf-8'))
            if isinstance(s, str):
                # If it's a path, read file
                try:
                    with open(s, 'r', encoding=kwargs.get('encoding', 'utf-8')) as f:
                        return f.read()
                except (OSError, IOError):
                    # Not a path or unreadable; treat as raw content
                    return s
            if isinstance(s, dict):
                return s  # already data
            raise TypeError(""Unsupported stream type for load"")

        def _infer_fmt(s, given_fmt):
            if given_fmt:
                return given_fmt.lower()
            # infer from filename if applicable
            if isinstance(stream, str):
                lower = stream.lower()
                if lower.endswith('.json'):
                    return 'json'
                if lower.endswith('.yml') or lower.endswith('.yaml'):
                    return 'yaml'
            # try content sniffing
            txt = s if isinstance(s, str) else None
            if isinstance(s, dict):
                return 'json'
            if txt is not None:
                t = txt.lstrip()
                if t.startswith('{') or t.startswith('['):
                    return 'json'
                # fallback to yaml if available
                if yaml is not None:
                    return 'yaml'
            return 'json'

        # Fast path if dict provided
        if isinstance(stream, dict):
            return cls(stream.copy())

        content = _read_from_stream(stream)
        fmt_final = _infer_fmt(content, fmt)

        if fmt_final == 'json':
            if isinstance(content, dict):
                data = content
            else:
                data = json.loads(content, **{k: v for k, v in kwargs.items() if k in {'cls', 'parse_float', 'parse_int', 'parse_constant', 'object_hook', 'object_pairs_hook'}})
            return cls(data)
        elif fmt_final == 'yaml':
            if yaml is None:
                raise ValueError(""YAML support requires PyYAML to be installed"")
            if isinstance(content, dict):
                data = content
            else:
                data = yaml.safe_load(content)
            return cls(data)
        else:
            raise ValueError(f""Unsupported format: {fmt_final}"")

    def dump(self, stream=None, fmt='json', **kwargs):
        import json

        yaml = None
        try:
            import yaml as _yaml  # optional
            yaml = _yaml
        except Exception:
            pass

        fmt = (fmt or 'json').lower()

        def _is_file_like(obj):
            return hasattr(obj, 'write') and callable(obj.write)

        def _serialize(data):
            if fmt == 'json':
                if stream is None:
                    return json.dumps(data, **{**{'ensure_ascii': False, 'indent': 2}, **kwargs})
                else:
                    return json.dumps(data, **kwargs)
            elif fmt == 'yaml':
                if yaml is None:
                    raise ValueError(""YAML support requires PyYAML to be installed"")
                default_opts = {'sort_keys': False}
                opts = {**default_opts, **kwargs}
                return yaml.safe_dump(data, **opts)
            else:
                raise ValueError(f""Unsupported format: {fmt}"")

        if stream is None:
            return _serialize(self.data)

        if _is_file_like(stream):
            text = _serialize(self.data)
            stream.write(text)
            return None

        if isinstance(stream, (str, bytes, bytearray)):
            path = stream.decode() if isinstance(stream, (bytes, bytearray)) else stream
            encoding = kwargs.pop('encoding', 'utf-8')
            text = _serialize(self.data)
            with open(path, 'w', encoding=encoding) as f:
                f.write(text)
            return None

        raise TypeError(""Unsupported stream type for dump"")"
322855,jhermann/rituals,jhermann_rituals/src/rituals/util/antglob.py,rituals.util.antglob.Pattern,"class Pattern:
    """"""A single pattern for either inclusion or exclusion.""""""

    def __init__(self, spec, inclusive):
        """"""Create regex-based pattern matcher from glob `spec`.""""""
        self.compiled = compile_glob(spec.rstrip('/'))
        self.inclusive = inclusive
        self.is_dir = spec.endswith('/')

    def __str__(self):
        """"""Return inclusiveness indicator and original glob pattern.""""""
        return ('+' if self.inclusive else '-') + self.compiled.pattern

    def matches(self, path):
        """"""Check this pattern against given `path`.""""""
        return bool(self.compiled.match(path))","class Pattern:
    
    def __init__(self, spec, inclusive):
        
        pass
    def __str__(self):
        
        pass
    def matches(self, path):
        
        pass",snippet_260,4,"class Pattern:
    
    def __init__(self, spec, inclusive):
        import re, fnmatch
        if not isinstance(spec, str):
            raise TypeError(""spec must be a string"")
        if not isinstance(inclusive, bool):
            raise TypeError(""inclusive must be a boolean"")
        self.spec = spec
        self.inclusive = inclusive
        # Precompile regex from glob-like spec
        self._regex = re.compile(fnmatch.translate(spec))
        
    def __str__(self):
        sign = ""+"" if self.inclusive else ""-""
        return f""{sign} {self.spec}""
    
    def matches(self, path):
        if not isinstance(path, str):
            raise TypeError(""path must be a string"")
        # fnmatch.translate produces a regex that matches the entire string
        matched = self._regex.fullmatch(path) is not None
        if not matched:
            return False
        return self.inclusive"
323184,SoftwareDefinedBuildings/XBOS,apps/Data_quality_analysis/Import_Data.py,Data_quality_analysis.Import_Data.Import_Data,"import pandas as pd
import glob
import numpy as np
import os

class Import_Data:
    """""" This class imports data from csv files. """"""

    def __init__(self):
        """""" Constructor: Store the imported data. """"""
        self.data = pd.DataFrame()

    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        """""" Imports csv file(s) and stores the result in data.

        Note
        ----
        1. If folder exists out of current directory, folder_name should contain correct regex
        2. Assuming there's no file called ""\\*.csv""

        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '\\*', i.e. all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.', i.e. current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe

        """"""
        if isinstance(file_name, str) and isinstance(folder_name, str):
            try:
                self.data = self._load_csv(file_name, folder_name, head_row, index_col, convert_col, concat_files)
            except Exception as e:
                raise e
        elif isinstance(file_name, list) and isinstance(folder_name, str):
            for i, file in enumerate(file_name):
                if isinstance(head_row, list):
                    _head_row = head_row[i]
                else:
                    _head_row = head_row
                if isinstance(index_col, list):
                    _index_col = index_col[i]
                else:
                    _index_col = index_col
                try:
                    data_tmp = self._load_csv(file, folder_name, _head_row, _index_col, convert_col, concat_files)
                    if concat_files:
                        self.data = self.data.append(data_tmp, ignore_index=False, verify_integrity=False)
                    else:
                        self.data = self.data.join(data_tmp, how='outer')
                except Exception as e:
                    raise e
        else:
            raise NotImplementedError(""Filename and Folder name can't both be of type list."")

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        """""" Load single csv file.

        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '*' - all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.' - current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe

        Returns
        -------
        pd.DataFrame()
            Dataframe containing csv data

        """"""
        if file_name == '*':
            if not os.path.isdir(folder_name):
                raise OSError('Folder does not exist.')
            else:
                file_name_list = sorted(glob.glob(folder_name + '*.csv'))
                if not file_name_list:
                    raise OSError('Either the folder does not contain any csv files or invalid folder provided.')
                else:
                    self.import_csv(file_name=file_name_list, head_row=head_row, index_col=index_col, convert_col=convert_col, concat_files=concat_files)
                    return self.data
        elif not os.path.isdir(folder_name):
            raise OSError('Folder does not exist.')
        else:
            path = os.path.join(folder_name, file_name)
            if head_row > 0:
                data = pd.read_csv(path, index_col=index_col, skiprows=[i for i in range(head_row - 1)])
            else:
                data = pd.read_csv(path, index_col=index_col)
            try:
                data.index = pd.to_datetime(data.index, format='%m/%d/%y %H:%M')
            except:
                data.index = pd.to_datetime(data.index, dayfirst=False, infer_datetime_format=True)
        if convert_col:
            for col in data.columns:
                if data[col].dtype != np.number:
                    data[col] = pd.to_numeric(data[col], errors='coerce')
        return data","
class Import_Data:
    
    def __init__(self):
        
        pass
    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        
        pass

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        
        pass",snippet_261,4,"import os
from pathlib import Path
from typing import Union, List, Dict
import pandas as pd
import glob


class Import_Data:
    
    def __init__(self):
        self.last_data = None
        self.last_files: List[Path] = []

    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        return self._load_csv(
            file_name=file_name,
            folder_name=folder_name,
            head_row=head_row,
            index_col=index_col,
            convert_col=convert_col,
            concat_files=concat_files
        )

    def _load_csv(self, file_name: Union[str, List[Union[str, Path]]], folder_name: Union[str, Path], head_row, index_col, convert_col: bool, concat_files: bool):
        folder = Path(folder_name).expanduser().resolve()

        def normalize_pattern(name: str) -> str:
            if any(ch in name for ch in [""*"", ""?"", ""[""]):
                return name
            # If direct file path provided
            p = Path(name)
            if p.suffix:  # has extension
                return str(p)
            # otherwise assume csv extension
            return f""{name}.csv""

        files: List[Path] = []

        if isinstance(file_name, (list, tuple)):
            for fn in file_name:
                f = Path(fn)
                if f.exists():
                    files.append(f.resolve())
                else:
                    pattern = normalize_pattern(str(fn))
                    matches = [Path(m).resolve() for m in glob.glob(str(folder / pattern))]
                    files.extend(matches)
        else:
            # single name or pattern
            pattern = normalize_pattern(str(file_name))
            # If it's an absolute path to a file
            p = Path(pattern)
            if p.exists() and p.is_file():
                files = [p.resolve()]
            else:
                files = [Path(m).resolve() for m in glob.glob(str(folder / pattern))]

        # Filter for csv-like files if no explicit extension given
        if isinstance(file_name, str) and not Path(file_name).suffix and not any(ch in file_name for ch in [""*"", ""?"", ""[""]):
            files = [f for f in files if f.suffix.lower() == "".csv""]

        # Final fallback: if user gave '*' default, ensure csv only
        if file_name == '*' or (isinstance(file_name, str) and '*' in file_name and '.csv' not in file_name.lower()):
            files = [f for f in files if f.suffix.lower() == "".csv""]

        # Deduplicate and sort
        files = sorted(set(files))

        if not files:
            raise FileNotFoundError(""No CSV files matched the given name/pattern and folder."")

        def read_one(path: Path) -> pd.DataFrame:
            df = pd.read_csv(
                path,
                header=head_row,
                index_col=index_col if index_col is not None else None
            )
            if convert_col:
                # Try numeric conversion on object columns
                for col in df.columns:
                    if pd.api.types.is_object_dtype(df[col]):
                        converted_num = pd.to_numeric(df[col], errors='ignore')
                        df[col] = converted_num
                # Optional: convert dtypes to best possible
                df = df.convert_dtypes()
            return df

        dataframes: Dict[Path, pd.DataFrame] = {}
        for f in files:
            dataframes[f] = read_one(f)

        self.last_files = files

        if concat_files:
            result = pd.concat(
                list(dataframes.values()),
                axis=0,
                ignore_index=False,
                sort=False
            )
            self.last_data = result
            return result

        if len(files) == 1:
            self.last_data = dataframes[files[0]]
            return dataframes[files[0]]

        # Multiple files: return dict keyed by file name
        result_dict = {f.name: df for f, df in dataframes.items()}
        self.last_data = result_dict
        return result_dict"
323187,SoftwareDefinedBuildings/XBOS,apps/Data_quality_analysis/Plot_Data.py,Data_quality_analysis.Plot_Data.Plot_Data,"import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

class Plot_Data:
    """""" This class contains functions for displaying various plots.

    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.

    """"""
    count = 1

    def __init__(self, figsize=(18, 5)):
        """""" Constructor.

        Parameters
        ----------
        figsize : tuple
            Size of figure.

        """"""
        self.figsize = figsize

    def correlation_plot(self, data):
        """""" Create heatmap of Pearson's correlation coefficient.

        Parameters
        ----------
        data    : pd.DataFrame()
            Data to display.

        Returns
        -------
        matplotlib.figure
            Heatmap.

        """"""
        fig = plt.figure(Plot_Data.count)
        corr = data.corr()
        ax = sns.heatmap(corr)
        Plot_Data.count += 1
        return fig

    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        """""" Create baseline and projection plots.

        Parameters
        ----------
        y_true              : pd.Series()
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.Dataframe()
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.

        Returns
        -------
        matplotlib.figure
            Baseline plot

        """"""
        fig = plt.figure(Plot_Data.count)
        if projection_period:
            nrows = len(baseline_period) + len(projection_period) / 2
        else:
            nrows = len(baseline_period) / 2
        base_df = pd.DataFrame()
        base_df['y_true'] = y_true
        base_df['y_pred'] = y_pred
        ax1 = fig.add_subplot(nrows, 1, 1)
        base_df.plot(ax=ax1, figsize=self.figsize, title='Baseline Period ({}-{}). \nBest Model: {}. \nBaseline Adj R2: {}. \nSite: {}.'.format(baseline_period[0], baseline_period[1], model_name, adj_r2, site))
        if projection_period:
            num_plot = 2
            for i in range(0, len(projection_period), 2):
                ax = fig.add_subplot(nrows, 1, num_plot)
                period = slice(projection_period[i], projection_period[i + 1])
                project_df = pd.DataFrame()
                try:
                    project_df['y_true'] = data.loc[period, output_col]
                    project_df['y_pred'] = model.predict(data.loc[period, input_col])
                    project_df['y_pred'][project_df['y_pred'] < 0] = 0
                    project_df.plot(ax=ax, figsize=self.figsize, title='Projection Period ({}-{})'.format(projection_period[i], projection_period[i + 1]))
                    num_plot += 1
                    fig.tight_layout()
                    Plot_Data.count += 1
                    return (fig, project_df['y_true'], project_df['y_pred'])
                except:
                    raise TypeError('If projecting into the future, please specify project_ind_col that has data available                                         in the future time period requested.')
        return (fig, None, None)","
class Plot_Data:
    
    def __init__(self, figsize=(18, 5)):
        
        pass
    def correlation_plot(self, data):
        
        pass

    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        
        pass",snippet_262,4,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error


class Plot_Data:
    
    def __init__(self, figsize=(18, 5)):
        self.figsize = figsize
        sns.set_style(""whitegrid"")

    def correlation_plot(self, data):
        if not isinstance(data, pd.DataFrame):
            data = pd.DataFrame(data)
        corr = data.corr(numeric_only=True)
        fig, ax = plt.subplots(figsize=self.figsize)
        sns.heatmap(
            corr,
            ax=ax,
            annot=True,
            fmt="".2f"",
            cmap=""coolwarm"",
            center=0,
            square=True,
            cbar_kws={""shrink"": 0.8, ""label"": ""Correlation""},
        )
        ax.set_title(""Correlation Matrix"")
        plt.tight_layout()
        return fig, ax

    def baseline_projection_plot(
        self,
        y_true,
        y_pred,
        baseline_period,
        projection_period,
        model_name,
        adj_r2,
        data,
        input_col,
        output_col,
        model,
        site,
    ):
        def _as_series(arr, index=None, name=None):
            if isinstance(arr, pd.Series):
                s = arr.copy()
            else:
                s = pd.Series(arr, index=index)
            if name is not None:
                s.name = name
            return s

        def _normalize_periods(period):
            if period is None:
                return []
            # Allow (start, end), list/tuple of pairs
            if isinstance(period, (tuple, list)) and len(period) == 2 and not isinstance(period[0], (list, tuple)):
                periods = [period]
            else:
                periods = list(period)
            norm = []
            for p in periods:
                if p is None:
                    continue
                start = pd.to_datetime(p[0])
                end = pd.to_datetime(p[1])
                if pd.isna(start) or pd.isna(end):
                    continue
                if end < start:
                    start, end = end, start
                norm.append((start, end))
            return norm

        # Prepare data/index alignment
        if not isinstance(data, pd.DataFrame):
            data = pd.DataFrame(data)
        index = data.index
        y_true = _as_series(y_true, index=index, name=""Observed"")
        y_pred = _as_series(y_pred, index=index, name=""Predicted"")

        df = pd.concat([y_true, y_pred], axis=1).dropna()
        if df.empty:
            raise ValueError(""No overlapping non-NaN data found between y_true and y_pred."")

        # Metrics
        r2 = r2_score(df[""Observed""], df[""Predicted""])
        mae = mean_absolute_error(df[""Observed""], df[""Predicted""])
        rmse = mean_squared_error(df[""Observed""], df[""Predicted""], squared=False)

        # Plot
        fig, axes = plt.subplots(1, 2, figsize=self.figsize)

        # Left: Predicted vs Observed scatter
        ax0 = axes[0]
        sns.scatterplot(
            x=df[""Observed""],
            y=df[""Predicted""],
            ax=ax0,
            s=30,
            color=""#2a9d8f"",
            edgecolor=""white"",
            alpha=0.8,
        )
        mn = np.nanmin([df[""Observed""].min(), df[""Predicted""].min()])
        mx = np.nanmax([df[""Observed""].max(), df[""Predicted""].max()])
        ax0.plot([mn, mx], [mn, mx], ls=""--"", c=""#264653"", lw=1.5, label=""1:1 line"")
        ax0.set_xlabel(f""Observed ({output_col})"")
        ax0.set_ylabel(f""Predicted ({output_col})"")
        ax0.set_title(f""{model_name} — Predicted vs Observed"")
        ax0.legend(loc=""best"")

        metrics_text = (
            f""Site: {site}\n""
            f""R²: {r2:.3f}\n""
            f""Adj. R²: {adj_r2:.3f}\n""
            f""RMSE: {rmse:.3f}\n""
            f""MAE: {mae:.3f}""
        )
        ax0.text(
            0.05,
            0.95,
            metrics_text,
            transform=ax0.transAxes,
            va=""top"",
            ha=""left"",
            bbox=dict(boxstyle=""round,pad=0.3"", fc=""white"", ec=""lightgray"", alpha=0.9),
            fontsize=9,
        )

        # Right: Time series with baseline/projection shading
        ax1 = axes[1]
        df.sort_index(inplace=True)
        ax1.plot(df.index, df[""Observed""], label=""Observed"", color=""#1f77b4"", lw=1.5)
        ax1.plot(df.index, df[""Predicted""], label=""Predicted"", color=""#d62728"", lw=1.5, alpha=0.9)
        ax1.set_title(f""{model_name} — Time Series\nSite: {site}"")
        ax1.set_xlabel(""Time"")
        ax1.set_ylabel(output_col)
        ax1.legend(loc=""best"")

        baseline_periods = _normalize_periods(baseline_period)
        projection_periods = _normalize_periods(projection_period)

        def _shade(ax, periods, color, label):
            first = True
            for (start, end) in periods:
                ax.axvspan(start, end, color=color, alpha=0.15, label=label if first else None)
                first = False

        _shade(ax1, baseline_periods, ""#2ca02c"", ""Baseline"")
        _shade(ax1, projection_periods, ""#ff7f0e"", ""Projection"")

        # Try to mark transition if contiguous and non-overlapping
        all_marks = []
        for (s, e) in baseline_periods + projection_periods:
            all_marks.extend([s, e])
        for t in sorted(set(all_marks)):
            if df.index.min() <= t <= df.index.max():
                ax1.axvline(t, color=""k"", lw=0.7, ls="":"")

        plt.tight_layout()
        return fig, axes"
323204,SoftwareDefinedBuildings/XBOS,apps/data_analysis/XBOS_data_analytics/Import_Data.py,XBOS_data_analytics.Import_Data.Import_Data,"import os
import pandas as pd
import numpy as np
import glob

class Import_Data:
    """""" This class imports data from csv files """"""

    def __init__(self):
        """""" Constructor.

        This class stores the imported data.

        """"""
        self.data = pd.DataFrame()

    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        """""" Imports csv file(s) and stores the result in data.

        Note
        ----
        1. If folder exists out of current directory, folder_name should contain correct regex
        2. Assuming there's no file called ""\\*.csv""

        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '\\*', i.e. all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.', i.e. current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe

        """"""
        if isinstance(file_name, str) and isinstance(folder_name, str):
            try:
                self.data = self._load_csv(file_name, folder_name, head_row, index_col, convert_col, concat_files)
            except Exception as e:
                raise e
        elif isinstance(file_name, list) and isinstance(folder_name, str):
            for i, file in enumerate(file_name):
                if isinstance(head_row, list):
                    _head_row = head_row[i]
                else:
                    _head_row = head_row
                if isinstance(index_col, list):
                    _index_col = index_col[i]
                else:
                    _index_col = index_col
                try:
                    data_tmp = self._load_csv(file, folder_name, _head_row, _index_col, convert_col, concat_files)
                    if concat_files:
                        self.data = self.data.append(data_tmp, ignore_index=False, verify_integrity=False)
                    else:
                        self.data = self.data.join(data_tmp, how='outer')
                except Exception as e:
                    raise e
        else:
            raise NotImplementedError(""Filename and Folder name can't both be of type list."")

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        """""" Load single csv file.

        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '*' - all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.' - current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe

        Returns
        -------
        pd.DataFrame()
            Dataframe containing csv data

        """"""
        if file_name == '*':
            if not os.path.isdir(folder_name):
                raise OSError('Folder does not exist.')
            else:
                file_name_list = sorted(glob.glob(folder_name + '*.csv'))
                if not file_name_list:
                    raise OSError('Either the folder does not contain any csv files or invalid folder provided.')
                else:
                    self.import_csv(file_name=file_name_list, head_row=head_row, index_col=index_col, convert_col=convert_col, concat_files=concat_files)
                    return self.data
        elif not os.path.isdir(folder_name):
            raise OSError('Folder does not exist.')
        else:
            path = os.path.join(folder_name, file_name)
            if head_row > 0:
                data = pd.read_csv(path, index_col=index_col, skiprows=[i for i in range(head_row - 1)])
            else:
                data = pd.read_csv(path, index_col=index_col)
            try:
                data.index = pd.to_datetime(data.index, format='%m/%d/%y %H:%M')
            except:
                data.index = pd.to_datetime(data.index, dayfirst=False, infer_datetime_format=True)
        if convert_col:
            for col in data.columns:
                if data[col].dtype != np.number:
                    data[col] = pd.to_numeric(data[col], errors='coerce')
        return data","
class Import_Data:
    
    def __init__(self):
        
        pass
    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        
        pass

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        
        pass",snippet_263,4,"import os
import glob
import pandas as pd


class Import_Data:
    def __init__(self):
        pass

    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        paths = self._resolve_paths(file_name, folder_name)
        if not paths:
            raise FileNotFoundError(f""No CSV files matched for file_name={file_name!r} in folder_name={folder_name!r}"")

        dataframes = {}
        for p in paths:
            df = self._load_csv(p, folder_name, head_row, index_col, convert_col, concat_files)
            dataframes[os.path.basename(p)] = df

        if concat_files:
            if not dataframes:
                return pd.DataFrame()
            return pd.concat(list(dataframes.values()), axis=0, ignore_index=False, sort=False)

        if len(dataframes) == 1:
            return next(iter(dataframes.values()))
        return dataframes

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        path = file_name if os.path.isabs(file_name) else os.path.join(folder_name, file_name)

        header = head_row if head_row is not None else None
        idx_col = index_col if index_col is not None else None

        try:
            df = pd.read_csv(path, header=header, index_col=idx_col)
        except UnicodeDecodeError:
            df = pd.read_csv(path, header=header, index_col=idx_col, encoding='latin-1')

        if convert_col:
            # Strip whitespace from string columns
            obj_cols = df.select_dtypes(include=['object']).columns
            for col in obj_cols:
                df[col] = df[col].map(lambda x: x.strip() if isinstance(x, str) else x)

            # Try to convert to numeric where possible
            for col in df.columns:
                if pd.api.types.is_object_dtype(df[col]):
                    converted = pd.to_numeric(df[col], errors='ignore')
                    df[col] = converted

        return df

    def _resolve_paths(self, file_name, folder_name):
        if isinstance(file_name, (list, tuple)):
            paths = []
            for f in file_name:
                if any(ch in str(f) for ch in ['*', '?', '[']):
                    pattern = f
                    if not os.path.isabs(pattern):
                        pattern = os.path.join(folder_name, pattern if pattern.endswith('.csv') or '.' in os.path.basename(pattern) else pattern + '.csv')
                    paths.extend(glob.glob(pattern))
                else:
                    p = f if os.path.isabs(f) else os.path.join(folder_name, f)
                    if os.path.isdir(p):
                        p = os.path.join(p, '*.csv')
                        paths.extend(glob.glob(p))
                    else:
                        if not os.path.splitext(p)[1]:
                            p = p + '.csv'
                        if os.path.exists(p):
                            paths.append(p)
            return sorted(set(paths))

        # Single string case
        if file_name in (None, '', '*'):
            pattern = os.path.join(folder_name, '*.csv')
            return sorted(glob.glob(pattern))

        if any(ch in str(file_name) for ch in ['*', '?', '[']):
            pattern = file_name if os.path.isabs(file_name) else os.path.join(folder_name, file_name)
            if not os.path.splitext(pattern)[1] or pattern.endswith('.*'):
                # Ensure it targets csv if no extension specified
                if pattern.endswith('.*'):
                    pattern = pattern[:-2] + 'csv'
                elif not pattern.lower().endswith('.csv'):
                    if pattern.endswith('*'):
                        pattern = pattern + '.csv'
            return sorted(glob.glob(pattern))

        # Plain file name
        p = file_name if os.path.isabs(file_name) else os.path.join(folder_name, file_name)
        if not os.path.splitext(p)[1]:
            p = p + '.csv'
        return [p] if os.path.exists(p) else []"
323208,SoftwareDefinedBuildings/XBOS,apps/data_analysis/XBOS_data_analytics/Plot_Data.py,XBOS_data_analytics.Plot_Data.Plot_Data,"import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

class Plot_Data:
    """""" This class contains functions for displaying various plots.

    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.

    """"""
    count = 1

    def __init__(self, figsize=(18, 5)):
        """""" Constructor.

        Parameters
        ----------
        figsize : tuple
            Size of figure.

        """"""
        self.figsize = figsize

    def correlation_plot(self, data):
        """""" Create heatmap of Pearson's correlation coefficient.

        Parameters
        ----------
        data    : pd.DataFrame()
            Data to display.

        Returns
        -------
        matplotlib.figure
            Heatmap.

        """"""
        fig = plt.figure(Plot_Data.count)
        corr = data.corr()
        ax = sns.heatmap(corr)
        Plot_Data.count += 1
        return fig

    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        """""" Create baseline and projection plots.

        Parameters
        ----------
        y_true              : pd.Series()
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.Dataframe()
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.

        Returns
        -------
        matplotlib.figure
            Baseline plot

        """"""
        fig = plt.figure(Plot_Data.count)
        if projection_period:
            nrows = len(baseline_period) + len(projection_period) / 2
        else:
            nrows = len(baseline_period) / 2
        base_df = pd.DataFrame()
        base_df['y_true'] = y_true
        base_df['y_pred'] = y_pred
        ax1 = fig.add_subplot(nrows, 1, 1)
        base_df.plot(ax=ax1, figsize=self.figsize, title='Baseline Period ({}-{}). \nBest Model: {}. \nBaseline Adj R2: {}. \nSite: {}.'.format(baseline_period[0], baseline_period[1], model_name, adj_r2, site))
        if projection_period:
            num_plot = 2
            for i in range(0, len(projection_period), 2):
                ax = fig.add_subplot(nrows, 1, num_plot)
                period = slice(projection_period[i], projection_period[i + 1])
                project_df = pd.DataFrame()
                try:
                    project_df['y_true'] = data.loc[period, output_col]
                    project_df['y_pred'] = model.predict(data.loc[period, input_col])
                    project_df['y_pred'][project_df['y_pred'] < 0] = 0
                    project_df.plot(ax=ax, figsize=self.figsize, title='Projection Period ({}-{})'.format(projection_period[i], projection_period[i + 1]))
                    num_plot += 1
                    fig.tight_layout()
                    Plot_Data.count += 1
                    return (fig, project_df['y_true'], project_df['y_pred'])
                except:
                    raise SystemError('If projecting into the future, please specify project_ind_col that has data available                                         in the future time period requested.')
        return (fig, None, None)","
class Plot_Data:
    
    def __init__(self, figsize=(18, 5)):
        
        pass
    def correlation_plot(self, data):
        
        pass

    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        
        pass",snippet_264,4,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import gridspec
from typing import Optional, Sequence, Tuple, Union


class Plot_Data:
    def __init__(self, figsize: Tuple[int, int] = (18, 5)):
        self.figsize = figsize
        try:
            sns.set_style(""whitegrid"")
        except Exception:
            pass

    def correlation_plot(self, data: Union[pd.DataFrame, np.ndarray, dict]):
        if isinstance(data, dict):
            data = pd.DataFrame(data)
        elif isinstance(data, np.ndarray):
            data = pd.DataFrame(data)

        if not isinstance(data, pd.DataFrame):
            raise TypeError(""data must be a pandas DataFrame, numpy array, or dict-like."")

        if data.empty:
            raise ValueError(""data is empty."")

        corr = data.corr(numeric_only=True)

        fig, ax = plt.subplots(figsize=self.figsize)
        sns.heatmap(corr, annot=True, fmt="".2f"", cmap=""coolwarm"", square=True, cbar=True, ax=ax)
        ax.set_title(""Correlation Matrix"")
        fig.tight_layout()
        return fig, ax

    def baseline_projection_plot(
        self,
        y_true: Union[pd.Series, np.ndarray],
        y_pred: Union[pd.Series, np.ndarray],
        baseline_period: Optional[Tuple[Union[str, pd.Timestamp], Union[str, pd.Timestamp]]],
        projection_period: Optional[Tuple[Union[str, pd.Timestamp], Union[str, pd.Timestamp]]],
        model_name: str,
        adj_r2: float,
        data: pd.DataFrame,
        input_col: Optional[Sequence[str]],
        output_col: str,
        model,
        site: Optional[str] = None,
    ):
        def _to_ts(v):
            if v is None:
                return None
            if isinstance(v, (pd.Timestamp, np.datetime64)):
                return pd.Timestamp(v)
            return pd.to_datetime(v, errors=""coerce"")

        if isinstance(data, pd.Series):
            data = data.to_frame(name=output_col)

        if not isinstance(data, pd.DataFrame):
            raise TypeError(""data must be a pandas DataFrame."")

        if output_col not in data.columns:
            raise ValueError(f""output_col '{output_col}' not found in data."")

        # Prepare index and alignment
        x_index = data.index
        if not isinstance(x_index, (pd.DatetimeIndex, pd.PeriodIndex)):
            # Try to convert to datetime if possible
            try:
                x_index = pd.to_datetime(x_index, errors=""coerce"")
            except Exception:
                pass

        # Align y_true and y_pred to data index if they are Series with their own index
        def _align_to_data(series_like):
            if isinstance(series_like, pd.Series):
                return series_like.reindex(data.index)
            arr = np.asarray(series_like).reshape(-1)
            if len(arr) != len(data):
                raise ValueError(""Length of y arrays must match length of data when not pandas Series."")
            return pd.Series(arr, index=data.index)

        y_true_s = _align_to_data(y_true)
        y_pred_s = _align_to_data(y_pred)

        # Define periods
        b_start = _to_ts(baseline_period[0]) if baseline_period else None
        b_end = _to_ts(baseline_period[1]) if baseline_period else None
        p_start = _to_ts(projection_period[0]) if projection_period else None
        p_end = _to_ts(projection_period[1]) if projection_period else None

        # Build masks
        idx = pd.Index(x_index)
        if isinstance(idx, pd.DatetimeIndex):
            b_mask = pd.Series(True, index=idx)
            p_mask = pd.Series(False, index=idx)

            if b_start is not None:
                b_mask &= idx >= b_start
            if b_end is not None:
                b_mask &= idx <= b_end

            if p_start is not None:
                p_mask |= idx >= p_start
            if p_end is not None:
                p_mask &= idx <= p_end if p_mask.any() else (idx <= p_end)

            # If projection not given, infer as not baseline
            if projection_period is None:
                p_mask = ~b_mask
        else:
            # Non-datetime index: use full baseline, empty projection by default
            b_mask = pd.Series(True, index=idx)
            p_mask = pd.Series(False, index=idx)

        # Determine if we can compute feature importance
        feature_importances = None
        feature_names = None
        if input_col is not None:
            feature_names = list(input_col)
            if hasattr(model, ""feature_importances_""):
                try:
                    vals = np.asarray(model.feature_importances_).reshape(-1)
                    if len(vals) == len(feature_names):
                        feature_importances = vals
                except Exception:
                    pass
            elif hasattr(model, ""coef_""):
                try:
                    coef = np.asarray(model.coef_)
                    if coef.ndim > 1:
                        coef = coef.ravel()
                    if len(coef) == len(feature_names):
                        feature_importances = np.abs(coef)
                except Exception:
                    pass

        # Layout
        if feature_importances is None:
            ncols = 2
            width_ratios = [2.5, 1.5]
        else:
            ncols = 3
            width_ratios = [2.5, 1.2, 1.3]

        fig = plt.figure(figsize=self.figsize)
        gs = gridspec.GridSpec(1, ncols, width_ratios=width_ratios, figure=fig, wspace=0.25)

        # Time series plot
        ax_ts = fig.add_subplot(gs[0, 0])

        # Actuals
        ax_ts.plot(x_index, data[output_col], color=""black"", linewidth=1.2, label=""Actual"")

        # Predictions on baseline
        if b_mask.any():
            ax_ts.plot(x_index[b_mask], y_pred_s[b_mask], color=""#1f77b4"", linewidth=1.5, label=""Predicted (baseline)"")

        # Predictions on projection
        if p_mask.any():
            ax_ts.plot(x_index[p_mask], y_pred_s[p_mask], color=""#d62728"", linewidth=1.5, linestyle=""--"", label=""Predicted (projection)"")

        # Shading
        def _shade(ax, mask, color, label):
            if not mask.any():
                return
            # Find contiguous intervals
            m = mask.astype(int).values
            idx_arr = np.arange(len(mask))
            starts = []
            ends = []
            in_block = False
            for i, val in enumerate(m):
                if val and not in_block:
                    in_block = True
                    starts.append(i)
                if in_block and (i == len(m) - 1 or not m[i + 1]):
                    ends.append(i)
                    in_block = False
            for s, e in zip(starts, ends):
                ax.axvspan(x_index[s], x_index[e], color=color, alpha=0.08, label=label)
                label = None  # only label first patch

        _shade(ax_ts, b_mask, ""#1f77b4"", ""Baseline period"")
        _shade(ax_ts, p_mask, ""#d62728"", ""Projection period"")

        title_site = f"" | Site: {site}"" if site else """"
        ax_ts.set_title(f""{model_name}{title_site} | Adj R^2 = {adj_r2:.3f}"")
        ax_ts.set_xlabel(""Time"")
        ax_ts.set_ylabel(output_col)
        ax_ts.legend(loc=""best"")
        ax_ts.grid(True, alpha=0.3)

        # Parity plot
        ax_par = fig.add_subplot(gs[0, 1])
        ax_par.scatter(y_true_s, y_pred_s, s=14, alpha=0.6, color=""#2ca02c"", edgecolor=""none"", label=""Samples"")
        min_v = np.nanmin([np.nanmin(y_true_s.values), np.nanmin(y_pred_s.values)])
        max_v = np.nanmax([np.nanmax(y_true_s.values), np.nanmax(y_pred_s.values)])
        if np.isfinite(min_v) and np.isfinite(max_v):
            pad = 0.02 * (max_v - min_v if max_v > min_v else 1.0)
            ax_par.plot([min_v - pad, max_v + pad], [min_v - pad, max_v + pad], color=""black"", linestyle=""--"", linewidth=1, label=""1:1"")
            ax_par.set_xlim(min_v - pad, max_v + pad)
            ax_par.set_ylim(min_v - pad, max_v + pad)

        # Metrics
        with np.errstate(invalid=""ignore""):
            resid = y_true_s.values - y_pred_s.values
            rmse = float(np.sqrt(np.nanmean(resid**2)))
            mae = float(np.nanmean(np.abs(resid)))
            # Unadjusted R^2 for parity scatter
            y_mean = np.nanmean(y_true_s.values)
            ss_tot = np.nansum((y_true_s.values - y_mean) ** 2)
            ss_res = np.nansum(resid ** 2)
            r2 = 1.0 - ss_res / ss_tot if ss_tot > 0 else np.nan

        ax_par.set_title(""Parity Plot"")
        ax_par.set_xlabel(""Observed"")
        ax_par.set_ylabel(""Predicted"")
        ax_par.legend(loc=""best"")
        txt = f""R²={r2:.3f}\nRMSE={rmse:.3f}\nMAE={mae:.3f}\nAdj R²={adj_r2:.3f}""
        ax_par.text(0.05, 0.95, txt, transform=ax_par.transAxes, va=""top"", ha=""left"",
                    bbox=dict(boxstyle=""round,pad=0.3"", facecolor=""white"", alpha=0.7, edgecolor=""gray""))

        axes = {""time_series"": ax_ts, ""parity"": ax_par}

        # Feature importances
        if feature_importances is not None and feature_names is not None and len(feature_importances) == len(feature_names):
            ax_imp = fig.add_subplot(gs[0, 2])
            order = np.argsort(feature_importances)
            fi_sorted = feature_importances[order]
            fn_sorted = [feature_names[i] for i in order]
            ax_imp.barh(fn_sorted, fi_sorted, color=""#9467bd"", alpha=0.9)
            ax_imp.set_title(""Feature Importance"")
            ax_imp.set_xlabel(""Importance"")
            fig.tight_layout()
            axes[""feature_importance""] = ax_imp
        else:
            fig.tight_layout()

        return fig, axes"
339242,pawelad/pymonzo,pawelad_pymonzo/src/pymonzo/client.py,pymonzo.client.MonzoAPI,"from pymonzo.exceptions import MonzoAPIError, NoSettingsFile
from authlib.integrations.base_client import OAuthError
from pymonzo.webhooks import WebhooksResource
from pathlib import Path
from pymonzo.attachments import AttachmentsResource
from json import JSONDecodeError
from pymonzo.transactions import TransactionsResource
from pymonzo.pots import PotsResource
from pymonzo.whoami import WhoAmIResource
from pymonzo.utils import get_authorization_response_url
from urllib.parse import urlparse
from typing import Any, Optional
from pymonzo.balance import BalanceResource
import webbrowser
from pymonzo.feed import FeedResource
from pymonzo.accounts import AccountsResource
from authlib.integrations.httpx_client import OAuth2Client
from pymonzo.settings import PyMonzoSettings

class MonzoAPI:
    """"""Monzo public API client.

    To use it, you need to create a new OAuth client in [Monzo Developer Portal].
    The `Redirect URLs` should be set to `http://localhost:6600/pymonzo` and
    `Confidentiality` should be set to `Confidential` if you'd like to automatically
    refresh the access token when it expires.

    You can now use `Client ID` and `Client secret` in [`pymonzo.MonzoAPI.authorize`][]
    to finish the OAuth 2 'Authorization Code Flow' and get the API access token
    (which is by default saved to disk and refreshed when expired).

    [Monzo Developer Portal]: https://developers.monzo.com/

    Note:
        Monzo API docs: https://docs.monzo.com/
    """"""
    api_url = 'https://api.monzo.com'
    authorization_endpoint = 'https://auth.monzo.com/'
    token_endpoint = 'https://api.monzo.com/oauth2/token'
    settings_path = Path.home() / '.pymonzo'

    def __init__(self, access_token: Optional[str]=None) -> None:
        """"""Initialize Monzo API client and mount all resources.

        It expects [`pymonzo.MonzoAPI.authorize`][] to be called beforehand, so
        it can load the local settings file containing the API access token. You
        can also explicitly pass the `access_token`, but it won't be able to
        automatically refresh it once it expires.

        Arguments:
            access_token: OAuth access token. You can obtain it (and by default, save
                it to disk, so it can refresh automatically) by running
                [`pymonzo.MonzoAPI.authorize`][]. Alternatively, you can get a
                temporary access token from the [Monzo Developer Portal].

                [Monzo Developer Portal]: https://developers.monzo.com/

        Raises:
            NoSettingsFile: When the access token wasn't passed explicitly and the
                settings file couldn't be loaded.

        """"""
        if access_token:
            self._settings = PyMonzoSettings(token={'access_token': access_token})
        else:
            try:
                self._settings = PyMonzoSettings.load_from_disk(self.settings_path)
            except (FileNotFoundError, JSONDecodeError) as e:
                raise NoSettingsFile('No settings file found. You need to either run `MonzoAPI.authorize(client_id, client_secret)` to get the authorization token (and save it to disk), or explicitly pass the `access_token`.') from e
        self.session = OAuth2Client(client_id=self._settings.client_id, client_secret=self._settings.client_secret, token=self._settings.token, authorization_endpoint=self.authorization_endpoint, token_endpoint=self.token_endpoint, token_endpoint_auth_method='client_secret_post', update_token=self._update_token, base_url=self.api_url)
        self.whoami = WhoAmIResource(client=self).whoami
        '\n        Mounted Monzo `whoami` endpoint. For more information see\n        [`pymonzo.whoami.WhoAmIResource.whoami`][].\n        '
        self.accounts = AccountsResource(client=self)
        '\n        Mounted Monzo `accounts` resource. For more information see\n        [`pymonzo.accounts.AccountsResource`][].\n        '
        self.attachments = AttachmentsResource(client=self)
        '\n        Mounted Monzo `attachments` resource. For more information see\n        [`pymonzo.attachments.AttachmentsResource`][].\n        '
        self.balance = BalanceResource(client=self)
        '\n        Mounted Monzo `balance` resource. For more information see\n        [`pymonzo.balance.BalanceResource`][].\n        '
        self.feed = FeedResource(client=self)
        '\n        Mounted Monzo `feed` resource. For more information see\n        [`pymonzo.feed.FeedResource`][].\n        '
        self.pots = PotsResource(client=self)
        '\n        Mounted Monzo `pots` resource. For more information see\n        [`pymonzo.pots.PotsResource`][].\n        '
        self.transactions = TransactionsResource(client=self)
        '\n        Mounted Monzo `transactions` resource. For more information see\n        [`pymonzo.transactions.TransactionsResource`][].\n        '
        self.webhooks = WebhooksResource(client=self)
        '\n        Mounted Monzo `webhooks` resource. For more information see\n        [`pymonzo.webhooks.WebhooksResource`][].\n        '

    @classmethod
    def authorize(cls, client_id: str, client_secret: str, *, save_to_disk: bool=True, redirect_uri: str='http://localhost:6600/pymonzo') -> dict:
        """"""Use OAuth 2 'Authorization Code Flow' to get Monzo API access token.

        By default, it also saves the token to disk, so it can be loaded during
        [`pymonzo.MonzoAPI`][] initialization.

        Note:
            Monzo API docs: https://docs.monzo.com/#authentication

        Arguments:
            client_id: OAuth client ID.
            client_secret: OAuth client secret.
            save_to_disk: Whether to save the token to disk.
            redirect_uri: Redirect URI specified in OAuth client.

        Returns:
            OAuth token.
        """"""
        client = OAuth2Client(client_id=client_id, client_secret=client_secret, redirect_uri=redirect_uri, token_endpoint_auth_method='client_secret_post')
        url, state = client.create_authorization_url(cls.authorization_endpoint)
        print(f'Please visit this URL to authorize: {url}')
        webbrowser.open(url)
        parsed_url = urlparse(redirect_uri)
        assert parsed_url.hostname is not None
        assert parsed_url.port is not None
        authorization_response = get_authorization_response_url(host=parsed_url.hostname, port=parsed_url.port)
        try:
            token = client.fetch_token(url=cls.token_endpoint, authorization_response=authorization_response)
        except (OAuthError, JSONDecodeError) as e:
            raise MonzoAPIError('Error while fetching API access token') from e
        if save_to_disk:
            settings = PyMonzoSettings(client_id=client_id, client_secret=client_secret, token=token)
            settings.save_to_disk(cls.settings_path)
        return token

    def _update_token(self, token: dict, **kwargs: Any) -> None:
        """"""Update settings with refreshed access token and save it to disk.

        Arguments:
            token: OAuth access token.
            **kwargs: Extra kwargs.
        """"""
        self._settings.token = token
        if self.settings_path.exists():
            self._settings.save_to_disk(self.settings_path)","
class MonzoAPI:
    
    def __init__(self, access_token: Optional[str]=None) -> None:
        
        pass
    @classmethod
    def authorize(cls, client_id: str, client_secret: str, *, save_to_disk: bool=True, redirect_uri: str='http://localhost:6600/pymonzo') -> dict:
        
        pass

    def _update_token(self, token: dict, **kwargs: Any) -> None:
        
        pass",snippet_268,4,"from __future__ import annotations

import json
import os
import threading
import time
import webbrowser
from http.server import BaseHTTPRequestHandler, HTTPServer
from typing import Any, Optional
from urllib.parse import parse_qs, urlparse, urlencode

import requests
import secrets


class MonzoAPI:
    _DEFAULT_TOKEN_PATH = os.path.join(os.path.expanduser(""~""), "".pymonzo_token.json"")
    _AUTH_URL = ""https://auth.monzo.com/""
    _TOKEN_URL = ""https://api.monzo.com/oauth2/token""

    def __init__(self, access_token: Optional[str] = None) -> None:
        self.token: dict[str, Any] = {}
        self.access_token: Optional[str] = None
        self._token_path = self._DEFAULT_TOKEN_PATH

        if access_token:
            self._update_token({""access_token"": access_token, ""created_at"": int(time.time())})
        else:
            # Attempt to load token from disk
            try:
                if os.path.isfile(self._token_path):
                    with open(self._token_path, ""r"", encoding=""utf-8"") as f:
                        token = json.load(f)
                        if isinstance(token, dict) and ""access_token"" in token:
                            self._update_token(token)
            except Exception:
                pass

    @classmethod
    def authorize(
        cls,
        client_id: str,
        client_secret: str,
        *,
        save_to_disk: bool = True,
        redirect_uri: str = ""http://localhost:6600/pymonzo"",
    ) -> dict:
        parsed = urlparse(redirect_uri)
        if parsed.scheme != ""http"" or parsed.hostname not in (""localhost"", ""127.0.0.1""):
            raise ValueError(""redirect_uri must be http://localhost:<port>/<path>"")

        state = secrets.token_urlsafe(16)
        params = {
            ""client_id"": client_id,
            ""redirect_uri"": redirect_uri,
            ""response_type"": ""code"",
            ""state"": state,
        }
        auth_url = f""{cls._AUTH_URL}?{urlencode(params)}""

        result: dict[str, Any] = {""code"": None, ""state"": None, ""error"": None}

        class _Handler(BaseHTTPRequestHandler):
            def log_message(self, format: str, *args: Any) -> None:
                return

            def do_GET(self) -> None:
                query = parse_qs(urlparse(self.path).query)
                code = (query.get(""code"") or [None])[0]
                recv_state = (query.get(""state"") or [None])[0]
                error = (query.get(""error"") or [None])[0]

                result[""code""] = code
                result[""state""] = recv_state
                result[""error""] = error

                self.send_response(200 if code and not error else 400)
                self.send_header(""Content-Type"", ""text/html; charset=utf-8"")
                self.end_headers()
                if error:
                    self.wfile.write(f""<h1>Authorization failed</h1><p>{error}</p>"".encode(""utf-8""))
                else:
                    self.wfile.write(b""<h1>Authorization complete</h1><p>You may close this tab.</p>"")

        port = parsed.port or 80
        server = HTTPServer((parsed.hostname, port), _Handler)

        server_thread = threading.Thread(target=server.serve_forever, daemon=True)
        server_thread.start()

        try:
            webbrowser.open(auth_url)
        except Exception:
            pass

        deadline = time.time() + 300
        try:
            while time.time() < deadline and result[""code""] is None and result[""error""] is None:
                time.sleep(0.1)
        finally:
            server.shutdown()
            server.server_close()

        if result[""error""]:
            raise RuntimeError(f""Authorization error: {result['error']}"")
        if result[""code""] is None:
            raise TimeoutError(""Authorization timed out waiting for redirect with code"")
        if result[""state""] != state:
            raise RuntimeError(""State mismatch during authorization"")

        data = {
            ""grant_type"": ""authorization_code"",
            ""client_id"": client_id,
            ""client_secret"": client_secret,
            ""redirect_uri"": redirect_uri,
            ""code"": result[""code""],
        }
        resp = requests.post(cls._TOKEN_URL, data=data, timeout=30)
        resp.raise_for_status()
        token = resp.json()

        # Normalize fields
        if ""created_at"" not in token:
            token[""created_at""] = int(time.time())

        if save_to_disk:
            try:
                with open(cls._DEFAULT_TOKEN_PATH, ""w"", encoding=""utf-8"") as f:
                    json.dump(token, f, ensure_ascii=False, indent=2)
            except Exception:
                pass

        return token

    def _update_token(self, token: dict, **kwargs: Any) -> None:
        self.token.update(token or {})
        self.access_token = self.token.get(""access_token"")

        if ""created_at"" not in self.token:
            self.token[""created_at""] = int(time.time())

        if ""expires_in"" in self.token and isinstance(self.token[""expires_in""], (int, float)):
            try:
                self.token[""expires_at""] = int(self.token[""created_at""]) + int(self.token[""expires_in""])
            except Exception:
                pass

        if kwargs.get(""save"", False):
            try:
                with open(self._token_path, ""w"", encoding=""utf-8"") as f:
                    json.dump(self.token, f, ensure_ascii=False, indent=2)
            except Exception:
                pass"
339313,chaoss/grimoirelab-kingarthur,arthur/tasks.py,arthur.tasks._TaskConfig,"import re
from grimoirelab_toolkit.introspect import find_class_properties

class _TaskConfig:
    """"""Abstract class to store task configuration options.

    This class defines how to store specific task configuration
    arguments such as scheduling or archiving options. It is not
    meant to be instantiated on its own.

    Configuration options must be defined using `property` and `setter`
    decorators. Setters must check whether the given value is valid
    or not. When it is invalid, a `ValueError` exception should be
    raised. The rationale behind this is to use these methods as
    parsers when `from_dict` class method is called. It will create
    a new instance of the subclass passing its properties from a
    dictionary.
    """"""
    KW_ARGS_ERROR_REGEX = re.compile(""^.+ got an unexpected keyword argument '(.+)'$"")

    def to_dict(self):
        """"""Returns a dict with the representation of this task configuration object.""""""
        properties = find_class_properties(self.__class__)
        config = {name: self.__getattribute__(name) for name, _ in properties}
        return config

    @classmethod
    def from_dict(cls, config):
        """"""Create an configuration object from a dictionary.

        Key,value pairs will be used to initialize a task configuration
        object. If 'config' contains invalid configuration parameters
        a `ValueError` exception will be raised.

        :param config: dictionary used to create an instance of this object

        :returns: a task config instance

        :raises ValueError: when an invalid configuration parameter is found
        """"""
        try:
            obj = cls(**config)
        except TypeError as e:
            m = cls.KW_ARGS_ERROR_REGEX.match(str(e))
            if m:
                raise ValueError(""unknown '%s' task config parameter"" % m.group(1))
            else:
                raise e
        else:
            return obj","
class _TaskConfig:
    
    def to_dict(self):
        
        pass
    @classmethod
    def from_dict(cls, config):
        
        pass",snippet_269,3,"class _TaskConfig:
    
    def to_dict(self):
        return {k: v for k, v in self.__dict__.items() if k.isidentifier()}
    
    @classmethod
    def from_dict(cls, config):
        if config is None:
            return cls()
        if not isinstance(config, dict):
            raise TypeError(""config must be a dict"")
        inst = cls()
        for k, v in config.items():
            if isinstance(k, str) and k.isidentifier():
                setattr(inst, k, v)
        return inst"
341459,wright-group/WrightTools,wright-group_WrightTools/WrightTools/diagrams/WMEL.py,WrightTools.diagrams.WMEL.Subplot,"import numpy as np

class Subplot:
    """"""Subplot containing WMEL.""""""

    def __init__(self, ax, energies, number_of_interactions=4, title='', title_font_size=16, state_names=None, virtual=[None], state_font_size=14, state_text_buffer=0.5, label_side='left'):
        """"""Subplot.

        Parameters
        ----------
        ax : matplotlib axis
            The axis.
        energies : 1D array-like
            Energies (scaled between 0 and 1)
        number_of_interactions : integer
            Number of interactions in diagram.
        title : string (optional)
            Title of subplot. Default is empty string.
        state_names: list of str (optional)
            list of the names of the states
        virtual: list of ints (optional)
            list of indexes of any vitual energy states
        state_font_size: numtype (optional)
            font size for the state lables
        state_text_buffer: numtype (optional)
            space between the energy level bars and the state labels
        """"""
        self.ax = ax
        self.energies = energies
        self.interactions = number_of_interactions
        self.state_names = state_names
        for i in range(len(self.energies)):
            if i in virtual:
                linestyle = '--'
            else:
                linestyle = '-'
            self.ax.axhline(self.energies[i], color='k', linewidth=2, ls=linestyle, zorder=5)
        if isinstance(state_names, list):
            for i in range(len(self.energies)):
                if label_side == 'left':
                    ax.text(-state_text_buffer, energies[i], state_names[i], fontsize=state_font_size, verticalalignment='center', horizontalalignment='center')
                elif label_side == 'right':
                    ax.text(1 + state_text_buffer, energies[i], state_names[i], fontsize=state_font_size, verticalalignment='center', horizontalalignment='center')
        self.x_pos = np.linspace(0, 1, number_of_interactions)
        self.ax.set_xlim(-0.1, 1.1)
        self.ax.set_ylim(-0.01, 1.01)
        self.ax.axis('off')
        self.ax.set_title(title, fontsize=title_font_size)

    def add_arrow(self, index, between, kind, label='', head_length=10, head_aspect=1, font_size=14, color='k'):
        """"""Add an arrow to the WMEL diagram.

        Parameters
        ----------
        index : integer
            The interaction, or start and stop interaction for the arrow.
        between : 2-element iterable of integers
            The inital and final state of the arrow
        kind : {'ket', 'bra', 'outbra', 'outket'}
            The kind of interaction.
        label : string (optional)
            Interaction label. Default is empty string.
        head_length: number (optional)
            size of arrow head
        font_size : number (optional)
            Label font size. Default is 14.
        color : matplotlib color (optional)
            Arrow color. Default is black.

        Returns
        -------
        [line,arrow_head,text]
        """"""
        if hasattr(index, 'index'):
            x_pos = list(index)
        else:
            x_pos = [index] * 2
        x_pos = [np.linspace(0, 1, self.interactions)[i] for i in x_pos]
        y_pos = [self.energies[between[0]], self.energies[between[1]]]
        arrow_length = self.energies[between[1]] - self.energies[between[0]]
        arrow_end = self.energies[between[1]]
        if arrow_length > 0:
            direction = 1
        elif arrow_length < 0:
            direction = -1
        else:
            raise ValueError('between invalid!')
        length = abs(y_pos[0] - y_pos[1])
        if kind == 'ket':
            line = self.ax.plot(x_pos, y_pos, linestyle='-', color=color, linewidth=2, zorder=9)
        elif kind == 'bra':
            line = self.ax.plot(x_pos, y_pos, linestyle='--', color=color, linewidth=2, zorder=9)
        elif kind == 'out':
            yi = np.linspace(y_pos[0], y_pos[1], 100)
            xi = np.sin((yi - y_pos[0]) * int(1 / length * 20) * 2 * np.pi * length) / 40 + x_pos[0]
            line = self.ax.plot(xi[:-5], yi[:-5], linestyle='-', color=color, linewidth=2, solid_capstyle='butt', zorder=9)
        elif kind == 'outbra':
            yi = np.linspace(y_pos[0], y_pos[1], 100)
            xi = np.sin((yi - y_pos[0]) * int(1 / length * 20) * 2 * np.pi * length) / 40 + x_pos[0]
            counter = 0
            while counter - 13 <= len(yi):
                subyi = yi[counter:counter + 15]
                subxi = xi[counter:counter + 15]
                line = self.ax.plot(subxi[:-5], subyi[:-5], linestyle='-', color=color, linewidth=2, solid_capstyle='butt', zorder=9)
                counter += 13
        else:
            raise ValueError(""kind is not 'ket', 'bra', 'out' or 'outbra'."")
        dx = x_pos[1] - x_pos[0]
        dy = y_pos[1] - y_pos[0]
        xytext = (x_pos[1] - dx * 0.01, y_pos[1] - dy * 0.01)
        annotation = self.ax.annotate('', xy=(x_pos[1], y_pos[1]), xytext=xytext, arrowprops=dict(fc=color, ec=color, shrink=0, headwidth=head_length * head_aspect, headlength=head_length, linewidth=0, zorder=10), size=25)
        text = self.ax.text(np.mean(x_pos), -0.15, label, fontsize=font_size, horizontalalignment='center')
        return (line, annotation.arrow_patch, text)","
class Subplot:
    
    def __init__(self, ax, energies, number_of_interactions=4, title='', title_font_size=16, state_names=None, virtual=[None], state_font_size=14, state_text_buffer=0.5, label_side='left'):
        
        pass
    def add_arrow(self, index, between, kind, label='', head_length=10, head_aspect=1, font_size=14, color='k'):
        
        pass",snippet_271,3,"import matplotlib.pyplot as plt
from matplotlib.patches import FancyArrowPatch
import numpy as np

class Subplot:
    
    def __init__(self, ax, energies, number_of_interactions=4, title='', title_font_size=16, state_names=None, virtual=[None], state_font_size=14, state_text_buffer=0.5, label_side='left'):
        self.ax = ax
        self.energies = list(energies)
        self.n_states = len(self.energies)
        self.n_interactions = int(number_of_interactions)
        self.title = title
        self.title_font_size = title_font_size
        self.state_names = state_names if state_names is not None else [f""State {i}"" for i in range(self.n_states)]
        self.virtual = set() if virtual is None else set([v for v in virtual if v is not None])
        self.state_font_size = state_font_size
        self.state_text_buffer = float(state_text_buffer)
        self.label_side = label_side if label_side in ('left', 'right') else 'left'

        # Prepare axes
        self.ax.set_title(self.title, fontsize=self.title_font_size)
        self._draw_energy_levels()
        self._decorate_axes()

    def _draw_energy_levels(self):
        # Draw short horizontal ticks for each state at each interaction column
        xs = np.arange(self.n_interactions, dtype=float)
        tick_half_width = 0.35
        color_normal = 'k'
        color_virtual = '0.6'
        lw_normal = 2.0
        lw_virtual = 1.5
        ls_normal = '-'
        ls_virtual = '--'

        for s, E in enumerate(self.energies):
            is_virtual = s in self.virtual
            c = color_virtual if is_virtual else color_normal
            lw = lw_virtual if is_virtual else lw_normal
            ls = ls_virtual if is_virtual else ls_normal
            for x in xs:
                self.ax.plot([x - tick_half_width, x + tick_half_width], [E, E], color=c, lw=lw, ls=ls, solid_capstyle='butt')

        # State labels
        if self.label_side == 'left':
            x_text = -self.state_text_buffer
            ha = 'right'
        else:
            x_text = (self.n_interactions - 1) + self.state_text_buffer
            ha = 'left'

        for s, E in enumerate(self.energies):
            txt = self.state_names[s] if s < len(self.state_names) else f""State {s}""
            self.ax.text(x_text, E, txt, fontsize=self.state_font_size, va='center', ha=ha)

    def _decorate_axes(self):
        # Limits and cleanup
        x_margin_left = 1.0 if self.label_side == 'left' else 0.25
        x_margin_right = 1.0 if self.label_side == 'right' else 0.25
        self.ax.set_xlim(-x_margin_left, (self.n_interactions - 1) + x_margin_right)

        if len(self.energies) > 0:
            e_min = min(self.energies)
            e_max = max(self.energies)
            e_pad = 0.05 * (e_max - e_min if e_max > e_min else 1.0)
            self.ax.set_ylim(e_min - e_pad, e_max + e_pad)

        self.ax.spines['top'].set_visible(False)
        self.ax.spines['right'].set_visible(False)
        self.ax.spines['bottom'].set_visible(False)
        self.ax.yaxis.set_ticks_position('left')
        self.ax.xaxis.set_visible(False)
        self.ax.grid(False)

    def add_arrow(self, index, between, kind, label='', head_length=10, head_aspect=1, font_size=14, color='k'):
        # Resolve x position
        x = float(index)
        # Resolve states
        if not (isinstance(between, (list, tuple)) and len(between) == 2):
            raise ValueError(""between must be a tuple/list of two state indices (from_state, to_state)"")
        i0, i1 = int(between[0]), int(between[1])
        if not (0 <= i0 < self.n_states and 0 <= i1 < self.n_states):
            raise IndexError(""State indices out of range"")
        y0 = float(self.energies[i0])
        y1 = float(self.energies[i1])

        # Determine arrow style
        k = (kind or '').lower()
        if k in ('up', 'absorption'):
            arrowstyle = '-|>'
            y_start, y_end = (min(y0, y1), max(y0, y1))
        elif k in ('down', 'emission'):
            arrowstyle = '<|-'
            y_start, y_end = (max(y0, y1), min(y0, y1))
        elif k in ('both', 'two-headed', 'double'):
            arrowstyle = '<|-|>'
            y_start, y_end = (min(y0, y1), max(y0, y1))
        else:
            # Default: point towards the actual target state i1
            if y1 > y0:
                arrowstyle = '-|>'
                y_start, y_end = (y0, y1)
            elif y1 < y0:
                arrowstyle = '<|-'
                y_start, y_end = (y0, y1)
            else:
                # Same energy: draw small horizontal arrow to the right
                dx = 0.6
                arrow = FancyArrowPatch(
                    (x - 0.3, y0),
                    (x + dx - 0.3, y0),
                    arrowstyle='-|>',
                    mutation_scale=max(2.0, float(head_length) * float(head_aspect)),
                    linewidth=2.0,
                    color=color
                )
                self.ax.add_patch(arrow)
                if label:
                    self.ax.text(x + dx - 0.2, y0, label, fontsize=font_size, va='center', ha='left', color=color)
                return arrow

        # Line style
        ls = '--' if ('dash' in k or 'virtual' in k) else '-'
        ms = max(2.0, float(head_length) * float(head_aspect))

        arrow = FancyArrowPatch(
            (x, y_start),
            (x, y_end),
            arrowstyle=arrowstyle,
            mutation_scale=ms,
            linewidth=2.0,
            linestyle=ls,
            color=color
        )
        self.ax.add_patch(arrow)

        if label:
            y_mid = (y_start + y_end) / 2.0
            x_off = 0.15
            ha = 'left'
            self.ax.text(x + x_off, y_mid, label, fontsize=font_size, va='center', ha=ha, color=color)

        return arrow"
341765,peopledoc/django-agnocomplete,peopledoc_django-agnocomplete/agnocomplete/views.py,agnocomplete.views.UserContextFormViewMixin,"class UserContextFormViewMixin:
    """"""
    This mixin is injecting the context variable into the form kwargs
    """"""

    def get_agnocomplete_context(self):
        """"""
        Return the view current user.

        You may want to change this value by overrding this method.
        """"""
        return self.request.user

    def get_form_kwargs(self):
        """"""
        Return the form kwargs.

        This method injects the context variable, defined in
        :meth:`get_agnocomplete_context`. Override this method to adjust it to
        your needs.
        """"""
        data = super().get_form_kwargs()
        data.update({'user': self.get_agnocomplete_context()})
        return data","class UserContextFormViewMixin:
    
    def get_agnocomplete_context(self):
        
        pass
    def get_form_kwargs(self):
        
        pass",snippet_272,3,"class UserContextFormViewMixin:
    def get_agnocomplete_context(self):
        form = getattr(self, ""form"", None)
        if form is None and hasattr(self, ""get_form""):
            try:
                form = self.get_form()
            except Exception:
                form = None

        agnocomplete_fields = []
        if form is not None and hasattr(form, ""fields""):
            for name, field in getattr(form, ""fields"", {}).items():
                widget = getattr(field, ""widget"", None)
                if widget is None:
                    continue
                clsname = widget.__class__.__name__.lower()
                if (
                    hasattr(widget, ""is_agnocomplete"")
                    or hasattr(widget, ""agnocomplete"")
                    or ""agnocomplete"" in clsname
                ):
                    agnocomplete_fields.append(name)

        return {""agnocomplete_fields"": agnocomplete_fields}

    def get_form_kwargs(self):
        kwargs = {}
        try:
            if hasattr(super(), ""get_form_kwargs""):
                kwargs = super().get_form_kwargs()  # type: ignore[misc]
        except Exception:
            kwargs = {}

        request = getattr(self, ""request"", None)
        user = getattr(request, ""user"", None) if request is not None else None
        if user is not None:
            kwargs.setdefault(""user"", user)
        return kwargs"
343261,danpoland/pyramid-restful-framework,danpoland_pyramid-restful-framework/pyramid_restful/pagination/base.py,pyramid_restful.pagination.base.BasePagination,"class BasePagination:
    """"""
    The base class each Pagination class should implement.
    """"""

    def paginate_query(self, query, request):
        """"""
        :param query: SQLAlchemy ``query``.
        :param request: The request from the view
        :return: The paginated date based on the provided query and request.
        """"""
        raise NotImplementedError('paginate_query() must be implemented.')

    def get_paginated_response(self, data):
        """"""
        :param data: The paginated data.
        :return: A response containing the paginated data.
        """"""
        raise NotImplementedError('get_paginated_response() must be implemented.')","class BasePagination:
    
    def paginate_query(self, query, request):
        
        pass
    def get_paginated_response(self, data):
        
        pass",snippet_273,3,"class BasePagination:
    def __init__(self, default_page_size=20, max_page_size=1000):
        self.default_page_size = int(default_page_size) if default_page_size else 20
        self.max_page_size = int(max_page_size) if max_page_size else 1000

        self.page = 1
        self.page_size = self.default_page_size
        self.total = 0
        self.total_pages = 0
        self.next_page = None
        self.previous_page = None

    def _get_params_source(self, request):
        if request is None:
            return {}
        # Dict-like
        if hasattr(request, ""get"") and callable(request.get):
            return request
        # Django-like
        if hasattr(request, ""query_params""):
            return request.query_params
        if hasattr(request, ""GET""):
            return request.GET
        # Fallback: try to treat as mapping
        try:
            _ = request.keys()
            return request
        except Exception:
            return {}

    def _to_int(self, value, default):
        try:
            return int(value)
        except Exception:
            return default

    def _materialize_sequence(self, query):
        # If it's already sliceable with len, keep it as is
        try:
            _ = len(query)  # may raise
            _ = query[0:0]  # test slicing
            return query, False
        except Exception:
            # Convert to list
            return list(query), True

    def paginate_query(self, query, request):
        params = self._get_params_source(request)
        page = self._to_int(params.get(""page"", None), 1)
        page_size = self._to_int(params.get(""page_size"", None), self.default_page_size)

        if page < 1:
            page = 1
        if page_size < 1:
            page_size = 1
        if self.max_page_size:
            page_size = min(page_size, self.max_page_size)

        seq, materialized = self._materialize_sequence(query)
        total = len(seq)

        if total == 0:
            self.page = 1
            self.page_size = page_size
            self.total = 0
            self.total_pages = 0
            self.next_page = None
            self.previous_page = None
            return [] if materialized else seq[0:0]

        total_pages = (total + page_size - 1) // page_size
        if page > total_pages:
            page = total_pages

        start = (page - 1) * page_size
        end = start + page_size

        results = seq[start:end]

        self.page = page
        self.page_size = page_size
        self.total = total
        self.total_pages = total_pages
        self.previous_page = page - 1 if page > 1 else None
        self.next_page = page + 1 if page < total_pages else None

        return results

    def get_paginated_response(self, data):
        return {
            ""count"": self.total,
            ""total_pages"": self.total_pages,
            ""page"": self.page,
            ""page_size"": self.page_size,
            ""next_page"": self.next_page,
            ""previous_page"": self.previous_page,
            ""results"": data,
        }"
343270,danpoland/pyramid-restful-framework,danpoland_pyramid-restful-framework/pyramid_restful/permissions.py,pyramid_restful.permissions.BasePermission,"class BasePermission:
    """"""
    All permission classes should inherit from this class.
    """"""
    message = None

    def has_permission(self, request, view):
        """"""
        Checked on every request to a view. Return ``True`` if permission is granted else ``False``.

        :param request: The request sent to the view.
        :param view: The instance of the view being accessed.
        :return: Boolean
        """"""
        return True

    def has_object_permission(self, request, view, obj):
        """"""
        Checked when a request is for a specific object. Return ``True`` if permission is granted else ``False``.

        :param request: The request sent to the view.
        :param view:  The instance of the view being accessed.
        :param obj: The object being accessed.
        :return: Boolean
        """"""
        return True","class BasePermission:
    
    def has_permission(self, request, view):
        
        pass
    def has_object_permission(self, request, view, obj):
        
        pass",snippet_274,3,"class BasePermission:
    message = None

    def has_permission(self, request, view):
        return True

    def has_object_permission(self, request, view, obj):
        return True"
344249,openid/JWTConnect-Python-CryptoJWT,src/cryptojwt/jws/__init__.py,cryptojwt.jws.Signer,"class Signer:
    """"""Abstract base class for signing algorithms.""""""

    def sign(self, msg, key):
        """"""Sign ``msg`` with ``key`` and return the signature.""""""
        raise NotImplementedError()

    def verify(self, msg, sig, key):
        """"""Return True if ``sig`` is a valid signature for ``msg``.""""""
        raise NotImplementedError()","class Signer:
    
    def sign(self, msg, key):
        
        pass
    def verify(self, msg, sig, key):
        
        pass",snippet_275,3,"class Signer:
    def _to_bytes(self, x):
        if isinstance(x, bytes):
            return x
        if isinstance(x, bytearray):
            return bytes(x)
        return str(x).encode(""utf-8"")

    def sign(self, msg, key):
        import hmac, hashlib
        k = self._to_bytes(key)
        m = self._to_bytes(msg)
        return hmac.new(k, m, hashlib.sha256).hexdigest()

    def verify(self, msg, sig, key):
        import hmac, hashlib
        k = self._to_bytes(key)
        m = self._to_bytes(msg)
        mac = hmac.new(k, m, hashlib.sha256)
        expected_hex = mac.hexdigest()
        expected_raw = mac.digest()

        # Accept raw bytes or hex string
        if isinstance(sig, (bytes, bytearray)):
            s = bytes(sig)
            if len(s) == len(expected_raw):
                return hmac.compare_digest(s, expected_raw)
            try:
                s_str = s.decode(""ascii"")
            except Exception:
                return False
            return hmac.compare_digest(s_str.lower(), expected_hex)
        else:
            s_str = str(sig)
            return hmac.compare_digest(s_str.lower(), expected_hex)"
347062,adobe-apiplatform/umapi-client.py,umapi_client/api.py,umapi_client.api.QuerySingle,"class QuerySingle:
    """"""
    Look for a single object
    """"""

    def __init__(self, connection, object_type, url_params=None, query_params=None):
        """"""
        Provide the connection and query parameters when you create the query.

        :param connection: The Connection to run the query against
        :param object_type: The type of object being queried (e.g., ""user"" or ""group"")
        :param url_params: Query qualifiers that go in the URL path (e.g., a group name when querying users)
        :param query_params: Query qualifiers that go in the query string (e.g., a domain name)
        """"""
        self.conn = connection
        self.object_type = object_type
        self.url_params = url_params if url_params else []
        self.query_params = query_params if query_params else {}
        self._result = None

    def reload(self):
        """"""
        Rerun the query (lazily).
        The result will contain a value on the server side that have changed since the last run.
        :return: None
        """"""
        self._result = None

    def _fetch_result(self):
        """"""
        Fetch the queried object.
        """"""
        self._result = self.conn.query_single(self.object_type, self.url_params, self.query_params)

    def result(self):
        """"""
        Fetch the result, if we haven't already or if reload has been called.
        :return: the result object of the query.
        """"""
        if self._result is None:
            self._fetch_result()
        return self._result","class QuerySingle:
    
    def __init__(self, connection, object_type, url_params=None, query_params=None):
        
        pass
    def reload(self):
        
        pass
    def _fetch_result(self):
        
        pass

    def result(self):
        
        pass",snippet_276,5,"class QuerySingle:
    
    def __init__(self, connection, object_type, url_params=None, query_params=None):
        self._connection = connection
        self._object_type = object_type
        self._url_params = url_params or {}
        self._query_params = query_params or {}
        self._result = None
        self._loaded = False

    def reload(self):
        self._loaded = False
        self._result = None
        return self.result()

    def _fetch_result(self):
        path = None

        if hasattr(self._object_type, ""endpoint""):
            path = getattr(self._object_type, ""endpoint"")
        elif isinstance(self._object_type, str):
            path = self._object_type
        else:
            name = getattr(self._object_type, ""__name__"", str(self._object_type))
            path = name.lower()

        if isinstance(path, str) and self._url_params:
            try:
                path = path.format(**self._url_params)
            except Exception:
                pass  # leave path as-is if formatting fails

        data = None
        conn = self._connection

        if hasattr(conn, ""request""):
            data = conn.request(""GET"", path, params=self._query_params)
        elif hasattr(conn, ""get""):
            try:
                data = conn.get(path, params=self._query_params)
            except TypeError:
                data = conn.get(path, self._query_params)
        elif callable(conn):
            try:
                data = conn(path, self._query_params)
            except TypeError:
                data = conn(path)
        else:
            raise TypeError(""Unsupported connection interface"")

        obj = data
        typ = self._object_type

        if hasattr(typ, ""from_dict"") and callable(getattr(typ, ""from_dict"")):
            obj = typ.from_dict(data)
        elif hasattr(typ, ""from_json"") and callable(getattr(typ, ""from_json"")):
            obj = typ.from_json(data)
        elif callable(typ) and not isinstance(typ, str):
            try:
                obj = typ(data)
            except TypeError:
                obj = typ()

        self._result = obj
        self._loaded = True

    def result(self):
        if not self._loaded:
            self._fetch_result()
        return self._result"
347189,facelessuser/backrefs,facelessuser_backrefs/backrefs/util.py,backrefs.util.Immutable,"from typing import Any, Callable, AnyStr

class Immutable:
    """"""Immutable.""""""
    __slots__: tuple[Any, ...] = ()

    def __init__(self, **kwargs: Any) -> None:
        """"""Initialize.""""""
        for k, v in kwargs.items():
            super().__setattr__(k, v)

    def __setattr__(self, name: str, value: Any) -> None:
        """"""Prevent mutability.""""""
        raise AttributeError('Class is immutable!')","
class Immutable:
    
    def __init__(self, **kwargs: Any) -> None:
        
        pass
    def __setattr__(self, name: str, value: Any) -> None:
        
        pass",snippet_277,3,"from typing import Any

class Immutable:
    
    def __init__(self, **kwargs: Any) -> None:
        object.__setattr__(self, '_locked', False)
        for k, v in kwargs.items():
            object.__setattr__(self, k, v)
        object.__setattr__(self, '_locked', True)

    def __setattr__(self, name: str, value: Any) -> None:
        if getattr(self, '_locked', False):
            raise AttributeError(f""{self.__class__.__name__} is immutable; cannot set attribute '{name}'"")
        object.__setattr__(self, name, value)"
347503,inveniosoftware/invenio-records,inveniosoftware_invenio-records/invenio_records/dumpers/base.py,invenio_records.dumpers.base.Dumper,"from copy import deepcopy

class Dumper:
    """"""Interface for dumpers.""""""

    def dump(self, record, data):
        """"""Dump a record that can be used a source document for the search engine.

        The job of this method is to create a Python dictionary from the record
        provided in the argument.

        If you overwrite this method without calling super, then you should
        ensure that you make a deep copy of the record dictionary, to avoid
        that changes to the dump affects the record.

        :param record: The record to dump.
        :param data: The initial dump data passed in by ``record.dumps()``.
        """"""
        data.update(deepcopy(dict(record)))
        return data

    def load(self, data, record_cls):
        """"""Load a record from the source document of a search engine hit.

        The job of this method, is to create a record of type ``record_cls``
        based on the input ``data``.

        :param data: A Python dictionary representing the data to load.
        :param records_cls: The record class to be constructed.
        :returns: A instance of ``record_cls``.
        """"""
        raise NotImplementedError()","
class Dumper:
    
    def dump(self, record, data):
        
        pass
    def load(self, data, record_cls):
        
        pass",snippet_278,3,"class Dumper:
    def dump(self, record, data):
        import dataclasses
        from collections.abc import Mapping

        if data is None:
            data = {}
        elif not isinstance(data, Mapping):
            raise TypeError(""data must be a mapping or None"")

        # Determine how to obtain a dict representation of the record
        if hasattr(record, ""to_dict"") and callable(getattr(record, ""to_dict"")):
            payload = record.to_dict()
        elif dataclasses.is_dataclass(record):
            payload = dataclasses.asdict(record)
        elif hasattr(record, ""__dict__""):
            # Shallow copy of instance attributes
            payload = dict(record.__dict__)
        else:
            raise TypeError(""record is not serializable: no to_dict, not a dataclass, and no __dict__"")

        if not isinstance(payload, Mapping):
            raise TypeError(""Serialized record must be a mapping"")

        # Merge into provided data
        out = dict(data)
        out.update(payload)
        return out

    def load(self, data, record_cls):
        import dataclasses
        from collections.abc import Mapping

        if not isinstance(data, Mapping):
            raise TypeError(""data must be a mapping"")

        # If class provides its own constructor from dict, use it
        from_dict = getattr(record_cls, ""from_dict"", None)
        if callable(from_dict):
            return from_dict(dict(data))

        # Dataclass support
        if dataclasses.is_dataclass(record_cls):
            fields = {f.name for f in dataclasses.fields(record_cls)}
            init_kwargs = {k: v for k, v in data.items() if k in fields}
            return record_cls(**init_kwargs)

        # Try constructing via kwargs
        try:
            return record_cls(**data)
        except TypeError:
            # Fallback: create empty instance and set attributes
            try:
                obj = record_cls()
            except TypeError as e:
                raise TypeError(f""Cannot instantiate {record_cls} without arguments and no compatible kwargs provided"") from e
            for k, v in data.items():
                try:
                    setattr(obj, k, v)
                except Exception:
                    # Ignore attributes that cannot be set
                    pass
            return obj"
347507,inveniosoftware/invenio-records,inveniosoftware_invenio-records/invenio_records/dumpers/search.py,invenio_records.dumpers.search.SearchDumperExt,"class SearchDumperExt:
    """"""Interface for Search dumper extensions.""""""

    def dump(self, record, data):
        """"""Dump the data.""""""

    def load(self, data, record_cls):
        """"""Load the data.

        Reverse the changes made by the dump method.
        """"""","class SearchDumperExt:
    
    def dump(self, record, data):
        
        pass
    def load(self, data, record_cls):
        
        pass",snippet_279,3,"class SearchDumperExt:
    def dump(self, record, data):
        base = {}
        if record is not None:
            if hasattr(record, ""to_dict"") and callable(getattr(record, ""to_dict"")):
                try:
                    base = record.to_dict()
                except Exception:
                    base = {}
            elif hasattr(record, ""dict"") and callable(getattr(record, ""dict"")):
                try:
                    base = record.dict()
                except Exception:
                    base = {}
            elif hasattr(record, ""__dict__""):
                try:
                    base = {k: v for k, v in vars(record).items() if not k.startswith(""_"")}
                except Exception:
                    base = {}
        if data is None:
            return dict(base)
        if isinstance(data, dict):
            merged = dict(base)
            merged.update(data)
            return merged
        return dict(base)

    def load(self, data, record_cls):
        if data is None:
            try:
                return record_cls()
            except Exception:
                return None

        if hasattr(record_cls, ""from_dict"") and callable(getattr(record_cls, ""from_dict"")):
            try:
                return record_cls.from_dict(data)
            except Exception:
                pass

        if hasattr(record_cls, ""parse_obj"") and callable(getattr(record_cls, ""parse_obj"")):
            try:
                return record_cls.parse_obj(data)
            except Exception:
                pass

        try:
            return record_cls(**data)
        except Exception:
            try:
                instance = record_cls()
            except Exception:
                return None
            if isinstance(data, dict):
                for k, v in data.items():
                    try:
                        setattr(instance, k, v)
                    except Exception:
                        continue
            return instance"
347521,inveniosoftware/invenio-records,inveniosoftware_invenio-records/invenio_records/systemfields/base.py,invenio_records.systemfields.base.SystemFieldContext,"class SystemFieldContext:
    """"""Base class for a system field context.

    A system field context is created once you access a field's attribute on
    a class. As the system field may be defined on a super class, this context
    allows us to know from which class the field was accessed.

    Normally you should subclass this class, and implement methods the methods
    on it that requires you to know the record class.
    """"""

    def __init__(self, field, record_cls):
        """"""Initialise the field context.""""""
        self._field = field
        self._record_cls = record_cls

    @property
    def field(self):
        """"""Access the field to prevent it from being overwritten.""""""
        return self._field

    @property
    def record_cls(self):
        """"""Record class to prevent it from being overwritten.""""""
        return self._record_cls","class SystemFieldContext:
    
    def __init__(self, field, record_cls):
        
        pass
    @property
    def field(self):
        
        pass
    @property
    def record_cls(self):
        
        pass",snippet_280,4,"class SystemFieldContext:
    
    def __init__(self, field, record_cls):
        self._field = field
        self._record_cls = record_cls

    @property
    def field(self):
        return self._field

    @property
    def record_cls(self):
        return self._record_cls"
348074,edx/edx-val,edx_edx-val/edxval/transcript_utils.py,edxval.transcript_utils.Transcript,"import json
from pysrt import SubRipFile, SubRipItem, SubRipTime
from edxval.exceptions import TranscriptsGenerationException
from pysrt.srtexc import Error

class Transcript:
    """"""
    Container for transcript methods.
    """"""
    SRT = 'srt'
    SJSON = 'sjson'

    @staticmethod
    def generate_sjson_from_srt(srt_subs):
        """"""
        Generate transcripts from sjson to SubRip (*.srt).

        Arguments:
            srt_subs(SubRip): ""SRT"" subs object

        Returns:
            Subs converted to ""SJSON"" format.
        """"""
        sub_starts = []
        sub_ends = []
        sub_texts = []
        for sub in srt_subs:
            sub_starts.append(sub.start.ordinal)
            sub_ends.append(sub.end.ordinal)
            sub_texts.append(sub.text.replace('\n', ' '))
        sjson_subs = {'start': sub_starts, 'end': sub_ends, 'text': sub_texts}
        return sjson_subs

    @staticmethod
    def generate_srt_from_sjson(sjson_subs):
        """"""
        Generate transcripts from sjson to SubRip (*.srt)

        Arguments:
            sjson_subs (dict): `sjson` subs.

        Returns:
            Subtitles in SRT format.
        """"""
        output = ''
        equal_len = len(sjson_subs['start']) == len(sjson_subs['end']) == len(sjson_subs['text'])
        if not equal_len:
            return output
        for i in range(len(sjson_subs['start'])):
            item = SubRipItem(index=i, start=SubRipTime(milliseconds=sjson_subs['start'][i]), end=SubRipTime(milliseconds=sjson_subs['end'][i]), text=sjson_subs['text'][i])
            output += str(item)
            output += '\n'
        return output

    @classmethod
    def convert(cls, content, input_format, output_format):
        """"""
        Convert transcript `content` from `input_format` to `output_format`.

        Arguments:
            content: Transcript content byte-stream.
            input_format: Input transcript format.
            output_format: Output transcript format.

        Accepted input formats: sjson, srt.
        Accepted output format: srt, sjson.

        Raises:
            TranscriptsGenerationException: On parsing the invalid srt
            content during conversion from srt to sjson.
        """"""
        assert input_format in ('srt', 'sjson')
        assert output_format in ('srt', 'sjson')
        try:
            content = content.decode('utf-8-sig')
        except UnicodeDecodeError:
            content = content.decode('latin-1')
        if input_format == output_format:
            return content
        if input_format == 'srt':
            if output_format == 'sjson':
                try:
                    srt_subs = SubRipFile.from_string(content, error_handling=SubRipFile.ERROR_RAISE)
                except Error as ex:
                    raise TranscriptsGenerationException(str(ex)) from ex
                return json.dumps(cls.generate_sjson_from_srt(srt_subs))
        if input_format == 'sjson':
            if output_format == 'srt':
                return cls.generate_srt_from_sjson(json.loads(content))","
class Transcript:
    
    @staticmethod
    def generate_sjson_from_srt(srt_subs):
        
        pass
    @staticmethod
    def generate_srt_from_sjson(sjson_subs):
        
        pass
    @classmethod
    def convert(cls, content, input_format, output_format):
        
        pass",snippet_282,4,"class Transcript:
    
    @staticmethod
    def _parse_srt_timestamp(ts):
        ts = ts.strip()
        # Accept ""HH:MM:SS,mmm"" or ""HH:MM:SS.mmm""
        hms, ms = ts.replace(',', '.').split('.')
        h, m, s = hms.split(':')
        total = int(h) * 3600 + int(m) * 60 + int(s) + int(ms.ljust(3, '0')[:3]) / 1000.0
        return total

    @staticmethod
    def _format_srt_timestamp(seconds):
        if seconds < 0:
            seconds = 0
        total_ms = int(round(seconds * 1000))
        h = total_ms // 3600000
        rem = total_ms % 3600000
        m = rem // 60000
        rem = rem % 60000
        s = rem // 1000
        ms = rem % 1000
        return f""{h:02d}:{m:02d}:{s:02d},{ms:03d}""

    @staticmethod
    def generate_sjson_from_srt(srt_subs):
        import json
        if not isinstance(srt_subs, str):
            raise TypeError(""srt_subs must be a string"")
        text = srt_subs.replace(""\r\n"", ""\n"").replace(""\r"", ""\n"").strip(""\n"")
        if not text:
            return json.dumps({""segments"": []}, ensure_ascii=False)
        blocks = []
        current = []
        for line in text.split(""\n""):
            if line.strip() == """":
                if current:
                    blocks.append(current)
                    current = []
            else:
                current.append(line)
        if current:
            blocks.append(current)

        segments = []
        seq = 1
        for blk in blocks:
            # Expected:
            # [index?], timing line, text lines...
            # Identify timing line containing -->
            idx = None
            timing_line_idx = None
            for i, line in enumerate(blk[:2]):  # timing line usually in first two lines
                if ""-->"" in line:
                    timing_line_idx = i
                    break
            if timing_line_idx is None:
                # Fallback: search anywhere
                for i, line in enumerate(blk):
                    if ""-->"" in line:
                        timing_line_idx = i
                        break
            if timing_line_idx is None:
                # Cannot parse timing; skip block
                continue
            # Try index from previous line if numeric
            if timing_line_idx >= 1:
                try:
                    maybe_idx = blk[timing_line_idx - 1].strip()
                    if maybe_idx.isdigit():
                        idx = int(maybe_idx)
                except Exception:
                    idx = None
            # Parse timing
            timing = blk[timing_line_idx]
            parts = timing.split(""-->"")
            if len(parts) != 2:
                continue
            start_s = Transcript._parse_srt_timestamp(parts[0].strip())
            end_s = Transcript._parse_srt_timestamp(parts[1].strip().split()[0])
            # Remaining lines after timing are text, but if index consumed, skip it
            text_lines_start = timing_line_idx + 1
            lines = blk[text_lines_start:]
            text_payload = ""\n"".join(lines).strip()
            segments.append({
                ""id"": idx if isinstance(idx, int) else seq,
                ""start"": start_s,
                ""end"": end_s,
                ""text"": text_payload
            })
            seq += 1

        return json.dumps({""segments"": segments}, ensure_ascii=False)

    @staticmethod
    def generate_srt_from_sjson(sjson_subs):
        import json
        if not isinstance(sjson_subs, str):
            raise TypeError(""sjson_subs must be a string"")
        try:
            data = json.loads(sjson_subs)
        except Exception as e:
            raise ValueError(f""Invalid JSON: {e}"")
        segments = []
        if isinstance(data, dict) and ""segments"" in data and isinstance(data[""segments""], list):
            segments = data[""segments""]
        elif isinstance(data, list):
            segments = data
        else:
            raise ValueError(""SJSON must be an object with 'segments' list or a list of segments"")

        # Normalize segments
        norm = []
        for i, seg in enumerate(segments, 1):
            if not isinstance(seg, dict):
                continue
            start = seg.get(""start"", 0)
            end = seg.get(""end"", start)
            text = seg.get(""text"", """")
            sid = seg.get(""id"", i)
            # Coerce to float
            try:
                start = float(start)
            except Exception:
                start = 0.0
            try:
                end = float(end)
            except Exception:
                end = start
            # Coerce text
            if not isinstance(text, str):
                text = str(text)
            # Coerce id
            try:
                sid = int(sid)
            except Exception:
                sid = i
            norm.append((sid, start, end, text))

        # Sort by id then start for deterministic order
        norm.sort(key=lambda t: (t[0], t[1]))

        lines = []
        for idx, start, end, text in norm:
            lines.append(str(idx))
            lines.append(f""{Transcript._format_srt_timestamp(start)} --> {Transcript._format_srt_timestamp(end)}"")
            # Preserve multi-line text
            if text:
                lines.extend(text.split(""\n""))
            lines.append("""")  # blank line
        return ""\n"".join(lines).rstrip() + (""\n"" if lines else """")

    @classmethod
    def convert(cls, content, input_format, output_format):
        fmt_in = (input_format or """").strip().lower()
        fmt_out = (output_format or """").strip().lower()
        if fmt_in == fmt_out:
            return content
        if fmt_in == ""srt"" and fmt_out in (""sjson"", ""json""):
            return cls.generate_sjson_from_srt(content)
        if fmt_in in (""sjson"", ""json"") and fmt_out == ""srt"":
            return cls.generate_srt_from_sjson(content)
        raise ValueError(f""Unsupported conversion: {input_format} -> {output_format}"")"
350782,druids/django-chamber,druids_django-chamber/chamber/commands/__init__.py,chamber.commands.ProgressBarStream,"class ProgressBarStream:
    """"""
    OutputStream wrapper to remove default linebreak at line endings.
    """"""

    def __init__(self, stream):
        """"""
        Wrap the given stream.
        """"""
        self.stream = stream

    def write(self, *args, **kwargs):
        """"""
        Call the stream's write method without linebreaks at line endings.
        """"""
        return self.stream.write(*args, ending='', **kwargs)

    def flush(self):
        """"""
        Call the stream's flush method without any extra arguments.
        """"""
        return self.stream.flush()","class ProgressBarStream:
    
    def __init__(self, stream):
        
        pass
    def write(self, *args, **kwargs):
        
        pass
    def flush(self):
        
        pass",snippet_283,4,"class ProgressBarStream:
    
    def __init__(self, stream):
        self.stream = stream
        self._in_progress_line = False

    def write(self, *args, **kwargs):
        s = ''.join(str(a) for a in args)
        if not s:
            return
        if self._in_progress_line and not s.startswith('\r'):
            self.stream.write('\n')
            self._in_progress_line = False
        self.stream.write(s)
        if '\n' in s:
            self._in_progress_line = False
        elif '\r' in s:
            self._in_progress_line = True

    def flush(self):
        flush = getattr(self.stream, 'flush', None)
        if callable(flush):
            flush()"
350920,pytroll/trollimage,pytroll_trollimage/trollimage/_xrimage_rasterio.py,trollimage._xrimage_rasterio.RIODataset,"from rasterio.enums import Resampling
from rasterio.windows import Window

class RIODataset:
    """"""A wrapper for a rasterio dataset.""""""

    def __init__(self, rfile, overviews=None, overviews_resampling=None, overviews_minsize=256):
        """"""Init the rasterio dataset.""""""
        self.rfile = rfile
        self.overviews = overviews
        if overviews_resampling is None:
            overviews_resampling = 'nearest'
        self.overviews_resampling = Resampling[overviews_resampling]
        self.overviews_minsize = overviews_minsize

    def __setitem__(self, key, item):
        """"""Put the data chunk in the image.""""""
        if len(key) == 3:
            indexes = list(range(key[0].start + 1, key[0].stop + 1, key[0].step or 1))
            y = key[1]
            x = key[2]
        else:
            indexes = 1
            y = key[0]
            x = key[1]
        chy_off = y.start
        chy = y.stop - y.start
        chx_off = x.start
        chx = x.stop - x.start
        self.rfile.write(item, window=Window(chx_off, chy_off, chx, chy), indexes=indexes)

    def close(self):
        """"""Close the file.""""""
        if self.overviews is not None:
            overviews = self.overviews
            if len(overviews) == 0:
                from rasterio.rio.overview import get_maximum_overview_level
                width = self.rfile.width
                height = self.rfile.height
                max_level = get_maximum_overview_level(width, height, self.overviews_minsize)
                overviews = [2 ** j for j in range(1, max_level + 1)]
            logger.debug('Building overviews %s with %s resampling', str(overviews), self.overviews_resampling.name)
            self.rfile.build_overviews(overviews, resampling=self.overviews_resampling)
        return self.rfile.close()","
class RIODataset:
    
    def __init__(self, rfile, overviews=None, overviews_resampling=None, overviews_minsize=256):
        
        pass
    def __setitem__(self, key, item):
        
        pass

    def close(self):
        
        pass",snippet_284,4,"import os
from typing import Any, Iterable, Optional, Tuple, Union

try:
    import rasterio
    from rasterio.enums import Resampling
    from rasterio.windows import Window
    _HAS_RASTERIO = True
except Exception:  # pragma: no cover
    _HAS_RASTERIO = False
    rasterio = None
    Resampling = None
    Window = None


class RIODataset:
    def __init__(self, rfile, overviews=None, overviews_resampling=None, overviews_minsize=256):
        self.ds = None
        self.path = None
        self._closed = True
        self._writeable = False

        if not _HAS_RASTERIO:
            raise RuntimeError(""rasterio is required to use RIODataset"")

        if hasattr(rfile, ""close"") and hasattr(rfile, ""read""):
            self.ds = rfile
            self._closed = False
            try:
                # Writer has write methods, reader doesn't
                self._writeable = hasattr(self.ds, ""write"")
            except Exception:
                self._writeable = False
        else:
            self.path = str(rfile)
            # Try r+ first for overviews or potential writes
            self.ds = self._open_prefer_rw(self.path)
            self._closed = False

        if overviews is not None:
            try:
                self._ensure_writeable()
                factors = self._normalize_overview_factors(overviews, overviews_minsize)
                if factors:
                    method = self._normalize_resampling(overviews_resampling)
                    self.ds.build_overviews(factors, method)
                    try:
                        self.ds.update_tags(ns=""rio_overviews"",
                                            factors="","".join(map(str, factors)),
                                            resampling=method.name)
                    except Exception:
                        pass
                    try:
                        self.ds._close()  # flush internal caches for some drivers
                        self.ds = self._reopen_same_mode(self.ds)
                    except Exception:
                        pass
            except Exception:
                # Best-effort; ignore if building overviews fails
                pass

    def __setitem__(self, key, item):
        self._ensure_writeable()

        # Supported patterns:
        # - int band index
        # - iterable of band indices
        # - (indexes, window) where indexes is int or iterable; window is rasterio Window or tuple
        indexes: Optional[Union[int, Iterable[int]]] = None
        window: Optional[Any] = None

        if isinstance(key, tuple) and len(key) == 2:
            indexes, window = key
        else:
            indexes = key

        if window is not None and not isinstance(window, Window):
            # Accept ((row_off, col_off), (height, width)) or (row_off, col_off, height, width)
            if isinstance(window, tuple):
                if len(window) == 2 and all(isinstance(x, tuple) for x in window):
                    window = Window.from_slices(
                        slice(window[0][0], window[0][0] + window[1][0]),
                        slice(window[0][1], window[0][1] + window[1][1]),
                    )
                elif len(window) == 4:
                    window = Window(window[0], window[1], window[2], window[3])

        self.ds.write(item, indexes=indexes, window=window)

    def close(self):
        if getattr(self, ""ds"", None) is not None and not self._closed:
            try:
                self.ds.close()
            finally:
                self._closed = True

    # Internal helpers

    def _open_prefer_rw(self, path: str):
        ds = None
        # Try r+
        try:
            ds = rasterio.open(path, ""r+"")
            self._writeable = True
            return ds
        except Exception:
            pass
        # Fall back to r
        ds = rasterio.open(path, ""r"")
        self._writeable = False
        return ds

    def _reopen_same_mode(self, ds):
        path = getattr(ds, ""name"", None) or self.path
        mode = getattr(ds, ""mode"", ""r"")
        try:
            reopened = rasterio.open(path, mode)
            self._writeable = hasattr(reopened, ""write"")
            return reopened
        except Exception:
            return ds

    def _ensure_writeable(self):
        if self._writeable:
            return
        # Try to reopen in r+ if possible
        name = getattr(self.ds, ""name"", None) or self.path
        if name is not None:
            try:
                reopened = rasterio.open(name, ""r+"")
                try:
                    self.ds.close()
                except Exception:
                    pass
                self.ds = reopened
                self._writeable = True
                self._closed = False
                return
            except Exception:
                pass
        raise IOError(""Dataset is not open in write mode; cannot perform write operation."")

    def _normalize_overview_factors(self, overviews, minsize: int):
        if overviews is True or overviews == ""auto"":
            w = getattr(self.ds, ""width"", None)
            h = getattr(self.ds, ""height"", None)
            if not w or not h:
                return []
            factors = []
            f = 2
            while max(w // f, h // f) >= int(minsize):
                factors.append(f)
                f *= 2
            return factors
        if isinstance(overviews, (list, tuple)):
            return [int(x) for x in overviews if int(x) > 1]
        if isinstance(overviews, int):
            return [int(overviews)] if overviews > 1 else []
        return []

    def _normalize_resampling(self, method):
        if method is None:
            method = ""nearest""
        if isinstance(method, Resampling):
            return method
        name = str(method).lower()
        # Map common aliases
        alias = {
            ""nearest"": ""nearest"",
            ""mode"": ""mode"",
            ""avg"": ""average"",
            ""average"": ""average"",
            ""bilinear"": ""bilinear"",
            ""cubic"": ""cubic"",
            ""cubic_spline"": ""cubic_spline"",
            ""lanczos"": ""lanczos"",
            ""min"": ""min"",
            ""max"": ""max"",
            ""med"": ""med"",
            ""median"": ""med"",
            ""q1"": ""q1"",
            ""q3"": ""q3"",
            ""sum"": ""sum"",
        }
        key = alias.get(name, name)
        try:
            return getattr(Resampling, key)
        except Exception:
            return Resampling.nearest"
350922,pytroll/trollimage,pytroll_trollimage/trollimage/_xrimage_rasterio.py,trollimage._xrimage_rasterio.RIOTag,"class RIOTag:
    """"""Rasterio wrapper to allow da.store on tag.""""""

    def __init__(self, rfile, name):
        """"""Init the rasterio tag.""""""
        self.rfile = rfile
        self.name = name

    def __setitem__(self, key, item):
        """"""Put the data in the tag.""""""
        kwargs = {self.name: item.item()}
        self.rfile.update_tags(**kwargs)

    def close(self):
        """"""Close the file.""""""
        return self.rfile.close()","class RIOTag:
    
    def __init__(self, rfile, name):
        
        pass
    def __setitem__(self, key, item):
        
        pass
    def close(self):
        
        pass",snippet_285,4,"class RIOTag:
    
    def __init__(self, rfile, name):
        if not isinstance(name, str) or not name:
            raise ValueError(""name must be a non-empty string"")
        self.name = name
        self._owns_file = False
        if hasattr(rfile, ""write"") and hasattr(rfile, ""flush""):
            self._fh = rfile
        else:
            self._fh = open(rfile, ""a"", encoding=""utf-8"")
            self._owns_file = True
        self._closed = False
        self._header_written = False
        self._write_header()

    def _write_header(self):
        if self._header_written:
            return
        self._fh.write(f""[RIOTag:{self.name}]\n"")
        self._fh.flush()
        self._header_written = True

    def __setitem__(self, key, item):
        if self._closed:
            raise ValueError(""RIOTag is closed"")
        if not isinstance(key, str) or not key:
            raise ValueError(""key must be a non-empty string"")
        if ""\n"" in key:
            raise ValueError(""key cannot contain newline characters"")
        # Simple serialization
        try:
            import json
            value = json.dumps(item, ensure_ascii=False)
        except Exception:
            value = str(item)
        if ""\n"" in value:
            raise ValueError(""value cannot contain newline characters"")
        self._fh.write(f""{key} = {value}\n"")
        self._fh.flush()

    def close(self):
        if self._closed:
            return
        self._fh.write(f""[/RIOTag:{self.name}]\n"")
        self._fh.flush()
        if self._owns_file:
            try:
                self._fh.close()
            except Exception:
                pass
        self._closed = True"
353855,inveniosoftware/invenio-communities,inveniosoftware_invenio-communities/invenio_communities/communities/records/systemfields/access.py,invenio_communities.communities.records.systemfields.access.AccessEnumMixin,"class AccessEnumMixin:
    """"""Mixin for enum functionalities.""""""

    @classmethod
    def validate(cls, level):
        """"""Validate a string against the enum values.""""""
        return cls(level) in cls

    def __str__(self):
        """"""Return its value.""""""
        return self.value","class AccessEnumMixin:
    
    @classmethod
    def validate(cls, level):
        
        pass
    def __str__(self):
        
        pass",snippet_286,3,"from enum import Enum

class AccessEnumMixin:
    
    @classmethod
    def validate(cls, level):
        if isinstance(level, cls):
            return level

        if isinstance(level, str):
            key = level.strip()

            # Exact name match
            if key in cls.__members__:
                return cls.__members__[key]

            # Case-insensitive name match
            for name, member in cls.__members__.items():
                if name.lower() == key.lower():
                    return member

            # Match by string value (case-insensitive)
            for member in cls:
                val = getattr(member, ""value"", None)
                if isinstance(val, str) and (val == key or val.lower() == key.lower()):
                    return member
        else:
            # Try to construct by value
            try:
                return cls(level)
            except Exception:
                pass

        allowed = "", "".join(m.name for m in cls)
        raise ValueError(f""Invalid {cls.__name__}: {level!r}. Allowed: {allowed}"")

    def __str__(self):
        name = getattr(self, ""name"", None)
        if isinstance(name, str):
            return name.lower()
        return str(getattr(self, ""value"", super().__str__()))"
354005,inveniosoftware/invenio-communities,inveniosoftware_invenio-communities/invenio_communities/roles.py,invenio_communities.roles.Role,"from dataclasses import dataclass, field

@dataclass(frozen=True)
class Role:
    """"""Role class.""""""
    name: str = ''
    'Name of the role.'
    title: str = ''
    'Title of the role.'
    description: str = ''
    'Brief description of capabilities of the role.'
    can_manage_roles: list = field(default_factory=list)
    'List of other roles that this role can manage.'
    is_owner: bool = False
    'This role is the owner role (only one can exists).'
    can_manage: bool = False
    'This role has manage permissions.'
    can_curate: bool = False
    'This role has record manage permissions.'
    can_view: bool = False
    'This role has view restricted record permissions.'

    def can_manage_role(self, role_name):
        """"""Determine if this role can manage the role name.""""""
        return role_name in self.can_manage_roles

    def __hash__(self):
        """"""Compute a hash for use with e.g. sets.""""""
        return self.name.__hash__()","@dataclass(frozen=True)
class Role:
    
    def can_manage_role(self, role_name):
        
        pass
    def __hash__(self):
        
        pass",snippet_287,3,"from dataclasses import dataclass, field
from typing import FrozenSet, Optional

@dataclass(frozen=True)
class Role:
    name: str
    manageable_roles: FrozenSet[str] = field(default_factory=frozenset)

    def can_manage_role(self, role_name: Optional[str]):
        if not isinstance(role_name, str):
            return False
        return role_name == self.name or role_name in self.manageable_roles

    def __hash__(self):
        return hash((self.name, self.manageable_roles))"
356596,grycap/RADL,grycap_RADL/radl/radl.py,radl.radl.Aspect,"import copy

class Aspect:
    """"""A network, ansible_host, system, deploy, configure or contextualize element in a RADL.""""""

    def getId(self):
        """"""Return the id of the aspect.""""""
        return id(self)

    def clone(self):
        """"""Return a copy of this aspect.""""""
        return copy.deepcopy(self)","
class Aspect:
    
    def getId(self):
        
        pass
    def clone(self):
        
        pass",snippet_288,3,"import copy

class Aspect:
    def __init__(self, id=None):
        self._id = id

    def getId(self):
        return self._id

    def clone(self):
        return copy.deepcopy(self)"
357236,pytroll/posttroll,posttroll/backends/zmq/message_broadcaster.py,posttroll.backends.zmq.message_broadcaster.ZMQDesignatedReceiversSender,"import threading
from posttroll.backends.zmq.socket import close_socket, set_up_client_socket
from zmq import LINGER, NOBLOCK, REQ, ZMQError

class ZMQDesignatedReceiversSender:
    """"""Sends message to multiple *receivers* on *port*.""""""

    def __init__(self, default_port, receivers):
        """"""Set up the sender.""""""
        self.default_port = default_port
        self.receivers = receivers
        self._shutdown_event = threading.Event()

    def __call__(self, data):
        """"""Send data.""""""
        for receiver in self.receivers:
            self._send_to_address(receiver, data)

    def _send_to_address(self, address, data, timeout=10):
        """"""Send data to *address* and *port* without verification of response.""""""
        if address.find(':') == -1:
            full_address = 'tcp://%s:%d' % (address, self.default_port)
        else:
            full_address = 'tcp://%s' % address
        options = {LINGER: int(timeout * 1000)}
        socket = set_up_client_socket(REQ, full_address, options)
        try:
            socket.send_string(data)
            while not self._shutdown_event.is_set():
                try:
                    message = socket.recv_string(NOBLOCK)
                except ZMQError:
                    self._shutdown_event.wait(0.1)
                    continue
                if message != 'ok':
                    logger.warning('invalid acknowledge received: %s' % message)
                break
        finally:
            close_socket(socket)

    def close(self):
        """"""Close the sender.""""""
        self._shutdown_event.set()","
class ZMQDesignatedReceiversSender:
    
    def __init__(self, default_port, receivers):
        
        pass
    def __call__(self, data):
        
        pass

    def _send_to_address(self, address, data, timeout=10):
        
        pass

    def close(self):
        
        pass",snippet_289,5,"import zmq
import socket
import pickle
from typing import Any, Dict, Iterable, Tuple, Union


class ZMQDesignatedReceiversSender:
    
    def __init__(self, default_port: int, receivers: Union[Dict[Any, Any], Iterable[Any]]):
        self._ctx = zmq.Context.instance()
        self._default_port = int(default_port)
        self._closed = False
        self._receivers = self._normalize_receivers(receivers)

    def __call__(self, data):
        if self._closed:
            raise RuntimeError(""Sender is closed"")
        # data can be:
        # - dict: {receiver_key: payload}
        # - tuple: (receiver_key, payload[, timeout])
        # - any payload: broadcast to all
        if isinstance(data, dict):
            for key, payload in data.items():
                address = self._receivers.get(key)
                if address is None:
                    raise KeyError(f""Unknown receiver '{key}'"")
                self._send_to_address(address, payload)
            return
        if isinstance(data, tuple):
            if len(data) not in (2, 3):
                raise ValueError(""Tuple form must be (receiver_key, payload[, timeout])"")
            key, payload = data[0], data[1]
            timeout = data[2] if len(data) == 3 else 10
            address = self._receivers.get(key)
            if address is None:
                raise KeyError(f""Unknown receiver '{key}'"")
            self._send_to_address(address, payload, timeout=timeout)
            return
        # broadcast
        for address in self._receivers.values():
            self._send_to_address(address, data)

    def _send_to_address(self, address, data, timeout=10):
        if self._closed:
            raise RuntimeError(""Sender is closed"")
        endpoint = self._normalize_endpoint(address)
        sock = self._ctx.socket(zmq.REQ)
        try:
            # timeouts in milliseconds
            ms = max(0, int(timeout * 1000))
            sock.setsockopt(zmq.LINGER, 0)
            sock.setsockopt(zmq.SNDTIMEO, ms)
            sock.setsockopt(zmq.RCVTIMEO, ms)
            sock.connect(endpoint)
            payload = self._serialize(data)
            sock.send(payload)
            # Expect a simple acknowledgement; if no reply within timeout, raise
            try:
                _ = sock.recv()
            except zmq.Again:
                raise TimeoutError(f""Timed out waiting for reply from {endpoint}"")
        finally:
            sock.close(0)

    def close(self):
        if self._closed:
            return
        self._closed = True
        # Do not terminate the shared instance context globally for other users.
        # Create our own context if strict termination is needed.
        # Here, gracefully create a dummy socket to ensure context is alive until now.
        # No additional resources to release because sockets are per-send.
        pass

    def _normalize_receivers(self, receivers: Union[Dict[Any, Any], Iterable[Any]]) -> Dict[Any, str]:
        normalized: Dict[Any, str] = {}
        if isinstance(receivers, dict):
            items = receivers.items()
        else:
            # iterable of addresses; key them by index
            items = enumerate(receivers)
        for key, value in items:
            normalized[key] = self._normalize_endpoint(value)
        return normalized

    def _normalize_endpoint(self, address: Any) -> str:
        # Accept:
        # - full endpoint like ""tcp://host:port"" or ""ipc://...""
        # - tuple (host, port)
        # - ""host"" -> tcp://host:default_port
        # - ""host:port""
        if isinstance(address, tuple) and len(address) == 2:
            host, port = address
            return f""tcp://{host}:{int(port)}""
        if isinstance(address, str):
            addr = address.strip()
            if ""://"" in addr:
                return addr
            if "":"" in addr:
                host, port = addr.rsplit("":"", 1)
                return f""tcp://{host}:{int(port)}""
            return f""tcp://{addr}:{self._default_port}""
        # Fallback to str()
        s = str(address)
        if ""://"" in s:
            return s
        if "":"" in s:
            host, port = s.rsplit("":"", 1)
            return f""tcp://{host}:{int(port)}""
        return f""tcp://{s}:{self._default_port}""

    def _serialize(self, data: Any) -> bytes:
        if isinstance(data, (bytes, bytearray, memoryview)):
            return bytes(data)
        if isinstance(data, str):
            return data.encode(""utf-8"")
        try:
            return pickle.dumps(data, protocol=pickle.HIGHEST_PROTOCOL)
        except Exception as e:
            raise TypeError(f""Unable to serialize data of type {type(data).__name__}"") from e"
357237,pytroll/posttroll,posttroll/backends/zmq/ns.py,posttroll.backends.zmq.ns.ZMQNameServer,"from posttroll.backends.zmq.socket import SocketReceiver, close_socket, set_up_client_socket, set_up_server_socket
from contextlib import suppress
from zmq import LINGER, REP, REQ
from posttroll.ns import get_active_address, get_configured_nameserver_port

class ZMQNameServer:
    """"""The name server.""""""

    def __init__(self):
        """"""Set up the nameserver.""""""
        self.running: bool = True
        self.listener: SocketReceiver | None = None
        self._authenticator = None

    def run(self, address_receiver, address: str | None=None):
        """"""Run the listener and answer to requests.""""""
        port = get_configured_nameserver_port()
        try:
            if not self.running:
                return
            if address is None:
                address = '*'
            address = create_nameserver_address(address)
            self.listener, _, self._authenticator = set_up_server_socket(REP, address)
            logger.debug(f'Nameserver listening on port {port}')
            socket_receiver = SocketReceiver()
            socket_receiver.register(self.listener)
            while self.running:
                try:
                    for msg, _ in socket_receiver.receive(self.listener, timeout=1):
                        logger.debug('Replying to request: ' + str(msg))
                        active_address = get_active_address(msg.data['service'], address_receiver, msg.version)
                        self.listener.send_unicode(str(active_address))
                except TimeoutError:
                    continue
        except KeyboardInterrupt:
            pass
        finally:
            socket_receiver.unregister(self.listener)
            self.close_sockets_and_threads()

    def close_sockets_and_threads(self):
        """"""Close all sockets and threads.""""""
        with suppress(AttributeError):
            close_socket(self.listener)
        with suppress(AttributeError):
            self._authenticator.stop()

    def stop(self):
        """"""Stop the name server.""""""
        self.running = False","
class ZMQNameServer:
    
    def __init__(self):
        
        pass
    def run(self, address_receiver, address: str | None=None):
        
        pass

    def close_sockets_and_threads(self):
        
        pass

    def stop(self):
        
        pass",snippet_290,5,"import threading
import json
import time
from typing import Optional
try:
    import zmq
except ImportError as e:
    raise ImportError(""pyzmq is required to use ZMQNameServer. Install with: pip install pyzmq"") from e


class ZMQNameServer:
    def __init__(self):
        self._context = zmq.Context.instance()
        self._sub_sock = None
        self._rep_sock = None
        self._recv_thread = None
        self._serve_thread = None
        self._stop_event = threading.Event()
        self._registry = {}
        self._lock = threading.RLock()

    def run(self, address_receiver, address: str | None = None):
        if self._recv_thread or self._serve_thread:
            return

        if address is None:
            address = ""tcp://*:5555""

        self._sub_sock = self._context.socket(zmq.SUB)
        self._sub_sock.setsockopt(zmq.SUBSCRIBE, b"""")
        self._sub_sock.connect(address_receiver)

        self._rep_sock = self._context.socket(zmq.REP)
        self._rep_sock.bind(address)

        self._stop_event.clear()
        self._recv_thread = threading.Thread(target=self._recv_loop, name=""ZMQNameServer-Receiver"", daemon=True)
        self._serve_thread = threading.Thread(target=self._serve_loop, name=""ZMQNameServer-Server"", daemon=True)
        self._recv_thread.start()
        self._serve_thread.start()

    def close_sockets_and_threads(self):
        self._stop_event.set()

        try:
            if self._sub_sock is not None:
                self._sub_sock.close(linger=0)
        except Exception:
            pass
        finally:
            self._sub_sock = None

        try:
            if self._rep_sock is not None:
                self._rep_sock.close(linger=0)
        except Exception:
            pass
        finally:
            self._rep_sock = None

        if self._recv_thread is not None:
            self._recv_thread.join(timeout=2.0)
            self._recv_thread = None

        if self._serve_thread is not None:
            self._serve_thread.join(timeout=2.0)
            self._serve_thread = None

        try:
            # Do not terminate the shared instance context if other users might rely on it.
            # But if no sockets are open, terminating is fine.
            self._context.term()
        except Exception:
            pass

    def stop(self):
        self.close_sockets_and_threads()

    # Internal methods

    def _recv_loop(self):
        poller = zmq.Poller()
        if self._sub_sock is None:
            return
        poller.register(self._sub_sock, zmq.POLLIN)

        while not self._stop_event.is_set():
            try:
                events = dict(poller.poll(timeout=200))
            except zmq.error.ZMQError:
                break

            if self._sub_sock in events and events[self._sub_sock] & zmq.POLLIN:
                try:
                    msg = self._sub_sock.recv_string(flags=zmq.NOBLOCK).strip()
                except zmq.Again:
                    continue
                except Exception:
                    continue
                self._handle_announcement(msg)

        # Drain
        try:
            poller.unregister(self._sub_sock)
        except Exception:
            pass

    def _serve_loop(self):
        poller = zmq.Poller()
        if self._rep_sock is None:
            return
        poller.register(self._rep_sock, zmq.POLLIN)

        while not self._stop_event.is_set():
            try:
                events = dict(poller.poll(timeout=200))
            except zmq.error.ZMQError:
                break

            if self._rep_sock in events and events[self._rep_sock] & zmq.POLLIN:
                try:
                    req = self._rep_sock.recv_string().strip()
                except Exception:
                    # If something went wrong, try to send a generic error
                    self._safe_send_rep(""ERR"")
                    continue
                resp = self._handle_request(req)
                self._safe_send_rep(resp)

        try:
            poller.unregister(self._rep_sock)
        except Exception:
            pass

    def _safe_send_rep(self, msg: str):
        try:
            if self._rep_sock is not None:
                self._rep_sock.send_string(msg)
        except Exception:
            pass

    def _handle_announcement(self, msg: str):
        parts = msg.split()
        if not parts:
            return
        cmd = parts[0].upper()

        if cmd == ""REGISTER"" and len(parts) >= 3:
            name = parts[1]
            addr = "" "".join(parts[2:])
            with self._lock:
                self._registry[name] = addr
        elif cmd == ""UNREGISTER"" and len(parts) >= 2:
            name = parts[1]
            with self._lock:
                self._registry.pop(name, None)

    def _handle_request(self, req: str) -> str:
        parts = req.split()
        if not parts:
            return ""ERR empty""

        cmd = parts[0].upper()

        if cmd == ""PING"":
            return ""PONG""

        if cmd == ""RESOLVE"" and len(parts) >= 2:
            name = parts[1]
            with self._lock:
                addr = self._registry.get(name)
            if addr is None:
                return ""NOT_FOUND""
            return f""OK {addr}""

        if cmd == ""REGISTER"" and len(parts) >= 3:
            name = parts[1]
            addr = "" "".join(parts[2:])
            with self._lock:
                self._registry[name] = addr
            return ""OK""

        if cmd == ""UNREGISTER"" and len(parts) >= 2:
            name = parts[1]
            with self._lock:
                existed = name in self._registry
                self._registry.pop(name, None)
            return ""OK"" if existed else ""NOT_FOUND""

        if cmd == ""LIST"":
            with self._lock:
                data = json.dumps(self._registry, separators=("","", "":""))
            return f""OK {data}""

        if cmd == ""COUNT"":
            with self._lock:
                count = len(self._registry)
            return f""OK {count}""

        return ""ERR unknown_command"""
357239,pytroll/posttroll,posttroll/backends/zmq/socket.py,posttroll.backends.zmq.socket.SocketReceiver,"from posttroll.message import Message
import zmq

class SocketReceiver:
    """"""A receiver for mulitple sockets.""""""

    def __init__(self):
        """"""Set up the receiver.""""""
        self._poller = zmq.Poller()

    def register(self, socket):
        """"""Register the socket.""""""
        self._poller.register(socket, zmq.POLLIN)

    def unregister(self, socket):
        """"""Unregister the socket.""""""
        self._poller.unregister(socket)

    def receive(self, *sockets, timeout=None):
        """"""Timeout is in seconds.""""""
        if timeout:
            timeout *= 1000
        socks = dict(self._poller.poll(timeout=timeout))
        if socks:
            for sock in sockets:
                if socks.get(sock) == zmq.POLLIN:
                    received = sock.recv_string(zmq.NOBLOCK)
                    yield (Message.decode(received), sock)
        else:
            raise TimeoutError('Did not receive anything on sockets.')","
class SocketReceiver:
    
    def __init__(self):
        
        pass
    def register(self, socket):
        
        pass

    def unregister(self, socket):
        
        pass

    def receive(self, *sockets, timeout=None):
        
        pass",snippet_291,5,"import selectors
from typing import Iterable, Tuple, List, Optional


class SocketReceiver:
    
    def __init__(self):
        self._selector = selectors.DefaultSelector()
        self._registered = set()

    def register(self, socket):
        if socket in self._registered:
            return
        try:
            self._selector.register(socket, selectors.EVENT_READ)
            self._registered.add(socket)
        except KeyError:
            # Already registered in selector; keep state consistent
            self._registered.add(socket)

    def unregister(self, socket):
        if socket in self._registered:
            try:
                self._selector.unregister(socket)
            except Exception:
                pass
            finally:
                self._registered.discard(socket)

    def _select_and_recv(self, sel: selectors.BaseSelector, timeout: Optional[float]) -> List[Tuple[object, bytes]]:
        ready = sel.select(timeout)
        results: List[Tuple[object, bytes]] = []
        for key, _ in ready:
            sock = key.fileobj
            try:
                data = sock.recv(4096)
            except BlockingIOError:
                continue
            except InterruptedError:
                continue
            except Exception:
                # On unexpected errors, treat as closed
                data = b""""
            if sel is self._selector and data == b"""":
                # If connection closed, unregister from persistent selector
                self.unregister(sock)
            results.append((sock, data))
        return results

    def receive(self, *sockets, timeout=None):
        if sockets:
            temp_sel = selectors.DefaultSelector()
            try:
                # Allow iterables passed directly or multiple args
                socks: Iterable = sockets
                if len(sockets) == 1 and hasattr(sockets[0], ""__iter__"") and not hasattr(sockets[0], ""recv""):
                    socks = sockets[0]
                for s in socks:
                    try:
                        temp_sel.register(s, selectors.EVENT_READ)
                    except Exception:
                        continue
                return self._select_and_recv(temp_sel, timeout)
            finally:
                temp_sel.close()
        else:
            if not self._registered:
                return []
            return self._select_and_recv(self._selector, timeout)"
357241,pytroll/posttroll,posttroll/bbmcast.py,posttroll.bbmcast.MulticastReceiver,"import struct
from socket import AF_INET, INADDR_ANY, IP_ADD_MEMBERSHIP, IP_MULTICAST_IF, IP_MULTICAST_LOOP, IP_MULTICAST_TTL, IPPROTO_IP, SO_BROADCAST, SO_LINGER, SO_REUSEADDR, SOCK_DGRAM, SOL_IP, SOL_SOCKET, gethostbyname, inet_aton, socket, timeout

class MulticastReceiver:
    """"""Multicast receiver on *port* for an *mcgroup*.""""""
    BUFSIZE = 1024

    def __init__(self, port, mcgroup=None):
        """"""Set up the multicast receiver.""""""
        self.port = port
        self.socket, self.group = mcast_receiver(port, mcgroup)
        logger.info(f'Receiver initialized on group {self.group}.')

    def settimeout(self, tout=None):
        """"""Set timeout.

        A timeout will throw a 'socket.timeout'.
        """"""
        self.socket.settimeout(tout)
        return self

    def __call__(self):
        """"""Receive data from a socket.""""""
        data, sender = self.socket.recvfrom(self.BUFSIZE)
        return (data.decode(), sender)

    def close(self):
        """"""Close the receiver.""""""
        self.socket.setsockopt(SOL_SOCKET, SO_LINGER, struct.pack('ii', 1, 1))
        self.socket.close()","
class MulticastReceiver:
    
    def __init__(self, port, mcgroup=None):
        
        pass
    def settimeout(self, tout=None):
        
        pass

    def __call__(self):
        
        pass

    def close(self):
        
        pass",snippet_292,5,"import socket
import struct


class MulticastReceiver:
    def __init__(self, port, mcgroup=None):
        self._sock = None
        self._af = socket.AF_INET
        self._port = int(port)
        self._mcgroup = mcgroup
        self._timeout = None

        if mcgroup and ':' in mcgroup:
            self._af = socket.AF_INET6

        self._sock = socket.socket(self._af, socket.SOCK_DGRAM)

        try:
            self._sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        except OSError:
            pass
        try:
            self._sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)
        except (AttributeError, OSError):
            pass

        if self._af == socket.AF_INET6:
            bind_addr = '::'
            try:
                self._sock.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 1)
            except OSError:
                pass
        else:
            bind_addr = '0.0.0.0'

        self._sock.bind((bind_addr, self._port))

        if self._mcgroup:
            if self._af == socket.AF_INET:
                mreq = struct.pack('=4s4s', socket.inet_aton(self._mcgroup), socket.inet_aton('0.0.0.0'))
                self._sock.setsockopt(socket.IPPROTO_IP, socket.IP_ADD_MEMBERSHIP, mreq)
            else:
                group_bin = socket.inet_pton(socket.AF_INET6, self._mcgroup)
                mreq6 = struct.pack('=16sI', group_bin, 0)
                self._sock.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_JOIN_GROUP, mreq6)

    def settimeout(self, tout=None):
        self._timeout = tout
        self._sock.settimeout(tout)

    def __call__(self):
        return self._sock.recvfrom(65535)

    def close(self):
        if self._sock is not None:
            try:
                self._sock.close()
            finally:
                self._sock = None"
357242,pytroll/posttroll,posttroll/bbmcast.py,posttroll.bbmcast.MulticastSender,"class MulticastSender:
    """"""Multicast sender on *port* and *mcgroup*.""""""

    def __init__(self, port, mcgroup=None):
        """"""Set up the multicast sender.""""""
        self.port = port
        self.group = mcgroup
        self.socket, self.group = mcast_sender(mcgroup)
        logger.debug('Started multicast group %s', self.group)

    def __call__(self, data):
        """"""Send data to a socket.""""""
        self.socket.sendto(data.encode(), (self.group, self.port))

    def close(self):
        """"""Close the sender.""""""
        self.socket.close()","class MulticastSender:
    
    def __init__(self, port, mcgroup=None):
        
        pass
    def __call__(self, data):
        
        pass
    def close(self):
        
        pass",snippet_293,4,"import socket
import struct
import ipaddress


class MulticastSender:
    
    def __init__(self, port, mcgroup=None):
        if mcgroup is None:
            mcgroup = ""239.255.255.250""
        if not isinstance(port, int) or not (0 < port < 65536):
            raise ValueError(""port must be in 1..65535"")
        self.port = port
        self.group = str(mcgroup)
        self._closed = False

        addr = ipaddress.ip_address(self.group)
        if addr.version == 4:
            self.family = socket.AF_INET
            self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)
            try:
                self.sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_TTL, struct.pack(""b"", 1))
                self.sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_LOOP, struct.pack(""b"", 1))
            except OSError:
                pass
            self._dest = (self.group, self.port)
        else:
            self.family = socket.AF_INET6
            self.sock = socket.socket(socket.AF_INET6, socket.SOCK_DGRAM, socket.IPPROTO_UDP)
            try:
                self.sock.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_MULTICAST_HOPS, 1)
                self.sock.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_MULTICAST_LOOP, 1)
            except OSError:
                pass
            self._dest = (self.group, self.port, 0, 0)

    def __call__(self, data):
        if self._closed:
            raise RuntimeError(""MulticastSender is closed"")
        if isinstance(data, str):
            payload = data.encode(""utf-8"")
        elif isinstance(data, (bytes, bytearray, memoryview)):
            payload = data
        else:
            payload = bytes(data)
        return self.sock.sendto(payload, self._dest)

    def close(self):
        if not self._closed:
            try:
                self.sock.close()
            finally:
                self._closed = True"
357244,pytroll/posttroll,posttroll/listener.py,posttroll.listener.ListenerContainer,"from threading import Thread
from queue import Queue
import logging

class ListenerContainer:
    """"""Container for a listener instance.""""""
    logger = logging.getLogger(__name__ + '.ListenerContainer')

    def __init__(self, topics=None, addresses=None, nameserver='localhost', services=''):
        """"""Initialize the class.""""""
        self.listener = None
        self.output_queue = None
        self.thread = None
        self.addresses = addresses
        self.nameserver = nameserver
        if topics is not None:
            self.output_queue = Queue()
            self.listener = Listener(topics=topics, queue=self.output_queue, addresses=self.addresses, nameserver=self.nameserver, services=services)
            self.thread = Thread(target=self.listener.run, daemon=True)
            self.thread.start()

    def __setstate__(self, state):
        """"""Re-initialize the class.""""""
        self.__init__(**state)

    def restart_listener(self, topics):
        """"""Restart listener after configuration update.""""""
        if self.listener is not None:
            if self.listener.running:
                self.stop()
        self.__init__(topics=topics)

    def stop(self):
        """"""Stop listener.""""""
        self.logger.debug('Stopping listener.')
        self.listener.stop()
        if self.thread is not None:
            self.thread.join()
            self.thread = None
        self.logger.debug('Listener stopped.')","
class ListenerContainer:
    
    def __init__(self, topics=None, addresses=None, nameserver='localhost', services=''):
        
        pass
    def __setstate__(self, state):
        
        pass

    def restart_listener(self, topics):
        
        pass

    def stop(self):
        
        pass",snippet_294,5,"class ListenerContainer:
    import threading
    import logging
    import time

    class _Listener:
        def __init__(self, topic, address=None, logger=None):
            self.topic = topic
            self.address = address
            self._stop_event = ListenerContainer.threading.Event()
            self._thread = None
            self._logger = logger or ListenerContainer.logging.getLogger(__name__)

        def start(self):
            if self._thread and self._thread.is_alive():
                return
            self._thread = ListenerContainer.threading.Thread(target=self._run, name=f""Listener-{self.topic}"", daemon=True)
            self._thread.start()

        def _run(self):
            # Dummy loop to simulate a listener lifecycle
            self._logger.debug(""Listener started for topic=%s address=%s"", self.topic, self.address)
            while not self._stop_event.is_set():
                # In a real implementation this would block on I/O
                self._stop_event.wait(0.5)
            self._logger.debug(""Listener stopping for topic=%s"", self.topic)

        def stop(self, timeout=2.0):
            self._stop_event.set()
            if self._thread and self._thread.is_alive():
                self._thread.join(timeout)

        def is_alive(self):
            return self._thread.is_alive() if self._thread else False

    def __init__(self, topics=None, addresses=None, nameserver='localhost', services=''):
        self._log = self.logging.getLogger(self.__class__.__name__)
        self._lock = self.threading.RLock()

        self.nameserver = nameserver
        self.services = services

        self.topics = self._normalize_topics(topics)
        self.addresses = self._normalize_addresses(addresses, self.topics)

        self._listeners = {}
        self._running = False

        if self.topics:
            self._start_listeners(self.topics)

    def __setstate__(self, state):
        with self._lock:
            self.nameserver = state.get('nameserver', 'localhost')
            self.services = state.get('services', '')
            self.topics = self._normalize_topics(state.get('topics'))
            self.addresses = self._normalize_addresses(state.get('addresses'), self.topics)
            self._listeners = {}
            self._running = False
            if self.topics:
                self._start_listeners(self.topics)

    def restart_listener(self, topics):
        new_topics = self._normalize_topics(topics)
        with self._lock:
            to_stop = set(self._listeners.keys()) - new_topics
            to_start = new_topics - set(self._listeners.keys())

            for t in to_stop:
                lst = self._listeners.pop(t, None)
                if lst:
                    try:
                        lst.stop()
                    except Exception:
                        self._log.exception(""Error stopping listener for topic=%s"", t)

            # Update topics and addresses mapping
            self.topics = new_topics
            self.addresses = self._normalize_addresses(self.addresses, self.topics)

            for t in to_start:
                addr = self._resolve_address_for_topic(t)
                listener = self._Listener(topic=t, address=addr, logger=self._log)
                self._listeners[t] = listener
                try:
                    listener.start()
                except Exception:
                    self._log.exception(""Error starting listener for topic=%s"", t)

            self._running = bool(self._listeners)

    def stop(self):
        with self._lock:
            for t, lst in list(self._listeners.items()):
                try:
                    lst.stop()
                except Exception:
                    self._log.exception(""Error stopping listener for topic=%s"", t)
            self._listeners.clear()
            self._running = False

    # Internal helpers
    def _start_listeners(self, topics):
        with self._lock:
            for t in topics:
                if t in self._listeners:
                    continue
                addr = self._resolve_address_for_topic(t)
                listener = self._Listener(topic=t, address=addr, logger=self._log)
                self._listeners[t] = listener
                listener.start()
            self._running = bool(self._listeners)

    def _normalize_topics(self, topics):
        if topics is None:
            return set()
        if isinstance(topics, (set, list, tuple)):
            return {str(t) for t in topics if t is not None}
        return {str(topics)}

    def _normalize_addresses(self, addresses, topics):
        if not topics:
            return {}
        topics_list = list(topics)
        # If addresses is a mapping, filter to topics
        if isinstance(addresses, dict):
            return {str(k): addresses[k] for k in map(str, topics_list) if k in addresses}
        # If addresses is a list/tuple aligned with topics
        if isinstance(addresses, (list, tuple)):
            result = {}
            for i, t in enumerate(topics_list):
                result[str(t)] = addresses[i] if i < len(addresses) else None
            return result
        # Single address for all topics
        if addresses is None:
            default = None
        else:
            default = addresses
        return {str(t): default for t in topics_list}

    def _resolve_address_for_topic(self, topic):
        key = str(topic)
        if key in self.addresses:
            return self.addresses[key]
        # Fallback default
        return None"
357253,pytroll/posttroll,posttroll/message_broadcaster.py,posttroll.message_broadcaster.DesignatedReceiversSender,"from posttroll import config, message

class DesignatedReceiversSender:
    """"""Sends message to multiple *receivers* on *port*.""""""

    def __init__(self, default_port, receivers):
        """"""Set settings.""""""
        backend = config.get('backend', 'unsecure_zmq')
        if backend == 'unsecure_zmq':
            from posttroll.backends.zmq.message_broadcaster import ZMQDesignatedReceiversSender
            self._sender = ZMQDesignatedReceiversSender(default_port, receivers)
        else:
            raise NotImplementedError()

    def __call__(self, data):
        """"""Send messages from all receivers.""""""
        return self._sender(data)

    def close(self):
        """"""Close the sender.""""""
        return self._sender.close()","
class DesignatedReceiversSender:
    
    def __init__(self, default_port, receivers):
        
        pass
    def __call__(self, data):
        
        pass

    def close(self):
        
        pass",snippet_295,4,"import socket
from typing import Iterable, Tuple, Union, List, Dict, Any


class DesignatedReceiversSender:
    
    def __init__(self, default_port: int, receivers: Iterable[Union[str, Tuple[str, int], Tuple[str]]]):
        if not isinstance(default_port, int) or not (0 <= default_port <= 65535):
            raise ValueError(""default_port must be an integer between 0 and 65535"")
        self._default_port = default_port

        self._targets: List[Tuple[int, int, int, Any]] = []
        self._sockets: Dict[Tuple[int, int, int], socket.socket] = {}
        self._closed = False

        parsed = [self._parse_receiver(r) for r in receivers]
        if not parsed:
            raise ValueError(""receivers cannot be empty"")

        for host, port in parsed:
            for fam, stype, proto, _, sockaddr in socket.getaddrinfo(host, port, 0, socket.SOCK_DGRAM, 0, socket.AI_ADDRCONFIG):
                self._targets.append((fam, stype, proto, sockaddr))

        if not self._targets:
            raise ValueError(""No valid receiver addresses resolved"")

        for fam, stype, proto, _ in self._targets:
            key = (fam, stype, proto)
            if key not in self._sockets:
                s = socket.socket(fam, stype, proto)
                # Allow IPv6 socket to send IPv4-mapped if possible
                if fam == socket.AF_INET6:
                    try:
                        s.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 0)
                    except OSError:
                        pass
                s.setblocking(True)
                self._sockets[key] = s

    def __call__(self, data: Union[bytes, bytearray, memoryview, str]) -> int:
        if self._closed:
            raise RuntimeError(""Sender is closed"")
        if isinstance(data, str):
            payload = data.encode(""utf-8"")
        elif isinstance(data, (bytes, bytearray, memoryview)):
            payload = bytes(data)
        else:
            raise TypeError(""data must be bytes-like or str"")

        sent_count = 0
        for fam, stype, proto, sockaddr in self._targets:
            sock = self._sockets.get((fam, stype, proto))
            if sock is None:
                continue
            try:
                sock.sendto(payload, sockaddr)
                sent_count += 1
            except OSError:
                # Continue sending to other receivers
                continue
        return sent_count

    def close(self):
        if self._closed:
            return
        self._closed = True
        for s in self._sockets.values():
            try:
                s.close()
            except Exception:
                pass
        self._sockets.clear()
        self._targets.clear()

    def _parse_receiver(self, r: Union[str, Tuple[str, int], Tuple[str]]) -> Tuple[str, int]:
        if isinstance(r, tuple):
            if len(r) == 2:
                host, port = r
                if port is None:
                    port = self._default_port
            elif len(r) == 1:
                host = r[0]
                port = self._default_port
            else:
                raise ValueError(f""Invalid receiver tuple: {r}"")
            return self._validate_host_port(host, port)

        if isinstance(r, str):
            r = r.strip()
            if not r:
                raise ValueError(""Empty receiver string"")
            if r.startswith(""[""):
                # Bracketed IPv6: [addr]:port or [addr]
                end = r.find(""]"")
                if end == -1:
                    raise ValueError(f""Invalid bracketed address: {r}"")
                host = r[1:end]
                rest = r[end + 1 :]
                if rest.startswith("":""):
                    port_str = rest[1:]
                    port = int(port_str)
                else:
                    port = self._default_port
            else:
                if "":"" in r:
                    host_part, port_part = r.rsplit("":"", 1)
                    host = host_part
                    port = int(port_part)
                else:
                    host = r
                    port = self._default_port
            return self._validate_host_port(host, port)

        raise TypeError(f""Unsupported receiver type: {type(r)}"")

    def _validate_host_port(self, host: Any, port: Any) -> Tuple[str, int]:
        if not isinstance(host, str) or not host:
            raise ValueError(f""Invalid host: {host!r}"")
        if not isinstance(port, int) or not (0 <= port <= 65535):
            raise ValueError(f""Invalid port: {port!r}"")
        return host, port"
357255,pytroll/posttroll,posttroll/ns.py,posttroll.ns.NameServer,"from posttroll.address_receiver import AddressReceiver
import datetime as dt
from posttroll import config
from contextlib import suppress

class NameServer:
    """"""The name server.""""""

    def __init__(self, max_age=None, multicast_enabled=True, restrict_to_localhost=False):
        """"""Initialize nameserver.""""""
        self.loop = True
        self.listener = None
        self._max_age = max_age or dt.timedelta(minutes=10)
        self._multicast_enabled = multicast_enabled
        self._restrict_to_localhost = restrict_to_localhost
        backend = config['backend']
        if backend not in ['unsecure_zmq', 'secure_zmq']:
            raise NotImplementedError(f'Did not recognize backend: {backend}')
        from posttroll.backends.zmq.ns import ZMQNameServer
        self._ns = ZMQNameServer()

    def run(self, address_receiver=None, nameserver_address=None):
        """"""Run the listener and answer to requests.""""""
        if address_receiver is None:
            address_receiver = AddressReceiver(max_age=self._max_age, multicast_enabled=self._multicast_enabled, restrict_to_localhost=self._restrict_to_localhost)
            address_receiver.start()
        try:
            return self._ns.run(address_receiver, nameserver_address)
        finally:
            with suppress(AttributeError):
                address_receiver.stop()

    def stop(self):
        """"""Stop the nameserver.""""""
        return self._ns.stop()","
class NameServer:
    
    def __init__(self, max_age=None, multicast_enabled=True, restrict_to_localhost=False):
        
        pass
    def run(self, address_receiver=None, nameserver_address=None):
        
        pass

    def stop(self):
        
        pass",snippet_296,4,"class NameServer:
    def __init__(self, max_age=None, multicast_enabled=True, restrict_to_localhost=False):
        import threading
        self.max_age = max_age
        self.multicast_enabled = multicast_enabled
        self.restrict_to_localhost = restrict_to_localhost

        self._running = threading.Event()
        self._thread = None
        self._sock = None
        self._lock = threading.Lock()
        self._address = None

    def _parse_address(self, nameserver_address):
        if nameserver_address is None:
            host = ""127.0.0.1"" if self.restrict_to_localhost else ""0.0.0.0""
            port = 0
            return host, port
        if isinstance(nameserver_address, tuple) and len(nameserver_address) == 2:
            return nameserver_address[0], int(nameserver_address[1])
        if isinstance(nameserver_address, str):
            if "":"" in nameserver_address:
                host, port = nameserver_address.rsplit("":"", 1)
                return host, int(port)
            return nameserver_address, 0
        raise ValueError(""Unsupported nameserver_address format"")

    def _serve(self):
        import socket
        self._sock.settimeout(0.25)
        while self._running.is_set():
            try:
                # Accept and immediately close; just to keep the port alive.
                conn, _ = self._sock.accept()
            except socket.timeout:
                continue
            except OSError:
                break
            else:
                try:
                    conn.close()
                except Exception:
                    pass

    def run(self, address_receiver=None, nameserver_address=None):
        import socket
        with self._lock:
            if self._running.is_set():
                if callable(address_receiver):
                    try:
                        address_receiver(self._address)
                    except Exception:
                        pass
                return self._address

            host, port = self._parse_address(nameserver_address)

            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            try:
                sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            except Exception:
                pass
            sock.bind((host, port))
            sock.listen(5)
            bound_host, bound_port = sock.getsockname()
            self._sock = sock
            self._address = (bound_host, bound_port)
            self._running.set()

            if callable(address_receiver):
                # Notify in a separate thread to avoid blocking run.
                import threading

                def _notify():
                    try:
                        address_receiver(self._address)
                    except Exception:
                        pass

                threading.Thread(target=_notify, daemon=True).start()

            import threading
            self._thread = threading.Thread(target=self._serve, daemon=True)
            self._thread.start()

            return self._address

    def stop(self):
        with self._lock:
            if not self._running.is_set():
                return
            self._running.clear()
            try:
                if self._sock is not None:
                    try:
                        self._sock.shutdown(0)
                    except Exception:
                        pass
                    self._sock.close()
            finally:
                self._sock = None
        if self._thread is not None:
            try:
                self._thread.join(timeout=2.0)
            finally:
                self._thread = None
        self._address = None"
357257,pytroll/posttroll,posttroll/publisher.py,posttroll.publisher.Publish,"class Publish:
    """"""The publishing context.

    See :class:`Publisher` and :class:`NoisyPublisher` for more information on the arguments.

    The publisher is selected based on the arguments, see :func:`create_publisher_from_dict_config` for
    information how the selection is done.

    Example on how to use the :class:`Publish` context::

            from posttroll.publisher import Publish
            from posttroll.message import Message
            import time

            try:
                with Publish(""my_service"", port=9000) as pub:
                    counter = 0
                    while True:
                        counter += 1
                        message = Message(""/counter"", ""info"", str(counter))
                        print(""publishing"", message)
                        pub.send(message.encode())
                        time.sleep(3)
            except KeyboardInterrupt:
                print(""terminating publisher..."")

    """"""

    def __init__(self, name, port=0, aliases=None, broadcast_interval=2, nameservers=None, min_port=None, max_port=None):
        """"""Initialize the class.""""""
        settings = {'name': name, 'port': port, 'min_port': min_port, 'max_port': max_port, 'aliases': aliases, 'broadcast_interval': broadcast_interval, 'nameservers': nameservers}
        self.publisher = create_publisher_from_dict_config(settings)

    def __enter__(self):
        """"""Enter the context.""""""
        return self.publisher.start()

    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""Exit the context.""""""
        self.publisher.stop()","class Publish:
    '''The publishing context.
    See :class:`Publisher` and :class:`NoisyPublisher` for more information on the arguments.
    The publisher is selected based on the arguments, see :func:`create_publisher_from_dict_config` for
    information how the selection is done.
    Example on how to use the :class:`Publish` context::
            from posttroll.publisher import Publish
            from posttroll.message import Message
            import time
            try:
                with Publish(""my_service"", port=9000) as pub:
                    counter = 0
                    while True:
                        counter += 1
                        message = Message(""/counter"", ""info"", str(counter))
                        print(""publishing"", message)
                        pub.send(message.encode())
                        time.sleep(3)
            except KeyboardInterrupt:
                print(""terminating publisher..."")
    '''
    def __init__(self, name, port=0, aliases=None, broadcast_interval=2, nameservers=None, min_port=None, max_port=None):
        
        pass

    def __enter__(self):
        
        pass

    def __exit__(self, exc_type, exc_val, exc_tb):
        
        pass",snippet_297,4,"class Publish:
    '''The publishing context.
    See :class:`Publisher` and :class:`NoisyPublisher` for more information on the arguments.
    The publisher is selected based on the arguments, see :func:`create_publisher_from_dict_config` for
    information how the selection is done.
    Example on how to use the :class:`Publish` context::
            from posttroll.publisher import Publish
            from posttroll.message import Message
            import time
            try:
                with Publish(""my_service"", port=9000) as pub:
                    counter = 0
                    while True:
                        counter += 1
                        message = Message(""/counter"", ""info"", str(counter))
                        print(""publishing"", message)
                        pub.send(message.encode())
                        time.sleep(3)
            except KeyboardInterrupt:
                print(""terminating publisher..."")
    '''
    def __init__(self, name, port=0, aliases=None, broadcast_interval=2, nameservers=None, min_port=None, max_port=None):
        self.name = name
        self.port = port
        self.aliases = aliases
        self.broadcast_interval = broadcast_interval
        self.nameservers = nameservers
        self.min_port = min_port
        self.max_port = max_port

        self._publisher = None
        self._publisher_entered = False
        self._exit_callable = None
        self._closed = False
        self._fallback_messages = []

    def _build_config(self):
        cfg = {
            ""name"": self.name,
            ""port"": self.port,
            ""aliases"": self.aliases,
            ""broadcast_interval"": self.broadcast_interval,
            ""nameservers"": self.nameservers,
            ""min_port"": self.min_port,
            ""max_port"": self.max_port,
        }
        # Remove None values to let defaults apply
        return {k: v for k, v in cfg.items() if v is not None}

    def _create_publisher(self):
        try:
            # Try to import factory from posttroll if available
            from posttroll.publisher import create_publisher_from_dict_config  # type: ignore
        except Exception:
            create_publisher_from_dict_config = None

        if create_publisher_from_dict_config is None:
            # Fallback dummy publisher
            class _FallbackPublisher:
                def __init__(self, outer):
                    self._outer = outer

                def send(self, data):
                    self._outer._fallback_messages.append(data)

                def close(self):
                    pass

            return _FallbackPublisher(self)

        cfg = self._build_config()
        return create_publisher_from_dict_config(cfg)

    def __enter__(self):
        self._publisher = self._create_publisher()

        # If the underlying publisher is a context manager, enter it
        if hasattr(self._publisher, ""__enter__"") and hasattr(self._publisher, ""__exit__""):
            entered = self._publisher.__enter__()
            # Some __enter__ return self; others return a different handle
            # We will use whatever is returned for sending, but still
            # return this Publish wrapper for stable API.
            if entered is not None:
                self._publisher = entered
            self._publisher_entered = True
            self._exit_callable = getattr(self._publisher, ""__exit__"", None)
        else:
            # Prepare a generic exit/close routine
            def _generic_exit(exc_type, exc_val, exc_tb):
                # Try common termination methods in order
                for method_name in (""close"", ""stop"", ""shutdown"", ""terminate""):
                    m = getattr(self._publisher, method_name, None)
                    if callable(m):
                        try:
                            m()
                            break
                        except Exception:
                            pass
                return False

            self._exit_callable = _generic_exit

        return self

    def send(self, data):
        if self._closed:
            raise RuntimeError(""Publisher is closed"")
        sender = getattr(self._publisher, ""send"", None)
        if callable(sender):
            return sender(data)
        # Fallback: store locally if no sender available
        self._fallback_messages.append(data)
        return None

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self._closed:
            return False
        try:
            if callable(self._exit_callable):
                return bool(self._exit_callable(exc_type, exc_val, exc_tb))
            return False
        finally:
            self._closed = True"
357261,pytroll/posttroll,posttroll/subscriber.py,posttroll.subscriber.Subscribe,"from posttroll.message import _MAGICK

class Subscribe:
    """"""Subscriber context.

    See :class:`NSSubscriber` and :class:`Subscriber` for initialization parameters.

    The subscriber is selected based on the arguments, see :func:`create_subscriber_from_dict_config` for
    information how the selection is done.

    Example::
            del tmp

        from posttroll.subscriber import Subscribe

        with Subscribe(""a_service"", ""my_topic"",) as sub:
            for msg in sub.recv():
                print(msg)

    """"""

    def __init__(self, services='', topics=_MAGICK, addr_listener=False, addresses=None, timeout=10, translate=False, nameserver='localhost', message_filter=None):
        """"""Initialize the class.""""""
        settings = {'services': services, 'topics': topics, 'message_filter': message_filter, 'translate': translate, 'addr_listener': addr_listener, 'addresses': addresses, 'timeout': timeout, 'nameserver': nameserver}
        self.subscriber = create_subscriber_from_dict_config(settings)

    def __enter__(self):
        """"""Start the subscriber when used as a context manager.""""""
        return self.subscriber

    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""Stop the subscriber when used as a context manager.""""""
        return self.subscriber.stop()","
class Subscribe:
    
    def __init__(self, services='', topics=_MAGICK, addr_listener=False, addresses=None, timeout=10, translate=False, nameserver='localhost', message_filter=None):
        
        pass
    def __enter__(self):
        
        pass

    def __exit__(self, exc_type, exc_val, exc_tb):
        
        pass",snippet_298,4,"_MAGICK = object()

class Subscribe:
    
    def __init__(self, services='', topics=_MAGICK, addr_listener=False, addresses=None, timeout=10, translate=False, nameserver='localhost', message_filter=None):
        self.addr_listener = bool(addr_listener)
        self.timeout = float(timeout) if timeout is not None else None
        self.translate = bool(translate)
        self.nameserver = str(nameserver) if nameserver is not None else 'localhost'

        # Normalize services
        if services is None:
            self.services = []
        elif isinstance(services, str):
            parts = [s.strip() for s in services.replace(';', ',').split(',') if s.strip()]
            self.services = parts
        else:
            self.services = list(services)

        # Normalize topics with sentinel default
        if topics is _MAGICK:
            self.topics = []
        elif topics is None:
            self.topics = []
        elif isinstance(topics, str):
            parts = [t.strip() for t in topics.replace(';', ',').split(',') if t.strip()]
            self.topics = parts
        else:
            self.topics = list(topics)

        # Normalize addresses
        if addresses is None:
            self.addresses = []
        elif isinstance(addresses, str):
            parts = [a.strip() for a in addresses.replace(';', ',').split(',') if a.strip()]
            self.addresses = parts
        else:
            self.addresses = list(addresses)

        # Normalize message_filter
        self.message_filter = None
        if message_filter is not None:
            if callable(message_filter):
                self.message_filter = message_filter
            else:
                try:
                    import re
                    pattern = re.compile(str(message_filter))
                    self.message_filter = lambda msg: bool(pattern.search(msg if isinstance(msg, str) else str(msg)))
                except Exception:
                    # Fallback to string containment
                    needle = str(message_filter)
                    self.message_filter = lambda msg: needle in (msg if isinstance(msg, str) else str(msg))

        self._active = False
        self._closed = False

    def __enter__(self):
        self._active = True
        self._closed = False
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self._active = False
        self._closed = True
        return False"
357263,pytroll/posttroll,posttroll/subscriber.py,posttroll.subscriber._AddressListener,"from posttroll.address_receiver import get_configured_address_port

class _AddressListener:
    """"""Listener for new addresses of interest.""""""

    def __init__(self, subscriber, services='', nameserver='localhost'):
        """"""Initialize address listener.""""""
        if isinstance(services, str):
            services = [services]
        self.services = services
        self.subscriber = subscriber
        address_publish_port = get_configured_address_port()
        self.subscriber.add_hook_sub('tcp://' + nameserver + ':' + str(address_publish_port), ['pytroll://address'], self.handle_msg)

    def handle_msg(self, msg):
        """"""Handle the message *msg*.""""""
        addr_ = msg.data['URI']
        status = msg.data.get('status', True)
        if status:
            msg_services = msg.data.get('service')
            for service in self.services:
                if not service or service in msg_services:
                    LOGGER.debug('Adding address %s %s', str(addr_), str(service))
                    self.subscriber.add(addr_)
                    break
        else:
            LOGGER.debug('Removing address %s', str(addr_))
            self.subscriber.remove(addr_)","
class _AddressListener:
    
    def __init__(self, subscriber, services='', nameserver='localhost'):
        
        pass
    def handle_msg(self, msg):
        
        pass",snippet_299,3,"class _AddressListener:
    
    def __init__(self, subscriber, services='', nameserver='localhost'):
        self.subscriber = subscriber
        # Normalize services into a set; empty set means accept all
        if services is None or services == '':
            self.services = set()
        elif isinstance(services, (list, tuple, set)):
            self.services = {str(s).strip().lower() for s in services if str(s).strip()}
        elif isinstance(services, str):
            self.services = {s.strip().lower() for s in services.split(',') if s.strip()}
        else:
            self.services = {str(services).strip().lower()} if str(services).strip() else set()

        # Store nameserver as provided (no external DNS lib used)
        self.nameserver = str(nameserver) if nameserver is not None else 'localhost'

    def handle_msg(self, msg):
        # Accept dict or JSON str/bytes
        data = None
        if isinstance(msg, dict):
            data = msg
        elif isinstance(msg, (str, bytes, bytearray)):
            try:
                import json
                text = msg.decode() if isinstance(msg, (bytes, bytearray)) else msg
                data = json.loads(text)
            except Exception:
                return False
        else:
            return False

        # Extract fields with fallbacks
        service = data.get('service') or data.get('svc') or data.get('type') or ''
        service = str(service).strip()
        service_lc = service.lower()

        if self.services and service_lc not in self.services:
            return False

        address = data.get('address') or data.get('addr') or data.get('host') or data.get('hostname') or ''
        port = data.get('port') or data.get('p') or None
        name = data.get('name') or data.get('id') or data.get('instance') or ''

        # Normalize port to int if possible
        try:
            port = int(port) if port is not None and port != '' else None
        except (ValueError, TypeError):
            port = None

        event = {
            'service': service if service else None,
            'address': str(address) if address is not None else None,
            'port': port,
            'name': str(name) if name is not None else None,
            'raw': data,
            'nameserver': self.nameserver,
        }

        # Dispatch to subscriber
        handled = False
        try:
            if hasattr(self.subscriber, 'on_address') and callable(getattr(self.subscriber, 'on_address')):
                self.subscriber.on_address(event)
                handled = True
            elif callable(self.subscriber):
                self.subscriber(event)
                handled = True
            elif hasattr(self.subscriber, 'put') and callable(getattr(self.subscriber, 'put')):
                self.subscriber.put(event)
                handled = True
        except Exception:
            return False

        return handled"
360979,reanahub/reana-commons,reanahub_reana-commons/reana_commons/validation/compute_backends.py,reana_commons.validation.compute_backends.ComputeBackendValidatorBase,"from typing import Dict, List, Optional
from reana_commons.errors import REANAValidationError

class ComputeBackendValidatorBase:
    """"""REANA workflow compute backend validation base class.""""""

    def __init__(self, workflow_steps: Optional[List[Dict]]=None, supported_backends: Optional[List[str]]=[]):
        """"""Validate compute backends in REANA workflow steps.

        :param workflow_steps: list of dictionaries which represents different steps involved in workflow.
        :param supported_backends: a list of the supported compute backends.
        """"""
        self.workflow_steps = workflow_steps
        self.supported_backends = supported_backends

    def validate(self) -> None:
        """"""Validate compute backends in REANA workflow.""""""
        raise NotImplementedError

    def raise_error(self, compute_backend: str, step_name: str) -> None:
        """"""Raise validation error.""""""
        raise REANAValidationError(f'''Compute backend ""{compute_backend}"" found in step ""{step_name}"" is not supported. List of supported compute backends: ""{', '.join(self.supported_backends)}""''')","
class ComputeBackendValidatorBase:
    
    def __init__(self, workflow_steps: Optional[List[Dict]]=None, supported_backends: Optional[List[str]]=[]):
        
        pass
    def validate(self) -> None:
        
        pass

    def raise_error(self, compute_backend: str, step_name: str) -> None:
        
        pass",snippet_300,4,"from typing import List, Dict, Optional


class ComputeBackendValidatorBase:
    def __init__(self, workflow_steps: Optional[List[Dict]] = None, supported_backends: Optional[List[str]] = []):
        self.workflow_steps: List[Dict] = list(workflow_steps) if workflow_steps is not None else []
        self.supported_backends: List[str] = list(supported_backends) if supported_backends is not None else []

    def validate(self) -> None:
        if not self.workflow_steps:
            return
        for idx, step in enumerate(self.workflow_steps):
            if not isinstance(step, dict):
                raise TypeError(f""Workflow step at index {idx} must be a dict, got {type(step).__name__}"")
            step_name = str(step.get(""name"", f""step_{idx}""))
            compute_backend = step.get(""compute_backend"")
            if compute_backend is None:
                self.raise_error(""<missing>"", step_name)
            if self.supported_backends and compute_backend not in self.supported_backends:
                self.raise_error(str(compute_backend), step_name)

    def raise_error(self, compute_backend: str, step_name: str) -> None:
        supported = "", "".join(self.supported_backends) if self.supported_backends else ""none""
        raise ValueError(
            f""Unsupported compute backend '{compute_backend}' for step '{step_name}'. ""
            f""Supported backends: {supported}.""
        )"
361134,raymondEhlers/pachyderm,raymondEhlers_pachyderm/src/pachyderm/fit/base.py,pachyderm.fit.base.BaseFitResult,"import numpy as np
import numpy.typing as npt
from dataclasses import dataclass
from typing import TYPE_CHECKING, Any, TypeVar, cast

@dataclass
class BaseFitResult:
    """"""Base fit result.

    This represents the most basic fit result.

    Attributes:
        parameters: Names of the parameters used in the fit.
        free_parameters: Names of the free parameters used in the fit.
        fixed_parameters: Names of the fixed parameters used in the fit.
        values_at_minimum: Contains the values of the full RP fit function at the minimum. Keys are the
            names of parameters, while values are the numerical values at convergence.
        errors_on_parameters: Contains the values of the errors associated with the parameters
            determined via the fit.
        covariance_matrix: Contains the values of the covariance matrix. Keys are tuples
            with (param_name_a, param_name_b), and the values are covariance between the specified parameters.
            Note that fixed parameters are _not_ included in this matrix.
        errors: Store the errors associated with the component fit function.
    """"""
    parameters: list[str]
    free_parameters: list[str]
    fixed_parameters: list[str]
    values_at_minimum: dict[str, float]
    errors_on_parameters: dict[str, float]
    covariance_matrix: dict[tuple[str, str], float]
    errors: npt.NDArray[Any]

    @property
    def correlation_matrix(self) -> dict[tuple[str, str], float]:
        """"""The correlation matrix of the free parameters.

        These values are derived from the covariance matrix values stored in the fit.

        Note:
            This property caches the correlation matrix value so we don't have to calculate it every time.

        Args:
            None
        Returns:
            The correlation matrix of the fit result.
        """"""
        try:
            return self._correlation_matrix
        except AttributeError:

            def corr(i_name: str, j_name: str) -> float:
                """"""Calculate the correlation matrix (definition from iminuit) from the covariance matrix.""""""
                value = self.covariance_matrix[i_name, j_name] / (np.sqrt(self.covariance_matrix[i_name, i_name] * self.covariance_matrix[j_name, j_name]) + 1e-100)
                return float(value)
            matrix: dict[tuple[str, str], float] = {}
            for i_name in self.free_parameters:
                for j_name in self.free_parameters:
                    matrix[i_name, j_name] = corr(i_name, j_name)
            self._correlation_matrix = matrix
        return self._correlation_matrix","@dataclass
class BaseFitResult:
    
    @property
    def correlation_matrix(self) -> dict[tuple[str, str], float]:
        
        pass
    def correlation_matrix(self) -> dict[tuple[str, str], float]:
        
        pass",snippet_301,3,"from dataclasses import dataclass, field
from typing import Dict, Tuple, Mapping

@dataclass
class BaseFitResult:
    _correlation_matrix: Dict[Tuple[str, str], float] = field(default_factory=dict)
    
    @property
    def correlation_matrix(self) -> dict[tuple[str, str], float]:
        return self._correlation_matrix

    @correlation_matrix.setter
    def correlation_matrix(self, value: Mapping[Tuple[str, str], float]) -> None:
        if value is None:
            self._correlation_matrix = {}
            return
        if not isinstance(value, Mapping):
            raise TypeError(""correlation_matrix must be a mapping of (str, str) to float"")
        cm: Dict[Tuple[str, str], float] = {}
        for k, v in value.items():
            if not (isinstance(k, tuple) and len(k) == 2 and all(isinstance(x, str) for x in k)):
                raise TypeError(""All keys must be 2-tuples of strings"")
            try:
                fv = float(v)
            except Exception as e:
                raise TypeError(""All values must be convertible to float"") from e
            cm[(k[0], k[1])] = fv
        self._correlation_matrix = cm"
361458,AtteqCom/zsl,AtteqCom_zsl/src/zsl/errors.py,zsl.errors.ErrorHandler,"from abc import ABCMeta, abstractmethod

class ErrorHandler:
    """"""
    Custom error handler providing a response on a particular error.
    """"""
    __metaclass__ = ABCMeta

    @abstractmethod
    def can_handle(self, e):
        """"""
        Indicator if the handler is able to handle the given exception `e`.

        :param e: The exception that shall be determined if can be handled by the handler.
        :return: `True` or `False` depending on whether the handler can/should handle the method.
        """"""
        pass

    @abstractmethod
    def handle(self, e):
        """"""
        Handle the exception.

        :param e: The handled exception.
        :return: The error response for the exception.
        """"""
        pass","
class ErrorHandler:
    
    @abstractmethod
    def can_handle(self, e):
        
        pass
    @abstractmethod
    def handle(self, e):
        
        pass",snippet_302,3,"from typing import Any, Callable, Dict, Iterable, Optional, Tuple, Type, Union


class ErrorHandler:
    def __init__(
        self,
        handlers: Optional[
            Union[
                Dict[Type[BaseException], Callable[[BaseException], Any]],
                Iterable[Tuple[Type[BaseException], Callable[[BaseException], Any]]],
            ]
        ] = None,
        default: Optional[Callable[[BaseException], Any]] = None,
    ) -> None:
        self._handlers: Dict[Type[BaseException], Callable[[BaseException], Any]] = {}
        if handlers:
            if isinstance(handlers, dict):
                for exc_type, func in handlers.items():
                    self.register(exc_type, func)
            else:
                for exc_type, func in handlers:
                    self.register(exc_type, func)
        self._default = default

    def register(self, exc_type: Type[BaseException], handler: Callable[[BaseException], Any]) -> None:
        if not isinstance(exc_type, type) or not issubclass(exc_type, BaseException):
            raise TypeError(""exc_type must be an exception type"")
        if not callable(handler):
            raise TypeError(""handler must be callable"")
        self._handlers[exc_type] = handler

    def unregister(self, exc_type: Type[BaseException]) -> None:
        self._handlers.pop(exc_type, None)

    def can_handle(self, e):
        if not isinstance(e, BaseException):
            return False
        return self._find_best_match(type(e)) is not None or self._default is not None

    def handle(self, e):
        if not isinstance(e, BaseException):
            raise TypeError(""handle expects an exception instance"")
        match = self._find_best_match(type(e))
        if match:
            handler = self._handlers[match]
            return handler(e)
        if self._default is not None:
            return self._default(e)
        raise e

    def _find_best_match(self, exc_cls: Type[BaseException]) -> Optional[Type[BaseException]]:
        best_type: Optional[Type[BaseException]] = None
        best_distance = float(""inf"")
        for registered in self._handlers.keys():
            if issubclass(exc_cls, registered):
                distance = self._mro_distance(exc_cls, registered)
                if distance < best_distance:
                    best_distance = distance
                    best_type = registered
        return best_type

    @staticmethod
    def _mro_distance(child: Type[BaseException], parent: Type[BaseException]) -> int:
        try:
            return child.mro().index(parent)
        except ValueError:
            return float(""inf"")"
361483,AtteqCom/zsl,AtteqCom_zsl/src/zsl/resource/model_resource.py,zsl.resource.model_resource.ReadOnlyResourceMixin,"class ReadOnlyResourceMixin:
    """"""
    The mixin to be used to forbid the update/delete and create operations.
    Remember the Python's MRO and place this mixin at the right place in the inheritance declaration.

    .. automethod:: create
    .. automethod:: update
    .. automethod:: delete
    """"""
    OPERATION_CREATE = 'create'
    OPERATION_UPDATE = 'update'
    OPERATION_DELETE = 'delete'

    @staticmethod
    def create(params, args, data):
        """"""Raises exception.

        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.

        :raises ReadOnlyResourceUpdateOperationException: when accessed
        """"""
        raise ReadOnlyResourceUpdateOperationException(ReadOnlyResourceMixin.OPERATION_CREATE)

    @staticmethod
    def update(params, args, data):
        """"""Raises exception.

        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.

        :raises ReadOnlyResourceUpdateOperationException: when accessed
        """"""
        raise ReadOnlyResourceUpdateOperationException(ReadOnlyResourceMixin.OPERATION_UPDATE)

    @staticmethod
    def delete(params, args, data):
        """"""Raises exception.

        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.

        :raises ReadOnlyResourceUpdateOperationException: when accessed
        """"""
        raise ReadOnlyResourceUpdateOperationException(ReadOnlyResourceMixin.OPERATION_DELETE)","class ReadOnlyResourceMixin:
    
    @staticmethod
    def create(params, args, data):
        
        pass
    @staticmethod
    def update(params, args, data):
        
        pass
    @staticmethod
    def delete(params, args, data):
        
        pass",snippet_303,4,"class ReadOnlyResourceMixin:
    
    @staticmethod
    def create(params, args, data):
        raise NotImplementedError(""Create operation is not supported for read-only resources."")
    
    @staticmethod
    def update(params, args, data):
        raise NotImplementedError(""Update operation is not supported for read-only resources."")
    
    @staticmethod
    def delete(params, args, data):
        raise NotImplementedError(""Delete operation is not supported for read-only resources."")"
361534,AtteqCom/zsl,AtteqCom_zsl/src/zsl/utils/command_dispatcher.py,zsl.utils.command_dispatcher.CommandDispatcher,"import inspect

class CommandDispatcher:
    """"""
    A simple class for command dictionary. A command is a function
    which can take named parameters.
    """"""

    def __init__(self):
        """"""
        Create command dictionary
        """"""
        self.commands = {}

    def command(self, fn):
        """"""
        Add method or function to dispatcher. Can be use as a nice
        decorator.

        :param fn: function or method
        :type fn: function
        :return: the same function
        :rtype: function
        """"""
        self.commands[fn.__name__] = fn
        return fn
    'alias for ``CommandDispatcher.command``'
    add_function = command

    def execute_command(self, command, args=None):
        """"""
        Execute a command

        :param command: name of the command
        :type command: str
        :param args: optional named arguments for command
        :type args: dict
        :return: the result of command
        :raises KeyError: if command is not found
        """"""
        if args is None:
            args = {}
        command_fn = self.commands[command]
        return command_fn(**args)

    def bound(self, instance):
        """"""
        Return a new dispatcher, which will switch all command functions
        with bounded methods of given instance matched by name. It will
        match only regular methods.

        :param instance: object instance
        :type instance: object
        :return: new Dispatcher
        :rtype: CommandDispatcher
        """"""
        bounded_dispatcher = CommandDispatcher()
        bounded_dispatcher.commands = self.commands.copy()
        for name in self.commands:
            method = getattr(instance, name, None)
            if method and inspect.ismethod(method) and (method.__self__ == instance):
                bounded_dispatcher.commands[name] = method
        return bounded_dispatcher","
class CommandDispatcher:
    
    def __init__(self):
        
        pass
    def command(self, fn):
        
        pass

    def execute_command(self, command, args=None):
        
        pass

    def bound(self, instance):
        
        pass",snippet_304,5,"class CommandDispatcher:
    
    def __init__(self):
        self._commands = {}

    def command(self, fn):
        self._commands[fn.__name__] = fn
        return fn

    def execute_command(self, command, args=None):
        if isinstance(command, str):
            if command not in self._commands:
                raise KeyError(f""Unknown command: {command}"")
            fn = self._commands[command]
        elif callable(command):
            fn = command
        else:
            raise TypeError(""command must be a command name (str) or a callable"")

        if args is None:
            return fn()
        if isinstance(args, dict):
            return fn(**args)
        if isinstance(args, (list, tuple)):
            return fn(*args)
        # Fallback: single positional argument
        return fn(args)

    def bound(self, instance):
        dispatcher = self

        class _BoundDispatcher:
            def __init__(self, disp, inst):
                self._disp = disp
                self._inst = inst

            def execute_command(self, command, args=None):
                if isinstance(command, str):
                    if command not in self._disp._commands:
                        raise KeyError(f""Unknown command: {command}"")
                    fn = self._disp._commands[command]
                elif callable(command):
                    fn = command
                else:
                    raise TypeError(""command must be a command name (str) or a callable"")

                # Bind function to instance if possible (descriptor protocol)
                if hasattr(fn, ""__get__""):
                    bound_fn = fn.__get__(self._inst, type(self._inst))
                else:
                    bound_fn = fn

                if args is None:
                    return bound_fn()
                if isinstance(args, dict):
                    return bound_fn(**args)
                if isinstance(args, (list, tuple)):
                    return bound_fn(*args)
                return bound_fn(args)

            def command(self, fn):
                return self._disp.command(fn)

        return _BoundDispatcher(dispatcher, instance)"
362561,reanahub/reana-db,reanahub_reana-db/reana_db/models.py,reana_db.models.QuotaBase,"class QuotaBase:
    """"""Quota base functionality.""""""

    def _get_quota_by_type(self, resource_type):
        """"""Aggregate quota usage by resource type.""""""

        def _get_health_status(usage, limit):
            """"""Calculate quota health status.""""""
            health = QuotaHealth.healthy
            if limit:
                percentage = usage / limit * 100
                if percentage >= 80:
                    if percentage >= 100:
                        health = QuotaHealth.critical
                    else:
                        health = QuotaHealth.warning
            return health.name
        quota_usage = 0
        quota_limit = 0
        unit = None
        for resource in self.resources:
            if resource.resource.type_ == resource_type:
                if unit and unit != resource.resource.unit:
                    raise Exception('Error while calculating quota usage. Not all resources of resource type {} use the same units.'.format(resource_type))
                unit = resource.resource.unit
                quota_usage += resource.quota_used
                if hasattr(resource, 'quota_limit'):
                    quota_limit += resource.quota_limit
        usage_dict = {'usage': {'raw': quota_usage, 'human_readable': ResourceUnit.human_readable_unit(unit, quota_usage)}}
        if quota_limit:
            usage_dict['limit'] = {'raw': quota_limit, 'human_readable': ResourceUnit.human_readable_unit(unit, quota_limit)}
            usage_dict['health'] = _get_health_status(quota_usage, quota_limit)
        return usage_dict

    def get_quota_usage(self):
        """"""Get quota usage information.""""""
        used_resource_types = set((res.resource.type_ for res in self.resources))
        return {resource_type.name: self._get_quota_by_type(resource_type) for resource_type in used_resource_types}","class QuotaBase:
    
    def _get_quota_by_type(self, resource_type):
        
        pass
        def _get_health_status(usage, limit):
                
                pass
    def get_quota_usage(self):
        
        pass",snippet_305,4,"class QuotaBase:
    HEALTH_WARNING_THRESHOLD = 0.7
    HEALTH_CRITICAL_THRESHOLD = 0.9

    def _get_quota_by_type(self, resource_type):
        quotas = self.get_quota_usage()
        return quotas.get(resource_type)

    def _get_health_status(self, usage, limit):
        try:
            usage = 0 if usage is None else float(usage)
        except (TypeError, ValueError):
            return ""unknown""

        if limit is None:
            return ""unknown""

        try:
            limit = float(limit)
        except (TypeError, ValueError):
            return ""unknown""

        if limit < 0 or usage < 0:
            return ""unknown""

        if limit == 0:
            return ""over"" if usage > 0 else ""blocked""

        if usage > limit:
            return ""over""

        ratio = usage / limit
        if ratio >= self.HEALTH_CRITICAL_THRESHOLD:
            return ""critical""
        if ratio >= self.HEALTH_WARNING_THRESHOLD:
            return ""warning""
        return ""healthy""

    def get_quota_usage(self):
        raw = getattr(self, ""quotas"", None)
        if callable(raw):
            raw = raw()
        if raw is None:
            return {}

        result = {}
        if isinstance(raw, dict):
            for rtype, val in raw.items():
                used = None
                limit = None
                if isinstance(val, dict):
                    used = val.get(""used"", val.get(""usage"", 0))
                    limit = val.get(""limit"")
                elif isinstance(val, (list, tuple)) and len(val) >= 2:
                    used, limit = val[0], val[1]
                elif isinstance(val, (int, float)):
                    limit = val
                    used = 0
                else:
                    continue

                try:
                    used_f = float(0 if used is None else used)
                except (TypeError, ValueError):
                    used_f = None
                try:
                    limit_f = None if limit is None else float(limit)
                except (TypeError, ValueError):
                    limit_f = None

                health = self._get_health_status(used_f, limit_f)

                if limit_f in (None,):
                    remaining = None
                    percent_used = None
                else:
                    remaining = None if used_f is None else max(limit_f - used_f, 0)
                    percent_used = None if used_f is None or limit_f == 0 else min(used_f / limit_f, 1.0)

                result[rtype] = {
                    ""used"": used_f if used_f is not None else 0.0,
                    ""limit"": limit_f,
                    ""remaining"": remaining,
                    ""percent_used"": percent_used,
                    ""health"": health,
                }
        return result"
