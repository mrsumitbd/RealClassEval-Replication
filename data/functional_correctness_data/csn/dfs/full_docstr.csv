id,repository_name,file_path,class_name,human_written_code,class_skeleton,total_program_units,total_doc_str,AvgCountLine,AvgCountLineBlank,AvgCountLineCode,AvgCountLineComment,AvgCyclomatic,CommentToCodeRatio,CountClassBase,CountClassCoupled,CountClassCoupledModified,CountClassDerived,CountDeclInstanceMethod,CountDeclInstanceVariable,CountDeclMethod,CountDeclMethodAll,CountLine,CountLineBlank,CountLineCode,CountLineCodeDecl,CountLineCodeExe,CountLineComment,CountStmt,CountStmtDecl,CountStmtExe,MaxCyclomatic,MaxInheritanceTree,MaxNesting,SumCyclomatic,snippet_id
17245,jbarlow83/OCRmyPDF,src/ocrmypdf/pdfinfo/layout.py,ocrmypdf.pdfinfo.layout.PdfMinerState,"import pdfminer.psparser
import pdfminer.pdfinterp
from pathlib import Path
import pdfminer
from ocrmypdf.exceptions import EncryptedPdfError, InputFileError
import pdfminer.encodingdb
from pdfminer.layout import LAParams, LTChar, LTPage, LTTextBox
import pdfminer.pdfdevice
from pdfminer.pdfpage import PDFPage

class PdfMinerState:
    """"""Provide a context manager for using pdfminer.six.

    This ensures that the file is closed. It also provides a cache of pages
    from the PDF so that they can be reused if needed, to improve performance.
    """"""

    def __init__(self, infile: Path, pscript5_mode: bool) -> None:
        """"""Initialize the context manager.

        Args:
            infile: The path to the PDF file to be analyzed.
            pscript5_mode: Whether the PDF was generated by PScript5.dll.
        """"""
        self.infile = infile
        self.rman = pdfminer.pdfinterp.PDFResourceManager(caching=True)
        self.disable_boxes_flow = None
        self.page_iter = None
        self.page_cache: list[PDFPage] = []
        self.pscript5_mode = pscript5_mode
        self.file = None

    def __enter__(self):
        """"""Enter the context manager.""""""
        self.file = Path(self.infile).open('rb')
        self.page_iter = PDFPage.get_pages(self.file)
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        """"""Exit the context manager.""""""
        if self.file:
            self.file.close()
        return True

    def get_page_analysis(self, pageno: int):
        """"""Get the page analysis for a given page.""""""
        while len(self.page_cache) <= pageno:
            try:
                self.page_cache.append(next(self.page_iter))
            except StopIteration:
                raise InputFileError(f'pdfminer did not find page {pageno} in the input file.')
        page = self.page_cache[pageno]
        if not page:
            raise InputFileError(f'pdfminer could not process page {pageno} (counting from 0).')
        dev = TextPositionTracker(self.rman, laparams=LAParams(all_texts=True, detect_vertical=True, boxes_flow=self.disable_boxes_flow))
        interp = pdfminer.pdfinterp.PDFPageInterpreter(self.rman, dev)
        with patch_pdfminer(self.pscript5_mode):
            interp.process_page(page)
        return dev.get_result()","
class PdfMinerState:
    '''Provide a context manager for using pdfminer.six.
    This ensures that the file is closed. It also provides a cache of pages
    from the PDF so that they can be reused if needed, to improve performance.
    '''

    def __init__(self, infile: Path, pscript5_mode: bool) -> None:
        '''Initialize the context manager.
        Args:
            infile: The path to the PDF file to be analyzed.
            pscript5_mode: Whether the PDF was generated by PScript5.dll.
        '''
        pass

    def __enter__(self):
        '''Enter the context manager.'''
        pass

    def __exit__(self, exc_type, exc_value, traceback):
        '''Exit the context manager.'''
        pass

    def get_page_analysis(self, pageno: int):
        '''Get the page analysis for a given page.'''
        pass",5,5,11.0,1.0,8.0,2.0,2.0,0.3,0.0,8.0,3.0,0.0,5.0,6.0,5.0,5.0,65.0,9.0,43.0,17.0,37.0,13.0,34.0,16.0,28.0,5.0,0.0,3.0,11.0,snippet_1
17249,jbarlow83/OCRmyPDF,src/ocrmypdf/quality.py,ocrmypdf.quality.OcrQualityDictionary,"import re
from collections.abc import Iterable

class OcrQualityDictionary:
    """"""Manages a dictionary for simple OCR quality checks.""""""

    def __init__(self, *, wordlist: Iterable[str]):
        """"""Construct a dictionary from a list of words.

        Words for which capitalization is important should be capitalized in the
        dictionary. Words that contain spaces or other punctuation will never match.
        """"""
        self.dictionary = set(wordlist)

    def measure_words_matched(self, ocr_text: str) -> float:
        """"""Check how many unique words in the OCR text match a dictionary.

        Words with mixed capitalized are only considered a match if the test word
        matches that capitalization.

        Returns:
            number of words that match / number
        """"""
        text = re.sub('[0-9_]+', ' ', ocr_text)
        text = re.sub('\\W+', ' ', text)
        text_words_list = re.split('\\s+', text)
        text_words = {w for w in text_words_list if len(w) >= 3}
        matches = 0
        for w in text_words:
            if w in self.dictionary or (w != w.lower() and w.lower() in self.dictionary):
                matches += 1
        if matches > 0:
            hit_ratio = matches / len(text_words)
        else:
            hit_ratio = 0.0
        return hit_ratio","
class OcrQualityDictionary:
    '''Manages a dictionary for simple OCR quality checks.'''

    def __init__(self, *, wordlist: Iterable[str]):
        '''Construct a dictionary from a list of words.
        Words for which capitalization is important should be capitalized in the
        dictionary. Words that contain spaces or other punctuation will never match.
        '''
        pass

    def measure_words_matched(self, ocr_text: str) -> float:
        '''Check how many unique words in the OCR text match a dictionary.
        Words with mixed capitalized are only considered a match if the test word
        matches that capitalization.
        Returns:
            number of words that match / number
        '''
        pass",3,3,16.0,2.0,9.0,5.0,3.0,0.58,0.0,4.0,0.0,0.0,2.0,1.0,2.0,2.0,36.0,6.0,19.0,9.0,16.0,11.0,16.0,9.0,13.0,4.0,0.0,2.0,5.0,snippet_2
19627,slundberg/shap,slundberg_shap/shap/_serializable.py,shap._serializable.Serializable,"import inspect
import pickle

class Serializable:
    """"""This is the superclass of all serializable objects.""""""

    def save(self, out_file):
        """"""Save the model to the given file stream.""""""
        pickle.dump(type(self), out_file)

    @classmethod
    def load(cls, in_file, instantiate=True):
        """"""This is meant to be overridden by subclasses and called with super.

        We return constructor argument values when not being instantiated. Since there are no
        constructor arguments for the Serializable class we just return an empty dictionary.
        """"""
        if instantiate:
            return cls._instantiated_load(in_file)
        return {}

    @classmethod
    def _instantiated_load(cls, in_file, **kwargs):
        """"""This is meant to be overridden by subclasses and called with super.

        We return constructor argument values (we have no values to load in this abstract class).
        """"""
        obj_type = pickle.load(in_file)
        if obj_type is None:
            return None
        if not inspect.isclass(obj_type) or (not issubclass(obj_type, cls) and obj_type is not cls):
            raise Exception(f'Invalid object type loaded from file. {obj_type} is not a subclass of {cls}.')
        constructor_args = obj_type.load(in_file, instantiate=False, **kwargs)
        used_args = inspect.getfullargspec(obj_type.__init__)[0]
        return obj_type(**{k: constructor_args[k] for k in constructor_args if k in used_args})","
class Serializable:
    '''This is the superclass of all serializable objects.'''

    def save(self, out_file):
        '''Save the model to the given file stream.'''
        pass
    @classmethod
    def load(cls, in_file, instantiate=True):
        '''This is meant to be overridden by subclasses and called with super.
        We return constructor argument values when not being instantiated. Since there are no
        constructor arguments for the Serializable class we just return an empty dictionary.
        '''
        pass
    @classmethod
    def _instantiated_load(cls, in_file, **kwargs):
        '''This is meant to be overridden by subclasses and called with super.
        We return constructor argument values (we have no values to load in this abstract class).
        '''
        pass",4,4,9.0,1.0,5.0,3.0,2.0,0.56,0.0,2.0,0.0,3.0,1.0,0.0,3.0,3.0,35.0,7.0,18.0,9.0,12.0,10.0,16.0,7.0,12.0,3.0,0.0,1.0,6.0,snippet_3
19679,slundberg/shap,slundberg_shap/shap/maskers/_text.py,shap.maskers._text.SimpleTokenizer,"import re

class SimpleTokenizer:
    """"""A basic model agnostic tokenizer.""""""

    def __init__(self, split_pattern='\\W+'):
        """"""Create a tokenizer based on a simple splitting pattern.""""""
        self.split_pattern = re.compile(split_pattern)

    def __call__(self, s, return_offsets_mapping=True):
        """"""Tokenize the passed string, optionally returning the offsets of each token in the original string.""""""
        pos = 0
        offset_ranges = []
        input_ids = []
        for m in re.finditer(self.split_pattern, s):
            start, end = m.span(0)
            offset_ranges.append((pos, start))
            input_ids.append(s[pos:start])
            pos = end
        if pos != len(s):
            offset_ranges.append((pos, len(s)))
            input_ids.append(s[pos:])
        out = {}
        out['input_ids'] = input_ids
        if return_offsets_mapping:
            out['offset_mapping'] = offset_ranges
        return out",,3,3,11.0,1.0,10.0,1.0,3.0,0.15,0.0,0.0,0.0,0.0,2.0,1.0,2.0,2.0,26.0,3.0,20.0,10.0,17.0,3.0,20.0,10.0,17.0,4.0,0.0,1.0,5.0,snippet_4
19720,audreyr/cookiecutter,audreyr_cookiecutter/cookiecutter/environment.py,cookiecutter.environment.ExtensionLoaderMixin,"from cookiecutter.exceptions import UnknownExtension
from typing import Any

class ExtensionLoaderMixin:
    """"""Mixin providing sane loading of extensions specified in a given context.

    The context is being extracted from the keyword arguments before calling
    the next parent class in line of the child.
    """"""

    def __init__(self, *, context: dict[str, Any] | None=None, **kwargs: Any) -> None:
        """"""Initialize the Jinja2 Environment object while loading extensions.

        Does the following:

        1. Establishes default_extensions (currently just a Time feature)
        2. Reads extensions set in the cookiecutter.json _extensions key.
        3. Attempts to load the extensions. Provides useful error if fails.
        """"""
        context = context or {}
        default_extensions = ['cookiecutter.extensions.JsonifyExtension', 'cookiecutter.extensions.RandomStringExtension', 'cookiecutter.extensions.SlugifyExtension', 'cookiecutter.extensions.TimeExtension', 'cookiecutter.extensions.UUIDExtension']
        extensions = default_extensions + self._read_extensions(context)
        try:
            super().__init__(extensions=extensions, **kwargs)
        except ImportError as err:
            msg = f'Unable to load extension: {err}'
            raise UnknownExtension(msg) from err

    def _read_extensions(self, context: dict[str, Any]) -> list[str]:
        """"""Return list of extensions as str to be passed on to the Jinja2 env.

        If context does not contain the relevant info, return an empty
        list instead.
        """"""
        try:
            extensions = context['cookiecutter']['_extensions']
        except KeyError:
            return []
        else:
            return [str(ext) for ext in extensions]","
class ExtensionLoaderMixin:
    '''Mixin providing sane loading of extensions specified in a given context.
    The context is being extracted from the keyword arguments before calling
    the next parent class in line of the child.
        '''

    def __init__(self, *, context: dict[str, Any] | None=None, **kwargs: Any) -> None:
        '''Initialize the Jinja2 Environment object while loading extensions.
        Does the following:
        1. Establishes default_extensions (currently just a Time feature)
        2. Reads extensions set in the cookiecutter.json _extensions key.
        3. Attempts to load the extensions. Provides useful error if fails.
        '''
        pass

    def _read_extensions(self, context: dict[str, Any]) -> list[str]:
        '''Return list of extensions as str to be passed on to the Jinja2 env.
        If context does not contain the relevant info, return an empty
        list instead.
        '''
        pass",3,3,19.0,3.0,11.0,6.0,2.0,0.65,0.0,8.0,1.0,1.0,2.0,0.0,2.0,2.0,45.0,8.0,23.0,8.0,20.0,15.0,17.0,7.0,14.0,2.0,0.0,1.0,4.0,snippet_5
21340,Delgan/loguru,loguru/_recattrs.py,loguru._recattrs.RecordFile,"class RecordFile:
    """"""A class representing a file record with name and path.

    Attributes
    ----------
    name : str
        The name of the file
    path : str
        The path to the file
    """"""
    __slots__ = ('name', 'path')

    def __init__(self, name, path):
        """"""Initialize a RecordFile instance.

        Parameters
        ----------
        name : str
            The name of the file
        path : str
            The path to the file
        """"""
        self.name = name
        self.path = path

    def __repr__(self):
        """"""Return string representation of RecordFile.

        Returns
        -------
        str
            Formatted string with name and path
        """"""
        return '(name=%r, path=%r)' % (self.name, self.path)

    def __format__(self, spec):
        """"""Format the RecordFile instance.

        Parameters
        ----------
        spec : str
            Format specification

        Returns
        -------
        str
            Formatted name according to specification
        """"""
        return self.name.__format__(spec)","class RecordFile:
    '''A class representing a file record with name and path.
    Attributes
    ----------
    name : str
        The name of the file
    path : str
        The path to the file
    '''

    def __init__(self, name, path):
        '''Initialize a RecordFile instance.
        Parameters
        ----------
        name : str
            The name of the file
        path : str
            The path to the file
        '''
        pass

    def __repr__(self):
        '''Return string representation of RecordFile.
        Returns
        -------
        str
            Formatted string with name and path
        '''
        pass

    def __format__(self, spec):
        '''Format the RecordFile instance.
        Parameters
        ----------
        spec : str
            Format specification
        Returns
        -------
        str
            Formatted name according to specification
        '''
        pass",4,4,2.0,0.0,2.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,3.0,2.0,3.0,3.0,12.0,3.0,9.0,7.0,5.0,0.0,9.0,7.0,5.0,1.0,0.0,0.0,3.0,snippet_6
21341,Delgan/loguru,loguru/_recattrs.py,loguru._recattrs.RecordLevel,"class RecordLevel:
    """"""A class representing the logging level record with name, number and icon.

    Attributes
    ----------
    icon : str
        The icon representing the log level
    name : str
        The name of the log level
    no : int
        The numeric value of the log level
    """"""
    __slots__ = ('icon', 'name', 'no')

    def __init__(self, name, no, icon):
        """"""Initialize a RecordLevel instance.

        Parameters
        ----------
        name : str
            The name of the log level
        no : int
            The numeric value of the log level
        icon : str
            The icon representing the log level
        """"""
        self.name = name
        self.no = no
        self.icon = icon

    def __repr__(self):
        """"""Return string representation of RecordLevel.

        Returns
        -------
        str
            Formatted string with name, number and icon
        """"""
        return '(name=%r, no=%r, icon=%r)' % (self.name, self.no, self.icon)

    def __format__(self, spec):
        """"""Format the RecordLevel instance.

        Parameters
        ----------
        spec : str
            Format specification

        Returns
        -------
        str
            Formatted name according to specification
        """"""
        return self.name.__format__(spec)","class RecordLevel:
    '''A class representing the logging level record with name, number and icon.
    Attributes
    ----------
    icon : str
        The icon representing the log level
    name : str
        The name of the log level
    no : int
        The numeric value of the log level
    '''

    def __init__(self, name, no, icon):
        '''Initialize a RecordLevel instance.
        Parameters
        ----------
        name : str
            The name of the log level
        no : int
            The numeric value of the log level
        icon : str
            The icon representing the log level
        '''
        pass

    def __repr__(self):
        '''Return string representation of RecordLevel.
        Returns
        -------
        str
            Formatted string with name, number and icon
        '''
        pass

    def __format__(self, spec):
        '''Format the RecordLevel instance.
        Parameters
        ----------
        spec : str
            Format specification
        Returns
        -------
        str
            Formatted name according to specification
        '''
        pass",4,4,3.0,0.0,3.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,3.0,3.0,3.0,3.0,13.0,3.0,10.0,8.0,6.0,0.0,10.0,8.0,6.0,1.0,0.0,0.0,3.0,snippet_7
21342,Delgan/loguru,loguru/_recattrs.py,loguru._recattrs.RecordProcess,"class RecordProcess:
    """"""A class representing a process record with ID and name.

    Attributes
    ----------
    id : int
        The process ID
    name : str
        The process name
    """"""
    __slots__ = ('id', 'name')

    def __init__(self, id_, name):
        """"""Initialize a RecordProcess instance.

        Parameters
        ----------
        id_ : int
            The process ID
        name : str
            The process name
        """"""
        self.id = id_
        self.name = name

    def __repr__(self):
        """"""Return string representation of RecordProcess.

        Returns
        -------
        str
            Formatted string with id and name
        """"""
        return '(id=%r, name=%r)' % (self.id, self.name)

    def __format__(self, spec):
        """"""Format the RecordProcess instance.

        Parameters
        ----------
        spec : str
            Format specification

        Returns
        -------
        str
            Formatted ID according to specification
        """"""
        return self.id.__format__(spec)","class RecordProcess:
    '''A class representing a process record with ID and name.
    Attributes
    ----------
    id : int
        The process ID
    name : str
        The process name
    '''

    def __init__(self, id_, name):
        '''Initialize a RecordProcess instance.
        Parameters
        ----------
        id_ : int
            The process ID
        name : str
            The process name
        '''
        pass

    def __repr__(self):
        '''Return string representation of RecordProcess.
        Returns
        -------
        str
            Formatted string with id and name
        '''
        pass

    def __format__(self, spec):
        '''Format the RecordProcess instance.
        Parameters
        ----------
        spec : str
            Format specification
        Returns
        -------
        str
            Formatted ID according to specification
        '''
        pass",4,4,2.0,0.0,2.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,3.0,2.0,3.0,3.0,12.0,3.0,9.0,7.0,5.0,0.0,9.0,7.0,5.0,1.0,0.0,0.0,3.0,snippet_8
21343,Delgan/loguru,loguru/_recattrs.py,loguru._recattrs.RecordThread,"class RecordThread:
    """"""A class representing a thread record with ID and name.

    Attributes
    ----------
    id : int
        The thread ID
    name : str
        The thread name
    """"""
    __slots__ = ('id', 'name')

    def __init__(self, id_, name):
        """"""Initialize a RecordThread instance.

        Parameters
        ----------
        id_ : int
            The thread ID
        name : str
            The thread name
        """"""
        self.id = id_
        self.name = name

    def __repr__(self):
        """"""Return string representation of RecordThread.

        Returns
        -------
        str
            Formatted string with id and name
        """"""
        return '(id=%r, name=%r)' % (self.id, self.name)

    def __format__(self, spec):
        """"""Format the RecordThread instance.

        Parameters
        ----------
        spec : str
            Format specification

        Returns
        -------
        str
            Formatted ID according to specification
        """"""
        return self.id.__format__(spec)","class RecordThread:
    '''A class representing a thread record with ID and name.
    Attributes
    ----------
    id : int
        The thread ID
    name : str
        The thread name
    '''

    def __init__(self, id_, name):
        '''Initialize a RecordThread instance.
        Parameters
        ----------
        id_ : int
            The thread ID
        name : str
            The thread name
        '''
        pass

    def __repr__(self):
        '''Return string representation of RecordThread.
        Returns
        -------
        str
            Formatted string with id and name
        '''
        pass

    def __format__(self, spec):
        '''Format the RecordThread instance.
        Parameters
        ----------
        spec : str
            Format specification
        Returns
        -------
        str
            Formatted ID according to specification
        '''
        pass",4,4,2.0,0.0,2.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,3.0,2.0,3.0,3.0,12.0,3.0,9.0,7.0,5.0,0.0,9.0,7.0,5.0,1.0,0.0,0.0,3.0,snippet_9
33971,Miserlou/Zappa,Miserlou_Zappa/zappa/asynchronous.py,zappa.asynchronous.LambdaAsyncResponse,"import json
import uuid

class LambdaAsyncResponse:
    """"""
    Base Response Dispatcher class
    Can be used directly or subclassed if the method to send the message is changed.
    """"""

    def __init__(self, lambda_function_name=None, aws_region=None, capture_response=False, **kwargs):
        """""" """"""
        if kwargs.get('boto_session'):
            self.client = kwargs.get('boto_session').client('lambda')
        else:
            self.client = LAMBDA_CLIENT
        self.lambda_function_name = lambda_function_name
        self.aws_region = aws_region
        if capture_response:
            if ASYNC_RESPONSE_TABLE is None:
                print(""Warning! Attempted to capture a response without async_response_table configured in settings (you won't capture async responses)."")
                capture_response = False
                self.response_id = 'MISCONFIGURED'
            else:
                self.response_id = str(uuid.uuid4())
        else:
            self.response_id = None
        self.capture_response = capture_response

    def send(self, task_path, args, kwargs):
        """"""
        Create the message object and pass it to the actual sender.
        """"""
        message = {'task_path': task_path, 'capture_response': self.capture_response, 'response_id': self.response_id, 'args': args, 'kwargs': kwargs}
        self._send(message)
        return self

    def _send(self, message):
        """"""
        Given a message, directly invoke the lamdba function for this task.
        """"""
        message['command'] = 'zappa.asynchronous.route_lambda_task'
        payload = json.dumps(message).encode('utf-8')
        if len(payload) > LAMBDA_ASYNC_PAYLOAD_LIMIT:
            raise AsyncException('Payload too large for async Lambda call')
        self.response = self.client.invoke(FunctionName=self.lambda_function_name, InvocationType='Event', Payload=payload)
        self.sent = self.response.get('StatusCode', 0) == 202","
class LambdaAsyncResponse:
    '''
    Base Response Dispatcher class
    Can be used directly or subclassed if the method to send the message is changed.
        '''

    def __init__(self, lambda_function_name=None, aws_region=None, capture_response=False, **kwargs):
        ''' '''
        pass

    def send(self, task_path, args, kwargs):
        '''
        Create the message object and pass it to the actual sender.
        '''
        pass

    def _send(self, message):
        '''
        Given a message, directly invoke the lamdba function for this task.
        '''
        pass",4,4,17.0,1.0,14.0,3.0,2.0,0.33,0.0,2.0,1.0,1.0,3.0,7.0,3.0,3.0,60.0,6.0,43.0,13.0,39.0,14.0,26.0,13.0,22.0,4.0,0.0,2.0,7.0,snippet_10
44118,LonamiWebs/Telethon,LonamiWebs_Telethon/telethon/crypto/aesctr.py,telethon.crypto.aesctr.AESModeCTR,"import pyaes

class AESModeCTR:
    """"""Wrapper around pyaes.AESModeOfOperationCTR mode with custom IV""""""

    def __init__(self, key, iv):
        """"""
        Initializes the AES CTR mode with the given key/iv pair.

        :param key: the key to be used as bytes.
        :param iv: the bytes initialization vector. Must have a length of 16.
        """"""
        assert isinstance(key, bytes)
        self._aes = pyaes.AESModeOfOperationCTR(key)
        assert isinstance(iv, bytes)
        assert len(iv) == 16
        self._aes._counter._counter = list(iv)

    def encrypt(self, data):
        """"""
        Encrypts the given plain text through AES CTR.

        :param data: the plain text to be encrypted.
        :return: the encrypted cipher text.
        """"""
        return self._aes.encrypt(data)

    def decrypt(self, data):
        """"""
        Decrypts the given cipher text through AES CTR

        :param data: the cipher text to be decrypted.
        :return: the decrypted plain text.
        """"""
        return self._aes.decrypt(data)","
class AESModeCTR:
    '''Wrapper around pyaes.AESModeOfOperationCTR mode with custom IV'''

    def __init__(self, key, iv):
        '''
        Initializes the AES CTR mode with the given key/iv pair.
        :param key: the key to be used as bytes.
        :param iv: the bytes initialization vector. Must have a length of 16.
        '''
        pass

    def encrypt(self, data):
        '''
        Encrypts the given plain text through AES CTR.
        :param data: the plain text to be encrypted.
        :return: the encrypted cipher text.
        '''
        pass

    def decrypt(self, data):
        '''
        Decrypts the given cipher text through AES CTR
        :param data: the cipher text to be decrypted.
        :return: the decrypted plain text.
        '''
        pass",4,4,10.0,1.0,3.0,5.0,1.0,1.64,0.0,2.0,0.0,0.0,3.0,1.0,3.0,3.0,36.0,7.0,11.0,5.0,7.0,18.0,11.0,5.0,7.0,1.0,0.0,0.0,3.0,snippet_11
44121,LonamiWebs/Telethon,LonamiWebs_Telethon/telethon/crypto/factorization.py,telethon.crypto.factorization.Factorization,"from random import randint

class Factorization:
    """"""
    Simple module to factorize large numbers really quickly.
    """"""

    @classmethod
    def factorize(cls, pq):
        """"""
        Factorizes the given large integer.

        Implementation from https://comeoncodeon.wordpress.com/2010/09/18/pollard-rho-brent-integer-factorization/.

        :param pq: the prime pair pq.
        :return: a tuple containing the two factors p and q.
        """"""
        if pq % 2 == 0:
            return (2, pq // 2)
        y, c, m = (randint(1, pq - 1), randint(1, pq - 1), randint(1, pq - 1))
        g = r = q = 1
        x = ys = 0
        while g == 1:
            x = y
            for i in range(r):
                y = (pow(y, 2, pq) + c) % pq
            k = 0
            while k < r and g == 1:
                ys = y
                for i in range(min(m, r - k)):
                    y = (pow(y, 2, pq) + c) % pq
                    q = q * abs(x - y) % pq
                g = cls.gcd(q, pq)
                k += m
            r *= 2
        if g == pq:
            while True:
                ys = (pow(ys, 2, pq) + c) % pq
                g = cls.gcd(abs(x - ys), pq)
                if g > 1:
                    break
        p, q = (g, pq // g)
        return (p, q) if p < q else (q, p)

    @staticmethod
    def gcd(a, b):
        """"""
        Calculates the Greatest Common Divisor.

        :param a: the first number.
        :param b: the second number.
        :return: GCD(a, b)
        """"""
        while b:
            a, b = (b, a % b)
        return a","
class Factorization:
    '''
    Simple module to factorize large numbers really quickly.
    '''
    @classmethod
    def factorize(cls, pq):
        '''
        Factorizes the given large integer.
        Implementation from https://comeoncodeon.wordpress.com/2010/09/18/pollard-rho-brent-integer-factorization/.
        :param pq: the prime pair pq.
        :return: a tuple containing the two factors p and q.
        '''
        pass
    @staticmethod
    def gcd(a, b):
        '''
        Calculates the Greatest Common Divisor.
        :param a: the first number.
        :param b: the second number.
        :return: GCD(a, b)
        '''
        pass",3,3,27.0,6.0,16.0,6.0,6.0,0.44,0.0,1.0,0.0,0.0,0.0,0.0,2.0,2.0,61.0,12.0,34.0,11.0,29.0,15.0,32.0,9.0,29.0,10.0,0.0,3.0,12.0,snippet_12
49235,blue-yonder/tsfresh,blue-yonder_tsfresh/tsfresh/examples/driftbif_simulation.py,tsfresh.examples.driftbif_simulation.velocity,"import numpy as np

class velocity:
    """"""
    Simulates the velocity of a dissipative soliton (kind of self organized particle) [6]_.
    The equilibrium velocity without noise R=0 for
    $	au>1.0/\\kappa_3$ is $\\kappa_3 \\sqrt{(tau - 1.0/\\kappa_3)/Q}.
    Before the drift-bifurcation $	au \\le 1.0/\\kappa_3$ the velocity is zero.

    References
    ----------

    .. [6] Andreas Kempa-Liehr (2013, p. 159-170)
        Dynamics of Dissipative Soliton
        Dissipative Solitons in Reaction Diffusion Systems.
        Springer: Berlin


    >>> ds = velocity(tau=3.5) # Dissipative soliton with equilibrium velocity 1.5e-3
    >>> print(ds.label) # Discriminating before or beyond Drift-Bifurcation
    1

    # Equilibrium velocity
    >>> print(ds.deterministic)
    0.0015191090506254991

    # Simulated velocity as a time series with 20000 time steps being disturbed by Gaussian white noise
    >>> v = ds.simulate(20000)
    """"""

    def __init__(self, tau=3.8, kappa_3=0.3, Q=1950.0, R=0.0003, delta_t=0.05, seed=None):
        """"""
        :param tau: Bifurcation parameter determining the intrinsic velocity of the dissipative soliton,
                    which is zero for tau<=1.0/kappa_3 and np.sqrt(kappa_3**3/Q * (tau - 1.0/kappa_3)) otherwise
        :type tau: float
        :param kappa_3: Inverse bifurcation point.
        :type kappa_3:
        :param Q: Shape parameter of dissipative soliton
        :type Q: float
        :param R: Noise amplitude
        :type R: float
        :param delta_t: temporal discretization
        :type delta_t: float
        """"""
        self.delta_t = delta_t
        self.kappa_3 = kappa_3
        self.Q = Q
        self.tau = tau
        self.a = self.delta_t * kappa_3 ** 2 * (tau - 1.0 / kappa_3)
        self.b = self.delta_t * Q / kappa_3
        self.label = int(tau > 1.0 / kappa_3)
        self.c = np.sqrt(self.delta_t) * R
        self.delta_t = self.delta_t
        if seed is not None:
            np.random.seed(seed)
        if tau <= 1.0 / kappa_3:
            self.deterministic = 0.0
        else:
            self.deterministic = kappa_3 ** 1.5 * np.sqrt((tau - 1.0 / kappa_3) / Q)

    def __call__(self, v):
        """"""
        returns deterministic dynamic = acceleration (without noise)

        :param v: initial velocity vector
        :rtype v: ndarray
        :return: velocity vector of next time step
        :return type: ndarray
        """"""
        return v * (1.0 + self.a - self.b * np.dot(v, v))

    def simulate(self, N, v0=np.zeros(2)):
        """"""

        :param N: number of time steps
        :type N: int
        :param v0: initial velocity vector
        :type v0: ndarray
        :return: time series of velocity vectors with shape (N, v0.shape[0])
        :rtype: ndarray
        """"""
        v = [v0]
        n = N - 1
        gamma = np.random.randn(n, v0.size)
        for i in range(n):
            next_v = self.__call__(v[i]) + self.c * gamma[i]
            v.append(next_v)
        v_vec = np.array(v)
        return v_vec","
class velocity:
    '''
    Simulates the velocity of a dissipative soliton (kind of self organized particle) [6]_.
    The equilibrium velocity without noise R=0 for
    $    au>1.0/\kappa_3$ is $\kappa_3 \sqrt{(tau - 1.0/\kappa_3)/Q}.
    Before the drift-bifurcation $    au \le 1.0/\kappa_3$ the velocity is zero.
    References
    ----------
    .. [6] Andreas Kempa-Liehr (2013, p. 159-170)
        Dynamics of Dissipative Soliton
        Dissipative Solitons in Reaction Diffusion Systems.
        Springer: Berlin

    >>> ds = velocity(tau=3.5) # Dissipative soliton with equilibrium velocity 1.5e-3
    >>> print(ds.label) # Discriminating before or beyond Drift-Bifurcation
    1
    # Equilibrium velocity
    >>> print(ds.deterministic)
    0.0015191090506254991
    # Simulated velocity as a time series with 20000 time steps being disturbed by Gaussian white noise
    >>> v = ds.simulate(20000)
    '''

    def __init__(self, tau=3.8, kappa_3=0.3, Q=1950.0, R=0.0003, delta_t=0.05, seed=None):
        '''
        :param tau: Bifurcation parameter determining the intrinsic velocity of the dissipative soliton,
                    which is zero for tau<=1.0/kappa_3 and np.sqrt(kappa_3**3/Q * (tau - 1.0/kappa_3)) otherwise
        :type tau: float
        :param kappa_3: Inverse bifurcation point.
        :type kappa_3:
        :param Q: Shape parameter of dissipative soliton
        :type Q: float
        :param R: Noise amplitude
        :type R: float
        :param delta_t: temporal discretization
        :type delta_t: float
        '''
        pass

    def __call__(self, v):
        '''
        returns deterministic dynamic = acceleration (without noise)
        :param v: initial velocity vector
        :rtype v: ndarray
        :return: velocity vector of next time step
        :return type: ndarray
        '''
        pass

    def simulate(self, N, v0=np.zeros(2)):
        '''
        :param N: number of time steps
        :type N: int
        :param v0: initial velocity vector
        :type v0: ndarray
        :return: time series of velocity vectors with shape (N, v0.shape[0])
        :rtype: ndarray
        '''
        pass",4,4,21.0,2.0,9.0,11.0,2.0,1.86,0.0,2.0,0.0,0.0,3.0,9.0,3.0,3.0,94.0,16.0,28.0,19.0,24.0,52.0,27.0,19.0,23.0,3.0,0.0,1.0,6.0,snippet_13
98891,peterbrittain/asciimatics,peterbrittain_asciimatics/asciimatics/effects.py,asciimatics.effects._Flake,"from random import randint, random, choice
from asciimatics.screen import Screen

class _Flake:
    """"""
    Track a single snow flake.
    """"""
    _snow_chars = '.+*'
    _drift_chars = ' ,;#@'

    def __init__(self, screen: Screen):
        """"""
        :param screen: The Screen being used for the Scene.
        """"""
        self._screen = screen
        self._x = 0
        self._y = 0
        self._rate = 0
        self._char = ''
        self._reseed()

    def _reseed(self):
        """"""
        Randomly create a new snowflake once this one is finished.
        """"""
        self._char = choice(self._snow_chars)
        self._rate = randint(1, 3)
        self._x = randint(0, self._screen.width - 1)
        self._y = self._screen.start_line + randint(0, self._rate)

    def update(self, reseed: bool):
        """"""
        Update that snowflake!

        :param reseed: Whether we are in the normal reseed cycle or not.
        """"""
        self._screen.print_at(' ', self._x, self._y)
        cell = None
        for _ in range(self._rate):
            self._y += 1
            cell = self._screen.get_from(self._x, self._y)
            if cell is None or cell[0] != 32:
                break
        if (cell is not None and cell[0] in [ord(x) for x in self._snow_chars + ' ']) and self._y < self._screen.start_line + self._screen.height:
            self._screen.print_at(self._char, self._x, self._y)
        else:
            self._y = min(self._y, self._screen.start_line + self._screen.height)
            drift_index = -1
            if cell:
                drift_index = self._drift_chars.find(chr(cell[0]))
            if 0 <= drift_index < len(self._drift_chars) - 1:
                drift_char = self._drift_chars[drift_index + 1]
                self._screen.print_at(drift_char, self._x, self._y)
            else:
                self._screen.print_at(',', self._x, self._y - 1)
            if reseed:
                self._reseed()","
class _Flake:
    '''
    Track a single snow flake.
    '''

    def __init__(self, screen: Screen):
        '''
        :param screen: The Screen being used for the Scene.
        '''
        pass

    def _reseed(self):
        '''
        Randomly create a new snowflake once this one is finished.
        '''
        pass

    def update(self, reseed: bool):
        '''
        Update that snowflake!
        :param reseed: Whether we are in the normal reseed cycle or not.
        '''
        pass",4,4,17.0,1.0,13.0,3.0,3.0,0.34,0.0,1.0,0.0,0.0,3.0,5.0,3.0,3.0,61.0,7.0,41.0,15.0,37.0,14.0,36.0,15.0,32.0,8.0,0.0,2.0,10.0,snippet_14
98892,peterbrittain/asciimatics,peterbrittain_asciimatics/asciimatics/effects.py,asciimatics.effects._Star,"from random import randint, random, choice
from asciimatics.screen import Screen

class _Star:
    """"""
    Simple class to represent a single star for the Stars special effect.
    """"""

    def __init__(self, screen: Screen, pattern: str):
        """"""
        :param screen: The Screen being used for the Scene.
        :param pattern: The pattern to loop through
        """"""
        self._screen = screen
        self._star_chars = pattern
        self._cycle = 0
        self._old_char = ''
        self._respawn()

    def _respawn(self):
        """"""
        Pick a random location for the star making sure it does
        not overwrite an existing piece of text.
        """"""
        self._cycle = randint(0, len(self._star_chars))
        height, width = self._screen.dimensions
        while True:
            self._x = randint(0, width - 1)
            self._y = self._screen.start_line + randint(0, height - 1)
            c = self._screen.get_from(self._x, self._y)
            if c is not None and c[0] == 32:
                break
        self._old_char = ' '

    def update(self):
        """"""
        Draw the star.
        """"""
        if not self._screen.is_visible(self._x, self._y):
            self._respawn()
        c = self._screen.get_from(self._x, self._y)
        if c is not None and c[0] not in (ord(self._old_char), 32):
            self._respawn()
        self._cycle += 1
        if self._cycle >= len(self._star_chars):
            self._cycle = 0
        new_char = self._star_chars[self._cycle]
        if new_char == self._old_char:
            return
        self._screen.print_at(new_char, self._x, self._y)
        self._old_char = new_char","
class _Star:
    '''
    Simple class to represent a single star for the Stars special effect.
        '''

    def __init__(self, screen: Screen, pattern: str):
        '''
        :param screen: The Screen being used for the Scene.
        :param pattern: The pattern to loop through
        '''
        pass

    def _respawn(self):
        '''
        Pick a random location for the star making sure it does
        not overwrite an existing piece of text.
        '''
        pass

    def update(self):
        '''
        Draw the star.
        '''
        pass",4,4,15.0,1.0,10.0,4.0,3.0,0.47,0.0,0.0,0.0,0.0,3.0,6.0,3.0,3.0,51.0,7.0,30.0,13.0,26.0,14.0,30.0,13.0,26.0,5.0,0.0,2.0,9.0,snippet_15
98893,peterbrittain/asciimatics,peterbrittain_asciimatics/asciimatics/effects.py,asciimatics.effects._Trail,"from random import randint, random, choice
from asciimatics.screen import Screen

class _Trail:
    """"""
    Track a single trail  for a falling character effect (a la Matrix).
    """"""

    def __init__(self, screen: Screen, x: int):
        """"""
        :param screen: The Screen being used for the Scene.
        :param x: The column (y coordinate) for this trail to use.
        """"""
        self._screen = screen
        self._x = x
        self._y = 0
        self._life = 0
        self._rate = 0
        self._clear = True
        self._maybe_reseed(True)

    def _maybe_reseed(self, normal: bool):
        """"""
        Randomly create a new column once this one is finished.
        """"""
        self._y += self._rate
        self._life -= 1
        if self._life <= 0:
            self._clear = not self._clear if normal else True
            self._rate = randint(1, 2)
            if self._clear:
                self._y = 0
                self._life = self._screen.height // self._rate
            else:
                self._y = randint(0, self._screen.height // 2) - self._screen.height // 4
                self._life = randint(1, self._screen.height - self._y) // self._rate

    def update(self, reseed: bool):
        """"""
        Update that trail!

        :param reseed: Whether we are in the normal reseed cycle or not.
        """"""
        if self._clear:
            for i in range(0, 3):
                self._screen.print_at(' ', self._x, self._screen.start_line + self._y + i)
            self._maybe_reseed(reseed)
        else:
            for i in range(0, 3):
                self._screen.print_at(chr(randint(32, 126)), self._x, self._screen.start_line + self._y + i, Screen.COLOUR_GREEN)
            for i in range(4, 6):
                self._screen.print_at(chr(randint(32, 126)), self._x, self._screen.start_line + self._y + i, Screen.COLOUR_GREEN, Screen.A_BOLD)
            self._maybe_reseed(reseed)","
class _Trail:
    '''
    Track a single trail  for a falling character effect (a la Matrix).
    '''

    def __init__(self, screen: Screen, x: int):
        '''
        :param screen: The Screen being used for the Scene.
        :param x: The column (y coordinate) for this trail to use.
        '''
        pass

    def _maybe_reseed(self, normal: bool):
        '''
        Randomly create a new column once this one is finished.
        '''
        pass

    def update(self, reseed: bool):
        '''
        Update that trail!
        :param reseed: Whether we are in the normal reseed cycle or not.
        '''
        pass",4,4,18.0,0.0,14.0,4.0,3.0,0.33,0.0,2.0,1.0,0.0,3.0,6.0,3.0,3.0,61.0,4.0,43.0,11.0,39.0,14.0,30.0,11.0,26.0,5.0,0.0,2.0,10.0,snippet_16
99108,ConsenSys/mythril-classic,ConsenSys_mythril-classic/mythril/analysis/ops.py,mythril.analysis.ops.Variable,"class Variable:
    """"""The representation of a variable with value and type.""""""

    def __init__(self, val, _type):
        """"""

        :param val:
        :param _type:
        """"""
        self.val = val
        self.type = _type

    def __str__(self):
        """"""

        :return:
        """"""
        return str(self.val)","class Variable:
    '''The representation of a variable with value and type.'''

    def __init__(self, val, _type):
        '''
        :param val:
        :param _type:
        '''
        pass

    def __str__(self):
        '''
        :return:
        '''
        pass",3,3,7.0,1.0,3.0,4.0,1.0,1.33,0.0,1.0,0.0,0.0,2.0,2.0,2.0,2.0,18.0,4.0,6.0,5.0,3.0,8.0,6.0,5.0,3.0,1.0,0.0,0.0,2.0,snippet_17
99139,ConsenSys/mythril-classic,ConsenSys_mythril-classic/mythril/laser/ethereum/cfg.py,mythril.laser.ethereum.cfg.Node,"from typing import TYPE_CHECKING, Dict, List
from mythril.laser.ethereum.state.constraints import Constraints

class Node:
    """"""The representation of a call graph node.""""""

    def __init__(self, contract_name: str, start_addr=0, constraints=None, function_name='unknown') -> None:
        """"""

        :param contract_name:
        :param start_addr:
        :param constraints:
        """"""
        constraints = constraints if constraints else Constraints()
        self.contract_name = contract_name
        self.start_addr = start_addr
        self.states: List[GlobalState] = []
        self.constraints = constraints
        self.function_name = function_name
        self.flags = NodeFlags()
        self.uid = hash(self)

    def get_cfg_dict(self) -> Dict:
        """"""
        Generate a configuration dictionary for the current state of the contract.

        :return: A dictionary containing the contract's configuration details.
        """"""
        code_lines = [f""{instruction['address']} {instruction['opcode']}"" + (f"" {instruction['argument']}"" if instruction['opcode'].startswith('PUSH') and 'argument' in instruction else '') for state in self.states for instruction in [state.get_current_instruction()]]
        code = '\\n'.join(code_lines)
        return {'contract_name': self.contract_name, 'start_addr': self.start_addr, 'function_name': self.function_name, 'code': code}","
class Node:
    '''The representation of a call graph node.'''

    def __init__(self, contract_name: str, start_addr=0, constraints=None, function_name='unknown') -> None:
        '''
        :param contract_name:
        :param start_addr:
        :param constraints:
        '''
        pass

    def get_cfg_dict(self) -> Dict:
        '''
        Generate a configuration dictionary for the current state of the contract.
        :return: A dictionary containing the contract's configuration details.
        '''
        pass",3,3,24.0,2.0,17.0,5.0,2.0,0.29,0.0,3.0,2.0,0.0,2.0,7.0,2.0,2.0,51.0,6.0,35.0,19.0,26.0,10.0,14.0,12.0,11.0,2.0,0.0,0.0,4.0,snippet_18
99158,ConsenSys/mythril-classic,ConsenSys_mythril-classic/mythril/laser/ethereum/state/annotation.py,mythril.laser.ethereum.state.annotation.StateAnnotation,"class StateAnnotation:
    """"""The StateAnnotation class is used to persist information over traces.

    This allows modules to reason about traces without the need to
    traverse the state space themselves.
    """"""

    @property
    def persist_to_world_state(self) -> bool:
        """"""If this function returns true then laser will also annotate the
        world state.

        If you want annotations to persist through different user initiated message call transactions
        then this should be enabled.

        The default is set to False
        """"""
        return False

    @property
    def persist_over_calls(self) -> bool:
        """"""If this function returns true then laser will propagate the annotation between calls

        The default is set to False
        """"""
        return False

    @property
    def search_importance(self) -> int:
        """"""
        Used in estimating the priority of a state annotated with the corresponding annotation.
        Default is 1
        """"""
        return 1","class StateAnnotation:
    '''The StateAnnotation class is used to persist information over traces.
    This allows modules to reason about traces without the need to
    traverse the state space themselves.
    '''
    @property
    def persist_to_world_state(self) -> bool:
        '''If this function returns true then laser will also annotate the
        world state.
        If you want annotations to persist through different user initiated message call transactions
        then this should be enabled.
        The default is set to False
        '''
        pass
    @property
    def persist_over_calls(self) -> bool:
        '''If this function returns true then laser will propagate the annotation between calls
        The default is set to False
        '''
        pass
    @property
    def search_importance(self) -> int:
        '''
        Used in estimating the priority of a state annotated with the corresponding annotation.
        Default is 1
        '''
        pass",4,4,7.0,1.0,2.0,4.0,1.0,1.9,0.0,2.0,0.0,16.0,3.0,0.0,3.0,3.0,36.0,7.0,10.0,7.0,3.0,19.0,7.0,4.0,3.0,1.0,0.0,0.0,3.0,snippet_19
99242,ConsenSys/mythril-classic,ConsenSys_mythril-classic/mythril/laser/smt/function.py,mythril.laser.smt.function.Function,"from typing import Any, List, Set, cast
from mythril.laser.smt.bitvec import BitVec
import z3

class Function:
    """"""An uninterpreted function.""""""

    def __init__(self, name: str, domain: List[int], value_range: int):
        """"""Initializes an uninterpreted function.

        :param name: Name of the Function
        :param domain: The domain for the Function (10 -> all the values that a bv of size 10 could take)
        :param value_range: The range for the values of the function (10 -> all the values that a bv of size 10 could take)
        """"""
        self.domain = []
        for element in domain:
            self.domain.append(z3.BitVecSort(element))
        self.range = z3.BitVecSort(value_range)
        self.raw = z3.Function(name, *self.domain, self.range)

    def __call__(self, *items) -> BitVec:
        """"""Function accessor, item can be symbolic.""""""
        annotations: Set[Any] = set().union(*[item.annotations for item in items])
        return BitVec(cast(z3.BitVecRef, self.raw(*[item.raw for item in items])), annotations=annotations)","
class Function:
    '''An uninterpreted function.'''

    def __init__(self, name: str, domain: List[int], value_range: int):
        '''Initializes an uninterpreted function.
        :param name: Name of the Function
        :param domain: The domain for the Function (10 -> all the values that a bv of size 10 could take)
        :param value_range: The range for the values of the function (10 -> all the values that a bv of size 10 could take)
        '''
        pass

    def __call__(self, *items) -> BitVec:
        '''Function accessor, item can be symbolic.'''
        pass",3,3,10.0,1.0,6.0,3.0,2.0,0.54,0.0,5.0,1.0,0.0,2.0,3.0,2.0,2.0,23.0,3.0,13.0,8.0,10.0,7.0,10.0,8.0,7.0,2.0,0.0,1.0,3.0,snippet_20
99273,ConsenSys/mythril-classic,ConsenSys_mythril-classic/mythril/support/source_support.py,mythril.support.source_support.Source,"from mythril.ethereum.evmcontract import EVMContract
from mythril.solidity.soliditycontract import SolidityContract

class Source:
    """"""Class to handle to source data""""""

    def __init__(self, source_type=None, source_format=None, source_list=None):
        """"""
        :param source_type: whether it is a solidity-file or evm-bytecode
        :param source_format: whether it is bytecode, ethereum-address or text
        :param source_list: List of files
        :param meta: meta data
        """"""
        self.source_type = source_type
        self.source_format = source_format
        self.source_list = source_list or []
        self._source_hash = []

    def get_source_from_contracts_list(self, contracts):
        """"""
        get the source data from the contracts list
        :param contracts: the list of contracts
        :return:
        """"""
        if contracts is None or len(contracts) == 0:
            return
        if isinstance(contracts[0], SolidityContract):
            self.source_type = 'solidity-file'
            self.source_format = 'text'
            for contract in contracts:
                self.source_list += [file.filename for file in contract.solc_indices.values()]
                self._source_hash.append(contract.bytecode_hash)
                self._source_hash.append(contract.creation_bytecode_hash)
        elif isinstance(contracts[0], EVMContract):
            self.source_format = 'evm-byzantium-bytecode'
            self.source_type = 'ethereum-address' if len(contracts[0].name) == 42 and contracts[0].name.startswith('0x') else 'raw-bytecode'
            for contract in contracts:
                if contract.creation_code:
                    self.source_list.append(contract.creation_bytecode_hash)
                if contract.code:
                    self.source_list.append(contract.bytecode_hash)
            self._source_hash = self.source_list
        else:
            assert False

    def get_source_index(self, bytecode_hash: str) -> int:
        """"""
        Find the contract index in the list
        :param bytecode_hash: The contract hash
        :return: The index of the contract in the _source_hash list
        """"""
        try:
            return self._source_hash.index(bytecode_hash)
        except ValueError:
            self._source_hash.append(bytecode_hash)
            return len(self._source_hash) - 1","
class Source:
    '''Class to handle to source data'''

    def __init__(self, source_type=None, source_format=None, source_list=None):
        '''
        :param source_type: whether it is a solidity-file or evm-bytecode
        :param source_format: whether it is bytecode, ethereum-address or text
        :param source_list: List of files
        :param meta: meta data
        '''
        pass

    def get_source_from_contracts_list(self, contracts):
        '''
        get the source data from the contracts list
        :param contracts: the list of contracts
        :return:
        '''
        pass

    def get_source_index(self, bytecode_hash: str) -> int:
        '''
        Find the contract index in the list
        :param bytecode_hash: The contract hash
        :return: The index of the contract in the _source_hash list
        '''
        pass",4,4,19.0,0.0,13.0,6.0,4.0,0.49,0.0,5.0,2.0,0.0,3.0,4.0,3.0,3.0,61.0,4.0,39.0,9.0,35.0,19.0,31.0,9.0,27.0,9.0,0.0,3.0,12.0,snippet_21
100863,bigchaindb/bigchaindb,bigchaindb_bigchaindb/bigchaindb/validation.py,bigchaindb.validation.BaseValidationRules,"class BaseValidationRules:
    """"""Base validation rules for BigchainDB.

    A validation plugin must expose a class inheriting from this one via an entry_point.

    All methods listed below must be implemented.
    """"""

    @staticmethod
    def validate_transaction(bigchaindb, transaction):
        """"""See :meth:`bigchaindb.models.Transaction.validate`
        for documentation.
        """"""
        return transaction.validate(bigchaindb)

    @staticmethod
    def validate_block(bigchaindb, block):
        """"""See :meth:`bigchaindb.models.Block.validate` for documentation.""""""
        return block.validate(bigchaindb)","class BaseValidationRules:
    '''Base validation rules for BigchainDB.
    A validation plugin must expose a class inheriting from this one via an entry_point.
    All methods listed below must be implemented.
    '''
    @staticmethod
    def validate_transaction(bigchaindb, transaction):
        '''See :meth:`bigchaindb.models.Transaction.validate`
        for documentation.
        '''
        pass
    @staticmethod
    def validate_block(bigchaindb, block):
        '''See :meth:`bigchaindb.models.Block.validate` for documentation.'''
        pass",3,3,4.0,0.0,2.0,2.0,1.0,1.14,0.0,0.0,0.0,0.0,0.0,0.0,2.0,2.0,19.0,4.0,7.0,5.0,2.0,8.0,5.0,3.0,2.0,1.0,0.0,0.0,2.0,snippet_22
100865,bigchaindb/bigchaindb,bigchaindb_bigchaindb/bigchaindb/web/strip_content_type_middleware.py,bigchaindb.web.strip_content_type_middleware.StripContentTypeMiddleware,"class StripContentTypeMiddleware:
    """"""WSGI middleware to strip Content-Type header for GETs.""""""

    def __init__(self, app):
        """"""Create the new middleware.

        Args:
            app: a flask application
        """"""
        self.app = app

    def __call__(self, environ, start_response):
        """"""Run the middleware and then call the original WSGI application.""""""
        if environ['REQUEST_METHOD'] == 'GET':
            try:
                del environ['CONTENT_TYPE']
            except KeyError:
                pass
            else:
                logger.debug('Remove header ""Content-Type"" from GET request')
        return self.app(environ, start_response)","class StripContentTypeMiddleware:
    '''WSGI middleware to strip Content-Type header for GETs.'''

    def __init__(self, app):
        '''Create the new middleware.
        Args:
            app: a flask application
        '''
        pass

    def __call__(self, environ, start_response):
        '''Run the middleware and then call the original WSGI application.'''
        pass",3,3,9.0,1.0,6.0,3.0,2.0,0.5,0.0,1.0,0.0,0.0,2.0,1.0,2.0,2.0,22.0,4.0,12.0,4.0,9.0,6.0,12.0,4.0,9.0,3.0,0.0,2.0,4.0,snippet_23
100895,ansible/molecule,src/molecule/app.py,molecule.app.App,"from pathlib import Path
from subprocess import CalledProcessError, CompletedProcess
from molecule.console import original_stderr
from ansible_compat.runtime import Runtime
from molecule.ansi_output import CommandBorders
from molecule.util import print_environment_vars

class App:
    """"""App class that keep runtime status.""""""

    def __init__(self, path: Path) -> None:
        """"""Create a new app instance.

        Args:
            path: The path to the project.
        """"""
        self.runtime = Runtime(project_dir=path, isolated=False)

    def run_command(self, cmd: str | list[str], env: dict[str, str] | None=None, cwd: Path | None=None, *, debug: bool=False, echo: bool=False, quiet: bool=False, check: bool=False, command_borders: bool=False) -> CompletedProcess[str]:
        """"""Execute the given command and returns None.

        Args:
            cmd: A list of strings containing the command to run.
            env: A dict containing the shell's environment.
            cwd: An optional Path to the working directory.
            debug: An optional bool to toggle debug output.
            echo: An optional bool to toggle command echo.
            quiet: An optional bool to toggle command output.
            check: An optional bool to toggle command error checking.
            command_borders: An optional bool to enable borders around command output.

        Returns:
            A completed process object.

        Raises:
            CalledProcessError: If return code is nonzero and check is True.
        """"""
        if debug:
            print_environment_vars(env)
        borders = None
        if command_borders:
            borders = CommandBorders(cmd=cmd, original_stderr=original_stderr)
        result = self.runtime.run(args=cmd, env=env, cwd=cwd, tee=True, set_acp=False)
        if borders:
            borders.finalize(result.returncode)
        if result.returncode != 0 and check:
            raise CalledProcessError(returncode=result.returncode, cmd=result.args, output=result.stdout, stderr=result.stderr)
        return result","
class App:
    '''App class that keep runtime status.'''

    def __init__(self, path: Path) -> None:
        '''Create a new app instance.
        Args:
            path: The path to the project.
        '''
        pass

    def run_command(self, cmd: str | list[str], env: dict[str, str] | None=None, cwd: Path | None=None, *, debug: bool=False, echo: bool=False, quiet: bool=False, check: bool=False, command_borders: bool=False) -> CompletedProcess[str]:
        '''Execute the given command and returns None.
        Args:
            cmd: A list of strings containing the command to run.
            env: A dict containing the shell's environment.
            cwd: An optional Path to the working directory.
            debug: An optional bool to toggle debug output.
            echo: An optional bool to toggle command echo.
            quiet: An optional bool to toggle command output.
            check: An optional bool to toggle command error checking.
            command_borders: An optional bool to enable borders around command output.
        Returns:
            A completed process object.
        Raises:
            CalledProcessError: If return code is nonzero and check is True.
        '''
        pass",3,3,27.0,3.0,15.0,11.0,2.0,0.71,0.0,7.0,0.0,0.0,2.0,1.0,2.0,2.0,57.0,7.0,31.0,15.0,18.0,22.0,10.0,5.0,7.0,3.0,0.0,1.0,4.0,snippet_24
104713,autokey/autokey,autokey_autokey/lib/autokey/interface.py,autokey.interface.AbstractClipboard,"from abc import abstractmethod

class AbstractClipboard:
    """"""
    Abstract interface for clipboard interactions.
    This is an abstraction layer for platform dependent clipboard handling.
    It unifies clipboard handling for Qt and GTK.
    """"""

    @property
    @abstractmethod
    def text(self):
        """"""Get and set the keyboard clipboard content.""""""
        return

    @property
    @abstractmethod
    def selection(self):
        """"""Get and set the mouse selection clipboard content.""""""
        return","
class AbstractClipboard:
    '''
    Abstract interface for clipboard interactions.
    This is an abstraction layer for platform dependent clipboard handling.
    It unifies clipboard handling for Qt and GTK.
    '''
    @property
    @abstractmethod
    def text(self):
        '''Get and set the keyboard clipboard content.'''
        pass
    @property
    @abstractmethod
    def selection(self):
        '''Get and set the mouse selection clipboard content.'''
        pass",3,3,3.0,0.0,2.0,1.0,1.0,0.78,0.0,0.0,0.0,2.0,2.0,0.0,2.0,2.0,17.0,1.0,9.0,5.0,2.0,7.0,5.0,3.0,2.0,1.0,0.0,0.0,2.0,snippet_25
104788,autokey/autokey,autokey_autokey/lib/autokey/scripting/system.py,autokey.scripting.system.System,"import subprocess

class System:
    """"""
    Simplified access to some system commands.
    """"""

    @staticmethod
    def exec_command(command, getOutput=True):
        """"""
        Execute a shell command

        Usage: C{system.exec_command(command, getOutput=True)}

        Set getOutput to False if the command does not exit and return immediately. Otherwise
        AutoKey will not respond to any hotkeys/abbreviations etc until the process started
        by the command exits.

        @param command: command to be executed (including any arguments) - e.g. ""ls -l""
        @param getOutput: whether to capture the (stdout) output of the command
        @raise subprocess.CalledProcessError: if the command returns a non-zero exit code
        """"""
        if getOutput:
            with subprocess.Popen(command, shell=True, bufsize=-1, stdout=subprocess.PIPE, universal_newlines=True) as p:
                output = p.communicate()[0]
                output = output.rstrip('\n')
                if p.returncode:
                    raise subprocess.CalledProcessError(p.returncode, output)
                return output
        else:
            subprocess.Popen(command, shell=True, bufsize=-1)

    @staticmethod
    def create_file(file_name, contents=''):
        """"""
        Create a file with contents

        Usage: C{system.create_file(fileName, contents="""")}

        @param fileName: full path to the file to be created
        @param contents: contents to insert into the file
        """"""
        with open(file_name, 'w') as written_file:
            written_file.write(contents)","
class System:
    '''
    Simplified access to some system commands.
    '''
    @staticmethod
    def exec_command(command, getOutput=True):
        '''
        Execute a shell command
        Usage: C{system.exec_command(command, getOutput=True)}
        Set getOutput to False if the command does not exit and return immediately. Otherwise
        AutoKey will not respond to any hotkeys/abbreviations etc until the process started
        by the command exits.
        @param command: command to be executed (including any arguments) - e.g. ""ls -l""
        @param getOutput: whether to capture the (stdout) output of the command
        @raise subprocess.CalledProcessError: if the command returns a non-zero exit code
        '''
        pass
    @staticmethod
    def create_file(file_name, contents=''):
        '''
        Create a file with contents
        Usage: C{system.create_file(fileName, contents="""")}
        @param fileName: full path to the file to be created
        @param contents: contents to insert into the file
        '''
        pass",3,3,20.0,3.0,9.0,9.0,2.0,0.95,0.0,2.0,0.0,0.0,0.0,0.0,2.0,2.0,47.0,6.0,21.0,8.0,16.0,20.0,13.0,4.0,10.0,3.0,0.0,3.0,4.0,snippet_26
105760,insanum/gcalcli,insanum_gcalcli/gcalcli/details.py,gcalcli.details.Handler,"class Handler:
    """"""Handler for a specific detail of an event.""""""
    fieldnames: list[str] = []

    @classmethod
    def get(cls, event):
        """"""Return simple string representation for columnar output.""""""
        raise NotImplementedError

    @classmethod
    def data(cls, event):
        """"""Return plain data for formatted output.""""""
        return NotImplementedError

    @classmethod
    def patch(cls, cal, event, fieldname, value):
        """"""Patch event from value.""""""
        raise NotImplementedError","class Handler:
    '''Handler for a specific detail of an event.'''
    @classmethod
    def get(cls, event):
        '''Return simple string representation for columnar output.'''
        pass
    @classmethod
    def data(cls, event):
        '''Return plain data for formatted output.'''
        pass
    @classmethod
    def patch(cls, cal, event, fieldname, value):
        '''Patch event from value.'''
        pass",4,4,3.0,0.0,2.0,1.0,1.0,0.5,0.0,1.0,0.0,4.0,0.0,0.0,2.0,2.0,15.0,3.0,8.0,6.0,3.0,4.0,6.0,4.0,3.0,1.0,0.0,0.0,2.0,snippet_27
106092,mikedh/trimesh,trimesh/collision.py,trimesh.collision.DistanceData,"class DistanceData:
    """"""
    Data structure for holding information about a distance query.
    """"""

    def __init__(self, names, result):
        """"""
        Initialize a DistanceData.

        Parameters
        ----------
        names : list of str
          The names of the two objects in order.
        contact : fcl.DistanceResult
          The distance query result.
        """"""
        self.names = set(names)
        self._inds = {names[0]: result.b1, names[1]: result.b2}
        self._points = {names[0]: result.nearest_points[0], names[1]: result.nearest_points[1]}
        self._distance = result.min_distance

    @property
    def distance(self):
        """"""
        Returns the distance between the two objects.

        Returns
        -------
        distance : float
          The euclidean distance between the objects.
        """"""
        return self._distance

    def index(self, name):
        """"""
        Returns the index of the closest face for the mesh with
        the given name.

        Parameters
        ----------
        name : str
          The name of the target object.

        Returns
        -------
        index : int
          The index of the face in collisoin.
        """"""
        return self._inds[name]

    def point(self, name):
        """"""
        The 3D point of closest distance on the mesh with the given name.

        Parameters
        ----------
        name : str
          The name of the target object.

        Returns
        -------
        point : (3,) float
          The closest point.
        """"""
        return self._points[name]","class DistanceData:
    '''
    Data structure for holding information about a distance query.
    '''

    def __init__(self, names, result):
        '''
        Initialize a DistanceData.
        Parameters
        ----------
        names : list of str
          The names of the two objects in order.
        contact : fcl.DistanceResult
          The distance query result.
        '''
        pass
    @property
    def distance(self):
        '''
        Returns the distance between the two objects.
        Returns
        -------
        distance : float
          The euclidean distance between the objects.
        '''
        pass

    def index(self, name):
        '''
        Returns the index of the closest face for the mesh with
        the given name.
        Parameters
        ----------
        name : str
          The name of the target object.
        Returns
        -------
        index : int
          The index of the face in collisoin.
        '''
        pass

    def point(self, name):
        '''
        The 3D point of closest distance on the mesh with the given name.
        Parameters
        ----------
        name : str
          The name of the target object.
        Returns
        -------
        point : (3,) float
          The closest point.
        '''
        pass",5,5,15.0,2.0,4.0,10.0,1.0,2.63,0.0,1.0,0.0,0.0,4.0,4.0,4.0,4.0,68.0,10.0,16.0,10.0,10.0,42.0,12.0,9.0,7.0,1.0,0.0,0.0,4.0,snippet_28
106111,mikedh/trimesh,trimesh/path/packing.py,trimesh.path.packing.RectangleBin,"import numpy as np

class RectangleBin:
    """"""
    An N-dimensional binary space partition tree for packing
    hyper-rectangles. Split logic is pure `numpy` but behaves
    similarly to `scipy.spatial.Rectangle`.

    Mostly useful for packing 2D textures and 3D boxes and
    has not been tested outside of 2 and 3 dimensions.

    Original article about using this for packing textures:
    http://www.blackpawn.com/texts/lightmaps/
    """"""

    def __init__(self, bounds):
        """"""
        Create a rectangular bin.

        Parameters
        ------------
        bounds : (2, dimension *) float
          Bounds array are `[mins, maxes]`
        """"""
        self.child = []
        self.occupied = False
        self.bounds = np.array(bounds, dtype=np.float64)

    @property
    def extents(self):
        """"""
        Bounding box size.

        Returns
        ----------
        extents : (dimension,) float
          Edge lengths of bounding box
        """"""
        bounds = self.bounds
        return bounds[1] - bounds[0]

    def insert(self, size, rotate=True):
        """"""
        Insert a rectangle into the bin.

        Parameters
        -------------
        size : (dimension,) float
          Size of rectangle to insert/

        Returns
        ----------
        inserted : (2,) float or None
          Position of insertion in the tree or None
          if the insertion was unsuccessful.
        """"""
        for child in self.child:
            attempt = child.insert(size=size, rotate=rotate)
            if attempt is not None:
                return attempt
        if self.occupied:
            return None
        bounds = self.bounds.copy()
        extents = bounds[1] - bounds[0]
        if rotate:
            for roll in range(len(size)):
                size_test = extents - _roll(size, roll)
                fits = (size_test > -_TOL_ZERO).all()
                if fits:
                    size = _roll(size, roll)
                    break
            if not fits:
                return None
        else:
            size_test = extents - size
            if (size_test < -_TOL_ZERO).any():
                return None
        self.occupied = True
        if (size_test < _TOL_ZERO).all():
            return bounds
        axis = size_test.argmax()
        splits = np.vstack((bounds, bounds))
        splits[1:3, axis] = bounds[0][axis] + size[axis]
        self.child[:] = (RectangleBin(splits[:2]), RectangleBin(splits[2:]))
        return self.child[0].insert(size, rotate=rotate)","
class RectangleBin:
    '''
    An N-dimensional binary space partition tree for packing
    hyper-rectangles. Split logic is pure `numpy` but behaves
    similarly to `scipy.spatial.Rectangle`.
    Mostly useful for packing 2D textures and 3D boxes and
    has not been tested outside of 2 and 3 dimensions.
    Original article about using this for packing textures:
    http://www.blackpawn.com/texts/lightmaps/
    '''

    def __init__(self, bounds):
        '''
        Create a rectangular bin.
        Parameters
        ------------
        bounds : (2, dimension *) float
          Bounds array are `[mins, maxes]`
        '''
        pass
    @property
    def extents(self):
        '''
        Bounding box size.
        Returns
        ----------
        extents : (dimension,) float
          Edge lengths of bounding box
        '''
        pass

    def insert(self, size, rotate=True):
        '''
        Insert a rectangle into the bin.
        Parameters
        -------------
        size : (dimension,) float
          Size of rectangle to insert/
        Returns
        ----------
        inserted : (2,) float or None
          Position of insertion in the tree or None
          if the insertion was unsuccessful.
        '''
        pass",4,4,33.0,4.0,12.0,16.0,4.0,1.49,0.0,1.0,0.0,0.0,3.0,3.0,3.0,3.0,114.0,17.0,39.0,18.0,34.0,58.0,37.0,17.0,33.0,10.0,0.0,3.0,12.0,snippet_29
107609,weld-project/weld,python/grizzly/grizzly/lazy_op.py,grizzly.lazy_op.LazyOpResult,"class LazyOpResult:
    """"""Wrapper class around as yet un-evaluated Weld computation results

    Attributes:
        dim (int): Dimensionality of the output
        expr (WeldObject / Numpy.ndarray): The expression that needs to be
            evaluated
        weld_type (WeldType): Type of the output object
    """"""

    def __init__(self, expr, weld_type, dim):
        """"""Summary

        Args:
            expr (TYPE): Description
            weld_type (TYPE): Description
            dim (TYPE): Description
        """"""
        self.expr = expr
        self.weld_type = weld_type
        self.dim = dim

    def evaluate(self, verbose=True, decode=True, passes=None, num_threads=1, apply_experimental_transforms=False):
        """"""Summary

        Args:
            verbose (bool, optional): Description
            decode (bool, optional): Description

        Returns:
            TYPE: Description
        """"""
        if isinstance(self.expr, WeldObject):
            return self.expr.evaluate(to_weld_type(self.weld_type, self.dim), verbose, decode, passes=passes, num_threads=num_threads, apply_experimental_transforms=apply_experimental_transforms)
        return self.expr","class LazyOpResult:
    '''Wrapper class around as yet un-evaluated Weld computation results
    Attributes:
        dim (int): Dimensionality of the output
        expr (WeldObject / Numpy.ndarray): The expression that needs to be
            evaluated
        weld_type (WeldType): Type of the output object
    '''

    def __init__(self, expr, weld_type, dim):
        '''Summary
        Args:
            expr (TYPE): Description
            weld_type (TYPE): Description
            dim (TYPE): Description
        '''
        pass

    def evaluate(self, verbose=True, decode=True, passes=None, num_threads=1, apply_experimental_transforms=False):
        '''Summary
        Args:
            verbose (bool, optional): Description
            decode (bool, optional): Description
        Returns:
            TYPE: Description
        '''
        pass",3,3,17.0,2.0,9.0,7.0,2.0,1.11,0.0,1.0,1.0,3.0,2.0,3.0,2.0,2.0,44.0,6.0,18.0,7.0,14.0,20.0,9.0,6.0,6.0,2.0,0.0,1.0,3.0,snippet_30
107612,weld-project/weld,python/grizzly/grizzly/seriesweld.py,grizzly.seriesweld.StringSeriesWeld,"import grizzly_impl

class StringSeriesWeld:
    """"""Summary

    Attributes:
        column_name (TYPE): Description
        df (TYPE): Description
        dim (int): Description
        expr (TYPE): Description
        weld_type (TYPE): Description
    """"""

    def __init__(self, expr, weld_type, df=None, column_name=None):
        """"""Summary

        Args:
            expr (TYPE): Description
            weld_type (TYPE): Description
            df (None, optional): Description
            column_name (None, optional): Description
        """"""
        self.expr = expr
        self.weld_type = weld_type
        self.dim = 1
        self.df = df
        self.column_name = column_name

    def slice(self, start, size):
        """"""Summary

        Args:
            start (TYPE): Description
            size (TYPE): Description

        Returns:
            TYPE: Description
        """"""
        return SeriesWeld(grizzly_impl.slice(self.expr, start, size, self.weld_type), self.weld_type, self.df, self.column_name)","
class StringSeriesWeld:
    '''Summary
    Attributes:
        column_name (TYPE): Description
        df (TYPE): Description
        dim (int): Description
        expr (TYPE): Description
        weld_type (TYPE): Description
    '''

    def __init__(self, expr, weld_type, df=None, column_name=None):
        '''Summary
        Args:
            expr (TYPE): Description
            weld_type (TYPE): Description
            df (None, optional): Description
            column_name (None, optional): Description
        '''
        pass

    def slice(self, start, size):
        '''Summary
        Args:
            start (TYPE): Description
            size (TYPE): Description
        Returns:
            TYPE: Description
        '''
        pass",3,3,18.0,2.0,9.0,7.0,1.0,1.16,0.0,1.0,1.0,0.0,2.0,5.0,2.0,2.0,47.0,6.0,19.0,8.0,16.0,22.0,9.0,8.0,6.0,1.0,0.0,0.0,2.0,snippet_31
108112,explosion/thinc,explosion_thinc/thinc/types.py,thinc.types.ArgsKwargs,"from typing import Any, Callable, Container, Dict, Generic, Iterable, Iterator, List, Optional, Sequence, Sized, Tuple, TypeVar, Union, cast, overload
from dataclasses import dataclass

@dataclass
class ArgsKwargs:
    """"""A tuple of (args, kwargs) that can be spread into some function f:

    f(*args, **kwargs)
    """"""
    args: Tuple[Any, ...]
    kwargs: Dict[str, Any]

    @classmethod
    def from_items(cls, items: Sequence[Tuple[Union[int, str], Any]]) -> 'ArgsKwargs':
        """"""Create an ArgsKwargs object from a sequence of (key, value) tuples,
        such as produced by argskwargs.items(). Each key should be either a string
        or an integer. Items with int keys are added to the args list, and
        items with string keys are added to the kwargs list. The args list is
        determined by sequence order, not the value of the integer.
        """"""
        args = []
        kwargs = {}
        for key, value in items:
            if isinstance(key, int):
                args.append(value)
            else:
                kwargs[key] = value
        return cls(args=tuple(args), kwargs=kwargs)

    def keys(self) -> Iterable[Union[int, str]]:
        """"""Yield indices from self.args, followed by keys from self.kwargs.""""""
        yield from range(len(self.args))
        yield from self.kwargs.keys()

    def values(self) -> Iterable[Any]:
        """"""Yield elements of from self.args, followed by values from self.kwargs.""""""
        yield from self.args
        yield from self.kwargs.values()

    def items(self) -> Iterable[Tuple[Union[int, str], Any]]:
        """"""Yield enumerate(self.args), followed by self.kwargs.items()""""""
        yield from enumerate(self.args)
        yield from self.kwargs.items()","@dataclass
class ArgsKwargs:
    '''A tuple of (args, kwargs) that can be spread into some function f:
    f(*args, **kwargs)
    '''
    @classmethod
    def from_items(cls, items: Sequence[Tuple[Union[int, str], Any]]) -> 'ArgsKwargs':
        '''Create an ArgsKwargs object from a sequence of (key, value) tuples,
        such as produced by argskwargs.items(). Each key should be either a string
        or an integer. Items with int keys are added to the args list, and
        items with string keys are added to the kwargs list. The args list is
        determined by sequence order, not the value of the integer.
        '''
        pass

    def keys(self) -> Iterable[Union[int, str]]:
        '''Yield indices from self.args, followed by keys from self.kwargs.'''
        pass

    def values(self) -> Iterable[Any]:
        '''Yield elements of from self.args, followed by values from self.kwargs.'''
        pass

    def items(self) -> Iterable[Tuple[Union[int, str], Any]]:
        '''Yield enumerate(self.args), followed by self.kwargs.items()'''
        pass",5,5,7.0,0.0,5.0,2.0,2.0,0.55,0.0,6.0,0.0,0.0,3.0,0.0,4.0,4.0,40.0,6.0,22.0,9.0,16.0,12.0,20.0,8.0,15.0,3.0,0.0,2.0,6.0,snippet_32
113819,abseil/abseil-py,abseil_abseil-py/absl/app.py,absl.app.ExceptionHandler,"class ExceptionHandler:
    """"""Base exception handler from which other may inherit.""""""

    def wants(self, exc):
        """"""Returns whether this handler wants to handle the exception or not.

        This base class returns True for all exceptions by default. Override in
        subclass if it wants to be more selective.

        Args:
          exc: Exception, the current exception.
        """"""
        del exc
        return True

    def handle(self, exc):
        """"""Do something with the current exception.

        Args:
          exc: Exception, the current exception

        This method must be overridden.
        """"""
        raise NotImplementedError()","class ExceptionHandler:
    '''Base exception handler from which other may inherit.'''

    def wants(self, exc):
        '''Returns whether this handler wants to handle the exception or not.
        This base class returns True for all exceptions by default. Override in
        subclass if it wants to be more selective.
        Args:
          exc: Exception, the current exception.
        '''
        pass

    def handle(self, exc):
        '''Do something with the current exception.
        Args:
          exc: Exception, the current exception
        This method must be overridden.
        '''
        pass",3,3,10.0,2.0,3.0,6.0,1.0,2.17,0.0,1.0,0.0,1.0,2.0,0.0,2.0,2.0,24.0,6.0,6.0,3.0,3.0,13.0,6.0,3.0,3.0,1.0,0.0,0.0,2.0,snippet_33
114749,JelteF/PyLaTeX,JelteF_PyLaTeX/pylatex/config.py,pylatex.config.Version1,"from contextlib import contextmanager

class Version1:
    """"""The config used to get the behaviour of v1.x.y of the library.

    The default attributes are::

        indent = True
        booktabs = False
        microtype = False
        row_height = None
    """"""
    indent = True
    booktabs = False
    microtype = False
    row_height = None

    def __init__(self, **kwargs):
        """"""
        Args
        ----
        kwargs:
            Key value pairs of the default attributes that should be overridden
        """"""
        for k, v in kwargs.items():
            setattr(self, k, v)

    @contextmanager
    def use(self):
        """"""Use the config temporarily in specific context.

        A simple usage example::

            with Version1(indent=False).use():
                # Do stuff where indent should be False
                ...


        """"""
        global active
        prev = active
        active = self
        yield
        active = prev

    @contextmanager
    def change(self, **kwargs):
        """"""Override some attributes of the config in a specific context.

        A simple usage example::

            with pylatex.config.active.change(indent=False):
                # Do stuff where indent should be False
                ...

        Args
        ----
        kwargs:
            Key value pairs of the default attributes that should be overridden
        """"""
        old_attrs = {}
        for k, v in kwargs.items():
            old_attrs[k] = getattr(self, k, v)
            setattr(self, k, v)
        yield self
        for k, v in old_attrs.items():
            setattr(self, k, v)","
class Version1:
    '''The config used to get the behaviour of v1.x.y of the library.
    The default attributes are::
        indent = True
        booktabs = False
        microtype = False
        row_height = None
    '''

    def __init__(self, **kwargs):
        '''
        Args
        ----
        kwargs:
            Key value pairs of the default attributes that should be overridden
        '''
        pass
    @contextmanager
    def use(self):
        '''Use the config temporarily in specific context.
        A simple usage example::
            with Version1(indent=False).use():
                # Do stuff where indent should be False
                ...

        '''
        pass
    @contextmanager
    def change(self, **kwargs):
        '''Override some attributes of the config in a specific context.
        A simple usage example::
            with pylatex.config.active.change(indent=False):
                # Do stuff where indent should be False
                ...
        Args
        ----
        kwargs:
            Key value pairs of the default attributes that should be overridden
        '''
        pass",4,4,17.0,4.0,6.0,8.0,2.0,1.25,0.0,0.0,0.0,1.0,3.0,0.0,3.0,3.0,70.0,17.0,24.0,15.0,17.0,30.0,22.0,13.0,17.0,3.0,0.0,1.0,6.0,snippet_34
116413,QuantEcon/QuantEcon.py,quantecon/util/timing.py,quantecon.util.timing.__Timer__,"import time
import numpy as np

class __Timer__:
    """"""Computes elapsed time, between tic, tac, and toc.

    Methods
    -------
    tic :
        Resets timer.
    toc :
        Returns and prints time elapsed since last tic().
    tac :
        Returns and prints time elapsed since last
             tic(), tac() or toc() whichever occured last.
    loop_timer :
        Returns and prints the total and average time elapsed for n runs
        of a given function.

    """"""
    start = None
    last = None

    def tic(self):
        """"""
        Save time for future use with `tac()` or `toc()`.

        Returns
        -------
        None
            This function doesn't return a value.
        """"""
        t = time.time()
        self.start = t
        self.last = t

    def tac(self, verbose=True, digits=2):
        """"""
        Return and print elapsed time since last `tic()`, `tac()`, or
        `toc()`.

        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.

        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.

        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`, `tac()`, or `toc()`.

        """"""
        if self.start is None:
            raise Exception('tac() without tic()')
        t = time.time()
        elapsed = t - self.last
        self.last = t
        if verbose:
            m, s = divmod(elapsed, 60)
            h, m = divmod(m, 60)
            print('TAC: Elapsed: %d:%02d:%0d.%0*d' % (h, m, s, digits, s % 1 * 10 ** digits))
        return elapsed

    def toc(self, verbose=True, digits=2):
        """"""
        Return and print time elapsed since last `tic()`.

        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.

        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.

        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`.

        """"""
        if self.start is None:
            raise Exception('toc() without tic()')
        t = time.time()
        self.last = t
        elapsed = t - self.start
        if verbose:
            m, s = divmod(elapsed, 60)
            h, m = divmod(m, 60)
            print('TOC: Elapsed: %d:%02d:%0d.%0*d' % (h, m, s, digits, s % 1 * 10 ** digits))
        return elapsed

    def loop_timer(self, n, function, args=None, verbose=True, digits=2, best_of=3):
        """"""
        Return and print the total and average time elapsed for n runs
        of function.

        Parameters
        ----------
        n : scalar(int)
            Number of runs.

        function : function
            Function to be timed.

        args : list, optional(default=None)
            Arguments of the function.

        verbose : bool, optional(default=True)
            If True, then prints average time.

        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.

        best_of : scalar(int), optional(default=3)
            Average time over best_of runs.

        Returns
        -------
        average_time : scalar(float)
            Average time elapsed for n runs of function.

        average_of_best : scalar(float)
            Average of best_of times for n runs of function.

        """"""
        tic()
        all_times = np.empty(n)
        for run in range(n):
            if hasattr(args, '__iter__'):
                function(*args)
            elif args is None:
                function()
            else:
                function(args)
            all_times[run] = tac(verbose=False, digits=digits)
        elapsed = toc(verbose=False, digits=digits)
        m, s = divmod(elapsed, 60)
        h, m = divmod(m, 60)
        print('Total run time: %d:%02d:%0d.%0*d' % (h, m, s, digits, s % 1 * 10 ** digits))
        average_time = all_times.mean()
        average_of_best = np.sort(all_times)[:best_of].mean()
        if verbose:
            m, s = divmod(average_time, 60)
            h, m = divmod(m, 60)
            print('Average time for %d runs: %d:%02d:%0d.%0*d' % (n, h, m, s, digits, s % 1 * 10 ** digits))
            m, s = divmod(average_of_best, 60)
            h, m = divmod(m, 60)
            print('Average of %d best times: %d:%02d:%0d.%0*d' % (best_of, h, m, s, digits, s % 1 * 10 ** digits))
        return (average_time, average_of_best)","
class __Timer__:
    '''Computes elapsed time, between tic, tac, and toc.
    Methods
    -------
    tic :
        Resets timer.
    toc :
        Returns and prints time elapsed since last tic().
    tac :
        Returns and prints time elapsed since last
             tic(), tac() or toc() whichever occured last.
    loop_timer :
        Returns and prints the total and average time elapsed for n runs
        of a given function.
    '''

    def tic(self):
        '''
        Save time for future use with `tac()` or `toc()`.
        Returns
        -------
        None
            This function doesn't return a value.
        '''
        pass

    def tac(self, verbose=True, digits=2):
        '''
        Return and print elapsed time since last `tic()`, `tac()`, or
        `toc()`.
        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.
        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.
        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`, `tac()`, or `toc()`.
        '''
        pass

    def toc(self, verbose=True, digits=2):
        '''
        Return and print time elapsed since last `tic()`.
        Parameters
        ----------
        verbose : bool, optional(default=True)
            If True, then prints time.
        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.
        Returns
        -------
        elapsed : scalar(float)
            Time elapsed since last `tic()`.
        '''
        pass

    def loop_timer(self, n, function, args=None, verbose=True, digits=2, best_of=3):
        '''
        Return and print the total and average time elapsed for n runs
        of function.
        Parameters
        ----------
        n : scalar(int)
            Number of runs.
        function : function
            Function to be timed.
        args : list, optional(default=None)
            Arguments of the function.
        verbose : bool, optional(default=True)
            If True, then prints average time.
        digits : scalar(int), optional(default=2)
            Number of digits printed for time elapsed.
        best_of : scalar(int), optional(default=3)
            Average time over best_of runs.
        Returns
        -------
        average_time : scalar(float)
            Average time elapsed for n runs of function.
        average_of_best : scalar(float)
            Average of best_of times for n runs of function.
        '''
        pass",5,5,35.0,8.0,14.0,14.0,3.0,1.13,0.0,2.0,0.0,0.0,4.0,0.0,4.0,4.0,164.0,36.0,60.0,24.0,54.0,68.0,52.0,23.0,47.0,5.0,0.0,2.0,12.0,snippet_35
116421,DerwenAI/pytextrank,DerwenAI_pytextrank/pytextrank/base.py,pytextrank.base.Sentence,"import typing
from spacy.tokens import Doc, Span, Token
from dataclasses import dataclass

@dataclass
class Sentence:
    """"""
A data class representing the distance measure for one sentence.
    """"""
    start: int
    end: int
    sent_id: int
    phrases: typing.Set[int]
    distance: float

    def empty(self) -> bool:
        """"""
Test whether this sentence includes any ranked phrases.

    returns:
`True` if the `phrases` is not empty.
        """"""
        return len(self.phrases) == 0

    def text(self, doc: Doc) -> str:
        """"""
Accessor for the text slice of the `spaCy` [`Doc`](https://spacy.io/api/doc)
document represented by this sentence.

    doc:
source document

    returns:
the sentence text
        """"""
        return doc[self.start:self.end]","@dataclass
class Sentence:
    '''
A data class representing the distance measure for one sentence.
    '''

    def empty(self) -> bool:
        '''
Test whether this sentence includes any ranked phrases.
    returns:
`True` if the `phrases` is not empty.
        '''
        pass

    def text(self, doc: Doc) -> str:
        '''
Accessor for the text slice of the `spaCy` [`Doc`](https://spacy.io/api/doc)
document represented by this sentence.
    doc:
source document
    returns:
the sentence text
        '''
        pass",3,3,13.0,2.0,5.0,7.0,1.0,1.07,0.0,2.0,0.0,0.0,2.0,0.0,2.0,2.0,38.0,7.0,15.0,8.0,7.0,16.0,10.0,3.0,7.0,1.0,0.0,0.0,2.0,snippet_36
122712,dfunckt/django-rules,dfunckt_django-rules/rules/contrib/models.py,rules.contrib.models.RulesModelMixin,"class RulesModelMixin:
    """"""
    A mixin for Django's Model that adds hooks for stepping into the process of
    permission registration, which are called by the metaclass implementation in
    RulesModelBaseMixin.

    Use this mixin in a custom subclass of Model in order to change its behavior.
    """"""

    @classmethod
    def get_perm(cls, perm_type):
        """"""Converts permission type (""add"") to permission name (""app.add_modelname"")

        :param perm_type: ""add"", ""change"", etc., or custom value
        :type  perm_type: str
        :returns str:
        """"""
        return '%s.%s_%s' % (cls._meta.app_label, perm_type, cls._meta.model_name)

    @classmethod
    def preprocess_rules_permissions(cls, perms):
        """"""May alter a permissions dict before it's processed further.

        Use this, for instance, to alter the supplied permissions or insert default
        values into the given dict.

        :param perms:
            Shallow-copied value of the rules_permissions model Meta option
        :type  perms: dict
        """"""","class RulesModelMixin:
    '''
    A mixin for Django's Model that adds hooks for stepping into the process of
    permission registration, which are called by the metaclass implementation in
    RulesModelBaseMixin.
    Use this mixin in a custom subclass of Model in order to change its behavior.
        '''
    @classmethod
    def get_perm(cls, perm_type):
        '''Converts permission type (""add"") to permission name (""app.add_modelname"")
        :param perm_type: ""add"", ""change"", etc., or custom value
        :type  perm_type: str
        :returns str:
        '''
        pass
    @classmethod
    def preprocess_rules_permissions(cls, perms):
        '''May alter a permissions dict before it's processed further.
        Use this, for instance, to alter the supplied permissions or insert default
        values into the given dict.
        :param perms:
            Shallow-copied value of the rules_permissions model Meta option
        :type  perms: dict
        '''
        pass",3,3,9.0,2.0,2.0,6.0,1.0,3.0,0.0,0.0,0.0,1.0,0.0,0.0,2.0,2.0,30.0,6.0,6.0,5.0,1.0,18.0,4.0,3.0,1.0,1.0,0.0,0.0,2.0,snippet_37
126370,Bouke/django-two-factor-auth,two_factor/admin.py,two_factor.admin.AdminSiteOTPRequiredMixin,"from django.contrib.auth import REDIRECT_FIELD_NAME
from django.conf import settings
from django.contrib.auth.views import redirect_to_login
from django.utils.http import url_has_allowed_host_and_scheme
from django.shortcuts import resolve_url

class AdminSiteOTPRequiredMixin:
    """"""
    Mixin for enforcing OTP verified staff users.

    Custom admin views should either be wrapped using :meth:`admin_view` or
    use :meth:`has_permission` in order to secure those views.
    """"""

    def has_permission(self, request):
        """"""
        Returns True if the given HttpRequest has permission to view
        *at least one* page in the admin site.
        """"""
        if not super().has_permission(request):
            return False
        return request.user.is_verified()

    def login(self, request, extra_context=None):
        """"""
        Redirects to the site login page for the given HttpRequest.
        """"""
        redirect_to = request.POST.get(REDIRECT_FIELD_NAME, request.GET.get(REDIRECT_FIELD_NAME))
        if not redirect_to or not url_has_allowed_host_and_scheme(url=redirect_to, allowed_hosts=[request.get_host()]):
            redirect_to = resolve_url(settings.LOGIN_REDIRECT_URL)
        return redirect_to_login(redirect_to)","
class AdminSiteOTPRequiredMixin:
    '''
    Mixin for enforcing OTP verified staff users.
    Custom admin views should either be wrapped using :meth:`admin_view` or
    use :meth:`has_permission` in order to secure those views.
    '''

    def has_permission(self, request):
        '''
        Returns True if the given HttpRequest has permission to view
        *at least one* page in the admin site.
        '''
        pass

    def login(self, request, extra_context=None):
        '''
        Redirects to the site login page for the given HttpRequest.
        '''
        pass",3,3,9.0,1.0,5.0,4.0,2.0,1.2,0.0,1.0,0.0,1.0,2.0,0.0,2.0,2.0,27.0,5.0,10.0,4.0,7.0,12.0,10.0,4.0,7.0,2.0,0.0,1.0,4.0,snippet_38
128868,arviz-devs/arviz,arviz-devs_arviz/arviz/utils.py,arviz.utils.Dask,"class Dask:
    """"""Class to toggle Dask states.

    Warnings
    --------
    Dask integration is an experimental feature still in progress. It can already be used
    but it doesn't work with all stats nor diagnostics yet.
    """"""
    dask_flag = False
    'bool: Enables Dask parallelization when set to True. Defaults to False.'
    dask_kwargs = None
    'dict: Additional keyword arguments for Dask configuration.\n    Defaults to an empty dictionary.'

    @classmethod
    def enable_dask(cls, dask_kwargs=None):
        """"""To enable Dask.

        Parameters
        ----------
        dask_kwargs : dict
            Dask related kwargs passed to :func:`~arviz.wrap_xarray_ufunc`.
        """"""
        cls.dask_flag = True
        cls.dask_kwargs = dask_kwargs

    @classmethod
    def disable_dask(cls):
        """"""To disable Dask.""""""
        cls.dask_flag = False
        cls.dask_kwargs = None","class Dask:
    '''Class to toggle Dask states.
    Warnings
    --------
    Dask integration is an experimental feature still in progress. It can already be used
    but it doesn't work with all stats nor diagnostics yet.
    '''
    @classmethod
    def enable_dask(cls, dask_kwargs=None):
        '''To enable Dask.
        Parameters
        ----------
        dask_kwargs : dict
            Dask related kwargs passed to :func:`~arviz.wrap_xarray_ufunc`.
        '''
        pass
    @classmethod
    def disable_dask(cls):
        '''To disable Dask.'''
        pass",3,3,7.0,1.0,3.0,4.0,1.0,1.18,0.0,0.0,0.0,0.0,0.0,0.0,2.0,2.0,29.0,5.0,11.0,7.0,6.0,13.0,9.0,5.0,6.0,1.0,0.0,0.0,2.0,snippet_39
128871,arviz-devs/arviz,arviz-devs_arviz/arviz/utils.py,arviz.utils.interactive_backend,"import matplotlib.pyplot as plt

class interactive_backend:
    """"""Context manager to change backend temporarily in ipython sesson.

    It uses ipython magic to change temporarily from the ipython inline backend to
    an interactive backend of choice. It cannot be used outside ipython sessions nor
    to change backends different than inline -> interactive.

    Notes
    -----
    The first time ``interactive_backend`` context manager is called, any of the available
    interactive backends can be chosen. The following times, this same backend must be used
    unless the kernel is restarted.

    Parameters
    ----------
    backend : str, optional
        Interactive backend to use. It will be passed to ``%matplotlib`` magic, refer to
        its docs to see available options.

    Examples
    --------
    Inside an ipython session (i.e. a jupyter notebook) with the inline backend set:

    .. code::

        >>> import arviz as az
        >>> idata = az.load_arviz_data(""centered_eight"")
        >>> az.plot_posterior(idata) # inline
        >>> with az.interactive_backend():
        ...     az.plot_density(idata) # interactive
        >>> az.plot_trace(idata) # inline

    """"""

    def __init__(self, backend=''):
        """"""Initialize context manager.""""""
        try:
            from IPython import get_ipython
        except ImportError as err:
            raise ImportError(f'The exception below was risen while importing Ipython, this context manager can only be used inside ipython sessions:\n{err}') from err
        self.ipython = get_ipython()
        if self.ipython is None:
            raise EnvironmentError('This context manager can only be used inside ipython sessions')
        self.ipython.magic(f'matplotlib {backend}')

    def __enter__(self):
        """"""Enter context manager.""""""
        return self

    def __exit__(self, exc_type, exc_value, exc_tb):
        """"""Exit context manager.""""""
        plt.show(block=True)
        self.ipython.magic('matplotlib inline')","
class interactive_backend:
    '''Context manager to change backend temporarily in ipython sesson.
    It uses ipython magic to change temporarily from the ipython inline backend to
    an interactive backend of choice. It cannot be used outside ipython sessions nor
    to change backends different than inline -> interactive.
    Notes
    -----
    The first time ``interactive_backend`` context manager is called, any of the available
    interactive backends can be chosen. The following times, this same backend must be used
    unless the kernel is restarted.
    Parameters
    ----------
    backend : str, optional
        Interactive backend to use. It will be passed to ``%matplotlib`` magic, refer to
        its docs to see available options.
    Examples
    --------
    Inside an ipython session (i.e. a jupyter notebook) with the inline backend set:
    .. code::
        >>> import arviz as az
        >>> idata = az.load_arviz_data(""centered_eight"")
        >>> az.plot_posterior(idata) # inline
        >>> with az.interactive_backend():
        ...     az.plot_density(idata) # interactive
        >>> az.plot_trace(idata) # inline
    '''

    def __init__(self, backend=''):
        '''Initialize context manager.'''
        pass

    def __enter__(self):
        '''Enter context manager.'''
        pass

    def __exit__(self, exc_type, exc_value, exc_tb):
        '''Exit context manager.'''
        pass",4,4,7.0,0.0,6.0,1.0,2.0,1.67,0.0,1.0,0.0,0.0,3.0,1.0,3.0,3.0,57.0,10.0,18.0,7.0,13.0,30.0,15.0,6.0,10.0,3.0,0.0,1.0,5.0,snippet_40
128872,arviz-devs/arviz,arviz-devs_arviz/arviz/utils.py,arviz.utils.lazy_property,"import functools

class lazy_property:
    """"""Used to load numba first time it is needed.""""""

    def __init__(self, fget):
        """"""Lazy load a property with `fget`.""""""
        self.fget = fget
        functools.update_wrapper(self, fget)

    def __get__(self, obj, cls):
        """"""Call the function, set the attribute.""""""
        if obj is None:
            return self
        value = self.fget(obj)
        setattr(obj, self.fget.__name__, value)
        return value","
class lazy_property:
    '''Used to load numba first time it is needed.'''

    def __init__(self, fget):
        '''Lazy load a property with `fget`.'''
        pass

    def __get__(self, obj, cls):
        '''Call the function, set the attribute.'''
        pass",3,3,7.0,1.0,5.0,2.0,2.0,0.5,0.0,0.0,0.0,0.0,2.0,1.0,2.0,2.0,18.0,4.0,10.0,5.0,7.0,5.0,10.0,5.0,7.0,2.0,0.0,1.0,3.0,snippet_41
129413,dtmilano/AndroidViewClient,dtmilano_AndroidViewClient/src/com/dtmilano/android/adb/adbclient.py,com.dtmilano.android.adb.adbclient.WifiManager,"import re
import sys

class WifiManager:
    """"""
    Simulates Android WifiManager.

    @see: http://developer.android.com/reference/android/net/wifi/WifiManager.html
    """"""
    WIFI_STATE_DISABLING = 0
    WIFI_STATE_DISABLED = 1
    WIFI_STATE_ENABLING = 2
    WIFI_STATE_ENABLED = 3
    WIFI_STATE_UNKNOWN = 4
    WIFI_IS_ENABLED_RE = re.compile('Wi-Fi is enabled')
    WIFI_IS_DISABLED_RE = re.compile('Wi-Fi is disabled')

    def __init__(self, device):
        """"""
        Constructor.
        :param device:
        :type device:
        """"""
        self.device = device

    def getWifiState(self):
        """"""
        Gets the Wi-Fi enabled state.

        @return: One of WIFI_STATE_DISABLED, WIFI_STATE_DISABLING, WIFI_STATE_ENABLED, WIFI_STATE_ENABLING, WIFI_STATE_UNKNOWN
        """"""
        result = self.device.shell('dumpsys wifi')
        if result:
            state = result.splitlines()[0]
            if self.WIFI_IS_ENABLED_RE.match(state):
                return self.WIFI_STATE_ENABLED
            elif self.WIFI_IS_DISABLED_RE.match(state):
                return self.WIFI_STATE_DISABLED
        print('UNKNOWN WIFI STATE:', state, file=sys.stderr)
        return self.WIFI_STATE_UNKNOWN","
class WifiManager:
    '''
    Simulates Android WifiManager.
    @see: http://developer.android.com/reference/android/net/wifi/WifiManager.html
    '''

    def __init__(self, device):
        '''
        Constructor.
        :param device:
        :type device:
        '''
        pass

    def getWifiState(self):
        '''
        Gets the Wi-Fi enabled state.
        @return: One of WIFI_STATE_DISABLED, WIFI_STATE_DISABLING, WIFI_STATE_ENABLED, WIFI_STATE_ENABLING, WIFI_STATE_UNKNOWN
        '''
        pass",3,3,12.0,1.0,6.0,5.0,3.0,0.65,0.0,0.0,0.0,0.0,2.0,1.0,2.0,2.0,40.0,7.0,20.0,13.0,17.0,13.0,19.0,13.0,16.0,4.0,0.0,2.0,5.0,snippet_42
129620,neovim/pynvim,neovim_pynvim/pynvim/msgpack_rpc/async_session.py,pynvim.msgpack_rpc.async_session.Response,"from pynvim.msgpack_rpc.msgpack_stream import MsgpackStream

class Response:
    """"""Response to a msgpack-rpc request that came from Nvim.

    When Nvim sends a msgpack-rpc request, an instance of this class is
    created for remembering state required to send a response.
    """"""

    def __init__(self, msgpack_stream: MsgpackStream, request_id: int):
        """"""Initialize the Response instance.""""""
        self._msgpack_stream = msgpack_stream
        self._request_id = request_id

    def send(self, value, error=False):
        """"""Send the response.

        If `error` is True, it will be sent as an error.
        """"""
        if error:
            resp = [1, self._request_id, value, None]
        else:
            resp = [1, self._request_id, None, value]
        debug('sending response to request %d: %s', self._request_id, resp)
        self._msgpack_stream.send(resp)","
class Response:
    '''Response to a msgpack-rpc request that came from Nvim.
    When Nvim sends a msgpack-rpc request, an instance of this class is
    created for remembering state required to send a response.
    '''

    def __init__(self, msgpack_stream: MsgpackStream, request_id: int):
        '''Initialize the Response instance.'''
        pass

    def send(self, value, error=False):
        '''Send the response.
        If `error` is True, it will be sent as an error.
        '''
        pass",3,3,8.0,1.0,5.0,2.0,2.0,0.73,0.0,2.0,1.0,0.0,2.0,2.0,2.0,2.0,23.0,4.0,11.0,6.0,8.0,8.0,10.0,6.0,7.0,2.0,0.0,1.0,3.0,snippet_43
129967,sendgrid/sendgrid-python,sendgrid_sendgrid-python/sendgrid/helpers/eventwebhook/__init__.py,sendgrid.helpers.eventwebhook.EventWebhook,"from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.asymmetric import ec
from cryptography.exceptions import InvalidSignature
from cryptography.hazmat.primitives.serialization import load_pem_public_key
import base64

class EventWebhook:
    """"""
    This class allows you to use the Event Webhook feature. Read the docs for
    more details: https://sendgrid.com/docs/for-developers/tracking-events/event
    """"""

    def __init__(self, public_key=None):
        """"""
        Construct the Event Webhook verifier object
        :param public_key: verification key under Mail Settings
        :type public_key: string
        """"""
        self.public_key = self.convert_public_key_to_ecdsa(public_key) if public_key else public_key

    def convert_public_key_to_ecdsa(self, public_key):
        """"""
        Convert the public key string to an EllipticCurvePublicKey object.

        :param public_key: verification key under Mail Settings
        :type public_key string
        :return: An EllipticCurvePublicKey object using the ECDSA algorithm
        :rtype cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey
        """"""
        pem_key = '-----BEGIN PUBLIC KEY-----\n' + public_key + '\n-----END PUBLIC KEY-----'
        return load_pem_public_key(pem_key.encode('utf-8'))

    def verify_signature(self, payload, signature, timestamp, public_key=None):
        """"""
        Verify signed event webhook requests.

        :param payload: event payload in the request body
        :type payload: string
        :param signature: value obtained from the 'X-Twilio-Email-Event-Webhook-Signature' header
        :type signature: string
        :param timestamp: value obtained from the 'X-Twilio-Email-Event-Webhook-Timestamp' header
        :type timestamp: string
        :param public_key: elliptic curve public key
        :type public_key: cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey
        :return: true or false if signature is valid
        """"""
        timestamped_payload = (timestamp + payload).encode('utf-8')
        decoded_signature = base64.b64decode(signature)
        key = public_key or self.public_key
        try:
            key.verify(decoded_signature, timestamped_payload, ec.ECDSA(hashes.SHA256()))
            return True
        except InvalidSignature:
            return False","
class EventWebhook:
    '''
    This class allows you to use the Event Webhook feature. Read the docs for
    more details: https://sendgrid.com/docs/for-developers/tracking-events/event
    '''

    def __init__(self, public_key=None):
        '''
        Construct the Event Webhook verifier object
        :param public_key: verification key under Mail Settings
        :type public_key: string
        '''
        pass

    def convert_public_key_to_ecdsa(self, public_key):
        '''
        Convert the public key string to an EllipticCurvePublicKey object.
        :param public_key: verification key under Mail Settings
        :type public_key string
        :return: An EllipticCurvePublicKey object using the ECDSA algorithm
        :rtype cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey
        '''
        pass

    def verify_signature(self, payload, signature, timestamp, public_key=None):
        '''
        Verify signed event webhook requests.
        :param payload: event payload in the request body
        :type payload: string
        :param signature: value obtained from the 'X-Twilio-Email-Event-Webhook-Signature' header
        :type signature: string
        :param timestamp: value obtained from the 'X-Twilio-Email-Event-Webhook-Timestamp' header
        :type timestamp: string
        :param public_key: elliptic curve public key
        :type public_key: cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePublicKey
        :return: true or false if signature is valid
        '''
        pass",4,4,12.0,1.0,3.0,8.0,1.0,2.8,0.0,0.0,0.0,0.0,3.0,1.0,3.0,3.0,44.0,6.0,10.0,8.0,6.0,28.0,10.0,8.0,6.0,2.0,0.0,0.0,4.0,snippet_44
131054,SoCo/SoCo,SoCo_SoCo/soco/music_services/token_store.py,soco.music_services.token_store.TokenStoreBase,"class TokenStoreBase:
    """"""Token store base class""""""

    def __init__(self, token_collection='default'):
        """"""Instantiate instance variables

        Args:
            token_collection (str): The name of the token collection to use. This may be
                used to store different token collections for different client programs.
        """"""
        self.token_collection = token_collection

    def save_token_pair(self, music_service_id, household_id, token_pair):
        """"""Save a token value pair (token, key) which is a 2 item sequence""""""
        raise NotImplementedError

    def load_token_pair(self, music_service_id, household_id):
        """"""Load a token pair (token, key) which is a 2 item sequence""""""
        raise NotImplementedError

    def has_token(self, music_service_id, household_id):
        """"""Return True if a token is stored for the music service and household ID""""""
        raise NotImplementedError","class TokenStoreBase:
    '''Token store base class'''

    def __init__(self, token_collection='default'):
        '''Instantiate instance variables
        Args:
            token_collection (str): The name of the token collection to use. This may be
                used to store different token collections for different client programs.
        '''
        pass

    def save_token_pair(self, music_service_id, household_id, token_pair):
        '''Save a token value pair (token, key) which is a 2 item sequence'''
        pass

    def load_token_pair(self, music_service_id, household_id):
        '''Load a token pair (token, key) which is a 2 item sequence'''
        pass

    def has_token(self, music_service_id, household_id):
        '''Return True if a token is stored for the music service and household ID'''
        pass",5,5,4.0,0.0,2.0,2.0,1.0,1.0,0.0,1.0,0.0,1.0,4.0,1.0,4.0,4.0,23.0,5.0,9.0,6.0,4.0,9.0,9.0,6.0,4.0,1.0,0.0,0.0,4.0,snippet_45
131060,SoCo/SoCo,SoCo_SoCo/soco/plugins/sharelink.py,soco.plugins.sharelink.ShareClass,"class ShareClass:
    """"""Base class for supported services.""""""

    def canonical_uri(self, uri):
        """"""Recognize a share link and return its canonical representation.

        Args:
            uri (str): A URI like ""https://tidal.com/browse/album/157273956"".

        Returns:
            str: The canonical URI or None if not recognized.
        """"""
        raise NotImplementedError

    def service_number(self):
        """"""Return the service number.

        Returns:
            int: A number identifying the supported music service.
        """"""
        raise NotImplementedError

    @staticmethod
    def magic():
        """"""Return magic.

        Returns:
            dict: Magic prefix/key/class values for each share type.
        """"""
        return {'album': {'prefix': 'x-rincon-cpcontainer:1004206c', 'key': '00040000', 'class': 'object.container.album.musicAlbum'}, 'episode': {'prefix': '', 'key': '00032020', 'class': 'object.item.audioItem.musicTrack'}, 'track': {'prefix': '', 'key': '00032020', 'class': 'object.item.audioItem.musicTrack'}, 'show': {'prefix': 'x-rincon-cpcontainer:1006206c', 'key': '1006206c', 'class': 'object.container.playlistContainer'}, 'song': {'prefix': '', 'key': '10032020', 'class': 'object.item.audioItem.musicTrack'}, 'playlist': {'prefix': 'x-rincon-cpcontainer:1006206c', 'key': '1006206c', 'class': 'object.container.playlistContainer'}}

    def extract(self, uri):
        """"""Extract the share type and encoded URI from a share link.

        Returns:
            share_type: The shared type, like ""album"" or ""track"".
            encoded_uri: An escaped URI with a service-specific format.
        """"""
        raise NotImplementedError","class ShareClass:
    '''Base class for supported services.'''

    def canonical_uri(self, uri):
        '''Recognize a share link and return its canonical representation.
        Args:
            uri (str): A URI like ""https://tidal.com/browse/album/157273956"".
        Returns:
            str: The canonical URI or None if not recognized.
        '''
        pass

    def service_number(self):
        '''Return the service number.
        Returns:
            int: A number identifying the supported music service.
        '''
        pass
    @staticmethod
    def magic():
        '''Return magic.
        Returns:
            dict: Magic prefix/key/class values for each share type.
        '''
        pass

    def extract(self, uri):
        '''Extract the share type and encoded URI from a share link.
        Returns:
            share_type: The shared type, like ""album"" or ""track"".
            encoded_uri: An escaped URI with a service-specific format.
        '''
        pass",5,5,16.0,1.0,10.0,5.0,1.0,0.49,0.0,1.0,0.0,4.0,3.0,0.0,4.0,4.0,70.0,9.0,41.0,6.0,35.0,20.0,9.0,5.0,4.0,1.0,0.0,0.0,4.0,snippet_46
131924,mjg59/python-broadlink,mjg59_python-broadlink/broadlink/helpers.py,broadlink.helpers.CRC16,"from typing import Dict, List, Sequence

class CRC16:
    """"""Helps with CRC-16 calculation.

    CRC tables are cached for performance.
    """"""
    _cache: Dict[int, List[int]] = {}

    @classmethod
    def get_table(cls, polynomial: int) -> List[int]:
        """"""Return the CRC-16 table for a polynomial.""""""
        try:
            crc_table = cls._cache[polynomial]
        except KeyError:
            crc_table = []
            for dividend in range(0, 256):
                remainder = dividend
                for _ in range(0, 8):
                    if remainder & 1:
                        remainder = remainder >> 1 ^ polynomial
                    else:
                        remainder = remainder >> 1
                crc_table.append(remainder)
            cls._cache[polynomial] = crc_table
        return crc_table

    @classmethod
    def calculate(cls, sequence: Sequence[int], polynomial: int=40961, init_value: int=65535) -> int:
        """"""Calculate the CRC-16 of a sequence of integers.""""""
        crc_table = cls.get_table(polynomial)
        crc = init_value
        for item in sequence:
            crc = crc >> 8 ^ crc_table[(crc ^ item) & 255]
        return crc","
class CRC16:
    '''Helps with CRC-16 calculation.
    CRC tables are cached for performance.
    '''
    @classmethod
    def get_table(cls, polynomial: int) -> List[int]:
        '''Return the CRC-16 table for a polynomial.'''
        pass
    @classmethod
    def calculate(cls, sequence: Sequence[int], polynomial: int=40961, init_value: int=65535) -> int:
        '''Calculate the CRC-16 of a sequence of integers.'''
        pass",3,3,14.0,0.0,13.0,2.0,4.0,0.2,0.0,3.0,0.0,0.0,0.0,0.0,2.0,2.0,39.0,4.0,30.0,18.0,20.0,6.0,22.0,11.0,19.0,5.0,0.0,4.0,7.0,snippet_47
131928,mjg59/python-broadlink,mjg59_python-broadlink/broadlink/protocol.py,broadlink.protocol.Datetime,"import time
import datetime as dt

class Datetime:
    """"""Helps to pack and unpack datetime objects for the Broadlink protocol.""""""

    @staticmethod
    def pack(datetime: dt.datetime) -> bytes:
        """"""Pack the timestamp to be sent over the Broadlink protocol.""""""
        data = bytearray(12)
        utcoffset = int(datetime.utcoffset().total_seconds() / 3600)
        data[:4] = utcoffset.to_bytes(4, 'little', signed=True)
        data[4:6] = datetime.year.to_bytes(2, 'little')
        data[6] = datetime.minute
        data[7] = datetime.hour
        data[8] = int(datetime.strftime('%y'))
        data[9] = datetime.isoweekday()
        data[10] = datetime.day
        data[11] = datetime.month
        return data

    @staticmethod
    def unpack(data: bytes) -> dt.datetime:
        """"""Unpack a timestamp received over the Broadlink protocol.""""""
        utcoffset = int.from_bytes(data[0:4], 'little', signed=True)
        year = int.from_bytes(data[4:6], 'little')
        minute = data[6]
        hour = data[7]
        subyear = data[8]
        isoweekday = data[9]
        day = data[10]
        month = data[11]
        tz_info = dt.timezone(dt.timedelta(hours=utcoffset))
        datetime = dt.datetime(year, month, day, hour, minute, 0, 0, tz_info)
        if datetime.isoweekday() != isoweekday:
            raise ValueError('isoweekday does not match')
        if int(datetime.strftime('%y')) != subyear:
            raise ValueError('subyear does not match')
        return datetime

    @staticmethod
    def now() -> dt.datetime:
        """"""Return the current date and time with timezone info.""""""
        tz_info = dt.timezone(dt.timedelta(seconds=-time.timezone))
        return dt.datetime.now(tz_info)","
class Datetime:
    '''Helps to pack and unpack datetime objects for the Broadlink protocol.'''
    @staticmethod
    def pack(datetime: dt.datetime) -> bytes:
        '''Pack the timestamp to be sent over the Broadlink protocol.'''
        pass
    @staticmethod
    def unpack(data: bytes) -> dt.datetime:
        '''Unpack a timestamp received over the Broadlink protocol.'''
        pass
    @staticmethod
    def now() -> dt.datetime:
        '''Return the current date and time with timezone info.'''
        pass",4,4,12.0,1.0,10.0,1.0,2.0,0.11,0.0,7.0,0.0,0.0,0.0,0.0,3.0,3.0,45.0,6.0,35.0,20.0,28.0,4.0,32.0,17.0,28.0,3.0,0.0,1.0,5.0,snippet_48
132032,rigetti/pyquil,pyquil/quilatom.py,pyquil.quilatom.QuilAtom,"class QuilAtom:
    """"""Abstract class for atomic elements of Quil.""""""

    def out(self) -> str:
        """"""Return the element as a valid Quil string.""""""
        raise NotImplementedError()

    def __str__(self) -> str:
        """"""Get a string representation of the element, possibly not valid Quil.""""""
        raise NotImplementedError()

    def __eq__(self, other: object) -> bool:
        """"""Return True if the other object is equal to this one.""""""
        raise NotImplementedError()

    def __hash__(self) -> int:
        """"""Return a hash of the object.""""""
        raise NotImplementedError()","class QuilAtom:
    '''Abstract class for atomic elements of Quil.'''

    def out(self) -> str:
        '''Return the element as a valid Quil string.'''
        pass

    def __str__(self) -> str:
        '''Get a string representation of the element, possibly not valid Quil.'''
        pass

    def __eq__(self, other: object) -> bool:
        '''Return True if the other object is equal to this one.'''
        pass

    def __hash__(self) -> int:
        '''Return a hash of the object.'''
        pass",5,5,3.0,0.0,2.0,1.0,1.0,0.56,0.0,5.0,0.0,9.0,4.0,0.0,4.0,4.0,18.0,4.0,9.0,5.0,4.0,5.0,9.0,5.0,4.0,1.0,0.0,0.0,4.0,snippet_49
134597,Unidata/MetPy,Unidata_MetPy/tools/flake8-metpy/flake8_metpy.py,flake8_metpy.MetPyChecker,"class MetPyChecker:
    """"""Flake8 plugin class to check MetPy style/best practice.""""""
    name = __name__
    version = '1.0'

    def __init__(self, tree):
        """"""Initialize the plugin.""""""
        self.tree = tree

    def run(self):
        """"""Run the plugin and yield errors.""""""
        visitor = MetPyVisitor()
        visitor.visit(self.tree)
        for err in visitor.errors:
            yield self.error(err)

    def error(self, err):
        """"""Format errors into Flake8's required format.""""""
        return (err.lineno, err.col, f'MPY{err.code:03d}: Multiplying/dividing by units--use units.Quantity()', type(self))","class MetPyChecker:
    '''Flake8 plugin class to check MetPy style/best practice.'''

    def __init__(self, tree):
        '''Initialize the plugin.'''
        pass

    def run(self):
        '''Run the plugin and yield errors.'''
        pass

    def error(self, err):
        '''Format errors into Flake8's required format.'''
        pass",4,4,5.0,0.0,4.0,1.0,1.0,0.29,0.0,2.0,1.0,0.0,3.0,1.0,3.0,3.0,22.0,4.0,14.0,9.0,10.0,4.0,12.0,9.0,8.0,2.0,0.0,1.0,4.0,snippet_50
134620,Unidata/MetPy,Unidata_MetPy/src/metpy/io/_tools.py,metpy.io._tools.Array,"from struct import Struct

class Array:
    """"""Use a Struct as a callable to unpack a bunch of bytes as a list.""""""

    def __init__(self, fmt):
        """"""Initialize the Struct unpacker.""""""
        self._struct = Struct(fmt)

    def __call__(self, buf):
        """"""Perform the actual unpacking.""""""
        return list(self._struct.unpack(buf))","
class Array:
    '''Use a Struct as a callable to unpack a bunch of bytes as a list.'''

    def __init__(self, fmt):
        '''Initialize the Struct unpacker.'''
        pass

    def __call__(self, buf):
        '''Perform the actual unpacking.'''
        pass",3,3,3.0,0.0,2.0,1.0,1.0,0.6,0.0,1.0,0.0,0.0,2.0,1.0,2.0,2.0,10.0,2.0,5.0,4.0,2.0,3.0,5.0,4.0,2.0,1.0,0.0,0.0,2.0,snippet_51
134621,Unidata/MetPy,Unidata_MetPy/src/metpy/io/_tools.py,metpy.io._tools.BitField,"class BitField:
    """"""Convert an integer to a string for each bit.""""""

    def __init__(self, *names):
        """"""Initialize the list of named bits.""""""
        self._names = names

    def __call__(self, val):
        """"""Return a list with a string for each True bit in the integer.""""""
        if not val:
            return None
        bits = []
        for n in self._names:
            if val & 1:
                bits.append(n)
            val >>= 1
            if not val:
                break
        return bits[0] if len(bits) == 1 else bits","class BitField:
    '''Convert an integer to a string for each bit.'''

    def __init__(self, *names):
        '''Initialize the list of named bits.'''
        pass

    def __call__(self, val):
        '''Return a list with a string for each True bit in the integer.'''
        pass",3,3,9.0,1.0,7.0,2.0,4.0,0.29,0.0,0.0,0.0,0.0,2.0,1.0,2.0,2.0,22.0,4.0,14.0,6.0,11.0,4.0,14.0,6.0,11.0,6.0,0.0,2.0,7.0,snippet_52
134622,Unidata/MetPy,Unidata_MetPy/src/metpy/io/_tools.py,metpy.io._tools.Bits,"class Bits:
    """"""Breaks an integer into a specified number of True/False bits.""""""

    def __init__(self, num_bits):
        """"""Initialize the number of bits.""""""
        self._bits = range(num_bits)

    def __call__(self, val):
        """"""Convert the integer to the list of True/False values.""""""
        return [bool(val >> i & 1) for i in self._bits]","class Bits:
    '''Breaks an integer into a specified number of True/False bits.'''

    def __init__(self, num_bits):
        '''Initialize the number of bits.'''
        pass

    def __call__(self, val):
        '''Convert the integer to the list of True/False values.'''
        pass",3,3,3.0,0.0,2.0,1.0,1.0,0.6,0.0,2.0,0.0,0.0,2.0,1.0,2.0,2.0,10.0,2.0,5.0,4.0,2.0,3.0,5.0,4.0,2.0,1.0,0.0,0.0,2.0,snippet_53
134624,Unidata/MetPy,Unidata_MetPy/src/metpy/io/_tools.py,metpy.io._tools.Enum,"class Enum:
    """"""Map values to specific strings.""""""

    def __init__(self, *args, **kwargs):
        """"""Initialize the mapping.""""""
        self.val_map = dict(enumerate(args))
        self.val_map.update(zip(kwargs.values(), kwargs.keys(), strict=False))

    def __call__(self, val):
        """"""Map an integer to the string representation.""""""
        return self.val_map.get(val, f'Unknown ({val})')","class Enum:
    '''Map values to specific strings.'''

    def __init__(self, *args, **kwargs):
        '''Initialize the mapping.'''
        pass

    def __call__(self, val):
        '''Map an integer to the string representation.'''
        pass",3,3,5.0,1.0,3.0,2.0,1.0,0.83,0.0,3.0,0.0,0.0,2.0,1.0,2.0,2.0,14.0,3.0,6.0,4.0,3.0,5.0,6.0,4.0,3.0,1.0,0.0,0.0,2.0,snippet_54
134657,Unidata/MetPy,Unidata_MetPy/src/metpy/package_tools.py,metpy.package_tools.Exporter,"class Exporter:
    """"""Manages exporting of symbols from the module.

    Grabs a reference to `globals()` for a module and provides a decorator to add
    functions and classes to `__all__` rather than requiring a separately maintained list.
    Also provides a context manager to do this for instances by adding all instances added
    within a block to `__all__`.
    """"""

    def __init__(self, globls):
        """"""Initialize the Exporter.""""""
        self.globls = globls
        self.exports = globls.setdefault('__all__', [])

    def export(self, defn):
        """"""Declare a function or class as exported.""""""
        self.exports.append(defn.__name__)
        return defn

    def __enter__(self):
        """"""Start a block tracking all instances created at global scope.""""""
        self.start_vars = set(self.globls)

    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""Exit the instance tracking block.""""""
        self.exports.extend(set(self.globls) - self.start_vars)
        del self.start_vars","class Exporter:
    '''Manages exporting of symbols from the module.
    Grabs a reference to `globals()` for a module and provides a decorator to add
    functions and classes to `__all__` rather than requiring a separately maintained list.
    Also provides a context manager to do this for instances by adding all instances added
    within a block to `__all__`.
    '''

    def __init__(self, globls):
        '''Initialize the Exporter.'''
        pass

    def export(self, defn):
        '''Declare a function or class as exported.'''
        pass

    def __enter__(self):
        '''Start a block tracking all instances created at global scope.'''
        pass

    def __exit__(self, exc_type, exc_val, exc_tb):
        '''Exit the instance tracking block.'''
        pass",5,5,4.0,0.0,3.0,1.0,1.0,0.83,0.0,1.0,0.0,0.0,4.0,3.0,4.0,4.0,27.0,5.0,12.0,8.0,7.0,10.0,12.0,8.0,7.0,1.0,0.0,0.0,4.0,snippet_55
137119,jpype-project/jpype,jpype-project_jpype/doc/java/util.py,java.util.Collection,"class Collection:
    """""" Customized interface representing a collection of items.

    JPype wraps ``java.util.Collection`` as a Python collection.
    """"""

    def __len__(self) -> int:
        """""" Get the length of this collection.

        Use ``len(collection)`` to find the number of items in this
        collection.

        """"""
        ...

    def __delitem__(self, item):
        """""" Collections do not support remove by index. """"""
        ...

    def __contains__(self, item) -> bool:
        """""" Check if this collection contains this item.

        Use ``item in collection`` to check if the item is 
        present.

        Args:
           item: is the item to check for.  This must be a Java
           object or an object which can be automatically converted
           such as a string.

        Returns:
           bool: True if the item is in the collection.
        """"""
        ...","class Collection:
    ''' Customized interface representing a collection of items.
    JPype wraps ``java.util.Collection`` as a Python collection.
    '''

    def __len__(self) -> int:
        ''' Get the length of this collection.
        Use ``len(collection)`` to find the number of items in this
        collection.
        '''
        pass

    def __delitem__(self, item):
        ''' Collections do not support remove by index. '''
        pass

    def __contains__(self, item) -> bool:
        ''' Check if this collection contains this item.
        Use ``item in collection`` to check if the item is 
        present.
        Args:
           item: is the item to check for.  This must be a Java
           object or an object which can be automatically converted
           such as a string.
        Returns:
           bool: True if the item is in the collection.
        '''
        pass",4,4,9.0,2.0,2.0,5.0,1.0,2.57,0.0,2.0,0.0,1.0,3.0,0.0,3.0,3.0,34.0,9.0,7.0,4.0,3.0,18.0,7.0,4.0,3.0,1.0,0.0,0.0,3.0,snippet_56
138700,sebp/scikit-survival,sebp_scikit-survival/sksurv/util.py,sksurv.util.Surv,"from sklearn.utils.validation import check_array, check_consistent_length
import numpy as np
import pandas as pd

class Surv:
    """"""A helper class to create a structured array for survival analysis.

    This class provides helper functions to create a structured array that
    encapsulates the event indicator and the observed time. The resulting
    structured array is the recommended format for the ``y`` argument in
    scikit-survival's estimators.
    """"""

    @staticmethod
    def from_arrays(event, time, name_event=None, name_time=None):
        """"""Create structured array from event indicator and time arrays.

        Parameters
        ----------
        event : array-like, shape=(n_samples,)
            Event indicator. A boolean array or array with values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : array-like, shape=(n_samples,)
            Observed time. Time to event or time of censoring.
        name_event : str, optional, default: 'event'
            Name of the event field in the structured array.
        name_time : str, optional, default: 'time'
            Name of the observed time field in the structured array.

        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are set to the values of `name_event` and `name_time`.

        Examples
        --------
        >>> from sksurv.util import Surv
        >>>
        >>> y = Surv.from_arrays(event=[True, False, True],
        ...                      time=[10, 25, 15])
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('event', '?'), ('time', '<f8')])
        >>> y['event']
        array([ True, False,  True])
        >>> y['time']
        array([10., 25., 15.])
        """"""
        name_event = name_event or 'event'
        name_time = name_time or 'time'
        if name_time == name_event:
            raise ValueError('name_time must be different from name_event')
        time = np.asanyarray(time, dtype=float)
        y = np.empty(time.shape[0], dtype=[(name_event, bool), (name_time, float)])
        y[name_time] = time
        event = np.asanyarray(event)
        check_consistent_length(time, event)
        if np.issubdtype(event.dtype, np.bool_):
            y[name_event] = event
        else:
            events = np.unique(event)
            events.sort()
            if len(events) != 2:
                raise ValueError('event indicator must be binary')
            if np.all(events == np.array([0, 1], dtype=events.dtype)):
                y[name_event] = event.astype(bool)
            else:
                raise ValueError('non-boolean event indicator must contain 0 and 1 only')
        return y

    @staticmethod
    def from_dataframe(event, time, data):
        """"""Create structured array from columns in a pandas DataFrame.

        Parameters
        ----------
        event : str
            Name of the column in ``data`` containing the event indicator.
            It must be a boolean or have values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : str
            Name of the column in ``data`` containing the observed time
            (time to event or time of censoring).
        data : pandas.DataFrame
            A DataFrame with columns for event and time.

        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are the respective column names.

        Examples
        --------
        >>> import pandas as pd
        >>> from sksurv.util import Surv
        >>>
        >>> df = pd.DataFrame({
        ...     'status': [True, False, True],
        ...     'followup_time': [10, 25, 15],
        ... })
        >>> y = Surv.from_dataframe(
        ...     event='status', time='followup_time', data=df,
        ... )
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('status', '?'), ('followup_time', '<f8')])
        >>> y['status']
        array([ True, False,  True])
        >>> y['followup_time']
        array([10., 25., 15.])
        """"""
        if not isinstance(data, pd.DataFrame):
            raise TypeError(f'expected pandas.DataFrame, but got {type(data)!r}')
        return Surv.from_arrays(data.loc[:, event].values, data.loc[:, time].values, name_event=str(event), name_time=str(time))","
class Surv:
    '''A helper class to create a structured array for survival analysis.
    This class provides helper functions to create a structured array that
    encapsulates the event indicator and the observed time. The resulting
    structured array is the recommended format for the ``y`` argument in
    scikit-survival's estimators.
    '''
    @staticmethod
    def from_arrays(event, time, name_event=None, name_time=None):
        '''Create structured array from event indicator and time arrays.
        Parameters
        ----------
        event : array-like, shape=(n_samples,)
            Event indicator. A boolean array or array with values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : array-like, shape=(n_samples,)
            Observed time. Time to event or time of censoring.
        name_event : str, optional, default: 'event'
            Name of the event field in the structured array.
        name_time : str, optional, default: 'time'
            Name of the observed time field in the structured array.
        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are set to the values of `name_event` and `name_time`.
        Examples
        --------
        >>> from sksurv.util import Surv
        >>>
        >>> y = Surv.from_arrays(event=[True, False, True],
        ...                      time=[10, 25, 15])
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('event', '?'), ('time', '<f8')])
        >>> y['event']
        array([ True, False,  True])
        >>> y['time']
        array([10., 25., 15.])
        '''
        pass
    @staticmethod
    def from_dataframe(event, time, data):
        '''Create structured array from columns in a pandas DataFrame.
        Parameters
        ----------
        event : str
            Name of the column in ``data`` containing the event indicator.
            It must be a boolean or have values 0/1,
            where ``True`` or 1 indicates an event and ``False`` or 0
            indicates right-censoring.
        time : str
            Name of the column in ``data`` containing the observed time
            (time to event or time of censoring).
        data : pandas.DataFrame
            A DataFrame with columns for event and time.
        Returns
        -------
        y : numpy.ndarray
            A structured array with two fields. The first field is a boolean
            where ``True`` indicates an event and ``False`` indicates right-censoring.
            The second field is a float with the time of event or time of censoring.
            The names of the fields are the respective column names.
        Examples
        --------
        >>> import pandas as pd
        >>> from sksurv.util import Surv
        >>>
        >>> df = pd.DataFrame({
        ...     'status': [True, False, True],
        ...     'followup_time': [10, 25, 15],
        ... })
        >>> y = Surv.from_dataframe(
        ...     event='status', time='followup_time', data=df,
        ... )
        >>> y
        array([( True, 10.), (False, 25.), ( True, 15.)],
            dtype=[('status', '?'), ('followup_time', '<f8')])
        >>> y['status']
        array([ True, False,  True])
        >>> y['followup_time']
        array([10., 25., 15.])
        '''
        pass",3,3,34.0,5.0,14.0,15.0,4.0,1.06,0.0,6.0,0.0,0.0,0.0,0.0,2.0,2.0,76.0,12.0,31.0,7.0,26.0,33.0,25.0,5.0,22.0,5.0,0.0,2.0,7.0,snippet_57
139652,sentinel-hub/eo-learn,sentinel-hub_eo-learn/eolearn/geometry/morphology.py,eolearn.geometry.morphology.MorphologicalStructFactory,"import cv2
import numpy as np

class MorphologicalStructFactory:
    """"""
    Factory methods for generating morphological structuring elements
    """"""

    @staticmethod
    def get_disk(radius: int) -> np.ndarray:
        """"""
        :param radius: Radius of disk
        :return: The structuring element where elements of the neighborhood are 1 and 0 otherwise.
        """"""
        return cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (radius, radius))

    @staticmethod
    def get_rectangle(width: int, height: int) -> np.ndarray:
        """"""
        :param width: Width of rectangle
        :param height: Height of rectangle
        :return: A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.
        """"""
        return cv2.getStructuringElement(cv2.MORPH_RECT, (height, width))

    @staticmethod
    def get_square(width: int) -> np.ndarray:
        """"""
        :param width: Size of square
        :return: A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.
        """"""
        return cv2.getStructuringElement(cv2.MORPH_RECT, (width, width))","
class MorphologicalStructFactory:
    '''
    Factory methods for generating morphological structuring elements
    '''
    @staticmethod
    def get_disk(radius: int) -> np.ndarray:
        '''
        :param radius: Radius of disk
        :return: The structuring element where elements of the neighborhood are 1 and 0 otherwise.
        '''
        pass
    @staticmethod
    def get_rectangle(width: int, height: int) -> np.ndarray:
        '''
        :param width: Width of rectangle
        :param height: Height of rectangle
        :return: A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.
        '''
        pass
    @staticmethod
    def get_square(width: int) -> np.ndarray:
        '''
        :param width: Size of square
        :return: A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.
        '''
        pass",4,4,6.0,0.0,2.0,4.0,1.0,1.6,0.0,1.0,0.0,0.0,0.0,0.0,3.0,3.0,29.0,3.0,10.0,7.0,3.0,16.0,7.0,4.0,3.0,1.0,0.0,0.0,3.0,snippet_58
139672,sentinel-hub/eo-learn,sentinel-hub_eo-learn/eolearn/visualization/eoexecutor.py,eolearn.visualization.eoexecutor._ErrorSummary,"from dataclasses import dataclass

@dataclass()
class _ErrorSummary:
    """"""Contains data for errors of a node.""""""
    origin: str
    example_message: str
    failed_indexed_executions: list[tuple[int, str]]

    def add_execution(self, index: int, name: str) -> None:
        """"""Adds an execution to the summary.""""""
        self.failed_indexed_executions.append((index, name))

    @property
    def num_failed(self) -> int:
        """"""Helps with jinja""""""
        return len(self.failed_indexed_executions)","@dataclass()
class _ErrorSummary:
    '''Contains data for errors of a node.'''

    def add_execution(self, index: int, name: str) -> None:
        '''Adds an execution to the summary.'''
        pass
    @property
    def num_failed(self) -> int:
        '''Helps with jinja'''
        pass",3,3,3.0,0.0,2.0,1.0,1.0,0.33,0.0,2.0,0.0,0.0,2.0,0.0,2.0,2.0,15.0,3.0,9.0,4.0,5.0,3.0,8.0,3.0,5.0,1.0,0.0,0.0,2.0,snippet_59
139835,python-escpos/python-escpos,python-escpos_python-escpos/src/escpos/codepages.py,escpos.codepages.CodePageManager,"class CodePageManager:
    """"""Holds information about all the code pages.

    Information as defined in escpos-printer-db.
    """"""

    def __init__(self, data):
        """"""Initialize code page manager.""""""
        self.data = data

    @staticmethod
    def get_encoding_name(encoding):
        """"""Get encoding name.

        .. todo:: Resolve the encoding alias.
        """"""
        return encoding.upper()

    def get_encoding(self, encoding):
        """"""Return the encoding data.""""""
        return self.data[encoding]","class CodePageManager:
    '''Holds information about all the code pages.
    Information as defined in escpos-printer-db.
        '''

    def __init__(self, data):
        '''Initialize code page manager.'''
        pass
    @staticmethod
    def get_encoding_name(encoding):
        '''Get encoding name.
        .. todo:: Resolve the encoding alias.
        '''
        pass
    def get_encoding_name(encoding):
        '''Return the encoding data.'''
        pass",4,4,4.0,0.0,2.0,2.0,1.0,1.0,0.0,0.0,0.0,0.0,2.0,1.0,3.0,3.0,21.0,5.0,8.0,6.0,3.0,8.0,7.0,5.0,3.0,1.0,0.0,0.0,3.0,snippet_60
140450,google/budou,google_budou/budou/cachefactory.py,budou.cachefactory.BudouCache,"import six
from abc import ABCMeta, abstractmethod

@six.add_metaclass(ABCMeta)
class BudouCache:
    """"""Base class for cache system.
    """"""

    @abstractmethod
    def get(self, key):
        """"""Abstract method: Gets a value by a key.

        Args:
          key (str): Key to retrieve the value.

        Returns:
          Retrieved value (str or None).

        Raises:
          NotImplementedError: If it's not implemented.
        """"""
        raise NotImplementedError()

    @abstractmethod
    def set(self, key, val):
        """"""Abstract method: Sets a value in a key.

        Args:
          key (str): Key for the value.
          val (str): Value to set.

        Raises:
          NotImplementedError: If it's not implemented.
        """"""
        raise NotImplementedError()","@six.add_metaclass(ABCMeta)
class BudouCache:
    '''Base class for cache system.
        '''
    @abstractmethod
    def get(self, key):
        '''Abstract method: Gets a value by a key.
        Args:
          key (str): Key to retrieve the value.
        Returns:
          Retrieved value (str or None).
        Raises:
          NotImplementedError: If it's not implemented.
        '''
        pass
    @abstractmethod
    def set(self, key, val):
        '''Abstract method: Sets a value in a key.
        Args:
          key (str): Key for the value.
          val (str): Value to set.
        Raises:
          NotImplementedError: If it's not implemented.
        '''
        pass",3,3,12.0,3.0,2.0,8.0,1.0,2.43,0.0,1.0,0.0,2.0,2.0,0.0,2.0,2.0,31.0,7.0,7.0,5.0,2.0,17.0,5.0,3.0,2.0,1.0,0.0,0.0,2.0,snippet_61
141706,mar10/wsgidav,mar10_wsgidav/wsgidav/rw_lock.py,wsgidav.rw_lock.ReadWriteLock,"from threading import Condition, Lock, current_thread
from time import time

class ReadWriteLock:
    """"""Read-Write lock class. A read-write lock differs from a standard
    threading.RLock() by allowing multiple threads to simultaneously hold a
    read lock, while allowing only a single thread to hold a write lock at the
    same point of time.

    When a read lock is requested while a write lock is held, the reader
    is blocked; when a write lock is requested while another write lock is
    held or there are read locks, the writer is blocked.

    Writers are always preferred by this implementation: if there are blocked
    threads waiting for a write lock, current readers may request more read
    locks (which they eventually should free, as they starve the waiting
    writers otherwise), but a new thread requesting a read lock will not
    be granted one, and block. This might mean starvation for readers if
    two writer threads interweave their calls to acquire_write() without
    leaving a window only for readers.

    In case a current reader requests a write lock, this can and will be
    satisfied without giving up the read locks first, but, only one thread
    may perform this kind of lock upgrade, as a deadlock would otherwise
    occur. After the write lock has been granted, the thread will hold a
    full write lock, and not be downgraded after the upgrading call to
    acquire_write() has been match by a corresponding release().
    """"""

    def __init__(self):
        """"""Initialize this read-write lock.""""""
        self.__condition = Condition(Lock())
        self.__writer = None
        self.__upgradewritercount = 0
        self.__pendingwriters = []
        self.__readers = {}

    def acquire_read(self, *, timeout=None):
        """"""Acquire a read lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.

        In case timeout is None, the call to acquire_read blocks until the
        lock request can be serviced.

        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.""""""
        if timeout is not None:
            endtime = time() + timeout
        me = current_thread()
        self.__condition.acquire()
        try:
            if self.__writer is me:
                self.__writercount += 1
                return
            while True:
                if self.__writer is None:
                    if self.__upgradewritercount or self.__pendingwriters:
                        if me in self.__readers:
                            self.__readers[me] += 1
                            return
                    else:
                        self.__readers[me] = self.__readers.get(me, 0) + 1
                        return
                if timeout is not None:
                    remaining = endtime - time()
                    if remaining <= 0:
                        raise RuntimeError('Acquiring read lock timed out')
                    self.__condition.wait(remaining)
                else:
                    self.__condition.wait()
        finally:
            self.__condition.release()

    def acquire_write(self, *, timeout=None):
        """"""Acquire a write lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.

        In case the write lock cannot be serviced due to the deadlock
        condition mentioned above, a ValueError is raised.

        In case timeout is None, the call to acquire_write blocks until the
        lock request can be serviced.

        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.""""""
        if timeout is not None:
            endtime = time() + timeout
        me, upgradewriter = (current_thread(), False)
        self.__condition.acquire()
        try:
            if self.__writer is me:
                self.__writercount += 1
                return
            elif me in self.__readers:
                if self.__upgradewritercount:
                    raise ValueError('Inevitable dead lock, denying write lock')
                upgradewriter = True
                self.__upgradewritercount = self.__readers.pop(me)
            else:
                self.__pendingwriters.append(me)
            while True:
                if not self.__readers and self.__writer is None:
                    if self.__upgradewritercount:
                        if upgradewriter:
                            self.__writer = me
                            self.__writercount = self.__upgradewritercount + 1
                            self.__upgradewritercount = 0
                            return
                    elif self.__pendingwriters[0] is me:
                        self.__writer = me
                        self.__writercount = 1
                        self.__pendingwriters = self.__pendingwriters[1:]
                        return
                if timeout is not None:
                    remaining = endtime - time()
                    if remaining <= 0:
                        if upgradewriter:
                            self.__readers[me] = self.__upgradewritercount
                            self.__upgradewritercount = 0
                        else:
                            self.__pendingwriters.remove(me)
                        raise RuntimeError('Acquiring write lock timed out')
                    self.__condition.wait(remaining)
                else:
                    self.__condition.wait()
        finally:
            self.__condition.release()

    def release(self):
        """"""Release the currently held lock.

        In case the current thread holds no lock, a ValueError is thrown.""""""
        me = current_thread()
        self.__condition.acquire()
        try:
            if self.__writer is me:
                self.__writercount -= 1
                if not self.__writercount:
                    self.__writer = None
                    self.__condition.notify_all()
            elif me in self.__readers:
                self.__readers[me] -= 1
                if not self.__readers[me]:
                    del self.__readers[me]
                    if not self.__readers:
                        self.__condition.notify_all()
            else:
                raise ValueError('Trying to release unheld lock')
        finally:
            self.__condition.release()","
class ReadWriteLock:
    '''Read-Write lock class. A read-write lock differs from a standard
    threading.RLock() by allowing multiple threads to simultaneously hold a
    read lock, while allowing only a single thread to hold a write lock at the
    same point of time.
    When a read lock is requested while a write lock is held, the reader
    is blocked; when a write lock is requested while another write lock is
    held or there are read locks, the writer is blocked.
    Writers are always preferred by this implementation: if there are blocked
    threads waiting for a write lock, current readers may request more read
    locks (which they eventually should free, as they starve the waiting
    writers otherwise), but a new thread requesting a read lock will not
    be granted one, and block. This might mean starvation for readers if
    two writer threads interweave their calls to acquire_write() without
    leaving a window only for readers.
    In case a current reader requests a write lock, this can and will be
    satisfied without giving up the read locks first, but, only one thread
    may perform this kind of lock upgrade, as a deadlock would otherwise
    occur. After the write lock has been granted, the thread will hold a
    full write lock, and not be downgraded after the upgrading call to
    acquire_write() has been match by a corresponding release().
    '''

    def __init__(self):
        '''Initialize this read-write lock.'''
        pass

    def acquire_read(self, *, timeout=None):
        '''Acquire a read lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.
        In case timeout is None, the call to acquire_read blocks until the
        lock request can be serviced.
        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.'''
        pass

    def acquire_write(self, *, timeout=None):
        '''Acquire a write lock for the current thread, waiting at most
        timeout seconds or doing a non-blocking check in case timeout is <= 0.
        In case the write lock cannot be serviced due to the deadlock
        condition mentioned above, a ValueError is raised.
        In case timeout is None, the call to acquire_write blocks until the
        lock request can be serviced.
        In case the timeout expires before the lock could be serviced, a
        RuntimeError is thrown.'''
        pass

    def release(self):
        '''Release the currently held lock.
        In case the current thread holds no lock, a ValueError is thrown.'''
        pass",5,5,44.0,3.0,24.0,17.0,7.0,0.92,0.0,3.0,0.0,0.0,4.0,6.0,4.0,4.0,203.0,19.0,96.0,18.0,91.0,88.0,84.0,18.0,79.0,13.0,0.0,5.0,29.0,snippet_62
141733,mar10/wsgidav,mar10_wsgidav/wsgidav/stream_tools.py,wsgidav.stream_tools.StreamingFile,"class StreamingFile:
    """"""A file object wrapped around an iterator / data stream.""""""

    def __init__(self, data_stream):
        """"""Initialise the object with the data stream.""""""
        self.data_stream = data_stream
        self.buffer = ''

    def read(self, size=None):
        """"""Read bytes from an iterator.""""""
        while size is None or len(self.buffer) < size:
            try:
                self.buffer += next(self.data_stream)
            except StopIteration:
                break
        sized_chunk = self.buffer[:size]
        if size is None:
            self.buffer = ''
        else:
            self.buffer = self.buffer[size:]
        return sized_chunk","class StreamingFile:
    '''A file object wrapped around an iterator / data stream.'''

    def __init__(self, data_stream):
        '''Initialise the object with the data stream.'''
        pass

    def read(self, size=None):
        '''Read bytes from an iterator.'''
        pass",3,3,9.0,1.0,8.0,1.0,3.0,0.19,0.0,1.0,0.0,0.0,2.0,2.0,2.0,2.0,22.0,3.0,16.0,6.0,13.0,3.0,15.0,6.0,12.0,4.0,0.0,2.0,5.0,snippet_63
142723,PyThaiNLP/pythainlp,PyThaiNLP_pythainlp/pythainlp/augment/lm/fasttext.py,pythainlp.augment.lm.fasttext.FastTextAug,"from gensim.models.fasttext import FastText as FastText_gensim
from typing import List, Tuple
import itertools
from gensim.models.keyedvectors import KeyedVectors
from pythainlp.tokenize import word_tokenize

class FastTextAug:
    """"""
    Text Augment from fastText

    :param str model_path: path of model file
    """"""

    def __init__(self, model_path: str):
        """"""
        :param str model_path: path of model file
        """"""
        if model_path.endswith('.bin'):
            self.model = FastText_gensim.load_facebook_vectors(model_path)
        elif model_path.endswith('.vec'):
            self.model = KeyedVectors.load_word2vec_format(model_path)
        else:
            self.model = FastText_gensim.load(model_path)
        self.dict_wv = list(self.model.key_to_index.keys())

    def tokenize(self, text: str) -> List[str]:
        """"""
        Thai text tokenization for fastText

        :param str text: Thai text

        :return: list of words
        :rtype: List[str]
        """"""
        return word_tokenize(text, engine='icu')

    def modify_sent(self, sent: str, p: float=0.7) -> List[List[str]]:
        """"""
        :param str sent: text of sentence
        :param float p: probability
        :rtype: List[List[str]]
        """"""
        list_sent_new = []
        for i in sent:
            if i in self.dict_wv:
                w = [j for j, v in self.model.most_similar(i) if v >= p]
                if w == []:
                    list_sent_new.append([i])
                else:
                    list_sent_new.append(w)
            else:
                list_sent_new.append([i])
        return list_sent_new

    def augment(self, sentence: str, n_sent: int=1, p: float=0.7) -> List[Tuple[str]]:
        """"""
        Text Augment from fastText

        You may want to download the Thai model
        from https://fasttext.cc/docs/en/crawl-vectors.html.

        :param str sentence: Thai sentence
        :param int n_sent: number of sentences
        :param float p: probability of word

        :return: list of synonyms
        :rtype: List[Tuple[str]]
        """"""
        self.sentence = self.tokenize(sentence)
        self.list_synonym = self.modify_sent(self.sentence, p=p)
        new_sentences = []
        for x in list(itertools.product(*self.list_synonym))[0:n_sent]:
            new_sentences.append(x)
        return new_sentences","
class FastTextAug:
    '''
    Text Augment from fastText
    :param str model_path: path of model file
    '''

    def __init__(self, model_path: str):
        '''
        :param str model_path: path of model file
        '''
        pass

    def tokenize(self, text: str) -> List[str]:
        '''
        Thai text tokenization for fastText
        :param str text: Thai text
        :return: list of words
        :rtype: List[str]
        '''
        pass

    def modify_sent(self, sent: str, p: float=0.7) -> List[List[str]]:
        '''
        :param str sent: text of sentence
        :param float p: probability
        :rtype: List[List[str]]
        '''
        pass

    def augment(self, sentence: str, n_sent: int=1, p: float=0.7) -> List[Tuple[str]]:
        '''
        Text Augment from fastText
        You may want to download the Thai model
        from https://fasttext.cc/docs/en/crawl-vectors.html.
        :param str sentence: Thai sentence
        :param int n_sent: number of sentences
        :param float p: probability of word
        :return: list of synonyms
        :rtype: List[Tuple[str]]
        '''
        pass",5,5,15.0,1.0,8.0,6.0,3.0,0.88,0.0,5.0,0.0,0.0,4.0,4.0,4.0,4.0,70.0,10.0,32.0,16.0,25.0,28.0,26.0,14.0,21.0,4.0,0.0,3.0,10.0,snippet_64
142777,PyThaiNLP/pythainlp,PyThaiNLP_pythainlp/pythainlp/tokenize/core.py,pythainlp.tokenize.core.Tokenizer,"from typing import Iterable, List, Union
from pythainlp.tokenize import DEFAULT_SENT_TOKENIZE_ENGINE, DEFAULT_SUBWORD_TOKENIZE_ENGINE, DEFAULT_SYLLABLE_DICT_TRIE, DEFAULT_SYLLABLE_TOKENIZE_ENGINE, DEFAULT_WORD_DICT_TRIE, DEFAULT_WORD_TOKENIZE_ENGINE
from pythainlp.util.trie import Trie, dict_trie

class Tokenizer:
    """"""
    Tokenizer class for a custom tokenizer.

    This class allows users to pre-define custom dictionary along with
    tokenizer and encapsulate them into one single object.
    It is an wrapper for both functions, that are
    :func:`pythainlp.tokenize.word_tokenize`,
    and :func:`pythainlp.util.dict_trie`

    :Example:

    Tokenizer object instantiated with :class:`pythainlp.util.Trie`::

        from pythainlp.tokenize import Tokenizer
        from pythainlp.corpus.common import thai_words
        from pythainlp.util import dict_trie

        custom_words_list = set(thai_words())
        custom_words_list.add('อะเฟเซีย')
        custom_words_list.add('Aphasia')
        trie = dict_trie(dict_source=custom_words_list)

        text = ""อะเฟเซีย (Aphasia*) เป็นอาการผิดปกติของการพูด""
        _tokenizer = Tokenizer(custom_dict=trie, engine='newmm')
        _tokenizer.word_tokenize(text)
        # output: ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ',
        'ผิดปกติ', 'ของ', 'การ', 'พูด']

    Tokenizer object instantiated with a list of words::

        text = ""อะเฟเซีย (Aphasia) เป็นอาการผิดปกติของการพูด""
        _tokenizer = Tokenizer(custom_dict=list(thai_words()), engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะ', 'เฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ',
        #   'ผิดปกติ', 'ของ', 'การ', 'พูด']

    Tokenizer object instantiated with a file path containing a list of
    words separated with *newline* and explicitly setting a new tokenizer
    after initiation::

        PATH_TO_CUSTOM_DICTIONARY = './custom_dictionary.txtt'

        # write a file
        with open(PATH_TO_CUSTOM_DICTIONARY, 'w', encoding='utf-8') as f:
            f.write('อะเฟเซีย\\nAphasia\\nผิด\\nปกติ')

        text = ""อะเฟเซีย (Aphasia) เป็นอาการผิดปกติของการพูด""

        # initiate an object from file with `attacut` as tokenizer
        _tokenizer = Tokenizer(custom_dict=PATH_TO_CUSTOM_DICTIONARY, \\
            engine='attacut')

        _tokenizer.word_tokenize(text)
        # output:
        # ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ', 'ผิด',
        #   'ปกติ', 'ของ', 'การ', 'พูด']

        # change tokenizer to `newmm`
        _tokenizer.set_tokenizer_engine(engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็นอาการ', 'ผิด',
        #   'ปกติ', 'ของการพูด']
    """"""

    def __init__(self, custom_dict: Union[Trie, Iterable[str], str]=[], engine: str='newmm', keep_whitespace: bool=True, join_broken_num: bool=True):
        """"""
        Initialize tokenizer object.

        :param str custom_dict: a file path, a list of vocaburaies* to be
                    used to create a trie, or an instantiated
                    :class:`pythainlp.util.Trie` object.
        :param str engine: choose between different options of tokenizer engines
                            (i.e.  *newmm*, *mm*, *longest*, *deepcut*)
        :param bool keep_whitespace: True to keep whitespace, a common mark
                                     for end of phrase in Thai
        """"""
        self.__trie_dict = Trie([])
        if custom_dict:
            self.__trie_dict = dict_trie(custom_dict)
        else:
            self.__trie_dict = DEFAULT_WORD_DICT_TRIE
        self.__engine = engine
        if self.__engine not in ['newmm', 'mm', 'longest', 'deepcut']:
            raise NotImplementedError('\n                The Tokenizer class is not support %s for custom tokenizer\n                ' % self.__engine)
        self.__keep_whitespace = keep_whitespace
        self.__join_broken_num = join_broken_num

    def word_tokenize(self, text: str) -> List[str]:
        """"""
        Main tokenization function.

        :param str text: text to be tokenized
        :return: list of words, tokenized from the text
        :rtype: list[str]
        """"""
        return word_tokenize(text, custom_dict=self.__trie_dict, engine=self.__engine, keep_whitespace=self.__keep_whitespace, join_broken_num=self.__join_broken_num)

    def set_tokenize_engine(self, engine: str) -> None:
        """"""
        Set the tokenizer's engine.

        :param str engine: choose between different options of tokenizer engines
                           (i.e. *newmm*, *mm*, *longest*, *deepcut*)
        """"""
        self.__engine = engine","
class Tokenizer:
    '''
    Tokenizer class for a custom tokenizer.
    This class allows users to pre-define custom dictionary along with
    tokenizer and encapsulate them into one single object.
    It is an wrapper for both functions, that are
    :func:`pythainlp.tokenize.word_tokenize`,
    and :func:`pythainlp.util.dict_trie`
    :Example:
    Tokenizer object instantiated with :class:`pythainlp.util.Trie`::
        from pythainlp.tokenize import Tokenizer
        from pythainlp.corpus.common import thai_words
        from pythainlp.util import dict_trie
        custom_words_list = set(thai_words())
        custom_words_list.add('อะเฟเซีย')
        custom_words_list.add('Aphasia')
        trie = dict_trie(dict_source=custom_words_list)
        text = ""อะเฟเซีย (Aphasia*) เป็นอาการผิดปกติของการพูด""
        _tokenizer = Tokenizer(custom_dict=trie, engine='newmm')
        _tokenizer.word_tokenize(text)
        # output: ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ',
        'ผิดปกติ', 'ของ', 'การ', 'พูด']
    Tokenizer object instantiated with a list of words::
        text = ""อะเฟเซีย (Aphasia) เป็นอาการผิดปกติของการพูด""
        _tokenizer = Tokenizer(custom_dict=list(thai_words()), engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะ', 'เฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ',
        #   'ผิดปกติ', 'ของ', 'การ', 'พูด']
    Tokenizer object instantiated with a file path containing a list of
    words separated with *newline* and explicitly setting a new tokenizer
    after initiation::
        PATH_TO_CUSTOM_DICTIONARY = './custom_dictionary.txtt'
        # write a file
        with open(PATH_TO_CUSTOM_DICTIONARY, 'w', encoding='utf-8') as f:
            f.write('อะเฟเซีย\nAphasia\nผิด\nปกติ')
        text = ""อะเฟเซีย (Aphasia) เป็นอาการผิดปกติของการพูด""
        # initiate an object from file with `attacut` as tokenizer
        _tokenizer = Tokenizer(custom_dict=PATH_TO_CUSTOM_DICTIONARY, \
            engine='attacut')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็น', 'อาการ', 'ผิด',
        #   'ปกติ', 'ของ', 'การ', 'พูด']
        # change tokenizer to `newmm`
        _tokenizer.set_tokenizer_engine(engine='newmm')
        _tokenizer.word_tokenize(text)
        # output:
        # ['อะเฟเซีย', ' ', '(', 'Aphasia', ')', ' ', 'เป็นอาการ', 'ผิด',
        #   'ปกติ', 'ของการพูด']
    '''

    def __init__(self, custom_dict: Union[Trie, Iterable[str], str]=[], engine: str='newmm', keep_whitespace: bool=True, join_broken_num: bool=True):
        '''
        Initialize tokenizer object.
        :param str custom_dict: a file path, a list of vocaburaies* to be
                    used to create a trie, or an instantiated
                    :class:`pythainlp.util.Trie` object.
        :param str engine: choose between different options of tokenizer engines
                            (i.e.  *newmm*, *mm*, *longest*, *deepcut*)
        :param bool keep_whitespace: True to keep whitespace, a common mark
                                     for end of phrase in Thai
        '''
        pass

    def word_tokenize(self, text: str) -> List[str]:
        '''
        Main tokenization function.
        :param str text: text to be tokenized
        :return: list of words, tokenized from the text
        :rtype: list[str]
        '''
        pass

    def set_tokenize_engine(self, engine: str) -> None:
        '''
        Set the tokenizer's engine.
        :param str engine: choose between different options of tokenizer engines
                           (i.e. *newmm*, *mm*, *longest*, *deepcut*)
        '''
        pass",4,4,19.0,1.0,11.0,7.0,2.0,2.15,0.0,4.0,1.0,0.0,3.0,4.0,3.0,3.0,125.0,21.0,33.0,14.0,23.0,71.0,15.0,8.0,11.0,3.0,0.0,1.0,5.0,snippet_65
145446,TomasTomecek/sen,TomasTomecek_sen/sen/tui/commands/base.py,sen.tui.commands.base.ArgumentProcessor,"class ArgumentProcessor:
    """"""
    responsible for parsing given list of arguments
    """"""

    def __init__(self, options, arguments):
        """"""
        :param options: list of options
        :param arguments: list of arguments
        """"""
        self.given_arguments = {}
        self.options = {}
        for a in options:
            self.options[a.name] = a
            self.given_arguments[normalize_arg_name(a.name)] = a.default
            for alias in a.aliases:
                self.options[alias] = a
        for o in arguments:
            self.given_arguments[normalize_arg_name(o.name)] = o.default
        self.arguments = arguments
        logger.info('arguments = %s', arguments)
        logger.info('options = %s', options)

    def process(self, argument_list):
        """"""
        :param argument_list: list of str, input from user
        :return: dict:
            {""cleaned_arg_name"": ""value""}
        """"""
        arg_index = 0
        for a in argument_list:
            opt_and_val = a.split('=', 1)
            opt_name = opt_and_val[0]
            try:
                argument = self.options[opt_name]
            except KeyError:
                try:
                    argument = self.arguments[arg_index]
                except IndexError:
                    logger.error('option/argument %r not specified', a)
                    raise NoSuchOptionOrArgument('No such option or argument: %r' % opt_name)
            logger.info('argument found: %s', argument)
            safe_arg_name = normalize_arg_name(argument.name)
            logger.info('argument is available under name %r', safe_arg_name)
            if isinstance(argument, Argument):
                arg_index += 1
                value = (a,)
            else:
                try:
                    value = (opt_and_val[1],)
                except IndexError:
                    value = tuple()
            arg_val = argument.action(*value)
            logger.info('argument %r has value %r', safe_arg_name, arg_val)
            self.given_arguments[safe_arg_name] = arg_val
        return self.given_arguments","class ArgumentProcessor:
    '''
    responsible for parsing given list of arguments
    '''

    def __init__(self, options, arguments):
        '''
        :param options: list of options
        :param arguments: list of arguments
        '''
        pass

    def process(self, argument_list):
        '''
        :param argument_list: list of str, input from user
        :return: dict:
            {""cleaned_arg_name"": ""value""}
        '''
        pass",3,3,29.0,3.0,21.0,6.0,5.0,0.36,0.0,5.0,2.0,0.0,2.0,3.0,2.0,2.0,62.0,6.0,42.0,17.0,39.0,15.0,41.0,17.0,38.0,6.0,0.0,3.0,10.0,snippet_66
147940,minio/minio-py,minio_minio-py/minio/sse.py,minio.sse.Sse,"from abc import ABC, abstractmethod

class Sse(ABC):
    """"""Server-side encryption base class.""""""

    @abstractmethod
    def headers(self) -> dict[str, str]:
        """"""Return headers.""""""

    def tls_required(self) -> bool:
        """"""Return TLS required to use this server-side encryption.""""""
        return True

    def copy_headers(self) -> dict[str, str]:
        """"""Return copy headers.""""""
        return {}","
class Sse(ABC):
    '''Server-side encryption base class.'''
    @abstractmethod
    def headers(self) -> dict[str, str]:
        '''Return headers.'''
        pass

    def tls_required(self) -> bool:
        '''Return TLS required to use this server-side encryption.'''
        pass

    def copy_headers(self) -> dict[str, str]:
        '''Return copy headers.'''
        pass",4,4,3.0,0.0,2.0,2.0,1.0,0.75,0.0,3.0,0.0,3.0,3.0,0.0,3.0,3.0,15.0,3.0,8.0,6.0,3.0,6.0,7.0,5.0,3.0,1.0,0.0,0.0,3.0,snippet_67
149753,airspeed-velocity/asv,airspeed-velocity_asv/asv/profiling.py,asv.profiling.ProfilerGui,"class ProfilerGui:
    """"""
    A base class to define a Profiler GUI that is available through
    the ``asv profile`` command.
    """"""
    name = None
    description = None

    @classmethod
    def is_available(cls):
        """"""
        Return `True` is the given GUI tool appears to be installed
        and available.
        """"""
        raise NotImplementedError()

    @classmethod
    def open_profiler_gui(cls, profiler_file):
        """"""
        Open the profiler GUI to display the results in the given
        profiler file.
        """"""
        raise NotImplementedError()","class ProfilerGui:
    '''
    A base class to define a Profiler GUI that is available through
    the ``asv profile`` command.
    '''
    @classmethod
    def is_available(cls):
        '''
        Return `True` is the given GUI tool appears to be installed
        and available.
        '''
        pass
    @classmethod
    def open_profiler_gui(cls, profiler_file):
        '''
        Open the profiler GUI to display the results in the given
        profiler file.
        '''
        pass",3,3,6.0,0.0,2.0,4.0,1.0,1.33,0.0,1.0,0.0,3.0,0.0,0.0,2.0,2.0,23.0,2.0,9.0,7.0,4.0,12.0,7.0,5.0,4.0,1.0,0.0,0.0,2.0,snippet_68
152154,hellock/icrawler,hellock_icrawler/icrawler/storage/base.py,icrawler.storage.base.BaseStorage,"from abc import ABCMeta, abstractmethod

class BaseStorage:
    """"""Base class of backend storage""""""
    __metaclass__ = ABCMeta

    @abstractmethod
    def write(self, id, data):
        """"""Abstract interface of writing data

        Args:
            id (str): unique id of the data in the storage.
            data (bytes or str): data to be stored.
        """"""
        return

    @abstractmethod
    def exists(self, id):
        """"""Check the existence of some data

        Args:
            id (str): unique id of the data in the storage

        Returns:
            bool: whether the data exists
        """"""
        return False

    @abstractmethod
    def max_file_idx(self):
        """"""Get the max existing file index

        Returns:
            int: the max index
        """"""
        return 0","
class BaseStorage:
    '''Base class of backend storage'''
    @abstractmethod
    def write(self, id, data):
        '''Abstract interface of writing data
        Args:
            id (str): unique id of the data in the storage.
            data (bytes or str): data to be stored.
        '''
        pass
    @abstractmethod
    def exists(self, id):
        '''Check the existence of some data
        Args:
            id (str): unique id of the data in the storage
        Returns:
            bool: whether the data exists
        '''
        pass
    @abstractmethod
    def max_file_idx(self):
        '''Get the max existing file index
        Returns:
            int: the max index
        '''
        pass",4,4,8.0,1.0,2.0,5.0,1.0,1.45,0.0,0.0,0.0,2.0,3.0,0.0,3.0,3.0,35.0,8.0,11.0,8.0,4.0,16.0,8.0,5.0,4.0,1.0,0.0,0.0,3.0,snippet_69
152232,FPGAwars/apio,apio/managers/downloader.py,apio.managers.downloader.FileDownloader,"from math import ceil
from apio.utils import util
from apio.common.apio_styles import ERROR
from rich.progress import track
from apio.common.apio_console import cout, console
import requests

class FileDownloader:
    """"""Class for downloading files""""""
    CHUNK_SIZE = 1024

    def __init__(self, url: str, dest_dir=None):
        """"""Initialize a FileDownloader object
        * INPUTs:
          * url: File to download (full url)
                 (Ex. 'https://github.com/FPGAwars/apio-examples/
                       releases/download/0.0.35/apio-examples-0.0.35.zip')
          * dest_dir: Destination folder (where to download the file)
        """"""
        self._url = url
        self.fname = url.split('/')[-1]
        self.destination = self.fname
        if dest_dir:
            self.destination = dest_dir / self.fname
        self._request = requests.get(url, stream=True, timeout=TIMEOUT_SECS)
        if self._request.status_code != 200:
            cout(f'Got an unexpected HTTP status code: {self._request.status_code}', f'When downloading {url}', style=ERROR)
            raise util.ApioException()

    def get_size(self) -> int:
        """"""Return the size (in bytes) of the latest bytes block received""""""
        return int(self._request.headers['content-length'])

    def start(self):
        """"""Start the downloading of the file""""""
        itercontent = self._request.iter_content(chunk_size=self.CHUNK_SIZE)
        with open(self.destination, 'wb') as file:
            num_chunks = int(ceil(self.get_size() / float(self.CHUNK_SIZE)))
            for _ in track(range(num_chunks), description='Downloading', console=console()):
                file.write(next(itercontent))
            assert next(itercontent, None) is None
        self._request.close()

    def __del__(self):
        """"""Close any pending request""""""
        if self._request:
            self._request.close()","
class FileDownloader:
    '''Class for downloading files'''

    def __init__(self, url: str, dest_dir=None):
        '''Initialize a FileDownloader object
        * INPUTs:
          * url: File to download (full url)
                 (Ex. 'https://github.com/FPGAwars/apio-examples/
                       releases/download/0.0.35/apio-examples-0.0.35.zip')
          * dest_dir: Destination folder (where to download the file)
        '''
        pass

    def get_size(self) -> int:
        '''Return the size (in bytes) of the latest bytes block received'''
        pass

    def start(self):
        '''Start the downloading of the file'''
        pass

    def __del__(self):
        '''Close any pending request'''
        pass",5,5,18.0,4.0,8.0,6.0,2.0,0.74,0.0,5.0,1.0,0.0,4.0,4.0,4.0,4.0,79.0,20.0,34.0,14.0,29.0,25.0,25.0,13.0,20.0,3.0,0.0,2.0,8.0,snippet_70
152243,FPGAwars/apio,apio/managers/unpacker.py,apio.managers.unpacker.FileUnpacker,"from pathlib import Path
from apio.utils import util
from rich.progress import track
from apio.common.apio_console import console, cerror

class FileUnpacker:
    """"""Class for unpacking compressed files""""""

    def __init__(self, archpath: Path, dest_dir=Path('.')):
        """"""Initialize the unpacker object
        * INPUT:
          - archpath: filename with path to uncompress
          - des_dir: Destination folder
        """"""
        self._archpath = archpath
        self._dest_dir = dest_dir
        self._unpacker = None
        arch_ext = archpath.suffix
        if arch_ext in '.tgz':
            self._unpacker = TARArchive(archpath)
        if not self._unpacker:
            cerror(f""Can not unpack file '{archpath}'"")
            raise util.ApioException()

    def start(self) -> bool:
        """"""Start unpacking the file""""""
        items = self._unpacker.get_items()
        for i in track(range(len(items)), description='Unpacking  ', console=console()):
            self._unpacker.extract_item(items[i], self._dest_dir)
        return True","
class FileUnpacker:
    '''Class for unpacking compressed files'''

    def __init__(self, archpath: Path, dest_dir=Path('.')):
        '''Initialize the unpacker object
        * INPUT:
          - archpath: filename with path to uncompress
          - des_dir: Destination folder
        '''
        pass

    def start(self) -> bool:
        '''Start unpacking the file'''
        pass",3,3,21.0,4.0,11.0,7.0,3.0,0.64,0.0,6.0,3.0,0.0,2.0,3.0,2.0,2.0,46.0,10.0,22.0,9.0,19.0,14.0,17.0,9.0,14.0,4.0,0.0,1.0,6.0,snippet_71
152262,FPGAwars/apio,apio/utils/pkg_util.py,apio.utils.pkg_util.PackageScanResults,"from apio.common.apio_console import cout, cstyle
from dataclasses import dataclass
from typing import List, Callable, Tuple

@dataclass
class PackageScanResults:
    """"""Represents results of packages scan.""""""
    installed_ok_package_names: List[str]
    bad_version_package_names: List[str]
    uninstalled_package_names: List[str]
    broken_package_names: List[str]
    orphan_package_names: List[str]
    orphan_dir_names: List[str]
    orphan_file_names: List[str]

    def packages_installed_ok(self) -> bool:
        """"""Returns true if all packages are installed ok, regardless of
        other fixable errors.""""""
        return len(self.bad_version_package_names) == 0 and len(self.uninstalled_package_names) == 0 and (len(self.broken_package_names) == 0)

    def num_errors_to_fix(self) -> bool:
        """"""Returns the number of errors that required , having a non installed
        packages is not considered an error that need to be fix.""""""
        return len(self.bad_version_package_names) + len(self.broken_package_names) + len(self.orphan_package_names) + len(self.orphan_dir_names) + len(self.orphan_file_names)

    def is_all_ok(self) -> bool:
        """"""Return True if all packages are installed properly with no
        issues.""""""
        return not self.num_errors_to_fix() and (not self.uninstalled_package_names)

    def dump(self):
        """"""Dump the content of this object. For debugging.""""""
        cout()
        cout('Package scan results:')
        cout(f'  Installed     {self.installed_ok_package_names}')
        cout(f'  bad version   {self.bad_version_package_names}')
        cout(f'  Uninstalled   {self.uninstalled_package_names}')
        cout(f'  Broken        {self.broken_package_names}')
        cout(f'  Orphan ids    {self.orphan_package_names}')
        cout(f'  Orphan dirs   {self.orphan_dir_names}')
        cout(f'  Orphan files  {self.orphan_file_names}')","@dataclass
class PackageScanResults:
    '''Represents results of packages scan.'''

    def packages_installed_ok(self) -> bool:
        '''Returns true if all packages are installed ok, regardless of
        other fixable errors.'''
        pass

    def num_errors_to_fix(self) -> bool:
        '''Returns the number of errors that required , having a non installed
        packages is not considered an error that need to be fix.'''
        pass

    def is_all_ok(self) -> bool:
        '''Return True if all packages are installed properly with no
        issues.'''
        pass

    def dump(self):
        '''Dump the content of this object. For debugging.'''
        pass",5,5,9.0,0.0,7.0,2.0,1.0,0.6,0.0,1.0,0.0,0.0,4.0,0.0,4.0,4.0,61.0,5.0,35.0,5.0,30.0,21.0,23.0,5.0,18.0,1.0,0.0,0.0,4.0,snippet_72
155190,sentinel-hub/sentinelhub-py,sentinelhub/download/rate_limit.py,sentinelhub.download.rate_limit.SentinelHubRateLimit,"import time

class SentinelHubRateLimit:
    """"""Class implementing rate limiting logic of Sentinel Hub service

    It has 2 public methods:

    - register_next - tells if next download can start or if not, what is the wait before it can be asked again
    - update - updates expectations according to headers obtained from download

    The rate limiting object is collecting information about the status of rate limiting policy buckets from
    Sentinel Hub service. According to this information and a feedback from download requests it adapts expectations
    about when the next download attempt will be possible.
    """"""
    RETRY_HEADER = 'Retry-After'
    UNITS_SPENT_HEADER = 'X-ProcessingUnits-Spent'

    def __init__(self, num_processes: int=1, minimum_wait_time: float=0.05, maximum_wait_time: float=60.0):
        """"""
        :param num_processes: Number of parallel download processes running.
        :param minimum_wait_time: Minimum wait time between two consecutive download requests in seconds.
        :param maximum_wait_time: Maximum wait time between two consecutive download requests in seconds.
        """"""
        self.wait_time = min(num_processes * minimum_wait_time, maximum_wait_time)
        self.next_download_time = time.monotonic()

    def register_next(self) -> float:
        """"""Determines if next download request can start or not by returning the waiting time in seconds.""""""
        current_time = time.monotonic()
        wait_time = max(self.next_download_time - current_time, 0)
        if wait_time == 0:
            self.next_download_time = max(current_time + self.wait_time, self.next_download_time)
        return wait_time

    def update(self, headers: dict, *, default: float) -> None:
        """"""Update the next possible download time if the service has responded with the rate limit.

        :param headers: The headers that (may) contain information about waiting times.
        :param default: The default waiting time (in milliseconds) when retrying after getting a
            TOO_MANY_REQUESTS response without appropriate retry headers.
        """"""
        retry_after: float = int(headers.get(self.RETRY_HEADER, default))
        retry_after = retry_after / 1000
        if retry_after:
            self.next_download_time = max(time.monotonic() + retry_after, self.next_download_time)","
class SentinelHubRateLimit:
    '''Class implementing rate limiting logic of Sentinel Hub service
    It has 2 public methods:
    - register_next - tells if next download can start or if not, what is the wait before it can be asked again
    - update - updates expectations according to headers obtained from download
    The rate limiting object is collecting information about the status of rate limiting policy buckets from
    Sentinel Hub service. According to this information and a feedback from download requests it adapts expectations
    about when the next download attempt will be possible.
    '''

    def __init__(self, num_processes: int=1, minimum_wait_time: float=0.05, maximum_wait_time: float=60.0):
        '''
        :param num_processes: Number of parallel download processes running.
        :param minimum_wait_time: Minimum wait time between two consecutive download requests in seconds.
        :param maximum_wait_time: Maximum wait time between two consecutive download requests in seconds.
        '''
        pass

    def register_next(self) -> float:
        '''Determines if next download request can start or not by returning the waiting time in seconds.'''
        pass

    def update(self, headers: dict, *, default: float) -> None:
        '''Update the next possible download time if the service has responded with the rate limit.
        :param headers: The headers that (may) contain information about waiting times.
        :param default: The default waiting time (in milliseconds) when retrying after getting a
            TOO_MANY_REQUESTS response without appropriate retry headers.
        '''
        pass",4,4,10.0,1.0,5.0,4.0,2.0,1.18,0.0,3.0,0.0,0.0,3.0,2.0,3.0,3.0,47.0,11.0,17.0,11.0,13.0,20.0,17.0,11.0,13.0,2.0,0.0,1.0,5.0,snippet_73
156520,textX/textX,textx/scoping/providers.py,textx.scoping.providers.PlainName,"from textx.exceptions import TextXSemanticError

class PlainName:
    """"""
    plain name scope provider
    """"""

    def __init__(self, multi_metamodel_support=True):
        """"""
        the default scope provider constructor

        Args:
            multi_metamodel_support: enable a AST based search, instead
            of using the parser._instances
        """"""
        self.multi_metamodel_support = multi_metamodel_support
        pass

    def __call__(self, obj, attr, obj_ref):
        """"""
        the default scope provider

        Args:
            obj: unused (used for multi_metamodel_support)
            attr: unused
            obj_ref: the cross reference to be resolved

        Returns:
            the resolved reference or None
        """"""
        from textx.const import RULE_ABSTRACT, RULE_COMMON
        from textx.model import ObjCrossRef
        from textx.scoping.tools import get_parser
        if obj_ref is None:
            return None
        assert type(obj_ref) is ObjCrossRef, type(obj_ref)
        if get_parser(obj).debug:
            get_parser(obj).dprint(f'Resolving obj crossref: {obj_ref.cls}:{obj_ref.obj_name}')

        def _inner_resolve_link_rule_ref(cls, obj_name):
            """"""
            Depth-first resolving of link rule reference.
            """"""
            if cls._tx_type is RULE_ABSTRACT:
                for inherited in cls._tx_inh_by:
                    result = _inner_resolve_link_rule_ref(inherited, obj_name)
                    if result:
                        return result
            elif cls._tx_type == RULE_COMMON and id(cls) in get_parser(obj)._instances:
                objs = get_parser(obj)._instances[id(cls)]
                return objs.get(obj_name)
        if self.multi_metamodel_support:
            from textx import get_children, get_model, textx_isinstance
            result_lst = get_children(lambda x: hasattr(x, 'name') and x.name == obj_ref.obj_name and textx_isinstance(x, obj_ref.cls), get_model(obj))
            if len(result_lst) == 1:
                result = result_lst[0]
            elif len(result_lst) > 1:
                line, col = get_parser(obj).pos_to_linecol(obj_ref.position)
                raise TextXSemanticError(f'name {obj_ref.obj_name} is not unique.', line=line, col=col, filename=get_model(obj)._tx_filename)
            else:
                result = None
        else:
            result = _inner_resolve_link_rule_ref(obj_ref.cls, obj_ref.obj_name)
        return result","
class PlainName:
    '''
    plain name scope provider
    '''

    def __init__(self, multi_metamodel_support=True):
        '''
        the default scope provider constructor
        Args:
            multi_metamodel_support: enable a AST based search, instead
            of using the parser._instances
        '''
        pass

    def __call__(self, obj, attr, obj_ref):
        '''
        the default scope provider
        Args:
            obj: unused (used for multi_metamodel_support)
            attr: unused
            obj_ref: the cross reference to be resolved
        Returns:
            the resolved reference or None
        '''
        pass

        def _inner_resolve_link_rule_ref(cls, obj_name):
            '''
            Depth-first resolving of link rule reference.
                '''
                pass",4,4,35.0,3.0,18.0,14.0,4.0,0.7,0.0,3.0,2.0,0.0,2.0,1.0,2.0,2.0,89.0,11.0,47.0,15.0,39.0,33.0,31.0,15.0,23.0,6.0,0.0,3.0,12.0,snippet_74
156861,csparpa/pyowm,csparpa_pyowm/pyowm/utils/geo.py,pyowm.utils.geo.Geometry,"class Geometry:
    """"""
    Abstract parent class for geotypes

    """"""

    def geojson(self):
        """"""
        Returns a GeoJSON string representation of this geotype, compliant to
        RFC 7946 (https://tools.ietf.org/html/rfc7946)
        :return: str
        """"""
        raise NotImplementedError()

    def to_dict(self):
        """"""
        Returns a dict representation of this geotype
        :return: dict
        """"""
        raise NotImplementedError()","class Geometry:
    '''
    Abstract parent class for geotypes
    '''

    def geojson(self):
        '''
        Returns a GeoJSON string representation of this geotype, compliant to
        RFC 7946 (https://tools.ietf.org/html/rfc7946)
        :return: str
        '''
        pass

    def to_dict(self):
        '''
        Returns a dict representation of this geotype
        :return: dict
        '''
        pass",3,3,7.0,0.0,2.0,5.0,1.0,2.4,0.0,1.0,0.0,4.0,2.0,0.0,2.0,2.0,19.0,2.0,5.0,3.0,2.0,12.0,5.0,3.0,2.0,1.0,0.0,0.0,2.0,snippet_75
157145,Bachmann1234/diff-cover,diff_cover/git_path.py,diff_cover.git_path.GitPathTool,"from diff_cover.command_runner import execute
import sys
import os
from diff_cover.util import to_unix_path

class GitPathTool:
    """"""
    Converts `git diff` paths to absolute paths or relative paths to cwd.
    This class should be used throughout the project to change paths from
    the paths yielded by `git diff` to correct project paths
    """"""
    _cwd = None
    _root = None

    @classmethod
    def set_cwd(cls, cwd):
        """"""
        Set the cwd that is used to manipulate paths.
        """"""
        if not cwd:
            cwd = os.getcwd()
        if isinstance(cwd, bytes):
            cwd = cwd.decode(sys.getdefaultencoding())
        cls._cwd = cwd
        cls._root = cls._git_root()

    @classmethod
    def relative_path(cls, git_diff_path):
        """"""
        Returns git_diff_path relative to cwd.
        """"""
        root_rel_path = os.path.relpath(cls._cwd, cls._root)
        return os.path.relpath(git_diff_path, root_rel_path)

    @classmethod
    def absolute_path(cls, src_path):
        """"""
        Returns absolute git_diff_path
        """"""
        return to_unix_path(os.path.join(cls._root, src_path))

    @classmethod
    def _git_root(cls):
        """"""
        Returns the output of `git rev-parse --show-toplevel`, which
        is the absolute path for the git project root.
        """"""
        command = ['git', 'rev-parse', '--show-toplevel', '--encoding=utf-8']
        git_root = execute(command)[0]
        return git_root.split('\n', maxsplit=1)[0] if git_root else ''","
class GitPathTool:
    '''
    Converts `git diff` paths to absolute paths or relative paths to cwd.
    This class should be used throughout the project to change paths from
    the paths yielded by `git diff` to correct project paths
    '''
    @classmethod
    def set_cwd(cls, cwd):
        '''
        Set the cwd that is used to manipulate paths.
        '''
        pass
    @classmethod
    def relative_path(cls, git_diff_path):
        '''
        Returns git_diff_path relative to cwd.
        '''
        pass
    @classmethod
    def absolute_path(cls, src_path):
        '''
        Returns absolute git_diff_path
        '''
        pass
    @classmethod
    def _git_root(cls):
        '''
        Returns the output of `git rev-parse --show-toplevel`, which
        is the absolute path for the git project root.
        '''
        pass",5,5,10.0,0.0,5.0,5.0,2.0,0.96,0.0,2.0,0.0,0.0,0.0,0.0,4.0,4.0,57.0,6.0,26.0,14.0,17.0,25.0,22.0,10.0,17.0,4.0,0.0,2.0,8.0,snippet_76
157653,pixelogik/NearPy,pixelogik_NearPy/nearpy/hashes/permutation/permute.py,nearpy.hashes.permutation.permute.Permute,"from bisect import bisect_left
import random

class Permute:
    """"""
    Permute provide a random [n] -> [n] permute operation.
    e.g, a [3] -> [3] permute operation could be:
    [0,1,2] -> [1,0,2], ""010"" => ""100""
    """"""

    def __init__(self, n):
        """"""
        Init a Permute object. Randomly generate a mapping, e.g. [0,1,2] -> [1,0,2]
        """"""
        m = list(range(n))
        for end in xrange(n - 1, 0, -1):
            r = random.randint(0, end)
            tmp = m[end]
            m[end] = m[r]
            m[r] = tmp
        self.mapping = m

    def permute(self, ba):
        """"""
        Permute the bitarray ba inplace.
        """"""
        c = ba.copy()
        for i in xrange(len(self.mapping)):
            ba[i] = c[self.mapping[i]]
        return ba

    def revert(self, ba):
        """"""
        Reversely permute the bitarray ba inplace.
        """"""
        c = ba.copy()
        for i in xrange(len(self.mapping)):
            ba[self.mapping[i]] = c[i]
        return ba

    def search_revert(self, bas, ba, beam_size):
        """"""
        ba: query bitarray
        bas: a sorted list of tuples of (permuted bitarray, original bitarray)
        return : query bitarray's beam-size neighbours (unpermuted bitarray)
        """"""
        pba = ba.copy()
        self.permute(pba)
        assert beam_size % 2 == 0
        half_beam = beam_size / 2
        idx = bisect_left(bas, (pba, ba))
        start = int(max(0, idx - half_beam))
        end = int(min(len(bas), idx + half_beam))
        res = bas[start:end]
        res = [x[1] for x in res]
        return res","
class Permute:
    '''
    Permute provide a random [n] -> [n] permute operation.
    e.g, a [3] -> [3] permute operation could be:
    [0,1,2] -> [1,0,2], ""010"" => ""100""
    '''

    def __init__(self, n):
        '''
        Init a Permute object. Randomly generate a mapping, e.g. [0,1,2] -> [1,0,2]
        '''
        pass

    def permute(self, ba):
        '''
        Permute the bitarray ba inplace.
        '''
        pass

    def revert(self, ba):
        '''
        Reversely permute the bitarray ba inplace.
        '''
        pass

    def search_revert(self, bas, ba, beam_size):
        '''
        ba: query bitarray
        bas: a sorted list of tuples of (permuted bitarray, original bitarray)
        return : query bitarray's beam-size neighbours (unpermuted bitarray)
        '''
        pass",5,5,12.0,1.0,7.0,4.0,2.0,0.7,0.0,3.0,0.0,0.0,4.0,1.0,4.0,4.0,60.0,9.0,30.0,20.0,25.0,21.0,30.0,20.0,25.0,2.0,0.0,1.0,7.0,snippet_77
157718,jwkvam/bowtie,bowtie/_cache.py,bowtie._cache._Cache,"from eventlet.queue import LightQueue
from flask_socketio import emit
import eventlet
import flask
import msgpack
from bowtie._component import pack

class _Cache:
    """"""Store data in the browser.

    This cache uses session storage so data will stay
    in the browser until the tab is closed.
    All data must be serializable, which means if the
    serialization transforms the data it won't be the same
    when it is fetched.

    Examples
    --------
    >>> from bowtie import cache
    >>> cache['a'] = True  # doctest: +SKIP
    >>> cache['a']  # doctest: +SKIP
    True
    >>> cache['b'] = np.arange(5)  # doctest: +SKIP
    >>> cache['b']  # doctest: +SKIP
    [1, 2, 3, 4, 5]

    """"""

    def __getitem__(self, key):
        """"""Load the value stored with the key.

        Parameters
        ----------
        key : str
            The key to lookup the value stored.

        Returns
        -------
        object
            The value if the key exists in the cache, otherwise None.

        """"""
        validate(key)
        signal = 'cache_load'
        event = LightQueue(1)
        if flask.has_request_context():
            emit(signal, {'data': pack(key)}, callback=event.put)
        else:
            sio = flask.current_app.extensions['socketio']
            sio.emit(signal, {'data': pack(key)}, callback=event.put)
        return msgpack.unpackb(bytes(event.get(timeout=10)), encoding='utf8')

    def __setitem__(self, key, value):
        """"""Store the key value pair.

        Parameters
        ----------
        key : str
            The key to determine where it's stored, you'll need this to load the value later.
        value : object
            The value to store in the cache.

        Returns
        -------
        None

        """"""
        validate(key)
        signal = 'cache_save'
        if flask.has_request_context():
            emit(signal, {'key': pack(key), 'data': pack(value)})
        else:
            sio = flask.current_app.extensions['socketio']
            sio.emit(signal, {'key': pack(key), 'data': pack(value)})
        eventlet.sleep()","
class _Cache:
    '''Store data in the browser.
    This cache uses session storage so data will stay
    in the browser until the tab is closed.
    All data must be serializable, which means if the
    serialization transforms the data it won't be the same
    when it is fetched.
    Examples
    --------
    >>> from bowtie import cache
    >>> cache['a'] = True  # doctest: +SKIP
    >>> cache['a']  # doctest: +SKIP
    True
    >>> cache['b'] = np.arange(5)  # doctest: +SKIP
    >>> cache['b']  # doctest: +SKIP
    [1, 2, 3, 4, 5]
    '''

    def __getitem__(self, key):
        '''Load the value stored with the key.
        Parameters
        ----------
        key : str
            The key to lookup the value stored.
        Returns
        -------
        object
            The value if the key exists in the cache, otherwise None.
        '''
        pass

    def __setitem__(self, key, value):
        '''Store the key value pair.
        Parameters
        ----------
        key : str
            The key to determine where it's stored, you'll need this to load the value later.
        value : object
            The value to store in the cache.
        Returns
        -------
        None
        '''
        pass",3,3,23.0,3.0,10.0,11.0,2.0,1.85,0.0,1.0,0.0,0.0,2.0,0.0,2.0,2.0,68.0,11.0,20.0,8.0,17.0,37.0,18.0,8.0,15.0,2.0,0.0,1.0,4.0,snippet_78
159255,SectorLabs/django-postgres-extra,SectorLabs_django-postgres-extra/psqlextra/partitioning/partition.py,psqlextra.partitioning.partition.PostgresPartition,"from typing import Optional, Type
from psqlextra.models import PostgresPartitionedModel
from psqlextra.backend.schema import PostgresSchemaEditor
from abc import abstractmethod

class PostgresPartition:
    """"""Base class for a PostgreSQL table partition.""""""

    @abstractmethod
    def name(self) -> str:
        """"""Generates/computes the name for this partition.""""""

    @abstractmethod
    def create(self, model: Type[PostgresPartitionedModel], schema_editor: PostgresSchemaEditor, comment: Optional[str]=None) -> None:
        """"""Creates this partition in the database.""""""

    @abstractmethod
    def delete(self, model: Type[PostgresPartitionedModel], schema_editor: PostgresSchemaEditor) -> None:
        """"""Deletes this partition from the database.""""""

    def deconstruct(self) -> dict:
        """"""Deconstructs this partition into a dict of attributes/fields.""""""
        return {'name': self.name()}","
class PostgresPartition:
    '''Base class for a PostgreSQL table partition.'''
    @abstractmethod
    def name(self) -> str:
        '''Generates/computes the name for this partition.'''
        pass
    @abstractmethod
    def create(self, model: Type[PostgresPartitionedModel], schema_editor: PostgresSchemaEditor, comment: Optional[str]=None) -> None:
        '''Creates this partition in the database.'''
        pass
    @abstractmethod
    def delete(self, model: Type[PostgresPartitionedModel], schema_editor: PostgresSchemaEditor) -> None:
        '''Deletes this partition from the database.'''
        pass

    def deconstruct(self) -> dict:
        '''Deconstructs this partition into a dict of attributes/fields.'''
        pass",5,5,5.0,0.0,4.0,1.0,1.0,0.28,0.0,4.0,2.0,1.0,4.0,0.0,4.0,4.0,28.0,5.0,18.0,17.0,1.0,5.0,6.0,5.0,1.0,1.0,0.0,0.0,4.0,snippet_79
159496,Erotemic/ubelt,Erotemic_ubelt/ubelt/util_mixins.py,ubelt.util_mixins.NiceRepr,"import warnings

class NiceRepr:
    """"""
    Inherit from this class and define ``__nice__`` to ""nicely"" print your
    objects.

    Defines ``__str__`` and ``__repr__`` in terms of ``__nice__`` function
    Classes that inherit from :class:`NiceRepr` should redefine ``__nice__``.
    If the inheriting class has a ``__len__``, method then the default
    ``__nice__`` method will return its length.

    Example:
        >>> import ubelt as ub
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        return 'info'
        >>> foo = Foo()
        >>> assert str(foo) == '<Foo(info)>'
        >>> assert repr(foo).startswith('<Foo(info) at ')

    Example:
        >>> import ubelt as ub
        >>> class Bar(ub.NiceRepr):
        ...    pass
        >>> bar = Bar()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     assert 'object at' in str(bar)
        >>>     assert 'object at' in repr(bar)

    Example:
        >>> import ubelt as ub
        >>> class Baz(ub.NiceRepr):
        ...    def __len__(self):
        ...        return 5
        >>> baz = Baz()
        >>> assert str(baz) == '<Baz(5)>'

    Example:
        >>> import ubelt as ub
        >>> # If your nice message has a bug, it shouldn't bring down the house
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        assert False
        >>> foo = Foo()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     print('foo = {!r}'.format(foo))
        foo = <...Foo ...>

    Example:
        >>> import ubelt as ub
        >>> class Animal(ub.NiceRepr):
        ...    def __init__(self):
        ...        ...
        ...    def __nice__(self):
        ...        return ''
        >>> class Cat(Animal):
        >>>     ...
        >>> class Dog(Animal):
        >>>     ...
        >>> class Beagle(Dog):
        >>>     ...
        >>> class Ragdoll(Cat):
        >>>     ...
        >>> instances = [Animal(), Cat(), Dog(), Beagle(), Ragdoll()]
        >>> for inst in instances:
        >>>     print(str(inst))
        <Animal()>
        <Cat()>
        <Dog()>
        <Beagle()>
        <Ragdoll()>

    In the case where you cant or dont want to use ubelt.NiceRepr you can get
    similar behavior by pasting the methods from the following snippet into
    your class:

    .. code:: python

        class MyClass:

            def __nice__(self):
                return 'your concise information'

            def __repr__(self):
                nice = self.__nice__()
                classname = self.__class__.__name__
                return '<{0}({1}) at {2}>'.format(classname, nice, hex(id(self)))

            def __str__(self):
                classname = self.__class__.__name__
                nice = self.__nice__()
                return '<{0}({1})>'.format(classname, nice)
    """"""

    def __nice__(self):
        """"""
        Returns:
            str
        """"""
        if hasattr(self, '__len__'):
            return str(len(self))
        else:
            raise NotImplementedError('Define the __nice__ method for {!r}'.format(self.__class__))

    def __repr__(self):
        """"""
        Returns:
            str
        """"""
        try:
            nice = self.__nice__()
            classname = self.__class__.__name__
            return '<{0}({1}) at {2}>'.format(classname, nice, hex(id(self)))
        except Exception as ex:
            warnings.warn(str(ex), category=RuntimeWarning)
            return object.__repr__(self)

    def __str__(self):
        """"""
        Returns:
            str
        """"""
        try:
            classname = self.__class__.__name__
            nice = self.__nice__()
            return '<{0}({1})>'.format(classname, nice)
        except Exception as ex:
            warnings.warn(str(ex), category=RuntimeWarning)
            return object.__repr__(self)","
class NiceRepr:
    '''
    Inherit from this class and define ``__nice__`` to ""nicely"" print your
    objects.
    Defines ``__str__`` and ``__repr__`` in terms of ``__nice__`` function
    Classes that inherit from :class:`NiceRepr` should redefine ``__nice__``.
    If the inheriting class has a ``__len__``, method then the default
    ``__nice__`` method will return its length.
    Example:
        >>> import ubelt as ub
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        return 'info'
        >>> foo = Foo()
        >>> assert str(foo) == '<Foo(info)>'
        >>> assert repr(foo).startswith('<Foo(info) at ')
    Example:
        >>> import ubelt as ub
        >>> class Bar(ub.NiceRepr):
        ...    pass
        >>> bar = Bar()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     assert 'object at' in str(bar)
        >>>     assert 'object at' in repr(bar)
    Example:
        >>> import ubelt as ub
        >>> class Baz(ub.NiceRepr):
        ...    def __len__(self):
        ...        return 5
        >>> baz = Baz()
        >>> assert str(baz) == '<Baz(5)>'
    Example:
        >>> import ubelt as ub
        >>> # If your nice message has a bug, it shouldn't bring down the house
        >>> class Foo(ub.NiceRepr):
        ...    def __nice__(self):
        ...        assert False
        >>> foo = Foo()
        >>> import pytest
        >>> with pytest.warns(RuntimeWarning) as record:
        >>>     print('foo = {!r}'.format(foo))
        foo = <...Foo ...>
    Example:
        >>> import ubelt as ub
        >>> class Animal(ub.NiceRepr):
        ...    def __init__(self):
        ...        ...
        ...    def __nice__(self):
        ...        return ''
        >>> class Cat(Animal):
        >>>     ...
        >>> class Dog(Animal):
        >>>     ...
        >>> class Beagle(Dog):
        >>>     ...
        >>> class Ragdoll(Cat):
        >>>     ...
        >>> instances = [Animal(), Cat(), Dog(), Beagle(), Ragdoll()]
        >>> for inst in instances:
        >>>     print(str(inst))
        <Animal()>
        <Cat()>
        <Dog()>
        <Beagle()>
        <Ragdoll()>
    In the case where you cant or dont want to use ubelt.NiceRepr you can get
    similar behavior by pasting the methods from the following snippet into
    your class:
    .. code:: python
        class MyClass:
            def __nice__(self):
                return 'your concise information'
            def __repr__(self):
                nice = self.__nice__()
                classname = self.__class__.__name__
                return '<{0}({1}) at {2}>'.format(classname, nice, hex(id(self)))
            def __str__(self):
                classname = self.__class__.__name__
                nice = self.__nice__()
                return '<{0}({1})>'.format(classname, nice)
                    '''

            def __nice__(self):
                '''
        Returns:
            str
                        '''
                        pass

            def __repr__(self):
                '''
        Returns:
            str
                        '''
                        pass

            def __str__(self):
                '''
        Returns:
            str
                        '''
                        pass",4,4,12.0,0.0,7.0,5.0,2.0,4.17,0.0,5.0,0.0,2.0,3.0,0.0,3.0,3.0,134.0,15.0,23.0,10.0,19.0,96.0,21.0,8.0,17.0,2.0,0.0,1.0,6.0,snippet_80
159497,Erotemic/ubelt,Erotemic_ubelt/ubelt/util_path.py,ubelt.util_path.ChDir,"import os

class ChDir:
    """"""
    Context manager that changes the current working directory and then
    returns you to where you were.

    This is nearly the same as the stdlib :func:`contextlib.chdir`, with the
    exception that it will do nothing if the input path is None (i.e. the user
    did not want to change directories).

    SeeAlso:
        :func:`contextlib.chdir`

    Example:
        >>> import ubelt as ub
        >>> dpath = ub.Path.appdir('ubelt/tests/chdir').ensuredir()
        >>> dir1 = (dpath / 'dir1').ensuredir()
        >>> dir2 = (dpath / 'dir2').ensuredir()
        >>> with ChDir(dpath):
        >>>     assert ub.Path.cwd() == dpath
        >>>     # change to the given directory, and then returns back
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(dir2):
        >>>             assert ub.Path.cwd() == dir2
        >>>             # changes inside the context manager will be reset
        >>>             os.chdir(dpath)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(None):
        >>>             assert ub.Path.cwd() == dir1
        >>>             # When disabled, the cwd does *not* reset at context exit
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir2
        >>>         os.chdir(dir1)
        >>>         # Dont change dirs, but reset to your cwd at context end
        >>>         with ChDir('.'):
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
    """"""

    def __init__(self, dpath):
        """"""
        Args:
            dpath (str | PathLike | None):
                The new directory to work in.
                If None, then the context manager is disabled.
        """"""
        self._context_dpath = dpath
        self._orig_dpath = None

    def __enter__(self):
        """"""
        Returns:
            ChDir: self
        """"""
        if self._context_dpath is not None:
            self._orig_dpath = os.getcwd()
            os.chdir(self._context_dpath)
        return self

    def __exit__(self, ex_type, ex_value, ex_traceback):
        """"""
        Args:
            ex_type (Type[BaseException] | None):
            ex_value (BaseException | None):
            ex_traceback (TracebackType | None):

        Returns:
            bool | None
        """"""
        if self._context_dpath is not None:
            os.chdir(self._orig_dpath)","
class ChDir:
    '''
    Context manager that changes the current working directory and then
    returns you to where you were.
    This is nearly the same as the stdlib :func:`contextlib.chdir`, with the
    exception that it will do nothing if the input path is None (i.e. the user
    did not want to change directories).
    SeeAlso:
        :func:`contextlib.chdir`
    Example:
        >>> import ubelt as ub
        >>> dpath = ub.Path.appdir('ubelt/tests/chdir').ensuredir()
        >>> dir1 = (dpath / 'dir1').ensuredir()
        >>> dir2 = (dpath / 'dir2').ensuredir()
        >>> with ChDir(dpath):
        >>>     assert ub.Path.cwd() == dpath
        >>>     # change to the given directory, and then returns back
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(dir2):
        >>>             assert ub.Path.cwd() == dir2
        >>>             # changes inside the context manager will be reset
        >>>             os.chdir(dpath)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
        >>>     with ChDir(dir1):
        >>>         assert ub.Path.cwd() == dir1
        >>>         with ChDir(None):
        >>>             assert ub.Path.cwd() == dir1
        >>>             # When disabled, the cwd does *not* reset at context exit
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir2
        >>>         os.chdir(dir1)
        >>>         # Dont change dirs, but reset to your cwd at context end
        >>>         with ChDir('.'):
        >>>             os.chdir(dir2)
        >>>         assert ub.Path.cwd() == dir1
        >>>     assert ub.Path.cwd() == dpath
    '''

    def __init__(self, dpath):
        '''
        Args:
            dpath (str | PathLike | None):
                The new directory to work in.
                If None, then the context manager is disabled.
        '''
        pass

    def __enter__(self):
        '''
        Returns:
            ChDir: self
        '''
        pass

    def __exit__(self, ex_type, ex_value, ex_traceback):
        '''
        Args:
            ex_type (Type[BaseException] | None):
            ex_value (BaseException | None):
            ex_traceback (TracebackType | None):
        Returns:
            bool | None
        '''
        pass",4,4,10.0,0.0,4.0,6.0,2.0,4.67,0.0,0.0,0.0,0.0,3.0,2.0,3.0,3.0,74.0,6.0,12.0,6.0,8.0,56.0,12.0,6.0,8.0,2.0,0.0,1.0,5.0,snippet_81
159980,pazz/alot,alot/completion/completer.py,alot.completion.completer.Completer,"import abc

class Completer:
    """"""base class for completers""""""
    __metaclass__ = abc.ABCMeta

    @abc.abstractmethod
    def complete(self, original, pos):
        """"""returns a list of completions and cursor positions for the string
        `original` from position `pos` on.

        :param original: the string to complete
        :type original: str
        :param pos: starting position to complete from
        :type pos: int
        :returns: pairs of completed string and cursor position in the
                  new string
        :rtype: list of (str, int)
        :raises: :exc:`CompletionError`
        """"""
        pass

    def relevant_part(self, original, pos):
        """"""
        Calculate the subword in a ' '-separated list of substrings of
        `original` that `pos` is in.
        """"""
        start = original.rfind(' ', 0, pos) + 1
        end = original.find(' ', pos - 1)
        if end == -1:
            end = len(original)
        return (original[start:end], start, end, pos - start)","
class Completer:
    '''base class for completers'''
    @abc.abstractmethod
    def complete(self, original, pos):
        '''returns a list of completions and cursor positions for the string
        `original` from position `pos` on.
        :param original: the string to complete
        :type original: str
        :param pos: starting position to complete from
        :type pos: int
        :returns: pairs of completed string and cursor position in the
                  new string
        :rtype: list of (str, int)
        :raises: :exc:`CompletionError`
        '''
        pass

    def relevant_part(self, original, pos):
        '''
        Calculate the subword in a ' '-separated list of substrings of
        `original` that `pos` is in.
        '''
        pass",3,3,12.0,1.0,4.0,8.0,2.0,1.45,0.0,0.0,0.0,9.0,2.0,0.0,2.0,2.0,31.0,4.0,11.0,7.0,7.0,16.0,10.0,6.0,7.0,2.0,0.0,1.0,3.0,snippet_82
160363,splunk/splunk-sdk-python,splunk_splunk-sdk-python/splunklib/modularinput/argument.py,splunklib.modularinput.argument.Argument,"import xml.etree.ElementTree as ET

class Argument:
    """"""Class representing an argument to a modular input kind.

    ``Argument`` is meant to be used with ``Scheme`` to generate an XML
    definition of the modular input kind that Splunk understands.

    ``name`` is the only required parameter for the constructor.

        **Example with least parameters**::

            arg1 = Argument(name=""arg1"")

        **Example with all parameters**::

            arg2 = Argument(
                name=""arg2"",
                description=""This is an argument with lots of parameters"",
                validation=""is_pos_int('some_name')"",
                data_type=Argument.data_type_number,
                required_on_edit=True,
                required_on_create=True
            )
    """"""
    data_type_boolean = 'BOOLEAN'
    data_type_number = 'NUMBER'
    data_type_string = 'STRING'

    def __init__(self, name, description=None, validation=None, data_type=data_type_string, required_on_edit=False, required_on_create=False, title=None):
        """"""
        :param name: ``string``, identifier for this argument in Splunk.
        :param description: ``string``, human-readable description of the argument.
        :param validation: ``string`` specifying how the argument should be validated, if using internal validation.
               If using external validation, this will be ignored.
        :param data_type: ``string``, data type of this field; use the class constants.
               ""data_type_boolean"", ""data_type_number"", or ""data_type_string"".
        :param required_on_edit: ``Boolean``, whether this arg is required when editing an existing modular input of this kind.
        :param required_on_create: ``Boolean``, whether this arg is required when creating a modular input of this kind.
        :param title: ``String``, a human-readable title for the argument.
        """"""
        self.name = name
        self.description = description
        self.validation = validation
        self.data_type = data_type
        self.required_on_edit = required_on_edit
        self.required_on_create = required_on_create
        self.title = title

    def add_to_document(self, parent):
        """"""Adds an ``Argument`` object to this ElementTree document.

        Adds an <arg> subelement to the parent element, typically <args>
        and sets up its subelements with their respective text.

        :param parent: An ``ET.Element`` to be the parent of a new <arg> subelement
        :returns: An ``ET.Element`` object representing this argument.
        """"""
        arg = ET.SubElement(parent, 'arg')
        arg.set('name', self.name)
        if self.title is not None:
            ET.SubElement(arg, 'title').text = self.title
        if self.description is not None:
            ET.SubElement(arg, 'description').text = self.description
        if self.validation is not None:
            ET.SubElement(arg, 'validation').text = self.validation
        subelements = [('data_type', self.data_type), ('required_on_edit', self.required_on_edit), ('required_on_create', self.required_on_create)]
        for name, value in subelements:
            ET.SubElement(arg, name).text = str(value).lower()
        return arg","
class Argument:
    '''Class representing an argument to a modular input kind.
    ``Argument`` is meant to be used with ``Scheme`` to generate an XML
    definition of the modular input kind that Splunk understands.
    ``name`` is the only required parameter for the constructor.
        **Example with least parameters**::
            arg1 = Argument(name=""arg1"")
        **Example with all parameters**::
            arg2 = Argument(
                name=""arg2"",
                description=""This is an argument with lots of parameters"",
                validation=""is_pos_int('some_name')"",
                data_type=Argument.data_type_number,
                required_on_edit=True,
                required_on_create=True
            )
    '''

    def __init__(self, name, description=None, validation=None, data_type=data_type_string, required_on_edit=False, required_on_create=False, title=None):
        '''
        :param name: ``string``, identifier for this argument in Splunk.
        :param description: ``string``, human-readable description of the argument.
        :param validation: ``string`` specifying how the argument should be validated, if using internal validation.
               If using external validation, this will be ignored.
        :param data_type: ``string``, data type of this field; use the class constants.
               ""data_type_boolean"", ""data_type_number"", or ""data_type_string"".
        :param required_on_edit: ``Boolean``, whether this arg is required when editing an existing modular input of this kind.
        :param required_on_create: ``Boolean``, whether this arg is required when creating a modular input of this kind.
        :param title: ``String``, a human-readable title for the argument.
        '''
        pass

    def add_to_document(self, parent):
        '''Adds an ``Argument`` object to this ElementTree document.
        Adds an <arg> subelement to the parent element, typically <args>
        and sets up its subelements with their respective text.
        :param parent: An ``ET.Element`` to be the parent of a new <arg> subelement
        :returns: An ``ET.Element`` object representing this argument.
        '''
        pass",3,3,26.0,4.0,13.0,9.0,3.0,1.2,0.0,1.0,0.0,0.0,2.0,7.0,2.0,2.0,84.0,18.0,30.0,17.0,26.0,36.0,25.0,16.0,22.0,5.0,0.0,1.0,6.0,snippet_83
160367,splunk/splunk-sdk-python,splunk_splunk-sdk-python/splunklib/modularinput/scheme.py,splunklib.modularinput.scheme.Scheme,"import xml.etree.ElementTree as ET

class Scheme:
    """"""Class representing the metadata for a modular input kind.

    A ``Scheme`` specifies a title, description, several options of how Splunk should run modular inputs of this
    kind, and a set of arguments which define a particular modular input's properties.

    The primary use of ``Scheme`` is to abstract away the construction of XML to feed to Splunk.
    """"""
    streaming_mode_simple = 'SIMPLE'
    streaming_mode_xml = 'XML'

    def __init__(self, title):
        """"""
        :param title: ``string`` identifier for this Scheme in Splunk.
        """"""
        self.title = title
        self.description = None
        self.use_external_validation = True
        self.use_single_instance = False
        self.streaming_mode = Scheme.streaming_mode_xml
        self.arguments = []

    def add_argument(self, arg):
        """"""Add the provided argument, ``arg``, to the ``self.arguments`` list.

        :param arg: An ``Argument`` object to add to ``self.arguments``.
        """"""
        self.arguments.append(arg)

    def to_xml(self):
        """"""Creates an ``ET.Element`` representing self, then returns it.

        :returns: an ``ET.Element`` representing this scheme.
        """"""
        root = ET.Element('scheme')
        ET.SubElement(root, 'title').text = self.title
        if self.description is not None:
            ET.SubElement(root, 'description').text = self.description
        subelements = [('use_external_validation', self.use_external_validation), ('use_single_instance', self.use_single_instance), ('streaming_mode', self.streaming_mode)]
        for name, value in subelements:
            ET.SubElement(root, name).text = str(value).lower()
        endpoint = ET.SubElement(root, 'endpoint')
        args = ET.SubElement(endpoint, 'args')
        for arg in self.arguments:
            arg.add_to_document(args)
        return root","
class Scheme:
    '''Class representing the metadata for a modular input kind.
    A ``Scheme`` specifies a title, description, several options of how Splunk should run modular inputs of this
    kind, and a set of arguments which define a particular modular input's properties.
    The primary use of ``Scheme`` is to abstract away the construction of XML to feed to Splunk.
    '''

    def __init__(self, title):
        '''
        :param title: ``string`` identifier for this Scheme in Splunk.
        '''
        pass

    def add_argument(self, arg):
        '''Add the provided argument, ``arg``, to the ``self.arguments`` list.
        :param arg: An ``Argument`` object to add to ``self.arguments``.
        '''
        pass

    def to_xml(self):
        '''Creates an ``ET.Element`` representing self, then returns it.
        :returns: an ``ET.Element`` representing this scheme.
        '''
        pass",4,4,16.0,3.0,9.0,4.0,2.0,0.69,0.0,2.0,0.0,0.0,3.0,6.0,3.0,3.0,65.0,16.0,29.0,18.0,25.0,20.0,25.0,18.0,21.0,4.0,0.0,1.0,6.0,snippet_84
160991,ncclient/ncclient,ncclient_ncclient/ncclient/transport/session.py,ncclient.transport.session.SessionListener,"class SessionListener:
    """"""Base class for :class:`Session` listeners, which are notified when a new
    NETCONF message is received or an error occurs.

    .. note::
        Avoid time-intensive tasks in a callback's context.
    """"""

    def callback(self, root, raw):
        """"""Called when a new XML document is received. The *root* argument allows the callback to determine whether it wants to further process the document.

        Here, *root* is a tuple of *(tag, attributes)* where *tag* is the qualified name of the root element and *attributes* is a dictionary of its attributes (also qualified names).

        *raw* will contain the XML document as a string.
        """"""
        raise NotImplementedError

    def errback(self, ex):
        """"""Called when an error occurs.

        :type ex: :exc:`Exception`
        """"""
        raise NotImplementedError","class SessionListener:
    '''Base class for :class:`Session` listeners, which are notified when a new
    NETCONF message is received or an error occurs.
    .. note::
        Avoid time-intensive tasks in a callback's context.
    '''

    def callback(self, root, raw):
        '''Called when a new XML document is received. The *root* argument allows the callback to determine whether it wants to further process the document.
        Here, *root* is a tuple of *(tag, attributes)* where *tag* is the qualified name of the root element and *attributes* is a dictionary of its attributes (also qualified names).
        *raw* will contain the XML document as a string.
        '''
        pass

    def errback(self, ex):
        '''Called when an error occurs.
        :type ex: :exc:`Exception`
        '''
        pass",3,3,7.0,2.0,2.0,4.0,1.0,2.4,0.0,1.0,0.0,5.0,2.0,0.0,2.0,2.0,24.0,7.0,5.0,3.0,2.0,12.0,5.0,3.0,2.0,1.0,0.0,0.0,2.0,snippet_85
163149,python-cmd2/cmd2,python-cmd2_cmd2/cmd2/argparse_custom.py,cmd2.argparse_custom.Cmd2AttributeWrapper,"from typing import TYPE_CHECKING, Any, ClassVar, NoReturn, Protocol, cast, runtime_checkable

class Cmd2AttributeWrapper:
    """"""Wraps a cmd2-specific attribute added to an argparse Namespace.

    This makes it easy to know which attributes in a Namespace are
    arguments from a parser and which were added by cmd2.
    """"""

    def __init__(self, attribute: Any) -> None:
        """"""Initialize Cmd2AttributeWrapper instances.""""""
        self.__attribute = attribute

    def get(self) -> Any:
        """"""Get the value of the attribute.""""""
        return self.__attribute

    def set(self, new_val: Any) -> None:
        """"""Set the value of the attribute.""""""
        self.__attribute = new_val","
class Cmd2AttributeWrapper:
    '''Wraps a cmd2-specific attribute added to an argparse Namespace.
    This makes it easy to know which attributes in a Namespace are
    arguments from a parser and which were added by cmd2.
    '''

    def __init__(self, attribute: Any) -> None:
        '''Initialize Cmd2AttributeWrapper instances.'''
        pass

    def get(self) -> Any:
        '''Get the value of the attribute.'''
        pass

    def set(self, new_val: Any) -> None:
        '''Set the value of the attribute.'''
        pass",4,4,3.0,0.0,2.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0,3.0,1.0,3.0,3.0,17.0,3.0,7.0,5.0,3.0,7.0,7.0,5.0,3.0,1.0,0.0,0.0,3.0,snippet_86
163184,python-cmd2/cmd2,python-cmd2_cmd2/cmd2/utils.py,cmd2.utils.ContextFlag,"class ContextFlag:
    """"""A context manager which is also used as a boolean flag value within the default sigint handler.

    Its main use is as a flag to prevent the SIGINT handler in cmd2 from raising a KeyboardInterrupt
    while a critical code section has set the flag to True. Because signal handling is always done on the
    main thread, this class is not thread-safe since there is no need.
    """"""

    def __init__(self) -> None:
        """"""When this flag has a positive value, it is considered set. When it is 0, it is not set.

        It should never go below 0.
        """"""
        self.__count = 0

    def __bool__(self) -> bool:
        """"""Define the truth value of an object when it is used in a boolean context.""""""
        return self.__count > 0

    def __enter__(self) -> None:
        """"""When a with block is entered, the __enter__ method of the context manager is called.""""""
        self.__count += 1

    def __exit__(self, *args: object) -> None:
        """"""When the execution flow exits a with statement block this is called, regardless of whether an exception occurred.""""""
        self.__count -= 1
        if self.__count < 0:
            raise ValueError('count has gone below 0')","class ContextFlag:
    '''A context manager which is also used as a boolean flag value within the default sigint handler.
    Its main use is as a flag to prevent the SIGINT handler in cmd2 from raising a KeyboardInterrupt
    while a critical code section has set the flag to True. Because signal handling is always done on the
    main thread, this class is not thread-safe since there is no need.
        '''

    def __init__(self) -> None:
        '''When this flag has a positive value, it is considered set. When it is 0, it is not set.
        It should never go below 0.
        '''
        pass

    def __bool__(self) -> bool:
        '''Define the truth value of an object when it is used in a boolean context.'''
        pass

    def __enter__(self) -> None:
        '''When a with block is entered, the __enter__ method of the context manager is called.'''
        pass

    def __exit__(self, *args: object) -> None:
        '''When the execution flow exits a with statement block this is called, regardless of whether an exception occurred.'''
        pass",5,5,3.0,0.0,3.0,1.0,1.0,0.64,0.0,3.0,0.0,0.0,4.0,1.0,4.0,4.0,23.0,5.0,11.0,6.0,6.0,7.0,11.0,6.0,6.0,2.0,0.0,1.0,5.0,snippet_87
166548,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/cache/cache.py,pypyr.cache.cache.Cache,"import threading
from pypyr.config import config

class Cache:
    """"""Thread-safe general purpose cache for objects.

    Add things to the cache by calling get(key, creator). If the requested key
    doesn't exist, will add the item to the cache for you.
    """"""

    def __init__(self):
        """"""Instantiate the cache.""""""
        self._lock = threading.Lock()
        self._cache = {}

    def clear(self):
        """"""Clear the cache of all objects.""""""
        with self._lock:
            self._cache.clear()

    def get(self, key, creator):
        """"""Get key from cache. If key not exist, call creator and cache result.

        Looks for key in cache and returns object for that key.

        If key is not found, call creator and save the result to cache for that
        key.

        Be warned that get happens under the context of a Lock. . . so if
        creator takes a long time you might well be blocking.

        If config no_cache is True, bypasses cache entirely - will call
        creator each time and also not save the result to cache.

        Args:
            key: key (unique id) of cached item
            creator: callable that will create cached object if key not found

        Returns:
            Cached item at key or the result of creator()
        """"""
        if config.no_cache:
            logger.debug('no cache mode enabled. creating `%s` sans cache', key)
            return creator()
        with self._lock:
            if key in self._cache:
                logger.debug('`%s` loading from cache', key)
                obj = self._cache[key]
            else:
                logger.debug('`%s` not found in cache. . . creating', key)
                obj = creator()
                self._cache[key] = obj
        return obj","
class Cache:
    '''Thread-safe general purpose cache for objects.
    Add things to the cache by calling get(key, creator). If the requested key
    doesn't exist, will add the item to the cache for you.
    '''

    def __init__(self):
        '''Instantiate the cache.'''
        pass

    def clear(self):
        '''Clear the cache of all objects.'''
        pass

    def get(self, key, creator):
        '''Get key from cache. If key not exist, call creator and cache result.
        Looks for key in cache and returns object for that key.
        If key is not found, call creator and save the result to cache for that
        key.
        Be warned that get happens under the context of a Lock. . . so if
        creator takes a long time you might well be blocking.
        If config no_cache is True, bypasses cache entirely - will call
        creator each time and also not save the result to cache.
        Args:
            key: key (unique id) of cached item
            creator: callable that will create cached object if key not found
        Returns:
            Cached item at key or the result of creator()
        '''
        pass",4,4,15.0,3.0,7.0,5.0,2.0,0.95,0.0,0.0,0.0,5.0,3.0,2.0,3.0,3.0,53.0,12.0,21.0,7.0,17.0,20.0,19.0,7.0,15.0,3.0,0.0,2.0,5.0,snippet_88
166549,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/cache/loadercache.py,pypyr.cache.loadercache.Loader,"from pypyr.pipedef import PipelineDefinition, PipelineInfo
from pypyr.cache.cache import Cache
from collections.abc import Mapping
from pypyr.errors import PipelineDefinitionError

class Loader:
    """"""A single pipeline loader & the cache for all pipelines it has loaded.

    It loads pipelines using the get_pipeline_definition you assign to the
    loader at initialization.

    Attributes:
        name (str): Absolute module name of loader.
    """"""
    __slots__ = ['name', '_get_pipeline_definition', '_pipeline_cache']

    def __init__(self, name, get_pipeline_definition):
        """"""Initialize the loader and its pipeline cache.

        The expected function signature is:
        get_pipeline_definition(name: str,
                                parent: any) -> PipelineDefinition | Mapping

        Args:
            name: Absolute name of loader
            get_pipeline_definition: Reference to the function to call when
                loading a pipeline with this Loader.
        """"""
        self.name = name
        self._get_pipeline_definition = get_pipeline_definition
        self._pipeline_cache = Cache()

    def clear(self):
        """"""Clear all the pipelines in this Loader's cache.""""""
        self._pipeline_cache.clear()

    def get_pipeline(self, name, parent):
        """"""Get cached PipelineDefinition. Adds it to cache if it doesn't exist.

        The cache is local to this Loader instance.

        The combination of parent+name must be unique for this Loader. Parent
        should therefore have a sensible __str__ implementation because it
        forms part of the pipeline's identifying str key in the cache.

        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.

        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        """"""
        normalized_name = f'{parent}+{name}' if parent else name
        return self._pipeline_cache.get(normalized_name, lambda: self._load_pipeline(name, parent))

    def _load_pipeline(self, name, parent):
        """"""Execute get_pipeline_definition(name, parent) for this loader.

        If the loader get_pipeline_definition does not return a
        PipelineDefinition, this method will wrap the payload inside a
        PipelineDefinition for you.

        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.

        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        """"""
        logger.debug('starting')
        logger.debug('loading the pipeline definition with %s', self.name)
        pipeline_definition = self._get_pipeline_definition(pipeline_name=name, parent=parent)
        if not isinstance(pipeline_definition, PipelineDefinition):
            pipeline_definition = PipelineDefinition(pipeline=pipeline_definition, info=PipelineInfo(pipeline_name=name, loader=self.name, parent=parent))
        if not isinstance(pipeline_definition.pipeline, Mapping):
            raise PipelineDefinitionError(""A pipeline must be a mapping at the top level. Does your top-level yaml have a 'steps:' key? For example:\n\nsteps:\n  - name: pypyr.steps.echo\n    in:\n      echoMe: this is a bare bones pipeline example.\n"")
        logger.debug('done')
        return pipeline_definition","
class Loader:
    '''A single pipeline loader & the cache for all pipelines it has loaded.
    It loads pipelines using the get_pipeline_definition you assign to the
    loader at initialization.
    Attributes:
        name (str): Absolute module name of loader.
    '''

    def __init__(self, name, get_pipeline_definition):
        '''Initialize the loader and its pipeline cache.
        The expected function signature is:
        get_pipeline_definition(name: str,
                                parent: any) -> PipelineDefinition | Mapping
        Args:
            name: Absolute name of loader
            get_pipeline_definition: Reference to the function to call when
                loading a pipeline with this Loader.
        '''
        pass

    def clear(self):
        '''Clear all the pipelines in this Loader's cache.'''
        pass

    def get_pipeline(self, name, parent):
        '''Get cached PipelineDefinition. Adds it to cache if it doesn't exist.
        The cache is local to this Loader instance.
        The combination of parent+name must be unique for this Loader. Parent
        should therefore have a sensible __str__ implementation because it
        forms part of the pipeline's identifying str key in the cache.
        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.
        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        '''
        pass

    def _load_pipeline(self, name, parent):
        '''Execute get_pipeline_definition(name, parent) for this loader.
        If the loader get_pipeline_definition does not return a
        PipelineDefinition, this method will wrap the payload inside a
        PipelineDefinition for you.
        Args:
            name (str): Name of pipeline, sans .yaml at end.
            parent (any): Parent in which to look for pipeline.
        Returns:
            pypyr.pipedef.PipelineDefinition: Yaml payload and loader info
                metadata for the pipeline.
        '''
        pass",5,5,22.0,3.0,8.0,10.0,2.0,1.34,0.0,5.0,4.0,0.0,4.0,3.0,4.0,4.0,102.0,20.0,35.0,11.0,30.0,47.0,21.0,11.0,16.0,3.0,0.0,1.0,7.0,snippet_89
166558,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/dsl.py,pypyr.dsl.RetryDecorator,"from pypyr.utils import poll
from pypyr.errors import Call, ControlOfFlowInstruction, get_error_name, HandledError, LoopMaxExhaustedError, PipelineDefinitionError, Stop
from pypyr.cache.backoffcache import backoff_cache
from pypyr.config import config

class RetryDecorator:
    """"""Retry decorator, as interpreted by the pypyr pipeline definition yaml.

    Encapsulate the methods that run a step in a retry loop, and also maintains
    state necessary to run the loop. Given the need to maintain state, this is
    in a class, rather than purely functional code.

    In a normal world, Step invokes RetryDecorator. If you run it directly,
    you're responsible for the context and surrounding control-of-flow.

    External class consumers should probably use the retry_loop method.
    retry_loop serves as the blackbox entrypoint for this class' other methods.

    Attributes:
        backoff (str): default 'fixed'. Absolute name of back-off strategy.
            Builtin strategies allow aliases like fixed, linear, jitter. Custom
            backoff should give absolute name to callable derived from
            pypyr.retries.BackoffBase.
        backoff_args (any): User provided arguments for back-off strategy.
            Likely want to use a dict here.
        jrc (float): default 0. Jitter Range Coefficient. Jitter finds a random
            value between (jrc*sleep) and (sleep).
        max (int):  default None. Maximum loop iterations. None is infinite.
        sleep (float or list[float]):  defaults 0. Sleep in seconds between
            iterations.
        sleep_max (float): default None. Maximum value for sleep if using a
            backoff strategy that calculates sleep interval. None means sleep
            can increase indefinitely.
        stop_on (list[str]): default None. Always stop retry on these error
            types. None means retry on all errors.
        retry_on (list[str]): default None. Only retry on these error types.
            All other error types will stop retry loop. None means retry all
            errors.

    """"""

    def __init__(self, retry_definition):
        """"""Initialize the class. No duh, huh.

        You can happily expect the initializer to initialize all
        member attributes.

        Args:
            retry_definition: dict. This is the actual retry definition as it
                              exists in the pipeline yaml.

        """"""
        logger.debug('starting')
        if isinstance(retry_definition, dict):
            self.backoff = retry_definition.get('backoff', None)
            self.backoff_args = retry_definition.get('backoffArgs', None)
            self.jrc = retry_definition.get('jrc', 0)
            self.max = retry_definition.get('max', None)
            self.sleep = retry_definition.get('sleep', 0)
            self.sleep_max = retry_definition.get('sleepMax', None)
            self.stop_on = retry_definition.get('stopOn', None)
            self.retry_on = retry_definition.get('retryOn', None)
        else:
            logger.error('retry decorator definition incorrect.')
            raise PipelineDefinitionError('retry decorator must be a dict (i.e a map) type.')
        self.retry_counter = None
        logger.debug('done')

    def exec_iteration(self, counter, context, step_method, max):
        """"""Run a single retry iteration.

        This method abides by the signature invoked by poll.while_until_true,
        which is to say (counter, *args, **kwargs). In a normal execution
        chain, this method's args passed by self.retry_loop where context
        and step_method set. while_until_true injects counter as a 1st arg.

        Args:
            counter. int. loop counter, which number of iteration is this.
            context: (pypyr.context.Context) The pypyr context. This arg will
                     mutate - after method execution will contain the new
                     updated context.
            step_method: (method/function) This is the method/function that
                         will execute on every loop iteration. Signature is:
                         function(context)
            max: int. Execute step_method function up to this limit

         Returns:
            bool. True if step execution completed without error.
                  False if error occured during step execution.

        """"""
        logger.debug('starting')
        context['retryCounter'] = counter
        self.retry_counter = counter
        logger.info('retry: running step with counter %s', counter)
        try:
            step_method(context)
            result = True
        except (ControlOfFlowInstruction, Stop):
            raise
        except Exception as ex_info:
            if max:
                if counter == max:
                    logger.debug('retry: max %s retries exhausted. raising error.', counter)
                    raise
            if isinstance(ex_info, HandledError):
                ex_info = ex_info.__cause__
            if self.stop_on or self.retry_on:
                error_name = get_error_name(ex_info)
                if self.stop_on:
                    formatted_stop_list = context.get_formatted_value(self.stop_on)
                    if error_name in formatted_stop_list:
                        logger.error('%s in stopOn. Raising error and exiting retry.', error_name)
                        raise
                    else:
                        logger.debug('%s not in stopOn. Continue.', error_name)
                if self.retry_on:
                    formatted_retry_list = context.get_formatted_value(self.retry_on)
                    if error_name not in formatted_retry_list:
                        logger.error('%s not in retryOn. Raising error and exiting retry.', error_name)
                        raise
                    else:
                        logger.debug('%s in retryOn. Retry again.', error_name)
            result = False
            logger.error('retry: ignoring error because retryCounter < max.\n%s: %s', type(ex_info).__name__, ex_info)
        logger.debug('retry: done step with counter %s', counter)
        logger.debug('done')
        return result

    def retry_loop(self, context, step_method):
        """"""Run step inside a retry loop.

        Args:
            context: (pypyr.context.Context) The pypyr context. This arg will
                     mutate - after method execution will contain the new
                     updated context.
            step_method: (method/function) This is the method/function that
                         will execute on every loop iteration. Signature is:
                         function(context)

        """"""
        logger.debug('starting')
        context['retryCounter'] = 0
        self.retry_counter = 0
        sleep = context.get_formatted_value(self.sleep)
        backoff_name = context.get_formatted_value(self.backoff) if self.backoff else config.default_backoff
        max_sleep = None
        if self.sleep_max:
            max_sleep = context.get_formatted_as_type(self.sleep_max, out_type=float)
        jrc = context.get_formatted_value(self.jrc)
        backoff_args = context.get_formatted_value(self.backoff_args)
        backoff_callable = backoff_cache.get_backoff(backoff_name)(sleep=sleep, max_sleep=max_sleep, jrc=jrc, kwargs=backoff_args)
        if self.max:
            max = context.get_formatted_as_type(self.max, out_type=int)
            logger.info('retry decorator will try %d times with %s backoff starting at %ss intervals.', max, backoff_name, sleep)
        else:
            max = None
            logger.info('retry decorator will try indefinitely with %s backoff starting at %ss intervals.', backoff_name, sleep)
        is_retry_ok = poll.while_until_true(interval=backoff_callable, max_attempts=max)(self.exec_iteration)(context=context, step_method=step_method, max=max)
        assert is_retry_ok
        logger.debug('retry loop complete, reporting success.')
        logger.debug('done')","
class RetryDecorator:
    '''Retry decorator, as interpreted by the pypyr pipeline definition yaml.
    Encapsulate the methods that run a step in a retry loop, and also maintains
    state necessary to run the loop. Given the need to maintain state, this is
    in a class, rather than purely functional code.
    In a normal world, Step invokes RetryDecorator. If you run it directly,
    you're responsible for the context and surrounding control-of-flow.
    External class consumers should probably use the retry_loop method.
    retry_loop serves as the blackbox entrypoint for this class' other methods.
    Attributes:
        backoff (str): default 'fixed'. Absolute name of back-off strategy.
            Builtin strategies allow aliases like fixed, linear, jitter. Custom
            backoff should give absolute name to callable derived from
            pypyr.retries.BackoffBase.
        backoff_args (any): User provided arguments for back-off strategy.
            Likely want to use a dict here.
        jrc (float): default 0. Jitter Range Coefficient. Jitter finds a random
            value between (jrc*sleep) and (sleep).
        max (int):  default None. Maximum loop iterations. None is infinite.
        sleep (float or list[float]):  defaults 0. Sleep in seconds between
            iterations.
        sleep_max (float): default None. Maximum value for sleep if using a
            backoff strategy that calculates sleep interval. None means sleep
            can increase indefinitely.
        stop_on (list[str]): default None. Always stop retry on these error
            types. None means retry on all errors.
        retry_on (list[str]): default None. Only retry on these error types.
            All other error types will stop retry loop. None means retry all
            errors.
    '''

    def __init__(self, retry_definition):
        '''Initialize the class. No duh, huh.
        You can happily expect the initializer to initialize all
        member attributes.
        Args:
            retry_definition: dict. This is the actual retry definition as it
                              exists in the pipeline yaml.
        '''
        pass

    def exec_iteration(self, counter, context, step_method, max):
        '''Run a single retry iteration.
        This method abides by the signature invoked by poll.while_until_true,
        which is to say (counter, *args, **kwargs). In a normal execution
        chain, this method's args passed by self.retry_loop where context
        and step_method set. while_until_true injects counter as a 1st arg.
        Args:
            counter. int. loop counter, which number of iteration is this.
            context: (pypyr.context.Context) The pypyr context. This arg will
                     mutate - after method execution will contain the new
                     updated context.
            step_method: (method/function) This is the method/function that
                         will execute on every loop iteration. Signature is:
                         function(context)
            max: int. Execute step_method function up to this limit
         Returns:
            bool. True if step execution completed without error.
                  False if error occured during step execution.
        '''
        pass

    def retry_loop(self, context, step_method):
        '''Run step inside a retry loop.
        Args:
            context: (pypyr.context.Context) The pypyr context. This arg will
                     mutate - after method execution will contain the new
                     updated context.
            step_method: (method/function) This is the method/function that
                         will execute on every loop iteration. Signature is:
                         function(context)
        '''
        pass",4,4,61.0,12.0,33.0,16.0,6.0,0.79,0.0,9.0,4.0,0.0,3.0,9.0,3.0,3.0,220.0,43.0,99.0,25.0,95.0,78.0,73.0,24.0,69.0,11.0,0.0,4.0,17.0,snippet_90
166562,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/dsl.py,pypyr.dsl.WhileDecorator,"from pypyr.errors import Call, ControlOfFlowInstruction, get_error_name, HandledError, LoopMaxExhaustedError, PipelineDefinitionError, Stop
from pypyr.utils import poll

class WhileDecorator:
    """"""While Decorator, as interpreted by the pypyr pipeline definition yaml.

    Encapsulate the methods that run a step in a while loop, and also maintains
    state necessary to run the loop. Given the need to maintain state, this is
    in a class, rather than purely functional code.

    In a normal world, Step invokes WhileDecorator. If you run it directly,
    you're responsible for the context and surrounding control-of-flow.

    External class consumers should probably use the while_loop method.
    while_loop serves as the blackbox entrypoint for this class' other methods.

    Attributes:
        error_on_max: (bool) defaults False. Raise error if max reached.
        max: (int) default None. Maximum loop iterations. None is infinite.
        sleep: (float) defaults 0. Sleep in seconds between iterations.
        stop:(bool) defaults None. Exit loop when stop is True.

    """"""

    def __init__(self, while_definition):
        """"""Initialize the class. No duh, huh.

        You can happily expect the initializer to initialize all
        member attributes.

        Args:
            while_definition: dict. This is the actual while definition as it
                              exists in the pipeline yaml.

        """"""
        logger.debug('starting')
        if isinstance(while_definition, dict):
            self.error_on_max = while_definition.get('errorOnMax', False)
            self.max = while_definition.get('max', None)
            self.sleep = while_definition.get('sleep', 0)
            self.stop = while_definition.get('stop', None)
            if self.stop is None and self.max is None:
                logger.error('while decorator missing both max and stop.')
                raise PipelineDefinitionError('the while decorator must have either max or stop, or both. But not neither. Note that setting stop: False with no max is an infinite loop. If an infinite loop is really what you want, set stop: False')
        else:
            logger.error('while decorator definition incorrect.')
            raise PipelineDefinitionError('while decorator must be a dict (i.e a map) type.')
        self.while_counter = None
        logger.debug('done')

    def exec_iteration(self, counter, context, step_method):
        """"""Run a single loop iteration.

        This method abides by the signature invoked by poll.while_until_true,
        which is to say (counter, *args, **kwargs). In a normal execution
        chain, this method's args passed by self.while_loop where context
        and step_method set. while_until_true injects counter as a 1st arg.

        Args:
            counter. int. loop counter, which number of iteration is this.
            context: (pypyr.context.Context) The pypyr context. This arg will
                     mutate - after method execution will contain the new
                     updated context.
            step_method: (method/function) This is the method/function that
                         will execute on every loop iteration. Signature is:
                         function(context)

         Returns:
            bool. True if self.stop evaluates to True after step execution,
                  False otherwise.

        """"""
        logger.debug('starting')
        context['whileCounter'] = counter
        self.while_counter = counter
        logger.info('while: running step with counter %s', counter)
        step_method(context)
        logger.debug('while: done step %s', counter)
        result = False
        if self.stop:
            result = context.get_formatted_as_type(self.stop, out_type=bool)
        logger.debug('done')
        return result

    def while_loop(self, context, step_method):
        """"""Run step inside a while loop.

        Args:
            context: (pypyr.context.Context) The pypyr context. This arg will
                     mutate - after method execution will contain the new
                     updated context.
            step_method: (method/function) This is the method/function that
                         will execute on every loop iteration. Signature is:
                         function(context)

        """"""
        logger.debug('starting')
        context['whileCounter'] = 0
        self.while_counter = 0
        if self.stop is None and self.max is None:
            logger.error('while decorator missing both max and stop.')
            raise PipelineDefinitionError('the while decorator must have either max or stop, or both. But not neither.')
        error_on_max = context.get_formatted_as_type(self.error_on_max, out_type=bool)
        sleep = context.get_formatted_as_type(self.sleep, out_type=float)
        if self.max is None:
            max = None
            logger.info('while decorator will loop until %s evaluates to True at %ss intervals.', self.stop, sleep)
        else:
            max = context.get_formatted_as_type(self.max, out_type=int)
            if max < 1:
                logger.info('max %s is %s. while only runs when max > 0.', self.max, max)
                logger.debug('done')
                return
            if self.stop is None:
                logger.info('while decorator will loop %s times at %ss intervals.', max, sleep)
            else:
                logger.info('while decorator will loop %s times, or until %s evaluates to True at %ss intervals.', max, self.stop, sleep)
        if not poll.while_until_true(interval=sleep, max_attempts=max)(self.exec_iteration)(context=context, step_method=step_method):
            if error_on_max:
                logger.error('exhausted %s iterations of while loop, and errorOnMax is True.', max)
                if self.stop and max:
                    raise LoopMaxExhaustedError(f'while loop reached {max} and {self.stop} never evaluated to True.')
                else:
                    raise LoopMaxExhaustedError(f'while loop reached {max}.')
            elif self.stop and max:
                logger.info('while decorator looped %s times, and %s never evaluated to True.', max, self.stop)
            logger.debug('while loop done')
        else:
            logger.info('while loop done, stop condition %s evaluated True.', self.stop)
        logger.debug('done')","
class WhileDecorator:
    '''While Decorator, as interpreted by the pypyr pipeline definition yaml.
    Encapsulate the methods that run a step in a while loop, and also maintains
    state necessary to run the loop. Given the need to maintain state, this is
    in a class, rather than purely functional code.
    In a normal world, Step invokes WhileDecorator. If you run it directly,
    you're responsible for the context and surrounding control-of-flow.
    External class consumers should probably use the while_loop method.
    while_loop serves as the blackbox entrypoint for this class' other methods.
    Attributes:
        error_on_max: (bool) defaults False. Raise error if max reached.
        max: (int) default None. Maximum loop iterations. None is infinite.
        sleep: (float) defaults 0. Sleep in seconds between iterations.
        stop:(bool) defaults None. Exit loop when stop is True.
    '''

    def __init__(self, while_definition):
        '''Initialize the class. No duh, huh.
        You can happily expect the initializer to initialize all
        member attributes.
        Args:
            while_definition: dict. This is the actual while definition as it
                              exists in the pipeline yaml.
        '''
        pass

    def exec_iteration(self, counter, context, step_method):
        '''Run a single loop iteration.
        This method abides by the signature invoked by poll.while_until_true,
        which is to say (counter, *args, **kwargs). In a normal execution
        chain, this method's args passed by self.while_loop where context
        and step_method set. while_until_true injects counter as a 1st arg.
        Args:
            counter. int. loop counter, which number of iteration is this.
            context: (pypyr.context.Context) The pypyr context. This arg will
                     mutate - after method execution will contain the new
                     updated context.
            step_method: (method/function) This is the method/function that
                         will execute on every loop iteration. Signature is:
                         function(context)
         Returns:
            bool. True if self.stop evaluates to True after step execution,
                  False otherwise.
        '''
        pass

    def while_loop(self, context, step_method):
        '''Run step inside a while loop.
        Args:
            context: (pypyr.context.Context) The pypyr context. This arg will
                     mutate - after method execution will contain the new
                     updated context.
            step_method: (method/function) This is the method/function that
                         will execute on every loop iteration. Signature is:
                         function(context)
        '''
        pass",4,4,53.0,9.0,30.0,15.0,5.0,0.64,0.0,6.0,2.0,0.0,3.0,5.0,3.0,3.0,183.0,35.0,90.0,12.0,86.0,58.0,58.0,12.0,54.0,9.0,0.0,3.0,14.0,snippet_91
166587,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/pipedef.py,pypyr.pipedef.PipelineDefinition,"class PipelineDefinition:
    """"""The pipeline body and its metadata.

    A loader creates the PipelineDefinition and sets the metadata in .info.

    The PipelineDefinition is a globally shared cache of the pipeline body &
    meta-data.

    Attributes:
        pipeline (dict-like): The pipeline yaml body.
        info (PipelineInfo): Meta-data set by the loader for the pipeline.
    """"""
    __slots__ = ['pipeline', 'info']

    def __init__(self, pipeline, info):
        """"""Initialize a pipeline definition.

        Args:
            pipeline (dict-like): The pipeline yaml body itself.
            info (PipelineInfo): Meta-data set by the loader for the pipeline.
        """"""
        self.pipeline = pipeline
        self.info = info

    def __eq__(self, other):
        """"""Equality comparison checks Pipeline and info objects are equal.""""""
        type_self = type(self)
        if type_self is type(other):
            all_slots = [p for c in type_self.__mro__ for p in getattr(c, '__slots__', [])]
            return all((getattr(self, s, id(self)) == getattr(other, s, id(other)) for s in all_slots))
        else:
            return False","class PipelineDefinition:
    '''The pipeline body and its metadata.
    A loader creates the PipelineDefinition and sets the metadata in .info.
    The PipelineDefinition is a globally shared cache of the pipeline body &
    meta-data.
    Attributes:
        pipeline (dict-like): The pipeline yaml body.
        info (PipelineInfo): Meta-data set by the loader for the pipeline.
    '''

    def __init__(self, pipeline, info):
        '''Initialize a pipeline definition.
        Args:
            pipeline (dict-like): The pipeline yaml body itself.
            info (PipelineInfo): Meta-data set by the loader for the pipeline.
        '''
        pass

    def __eq__(self, other):
        '''Equality comparison checks Pipeline and info objects are equal.'''
        pass",3,3,11.0,1.0,7.0,3.0,2.0,0.93,0.0,1.0,0.0,0.0,2.0,2.0,2.0,2.0,37.0,8.0,15.0,8.0,12.0,14.0,11.0,8.0,8.0,2.0,0.0,1.0,3.0,snippet_92
166589,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/pipedef.py,pypyr.pipedef.PipelineInfo,"class PipelineInfo:
    """"""The common attributes that every pipeline loader should set.

    Custom loaders that want to add more properties to a pipeline's meta-data
    should probably derive from this class.

    Attributes:
        pipeline_name (str): Name of pipeline, as set by the loader.
        loader (str): Absolute module name of the pipeline loader.
        parent (any): pipeline_name resolves from parent. The parent can be any
            type - it is up to the loader to interpret the parent property.
        is_loader_cascading (bool): Loader cascades to child pipelines if not
            otherwise set on pype. Default True.
        is_parent_cascading (bool): Parent cascades to child pipelines if not
            otherwise set on pype. Default True.
    """"""
    __slots__ = ['pipeline_name', 'loader', 'parent', 'is_loader_cascading', 'is_parent_cascading']

    def __init__(self, pipeline_name, loader, parent, is_parent_cascading=True, is_loader_cascading=True):
        """"""Initialize PipelineInfo.

        Args:
            pipeline_name (str): name of pipeline, as set by the loader.
            loader (str): absolute module name of pypeloader.
            parent (any): pipeline_name resolves from parent.
            is_loader_cascading (bool): Loader cascades to child pipelines if
                not otherwise set on pype. Default True.
            is_parent_cascading (bool): Parent cascades to child pipelines if
                not otherwise set on pype. Default True.
        """"""
        self.pipeline_name = pipeline_name
        self.loader = loader
        self.parent = parent
        self.is_loader_cascading = is_loader_cascading
        self.is_parent_cascading = is_parent_cascading

    def __eq__(self, other):
        """"""Check all instance attributes are equal.""""""
        type_self = type(self)
        if type_self is type(other):
            all_slots = [p for c in type_self.__mro__ for p in getattr(c, '__slots__', [])]
            return all((getattr(self, s, id(self)) == getattr(other, s, id(other)) for s in all_slots))
        else:
            return False","class PipelineInfo:
    '''The common attributes that every pipeline loader should set.
    Custom loaders that want to add more properties to a pipeline's meta-data
    should probably derive from this class.
    Attributes:
        pipeline_name (str): Name of pipeline, as set by the loader.
        loader (str): Absolute module name of the pipeline loader.
        parent (any): pipeline_name resolves from parent. The parent can be any
            type - it is up to the loader to interpret the parent property.
        is_loader_cascading (bool): Loader cascades to child pipelines if not
            otherwise set on pype. Default True.
        is_parent_cascading (bool): Parent cascades to child pipelines if not
            otherwise set on pype. Default True.
    '''

    def __init__(self, pipeline_name, loader, parent, is_parent_cascading=True, is_loader_cascading=True):
        '''Initialize PipelineInfo.
        Args:
            pipeline_name (str): name of pipeline, as set by the loader.
            loader (str): absolute module name of pypeloader.
            parent (any): pipeline_name resolves from parent.
            is_loader_cascading (bool): Loader cascades to child pipelines if
                not otherwise set on pype. Default True.
            is_parent_cascading (bool): Parent cascades to child pipelines if
                not otherwise set on pype. Default True.
        '''
        pass

    def __eq__(self, other):
        '''Check all instance attributes are equal.'''
        pass",3,3,16.0,1.0,9.0,6.0,2.0,1.14,0.0,1.0,0.0,1.0,2.0,5.0,2.0,2.0,52.0,7.0,21.0,13.0,16.0,24.0,14.0,11.0,11.0,2.0,0.0,1.0,3.0,snippet_93
166604,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/steps/dsl/cmd.py,pypyr.steps.dsl.cmd.CmdStep,"import pypyr.utils.types
from pypyr.errors import ContextError, KeyInContextHasNoValueError, KeyNotInContextError
from collections.abc import Mapping, Sequence
from pypyr.context import Context
import logging
from pypyr.subproc import Command, SimpleCommandTypes

class CmdStep:
    """"""A pypyr step to run an executable or command as a subprocess.

    This models a step that takes config like this:
        cmd: <<cmd string>>

    OR, expanded syntax is as a dict
        cmd:
            run: str. mandatory. command + args to execute.
            save: bool. defaults False. save output to cmdOut. Treats output
                as text in the system's encoding and removes newlines at end.
            cwd: str/Pathlike. optional. Working directory for this command.
            bytes (bool): Default False. When `save` return output bytes from
                cmd unaltered, without applying any encoding & text newline
                processing.
            encoding (str): Default None. When `save`, decode cmd output with
                this encoding. The default of None uses the system encoding and
                should ""just work"".
            stdout (str | Path): Default None. Write stdout to this file path.
                Special value `/dev/null` writes to the system null device.
            stderr (str | Path): Default None. Write stderr to this file path.
                Special value `/dev/null` writes to the system null device.
                Special value `/dev/stdout` redirects err output to stdout.
            append (bool): Default False. When stdout/stderr a file, append
                rather than overwrite. Default is to overwrite.

    In expanded syntax, `run` can be a simple string or a list:
        cmd:
          run:
            - my-executable --arg
            - cmd here
          save: False
          cwd: ./path/here

    OR, as a list in simplified syntax:
        cmd:
          - my-executable --arg
          - ./another-executable --arg

    Any or all of the list items can use expanded syntax:
        cmd:
          - ./simple-cmd-here --arg1 value
          - run: cmd here
            save: False
            cwd: ./path/here
          - run:
              - my-executable --arg
              - ./another-executable --arg
            save: True cwd: ./path/here

    If save is True, will save the output to context as follows:
        cmdOut:
            returncode: 0
            stdout: 'stdout str here. None if empty.'
            stderr: 'stderr str here. None if empty.'

    If the cmd input contains a list of executables, cmdOut will be a list of
    cmdOut objects, in order executed.

    cmdOut.returncode is the exit status of the called process. Typically 0
    means OK. A negative value -N indicates that the child was terminated by
    signal N (POSIX only).

    The run_step method does the actual work. init parses the input yaml.

    Attributes:
        logger (logger): Logger instantiated by name of calling step.
        context: (pypyr.context.Context): The current pypyr Context.
        commands (list[pypyr.subproc.Command]): Commands to run as subprocess.
        is_shell (bool): True if subprocess should run through default shell.
        name (str): Name of calling step. Used for logging output & error
            messages.
    """"""

    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        """"""Initialize the CmdStep.

        The step config in the context dict in simplified syntax:
            cmd: <<cmd string>>

        OR, as a dict in expanded syntax:
            cmd:
                run: str. mandatory. command + args to execute.
                save: bool. optional. defaults False. save output to cmdOut.
                cwd: str/path. optional. if specified, change the working
                     directory just for the duration of the command.

        `run` can be a single string, or it can be a list of string if there
        are multiple commands to execute with the same settings.

        OR, as a list:
            cmd:
                - my-executable --arg
                - ./another-executable --arg

        Any or all of the list items can be in expanded syntax.

        Args:
            name (str): Unique name for step. Likely __name__ of calling step.
            context (pypyr.context.Context): Look for step config in this
                context instance.
            is_shell (bool): Set to true to execute cmd through the default
                shell.
        """"""
        assert name, 'name parameter must exist for CmdStep.'
        assert context, 'context param must exist for CmdStep.'
        self.name = name
        self.logger = logging.getLogger(name)
        context.assert_key_has_value(key='cmd', caller=name)
        self.context = context
        self.is_shell = is_shell
        cmd_config = context.get_formatted('cmd')
        commands: list[Command] = []
        if isinstance(cmd_config, SimpleCommandTypes):
            commands.append(Command(cmd_config, is_shell=is_shell))
        elif isinstance(cmd_config, Mapping):
            commands.append(self.create_command(cmd_config))
        elif isinstance(cmd_config, Sequence):
            for cmd in cmd_config:
                if isinstance(cmd, SimpleCommandTypes):
                    commands.append(Command(cmd, is_shell=is_shell))
                elif isinstance(cmd, Mapping):
                    commands.append(self.create_command(cmd))
                else:
                    raise ContextError(f'{cmd} in {name} cmd config is wrong.\nEach list item should be either a simple string or a dict for expanded syntax:\ncmd:\n  - my-executable --arg\n  - run: another-executable --arg value\n    cwd: ../mydir/subdir\n  - run:\n      - arb-executable1 --arg value1\n      - arb-executable2 --arg value2\n    cwd: ../mydir/arbdir')
        else:
            raise ContextError(f'{name} cmd config should be either a simple string:\ncmd: my-executable --arg\n\nor a dictionary:\ncmd:\n  run: subdir/my-executable --arg\n  cwd: ./mydir\n\nor a list of commands:\ncmd:\n  - my-executable --arg\n  - run: another-executable --arg value\n    cwd: ../mydir/subdir')
        self.commands: list[Command] = commands

    def create_command(self, cmd_input: Mapping) -> Command:
        """"""Create a pypyr.subproc.Command object from expanded step input.""""""
        try:
            cmd = cmd_input['run']
            if not cmd:
                raise KeyInContextHasNoValueError(f'cmd.run must have a value for {self.name}.\nThe `run` input should look something like this:\ncmd:\n  run: my-executable-here --arg1\n  cwd: ./mydir/subdir\n\nOr, `run` could be a list of commands:\ncmd:\n  run:\n    - arb-executable1 --arg value1\n    - arb-executable2 --arg value2\n  cwd: ../mydir/arbdir')
        except KeyError as err:
            raise KeyNotInContextError(f""cmd.run doesn't exist for {self.name}.\nThe input should look like this in the simplified syntax:\ncmd: my-executable-here --arg1\n\nOr in the expanded syntax:\ncmd:\n  run: my-executable-here --arg1\n\nIf you're passing in a list of commands, each command should be a simple string,\nor a dict with a `run` entry:\ncmd:\n  - my-executable --arg\n  - run: another-executable --arg value\n    cwd: ../mydir/subdir\n  - run:\n      - arb-executable1 --arg value1\n      - arb-executable2 --arg value2\n    cwd: ../mydir/arbdir"") from err
        is_save = pypyr.utils.types.cast_to_bool(cmd_input.get('save', False))
        cwd = cmd_input.get('cwd')
        is_bytes = cmd_input.get('bytes')
        is_text = not is_bytes if is_save else False
        stdout = cmd_input.get('stdout')
        stderr = cmd_input.get('stderr')
        if is_save:
            if stderr or stderr:
                raise ContextError(""You can't set `stdout` or `stderr` when `save` is True."")
        encoding = cmd_input.get('encoding')
        append = cmd_input.get('append', False)
        is_shell_override = cmd_input.get('shell', None)
        is_shell = self.is_shell if is_shell_override is None else is_shell_override
        return Command(cmd=cmd, is_shell=is_shell, cwd=cwd, is_save=is_save, is_text=is_text, stdout=stdout, stderr=stderr, encoding=encoding, append=append)

    def run_step(self) -> None:
        """"""Spawn a subprocess to run the command or program.

        If cmd.is_save==True, save result of each command to context 'cmdOut'.
        """"""
        results = []
        try:
            for cmd in self.commands:
                try:
                    cmd.run()
                finally:
                    if cmd.results:
                        results.extend(cmd.results)
        finally:
            if results:
                if len(results) == 1:
                    self.context['cmdOut'] = results[0]
                else:
                    self.context['cmdOut'] = results","
class CmdStep:
    '''A pypyr step to run an executable or command as a subprocess.
    This models a step that takes config like this:
        cmd: <<cmd string>>
    OR, expanded syntax is as a dict
        cmd:
            run: str. mandatory. command + args to execute.
            save: bool. defaults False. save output to cmdOut. Treats output
                as text in the system's encoding and removes newlines at end.
            cwd: str/Pathlike. optional. Working directory for this command.
            bytes (bool): Default False. When `save` return output bytes from
                cmd unaltered, without applying any encoding & text newline
                processing.
            encoding (str): Default None. When `save`, decode cmd output with
                this encoding. The default of None uses the system encoding and
                should ""just work"".
            stdout (str | Path): Default None. Write stdout to this file path.
                Special value `/dev/null` writes to the system null device.
            stderr (str | Path): Default None. Write stderr to this file path.
                Special value `/dev/null` writes to the system null device.
                Special value `/dev/stdout` redirects err output to stdout.
            append (bool): Default False. When stdout/stderr a file, append
                rather than overwrite. Default is to overwrite.
    In expanded syntax, `run` can be a simple string or a list:
        cmd:
          run:
            - my-executable --arg
            - cmd here
          save: False
          cwd: ./path/here
    OR, as a list in simplified syntax:
        cmd:
          - my-executable --arg
          - ./another-executable --arg
    Any or all of the list items can use expanded syntax:
        cmd:
          - ./simple-cmd-here --arg1 value
          - run: cmd here
            save: False
            cwd: ./path/here
          - run:
              - my-executable --arg
              - ./another-executable --arg
            save: True cwd: ./path/here
    If save is True, will save the output to context as follows:
        cmdOut:
            returncode: 0
            stdout: 'stdout str here. None if empty.'
            stderr: 'stderr str here. None if empty.'
    If the cmd input contains a list of executables, cmdOut will be a list of
    cmdOut objects, in order executed.
    cmdOut.returncode is the exit status of the called process. Typically 0
    means OK. A negative value -N indicates that the child was terminated by
    signal N (POSIX only).
    The run_step method does the actual work. init parses the input yaml.
    Attributes:
        logger (logger): Logger instantiated by name of calling step.
        context: (pypyr.context.Context): The current pypyr Context.
        commands (list[pypyr.subproc.Command]): Commands to run as subprocess.
        is_shell (bool): True if subprocess should run through default shell.
        name (str): Name of calling step. Used for logging output & error
            messages.
    '''

    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        '''Initialize the CmdStep.
        The step config in the context dict in simplified syntax:
            cmd: <<cmd string>>
        OR, as a dict in expanded syntax:
            cmd:
                run: str. mandatory. command + args to execute.
                save: bool. optional. defaults False. save output to cmdOut.
                cwd: str/path. optional. if specified, change the working
                     directory just for the duration of the command.
        `run` can be a single string, or it can be a list of string if there
        are multiple commands to execute with the same settings.
        OR, as a list:
            cmd:
                - my-executable --arg
                - ./another-executable --arg
        Any or all of the list items can be in expanded syntax.
        Args:
            name (str): Unique name for step. Likely __name__ of calling step.
            context (pypyr.context.Context): Look for step config in this
                context instance.
            is_shell (bool): Set to true to execute cmd through the default
                shell.
        '''
        pass

    def create_command(self, cmd_input: Mapping) -> Command:
        '''Create a pypyr.subproc.Command object from expanded step input.'''
        pass

    def run_step(self) -> None:
        '''Spawn a subprocess to run the command or program.
        If cmd.is_save==True, save result of each command to context 'cmdOut'.
        '''
        pass",4,4,58.0,6.0,42.0,10.0,6.0,0.73,0.0,11.0,5.0,0.0,3.0,5.0,3.0,3.0,249.0,32.0,126.0,29.0,119.0,92.0,54.0,25.0,50.0,7.0,0.0,4.0,19.0,snippet_94
166605,pypyr/pypyr-cli,pypyr_pypyr-cli/pypyr/steps/dsl/cmdasync.py,pypyr.steps.dsl.cmdasync.AsyncCmdStep,"import logging
from pypyr.aio.subproc import Command, Commands
from pypyr.context import Context
import pypyr.utils.types
from collections.abc import Mapping, Sequence
from pypyr.errors import ContextError, KeyInContextHasNoValueError, KeyNotInContextError
from pypyr.subproc import SimpleCommandTypes

class AsyncCmdStep:
    """"""A pypyr step to run executables/commands concurrently as a subprocess.

    This models a step that takes config like this in simple syntax:
        cmds:
            - <<cmd string 1>>
            - <<cmd string 2>>

    All the commands will run concurrently, in parallel.

    OR, expanded syntax is as a dict
        cmds:
            run: list[str | list[str]]. mandatory. command + args to execute.
                If list entry is another list[str], the sub-list will run in
                serial.
            save: bool. defaults False. save output to cmdOut. Treats output
                as text in the system's encoding and removes newlines at end.
            cwd: str/Pathlike. optional. Working directory for these commands.
            bytes (bool): Default False. When `save` return output bytes from
                cmds unaltered, without applying any encoding & text newline
                processing.
            encoding (str): Default None. When `save`, decode output with
                this encoding. The default of None uses the system encoding and
                should ""just work"".
            stdout (str | Path): Default None. Write stdout to this file path.
                Special value `/dev/null` writes to the system null device.
            stderr (str | Path): Default None. Write stderr to this file path.
                Special value `/dev/null` writes to the system null device.
                Special value `/dev/stdout` redirects err output to stdout.
            append (bool): Default False. When stdout/stderr a file, append
                rather than overwrite. Default is to overwrite.

    In expanded syntax, `run` can be a simple string or a list:
        cmds:
          run:
            - ./my-executable --arg
            - [./another-executable --arg, ./arb-executable arghere]
          save: False
          cwd: ./path/here

    As a list in simplified syntax:
        cmds:
          - my-executable --arg
          - ./another-executable --arg

    Any or all of the list items can use expanded syntax:
        cmds:
          - ./simple-cmd-here --arg1 value
          - run: cmd here
            save: False cwd: ./path/here
          - run:
              - my-executable --arg
              - ./another-executable --arg
            save: True
            cwd: ./path/here

    Any of the list items can in turn be a list. A sub-list will run in serial.

    In this example A, B.1 & C will start concurrently. B.2 will only run once
    B.1 is finished.

        cmds:
            - A
            - [B.1, B.2]
            - C

    If save is True, will save the output to context as cmdOut.

    cmdOut will be a list of pypyr.subproc.SubprocessResult objects, in order
    executed.

    SubprocessResult has the following properties:
    cmd: the cmd/args executed
    returncode: 0
    stdout: 'stdout str here. None if empty.'
    stderr: 'stderr str here. None if empty.'

    cmdOut.returncode is the exit status of the called process. Typically 0
    means OK. A negative value -N indicates that the child was terminated by
    signal N (POSIX only).

    The run_step method does the actual work. init parses the input yaml.

    Attributes:
        logger (logger): Logger instantiated by name of calling step.
        context: (pypyr.context.Context): The current pypyr Context.
        commands (pypyr.subproc.Commands): Commands to run as subprocess.
        is_shell (bool): True if subprocess should run through default shell.
        name (str): Name of calling step. Used for logging output & error
            messages.
    """"""

    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        """"""Initialize the CmdStep.

        The step config in the context dict in simplified syntax:
            cmd: <<cmd string>>

        OR, as a dict in expanded syntax:
            cmd:
                run: str. mandatory. command + args to execute.
                save: bool. optional. defaults False. save output to cmdOut.
                cwd: str/path. optional. if specified, change the working
                     directory just for the duration of the command.

        `run` can be a single string, or it can be a list of string if there
        are multiple commands to execute with the same settings.

        OR, as a list:
            cmd:
                - my-executable --arg
                - ./another-executable --arg

        Any or all of the list items can be in expanded syntax.

        Args:
            name (str): Unique name for step. Likely __name__ of calling step.
            context (pypyr.context.Context): Look for step config in this
                context instance.
            is_shell (bool): Set to true to execute cmd through the default
                shell.
        """"""
        assert name, 'name parameter must exist for CmdStep.'
        assert context, 'context param must exist for CmdStep.'
        self.name = name
        self.logger = logging.getLogger(name)
        context.assert_key_has_value(key='cmds', caller=name)
        self.context = context
        self.is_shell = is_shell
        cmd_config = context.get_formatted('cmds')
        commands = Commands()
        if isinstance(cmd_config, SimpleCommandTypes):
            commands.append(Command(cmd_config, is_shell=is_shell))
        elif isinstance(cmd_config, Mapping):
            commands.append(self.create_command(cmd_config))
        elif isinstance(cmd_config, Sequence):
            for cmd in cmd_config:
                if isinstance(cmd, SimpleCommandTypes):
                    commands.append(Command(cmd, is_shell=is_shell))
                elif isinstance(cmd, Sequence):
                    commands.append(Command([cmd], is_shell=is_shell))
                elif isinstance(cmd, Mapping):
                    commands.append(self.create_command(cmd))
                else:
                    raise ContextError(f'{cmd} in {name} cmds config is wrong.\nEach list item should be either a simple string, or a list to run in serial,\nor a dict for expanded syntax:\ncmds:\n  - ./my-executable --arg\n  - run:\n      - ./another-executable --arg value\n      - ./another-executable --arg value2\n    cwd: ../mydir/subdir\n  - run:\n      - ./arb-executable1 --arg value1\n      - [./arb-executable2.1, ./arb-executable2.2]\n    cwd: ../mydir/arbdir\n  - [./arb-executable3.1, ./arb-executable3.2]')
        else:
            raise ContextError(f'{name} cmds config should be either a list:\ncmds:\n  - ./my-executable --arg\n  - subdir/executable --arg1\n\nor a dictionary with a `run` sub-key:\ncmds:\n  run:\n    - ./my-executable --arg\n    - subdir/executable --arg1\n  cwd: ./mydir\n\nAny of the list items in root can be in expanded syntax:\ncmds:\n  - ./my-executable --arg\n  - subdir/executable --arg1\n  - run:\n      - ./arb-executable1 --arg value1\n      - [./arb-executable2.1, ./arb-executable2.2]\n    cwd: ../mydir/subdir\n  - [./arb-executable3.1, ./arb-executable3.2]')
        self.commands: Commands = commands

    def create_command(self, cmd_input: Mapping) -> Command:
        """"""Create pypyr.aio.subproc.Command object from expanded step input.""""""
        try:
            cmd = cmd_input['run']
            if not cmd:
                raise KeyInContextHasNoValueError(f'cmds.run must have a value for {self.name}.\nThe `run` input should look something like this:\ncmds:\n  run:\n    - ./arb-executable1 --arg value1\n    - ./arb-executable2 --arg value2\n  cwd: ../mydir/arbdir')
        except KeyError as err:
            raise KeyNotInContextError(f""cmds.run doesn't exist for {self.name}.\nThe input should look like this in expanded syntax:\ncmds:\n  run:\n    - ./my-executable --arg\n    - subdir/executable --arg1\n  cwd: ./mydir\n\nIf you're passing in a list of commands, each command should be a simple string,\nor a sub-list of commands to run in serial,\nor a dict with a `run` entry:\ncmds:\n  - ./my-executable --arg\n  - run: ./another-executable --arg value\n    cwd: ../mydir/subdir\n  - run:\n      - ./arb-executable1 --arg value1\n      - [./arb-executable2.1, ./arb-executable2.2]\n    cwd: ../mydir/arbdir\n  - [./arb-executable3.1, ./arb-executable3.2]"") from err
        is_save = pypyr.utils.types.cast_to_bool(cmd_input.get('save', False))
        cwd = cmd_input.get('cwd')
        is_bytes = cmd_input.get('bytes')
        is_text = not is_bytes if is_save else False
        stdout = cmd_input.get('stdout')
        stderr = cmd_input.get('stderr')
        if is_save:
            if stderr or stderr:
                raise ContextError(""You can't set `stdout` or `stderr` when `save` is True."")
        encoding = cmd_input.get('encoding')
        append = cmd_input.get('append', False)
        is_shell_override = cmd_input.get('shell', None)
        is_shell = self.is_shell if is_shell_override is None else is_shell_override
        return Command(cmd=cmd, is_shell=is_shell, cwd=cwd, is_save=is_save, is_text=is_text, stdout=stdout, stderr=stderr, encoding=encoding, append=append)

    def run_step(self) -> None:
        """"""Spawn subprocesses to run the commands asynchronously.

        If cmd.is_save==True, save aggregate result of all commands to context
        'cmdOut'.

        cmdOut will be a list of pypyr.subproc.SubprocessResult or Exception
        objects, in order executed.

        SubprocessResult has the following properties:
        cmd: the cmd/args executed
        returncode: 0
        stdout: 'stdout str here. None if empty.'
        stderr: 'stderr str here. None if empty.'
        """"""
        try:
            self.commands.run()
        finally:
            if self.commands.is_save:
                self.logger.debug('saving results to cmdOut')
                self.context['cmdOut'] = self.commands.results
            else:
                self.logger.debug('save is False: not saving results to cmdOut')","
class AsyncCmdStep:
    '''A pypyr step to run executables/commands concurrently as a subprocess.
    This models a step that takes config like this in simple syntax:
        cmds:
            - <<cmd string 1>>
            - <<cmd string 2>>
    All the commands will run concurrently, in parallel.
    OR, expanded syntax is as a dict
        cmds:
            run: list[str | list[str]]. mandatory. command + args to execute.
                If list entry is another list[str], the sub-list will run in
                serial.
            save: bool. defaults False. save output to cmdOut. Treats output
                as text in the system's encoding and removes newlines at end.
            cwd: str/Pathlike. optional. Working directory for these commands.
            bytes (bool): Default False. When `save` return output bytes from
                cmds unaltered, without applying any encoding & text newline
                processing.
            encoding (str): Default None. When `save`, decode output with
                this encoding. The default of None uses the system encoding and
                should ""just work"".
            stdout (str | Path): Default None. Write stdout to this file path.
                Special value `/dev/null` writes to the system null device.
            stderr (str | Path): Default None. Write stderr to this file path.
                Special value `/dev/null` writes to the system null device.
                Special value `/dev/stdout` redirects err output to stdout.
            append (bool): Default False. When stdout/stderr a file, append
                rather than overwrite. Default is to overwrite.
    In expanded syntax, `run` can be a simple string or a list:
        cmds:
          run:
            - ./my-executable --arg
            - [./another-executable --arg, ./arb-executable arghere]
          save: False
          cwd: ./path/here
    As a list in simplified syntax:
        cmds:
          - my-executable --arg
          - ./another-executable --arg
    Any or all of the list items can use expanded syntax:
        cmds:
          - ./simple-cmd-here --arg1 value
          - run: cmd here
            save: False cwd: ./path/here
          - run:
              - my-executable --arg
              - ./another-executable --arg
            save: True
            cwd: ./path/here
    Any of the list items can in turn be a list. A sub-list will run in serial.
    In this example A, B.1 & C will start concurrently. B.2 will only run once
    B.1 is finished.
        cmds:
            - A
            - [B.1, B.2]
            - C
    If save is True, will save the output to context as cmdOut.
    cmdOut will be a list of pypyr.subproc.SubprocessResult objects, in order
    executed.
    SubprocessResult has the following properties:
    cmd: the cmd/args executed
    returncode: 0
    stdout: 'stdout str here. None if empty.'
    stderr: 'stderr str here. None if empty.'
    cmdOut.returncode is the exit status of the called process. Typically 0
    means OK. A negative value -N indicates that the child was terminated by
    signal N (POSIX only).
    The run_step method does the actual work. init parses the input yaml.
    Attributes:
        logger (logger): Logger instantiated by name of calling step.
        context: (pypyr.context.Context): The current pypyr Context.
        commands (pypyr.subproc.Commands): Commands to run as subprocess.
        is_shell (bool): True if subprocess should run through default shell.
        name (str): Name of calling step. Used for logging output & error
            messages.
    '''

    def __init__(self, name: str, context: Context, is_shell: bool=False) -> None:
        '''Initialize the CmdStep.
        The step config in the context dict in simplified syntax:
            cmd: <<cmd string>>
        OR, as a dict in expanded syntax:
            cmd:
                run: str. mandatory. command + args to execute.
                save: bool. optional. defaults False. save output to cmdOut.
                cwd: str/path. optional. if specified, change the working
                     directory just for the duration of the command.
        `run` can be a single string, or it can be a list of string if there
        are multiple commands to execute with the same settings.
        OR, as a list:
            cmd:
                - my-executable --arg
                - ./another-executable --arg
        Any or all of the list items can be in expanded syntax.
        Args:
            name (str): Unique name for step. Likely __name__ of calling step.
            context (pypyr.context.Context): Look for step config in this
                context instance.
            is_shell (bool): Set to true to execute cmd through the default
                shell.
        '''
        pass

    def create_command(self, cmd_input: Mapping) -> Command:
        '''Create pypyr.aio.subproc.Command object from expanded step input.'''
        pass

    def run_step(self) -> None:
        '''Spawn subprocesses to run the commands asynchronously.
        If cmd.is_save==True, save aggregate result of all commands to context
        'cmdOut'.
        cmdOut will be a list of pypyr.subproc.SubprocessResult or Exception
        objects, in order executed.
        SubprocessResult has the following properties:
        cmd: the cmd/args executed
        returncode: 0
        stdout: 'stdout str here. None if empty.'
        stderr: 'stderr str here. None if empty.'
        '''
        pass",4,4,65.0,7.0,45.0,13.0,6.0,0.82,0.0,11.0,6.0,0.0,3.0,5.0,3.0,3.0,288.0,39.0,137.0,27.0,130.0,113.0,50.0,23.0,46.0,8.0,0.0,3.0,17.0,snippet_95
166960,ctuning/ck,ctuning_ck/cm/cmind/artifact.py,cmind.artifact.Artifact,"import os
from cmind import utils

class Artifact:
    """"""
    CM artifact class
    """"""

    def __init__(self, cmind, path):
        """"""
        Initialize CM artifact class

        Args:
            cmind (CM class)
            path (str): path to a CM artifact

        Returns:
            (python class) with the following vars:

            * path (str): path to CM artifact

            * original_meta (dict): original meta description of this artifact without inheritance

            * meta (dict): meta description of this artifact after inheritance


        """"""
        self.cmind = cmind
        self.cfg = cmind.cfg
        self.path = path
        self.original_meta = {}
        self.meta = {}
        self.repo_path = ''
        self.repo_meta = {}

    def load(self, ignore_inheritance=False, base_recursion=0):
        """"""
        Load CM artifact

        Args:    
                 ignore_inheritance (bool): if True ignore artifact meta description inheritance
                 base_recursion (int): internal to avoid infinite recursion during inheritance

        Returns: 
            (CM return dict):

            * return (int): return code == 0 if no error and >0 if error
            * (error) (str): error string if return>0
        """"""
        import copy
        path_artifact_meta = os.path.join(self.path, self.cfg['file_cmeta'])
        r = utils.is_file_json_or_yaml(path_artifact_meta)
        if r['return'] > 0:
            return r
        if not r['is_file']:
            return {'return': 16, 'error': 'CM artifact not found in path {}'.format(self.path)}
        r = utils.load_yaml_and_json(file_name_without_ext=path_artifact_meta)
        if r['return'] > 0:
            r['error'] = ""Can't load artifact meta in path {}: {}"".format(self.path, r['error'])
            return r
        original_meta = r['meta']
        self.original_meta = copy.deepcopy(original_meta)
        meta = original_meta
        if not ignore_inheritance:
            automation_uid = meta.get('automation_uid', '')
            automation_alias = meta.get('automation_alias', '')
            automation = automation_alias
            if automation_uid != '':
                automation += ',' + automation_uid
            r = utils.process_meta_for_inheritance({'automation': automation, 'meta': meta, 'cmind': self.cmind, 'base_recursion': base_recursion})
            if r['return'] > 0:
                return r
            meta = r['meta']
        self.meta = meta
        return {'return': 0}

    def update(self, meta, append_lists=True, replace=False, tags=[]):
        """"""
        Update CM artifact

        Args:
             meta (dict): new meta description
             replace (bool): if True, replace original meta description instead of merging
             append_lists (bool): if True and replace is False, append lists when merging meta descriptions instead of substituting
             tags (list): replace tags in meta

        Returns: 
            (CM return dict):

            * return (int): return code == 0 if no error and >0 if error
            * (error) (str): error string if return>0

        """"""
        from cmind import utils
        current_meta = self.original_meta
        if replace:
            save_info = {}
            for k in ['uid', 'alias', 'automation_uid', 'automation_alias']:
                if k not in meta and k in current_meta:
                    save_info[k] = current_meta[k]
            self.original_meta = meta
            if len(save_info) > 0:
                self.original_meta.update(save_info)
        elif len(meta) > 0:
            r = utils.merge_dicts({'dict1': current_meta, 'dict2': meta, 'append_lists': append_lists})
            if r['return'] > 0:
                return
            self.original_meta = r['dict1']
        if len(tags) > 0:
            self.original_meta['tags'] = utils.filter_tags(tags)
        path_artifact_meta_json = os.path.join(self.path, self.cfg['file_cmeta'] + '.json')
        r = utils.save_json(file_name=path_artifact_meta_json, meta=self.original_meta)
        if r['return'] > 0:
            return r
        r = self.load()
        return r","
class Artifact:
    '''
    CM artifact class
        '''

    def __init__(self, cmind, path):
        '''
        Initialize CM artifact class
        Args:
            cmind (CM class)
            path (str): path to a CM artifact
        Returns:
            (python class) with the following vars:
            * path (str): path to CM artifact
            * original_meta (dict): original meta description of this artifact without inheritance
            * meta (dict): meta description of this artifact after inheritance

        '''
        pass

    def load(self, ignore_inheritance=False, base_recursion=0):
        '''
        Load CM artifact
        Args:    
                 ignore_inheritance (bool): if True ignore artifact meta description inheritance
                 base_recursion (int): internal to avoid infinite recursion during inheritance
        Returns: 
            (CM return dict):
            * return (int): return code == 0 if no error and >0 if error
            * (error) (str): error string if return>0
        '''
        pass

    def update(self, meta, append_lists=True, replace=False, tags=[]):
        '''
        Update CM artifact
        Args:
             meta (dict): new meta description
             replace (bool): if True, replace original meta description instead of merging
             append_lists (bool): if True and replace is False, append lists when merging meta descriptions instead of substituting
             tags (list): replace tags in meta
        Returns: 
            (CM return dict):
            * return (int): return code == 0 if no error and >0 if error
            * (error) (str): error string if return>0
        '''
        pass",4,4,48.0,15.0,19.0,14.0,6.0,0.83,0.0,0.0,0.0,0.0,3.0,7.0,3.0,3.0,152.0,48.0,58.0,25.0,52.0,48.0,59.0,25.0,53.0,9.0,0.0,3.0,17.0,snippet_96
166965,ctuning/ck,ctuning_ck/cm/cmind/repo.py,cmind.repo.Repo,"from cmind import utils
import os

class Repo:
    """"""
    CM repository class
    """"""

    def __init__(self, path, cfg):
        """"""
        Initialize CM repository class

        Args:
            path (str): path to a CM repository
            cfg (dict): CM configuration

        Returns:
            (python class) with the following vars:

            * path (str): path to a CM repository
            * path_prefix (str): use extra directory inside CM repository to keep CM artifacts
            * path_with_prefix (str): path to a CM repository if directory prefix is used

            * meta (dict): CM repository meta description

        """"""
        self.cfg = cfg
        self.path = path
        self.path_with_prefix = path
        self.path_prefix = ''
        self.meta = {}
        self.original_meta = {}

    def load(self, cmx=False):
        """"""
        Load CM repository

        Args:
            None

        Returns: 
            (CM return dict):

            * return (int): return code == 0 if no error and >0 if error
            * (error) (str): error string if return>0

        """"""
        if not os.path.isdir(self.path):
            return {'return': 1, 'error': 'repository path {} not found'.format(self.path)}
        full_path = os.path.join(self.path, self.cfg['file_meta_repo'])
        r = utils.load_yaml_and_json(file_name_without_ext=full_path)
        if r['return'] > 0:
            r['error'] = 'CM repository is broken ({})'.format(r['error'])
            r['return'] = 16
            return r
        self.meta = r['meta']
        self.original_meta = self.meta
        if cmx and self.meta.get('prefix_cmx', '') != '':
            self.path_prefix = self.meta.get('prefix_cmx', '')
        else:
            self.path_prefix = self.meta.get('prefix', '')
        if self.path_prefix is not None and self.path_prefix.strip() != '':
            self.path_with_prefix = os.path.join(self.path, self.path_prefix)
        if os.path.isdir(self.path) and (not os.path.isdir(self.path_with_prefix)):
            os.makedirs(self.path_with_prefix)
        return {'return': 0}","
class Repo:
    '''
    CM repository class
        '''

    def __init__(self, path, cfg):
        '''
        Initialize CM repository class
        Args:
            path (str): path to a CM repository
            cfg (dict): CM configuration
        Returns:
            (python class) with the following vars:
            * path (str): path to a CM repository
            * path_prefix (str): use extra directory inside CM repository to keep CM artifacts
            * path_with_prefix (str): path to a CM repository if directory prefix is used
            * meta (dict): CM repository meta description
        '''
        pass

    def load(self, cmx=False):
        '''
        Load CM repository
        Args:
            None
        Returns: 
            (CM return dict):
            * return (int): return code == 0 if no error and >0 if error
            * (error) (str): error string if return>0
        '''
        pass",3,3,36.0,11.0,14.0,13.0,4.0,1.04,0.0,0.0,0.0,0.0,2.0,6.0,2.0,2.0,79.0,23.0,28.0,11.0,25.0,29.0,27.0,11.0,24.0,6.0,0.0,1.0,7.0,snippet_97
167547,automl/HpBandSter,automl_HpBandSter/hpbandster/optimizers/learning_curve_models/base.py,hpbandster.optimizers.learning_curve_models.base.LCModel,"class LCModel:
    """"""
        base class for simple learning curve models
    """"""

    def fit(self, times, losses, configs=None):
        """"""
            function to train the model on the observed data

            Parameters:
            -----------

            times: list
                list of numpy arrays of the timesteps for each curve
            losses: list
                list of numpy arrays of the loss (the actual learning curve)
            configs: list or None
                list of the configurations for each sample. Each element
                has to be a numpy array. Set to None, if no configuration
                information is available.
        """"""
        raise NotImplementedError()

    def predict_unseen(self, times, config):
        """"""
            predict the loss of an unseen configuration

            Parameters:
            -----------

            times: numpy array
                times where to predict the loss
            config: numpy array
                the numerical representation of the config

            Returns:
            --------

            mean and variance prediction at input times for the given config
        """"""
        raise NotImplementedError()

    def extend_partial(self, times, obs_times, obs_losses, config=None):
        """"""
            extends a partially observed curve

            Parameters:
            -----------

            times: numpy array
                times where to predict the loss
            obs_times: numpy array
                times where the curve has already been observed
            obs_losses: numpy array
                corresponding observed losses
            config: numpy array
                numerical reperesentation of the config; None if no config
                information is available

            Returns:
            --------

            mean and variance prediction at input times


        """"""","class LCModel:
    '''
        base class for simple learning curve models
            '''

    def fit(self, times, losses, configs=None):
        '''
            function to train the model on the observed data
            Parameters:
            -----------
            times: list
                list of numpy arrays of the timesteps for each curve
            losses: list
                list of numpy arrays of the loss (the actual learning curve)
            configs: list or None
                list of the configurations for each sample. Each element
                has to be a numpy array. Set to None, if no configuration
                information is available.
        '''
        pass

    def predict_unseen(self, times, config):
        '''
            predict the loss of an unseen configuration
            Parameters:
            -----------
            times: numpy array
                times where to predict the loss
            config: numpy array
                the numerical representation of the config
            Returns:
            --------
            mean and variance prediction at input times for the given config
        '''
        pass

    def extend_partial(self, times, obs_times, obs_losses, config=None):
        '''
            extends a partially observed curve
            Parameters:
            -----------
            times: numpy array
                times where to predict the loss
            obs_times: numpy array
                times where the curve has already been observed
            obs_losses: numpy array
                corresponding observed losses
            config: numpy array
                numerical reperesentation of the config; None if no config
                information is available
            Returns:
            --------
            mean and variance prediction at input times

        '''
        pass",4,4,20.0,4.0,2.0,14.0,1.0,7.5,0.0,1.0,0.0,0.0,3.0,0.0,3.0,3.0,68.0,17.0,6.0,4.0,2.0,45.0,6.0,4.0,2.0,1.0,0.0,0.0,3.0,snippet_98
167593,pyamg/pyamg,pyamg_pyamg/pyamg/gallery/fem.py,pyamg.gallery.fem.Mesh,"from scipy import sparse
import numpy as np

class Mesh:
    """"""Simple mesh object that holds vertices and mesh functions.""""""

    def __init__(self, V, E, degree=1):
        """"""Initialize mesh.

        Parameters
        ----------
        V : ndarray
            nv x 2 list of coordinates
        E : ndarray
            ne x 3 list of vertices
        degree : int
            Polynomial degree, either 1 or 2

        """"""
        ids = np.full((E.max() + 1,), False)
        ids[E.ravel()] = True
        nv = np.sum(ids)
        if V.shape[0] != nv:
            print('fixing V and E')
            I = np.where(ids)[0]
            J = np.arange(E.max() + 1)
            J[I] = np.arange(nv)
            E = J[E]
            V = V[I, :]
        if not check_mesh(V, E):
            raise ValueError('triangles must be counter clockwise')
        self.V = V
        self.E = E
        self.X = V[:, 0]
        self.Y = V[:, 1]
        self.degree = degree
        self.nv = nv
        self.ne = E.shape[0]
        self.h = diameter(V, E)
        self.V2 = None
        self.E2 = None
        self.Edges = None
        self.newID = None
        if degree == 2:
            self.generate_quadratic()

    def generate_quadratic(self):
        """"""Generate a quadratic mesh.""""""
        if self.V2 is None:
            self.V2, self.E2, self.Edges = generate_quadratic(self.V, self.E, return_edges=True)
            self.X2 = self.V2[:, 0]
            self.Y2 = self.V2[:, 1]
            self.newID = self.nv + np.arange(self.Edges.shape[0])

    def refine(self, levels):
        """"""Refine the mesh.

        Parameters
        ----------
        levels : int
            Number of refinement levels.

        """"""
        self.V2 = None
        self.E2 = None
        self.Edges = None
        self.newID = None
        for _ in range(levels):
            self.V, self.E = refine2dtri(self.V, self.E)
        self.nv = self.V.shape[0]
        self.ne = self.E.shape[0]
        self.h = diameter(self.V, self.E)
        self.X = self.V[:, 0]
        self.Y = self.V[:, 1]
        if self.degree == 2:
            self.generate_quadratic()

    def smooth(self, maxit=10, tol=0.01):
        """"""Constrained Laplacian Smoothing.

        Parameters
        ----------
        maxit : int
            Iterations
        tol : float
            Convergence toleratnce measured in the maximum
            absolute distance the mesh moves (in one iteration).

        """"""
        nv = self.nv
        edge0 = self.E[:, [0, 0, 1, 1, 2, 2]].ravel()
        edge1 = self.E[:, [1, 2, 0, 2, 0, 1]].ravel()
        data = np.ones((edge0.shape[0],), dtype=int)
        G = sparse.coo_array((data, (edge0, edge1)), shape=(nv, nv))
        G.sum_duplicates()
        G.eliminate_zeros()
        bid = np.where(G.data == 1)[0]
        bid = np.unique(G.row[bid])
        G.data[:] = 1
        W = np.array(G.sum(axis=1)).flatten()
        Vnew = self.V.copy()
        edgelength = (Vnew[edge0, 0] - Vnew[edge1, 0]) ** 2 + (Vnew[edge0, 1] - Vnew[edge1, 1]) ** 2
        maxit = 100
        for _it in range(maxit):
            Vnew = G @ Vnew
            Vnew /= W[:, None]
            Vnew[bid, :] = self.V[bid, :]
            newedgelength = np.sqrt((Vnew[edge0, 0] - Vnew[edge1, 0]) ** 2 + (Vnew[edge0, 1] - Vnew[edge1, 1]) ** 2)
            move = np.max(np.abs(newedgelength - edgelength) / newedgelength)
            edgelength = newedgelength
            if move < tol:
                break
        self.V = Vnew
        return _it","
class Mesh:
    '''Simple mesh object that holds vertices and mesh functions.'''

    def __init__(self, V, E, degree=1):
        '''Initialize mesh.
        Parameters
        ----------
        V : ndarray
            nv x 2 list of coordinates
        E : ndarray
            ne x 3 list of vertices
        degree : int
            Polynomial degree, either 1 or 2
        '''
        pass

    def generate_quadratic(self):
        '''Generate a quadratic mesh.'''
        pass

    def refine(self, levels):
        '''Refine the mesh.
        Parameters
        ----------
        levels : int
            Number of refinement levels.
        '''
        pass

    def smooth(self, maxit=10, tol=0.01):
        '''Constrained Laplacian Smoothing.
        Parameters
        ----------
        maxit : int
            Iterations
        tol : float
            Convergence toleratnce measured in the maximum
            absolute distance the mesh moves (in one iteration).
        '''
        pass",5,5,31.0,5.0,19.0,8.0,3.0,0.44,0.0,3.0,0.0,0.0,4.0,14.0,4.0,4.0,134.0,24.0,77.0,36.0,72.0,34.0,74.0,36.0,69.0,4.0,0.0,2.0,12.0,snippet_99
168574,romanz/trezor-agent,romanz_trezor-agent/libagent/age/client.py,libagent.age.client.Client,"from cryptography.hazmat.primitives.kdf.hkdf import HKDF
from cryptography.hazmat.primitives import hashes

class Client:
    """"""Sign messages and get public keys from a hardware device.""""""

    def __init__(self, device):
        """"""C-tor.""""""
        self.device = device

    def pubkey(self, identity, ecdh=False):
        """"""Return public key as VerifyingKey object.""""""
        with self.device:
            pubkey = bytes(self.device.pubkey(ecdh=ecdh, identity=identity))
            assert len(pubkey) == 32
            return pubkey

    def ecdh(self, identity, peer_pubkey):
        """"""Derive shared secret using ECDH from peer public key.""""""
        log.info('please confirm AGE decryption on %s for ""%s""...', self.device, identity.to_string())
        with self.device:
            assert len(peer_pubkey) == 32
            result, self_pubkey = self.device.ecdh_with_pubkey(pubkey=b'@' + peer_pubkey, identity=identity)
            assert result[:1] == b'\x04'
            hkdf = HKDF(algorithm=hashes.SHA256(), length=32, salt=peer_pubkey + self_pubkey, info=b'age-encryption.org/v1/X25519')
            return hkdf.derive(result[1:])","
class Client:
    '''Sign messages and get public keys from a hardware device.'''

    def __init__(self, device):
        '''C-tor.'''
        pass

    def pubkey(self, identity, ecdh=False):
        '''Return public key as VerifyingKey object.'''
        pass

    def ecdh(self, identity, peer_pubkey):
        '''Derive shared secret using ECDH from peer public key.'''
        pass",4,4,8.0,0.0,7.0,1.0,1.0,0.18,0.0,1.0,0.0,0.0,3.0,1.0,3.0,3.0,29.0,3.0,22.0,8.0,18.0,4.0,16.0,8.0,12.0,1.0,0.0,1.0,3.0,snippet_100
168596,romanz/trezor-agent,romanz_trezor-agent/libagent/ssh/__init__.py,libagent.ssh.ClosableNamedTemporaryFile,"import tempfile
import os

class ClosableNamedTemporaryFile:
    """"""Creates a temporary file that is not deleted when the file is closed.

    This allows the file to be opened with an exclusive lock, but used by other programs before
    it is deleted
    """"""

    def __init__(self):
        """"""Create a temporary file.""""""
        self.file = tempfile.NamedTemporaryFile(prefix='trezor-ssh-pubkey-', mode='w', delete=False)
        self.name = self.file.name

    def write(self, buf):
        """"""Write `buf` to the file.""""""
        self.file.write(buf)

    def close(self):
        """"""Closes the file, allowing it to be opened by other programs. Does not delete the file.""""""
        self.file.close()

    def __del__(self):
        """"""Deletes the temporary file.""""""
        try:
            os.unlink(self.file.name)
        except OSError:
            log.warning('Failed to delete temporary file: %s', self.file.name)","
class ClosableNamedTemporaryFile:
    '''Creates a temporary file that is not deleted when the file is closed.
    This allows the file to be opened with an exclusive lock, but used by other programs before
    it is deleted
    '''

    def __init__(self):
        '''Create a temporary file.'''
        pass

    def write(self, buf):
        '''Write `buf` to the file.'''
        pass

    def close(self):
        '''Closes the file, allowing it to be opened by other programs. Does not delete the file.'''
        pass

    def __del__(self):
        '''Deletes the temporary file.'''
        pass",5,5,4.0,0.0,3.0,1.0,1.0,0.62,0.0,1.0,0.0,0.0,4.0,2.0,4.0,4.0,26.0,5.0,13.0,7.0,8.0,8.0,13.0,7.0,8.0,2.0,0.0,1.0,5.0,snippet_101
168603,romanz/trezor-agent,romanz_trezor-agent/libagent/util.py,libagent.util.ExpiringCache,"import time

class ExpiringCache:
    """"""Simple cache with a deadline.""""""

    def __init__(self, seconds, timer=time.time):
        """"""C-tor.""""""
        self.duration = seconds
        self.timer = timer
        self.value = None
        self.set(None)

    def get(self):
        """"""Returns existing value, or None if deadline has expired.""""""
        if self.timer() > self.deadline:
            self.value = None
        return self.value

    def set(self, value):
        """"""Set new value and reset the deadline for expiration.""""""
        self.deadline = self.timer() + self.duration
        self.value = value","
class ExpiringCache:
    '''Simple cache with a deadline.'''

    def __init__(self, seconds, timer=time.time):
        '''C-tor.'''
        pass

    def get(self):
        '''Returns existing value, or None if deadline has expired.'''
        pass

    def set(self, value):
        '''Set new value and reset the deadline for expiration.'''
        pass",4,4,5.0,0.0,4.0,1.0,1.0,0.31,0.0,0.0,0.0,0.0,3.0,4.0,3.0,3.0,20.0,3.0,13.0,8.0,9.0,4.0,13.0,8.0,9.0,2.0,0.0,1.0,4.0,snippet_102
168604,romanz/trezor-agent,romanz_trezor-agent/libagent/util.py,libagent.util.Reader,"import struct
import contextlib

class Reader:
    """"""Read basic type objects out of given stream.""""""

    def __init__(self, stream):
        """"""Create a non-capturing reader.""""""
        self.s = stream
        self._captured = None

    def readfmt(self, fmt):
        """"""Read a specified object, using a struct format string.""""""
        size = struct.calcsize(fmt)
        blob = self.read(size)
        obj, = struct.unpack(fmt, blob)
        return obj

    def read(self, size=None):
        """"""Read `size` bytes from stream.""""""
        blob = self.s.read(size)
        if size is not None and len(blob) < size:
            raise EOFError
        if self._captured:
            self._captured.write(blob)
        return blob

    @contextlib.contextmanager
    def capture(self, stream):
        """"""Capture all data read during this context.""""""
        self._captured = stream
        try:
            yield
        finally:
            self._captured = None","
class Reader:
    '''Read basic type objects out of given stream.'''

    def __init__(self, stream):
        '''Create a non-capturing reader.'''
        pass

    def readfmt(self, fmt):
        '''Read a specified object, using a struct format string.'''
        pass

    def readfmt(self, fmt):
        '''Read `size` bytes from stream.'''
        pass
    @contextlib.contextmanager
    def capture(self, stream):
        '''Capture all data read during this context.'''
        pass",5,5,6.0,0.0,5.0,1.0,2.0,0.22,0.0,1.0,0.0,0.0,4.0,2.0,4.0,4.0,32.0,4.0,23.0,12.0,17.0,5.0,21.0,11.0,16.0,3.0,0.0,1.0,6.0,snippet_103
171012,indygreg/python-zstandard,indygreg_python-zstandard/zstandard/backend_cffi.py,zstandard.backend_cffi.BufferSegment,"class BufferSegment:
    """"""Represents a segment within a ``BufferWithSegments``.

    This type is essentially a reference to N bytes within a
    ``BufferWithSegments``.

    The object conforms to the buffer protocol.
    """"""

    @property
    def offset(self):
        """"""The byte offset of this segment within its parent buffer.""""""
        raise NotImplementedError()

    def __len__(self):
        """"""Obtain the length of the segment, in bytes.""""""
        raise NotImplementedError()

    def tobytes(self):
        """"""Obtain bytes copy of this segment.""""""
        raise NotImplementedError()","class BufferSegment:
    '''Represents a segment within a ``BufferWithSegments``.
    This type is essentially a reference to N bytes within a
    ``BufferWithSegments``.
    The object conforms to the buffer protocol.
    '''
    @property
    def offset(self):
        '''The byte offset of this segment within its parent buffer.'''
        pass

    def __len__(self):
        '''Obtain the length of the segment, in bytes.'''
        pass

    def tobytes(self):
        '''Obtain bytes copy of this segment.'''
        pass",4,4,3.0,0.0,2.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0,3.0,0.0,3.0,3.0,21.0,5.0,8.0,5.0,3.0,8.0,7.0,4.0,3.0,1.0,0.0,0.0,3.0,snippet_104
171015,indygreg/python-zstandard,indygreg_python-zstandard/zstandard/backend_cffi.py,zstandard.backend_cffi.BufferWithSegmentsCollection,"class BufferWithSegmentsCollection:
    """"""A virtual spanning view over multiple BufferWithSegments.

    Instances are constructed from 1 or more :py:class:`BufferWithSegments`
    instances. The resulting object behaves like an ordered sequence whose
    members are the segments within each ``BufferWithSegments``.

    If the object is composed of 2 ``BufferWithSegments`` instances with the
    first having 2 segments and the second have 3 segments, then ``b[0]``
    and ``b[1]`` access segments in the first object and ``b[2]``, ``b[3]``,
    and ``b[4]`` access segments from the second.
    """"""

    def __len__(self):
        """"""The number of segments within all ``BufferWithSegments``.""""""
        raise NotImplementedError()

    def __getitem__(self, i):
        """"""Obtain the ``BufferSegment`` at an offset.""""""
        raise NotImplementedError()","class BufferWithSegmentsCollection:
    '''A virtual spanning view over multiple BufferWithSegments.
    Instances are constructed from 1 or more :py:class:`BufferWithSegments`
    instances. The resulting object behaves like an ordered sequence whose
    members are the segments within each ``BufferWithSegments``.
    If the object is composed of 2 ``BufferWithSegments`` instances with the
    first having 2 segments and the second have 3 segments, then ``b[0]``
    and ``b[1]`` access segments in the first object and ``b[2]``, ``b[3]``,
    and ``b[4]`` access segments from the second.
    '''

    def __len__(self):
        '''The number of segments within all ``BufferWithSegments``.'''
        pass

    def __getitem__(self, i):
        '''Obtain the ``BufferSegment`` at an offset.'''
        pass",3,3,3.0,0.0,2.0,1.0,1.0,2.2,0.0,1.0,0.0,0.0,2.0,0.0,2.0,2.0,20.0,4.0,5.0,3.0,2.0,11.0,5.0,3.0,2.0,1.0,0.0,0.0,2.0,snippet_105
171185,ssalentin/plip,ssalentin_plip/plip/exchange/xml.py,plip.exchange.xml.XMLStorage,"class XMLStorage:
    """"""Generic class for storing XML data from PLIP XML files.""""""

    @staticmethod
    def getdata(tree, location, force_string=False):
        """"""Gets XML data from a specific element and handles types.""""""
        found = tree.xpath('%s/text()' % location)
        if not found:
            return None
        else:
            data = found[0]
        if force_string:
            return data
        if data == 'True':
            return True
        elif data == 'False':
            return False
        else:
            try:
                return int(data)
            except ValueError:
                try:
                    return float(data)
                except ValueError:
                    return data

    @staticmethod
    def getcoordinates(tree, location):
        """"""Gets coordinates from a specific element in PLIP XML""""""
        return tuple((float(x) for x in tree.xpath('.//%s/*/text()' % location)))","class XMLStorage:
    '''Generic class for storing XML data from PLIP XML files.'''
    @staticmethod
    def getdata(tree, location, force_string=False):
        '''Gets XML data from a specific element and handles types.'''
        pass
    @staticmethod
    def getcoordinates(tree, location):
        '''Gets coordinates from a specific element in PLIP XML'''
        pass",3,3,13.0,0.0,11.0,2.0,4.0,0.16,0.0,4.0,0.0,3.0,0.0,0.0,2.0,2.0,31.0,2.0,25.0,7.0,20.0,4.0,20.0,5.0,17.0,7.0,0.0,3.0,8.0,snippet_106
172857,cogeotiff/rio-tiler,rio_tiler/colormap.py,rio_tiler.colormap.ColorMaps,"import attr
from rio_tiler.errors import ColorMapAlreadyRegistered, InvalidColorFormat, InvalidColorMapName, InvalidFormat
import json
import numpy
from typing import Dict, List, Sequence, Tuple, Union
import pathlib
from rio_tiler.types import ColorMapType, DataMaskType, DiscreteColorMapType, GDALColorMapType, IntervalColorMapType

@attr.s(frozen=True)
class ColorMaps:
    """"""Default Colormaps holder.

    Attributes:
        data (dict): colormaps. Defaults to `rio_tiler.colormap.DEFAULTS_CMAPS`.

    """"""
    data: Dict[str, Union[str, pathlib.Path, ColorMapType]] = attr.ib(default=attr.Factory(lambda: DEFAULT_CMAPS_FILES))

    def get(self, name: str) -> ColorMapType:
        """"""Fetch a colormap.

        Args:
            name (str): colormap name.

        Returns
            dict: colormap dictionary.

        """"""
        cmap = self.data.get(name, None)
        if cmap is None:
            raise InvalidColorMapName(f'Invalid colormap name: {name}')
        if isinstance(cmap, (pathlib.Path, str)):
            if isinstance(cmap, str):
                cmap = pathlib.Path(cmap)
            if cmap.suffix == '.npy':
                colormap = numpy.load(cmap)
                assert colormap.shape == (256, 4)
                assert colormap.dtype == numpy.uint8
                cmap_data = {idx: tuple(value) for idx, value in enumerate(colormap)}
            elif cmap.suffix == '.json':
                with cmap.open() as f:
                    cmap_data = json.load(f, object_hook=lambda x: {int(k): parse_color(v) for k, v in x.items()})
                if isinstance(cmap_data, Sequence):
                    cmap_data = [(tuple(inter), parse_color(v)) for inter, v in cmap_data]
            else:
                raise ValueError(f'Not supported {cmap.suffix} extension for ColorMap')
            self.data[name] = cmap_data
            return cmap_data
        return cmap

    def list(self) -> List[str]:
        """"""List registered Colormaps.

        Returns
            list: list of colormap names.

        """"""
        return list(self.data)

    def register(self, custom_cmap: Dict[str, Union[str, pathlib.Path, ColorMapType]], overwrite: bool=False) -> 'ColorMaps':
        """"""Register a custom colormap.

        Args:
            custom_cmap (dict): custom colormap(s) to register.
            overwrite (bool): Overwrite existing colormap with same key. Defaults to False.

        Examples:
            >>> cmap = cmap.register({""acmap"": {0: (0, 0, 0, 0), ...}})

            >>> cmap = cmap.register({""acmap"": ""acmap.npy""})

        """"""
        for name, _ in custom_cmap.items():
            if not overwrite and name in self.data:
                raise ColorMapAlreadyRegistered(f'{name} is already registered. Use force=True to overwrite.')
        return ColorMaps({**self.data, **custom_cmap})","@attr.s(frozen=True)
class ColorMaps:
    '''Default Colormaps holder.
    Attributes:
        data (dict): colormaps. Defaults to `rio_tiler.colormap.DEFAULTS_CMAPS`.
    '''

    def get(self, name: str) -> ColorMapType:
        '''Fetch a colormap.
        Args:
            name (str): colormap name.
        Returns
            dict: colormap dictionary.
        '''
        pass

    def list(self) -> List[str]:
        '''List registered Colormaps.
        Returns
            list: list of colormap names.
        '''
        pass

    def register(self, custom_cmap: Dict[str, Union[str, pathlib.Path, ColorMapType]], overwrite: bool=False) -> 'ColorMaps':
        '''Register a custom colormap.
        Args:
            custom_cmap (dict): custom colormap(s) to register.
            overwrite (bool): Overwrite existing colormap with same key. Defaults to False.
        Examples:
            >>> cmap = cmap.register({""acmap"": {0: (0, 0, 0, 0), ...}})
            >>> cmap = cmap.register({""acmap"": ""acmap.npy""})
        '''
        pass",4,4,27.0,6.0,14.0,7.0,4.0,0.55,0.0,10.0,2.0,0.0,3.0,0.0,3.0,3.0,95.0,23.0,47.0,14.0,39.0,26.0,29.0,9.0,25.0,7.0,0.0,3.0,11.0,snippet_107
174678,ipinfo/python,ipinfo_python/ipinfo/details.py,ipinfo.details.Details,"class Details:
    """"""Encapsulates data for single IP address.""""""

    def __init__(self, details):
        """"""Initialize by settings `details` attribute.""""""
        self.details = details

    def __getattr__(self, attr):
        """"""Return attribute if it exists in details array, else return error.""""""
        if attr not in self.details:
            raise AttributeError(f'{attr} is not a valid attribute of Details')
        return self.details[attr]

    @property
    def all(self):
        """"""Return all details as dict.""""""
        return self.details","class Details:
    '''Encapsulates data for single IP address.'''

    def __init__(self, details):
        '''Initialize by settings `details` attribute.'''
        pass

    def __getattr__(self, attr):
        '''Return attribute if it exists in details array, else return error.'''
        pass
    @property
    def all(self):
        '''Return all details as dict.'''
        pass",4,4,4.0,0.0,3.0,1.0,1.0,0.4,0.0,1.0,0.0,0.0,3.0,1.0,3.0,3.0,18.0,4.0,10.0,6.0,5.0,4.0,9.0,5.0,5.0,2.0,0.0,1.0,4.0,snippet_108
176091,opencobra/cobrapy,opencobra_cobrapy/src/cobra/util/context.py,cobra.util.context.HistoryManager,"from typing import TYPE_CHECKING, Any, Callable, Optional

class HistoryManager:
    """"""
    Define a base context manager.

    It records a list of actions to be taken at a later time.
    This is used to implement context managers that allow temporary
    changes to a `cobra.core.Model`.

    """"""

    def __init__(self, **kwargs) -> None:
        """"""Initialize the class.""""""
        super().__init__(**kwargs)
        self._history = []

    def __call__(self, operation: Callable[[Any], Any]) -> None:
        """"""Add the corresponding operation to the history stack.

        Parameters
        ----------
        operation : callable
            A function to be called at a later time.

        """"""
        self._history.append(operation)

    def reset(self) -> None:
        """"""Trigger executions for all items in the stack in reverse order.""""""
        while self._history:
            entry = self._history.pop()
            entry()

    def size(self) -> int:
        """"""Calculate number of operations on the stack.""""""
        return len(self._history)","
class HistoryManager:
    '''
    Define a base context manager.
    It records a list of actions to be taken at a later time.
    This is used to implement context managers that allow temporary
    changes to a `cobra.core.Model`.
    '''

    def __init__(self, **kwargs) -> None:
        '''Initialize the class.'''
        pass

    def __call__(self, operation: Callable[[Any], Any]) -> None:
        '''Add the corresponding operation to the history stack.
        Parameters
        ----------
        operation : callable
            A function to be called at a later time.
        '''
        pass

    def reset(self) -> None:
        '''Trigger executions for all items in the stack in reverse order.'''
        pass

    def size(self) -> int:
        '''Calculate number of operations on the stack.'''
        pass",5,5,6.0,1.0,3.0,3.0,1.0,1.33,0.0,3.0,0.0,0.0,4.0,1.0,4.0,4.0,36.0,8.0,12.0,7.0,7.0,16.0,12.0,7.0,7.0,2.0,0.0,1.0,5.0,snippet_109
176713,adafruit/Adafruit_Blinka,adafruit_Adafruit_Blinka/src/adafruit_blinka/__init__.py,adafruit_blinka.Enum,"class Enum:
    """"""
    Object supporting CircuitPython-style of static symbols
    as seen with Direction.OUTPUT, Pull.UP
    """"""

    def __repr__(self):
        """"""
        Assumes instance will be found as attribute of own class.
        Returns dot-subscripted path to instance
        (assuming absolute import of containing package)
        """"""
        cls = type(self)
        for key in dir(cls):
            if getattr(cls, key) is self:
                return '{}.{}.{}'.format(cls.__module__, cls.__qualname__, key)
        return repr(self)

    @classmethod
    def iteritems(cls):
        """"""
        Inspects attributes of the class for instances of the class
        and returns as key,value pairs mirroring dict#iteritems
        """"""
        for key in dir(cls):
            val = getattr(cls, key)
            if isinstance(cls, val):
                yield (key, val)","class Enum:
    '''
    Object supporting CircuitPython-style of static symbols
    as seen with Direction.OUTPUT, Pull.UP
    '''

    def __repr__(self):
        '''
        Assumes instance will be found as attribute of own class.
        Returns dot-subscripted path to instance
        (assuming absolute import of containing package)
        '''
        pass
    @classmethod
    def iteritems(cls):
        '''
        Inspects attributes of the class for instances of the class
        and returns as key,value pairs mirroring dict#iteritems
        '''
        pass",3,3,10.0,0.0,6.0,5.0,3.0,1.0,0.0,1.0,0.0,5.0,1.0,0.0,2.0,2.0,28.0,2.0,13.0,8.0,9.0,13.0,12.0,7.0,9.0,3.0,0.0,2.0,6.0,snippet_110
176756,adafruit/Adafruit_Blinka,adafruit_Adafruit_Blinka/src/adafruit_blinka/microcontroller/nova/__init__.py,adafruit_blinka.microcontroller.nova.Connection,"class Connection:
    """"""Connection class""""""
    __instance = None

    @staticmethod
    def getInstance():
        """"""Static access method.""""""
        if Connection.__instance is None:
            Connection()
        return Connection.__instance

    def __init__(self):
        """"""Virtually private constructor.""""""
        if Connection.__instance is not None:
            raise Exception('This class is a singleton!')
        from binhoHostAdapter import binhoHostAdapter
        from binhoHostAdapter import binhoUtilities
        devices = binhoUtilities.listAvailableDevices()
        if len(devices) > 0:
            Connection.__instance = binhoHostAdapter.binhoHostAdapter(devices[0])
        else:
            raise RuntimeError('No Binho Nova found!')","class Connection:
    '''Connection class'''
    @staticmethod
    def getInstance():
        '''Static access method.'''
        pass

    def __init__(self):
        '''Virtually private constructor.'''
        pass",3,3,12.0,2.0,8.0,3.0,3.0,0.32,0.0,2.0,0.0,0.0,1.0,0.0,2.0,2.0,30.0,6.0,19.0,8.0,13.0,6.0,15.0,7.0,10.0,3.0,0.0,1.0,5.0,snippet_111
176891,timofurrer/w1thermsensor,timofurrer_w1thermsensor/src/w1thermsensor/calibration_data.py,w1thermsensor.calibration_data.CalibrationData,"from w1thermsensor.errors import InvalidCalibrationDataError
from dataclasses import dataclass

@dataclass(frozen=True)
class CalibrationData:
    """"""
    This Class represents the data required for calibrating a temperature sensor and houses the
    logic to correct the temperature sensor's raw readings based on the calibration data.

    The method used for this class requires that you collect temperature readings of the low point
    and high point of water in your location with your sensor (this method obviously requires a
    waterproof sensor).

    To gather the low point: Take a large cup and fill it completely with ice and then add water.
    Submerse your temperature sensor ensuring it does not touch the cup.  Wait 2 minutes, and then
    begin polling the temperature sensor.  Once the temperature stabilizes and stays around the
    same value for 30 seconds, record this as your measured_low_point.

    To gather the high point: Bring a pot of water to a rapid boil.  Submerse your temperature
    sensor ensuring it
    does not touch the pot.   Begin polling the sensor.  Once the temperature stabilizes and
    stays around the same
    value for 30 seconds, record this as your measured_high_point.

    To gather the reference high point: The high point changes significantly with air pressure
    (and therefore altitude).  The easiest way to get the reference data for the high point is to
    find out the   elevation of your location and then find the high point at that elevation.
    This is not perfectly accurate, but it is generally close enough for most use cases.
    You can find this data here:
    https://www.engineeringtoolbox.com/high-points-water-altitude-d_1344.html

    To gather the reference low point: The low point of water does not change significantly with
    altitude like the high point does because it does not involve a gas (water vapor) and therefore
    air pressure is not as big of a factor.  Generally speaking the default value of 0.0 is
    accurate enough.

    You MUST provide the measured_low_point, measured_high_point, and reference_high_point in
    Celsius.

    This class is based on:
    https://www.instructables.com/Calibration-of-DS18B20-Sensor-With-Arduino-UNO/
    """"""
    measured_high_point: float
    measured_low_point: float
    reference_high_point: float
    reference_low_point: float = 0.0

    def __post_init__(self):
        """"""
        Validates that the required arguments are set and sanity check that the high points are
        higher than the associated low points.  This method does not sanity check that values make
        sense for high/low point outside of high_point > low_point.
        """"""
        if self.measured_high_point is None:
            raise InvalidCalibrationDataError('Measured high point must be provided.', self.__str__())
        if self.measured_low_point is None:
            raise InvalidCalibrationDataError('Measured low point must be provided.', self.__str__())
        if self.reference_high_point is None:
            raise InvalidCalibrationDataError('Reference high point must be provided.', self.__str__())
        if self.reference_low_point is None:
            raise InvalidCalibrationDataError('Reference low point must not set to None.', self.__str__())
        if self.measured_low_point >= self.measured_high_point:
            raise InvalidCalibrationDataError('Measured low point must be less than measured high point. Did you reverse the ' + 'values?', self.__str__())
        if self.reference_low_point >= self.reference_high_point:
            raise InvalidCalibrationDataError('Reference low point must be less than reference high point.  Did you reverse ' + 'the values?', self.__str__())

    def correct_temperature_for_calibration_data(self, raw_temperature):
        """"""
        Correct the temperature based on the calibration data provided.  This is done by taking
        the raw temperature reading and subtracting out the measured low point, scaling that by the
        scaling factor, and then adding back the reference low point.
        """"""
        reference_range = self.reference_high_point - self.reference_low_point
        measured_range = self.measured_high_point - self.measured_low_point
        scaling_factor = reference_range / measured_range
        return (raw_temperature - self.measured_low_point) * scaling_factor + self.reference_low_point","@dataclass(frozen=True)
class CalibrationData:
    '''
    This Class represents the data required for calibrating a temperature sensor and houses the
    logic to correct the temperature sensor's raw readings based on the calibration data.
    The method used for this class requires that you collect temperature readings of the low point
    and high point of water in your location with your sensor (this method obviously requires a
    waterproof sensor).
    To gather the low point: Take a large cup and fill it completely with ice and then add water.
    Submerse your temperature sensor ensuring it does not touch the cup.  Wait 2 minutes, and then
    begin polling the temperature sensor.  Once the temperature stabilizes and stays around the
    same value for 30 seconds, record this as your measured_low_point.
    To gather the high point: Bring a pot of water to a rapid boil.  Submerse your temperature
    sensor ensuring it
    does not touch the pot.   Begin polling the sensor.  Once the temperature stabilizes and
    stays around the same
    value for 30 seconds, record this as your measured_high_point.
    To gather the reference high point: The high point changes significantly with air pressure
    (and therefore altitude).  The easiest way to get the reference data for the high point is to
    find out the   elevation of your location and then find the high point at that elevation.
    This is not perfectly accurate, but it is generally close enough for most use cases.
    You can find this data here:
    https://www.engineeringtoolbox.com/high-points-water-altitude-d_1344.html
    To gather the reference low point: The low point of water does not change significantly with
    altitude like the high point does because it does not involve a gas (water vapor) and therefore
    air pressure is not as big of a factor.  Generally speaking the default value of 0.0 is
    accurate enough.
    You MUST provide the measured_low_point, measured_high_point, and reference_high_point in
    Celsius.
    This class is based on:
    https://www.instructables.com/Calibration-of-DS18B20-Sensor-With-Arduino-UNO/
    '''

    def __post_init__(self):
        '''
        Validates that the required arguments are set and sanity check that the high points are
        higher than the associated low points.  This method does not sanity check that values make
        sense for high/low point outside of high_point > low_point.
        '''
        pass

    def correct_temperature_for_calibration_data(self, raw_temperature):
        '''
        Correct the temperature based on the calibration data provided.  This is done by taking
        the raw temperature reading and subtracting out the measured low point, scaling that by the
        scaling factor, and then adding back the reference low point.
        '''
        pass",3,3,26.0,3.0,18.0,5.0,4.0,1.0,0.0,1.0,1.0,0.0,2.0,0.0,2.0,2.0,96.0,16.0,40.0,7.0,37.0,40.0,23.0,7.0,20.0,7.0,0.0,1.0,8.0,snippet_112
177578,ottogroup/palladium,ottogroup_palladium/palladium/persistence.py,palladium.persistence.FileLikeIO,"from abc import abstractmethod

class FileLikeIO:
    """"""Used by :class:`FileLike` to access low level file handle
    operations.
    """"""

    @abstractmethod
    def open(self, path, mode='r'):
        """"""Return a file handle

        For normal files, the implementation is:

        ```python
        return open(path, mode)
        ```
        """"""

    @abstractmethod
    def exists(self, path):
        """"""Test whether a path exists

        For normal files, the implementation is:

        ```python
        return os.path.exists(path)
        ```
        """"""

    @abstractmethod
    def remove(self, path):
        """"""Remove a file

        For normal files, the implementation is:

        ```python
        os.remove(path)
        ```
        """"""","
class FileLikeIO:
    '''Used by :class:`FileLike` to access low level file handle
    operations.
    '''
    @abstractmethod
    def open(self, path, mode='r'):
        '''Return a file handle
        For normal files, the implementation is:
        ```python
        return open(path, mode)
        ```
        '''
        pass
    @abstractmethod
    def exists(self, path):
        '''Test whether a path exists
        For normal files, the implementation is:
        ```python
        return os.path.exists(path)
        ```
        '''
        pass
    @abstractmethod
    def remove(self, path):
        '''Remove a file
        For normal files, the implementation is:
        ```python
        os.remove(path)
        ```
        '''
        pass",4,4,9.0,2.0,1.0,6.0,1.0,3.0,0.0,0.0,0.0,3.0,3.0,0.0,3.0,3.0,37.0,9.0,7.0,7.0,0.0,21.0,4.0,4.0,0.0,1.0,0.0,0.0,3.0,snippet_113
179407,python-constraint/python-constraint,python-constraint_python-constraint/constraint/constraints.py,constraint.constraints.Constraint,"from collections.abc import Sequence
from constraint.domain import Unassigned

class Constraint:
    """"""Abstract base class for constraints.""""""

    def __call__(self, variables: Sequence, domains: dict, assignments: dict, forwardcheck=False):
        """"""Perform the constraint checking.

        If the forwardcheck parameter is not false, besides telling if
        the constraint is currently broken or not, the constraint
        implementation may choose to hide values from the domains of
        unassigned variables to prevent them from being used, and thus
        prune the search space.

        Args:
            variables (sequence): :py:class:`Variables` affected by that constraint,
                in the same order provided by the user
            domains (dict): Dictionary mapping variables to their
                domains
            assignments (dict): Dictionary mapping assigned variables to
                their current assumed value
            forwardcheck: Boolean value stating whether forward checking
                should be performed or not

        Returns:
            bool: Boolean value stating if this constraint is currently
            broken or not
        """"""
        return True

    def preProcess(self, variables: Sequence, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Preprocess variable domains.

        This method is called before starting to look for solutions,
        and is used to prune domains with specific constraint logic
        when possible. For instance, any constraints with a single
        variable may be applied on all possible values and removed,
        since they may act on individual values even without further
        knowledge about other assignments.

        Args:
            variables (sequence): Variables affected by that constraint,
                in the same order provided by the user
            domains (dict): Dictionary mapping variables to their
                domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        """"""
        if len(variables) == 1:
            variable = variables[0]
            domain = domains[variable]
            for value in domain[:]:
                if not self(variables, domains, {variable: value}):
                    domain.remove(value)
            constraints.remove((self, variables))
            vconstraints[variable].remove((self, variables))

    def forwardCheck(self, variables: Sequence, domains: dict, assignments: dict, _unassigned=Unassigned):
        """"""Helper method for generic forward checking.

        Currently, this method acts only when there's a single
        unassigned variable.

        Args:
            variables (sequence): Variables affected by that constraint,
                in the same order provided by the user
            domains (dict): Dictionary mapping variables to their
                domains
            assignments (dict): Dictionary mapping assigned variables to
                their current assumed value

        Returns:
            bool: Boolean value stating if this constraint is currently
            broken or not
        """"""
        unassignedvariable = _unassigned
        for variable in variables:
            if variable not in assignments:
                if unassignedvariable is _unassigned:
                    unassignedvariable = variable
                else:
                    break
        else:
            if unassignedvariable is not _unassigned:
                domain = domains[unassignedvariable]
                if domain:
                    for value in domain[:]:
                        assignments[unassignedvariable] = value
                        if not self(variables, domains, assignments):
                            domain.hideValue(value)
                    del assignments[unassignedvariable]
                if not domain:
                    return False
        return True","
class Constraint:
    '''Abstract base class for constraints.'''

    def __call__(self, variables: Sequence, domains: dict, assignments: dict, forwardcheck=False):
        '''Perform the constraint checking.
        If the forwardcheck parameter is not false, besides telling if
        the constraint is currently broken or not, the constraint
        implementation may choose to hide values from the domains of
        unassigned variables to prevent them from being used, and thus
        prune the search space.
        Args:
            variables (sequence): :py:class:`Variables` affected by that constraint,
                in the same order provided by the user
            domains (dict): Dictionary mapping variables to their
                domains
            assignments (dict): Dictionary mapping assigned variables to
                their current assumed value
            forwardcheck: Boolean value stating whether forward checking
                should be performed or not
        Returns:
            bool: Boolean value stating if this constraint is currently
            broken or not
        '''
        pass

    def preProcess(self, variables: Sequence, domains: dict, constraints: list[tuple], vconstraints: dict):
        '''Preprocess variable domains.
        This method is called before starting to look for solutions,
        and is used to prune domains with specific constraint logic
        when possible. For instance, any constraints with a single
        variable may be applied on all possible values and removed,
        since they may act on individual values even without further
        knowledge about other assignments.
        Args:
            variables (sequence): Variables affected by that constraint,
                in the same order provided by the user
            domains (dict): Dictionary mapping variables to their
                domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        '''
        pass

    def forwardCheck(self, variables: Sequence, domains: dict, assignments: dict, _unassigned=Unassigned):
        '''Helper method for generic forward checking.
        Currently, this method acts only when there's a single
        unassigned variable.
        Args:
            variables (sequence): Variables affected by that constraint,
                in the same order provided by the user
            domains (dict): Dictionary mapping variables to their
                domains
            assignments (dict): Dictionary mapping assigned variables to
                their current assumed value
        Returns:
            bool: Boolean value stating if this constraint is currently
            broken or not
        '''
        pass",4,4,30.0,3.0,10.0,17.0,5.0,1.63,0.0,4.0,0.0,13.0,3.0,0.0,3.0,3.0,95.0,11.0,32.0,11.0,28.0,52.0,31.0,11.0,27.0,9.0,0.0,5.0,14.0,snippet_114
179419,python-constraint/python-constraint,python-constraint_python-constraint/constraint/domain.py,constraint.domain.Variable,"class Variable:
    """"""Helper class for variable definition.

    Using this class is optional, since any hashable object,
    including plain strings and integers, may be used as variables.
    """"""

    def __init__(self, name):
        """"""Initialization method.

        Args:
            name (string): Generic variable name for problem-specific
                purposes
        """"""
        self.name = name

    def __repr__(self):
        """"""Represents itself with the name attribute.""""""
        return self.name","class Variable:
    '''Helper class for variable definition.
    Using this class is optional, since any hashable object,
    including plain strings and integers, may be used as variables.
    '''

    def __init__(self, name):
        '''Initialization method.
        Args:
            name (string): Generic variable name for problem-specific
                purposes
        '''
        pass

    def __repr__(self):
        '''Represents itself with the name attribute.'''
        pass",3,3,6.0,1.0,2.0,3.0,1.0,2.0,0.0,0.0,0.0,0.0,2.0,1.0,2.0,2.0,19.0,4.0,5.0,4.0,2.0,10.0,5.0,4.0,2.0,1.0,0.0,0.0,2.0,snippet_115
179425,python-constraint/python-constraint,python-constraint_python-constraint/constraint/solvers.py,constraint.solvers.Solver,"class Solver:
    """"""Abstract base class for solvers.""""""
    requires_pickling = False

    def getSolution(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Return one solution for the given problem.

        Args:
            domains (dict): Dictionary mapping variables to their domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        """"""
        msg = f'{self.__class__.__name__} is an abstract class'
        raise NotImplementedError(msg)

    def getSolutions(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Return all solutions for the given problem.

        Args:
            domains (dict): Dictionary mapping variables to domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        """"""
        msg = f'{self.__class__.__name__} provides only a single solution'
        raise NotImplementedError(msg)

    def getSolutionIter(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        """"""Return an iterator for the solutions of the given problem.

        Args:
            domains (dict): Dictionary mapping variables to domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        """"""
        msg = f""{self.__class__.__name__} doesn't provide iteration""
        raise NotImplementedError(msg)","class Solver:
    '''Abstract base class for solvers.'''

    def getSolution(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        '''Return one solution for the given problem.
        Args:
            domains (dict): Dictionary mapping variables to their domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        '''
        pass

    def getSolutions(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        '''Return all solutions for the given problem.
        Args:
            domains (dict): Dictionary mapping variables to domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        '''
        pass

    def getSolutionIter(self, domains: dict, constraints: list[tuple], vconstraints: dict):
        '''Return an iterator for the solutions of the given problem.
        Args:
            domains (dict): Dictionary mapping variables to domains
            constraints (list): List of pairs of (constraint, variables)
            vconstraints (dict): Dictionary mapping variables to a list
                of constraints affecting the given variables.
        '''
        pass",4,4,11.0,1.0,3.0,7.0,1.0,2.2,0.0,4.0,0.0,4.0,3.0,0.0,3.0,3.0,38.0,6.0,10.0,7.0,6.0,22.0,10.0,7.0,6.0,1.0,0.0,0.0,3.0,snippet_116
179445,maroba/findiff,findiff/pde.py,findiff.pde.PDE,"import numpy as np
from scipy.sparse.linalg import spsolve
import scipy.sparse as sparse

class PDE:
    """"""
    Representation of a partial differential equation.
    """"""

    def __init__(self, lhs, rhs, bcs):
        """"""
        Initializes the PDE.

        You need to specify the left hand side (lhs) in terms of derivatives
        as well as the right hand side in terms of an array.

        Parameters
        ----------
        lhs: FinDiff object or combination of FinDiff objects
            the left hand side of the PDE
        rhs: numpy.ndarray
            the right hand side of the PDE
        bcs: BoundaryConditions
            the boundary conditions for the PDE

        """"""
        self.lhs = lhs
        self.rhs = rhs
        self.bcs = bcs
        self._L = None

    def solve(self):
        """"""
        Solves the PDE.

        Returns
        -------
        out: numpy.ndarray
            Array with the solution of the PDE.
        """"""
        shape = self.bcs.shape
        if self._L is None:
            self._L = self.lhs.matrix(shape)
        L = sparse.lil_matrix(self._L)
        f = self.rhs.reshape(-1, 1)
        nz = list(self.bcs.row_inds())
        L[nz, :] = self.bcs.lhs[nz, :]
        f[nz] = np.array(self.bcs.rhs[nz].toarray()).reshape(-1, 1)
        L = sparse.csr_matrix(L)
        return spsolve(L, f).reshape(shape)","
class PDE:
    '''
    Representation of a partial differential equation.
    '''

    def __init__(self, lhs, rhs, bcs):
        '''
        Initializes the PDE.
        You need to specify the left hand side (lhs) in terms of derivatives
        as well as the right hand side in terms of an array.
        Parameters
        ----------
        lhs: FinDiff object or combination of FinDiff objects
            the left hand side of the PDE
        rhs: numpy.ndarray
            the right hand side of the PDE
        bcs: BoundaryConditions
            the boundary conditions for the PDE
        '''
        pass

    def solve(self):
        '''
        Solves the PDE.
        Returns
        -------
        out: numpy.ndarray
            Array with the solution of the PDE.
        '''
        pass",3,3,23.0,5.0,8.0,11.0,2.0,1.41,0.0,3.0,0.0,0.0,2.0,4.0,2.0,2.0,51.0,11.0,17.0,11.0,14.0,24.0,17.0,11.0,14.0,2.0,0.0,1.0,3.0,snippet_117
185271,HazyResearch/pdftotree,HazyResearch_pdftotree/pdftotree/TreeVisualizer.py,pdftotree.TreeVisualizer.TreeVisualizer,"from wand.drawing import Drawing
from wand.image import Image
from wand.color import Color

class TreeVisualizer:
    """"""
    Object to display bounding boxes on a pdf document
    """"""

    def __init__(self, pdf_file):
        """"""
        :param pdf_path: directory where documents are stored
        :return:
        """"""
        self.pdf_file = pdf_file

    def display_boxes(self, tree, html_path, filename_prefix, alternate_colors=False):
        """"""
        Displays each of the bounding boxes passed in 'boxes' on images of the pdf
        pointed to by pdf_file
        boxes is a list of 5-tuples (page, top, left, bottom, right)
        """"""
        imgs = []
        colors = {'section_header': Color('blue'), 'figure': Color('green'), 'figure_caption': Color('green'), 'table_caption': Color('red'), 'list': Color('yellow'), 'paragraph': Color('gray'), 'table': Color('red'), 'header': Color('brown')}
        for i, page_num in enumerate(tree.keys()):
            img = self.pdf_to_img(page_num)
            draw = Drawing()
            draw.fill_color = Color('rgba(0, 0, 0, 0.0)')
            for clust in tree[page_num]:
                for pnum, pwidth, pheight, top, left, bottom, right in tree[page_num][clust]:
                    draw.stroke_color = colors[clust]
                    draw.rectangle(left=left, top=top, right=right, bottom=bottom)
                    draw.push()
                    draw.font_size = 20
                    draw.font_weight = 10
                    draw.fill_color = colors[clust]
                    if int(left) > 0 and int(top) > 0:
                        draw.text(x=int(left), y=int(top), body=clust)
                    draw.pop()
            draw(img)
            img.save(filename=html_path + filename_prefix + '_page_' + str(i) + '.png')
            imgs.append(img)
        return imgs

    def display_candidates(self, tree, html_path, filename_prefix):
        """"""
        Displays the bounding boxes corresponding to candidates on an image of the pdf
        boxes is a list of 5-tuples (page, top, left, bottom, right)
        """"""
        imgs = self.display_boxes(tree, html_path, filename_prefix, alternate_colors=True)
        return display(*imgs)

    def pdf_to_img(self, page_num, pdf_dim=None):
        """"""
        Converts pdf file into image
        :param pdf_file: path to the pdf file
        :param page_num: page number to convert (index starting at 1)
        :return: wand image object
        """"""
        if not pdf_dim:
            pdf_dim = get_pdf_dim(self.pdf_file)
        page_width, page_height = pdf_dim
        img = Image(filename='{}[{}]'.format(self.pdf_file, page_num - 1))
        img.resize(page_width, page_height)
        return img","
class TreeVisualizer:
    '''
    Object to display bounding boxes on a pdf document
    '''

    def __init__(self, pdf_file):
        '''
        :param pdf_path: directory where documents are stored
        :return:
        '''
        pass

    def display_boxes(self, tree, html_path, filename_prefix, alternate_colors=False):
        '''
        Displays each of the bounding boxes passed in 'boxes' on images of the pdf
        pointed to by pdf_file
        boxes is a list of 5-tuples (page, top, left, bottom, right)
        '''
        pass

    def display_candidates(self, tree, html_path, filename_prefix):
        '''
        Displays the bounding boxes corresponding to candidates on an image of the pdf
        boxes is a list of 5-tuples (page, top, left, bottom, right)
        '''
        pass

    def pdf_to_img(self, page_num, pdf_dim=None):
        '''
        Converts pdf file into image
        :param pdf_file: path to the pdf file
        :param page_num: page number to convert (index starting at 1)
        :return: wand image object
        '''
        pass",5,5,17.0,0.0,12.0,5.0,2.0,0.46,0.0,3.0,0.0,0.0,4.0,1.0,4.0,4.0,74.0,4.0,48.0,16.0,43.0,22.0,35.0,16.0,30.0,5.0,0.0,4.0,9.0,snippet_118
186773,sphinx-gallery/sphinx-gallery,sphinx_gallery/_dummy/__init__.py,sphinx_gallery._dummy.DummyClass,"class DummyClass:
    """"""Dummy class for testing method resolution.""""""

    def run(self):
        """"""Do nothing.""""""
        pass

    @property
    def prop(self):
        """"""Property.""""""
        return 'Property'","class DummyClass:
    '''Dummy class for testing method resolution.'''

    def run(self):
        '''Do nothing.'''
        pass
    @property
    def prop(self):
        '''Property.'''
        pass",3,3,3.0,0.0,2.0,1.0,1.0,0.5,0.0,0.0,0.0,0.0,2.0,0.0,2.0,2.0,11.0,2.0,6.0,4.0,2.0,3.0,5.0,3.0,2.0,1.0,0.0,0.0,2.0,snippet_119
186774,sphinx-gallery/sphinx-gallery,sphinx_gallery/_dummy/nested.py,sphinx_gallery._dummy.nested.NestedDummyClass,"class NestedDummyClass:
    """"""Nested dummy class for testing method resolution.""""""

    def run(self):
        """"""Do nothing.""""""
        pass

    @property
    def prop(self):
        """"""Property.""""""
        return 'Property'","class NestedDummyClass:
    '''Nested dummy class for testing method resolution.'''

    def run(self):
        '''Do nothing.'''
        pass
    @property
    def prop(self):
        '''Property.'''
        pass",3,3,3.0,0.0,2.0,1.0,1.0,0.5,0.0,0.0,0.0,0.0,2.0,0.0,2.0,2.0,11.0,2.0,6.0,4.0,2.0,3.0,5.0,3.0,2.0,1.0,0.0,0.0,2.0,snippet_120
190420,JamesPHoughton/pysd,JamesPHoughton_pysd/pysd/py_backend/external.py,pysd.py_backend.external.Excels,"import numpy as np
from openpyxl import load_workbook
import pandas as pd

class Excels:
    """"""
    Class to save the read Excel files and thus avoid double reading
    """"""
    _Excels, _Excels_opyxl = ({}, {})

    @classmethod
    def read(cls, file_name, tab):
        """"""
        Read the Excel file or return the previously read one
        """"""
        if file_name.joinpath(tab) in cls._Excels:
            return cls._Excels[file_name.joinpath(tab)]
        else:
            read_kwargs = {}
            ext = file_name.suffix.lower()
            if ext in _SPREADSHEET_EXTS:
                read_func = pd.read_excel
                read_kwargs['sheet_name'] = tab
            elif ext == '.csv':
                read_func = pd.read_csv
                if tab and (not tab[0].isalnum()):
                    read_kwargs['sep'] = tab
            else:
                read_func = pd.read_table
                if tab and (not tab[0].isalnum()):
                    read_kwargs['sep'] = tab
            excel = np.array([pd.to_numeric(ex, errors='coerce') for ex in read_func(file_name, header=None, **read_kwargs).values])
            cls._Excels[file_name.joinpath(tab)] = excel
            return excel

    @classmethod
    def read_opyxl(cls, file_name):
        """"""
        Read the Excel file using OpenPyXL or return the previously read one
        """"""
        if file_name in cls._Excels_opyxl:
            return cls._Excels_opyxl[file_name]
        else:
            excel = load_workbook(file_name, read_only=True, data_only=True)
            cls._Excels_opyxl[file_name] = excel
            return excel

    @classmethod
    def clean(cls):
        """"""
        Clean the dictionary of read files
        """"""
        for file in cls._Excels_opyxl.values():
            file.close()
        cls._Excels, cls._Excels_opyxl = ({}, {})","
class Excels:
    '''
    Class to save the read Excel files and thus avoid double reading
    '''
    @classmethod
    def read(cls, file_name, tab):
        '''
        Read the Excel file or return the previously read one
        '''
        pass
    @classmethod
    def read_opyxl(cls, file_name):
        '''
        Read the Excel file using OpenPyXL or return the previously read one
        '''
        pass
    @classmethod
    def clean(cls):
        '''
        Clean the dictionary of read files
        '''
        pass",4,4,16.0,0.0,12.0,5.0,3.0,0.42,0.0,0.0,0.0,0.0,0.0,0.0,3.0,3.0,60.0,3.0,40.0,14.0,33.0,17.0,29.0,11.0,25.0,6.0,0.0,3.0,10.0,snippet_121
222023,brettcannon/gidgethub,brettcannon_gidgethub/gidgethub/sansio.py,gidgethub.sansio.RateLimit,"from typing import Any, Dict, Mapping, Optional, Tuple, Type, Union
import datetime

class RateLimit:
    """"""The rate limit imposed upon the requester.

    The 'limit' attribute specifies the rate of requests per hour the client is
    limited to.

    The 'remaining' attribute specifies how many requests remain within the
    current rate limit that the client can make.

    The reset_datetime attribute is a datetime object representing when
    effectively 'left' resets to 'rate'. The datetime object is timezone-aware
    and set to UTC.

    The boolean value of an instance whether another request can be made. This
    is determined based on whether there are any remaining requests or if the
    reset datetime has passed.
    """"""

    def __init__(self, *, limit: int, remaining: int, reset_epoch: float) -> None:
        """"""Instantiate a RateLimit object.

        The reset_epoch argument should be in seconds since the UTC epoch.
        """"""
        self.limit = limit
        self.remaining = remaining
        self.reset_datetime = datetime.datetime.fromtimestamp(reset_epoch, datetime.timezone.utc)

    def __bool__(self) -> bool:
        """"""True if requests are remaining or the reset datetime has passed.""""""
        if self.remaining > 0:
            return True
        else:
            now = datetime.datetime.now(datetime.timezone.utc)
            return now > self.reset_datetime

    def __str__(self) -> str:
        """"""Provide all details in a reasonable format.""""""
        return f'< {self.remaining:,}/{self.limit:,} until {self.reset_datetime} >'

    @classmethod
    def from_http(cls, headers: Mapping[str, str]) -> Optional['RateLimit']:
        """"""Gather rate limit information from HTTP headers.

        The mapping providing the headers is expected to support lowercase
        keys.  Returns ``None`` if ratelimit info is not found in the headers.
        """"""
        try:
            limit = int(headers['x-ratelimit-limit'])
            remaining = int(headers['x-ratelimit-remaining'])
            reset_epoch = float(headers['x-ratelimit-reset'])
        except KeyError:
            return None
        else:
            return cls(limit=limit, remaining=remaining, reset_epoch=reset_epoch)",,5,5,10.0,1.0,6.0,3.0,2.0,1.04,0.0,7.0,0.0,0.0,3.0,3.0,4.0,4.0,62.0,11.0,25.0,13.0,19.0,26.0,21.0,12.0,16.0,2.0,0.0,1.0,6.0,snippet_122
222265,box/flaky,box_flaky/flaky/multiprocess_string_io.py,flaky.multiprocess_string_io.MultiprocessingStringIO,"import multiprocessing

class MultiprocessingStringIO:
    """"""
    Provide a StringIO-like interface to the multiprocessing ListProxy. The
    multiprocessing ListProxy needs to be instantiated before the flaky plugin
    is configured, so the list is created as a class variable.
    """"""
    _manager = multiprocessing.Manager()
    proxy = _manager.list()

    def getvalue(self):
        """"""
        Shadow the StringIO.getvalue method.
        """"""
        return ''.join((i for i in self.proxy))

    def writelines(self, content_list):
        """"""
        Shadow the StringIO.writelines method. Ingests a list and
        translates that to a string
        """"""
        for item in content_list:
            self.write(item)

    def write(self, content):
        """"""
        Shadow the StringIO.write method.
        """"""
        content.strip('\n')
        self.proxy.append(content)","
class MultiprocessingStringIO:
    '''
    Provide a StringIO-like interface to the multiprocessing ListProxy. The
    multiprocessing ListProxy needs to be instantiated before the flaky plugin
    is configured, so the list is created as a class variable.
        '''

    def getvalue(self):
        '''
        Shadow the StringIO.getvalue method.
        '''
        pass

    def writelines(self, content_list):
        '''
        Shadow the StringIO.writelines method. Ingests a list and
        translates that to a string
        '''
        pass

    def writelines(self, content_list):
        '''
        Shadow the StringIO.write method.
        '''
        pass",4,4,6.0,0.0,3.0,3.0,1.0,1.45,0.0,0.0,0.0,0.0,3.0,0.0,3.0,3.0,31.0,5.0,11.0,7.0,7.0,16.0,11.0,7.0,7.0,2.0,0.0,1.0,4.0,snippet_123
222348,joshspeagle/dynesty,joshspeagle_dynesty/py/dynesty/utils.py,dynesty.utils.DelayTimer,"import time

class DelayTimer:
    """""" Utility class that allows us to detect a certain
    time has passed""""""

    def __init__(self, delay):
        """""" Initialise the time with delay of dt seconds

        Parameters
        ----------

        delay: float
            The number of seconds in the timer
        """"""
        self.delay = delay
        self.last_time = time.time()

    def is_time(self):
        """"""
        Returns true if more than self.dt seconds has passed
        since the initialization or last call of successful is_time()

        Returns
        -------
        ret: bool
             True if specified amout of time has passed since the
             initialization or last successful is_time() call
        """"""
        curt = time.time()
        if curt - self.last_time > self.delay:
            self.last_time = curt
            return True
        return False","
class DelayTimer:
    ''' Utility class that allows us to detect a certain
        time has passed'''

    def __init__(self, delay):
        ''' Initialise the time with delay of dt seconds
        Parameters
        ----------
        delay: float
            The number of seconds in the timer
        '''
        pass

    def is_time(self):
        '''
        Returns true if more than self.dt seconds has passed
        since the initialization or last call of successful is_time()
        Returns
        -------
        ret: bool
             True if specified amout of time has passed since the
             initialization or last successful is_time() call
        '''
        pass",3,3,14.0,2.0,5.0,8.0,2.0,1.7,0.0,0.0,0.0,0.0,2.0,2.0,2.0,2.0,32.0,5.0,10.0,6.0,7.0,17.0,10.0,6.0,7.0,2.0,0.0,1.0,3.0,snippet_124
223977,jilljenn/tryalgo,jilljenn_tryalgo/tryalgo/fenwick.py,tryalgo.fenwick.FenwickMin,"class FenwickMin:
    """"""maintains a tree to allow quick updates and queries
    of a virtual table t
    """"""

    def __init__(self, size):
        """"""stores a table t and allows updates and queries
        of prefix sums in logarithmic time.

        :param size: length of the table
        """"""
        self.s = [float('+inf')] * (size + 1)

    def prefixMin(self, a):
        """"""
        :param int a: index in t, negative a will return infinity
        :returns: min(t[0], ... ,t[a])
        """"""
        i = a + 1
        retval = float('+inf')
        while i > 0:
            retval = min(retval, self.s[i])
            i -= i & -i
        return retval

    def update(self, a, val):
        """"""
        :param int a: index in t
        :param val: a value
        :modifies: sets t[a] to the minimum of t[a] and val
        """"""
        i = a + 1
        while i < len(self.s):
            self.s[i] = min(self.s[i], val)
            i += i & -i","class FenwickMin:
    '''maintains a tree to allow quick updates and queries
    of a virtual table t
    '''

    def __init__(self, size):
        '''stores a table t and allows updates and queries
        of prefix sums in logarithmic time.
        :param size: length of the table
        '''
        pass

    def prefixMin(self, a):
        '''
        :param int a: index in t, negative a will return infinity
        :returns: min(t[0], ... ,t[a])
        '''
        pass

    def update(self, a, val):
        '''
        :param int a: index in t
        :param val: a value
        :modifies: sets t[a] to the minimum of t[a] and val
        '''
        pass",4,4,9.0,0.0,5.0,7.0,2.0,1.6,0.0,1.0,0.0,0.0,3.0,1.0,3.0,3.0,34.0,3.0,15.0,8.0,11.0,24.0,15.0,8.0,11.0,2.0,0.0,1.0,5.0,snippet_125
223981,jilljenn/tryalgo,jilljenn_tryalgo/tryalgo/lowest_common_ancestor.py,tryalgo.lowest_common_ancestor.LowestCommonAncestorRMQ,"from tryalgo.range_minimum_query import RangeMinQuery

class LowestCommonAncestorRMQ:
    """"""Lowest common ancestor data structure using a reduction to
       range minimum query
    """"""

    def __init__(self, graph):
        """"""builds the structure from a given tree

        :param graph: adjacency matrix of a tree
        :complexity: O(n log n), with n = len(graph)
        """"""
        n = len(graph)
        dfs_trace = []
        self.last = [None] * n
        to_visit = [(0, 0, None)]
        succ = [0] * n
        while to_visit:
            level, node, father = to_visit[-1]
            self.last[node] = len(dfs_trace)
            dfs_trace.append((level, node))
            if succ[node] < len(graph[node]) and graph[node][succ[node]] == father:
                succ[node] += 1
            if succ[node] == len(graph[node]):
                to_visit.pop()
            else:
                neighbor = graph[node][succ[node]]
                succ[node] += 1
                to_visit.append((level + 1, neighbor, node))
        self.rmq = RangeMinQuery(dfs_trace, (float('inf'), None))

    def query(self, u, v):
        """""":returns: the lowest common ancestor of u and v
        :complexity: O(log n)
        """"""
        lu = self.last[u]
        lv = self.last[v]
        if lu > lv:
            lu, lv = (lv, lu)
        return self.rmq.range_min(lu, lv + 1)[1]","
class LowestCommonAncestorRMQ:
    '''Lowest common ancestor data structure using a reduction to
       range minimum query
    '''

    def __init__(self, graph):
        '''builds the structure from a given tree
        :param graph: adjacency matrix of a tree
        :complexity: O(n log n), with n = len(graph)
        '''
        pass

    def query(self, u, v):
        ''':returns: the lowest common ancestor of u and v
        :complexity: O(log n)
        '''
        pass",3,3,17.0,1.0,13.0,4.0,3.0,0.41,0.0,2.0,1.0,0.0,2.0,2.0,2.0,2.0,39.0,2.0,27.0,13.0,24.0,11.0,25.0,13.0,22.0,4.0,0.0,2.0,6.0,snippet_126
223982,jilljenn/tryalgo,jilljenn_tryalgo/tryalgo/lowest_common_ancestor.py,tryalgo.lowest_common_ancestor.LowestCommonAncestorShortcuts,"class LowestCommonAncestorShortcuts:
    """"""Lowest common ancestor data structure using shortcuts to ancestors
    """"""

    def __init__(self, prec):
        """"""builds the structure from a given tree

        :param prec: father for every node, with prec[0] = 0
        :assumes: prec[node] < node
        :complexity: O(n log n), with n = len(nodes)
        """"""
        n = len(prec)
        self.level = [None] * n
        self.level[0] = 0
        for u in range(1, n):
            self.level[u] = 1 + self.level[prec[u]]
        depth = log2ceil(max((self.level[u] for u in range(n)))) + 1
        self.anc = [[0] * n for _ in range(depth)]
        for u in range(n):
            self.anc[0][u] = prec[u]
        for k in range(1, depth):
            for u in range(n):
                self.anc[k][u] = self.anc[k - 1][self.anc[k - 1][u]]

    def query(self, u, v):
        """""":returns: the lowest common ancestor of u and v
        :complexity: O(log n)
        """"""
        if self.level[u] > self.level[v]:
            u, v = (v, u)
        depth = len(self.anc)
        for k in range(depth - 1, -1, -1):
            if self.level[u] <= self.level[v] - (1 << k):
                v = self.anc[k][v]
        assert self.level[u] == self.level[v]
        if u == v:
            return u
        for k in range(depth - 1, -1, -1):
            if self.anc[k][u] != self.anc[k][v]:
                u = self.anc[k][u]
                v = self.anc[k][v]
        assert self.anc[0][u] == self.anc[0][v]
        return self.anc[0][u]","class LowestCommonAncestorShortcuts:
    '''Lowest common ancestor data structure using shortcuts to ancestors
    '''

    def __init__(self, prec):
        '''builds the structure from a given tree
        :param prec: father for every node, with prec[0] = 0
        :assumes: prec[node] < node
        :complexity: O(n log n), with n = len(nodes)
        '''
        pass

    def query(self, u, v):
        ''':returns: the lowest common ancestor of u and v
        :complexity: O(log n)
        '''
        pass",3,3,21.0,1.0,15.0,6.0,6.0,0.47,0.0,1.0,0.0,0.0,2.0,2.0,2.0,2.0,45.0,2.0,30.0,11.0,27.0,14.0,30.0,11.0,27.0,7.0,0.0,2.0,12.0,snippet_127
223988,jilljenn/tryalgo,jilljenn_tryalgo/tryalgo/partition_refinement.py,tryalgo.partition_refinement.PartitionRefinement,"class PartitionRefinement:
    """"""This data structure implements an order preserving
    partition with refinements.
    """"""

    def __init__(self, n):
        """"""Start with the partition consisting of the unique class {0,1,..,n-1}
        complexity: O(n) both in time and space
        """"""
        c = PartitionClass()
        self.classes = c
        self.items = [PartitionItem(i, c) for i in range(n)]

    def refine(self, pivot):
        """"""Split every class C in the partition into C intersection pivot
        and C setminus pivot complexity: linear in size of pivot
        """"""
        has_split = []
        for i in pivot:
            if 0 <= i < len(self.items):
                x = self.items[i]
                c = x.theclass
                if not c.split:
                    c.split = PartitionClass(c)
                    if self.classes is c:
                        self.classes = c.split
                    has_split.append(c)
                x.remove()
                x.theclass = c.split
                c.split.append(x)
        for c in has_split:
            c.split = None
            if not c.items:
                c.remove()
                del c

    def tolist(self):
        """"""produce a list representation of the partition
        """"""
        return [[x.val for x in theclass.items] for theclass in self.classes]

    def order(self):
        """"""Produce a flatten list of the partition, ordered by classes
        """"""
        return [x.val for theclass in self.classes for x in theclass.items]","class PartitionRefinement:
    '''This data structure implements an order preserving
    partition with refinements.
    '''

    def __init__(self, n):
        '''Start with the partition consisting of the unique class {0,1,..,n-1}
        complexity: O(n) both in time and space
        '''
        pass

    def refine(self, pivot):
        '''Split every class C in the partition into C intersection pivot
        and C setminus pivot complexity: linear in size of pivot
        '''
        pass

    def tolist(self):
        '''produce a list representation of the partition
        '''
        pass

    def order(self):
        '''Produce a flatten list of the partition, ordered by classes
        '''
        pass",5,5,9.0,0.0,7.0,6.0,3.0,0.89,0.0,3.0,2.0,0.0,4.0,2.0,4.0,4.0,45.0,4.0,28.0,13.0,23.0,25.0,28.0,12.0,23.0,7.0,0.0,4.0,10.0,snippet_128
224105,rigetti/grove,rigetti_grove/grove/alpha/fermion_transforms/fenwick_tree.py,grove.alpha.fermion_transforms.fenwick_tree.FenwickNode,"class FenwickNode:
    """"""Fenwick Tree node.""""""
    parent = None
    children = None
    index = None

    def __init__(self, parent, children, index=None):
        """"""Fenwick Tree node. Single parent and multiple children.

        :param FenwickNode parent: a parent node
        :param list(FenwickNode) children: a list of children nodes
        :param int index: node label
        """"""
        self.children = children
        self.parent = parent
        self.index = index

    def get_ancestors(self):
        """"""Returns a list of ancestors of the node. Ordered from the earliest.

        :return: node's ancestors, ordered from most recent
        :rtype: list(FenwickNode)
        """"""
        node = self
        ancestor_list = []
        while node.parent is not None:
            ancestor_list.append(node.parent)
            node = node.parent
        return ancestor_list","class FenwickNode:
    '''Fenwick Tree node.'''

    def __init__(self, parent, children, index=None):
        '''Fenwick Tree node. Single parent and multiple children.
        :param FenwickNode parent: a parent node
        :param list(FenwickNode) children: a list of children nodes
        :param int index: node label
        '''
        pass

    def get_ancestors(self):
        '''Returns a list of ancestors of the node. Ordered from the earliest.
        :return: node's ancestors, ordered from most recent
        :rtype: list(FenwickNode)
        '''
        pass",3,3,12.0,2.0,6.0,5.0,2.0,0.67,0.0,0.0,0.0,0.0,2.0,0.0,2.0,2.0,30.0,5.0,15.0,8.0,12.0,10.0,15.0,8.0,12.0,2.0,0.0,1.0,3.0,snippet_129
227671,django-auth-ldap/django-auth-ldap,django_auth_ldap/config.py,django_auth_ldap.config._LDAPConfig,"import logging
import ldap.filter
import ldap

class _LDAPConfig:
    """"""
    A private class that loads and caches some global objects.
    """"""
    logger = None
    _ldap_configured = False

    @classmethod
    def get_ldap(cls, global_options=None):
        """"""
        Returns the configured ldap module.
        """"""
        if not cls._ldap_configured and global_options is not None:
            for opt, value in global_options.items():
                ldap.set_option(opt, value)
            cls._ldap_configured = True
        return ldap

    @classmethod
    def get_logger(cls):
        """"""
        Initializes and returns our logger instance.
        """"""
        if cls.logger is None:
            cls.logger = logging.getLogger('django_auth_ldap')
            cls.logger.addHandler(logging.NullHandler())
        return cls.logger","
class _LDAPConfig:
    '''
    A private class that loads and caches some global objects.
        '''
    @classmethod
    def get_ldap(cls, global_options=None):
        '''
        Returns the configured ldap module.
        '''
        pass
    @classmethod
    def get_logger(cls):
        '''
        Initializes and returns our logger instance.
        '''
        pass",3,3,11.0,2.0,6.0,4.0,3.0,0.63,0.0,1.0,0.0,0.0,0.0,0.0,2.0,2.0,33.0,7.0,16.0,8.0,11.0,10.0,14.0,6.0,11.0,3.0,0.0,2.0,5.0,snippet_130
227838,econ-ark/HARK,HARK/utilities.py,HARK.utilities.NullFunc,"import numpy as np

class NullFunc:
    """"""
    A trivial class that acts as a placeholder ""do nothing"" function.
    """"""

    def __call__(self, *args):
        """"""
        Returns meaningless output no matter what the input(s) is.  If no input,
        returns None.  Otherwise, returns an array of NaNs (or a single NaN) of
        the same size as the first input.
        """"""
        if len(args) == 0:
            return None
        else:
            arg = args[0]
            if hasattr(arg, 'shape'):
                return np.zeros_like(arg) + np.nan
            else:
                return np.nan

    def distance(self, other):
        """"""
        Trivial distance metric that only cares whether the other object is also
        an instance of NullFunc.  Intentionally does not inherit from HARKobject
        as this might create dependency problems.

        Parameters
        ----------
        other : any
            Any object for comparison to this instance of NullFunc.

        Returns
        -------
        (unnamed) : float
            The distance between self and other.  Returns 0 if other is also a
            NullFunc; otherwise returns an arbitrary high number.
        """"""
        try:
            if other.__class__ is self.__class__:
                return 0.0
            else:
                return 1000.0
        except:
            return 10000.0","
class NullFunc:
    '''
    A trivial class that acts as a placeholder ""do nothing"" function.
        '''

    def __call__(self, *args):
        '''
        Returns meaningless output no matter what the input(s) is.  If no input,
        returns None.  Otherwise, returns an array of NaNs (or a single NaN) of
        the same size as the first input.
        '''
        pass

    def distance(self, other):
        '''
        Trivial distance metric that only cares whether the other object is also
        an instance of NullFunc.  Intentionally does not inherit from HARKobject
        as this might create dependency problems.
        Parameters
        ----------
        other : any
            Any object for comparison to this instance of NullFunc.
        Returns
        -------
        (unnamed) : float
            The distance between self and other.  Returns 0 if other is also a
            NullFunc; otherwise returns an arbitrary high number.
        '''
        pass",3,3,19.0,1.0,9.0,10.0,3.0,1.22,0.0,0.0,0.0,0.0,2.0,0.0,2.0,2.0,44.0,4.0,18.0,4.0,15.0,22.0,15.0,4.0,12.0,3.0,0.0,2.0,6.0,snippet_131
228145,jcrobak/parquet-python,jcrobak_parquet-python/parquet/__init__.py,parquet.JsonWriter,"import json

class JsonWriter:
    """"""Utility for dumping rows as JSON objects.""""""

    def __init__(self, out):
        """"""Initialize with output destination.""""""
        self._out = out

    def writerow(self, row):
        """"""Write a single row.""""""
        json_text = json.dumps(row)
        if isinstance(json_text, bytes):
            json_text = json_text.decode('utf-8')
        self._out.write(json_text)
        self._out.write(u'\n')","
class JsonWriter:
    '''Utility for dumping rows as JSON objects.'''

    def __init__(self, out):
        '''Initialize with output destination.'''
        pass

    def writerow(self, row):
        '''Write a single row.'''
        pass",3,3,5.0,0.0,4.0,1.0,2.0,0.44,0.0,1.0,0.0,0.0,2.0,1.0,2.0,2.0,14.0,2.0,9.0,5.0,6.0,4.0,9.0,5.0,6.0,2.0,0.0,1.0,3.0,snippet_132
228682,common-workflow-language/cwltool,common-workflow-language_cwltool/cwltool/software_requirements.py,cwltool.software_requirements.DependenciesConfiguration,"import os
from typing import TYPE_CHECKING, Any, Optional, Union, cast
import argparse

class DependenciesConfiguration:
    """"""Dependency configuration class, for RuntimeContext.job_script_provider.""""""

    def __init__(self, args: argparse.Namespace) -> None:
        """"""Initialize.""""""
        self.tool_dependency_dir: Optional[str] = None
        self.dependency_resolvers_config_file: Optional[str] = None
        conf_file = getattr(args, 'beta_dependency_resolvers_configuration', None)
        tool_dependency_dir = getattr(args, 'beta_dependencies_directory', None)
        conda_dependencies = getattr(args, 'beta_conda_dependencies', None)
        if conf_file is not None and os.path.exists(conf_file):
            self.use_tool_dependencies = True
            if tool_dependency_dir is None:
                tool_dependency_dir = os.path.abspath(os.path.dirname(conf_file))
            self.tool_dependency_dir = tool_dependency_dir
            self.dependency_resolvers_config_file = os.path.abspath(conf_file)
        elif conda_dependencies is not None:
            if tool_dependency_dir is None:
                tool_dependency_dir = os.path.abspath('./cwltool_deps')
            self.tool_dependency_dir = tool_dependency_dir
            self.use_tool_dependencies = True
            self.dependency_resolvers_config_file = None
        else:
            self.use_tool_dependencies = False
        if self.tool_dependency_dir and (not os.path.exists(self.tool_dependency_dir)):
            os.makedirs(self.tool_dependency_dir)

    def build_job_script(self, builder: 'Builder', command: list[str]) -> str:
        """"""Use the galaxy-tool-util library to construct a build script.""""""
        ensure_galaxy_lib_available()
        resolution_config_dict = {'use': self.use_tool_dependencies, 'default_base_path': self.tool_dependency_dir}
        app_config = {'conda_auto_install': True, 'conda_auto_init': True, 'debug': builder.debug}
        tool_dependency_manager: 'deps.DependencyManager' = deps.build_dependency_manager(app_config_dict=app_config, resolution_config_dict=resolution_config_dict, conf_file=self.dependency_resolvers_config_file)
        handle_dependencies: str = ''
        if (dependencies := get_dependencies(builder)):
            handle_dependencies = '\n'.join(tool_dependency_manager.dependency_shell_commands(dependencies, job_directory=builder.tmpdir))
        template_kwds: dict[str, str] = dict(handle_dependencies=handle_dependencies)
        job_script = COMMAND_WITH_DEPENDENCIES_TEMPLATE.substitute(template_kwds)
        return job_script","
class DependenciesConfiguration:
    '''Dependency configuration class, for RuntimeContext.job_script_provider.'''

    def __init__(self, args: argparse.Namespace) -> None:
        '''Initialize.'''
        pass

    def build_job_script(self, builder: 'Builder', command: list[str]) -> str:
        '''Use the galaxy-tool-util library to construct a build script.'''
        pass",3,3,26.0,1.0,24.0,1.0,4.0,0.06,0.0,4.0,0.0,0.0,2.0,3.0,2.0,2.0,55.0,3.0,49.0,16.0,46.0,3.0,32.0,15.0,29.0,6.0,0.0,2.0,8.0,snippet_133
229565,sourceperl/pyModbusTCP,sourceperl_pyModbusTCP/examples/client_serial_gw.py,client_serial_gw.Serial2ModbusClient,"from pyModbusTCP.constants import EXP_GATEWAY_TARGET_DEVICE_FAILED_TO_RESPOND
import struct

class Serial2ModbusClient:
    """""" Customize a slave serial worker for map a modbus TCP client. """"""

    def __init__(self, serial_w, mbus_cli, slave_addr=1, allow_bcast=False):
        """"""Serial2ModbusClient constructor.

        :param serial_w: a SlaveSerialWorker instance
        :type serial_w: SlaveSerialWorker
        :param mbus_cli: a ModbusClient instance
        :type mbus_cli: ModbusClient
        :param slave_addr: modbus slave address
        :type slave_addr: int
        :param allow_bcast: allow processing broadcast frames (slave @0)
        :type allow_bcast: bool
        """"""
        self.serial_w = serial_w
        self.mbus_cli = mbus_cli
        self.slave_addr = slave_addr
        self.allow_bcast = allow_bcast
        self.serial_w.handle_request = self._handle_request

    def _handle_request(self):
        """"""Request handler for SlaveSerialWorker""""""
        if self.serial_w.request.slave_addr == 0 and self.allow_bcast:
            self.mbus_cli.custom_request(self.serial_w.request.pdu)
        elif self.serial_w.request.slave_addr == self.slave_addr:
            resp_pdu = self.mbus_cli.custom_request(self.serial_w.request.pdu)
            if resp_pdu:
                self.serial_w.response.build(raw_pdu=resp_pdu, slave_addr=self.serial_w.request.slave_addr)
            else:
                exp_pdu = struct.pack('BB', self.serial_w.request.function_code + 128, EXP_GATEWAY_TARGET_DEVICE_FAILED_TO_RESPOND)
                self.serial_w.response.build(raw_pdu=exp_pdu, slave_addr=self.serial_w.request.slave_addr)

    def run(self):
        """"""Start serial processing.""""""
        self.serial_w.run()","
class Serial2ModbusClient:
    ''' Customize a slave serial worker for map a modbus TCP client. '''

    def __init__(self, serial_w, mbus_cli, slave_addr=1, allow_bcast=False):
        '''Serial2ModbusClient constructor.
        :param serial_w: a SlaveSerialWorker instance
        :type serial_w: SlaveSerialWorker
        :param mbus_cli: a ModbusClient instance
        :type mbus_cli: ModbusClient
        :param slave_addr: modbus slave address
        :type slave_addr: int
        :param allow_bcast: allow processing broadcast frames (slave @0)
        :type allow_bcast: bool
        '''
        pass

    def _handle_request(self):
        '''Request handler for SlaveSerialWorker'''
        pass

    def run(self):
        '''Start serial processing.'''
        pass",4,4,13.0,0.0,6.0,7.0,2.0,1.05,0.0,0.0,0.0,0.0,3.0,4.0,3.0,3.0,45.0,4.0,20.0,10.0,16.0,21.0,17.0,10.0,13.0,4.0,0.0,2.0,6.0,snippet_134
230113,aws/aws-xray-sdk-python,aws_aws-xray-sdk-python/aws_xray_sdk/core/models/noop_traceid.py,aws_aws-xray-sdk-python.aws_xray_sdk.core.models.noop_traceid.NoOpTraceId,"class NoOpTraceId:
    """"""
    A trace ID tracks the path of a request through your application.
    A trace collects all the segments generated by a single request.
    A trace ID is required for a segment.
    """"""
    VERSION = '1'
    DELIMITER = '-'

    def __init__(self):
        """"""
        Generate a no-op trace id.
        """"""
        self.start_time = '00000000'
        self.__number = '000000000000000000000000'

    def to_id(self):
        """"""
        Convert TraceId object to a string.
        """"""
        return '%s%s%s%s%s' % (NoOpTraceId.VERSION, NoOpTraceId.DELIMITER, self.start_time, NoOpTraceId.DELIMITER, self.__number)","class NoOpTraceId:
    '''
    A trace ID tracks the path of a request through your application.
    A trace collects all the segments generated by a single request.
    A trace ID is required for a segment.
    '''

    def __init__(self):
        '''
        Generate a no-op trace id.
        '''
        pass

    def to_id(self):
        '''
        Convert TraceId object to a string.
        '''
        pass",3,3,7.0,0.0,4.0,3.0,1.0,1.1,0.0,0.0,0.0,0.0,2.0,2.0,2.0,2.0,23.0,2.0,10.0,7.0,7.0,11.0,8.0,7.0,5.0,1.0,0.0,0.0,2.0,snippet_135
230120,aws/aws-xray-sdk-python,aws_aws-xray-sdk-python/aws_xray_sdk/core/models/traceid.py,aws_aws-xray-sdk-python.aws_xray_sdk.core.models.traceid.TraceId,"import binascii
import os
import time

class TraceId:
    """"""
    A trace ID tracks the path of a request through your application.
    A trace collects all the segments generated by a single request.
    A trace ID is required for a segment.
    """"""
    VERSION = '1'
    DELIMITER = '-'

    def __init__(self):
        """"""
        Generate a random trace id.
        """"""
        self.start_time = int(time.time())
        self.__number = binascii.b2a_hex(os.urandom(12)).decode('utf-8')

    def to_id(self):
        """"""
        Convert TraceId object to a string.
        """"""
        return '%s%s%s%s%s' % (TraceId.VERSION, TraceId.DELIMITER, format(self.start_time, 'x'), TraceId.DELIMITER, self.__number)","
class TraceId:
    '''
    A trace ID tracks the path of a request through your application.
    A trace collects all the segments generated by a single request.
    A trace ID is required for a segment.
    '''

    def __init__(self):
        '''
        Generate a random trace id.
        '''
        pass

    def to_id(self):
        '''
        Convert TraceId object to a string.
        '''
        pass",3,3,7.0,0.0,4.0,3.0,1.0,1.1,0.0,1.0,0.0,0.0,2.0,2.0,2.0,2.0,23.0,2.0,10.0,7.0,7.0,11.0,8.0,7.0,5.0,1.0,0.0,0.0,2.0,snippet_136
230123,aws/aws-xray-sdk-python,aws_aws-xray-sdk-python/aws_xray_sdk/core/sampling/local/reservoir.py,aws_aws-xray-sdk-python.aws_xray_sdk.core.sampling.local.reservoir.Reservoir,"import time
import threading

class Reservoir:
    """"""
    Keeps track of the number of sampled segments within
    a single second. This class is implemented to be
    thread-safe to achieve accurate sampling.
    """"""

    def __init__(self, traces_per_sec=0):
        """"""
        :param int traces_per_sec: number of guranteed
            sampled segments.
        """"""
        self._lock = threading.Lock()
        self.traces_per_sec = traces_per_sec
        self.used_this_sec = 0
        self.this_sec = int(time.time())

    def take(self):
        """"""
        Returns True if there are segments left within the
        current second, otherwise return False.
        """"""
        with self._lock:
            now = int(time.time())
            if now != self.this_sec:
                self.used_this_sec = 0
                self.this_sec = now
            if self.used_this_sec >= self.traces_per_sec:
                return False
            self.used_this_sec = self.used_this_sec + 1
            return True","
class Reservoir:
    '''
    Keeps track of the number of sampled segments within
    a single second. This class is implemented to be
    thread-safe to achieve accurate sampling.
    '''

    def __init__(self, traces_per_sec=0):
        '''
        :param int traces_per_sec: number of guranteed
            sampled segments.
        '''
        pass

    def take(self):
        '''
        Returns True if there are segments left within the
        current second, otherwise return False.
        '''
        pass",3,3,13.0,2.0,8.0,4.0,2.0,0.81,0.0,1.0,0.0,0.0,2.0,4.0,2.0,2.0,33.0,4.0,16.0,8.0,13.0,13.0,16.0,8.0,13.0,3.0,0.0,2.0,4.0,snippet_137
230156,aws/aws-xray-sdk-python,aws_aws-xray-sdk-python/aws_xray_sdk/sdk_config.py,aws_aws-xray-sdk-python.aws_xray_sdk.sdk_config.SDKConfig,"import os

class SDKConfig:
    """"""
    Global Configuration Class that defines SDK-level configuration properties.

    Enabling/Disabling the SDK:
        By default, the SDK is enabled unless if an environment variable AWS_XRAY_SDK_ENABLED
            is set. If it is set, it needs to be a valid string boolean, otherwise, it will default
            to true. If the environment variable is set, all calls to set_sdk_enabled() will
            prioritize the value of the environment variable.
        Disabling the SDK affects the recorder, patcher, and middlewares in the following ways:
        For the recorder, disabling automatically generates DummySegments for subsequent segments
            and DummySubsegments for subsegments created and thus not send any traces to the daemon.
        For the patcher, module patching will automatically be disabled. The SDK must be disabled
            before calling patcher.patch() method in order for this to function properly.
        For the middleware, no modification is made on them, but since the recorder automatically
            generates DummySegments for all subsequent calls, they will not generate segments/subsegments
            to be sent.

    Environment variables:
        ""AWS_XRAY_SDK_ENABLED"" - If set to 'false' disables the SDK and causes the explained above
            to occur.
    """"""
    XRAY_ENABLED_KEY = 'AWS_XRAY_SDK_ENABLED'
    DISABLED_ENTITY_NAME = 'dummy'
    __SDK_ENABLED = None

    @classmethod
    def __get_enabled_from_env(cls):
        """"""
        Searches for the environment variable to see if the SDK should be disabled.
        If no environment variable is found, it returns True by default.

        :return: bool - True if it is enabled, False otherwise.
        """"""
        env_var_str = os.getenv(cls.XRAY_ENABLED_KEY, 'true').lower()
        if env_var_str in ('y', 'yes', 't', 'true', 'on', '1'):
            return True
        elif env_var_str in ('n', 'no', 'f', 'false', 'off', '0'):
            return False
        else:
            log.warning('Invalid literal passed into environment variable `AWS_XRAY_SDK_ENABLED`. Defaulting to True...')
            return True

    @classmethod
    def sdk_enabled(cls):
        """"""
        Returns whether the SDK is enabled or not.
        """"""
        if cls.__SDK_ENABLED is None:
            cls.__SDK_ENABLED = cls.__get_enabled_from_env()
        return cls.__SDK_ENABLED

    @classmethod
    def set_sdk_enabled(cls, value):
        """"""
        Modifies the enabled flag if the ""AWS_XRAY_SDK_ENABLED"" environment variable is not set,
        otherwise, set the enabled flag to be equal to the environment variable. If the
        env variable is an invalid string boolean, it will default to true.

        :param bool value: Flag to set whether the SDK is enabled or disabled.

        Environment variables AWS_XRAY_SDK_ENABLED overrides argument value.
        """"""
        if cls.XRAY_ENABLED_KEY in os.environ:
            cls.__SDK_ENABLED = cls.__get_enabled_from_env()
        elif type(value) == bool:
            cls.__SDK_ENABLED = value
        else:
            cls.__SDK_ENABLED = True
            log.warning('Invalid parameter type passed into set_sdk_enabled(). Defaulting to True...')","
class SDKConfig:
    '''
    Global Configuration Class that defines SDK-level configuration properties.
    Enabling/Disabling the SDK:
        By default, the SDK is enabled unless if an environment variable AWS_XRAY_SDK_ENABLED
            is set. If it is set, it needs to be a valid string boolean, otherwise, it will default
            to true. If the environment variable is set, all calls to set_sdk_enabled() will
            prioritize the value of the environment variable.
        Disabling the SDK affects the recorder, patcher, and middlewares in the following ways:
        For the recorder, disabling automatically generates DummySegments for subsequent segments
            and DummySubsegments for subsegments created and thus not send any traces to the daemon.
        For the patcher, module patching will automatically be disabled. The SDK must be disabled
            before calling patcher.patch() method in order for this to function properly.
        For the middleware, no modification is made on them, but since the recorder automatically
            generates DummySegments for all subsequent calls, they will not generate segments/subsegments
            to be sent.
    Environment variables:
        ""AWS_XRAY_SDK_ENABLED"" - If set to 'false' disables the SDK and causes the explained above
            to occur.
    '''
    @classmethod
    def __get_enabled_from_env(cls):
        '''
        Searches for the environment variable to see if the SDK should be disabled.
        If no environment variable is found, it returns True by default.
        :return: bool - True if it is enabled, False otherwise.
        '''
        pass
    @classmethod
    def sdk_enabled(cls):
        '''
        Returns whether the SDK is enabled or not.
        '''
        pass
    @classmethod
    def set_sdk_enabled(cls, value):
        '''
        Modifies the enabled flag if the ""AWS_XRAY_SDK_ENABLED"" environment variable is not set,
        otherwise, set the enabled flag to be equal to the environment variable. If the
        env variable is an invalid string boolean, it will default to true.
        :param bool value: Flag to set whether the SDK is enabled or disabled.
        Environment variables AWS_XRAY_SDK_ENABLED overrides argument value.
        '''
        pass",4,4,14.0,1.0,7.0,6.0,3.0,1.24,0.0,2.0,0.0,0.0,0.0,0.0,3.0,3.0,73.0,9.0,29.0,11.0,22.0,36.0,22.0,8.0,18.0,3.0,0.0,2.0,8.0,snippet_138
230719,django-salesforce/django-salesforce,django-salesforce_django-salesforce/salesforce/backend/indep.py,salesforce.backend.indep.LazyField,"from typing import Any, Callable, Dict, Tuple, Type
from inspect import signature

class LazyField:
    """"""A Field that can be later customized until it is binded to the final Model""""""
    counter = 0

    def __init__(self, klass: 'Type[Field[Any, Any]]') -> None:
        """"""Instantiate the field type""""""
        self.klass = klass
        self.kw = {}
        self.args = ()
        self.called = False
        self.counter = self.counter

    def __call__(self, *args: Any, **kwargs: Any) -> 'LazyField':
        """"""Instantiate a new field with options""""""
        assert not self.called
        bound_args = signature(self.klass.__init__).bind(self, *args, **kwargs)
        obj = type(self)(self.klass)
        obj.args = bound_args.args[1:]
        obj.kw = bound_args.kwargs
        setattr(type(self), 'counter', getattr(type(self), 'counter') + 1)
        return obj

    def update(self, **kwargs: Any) -> 'LazyField':
        """"""Customize the lazy field""""""
        assert not self.called
        self.kw.update(kwargs)
        return self

    def create(self) -> 'Field[Any, Any]':
        """"""Create a normal field from the lazy field""""""
        assert not self.called
        return self.klass(*self.args, **self.kw)","
class LazyField:
    '''A Field that can be later customized until it is binded to the final Model'''

    def __init__(self, klass: 'Type[Field[Any, Any]]') -> None:
        '''Instantiate the field type'''
        pass

    def __call__(self, *args: Any, **kwargs: Any) -> 'LazyField':
        '''Instantiate a new field with options'''
        pass

    def update(self, **kwargs: Any) -> 'LazyField':
        '''Customize the lazy field'''
        pass

    def create(self) -> 'Field[Any, Any]':
        '''Create a normal field from the lazy field'''
        pass",5,5,7.0,0.0,5.0,2.0,1.0,0.48,0.0,2.0,0.0,0.0,4.0,4.0,4.0,4.0,37.0,5.0,23.0,12.0,18.0,11.0,23.0,12.0,18.0,1.0,0.0,0.0,4.0,snippet_139
231223,weblyzard/inscriptis,benchmarking/run_benchmarking.py,run_benchmarking.AbstractHtmlConverter,"from time import time

class AbstractHtmlConverter:
    """"""
    An abstract HTML convert class.
    """"""

    def get_text(self, html):
        """"""
        Returns:
            a text representation of the given HTML snippet.
        """"""
        raise NotImplementedError

    def benchmark(self, html):
        """"""
        Benchmarks the classes HTML to text converter.

        Returns:
            A tuple of the required time and the obtained text representation.
        """"""
        start_time = time()
        for _ in range(TRIES):
            text = self.get_text(html)
        return (time() - start_time, text)","
class AbstractHtmlConverter:
    '''
    An abstract HTML convert class.
        '''

    def get_text(self, html):
        '''
        Returns:
            a text representation of the given HTML snippet.
        '''
        pass

    def benchmark(self, html):
        '''
        Benchmarks the classes HTML to text converter.
        Returns:
            A tuple of the required time and the obtained text representation.
        '''
        pass",3,3,9.0,1.0,4.0,5.0,2.0,1.5,0.0,2.0,0.0,6.0,2.0,0.0,2.0,2.0,23.0,3.0,8.0,6.0,5.0,12.0,8.0,6.0,5.0,2.0,0.0,1.0,3.0,snippet_140
236113,richardchien/python-aiocqhttp,richardchien_python-aiocqhttp/aiocqhttp/api.py,aiocqhttp.api.Api,"from typing import Callable, Any, Union, Awaitable
import functools
import abc

class Api:
    """"""
    API 接口类。

    继承此类的具体实现类应实现 `call_action` 方法。
    """"""

    @abc.abstractmethod
    def call_action(self, action: str, **params) -> Union[Awaitable[Any], Any]:
        """"""
        调用 OneBot API，`action` 为要调用的 API 动作名，`**params`
        为 API 所需参数。

        根据实现类的不同，此函数可能是异步也可能是同步函数。
        """"""
        pass

    def __getattr__(self, item: str) -> Callable[..., Union[Awaitable[Any], Any]]:
        """"""获取一个可调用对象，用于调用对应 API。""""""
        return functools.partial(self.call_action, item)","
class Api:
    '''
    API 接口类。
    继承此类的具体实现类应实现 `call_action` 方法。
    '''
    @abc.abstractmethod
    def call_action(self, action: str, **params) -> Union[Awaitable[Any], Any]:
        '''
        调用 OneBot API，`action` 为要调用的 API 动作名，`**params`
        为 API 所需参数。
        根据实现类的不同，此函数可能是异步也可能是同步函数。
        '''
        pass

    def __getattr__(self, item: str) -> Callable[..., Union[Awaitable[Any], Any]]:
        '''获取一个可调用对象，用于调用对应 API。'''
        pass",3,3,6.0,1.0,3.0,3.0,1.0,1.43,0.0,3.0,0.0,3.0,2.0,0.0,2.0,2.0,21.0,4.0,7.0,5.0,2.0,10.0,5.0,3.0,2.0,1.0,0.0,0.0,2.0,snippet_141
237136,project-generator/project_generator,project-generator_project_generator/project_generator/project.py,project_generator.project.ProjectTemplate,"class ProjectTemplate:
    """""" Public data which can be set in yaml files
        Yaml data available are:
            'build_dir' : build_dir,    # Build output path
            'debugger' : debugger,      # Debugger
            'export_dir': '',           # Export directory path
            'includes': [],             # include paths
            'linker_file': None,        # linker script file
            'name': name,               # project name
            'macros': [],               # macros
            'misc': {},                 # misc settings related to tools
            'output_type': output_type, # output type, default - exe
            'sources': [],              # source files/folders
            'target': '',               # target
            'template' : [],            # tool template
            'tools_supported': [],      # Tools which are supported,
    """"""

    @staticmethod
    def _get_common_data_template():
        """""" Data for tool specific """"""
        data_template = {'includes': [], 'linker_file': '', 'macros': [], 'sources': []}
        return data_template

    @staticmethod
    def _get_tool_specific_data_template():
        """""" Data for tool specific """"""
        data_template = {'misc': {}, 'template': []}
        return data_template

    @staticmethod
    def get_project_template(name='Default', output_type='exe', debugger=None, build_dir='build'):
        """""" Project data (+ data) """"""
        project_template = {'build_dir': build_dir, 'debugger': debugger, 'export_dir': '', 'name': name, 'output_type': output_type, 'target': '', 'tools_supported': []}
        project_template.update(ProjectTemplate._get_common_data_template())
        project_template.update(ProjectTemplate._get_tool_specific_data_template())
        return project_template","class ProjectTemplate:
    ''' Public data which can be set in yaml files
        Yaml data available are:
            'build_dir' : build_dir,    # Build output path
            'debugger' : debugger,      # Debugger
            'export_dir': '',           # Export directory path
            'includes': [],             # include paths
            'linker_file': None,        # linker script file
            'name': name,               # project name
            'macros': [],               # macros
            'misc': {},                 # misc settings related to tools
            'output_type': output_type, # output type, default - exe
            'sources': [],              # source files/folders
            'target': '',               # target
            'template' : [],            # tool template
            'tools_supported': [],      # Tools which are supported,
    '''
    @staticmethod
    def _get_common_data_template():
        ''' Data for tool specific '''
        pass
    @staticmethod
    def _get_tool_specific_data_template():
        ''' Data for tool specific '''
        pass
    @staticmethod
    def get_project_template(name='Default', output_type='exe', debugger=None, build_dir='build'):
        ''' Project data (+ data) '''
        pass",4,4,11.0,1.0,9.0,5.0,1.0,1.03,0.0,0.0,0.0,0.0,0.0,0.0,3.0,3.0,56.0,6.0,31.0,10.0,24.0,32.0,12.0,7.0,8.0,1.0,0.0,0.0,3.0,snippet_142
237503,astropy/photutils,photutils/psf/model_plotting.py,photutils.psf.model_plotting.ModelGridPlotMixin,"from astropy.utils import minversion
import numpy as np
from astropy.visualization import simple_norm
import astropy

class ModelGridPlotMixin:
    """"""
    Mixin class to plot a grid of ePSF models.
    """"""

    def _reshape_grid(self, data):
        """"""
        Reshape the 3D ePSF grid as a 2D array of horizontally and
        vertically stacked ePSFs.

        Parameters
        ----------
        data : `numpy.ndarray`
            The 3D array of ePSF data.

        Returns
        -------
        reshaped_data : `numpy.ndarray`
            The 2D array of ePSF data.
        """"""
        nypsfs = self._ygrid.shape[0]
        nxpsfs = self._xgrid.shape[0]
        ny, nx = self.data.shape[1:]
        data.shape = (nypsfs, nxpsfs, ny, nx)
        return data.transpose([0, 2, 1, 3]).reshape(nypsfs * ny, nxpsfs * nx)

    def plot_grid(self, *, ax=None, vmax_scale=None, peak_norm=False, deltas=False, cmap='viridis', dividers=True, divider_color='darkgray', divider_ls='-', figsize=None):
        """"""
        Plot the grid of ePSF models.

        Parameters
        ----------
        ax : `matplotlib.axes.Axes` or `None`, optional
            The matplotlib axes on which to plot. If `None`, then the
            current `~matplotlib.axes.Axes` instance is used.

        vmax_scale : float, optional
            Scale factor to apply to the image stretch limits. This
            value is multiplied by the peak ePSF value to determine the
            plotting ``vmax``. The defaults are 1.0 for plotting the
            ePSF data and 0.03 for plotting the ePSF difference data
            (``deltas=True``). If ``deltas=True``, the ``vmin`` is set
            to ``-vmax``. If ``deltas=False`` the ``vmin`` is set to
            ``vmax`` / 1e4.

        peak_norm : bool, optional
            Whether to normalize the ePSF data by the peak value. The
            default shows the ePSF flux per pixel.

        deltas : bool, optional
            Set to `True` to show the differences between each ePSF
            and the average ePSF.

        cmap : str or `matplotlib.colors.Colormap`, optional
            The colormap to use. The default is 'viridis'.

        dividers : bool, optional
            Whether to show divider lines between the ePSFs.

        divider_color, divider_ls : str, optional
            Matplotlib color and linestyle options for the divider
            lines between ePSFs. These keywords have no effect unless
            ``show_dividers=True``.

        figsize : (float, float), optional
            The figure (width, height) in inches.

        Returns
        -------
        fig : `matplotlib.figure.Figure`
            The matplotlib figure object. This will be the current
            figure if ``ax=None``. Use ``fig.savefig()`` to save the
            figure to a file.

        Notes
        -----
        This method returns a figure object. If you are using this
        method in a script, you will need to call ``plt.show()`` to
        display the figure. If you are using this method in a Jupyter
        notebook, the figure will be displayed automatically.

        When in a notebook, if you do not store the return value of this
        function, the figure will be displayed twice due to the REPL
        automatically displaying the return value of the last function
        call. Alternatively, you can append a semicolon to the end of
        the function call to suppress the display of the return value.
        """"""
        import matplotlib.pyplot as plt
        from mpl_toolkits.axes_grid1 import make_axes_locatable
        data = self.data.copy()
        if deltas:
            mask = np.zeros(data.shape[0], dtype=bool)
            for i, arr in enumerate(data):
                if np.count_nonzero(arr) == 0:
                    mask[i] = True
            data -= np.mean(data[~mask], axis=0)
            data[mask] = 0.0
        data = self._reshape_grid(data)
        if ax is None:
            if figsize is None and self.meta.get('detector', '') == 'NRCSW':
                figsize = (20, 8)
            fig, ax = plt.subplots(figsize=figsize)
        else:
            fig = plt.gcf()
        if peak_norm and data.max() != 0:
            data /= data.max()
        if deltas:
            if vmax_scale is None:
                vmax_scale = 0.03
            vmax = data.max() * vmax_scale
            vmin = -vmax
            if minversion(astropy, '6.1'):
                norm = simple_norm(data, 'linear', vmin=vmin, vmax=vmax)
            else:
                norm = simple_norm(data, 'linear', min_cut=vmin, max_cut=vmax)
        else:
            if vmax_scale is None:
                vmax_scale = 1.0
            vmax = data.max() * vmax_scale
            vmin = vmax / 10000.0
            if minversion(astropy, '6.1'):
                norm = simple_norm(data, 'log', vmin=vmin, vmax=vmax, log_a=10000.0)
            else:
                norm = simple_norm(data, 'log', min_cut=vmin, max_cut=vmax, log_a=10000.0)
        nypsfs = self._ygrid.shape[0]
        nxpsfs = self._xgrid.shape[0]
        extent = [-0.5, nxpsfs - 0.5, -0.5, nypsfs - 0.5]
        axim = ax.imshow(data, extent=extent, norm=norm, cmap=cmap, origin='lower')
        xticklabels = self._xgrid.astype(int)
        yticklabels = self._ygrid.astype(int)
        if self.meta.get('detector', '') == 'NRCSW':
            xticklabels = list(xticklabels[0:5]) * 4
            yticklabels = list(yticklabels[0:5]) * 2
        ax.set_xticks(np.arange(nxpsfs))
        ax.set_xticklabels(xticklabels)
        ax.set_xlabel('ePSF location in detector X pixels')
        ax.set_yticks(np.arange(nypsfs))
        ax.set_yticklabels(yticklabels)
        ax.set_ylabel('ePSF location in detector Y pixels')
        if dividers:
            for ix in range(nxpsfs - 1):
                ax.axvline(ix + 0.5, color=divider_color, ls=divider_ls)
            for iy in range(nypsfs - 1):
                ax.axhline(iy + 0.5, color=divider_color, ls=divider_ls)
        instrument = self.meta.get('instrument', '')
        if not instrument:
            instrument = self.meta.get('instrume', '')
        detector = self.meta.get('detector', '')
        filtername = self.meta.get('filter', '')
        if isinstance(instrument, (tuple, list, np.ndarray)):
            instrument = instrument[0]
        if isinstance(detector, (tuple, list, np.ndarray)):
            detector = detector[0]
        if isinstance(filtername, (tuple, list, np.ndarray)):
            filtername = filtername[0]
        title = f'{instrument} {detector} {filtername}'
        if title != '':
            title += ' '
        if deltas:
            minus = '−'
            ax.set_title(f'{title}(ePSFs {minus} <ePSF>)')
            if peak_norm:
                label = 'Difference relative to average ePSF peak'
            else:
                label = 'Difference relative to average ePSF values'
        else:
            ax.set_title(f'{title}ePSFs')
            if peak_norm:
                label = 'Scale relative to ePSF peak pixel'
            else:
                label = 'ePSF flux per pixel'
        divider = make_axes_locatable(ax)
        cax_cbar = divider.append_axes('right', size='3%', pad='3%')
        cbar = fig.colorbar(axim, cax=cax_cbar, label=label)
        if not deltas:
            cbar.ax.set_yscale('log')
        if self.meta.get('detector', '') == 'NRCSW':
            nxpsfs = len(self._xgrid)
            nypsfs = len(self._ygrid)
            plt.axhline(nypsfs / 2 - 0.5, color='orange')
            for i in range(1, 4):
                ax.axvline(nxpsfs / 4 * i - 0.5, color='orange')
            det_labels = [['A1', 'A3', 'B4', 'B2'], ['A2', 'A4', 'B3', 'B1']]
            for i in range(2):
                for j in range(4):
                    ax.text(j * nxpsfs / 4 - 0.45, (i + 1) * nypsfs / 2 - 0.55, det_labels[i][j], color='orange', verticalalignment='top', fontsize=12)
        fig.tight_layout()
        return fig","
class ModelGridPlotMixin:
    '''
    Mixin class to plot a grid of ePSF models.
        '''

    def _reshape_grid(self, data):
        '''
        Reshape the 3D ePSF grid as a 2D array of horizontally and
        vertically stacked ePSFs.
        Parameters
        ----------
        data : `numpy.ndarray`
            The 3D array of ePSF data.
        Returns
        -------
        reshaped_data : `numpy.ndarray`
            The 2D array of ePSF data.
        '''
        pass

    def plot_grid(self, *, ax=None, vmax_scale=None, peak_norm=False, deltas=False, cmap='viridis', dividers=True, divider_color='darkgray', divider_ls='-', figsize=None):
        '''
        Plot the grid of ePSF models.
        Parameters
        ----------
        ax : `matplotlib.axes.Axes` or `None`, optional
            The matplotlib axes on which to plot. If `None`, then the
            current `~matplotlib.axes.Axes` instance is used.
        vmax_scale : float, optional
            Scale factor to apply to the image stretch limits. This
            value is multiplied by the peak ePSF value to determine the
            plotting ``vmax``. The defaults are 1.0 for plotting the
            ePSF data and 0.03 for plotting the ePSF difference data
            (``deltas=True``). If ``deltas=True``, the ``vmin`` is set
            to ``-vmax``. If ``deltas=False`` the ``vmin`` is set to
            ``vmax`` / 1e4.
        peak_norm : bool, optional
            Whether to normalize the ePSF data by the peak value. The
            default shows the ePSF flux per pixel.
        deltas : bool, optional
            Set to `True` to show the differences between each ePSF
            and the average ePSF.
        cmap : str or `matplotlib.colors.Colormap`, optional
            The colormap to use. The default is 'viridis'.
        dividers : bool, optional
            Whether to show divider lines between the ePSFs.
        divider_color, divider_ls : str, optional
            Matplotlib color and linestyle options for the divider
            lines between ePSFs. These keywords have no effect unless
            ``show_dividers=True``.
        figsize : (float, float), optional
            The figure (width, height) in inches.
        Returns
        -------
        fig : `matplotlib.figure.Figure`
            The matplotlib figure object. This will be the current
            figure if ``ax=None``. Use ``fig.savefig()`` to save the
            figure to a file.
        Notes
        -----
        This method returns a figure object. If you are using this
        method in a script, you will need to call ``plt.show()`` to
        display the figure. If you are using this method in a Jupyter
        notebook, the figure will be displayed automatically.
        When in a notebook, if you do not store the return value of this
        function, the figure will be displayed twice due to the REPL
        automatically displaying the return value of the last function
        call. Alternatively, you can append a semicolon to the end of
        the function call to suppress the display of the return value.
                        '''
                        pass",3,3,112.0,17.0,58.0,37.0,15.0,0.66,0.0,6.0,0.0,2.0,2.0,0.0,2.0,2.0,229.0,35.0,117.0,36.0,110.0,77.0,102.0,34.0,97.0,29.0,0.0,3.0,30.0,snippet_143
237588,matthew-brett/delocate,matthew-brett_delocate/delocate/tmpdirs.py,delocate.tmpdirs.InGivenDirectory,"import os

class InGivenDirectory:
    """"""Change directory to given directory for duration of ``with`` block.

    Useful when you want to use `InTemporaryDirectory` for the final test, but
    you are still debugging.  For example, you may want to do this in the end:

    >>> with InTemporaryDirectory() as tmpdir:
    ...     # do something complicated which might break
    ...     pass

    But indeed the complicated thing does break, and meanwhile the
    ``InTemporaryDirectory`` context manager wiped out the directory with the
    temporary files that you wanted for debugging.  So, while debugging, you
    replace with something like:

    >>> with InGivenDirectory() as tmpdir: # Use working directory by default
    ...     # do something complicated which might break
    ...     pass

    You can then look at the temporary file outputs to debug what is happening,
    fix, and finally replace ``InGivenDirectory`` with ``InTemporaryDirectory``
    again.
    """"""

    def __init__(self, path=None):
        """"""Initialize directory context manager.

        Parameters
        ----------
        path : None or str, optional
            path to change directory to, for duration of ``with`` block.
            Defaults to ``os.getcwd()`` if None
        """"""
        if path is None:
            path = os.getcwd()
        self.path = os.path.abspath(path)

    def __enter__(self):
        """"""Chdir to the managed directory, creating it if needed.""""""
        self._pwd = os.path.abspath(os.getcwd())
        if not os.path.isdir(self.path):
            os.mkdir(self.path)
        os.chdir(self.path)
        return self.path

    def __exit__(self, exc, value, tb):
        """"""Revert the working directory.""""""
        os.chdir(self._pwd)","
class InGivenDirectory:
    '''Change directory to given directory for duration of ``with`` block.
    Useful when you want to use `InTemporaryDirectory` for the final test, but
    you are still debugging.  For example, you may want to do this in the end:
    >>> with InTemporaryDirectory() as tmpdir:
    ...     # do something complicated which might break
    ...     pass
    But indeed the complicated thing does break, and meanwhile the
    ``InTemporaryDirectory`` context manager wiped out the directory with the
    temporary files that you wanted for debugging.  So, while debugging, you
    replace with something like:
    >>> with InGivenDirectory() as tmpdir: # Use working directory by default
    ...     # do something complicated which might break
    ...     pass
    You can then look at the temporary file outputs to debug what is happening,
    fix, and finally replace ``InGivenDirectory`` with ``InTemporaryDirectory``
    again.
    '''

    def __init__(self, path=None):
        '''Initialize directory context manager.
        Parameters
        ----------
        path : None or str, optional
            path to change directory to, for duration of ``with`` block.
            Defaults to ``os.getcwd()`` if None
        '''
        pass

    def __enter__(self):
        '''Chdir to the managed directory, creating it if needed.'''
        pass

    def __exit__(self, exc, value, tb):
        '''Revert the working directory.'''
        pass",4,4,7.0,0.0,4.0,3.0,2.0,2.0,0.0,0.0,0.0,0.0,3.0,2.0,3.0,3.0,48.0,9.0,13.0,6.0,9.0,26.0,13.0,6.0,9.0,2.0,0.0,1.0,5.0,snippet_144
239516,blink1073/oct2py,oct2py/dynamic.py,oct2py.dynamic._MethodDocDescriptor,"class _MethodDocDescriptor:
    """"""An object that dynamically fetches the documentation
    for an Octave user class method.
    """"""

    def __init__(self, session_weakref, class_name, name):
        """"""Initialize the descriptor.""""""
        self.ref = session_weakref
        self.class_name = class_name
        self.name = name
        self.doc = None

    def __get__(self, instance, owner=None):
        """"""Get the documentation.""""""
        if self.doc is not None:
            return self.doc
        session = self.ref()
        class_name = self.class_name
        method = self.name
        doc = session._get_doc(f'@{class_name}/{method}')
        self.doc = doc or session._get_doc(method)
        return self.doc","class _MethodDocDescriptor:
    '''An object that dynamically fetches the documentation
    for an Octave user class method.
        '''

    def __init__(self, session_weakref, class_name, name):
        '''Initialize the descriptor.'''
        pass

    def __get__(self, instance, owner=None):
        '''Get the documentation.'''
        pass",3,3,8.0,0.0,7.0,1.0,2.0,0.33,0.0,0.0,0.0,0.0,2.0,4.0,2.0,2.0,22.0,2.0,15.0,11.0,12.0,5.0,15.0,11.0,12.0,2.0,0.0,1.0,3.0,snippet_145
239914,jazzband/django-simple-menu,simple_menu/menu.py,simple_menu.menu.MenuItem,"from django.conf import settings
import re
from django.utils.text import slugify

class MenuItem:
    """"""
    MenuItem represents an item in a menu, possibly one that has a sub-menu (children).
    """"""

    def __init__(self, title, url, children=[], weight=1, check=None, visible=True, slug=None, exact_url=False, **kwargs):
        """"""
        MenuItem constructor

        title       either a string or a callable to be used for the title
        url         the url of the item
        children    an array of MenuItems that are sub menus to this item
                    this can also be a callable that generates an array
        weight      used to sort adjacent MenuItems
        check       a callable to determine if this item is visible
        slug        used to generate id's in the HTML, auto generated from
                    the title if left as None
        exact_url   normally we check if the url matches the request prefix
                    this requires an exact match if set

        All other keyword arguments passed into the MenuItem constructor are
        assigned to the MenuItem object as attributes so that they may be used
        in your templates. This allows you to attach arbitrary data and use it
        in which ever way suits your menus the best.
        """"""
        self.url = url
        self.title = title
        self.visible = visible
        self.children = children
        self.weight = weight
        self.check_func = check
        self.slug = slug
        self.exact_url = exact_url
        self.selected = False
        self.parent = None
        for k in kwargs:
            setattr(self, k, kwargs[k])

    def check(self, request):
        """"""
        Evaluate if we should be visible for this request
        """"""
        if callable(self.check_func):
            self.visible = self.check_func(request)

    def process(self, request):
        """"""
        process determines if this item should be visible, if its selected, etc...
        """"""
        self.check(request)
        if not self.visible:
            return
        if callable(self.url):
            self.url = self.url(request)
        if callable(self.title):
            self.title = self.title(request)
        if self.slug is None:
            self.slug = slugify(self.title)
        if callable(self.children):
            children = list(self.children(request))
        else:
            children = list(self.children)
        for child in children:
            child.parent = self
            child.process(request)
        self.children = [child for child in children if child.visible]
        self.children.sort(key=lambda child: child.weight)
        hide_empty = getattr(settings, 'MENU_HIDE_EMPTY', False)
        if hide_empty and len(self.children) == 0:
            self.visible = False
            return
        curitem = None
        for item in self.children:
            item.selected = False
            if item.match_url(request):
                if curitem is None or len(curitem.url) < len(item.url):
                    curitem = item
        if curitem is not None:
            curitem.selected = True

    def match_url(self, request):
        """"""
        match url determines if this is selected
        """"""
        matched = False
        if self.exact_url:
            if re.match(f'{self.url}$', request.path):
                matched = True
        elif re.match('%s' % self.url, request.path):
            matched = True
        return matched","
class MenuItem:
    '''
    MenuItem represents an item in a menu, possibly one that has a sub-menu (children).
    '''

    def __init__(self, title, url, children=[], weight=1, check=None, visible=True, slug=None, exact_url=False, **kwargs):
        '''
        MenuItem constructor
        title       either a string or a callable to be used for the title
        url         the url of the item
        children    an array of MenuItems that are sub menus to this item
                    this can also be a callable that generates an array
        weight      used to sort adjacent MenuItems
        check       a callable to determine if this item is visible
        slug        used to generate id's in the HTML, auto generated from
                    the title if left as None
        exact_url   normally we check if the url matches the request prefix
                    this requires an exact match if set
        All other keyword arguments passed into the MenuItem constructor are
        assigned to the MenuItem object as attributes so that they may be used
        in your templates. This allows you to attach arbitrary data and use it
        in which ever way suits your menus the best.
        '''
        pass

    def check(self, request):
        '''
        Evaluate if we should be visible for this request
        '''
        pass

    def process(self, request):
        '''
        process determines if this item should be visible, if its selected, etc...
        '''
        pass

    def match_url(self, request):
        '''
        match url determines if this is selected
        '''
        pass",5,5,28.0,4.0,15.0,9.0,5.0,0.64,0.0,1.0,0.0,1.0,4.0,10.0,4.0,4.0,118.0,18.0,61.0,23.0,55.0,39.0,54.0,22.0,49.0,12.0,0.0,3.0,20.0,snippet_146
244777,markokr/rarfile,markokr_rarfile/rarfile.py,rarfile.NoHashContext,"class NoHashContext:
    """"""No-op hash function.""""""

    def __init__(self, data=None):
        """"""Initialize""""""

    def update(self, data):
        """"""Update data""""""

    def digest(self):
        """"""Final hash""""""

    def hexdigest(self):
        """"""Hexadecimal digest.""""""","class NoHashContext:
    '''No-op hash function.'''

    def __init__(self, data=None):
        '''Initialize'''
        pass

    def update(self, data):
        '''Update data'''
        pass

    def digest(self):
        '''Final hash'''
        pass

    def hexdigest(self):
        '''Hexadecimal digest.'''
        pass",5,5,2.0,0.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,4.0,0.0,4.0,4.0,10.0,0.0,5.0,5.0,0.0,5.0,5.0,5.0,0.0,1.0,0.0,0.0,4.0,snippet_147
244940,meejah/txtorcon,meejah_txtorcon/txtorcon/addrmap.py,txtorcon.addrmap.Addr,"import datetime
from txtorcon.util import maybe_ip_addr

class Addr:
    """"""
    One address mapping (e.g. example.com -> 127.0.0.1)
    """"""

    def __init__(self, map):
        """"""
        map is an AddrMap instance, used for scheduling expiries and
        updating the map.
        """"""
        self.map = map
        self.ip = None
        self.name = None
        self.expiry = None
        self.expires = None
        self.created = None

    def update(self, *args):
        """"""
        deals with an update from Tor; see parsing logic in torcontroller
        """"""
        gmtexpires = None
        name, ip, expires = args[:3]
        for arg in args:
            if arg.lower().startswith('expires='):
                gmtexpires = arg[8:]
        if gmtexpires is None:
            if len(args) == 3:
                gmtexpires = expires
            elif args[2] == 'NEVER':
                gmtexpires = args[2]
            else:
                gmtexpires = args[3]
        self.name = name
        self.ip = maybe_ip_addr(ip)
        if self.ip == '<error>':
            self._expire()
            return
        fmt = '%Y-%m-%d %H:%M:%S'
        oldexpires = self.expires
        if gmtexpires.upper() == 'NEVER':
            self.expires = None
        else:
            self.expires = datetime.datetime.strptime(gmtexpires, fmt)
        self.created = datetime.datetime.utcnow()
        if self.expires is not None:
            if oldexpires is None:
                if self.expires <= self.created:
                    diff = datetime.timedelta(seconds=0)
                else:
                    diff = self.expires - self.created
                self.expiry = self.map.scheduler.callLater(diff.seconds, self._expire)
            else:
                diff = self.expires - oldexpires
                self.expiry.delay(diff.seconds)

    def _expire(self):
        """"""
        callback done via callLater
        """"""
        del self.map.addr[self.name]
        self.map.notify('addrmap_expired', *[self.name], **{})","
class Addr:
    '''
    One address mapping (e.g. example.com -> 127.0.0.1)
    '''

    def __init__(self, map):
        '''
        map is an AddrMap instance, used for scheduling expiries and
        updating the map.
        '''
        pass

    def update(self, *args):
        '''
        deals with an update from Tor; see parsing logic in torcontroller
        '''
        pass

    def _expire(self):
        '''
        callback done via callLater
        '''
        pass",4,4,24.0,4.0,16.0,5.0,4.0,0.38,0.0,2.0,0.0,0.0,3.0,6.0,3.0,3.0,80.0,16.0,48.0,16.0,44.0,18.0,42.0,16.0,38.0,11.0,0.0,3.0,13.0,snippet_148
245022,meejah/txtorcon,meejah_txtorcon/txtorcon/torconfig.py,txtorcon.torconfig.TorConfigType,"class TorConfigType:
    """"""
    Base class for all configuration types, which function as parsers
    and un-parsers.
    """"""

    def parse(self, s):
        """"""
        Given the string s, this should return a parsed representation
        of it.
        """"""
        return s

    def validate(self, s, instance, name):
        """"""
        If s is not a valid type for this object, an exception should
        be thrown. The validated object should be returned.
        """"""
        return s","class TorConfigType:
    '''
    Base class for all configuration types, which function as parsers
    and un-parsers.
    '''

    def parse(self, s):
        '''
        Given the string s, this should return a parsed representation
        of it.
        '''
        pass

    def validate(self, s, instance, name):
        '''
        If s is not a valid type for this object, an exception should
        be thrown. The validated object should be returned.
        '''
        pass",3,3,6.0,0.0,2.0,4.0,1.0,2.4,0.0,0.0,0.0,9.0,2.0,0.0,2.0,2.0,19.0,2.0,5.0,3.0,2.0,12.0,5.0,3.0,2.0,1.0,0.0,0.0,2.0,snippet_149
245057,paterva/maltego-trx,paterva_maltego-trx/maltego_trx/oauth.py,maltego_trx.oauth.MaltegoOauth,"from cryptography.hazmat.backends import default_backend
import base64
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
from cryptography.hazmat.primitives import serialization, padding as primitives_padding
from cryptography.hazmat.primitives.asymmetric import padding as asymmetric_padding

class MaltegoOauth:
    """"""
    A Crypto Helper for Maltego OAuth Secrets received from the Transform Distribution Server
    The TDS will send back an encrypted combination of the following :
    1. Token
    2. Token Secret
    3. Refresh Token
    4. Expires In

    Contains Methods:
        1. decrypt_secrets(private_key_path=""pem file"", ciphertext=""request.getTransformSetting('name from TDS')"")
    """"""

    @staticmethod
    def _rsa_decrypt(private_key_path=None, ciphertext=None, password=None):
        """"""
        RSA Decryption function, returns decrypted plaintext in b64 encoding
        """"""
        ciphertext = base64.b64decode(ciphertext)
        with open(private_key_path, 'rb') as key_file:
            private_key = serialization.load_pem_private_key(key_file.read(), password, backend=None)
            plaintext = private_key.decrypt(ciphertext, asymmetric_padding.PKCS1v15())
        return plaintext

    @staticmethod
    def _aes_decrypt(key=None, ciphertext=None):
        """"""
        AES Decryption function, returns decrypted plaintext value
        """"""
        key = base64.b64decode(key)
        cipher = Cipher(algorithms.AES(key), modes.ECB(), backend=default_backend())
        decryptor = cipher.decryptor()
        ciphertext = base64.b64decode(ciphertext)
        padded_b64_plaintext = decryptor.update(ciphertext) + decryptor.finalize()
        unpadder = primitives_padding.PKCS7(128).unpadder()
        plaintext = (unpadder.update(padded_b64_plaintext) + unpadder.finalize()).decode('utf8')
        return plaintext

    @classmethod
    def decrypt_secrets(cls, private_key_path=None, encoded_ciphertext=None):
        """"""
        The TDS will send back an encrypted combination of the following :
        1. Token
        2. Token Secret
        3. Refresh Token
        4. Expires In

        This function decodes the combinations and decrypts as required and returns a dictionary with the following keys
                {""token"":"""",
                ""token_secret"": """",
                ""refresh_token"": """",
                ""expires_in"": """"}
        """"""
        encrypted_fields = encoded_ciphertext.split('$')
        if len(encrypted_fields) == 1:
            token = cls._rsa_decrypt(private_key_path, encrypted_fields[0])
            token_fields = {'token': token}
        elif len(encrypted_fields) == 2:
            token = cls._rsa_decrypt(private_key_path, encrypted_fields[0])
            token_secret = cls._rsa_decrypt(private_key_path, encrypted_fields[1])
            token_fields = {'token': token, 'token_secret': token_secret}
        elif len(encrypted_fields) == 3:
            aes_key = cls._rsa_decrypt(private_key_path, encrypted_fields[2])
            token = cls._aes_decrypt(aes_key, encrypted_fields[0])
            token_secret = cls._aes_decrypt(aes_key, encrypted_fields[1])
            token_fields = {'token': token, 'token_secret': token_secret}
        elif len(encrypted_fields) == 4:
            token = cls._rsa_decrypt(private_key_path, encrypted_fields[0])
            token_secret = cls._rsa_decrypt(private_key_path, encrypted_fields[1])
            refresh_token = cls._rsa_decrypt(private_key_path, encrypted_fields[2])
            expires_in = cls._rsa_decrypt(private_key_path, encrypted_fields[3])
            token_fields = {'token': token, 'token_secret': token_secret, 'refresh_token': refresh_token, 'expires_in': expires_in}
        elif len(encrypted_fields) == 5:
            aes_key = cls._rsa_decrypt(private_key_path, encrypted_fields[4])
            token = cls._aes_decrypt(aes_key, encrypted_fields[0])
            token_secret = cls._aes_decrypt(aes_key, encrypted_fields[1])
            refresh_token = cls._aes_decrypt(aes_key, encrypted_fields[2])
            expires_in = cls._aes_decrypt(aes_key, encrypted_fields[3])
            token_fields = {'token': token, 'token_secret': token_secret, 'refresh_token': refresh_token, 'expires_in': expires_in}
        else:
            token_fields = {'token': '', 'token_secret': '', 'refresh_token': '', 'expires_in': ''}
        return token_fields","
class MaltegoOauth:
    '''
    A Crypto Helper for Maltego OAuth Secrets received from the Transform Distribution Server
    The TDS will send back an encrypted combination of the following :
    1. Token
    2. Token Secret
    3. Refresh Token
    4. Expires In
    Contains Methods:
        1. decrypt_secrets(private_key_path=""pem file"", ciphertext=""request.getTransformSetting('name from TDS')"")
    '''
    @staticmethod
    def _rsa_decrypt(private_key_path=None, ciphertext=None, password=None):
        '''
        RSA Decryption function, returns decrypted plaintext in b64 encoding
        '''
        pass
    @staticmethod
    def _aes_decrypt(key=None, ciphertext=None):
        '''
        AES Decryption function, returns decrypted plaintext value
        '''
        pass
    @classmethod
    def decrypt_secrets(cls, private_key_path=None, encoded_ciphertext=None):
        '''
        The TDS will send back an encrypted combination of the following :
        1. Token
        2. Token Secret
        3. Refresh Token
        4. Expires In
        This function decodes the combinations and decrypts as required and returns a dictionary with the following keys
                {""token"":"""",
                ""token_secret"": """",
                ""refresh_token"": """",
                ""expires_in"": """"}
        '''
        pass",4,4,32.0,3.0,23.0,6.0,3.0,0.38,0.0,0.0,0.0,0.0,0.0,0.0,3.0,3.0,114.0,12.0,74.0,22.0,67.0,28.0,41.0,18.0,37.0,6.0,0.0,1.0,8.0,snippet_150
247043,CiscoDevNet/webexteamssdk,src/webexpythonsdk/models/cards/adaptive_card_component.py,webexpythonsdk.models.cards.adaptive_card_component.AdaptiveCardComponent,"import json
import enum

class AdaptiveCardComponent:
    """"""
    Base class for all Adaptive Card elements.

    Each element should inherit from this class and specify which of its
    properties fall into the following two categories:

    * Simple properties are basic types (int, float, str, etc.).

    * Serializable properties are properties that can themselves be serialized.
      This includes lists of items (i.e. the 'body' field of the adaptive card)
      or single objects that also inherit from Serializable
    """"""

    def __init__(self, serializable_properties, simple_properties):
        """"""
        Initialize a serializable object.

        Args:
            serializable_properties(list): List of all serializable properties
            simple_properties(list): List of all simple properties.
        """"""
        self.serializable_properties = serializable_properties
        self.simple_properties = simple_properties

    def to_dict(self):
        """"""
        Serialize the element into a Python dictionary.

        The to_dict() method recursively serializes the object's data into
        a Python dictionary.

        Returns:
            dict: Dictionary representation of this element.
        """"""
        serialized_data = {}
        for property_name in self.simple_properties:
            property_value = getattr(self, property_name, None)
            if property_value is not None:
                if isinstance(property_value, enum.Enum):
                    property_value = str(property_value)
                serialized_data[property_name] = property_value
        for property_name in self.serializable_properties:
            property_value = getattr(self, property_name, None)
            if property_value is not None:
                if isinstance(property_value, list):
                    serialized_data[property_name] = [item.to_dict() if hasattr(item, 'to_dict') else item for item in property_value]
                else:
                    serialized_data[property_name] = property_value.to_dict()
        return serialized_data

    def to_json(self, **kwargs):
        """"""
        Serialize the element into JSON text.

        Any keyword arguments provided are passed through the Python JSON
        encoder.
        """"""
        return json.dumps(self.to_dict(), **kwargs)","
class AdaptiveCardComponent:
    '''
    Base class for all Adaptive Card elements.
    Each element should inherit from this class and specify which of its
    properties fall into the following two categories:
    * Simple properties are basic types (int, float, str, etc.).
    * Serializable properties are properties that can themselves be serialized.
      This includes lists of items (i.e. the 'body' field of the adaptive card)
      or single objects that also inherit from Serializable
    '''

    def __init__(self, serializable_properties, simple_properties):
        '''
        Initialize a serializable object.
        Args:
            serializable_properties(list): List of all serializable properties
            simple_properties(list): List of all simple properties.
        '''
        pass

    def to_dict(self):
        '''
        Serialize the element into a Python dictionary.
        The to_dict() method recursively serializes the object's data into
        a Python dictionary.
        Returns:
            dict: Dictionary representation of this element.
        '''
        pass

    def to_json(self, **kwargs):
        '''
        Serialize the element into JSON text.
        Any keyword arguments provided are passed through the Python JSON
        encoder.
        '''
        pass",4,4,18.0,3.0,8.0,7.0,3.0,1.16,0.0,2.0,0.0,27.0,3.0,2.0,3.0,3.0,70.0,16.0,25.0,9.0,21.0,29.0,21.0,9.0,17.0,8.0,0.0,3.0,10.0,snippet_151
248349,facelessuser/soupsieve,facelessuser_soupsieve/soupsieve/css_match.py,soupsieve.css_match._FakeParent,"import bs4

class _FakeParent:
    """"""
    Fake parent class.

    When we have a fragment with no `BeautifulSoup` document object,
    we can't evaluate `nth` selectors properly.  Create a temporary
    fake parent so we can traverse the root element as a child.
    """"""

    def __init__(self, element: bs4.Tag) -> None:
        """"""Initialize.""""""
        self.contents = [element]

    def __len__(self) -> int:
        """"""Length.""""""
        return len(self.contents)","
class _FakeParent:
    '''
    Fake parent class.
    When we have a fragment with no `BeautifulSoup` document object,
    we can't evaluate `nth` selectors properly.  Create a temporary
    fake parent so we can traverse the root element as a child.
    '''

    def __init__(self, element: bs4.Tag) -> None:
        '''Initialize.'''
        pass

    def __len__(self) -> int:
        '''Length.'''
        pass",3,3,4.0,1.0,2.0,1.0,1.0,1.6,0.0,0.0,0.0,0.0,2.0,1.0,2.0,2.0,18.0,5.0,5.0,4.0,2.0,8.0,5.0,4.0,2.0,1.0,0.0,0.0,2.0,snippet_152
248351,facelessuser/soupsieve,facelessuser_soupsieve/soupsieve/css_parser.py,soupsieve.css_parser.SelectorPattern,"from typing import Match, Any, Iterator, cast
import re

class SelectorPattern:
    """"""Selector pattern.""""""

    def __init__(self, name: str, pattern: str) -> None:
        """"""Initialize.""""""
        self.name = name
        self.re_pattern = re.compile(pattern, re.I | re.X | re.U)

    def get_name(self) -> str:
        """"""Get name.""""""
        return self.name

    def match(self, selector: str, index: int, flags: int) -> Match[str] | None:
        """"""Match the selector.""""""
        return self.re_pattern.match(selector, index)","
class SelectorPattern:
    '''Selector pattern.'''

    def __init__(self, name: str, pattern: str) -> None:
        '''Initialize.'''
        pass

    def get_name(self) -> str:
        '''Get name.'''
        pass

    def match(self, selector: str, index: int, flags: int) -> Match[str] | None:
        '''Match the selector.'''
        pass",4,4,4.0,1.0,2.0,1.0,1.0,0.5,0.0,2.0,0.0,1.0,3.0,2.0,3.0,3.0,18.0,6.0,8.0,6.0,4.0,4.0,8.0,6.0,4.0,1.0,0.0,0.0,3.0,snippet_153
250153,Unidata/siphon,src/siphon/catalog.py,siphon.catalog.CatalogRef,"from urllib.parse import urljoin, urlparse

class CatalogRef:
    """"""
    An object for holding catalog references obtained from a THREDDS Client Catalog.

    Attributes
    ----------
    name : str
        The name of the :class:`CatalogRef` element
    href : str
        url to the :class:`CatalogRef`'s THREDDS Client Catalog
    title : str
        Title of the :class:`CatalogRef` element

    """"""

    def __init__(self, base_url, element_node):
        """"""
        Initialize the catalogRef object.

        Parameters
        ----------
        base_url : str
            URL to the base catalog that owns this reference
        element_node : :class:`~xml.etree.ElementTree.Element`
            An :class:`~xml.etree.ElementTree.Element` representing a catalogRef node

        """"""
        self.title = element_node.attrib['{http://www.w3.org/1999/xlink}title']
        self.name = element_node.attrib.get('name', self.title)
        href = element_node.attrib['{http://www.w3.org/1999/xlink}href']
        self.href = urljoin(base_url, href)

    def __str__(self):
        """"""Return a string representation of the catalog reference.""""""
        return str(self.title)

    def follow(self):
        """"""Follow the catalog reference and return a new :class:`TDSCatalog`.

        Returns
        -------
        TDSCatalog
            The referenced catalog

        """"""
        return TDSCatalog(self.href)
    __repr__ = __str__","
class CatalogRef:
    '''
    An object for holding catalog references obtained from a THREDDS Client Catalog.
    Attributes
    ----------
    name : str
        The name of the :class:`CatalogRef` element
    href : str
        url to the :class:`CatalogRef`'s THREDDS Client Catalog
    title : str
        Title of the :class:`CatalogRef` element
    '''

    def __init__(self, base_url, element_node):
        '''
        Initialize the catalogRef object.
        Parameters
        ----------
        base_url : str
            URL to the base catalog that owns this reference
        element_node : :class:`~xml.etree.ElementTree.Element`
            An :class:`~xml.etree.ElementTree.Element` representing a catalogRef node
        '''
        pass

    def __str__(self):
        '''Return a string representation of the catalog reference.'''
        pass

    def follow(self):
        '''Follow the catalog reference and return a new :class:`TDSCatalog`.
        Returns
        -------
        TDSCatalog
            The referenced catalog
        '''
        pass",4,4,10.0,2.0,3.0,6.0,1.0,2.55,0.0,2.0,1.0,0.0,3.0,3.0,3.0,3.0,50.0,11.0,11.0,9.0,7.0,28.0,11.0,9.0,7.0,1.0,0.0,0.0,3.0,snippet_154
250154,Unidata/siphon,src/siphon/catalog.py,siphon.catalog.CompoundService,"class CompoundService:
    """"""Hold information about compound services.

    Attributes
    ----------
    name : str
        The name of the compound service
    service_type : str
        The service type (for this object, service type will always be
        ""COMPOUND"")
    services : list[SimpleService]
        A list of :class:`SimpleService` objects

    """"""

    def __init__(self, service_node):
        """"""Initialize a :class:`CompoundService` object.

        Parameters
        ----------
        service_node : :class:`~xml.etree.ElementTree.Element`
            An :class:`~xml.etree.ElementTree.Element` representing a compound service node

        """"""
        self.name = service_node.attrib['name']
        self.service_type = CaseInsensitiveStr(service_node.attrib['serviceType'])
        self.base = service_node.attrib['base']
        services = []
        subservices = 0
        for child in list(service_node):
            services.append(SimpleService(child))
            subservices += 1
        self.services = services
        self.number_of_subservices = subservices

    def is_resolver(self):
        """"""Return whether the service is a resolver service.

        For a compound service, this is always False because it will never be
        a resolver.
        """"""
        return False","class CompoundService:
    '''Hold information about compound services.
    Attributes
    ----------
    name : str
        The name of the compound service
    service_type : str
        The service type (for this object, service type will always be
        ""COMPOUND"")
    services : list[SimpleService]
        A list of :class:`SimpleService` objects
    '''

    def __init__(self, service_node):
        '''Initialize a :class:`CompoundService` object.
        Parameters
        ----------
        service_node : :class:`~xml.etree.ElementTree.Element`
            An :class:`~xml.etree.ElementTree.Element` representing a compound service node
        '''
        pass

    def is_resolver(self):
        '''Return whether the service is a resolver service.
        For a compound service, this is always False because it will never be
        a resolver.
        '''
        pass",3,3,14.0,2.0,7.0,5.0,2.0,1.5,0.0,3.0,2.0,0.0,2.0,5.0,2.0,2.0,43.0,8.0,14.0,11.0,11.0,21.0,14.0,11.0,11.0,2.0,0.0,1.0,3.0,snippet_155
250158,Unidata/siphon,src/siphon/catalog.py,siphon.catalog.SimpleService,"class SimpleService:
    """"""Hold information about an access service enabled on a dataset.

    Attributes
    ----------
    name : str
        The name of the service
    service_type : str
        The service type (i.e. ""OPENDAP"", ""NetcdfSubset"", ""WMS"", etc.)
    access_urls : dict[str, str]
        A dictionary of access urls whose keywords are the access service
        types defined in the catalog (for example, ""OPENDAP"", ""NetcdfSubset"",
        ""WMS"", etc.)

    """"""

    def __init__(self, service_node):
        """"""Initialize the Dataset object.

        Parameters
        ----------
        service_node : :class:`~xml.etree.ElementTree.Element`
            An :class:`~xml.etree.ElementTree.Element` representing a service node

        """"""
        self.name = service_node.attrib['name']
        self.service_type = CaseInsensitiveStr(service_node.attrib['serviceType'])
        self.base = service_node.attrib['base']
        self.access_urls = {}

    def is_resolver(self):
        """"""Return whether the service is a resolver service.""""""
        return self.service_type == 'Resolver'","class SimpleService:
    '''Hold information about an access service enabled on a dataset.
    Attributes
    ----------
    name : str
        The name of the service
    service_type : str
        The service type (i.e. ""OPENDAP"", ""NetcdfSubset"", ""WMS"", etc.)
    access_urls : dict[str, str]
        A dictionary of access urls whose keywords are the access service
        types defined in the catalog (for example, ""OPENDAP"", ""NetcdfSubset"",
        ""WMS"", etc.)
    '''

    def __init__(self, service_node):
        '''Initialize the Dataset object.
        Parameters
        ----------
        service_node : :class:`~xml.etree.ElementTree.Element`
            An :class:`~xml.etree.ElementTree.Element` representing a service node
        '''
        pass

    def is_resolver(self):
        '''Return whether the service is a resolver service.'''
        pass",3,3,8.0,1.0,4.0,4.0,1.0,2.38,0.0,1.0,1.0,0.0,2.0,4.0,2.0,2.0,33.0,6.0,8.0,7.0,5.0,19.0,8.0,7.0,5.0,1.0,0.0,0.0,2.0,snippet_156
250272,incuna/django-pgcrypto-fields,incuna_django-pgcrypto-fields/pgcrypto/mixins.py,pgcrypto.mixins.HashMixin,"class HashMixin:
    """"""Keyed hash mixin.

    `HashMixin` uses 'pgcrypto' to encrypt data in a postgres database.
    """"""
    encrypt_sql = None

    def __init__(self, original=None, *args, **kwargs):
        """"""Tells the init the original attr.""""""
        self.original = original
        super(HashMixin, self).__init__(*args, **kwargs)

    def pre_save(self, model_instance, add):
        """"""Save the original_value.""""""
        if self.original:
            original_value = getattr(model_instance, self.original)
            setattr(model_instance, self.attname, original_value)
        return super(HashMixin, self).pre_save(model_instance, add)

    def get_placeholder(self, value=None, compiler=None, connection=None):
        """"""
        Tell postgres to encrypt this field with a hashing function.

        The `value` string is checked to determine if we need to hash or keep
        the current value.

        `compiler` and `connection` is ignored here as we don't need custom operators.
        """"""
        if value is None or value.startswith('\\x'):
            return '%s'
        return self.get_encrypt_sql(connection)

    def get_encrypt_sql(self, connection):
        """"""Get encrypt sql. This may be overidden by some implementations.""""""
        return self.encrypt_sql","class HashMixin:
    '''Keyed hash mixin.
    `HashMixin` uses 'pgcrypto' to encrypt data in a postgres database.
    '''

    def __init__(self, original=None, *args, **kwargs):
        '''Tells the init the original attr.'''
        pass

    def pre_save(self, model_instance, add):
        '''Save the original_value.'''
        pass

    def get_placeholder(self, value=None, compiler=None, connection=None):
        '''
        Tell postgres to encrypt this field with a hashing function.
        The `value` string is checked to determine if we need to hash or keep
        the current value.
        `compiler` and `connection` is ignored here as we don't need custom operators.
        '''
        pass

    def get_encrypt_sql(self, connection):
        '''Get encrypt sql. This may be overidden by some implementations.'''
        pass",5,5,7.0,1.0,4.0,2.0,2.0,0.81,0.0,1.0,0.0,2.0,4.0,1.0,4.0,4.0,38.0,10.0,16.0,8.0,11.0,13.0,16.0,8.0,11.0,2.0,0.0,1.0,6.0,snippet_157
251421,Yelp/py_zipkin,Yelp_py_zipkin/py_zipkin/encoding/_encoders.py,py_zipkin.encoding._encoders.IEncoder,"from typing import Union
from py_zipkin.encoding._helpers import Span
from typing import List

class IEncoder:
    """"""Encoder interface.""""""

    def fits(self, current_count: int, current_size: int, max_size: int, new_span: Union[str, bytes]) -> bool:
        """"""Returns whether the new span will fit in the list.

        :param current_count: number of spans already in the list.
        :type current_count: int
        :param current_size: sum of the sizes of all the spans already in the list.
        :type current_size: int
        :param max_size: max supported transport payload size.
        :type max_size: int
        :param new_span: encoded span object that we want to add the the list.
        :type new_span: str or bytes
        :return: True if the new span can be added to the list, False otherwise.
        :rtype: bool
        """"""
        raise NotImplementedError()

    def encode_span(self, span: Span) -> Union[str, bytes]:
        """"""Encodes a single span.

        :param span: Span object representing the span.
        :type span: Span
        :return: encoded span.
        :rtype: str or bytes
        """"""
        raise NotImplementedError()

    def encode_queue(self, queue: List[Union[str, bytes]]) -> Union[str, bytes]:
        """"""Encodes a list of pre-encoded spans.

        :param queue: list of encoded spans.
        :type queue: list
        :return: encoded list, type depends on the encoding.
        :rtype: str or bytes
        """"""
        raise NotImplementedError()","
class IEncoder:
    '''Encoder interface.'''

    def fits(self, current_count: int, current_size: int, max_size: int, new_span: Union[str, bytes]) -> bool:
        '''Returns whether the new span will fit in the list.
        :param current_count: number of spans already in the list.
        :type current_count: int
        :param current_size: sum of the sizes of all the spans already in the list.
        :type current_size: int
        :param max_size: max supported transport payload size.
        :type max_size: int
        :param new_span: encoded span object that we want to add the the list.
        :type new_span: str or bytes
        :return: True if the new span can be added to the list, False otherwise.
        :rtype: bool
        '''
        pass

    def encode_span(self, span: Span) -> Union[str, bytes]:
        '''Encodes a single span.
        :param span: Span object representing the span.
        :type span: Span
        :return: encoded span.
        :rtype: str or bytes
        '''
        pass

    def encode_queue(self, queue: List[Union[str, bytes]]) -> Union[str, bytes]:
        '''Encodes a list of pre-encoded spans.
        :param queue: list of encoded spans.
        :type queue: list
        :return: encoded list, type depends on the encoding.
        :rtype: str or bytes
        '''
        pass",4,4,13.0,1.0,4.0,8.0,1.0,1.92,0.0,6.0,1.0,3.0,3.0,0.0,3.0,3.0,44.0,6.0,13.0,10.0,3.0,25.0,7.0,4.0,3.0,1.0,0.0,0.0,3.0,snippet_158
251832,glut23/webvtt-py,glut23_webvtt-py/webvtt/models.py,webvtt.models.Style,"import typing

class Style:
    """"""Representation of a style.""""""

    def __init__(self, text: typing.Union[str, typing.List[str]]):
        """"""Initialize.

        :param: text: the style text
        """"""
        self.lines = text.splitlines() if isinstance(text, str) else text
        self.comments: typing.List[str] = []

    @property
    def text(self):
        """"""Return the text of the style.""""""
        return '\n'.join(self.lines)","
class Style:
    '''Representation of a style.'''

    def __init__(self, text: typing.Union[str, typing.List[str]]):
        '''Initialize.
        :param: text: the style text
        '''
        pass
    @property
    def text(self):
        '''Return the text of the style.'''
        pass",3,3,5.0,1.0,3.0,2.0,2.0,0.71,0.0,1.0,0.0,0.0,2.0,2.0,2.0,2.0,15.0,3.0,7.0,6.0,3.0,5.0,6.0,5.0,3.0,2.0,0.0,0.0,3.0,snippet_159
251834,glut23/webvtt-py,glut23_webvtt-py/webvtt/sbv.py,webvtt.sbv.SBVCueBlock,"import typing
import re

class SBVCueBlock:
    """"""Representation of a cue timing block.""""""
    CUE_TIMINGS_PATTERN = re.compile('\\s*(\\d{1,2}:\\d{1,2}:\\d{1,2}.\\d{3}),(\\d{1,2}:\\d{1,2}:\\d{1,2}.\\d{3})')

    def __init__(self, start: str, end: str, payload: typing.Sequence[str]):
        """"""
        Initialize.

        :param start: start time
        :param end: end time
        :param payload: caption text
        """"""
        self.start = start
        self.end = end
        self.payload = payload

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a cue time block.

        :param lines: the lines to be validated
        :returns: true for a matching cue time block
        """"""
        return bool(len(lines) >= 2 and re.match(cls.CUE_TIMINGS_PATTERN, lines[0]) and lines[1].strip())

    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SBVCueBlock':
        """"""
        Create a `SBVCueBlock` from lines of text.

        :param lines: the lines of text
        :returns: `SBVCueBlock` instance
        """"""
        match = re.match(cls.CUE_TIMINGS_PATTERN, lines[0])
        assert match is not None
        payload = lines[1:]
        return cls(match.group(1), match.group(2), payload)","
class SBVCueBlock:
    '''Representation of a cue timing block.'''

    def __init__(self, start: str, end: str, payload: typing.Sequence[str]):
        '''
        Initialize.
        :param start: start time
        :param end: end time
        :param payload: caption text
        '''
        pass
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        '''
        Validate the lines for a match of a cue time block.
        :param lines: the lines to be validated
        :returns: true for a matching cue time block
        '''
        pass
    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SBVCueBlock':
        '''
        Create a `SBVCueBlock` from lines of text.
        :param lines: the lines of text
        :returns: `SBVCueBlock` instance
        '''
        pass",4,4,16.0,2.0,9.0,5.0,1.0,0.53,0.0,2.0,0.0,0.0,1.0,3.0,3.0,3.0,58.0,9.0,32.0,23.0,15.0,17.0,13.0,10.0,9.0,1.0,0.0,0.0,3.0,snippet_160
251835,glut23/webvtt-py,glut23_webvtt-py/webvtt/srt.py,webvtt.srt.SRTCueBlock,"import typing
import re

class SRTCueBlock:
    """"""Representation of a cue timing block.""""""
    CUE_TIMINGS_PATTERN = re.compile('\\s*(\\d+:\\d{2}:\\d{2},\\d{3})\\s*-->\\s*(\\d+:\\d{2}:\\d{2},\\d{3})')

    def __init__(self, index: str, start: str, end: str, payload: typing.Sequence[str]):
        """"""
        Initialize.

        :param start: start time
        :param end: end time
        :param payload: caption text
        """"""
        self.index = index
        self.start = start
        self.end = end
        self.payload = payload

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a cue time block.

        :param lines: the lines to be validated
        :returns: true for a matching cue time block
        """"""
        return bool(len(lines) >= 3 and lines[0].isdigit() and re.match(cls.CUE_TIMINGS_PATTERN, lines[1]))

    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SRTCueBlock':
        """"""
        Create a `SRTCueBlock` from lines of text.

        :param lines: the lines of text
        :returns: `SRTCueBlock` instance
        """"""
        index = lines[0]
        match = re.match(cls.CUE_TIMINGS_PATTERN, lines[1])
        assert match is not None
        payload = lines[2:]
        return cls(index, match.group(1), match.group(2), payload)","
class SRTCueBlock:
    '''Representation of a cue timing block.'''

    def __init__(self, index: str, start: str, end: str, payload: typing.Sequence[str]):
        '''
        Initialize.
        :param start: start time
        :param end: end time
        :param payload: caption text
        '''
        pass
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        '''
        Validate the lines for a match of a cue time block.
        :param lines: the lines to be validated
        :returns: true for a matching cue time block
        '''
        pass
    @classmethod
    def from_lines(cls, lines: typing.Sequence[str]) -> 'SRTCueBlock':
        '''
        Create a `SRTCueBlock` from lines of text.
        :param lines: the lines of text
        :returns: `SRTCueBlock` instance
        '''
        pass",4,4,17.0,2.0,10.0,5.0,1.0,0.49,0.0,2.0,0.0,0.0,1.0,4.0,3.0,3.0,62.0,10.0,35.0,26.0,17.0,17.0,15.0,12.0,11.0,1.0,0.0,0.0,3.0,snippet_161
251838,glut23/webvtt-py,glut23_webvtt-py/webvtt/vtt.py,webvtt.vtt.WebVTTCommentBlock,"import typing
import re

class WebVTTCommentBlock:
    """"""Representation of a comment block.""""""
    COMMENT_PATTERN = re.compile('NOTE\\s(.*?)\\Z', re.DOTALL)

    def __init__(self, text: str):
        """"""
        Initialize.

        :param text: comment text
        """"""
        self.text = text

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a comment block.

        :param lines: the lines to be validated
        :returns: true for a matching comment block
        """"""
        return bool(lines and lines[0].startswith('NOTE'))

    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTCommentBlock':
        """"""
        Create a `WebVTTCommentBlock` from lines of text.

        :param lines: the lines of text
        :returns: `WebVTTCommentBlock` instance
        """"""
        match = cls.COMMENT_PATTERN.match('\n'.join(lines))
        return cls(text=match.group(1).strip() if match else '')

    @staticmethod
    def format_lines(lines: str) -> typing.List[str]:
        """"""
        Return the lines for a comment block.

        :param lines: comment lines
        :returns: list of lines for a comment block
        """"""
        list_of_lines = lines.split('\n')
        if len(list_of_lines) == 1:
            return [f'NOTE {lines}']
        return ['NOTE', *list_of_lines]","
class WebVTTCommentBlock:
    '''Representation of a comment block.'''

    def __init__(self, text: str):
        '''
        Initialize.
        :param text: comment text
        '''
        pass
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        '''
        Validate the lines for a match of a comment block.
        :param lines: the lines to be validated
        :returns: true for a matching comment block
        '''
        pass
    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTCommentBlock':
        '''
        Create a `WebVTTCommentBlock` from lines of text.
        :param lines: the lines of text
        :returns: `WebVTTCommentBlock` instance
        '''
        pass
    @staticmethod
    def format_lines(lines: str) -> typing.List[str]:
        '''
        Return the lines for a comment block.
        :param lines: comment lines
        :returns: list of lines for a comment block
        '''
        pass",5,5,11.0,2.0,5.0,5.0,2.0,0.87,0.0,2.0,0.0,0.0,1.0,1.0,4.0,4.0,54.0,11.0,23.0,18.0,9.0,20.0,14.0,9.0,9.0,2.0,0.0,1.0,6.0,snippet_162
251840,glut23/webvtt-py,glut23_webvtt-py/webvtt/vtt.py,webvtt.vtt.WebVTTStyleBlock,"import re
import typing

class WebVTTStyleBlock:
    """"""Representation of a style block.""""""
    STYLE_PATTERN = re.compile('STYLE\\s(.*?)\\Z', re.DOTALL)

    def __init__(self, text: str):
        """"""
        Initialize.

        :param text: style text
        """"""
        self.text = text

    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        """"""
        Validate the lines for a match of a style block.

        :param lines: the lines to be validated
        :returns: true for a matching style block
        """"""
        return len(lines) >= 2 and lines[0] == 'STYLE' and (not any((line.strip() == '' or '-->' in line for line in lines)))

    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTStyleBlock':
        """"""
        Create a `WebVTTStyleBlock` from lines of text.

        :param lines: the lines of text
        :returns: `WebVTTStyleBlock` instance
        """"""
        match = cls.STYLE_PATTERN.match('\n'.join(lines))
        return cls(text=match.group(1).strip() if match else '')

    @staticmethod
    def format_lines(lines: typing.List[str]) -> typing.List[str]:
        """"""
        Return the lines for a style block.

        :param lines: style lines
        :returns: list of lines for a style block
        """"""
        return ['STYLE', *lines]","
class WebVTTStyleBlock:
    '''Representation of a style block.'''

    def __init__(self, text: str):
        '''
        Initialize.
        :param text: style text
        '''
        pass
    @classmethod
    def is_valid(cls, lines: typing.Sequence[str]) -> bool:
        '''
        Validate the lines for a match of a style block.
        :param lines: the lines to be validated
        :returns: true for a matching style block
        '''
        pass
    @classmethod
    def from_lines(cls, lines: typing.Iterable[str]) -> 'WebVTTStyleBlock':
        '''
        Create a `WebVTTStyleBlock` from lines of text.
        :param lines: the lines of text
        :returns: `WebVTTStyleBlock` instance
        '''
        pass
    @staticmethod
    def format_lines(lines: typing.List[str]) -> typing.List[str]:
        '''
        Return the lines for a style block.
        :param lines: style lines
        :returns: list of lines for a style block
        '''
        pass",5,5,10.0,1.0,5.0,5.0,1.0,0.87,0.0,2.0,0.0,0.0,1.0,1.0,4.0,4.0,52.0,9.0,23.0,17.0,9.0,20.0,11.0,8.0,6.0,2.0,0.0,0.0,5.0,snippet_163
251866,Shoobx/xmldiff,Shoobx_xmldiff/xmldiff/diff_match_patch.py,xmldiff.diff_match_patch.patch_obj,"import urllib.parse

class patch_obj:
    """"""Class representing one patch operation.""""""

    def __init__(self):
        """"""Initializes with an empty list of diffs.""""""
        self.diffs = []
        self.start1 = None
        self.start2 = None
        self.length1 = 0
        self.length2 = 0

    def __str__(self):
        """"""Emulate GNU diff's format.
        Header: @@ -382,8 +481,9 @@
        Indices are printed as 1-based, not 0-based.

        Returns:
          The GNU diff string.
        """"""
        if self.length1 == 0:
            coords1 = str(self.start1) + ',0'
        elif self.length1 == 1:
            coords1 = str(self.start1 + 1)
        else:
            coords1 = str(self.start1 + 1) + ',' + str(self.length1)
        if self.length2 == 0:
            coords2 = str(self.start2) + ',0'
        elif self.length2 == 1:
            coords2 = str(self.start2 + 1)
        else:
            coords2 = str(self.start2 + 1) + ',' + str(self.length2)
        text = ['@@ -', coords1, ' +', coords2, ' @@\n']
        for op, data in self.diffs:
            if op == diff_match_patch.DIFF_INSERT:
                text.append('+')
            elif op == diff_match_patch.DIFF_DELETE:
                text.append('-')
            elif op == diff_match_patch.DIFF_EQUAL:
                text.append(' ')
            data = data.encode('utf-8')
            text.append(urllib.parse.quote(data, ""!~*'();/?:@&=+$,# "") + '\n')
        return ''.join(text)","
class patch_obj:
    '''Class representing one patch operation.'''

    def __init__(self):
        '''Initializes with an empty list of diffs.'''
        pass

    def __str__(self):
        '''Emulate GNU diff's format.
        Header: @@ -382,8 +481,9 @@
        Indices are printed as 1-based, not 0-based.
        Returns:
          The GNU diff string.
        '''
        pass",3,3,20.0,1.0,15.0,5.0,5.0,0.35,0.0,2.0,1.0,0.0,2.0,5.0,2.0,2.0,44.0,3.0,31.0,12.0,28.0,11.0,25.0,12.0,22.0,9.0,0.0,2.0,10.0,snippet_164
252656,happyleavesaoc/aoc-mgz,happyleavesaoc_aoc-mgz/mgz/model/inputs.py,mgz.model.inputs.Inputs,"from mgz.model.definitions import Input
from mgz.fast import Action as ActionEnum

class Inputs:
    """"""Normalize player inputs.""""""

    def __init__(self, gaia):
        """"""Initialize.""""""
        self._gaia = gaia
        self._buildings = {}
        self._oid_cache = {}
        self.inputs = []

    def add_chat(self, chat):
        """"""Add chat input.""""""
        self.inputs.append(Input(chat.timestamp, 'Chat', None, dict(message=chat.message), chat.player, None))

    def add_action(self, action):
        """"""Add action input.""""""
        if action.type in (ActionEnum.DE_TRANSFORM, ActionEnum.POSTGAME):
            return
        name = ACTION_TRANSLATE.get(action.type, action.type.name).replace('_', ' ').title()
        param = None
        if 'object_ids' in action.payload and action.payload['object_ids']:
            self._oid_cache[action.type] = action.payload['object_ids']
        elif action.type in self._oid_cache:
            action.payload['object_ids'] = self._oid_cache[action.type]
        if action.type is ActionEnum.SPECIAL:
            name = action.payload['order']
        elif action.type is ActionEnum.GAME:
            name = action.payload['command']
            if name == 'Speed':
                param = action.payload['speed']
        elif action.type is ActionEnum.STANCE:
            name = 'Stance'
            param = action.payload['stance']
        elif action.type is ActionEnum.FORMATION:
            name = 'Formation'
            param = action.payload['formation']
        elif action.type is ActionEnum.ORDER and action.payload['target_id'] in self._gaia:
            name = 'Gather'
            param = self._gaia[action.payload['target_id']]
        elif action.type is ActionEnum.ORDER and action.position and (action.position.hash() in self._buildings):
            name = 'Target'
            param = self._buildings[action.position.hash()]
        elif action.type is ActionEnum.GATHER_POINT:
            if action.payload['target_id'] in self._gaia:
                param = self._gaia[action.payload['target_id']]
            elif action.position and action.position.hash() in self._buildings:
                if len(action.payload['object_ids']) == 1 and action.payload['object_ids'][0] == action.payload['target_id']:
                    name = 'Spawn'
                param = self._buildings[action.position.hash()]
        elif action.type in (ActionEnum.BUY, ActionEnum.SELL):
            action.payload['amount'] *= 100
        elif action.type is ActionEnum.BUILD:
            param = action.payload['building']
            if action.position.hash() in self._buildings:
                if self._buildings[action.position.hash()] == 'Farm' and action.payload['building'] == 'Farm':
                    name = 'Reseed'
            self._buildings[action.position.hash()] = action.payload['building']
        elif action.type in (ActionEnum.QUEUE, ActionEnum.DE_QUEUE):
            param = action.payload['unit']
        elif action.type is ActionEnum.RESEARCH:
            param = action.payload['technology']
        new_input = Input(action.timestamp, name, param, action.payload, action.player, action.position)
        self.inputs.append(new_input)
        return new_input","
class Inputs:
    '''Normalize player inputs.'''

    def __init__(self, gaia):
        '''Initialize.'''
        pass

    def add_chat(self, chat):
        '''Add chat input.'''
        pass

    def add_action(self, action):
        '''Add action input.'''
        pass",4,4,22.0,0.0,21.0,1.0,8.0,0.06,0.0,3.0,2.0,0.0,3.0,4.0,3.0,3.0,71.0,3.0,64.0,11.0,60.0,4.0,45.0,11.0,41.0,21.0,0.0,3.0,23.0,snippet_165
255605,ungarj/mapchete,mapchete/formats/base.py,mapchete.formats.base.OutputSTACMixin,"from mapchete.path import MPath

class OutputSTACMixin:
    """"""Adds STAC related features.""""""
    path: MPath
    output_params: dict

    @property
    def stac_path(self) -> MPath:
        """"""Return path to STAC JSON file.""""""
        return self.path / f'{self.stac_item_id}.json'

    @property
    def stac_item_id(self) -> str:
        """"""
        Return STAC item ID according to configuration.

        Defaults to path basename.
        """"""
        return self.output_params.get('stac', {}).get('id') or self.path.stem

    @property
    def stac_item_metadata(self):
        """"""Custom STAC metadata.""""""
        return self.output_params.get('stac', {})

    @property
    def stac_asset_type(self):
        """"""Asset MIME type.""""""
        raise ValueError('no MIME type set for this output')","
class OutputSTACMixin:
    '''Adds STAC related features.'''
    @property
    def stac_path(self) -> MPath:
        '''Return path to STAC JSON file.'''
        pass
    @property
    def stac_item_id(self) -> str:
        '''
        Return STAC item ID according to configuration.
        Defaults to path basename.
        '''
        pass
    @property
    def stac_item_metadata(self):
        '''Custom STAC metadata.'''
        pass
    @property
    def stac_asset_type(self):
        '''Asset MIME type.'''
        pass",5,5,4.0,0.0,2.0,2.0,1.0,0.6,0.0,3.0,1.0,2.0,4.0,0.0,4.0,4.0,29.0,6.0,15.0,9.0,6.0,9.0,11.0,5.0,6.0,1.0,0.0,0.0,4.0,snippet_166
255649,ungarj/mapchete,mapchete/io/raster/read.py,mapchete.io.raster.read.RasterWindowMemoryFile,"from rasterio.profiles import Profile
from mapchete.io.raster.write import _write_tags
from rasterio.io import DatasetReader, DatasetWriter, MemoryFile
from mapchete.validate import validate_write_window_params
from typing import Any, Dict, Generator, Iterable, List, Optional, Tuple, Union
from mapchete.io.raster.array import extract_from_array, prepare_masked_array
from mapchete.tile import BufferedTile
import numpy.ma as ma

class RasterWindowMemoryFile:
    """"""Context manager around rasterio.io.MemoryFile.""""""

    def __init__(self, in_tile: BufferedTile, in_data: ma.MaskedArray, out_profile: Profile, out_tile: Optional[BufferedTile]=None, tags: Optional[Dict[str, Any]]=None):
        """"""Prepare data & profile.""""""
        out_tile = out_tile or in_tile
        validate_write_window_params(in_tile, out_tile, in_data, out_profile)
        self.data = extract_from_array(array=in_data, in_affine=in_tile.affine, out_tile=out_tile)
        if 'affine' in out_profile:
            out_profile['transform'] = out_profile.pop('affine')
        self.profile = out_profile
        self.tags = tags

    def __enter__(self):
        """"""Open MemoryFile, write data and return.""""""
        self.rio_memfile = MemoryFile()
        with self.rio_memfile.open(**self.profile) as dst:
            dst.write(self.data.astype(self.profile['dtype'], copy=False))
            _write_tags(dst, self.tags)
        return self.rio_memfile

    def __exit__(self, *args):
        """"""Make sure MemoryFile is closed.""""""
        self.rio_memfile.close()","
class RasterWindowMemoryFile:
    '''Context manager around rasterio.io.MemoryFile.'''

    def __init__(self, in_tile: BufferedTile, in_data: ma.MaskedArray, out_profile: Profile, out_tile: Optional[BufferedTile]=None, tags: Optional[Dict[str, Any]]=None):
        '''Prepare data & profile.'''
        pass

    def __enter__(self):
        '''Open MemoryFile, write data and return.'''
        pass

    def __exit__(self, *args):
        '''Make sure MemoryFile is closed.'''
        pass",4,4,8.0,0.0,7.0,1.0,1.0,0.24,0.0,0.0,0.0,0.0,3.0,4.0,3.0,3.0,29.0,3.0,21.0,11.0,15.0,5.0,17.0,8.0,13.0,2.0,0.0,1.0,4.0,snippet_167
256858,cherrypy/cheroot,cheroot/server.py,cheroot.server.Gateway,"class Gateway:
    """"""Base class to interface HTTPServer with other systems, such as WSGI.""""""

    def __init__(self, req):
        """"""Initialize Gateway instance with request.

        Args:
            req (HTTPRequest): current HTTP request
        """"""
        self.req = req

    def respond(self):
        """"""Process the current request. Must be overridden in a subclass.""""""
        raise NotImplementedError","class Gateway:
    '''Base class to interface HTTPServer with other systems, such as WSGI.'''

    def __init__(self, req):
        '''Initialize Gateway instance with request.
        Args:
            req (HTTPRequest): current HTTP request
        '''
        pass

    def respond(self):
        '''Process the current request. Must be overridden in a subclass.'''
        pass",3,3,5.0,1.0,2.0,3.0,1.0,1.4,0.0,1.0,0.0,2.0,2.0,1.0,2.0,2.0,14.0,3.0,5.0,4.0,2.0,7.0,5.0,4.0,2.0,1.0,0.0,0.0,2.0,snippet_168
258632,chovanecm/sacredboard,chovanecm_sacredboard/sacredboard/app/data/datastorage.py,sacredboard.app.data.datastorage.Cursor,"class Cursor:
    """"""Interface that abstracts the cursor object returned from databases.""""""

    def __init__(self):
        """"""Declare a new cursor to iterate over runs.""""""
        pass

    def count(self):
        """"""Return the number of items in this cursor.""""""
        raise NotImplementedError()

    def __iter__(self):
        """"""Iterate over elements.""""""
        raise NotImplementedError()","class Cursor:
    '''Interface that abstracts the cursor object returned from databases.'''

    def __init__(self):
        '''Declare a new cursor to iterate over runs.'''
        pass

    def count(self):
        '''Return the number of items in this cursor.'''
        pass

    def __iter__(self):
        '''Iterate over elements.'''
        pass",4,4,3.0,0.0,2.0,1.0,1.0,0.57,0.0,1.0,0.0,3.0,3.0,0.0,3.0,3.0,14.0,3.0,7.0,4.0,3.0,4.0,7.0,4.0,3.0,1.0,0.0,0.0,3.0,snippet_169
258643,chovanecm/sacredboard,chovanecm_sacredboard/sacredboard/app/data/metricsdao.py,sacredboard.app.data.metricsdao.MetricsDAO,"class MetricsDAO:
    """"""
    Interface for accessing Sacred metrics.

    Issue: https://github.com/chovanecm/sacredboard/issues/58

    Extended because of: https://github.com/chovanecm/sacredboard/issues/66
    """"""

    def get(self, run_id, metric_id):
        """"""
        Read a metric of the given id and run.

        The returned object has the following format (timestamps are datetime
         objects).

        .. code::

            {""steps"": [0,1,20,40,...],
            ""timestamps"": [timestamp1,timestamp2,timestamp3,...],
            ""values"": [0,1 2,3,4,5,6,...],
            ""name"": ""name of the metric"",
            ""metric_id"": ""metric_id"",
            ""run_id"": ""run_id""}

        :param run_id: ID of the Run that the metric belongs to.
        :param metric_id: The ID fo the metric.
        :return: The whole metric as specified.

        :raise NotFoundError
        """"""
        raise NotImplementedError('The MetricsDAO class is abstract.')

    def delete(self, run_id):
        """"""
        Delete all metrics belonging to the given run.

        :param run_id: ID of the Run that the metric belongs to.
        """"""
        raise NotImplementedError('The MetricsDAO class is abstract.')","class MetricsDAO:
    '''
    Interface for accessing Sacred metrics.
    Issue: https://github.com/chovanecm/sacredboard/issues/58
    Extended because of: https://github.com/chovanecm/sacredboard/issues/66
    '''

    def get(self, run_id, metric_id):
        '''
        Read a metric of the given id and run.
        The returned object has the following format (timestamps are datetime
         objects).
        .. code::
            {""steps"": [0,1,20,40,...],
            ""timestamps"": [timestamp1,timestamp2,timestamp3,...],
            ""values"": [0,1 2,3,4,5,6,...],
            ""name"": ""name of the metric"",
            ""metric_id"": ""metric_id"",
            ""run_id"": ""run_id""}
        :param run_id: ID of the Run that the metric belongs to.
        :param metric_id: The ID fo the metric.
        :return: The whole metric as specified.
        :raise NotFoundError
        '''
        pass

    def delete(self, run_id):
        '''
        Delete all metrics belonging to the given run.
        :param run_id: ID of the Run that the metric belongs to.
        '''
        pass",3,3,15.0,3.0,2.0,10.0,1.0,5.0,0.0,1.0,0.0,2.0,2.0,0.0,2.0,2.0,40.0,10.0,5.0,3.0,2.0,25.0,5.0,3.0,2.0,1.0,0.0,0.0,2.0,snippet_170
258651,chovanecm/sacredboard,chovanecm_sacredboard/sacredboard/app/data/rundao.py,sacredboard.app.data.rundao.RunDAO,"class RunDAO:
    """"""
    Interface for accessing Runs.

    Issue: https://github.com/chovanecm/sacredboard/issues/69
    """"""

    def get(self, run_id):
        """"""
        Return the run associated with the id.

        :raise NotFoundError when not found
        """"""
        raise NotImplementedError('RunDAO is abstract.')

    def get_runs(self, sort_by=None, sort_direction=None, start=0, limit=None, query={'type': 'and', 'filters': []}):
        """"""Return all runs that match the query.""""""
        raise NotImplementedError('RunDAO is abstract.')

    def delete(self, run_id):
        """"""
        Delete run with the given id from the backend.

        :param run_id: Id of the run to delete.
        :raise NotImplementedError If not supported by the backend.
        :raise DataSourceError General data source error.
        :raise NotFoundError The run was not found. (Some backends may succeed
        even if the run does not exist.
        """"""
        raise NotImplementedError('This database data source does not currently support this operation.')","class RunDAO:
    '''
    Interface for accessing Runs.
    Issue: https://github.com/chovanecm/sacredboard/issues/69
    '''

    def get(self, run_id):
        '''
        Return the run associated with the id.
        :raise NotFoundError when not found
        '''
        pass

    def get_runs(self, sort_by=None, sort_direction=None, start=0, limit=None, query={'type': 'and', 'filters': []}):
        '''Return all runs that match the query.'''
        pass

    def delete(self, run_id):
        '''
        Delete run with the given id from the backend.
        :param run_id: Id of the run to delete.
        :raise NotImplementedError If not supported by the backend.
        :raise DataSourceError General data source error.
        :raise NotFoundError The run was not found. (Some backends may succeed
        even if the run does not exist.
        '''
        pass",4,4,8.0,1.0,3.0,4.0,1.0,1.7,0.0,1.0,0.0,2.0,3.0,0.0,3.0,3.0,33.0,6.0,10.0,5.0,5.0,17.0,7.0,4.0,3.0,1.0,0.0,0.0,3.0,snippet_171
259541,mozilla-iot/webthing-python,mozilla-iot_webthing-python/webthing/server.py,webthing.server.MultipleThings,"class MultipleThings:
    """"""A container for multiple things.""""""

    def __init__(self, things, name):
        """"""
        Initialize the container.

        things -- the things to store
        name -- the mDNS server name
        """"""
        self.things = things
        self.name = name

    def get_thing(self, idx):
        """"""
        Get the thing at the given index.

        idx -- the index
        """"""
        try:
            idx = int(idx)
        except ValueError:
            return None
        if idx < 0 or idx >= len(self.things):
            return None
        return self.things[idx]

    def get_things(self):
        """"""Get the list of things.""""""
        return self.things

    def get_name(self):
        """"""Get the mDNS server name.""""""
        return self.name","class MultipleThings:
    '''A container for multiple things.'''

    def __init__(self, things, name):
        '''
        Initialize the container.
        things -- the things to store
        name -- the mDNS server name
        '''
        pass

    def get_thing(self, idx):
        '''
        Get the thing at the given index.
        idx -- the index
        '''
        pass

    def get_things(self):
        '''Get the list of things.'''
        pass

    def get_name(self):
        '''Get the mDNS server name.'''
        pass",5,5,8.0,1.0,4.0,3.0,2.0,0.75,0.0,2.0,0.0,0.0,4.0,2.0,4.0,4.0,36.0,8.0,16.0,7.0,11.0,12.0,16.0,7.0,11.0,3.0,0.0,1.0,6.0,snippet_172
259544,mozilla-iot/webthing-python,mozilla-iot_webthing-python/webthing/server.py,webthing.server.SingleThing,"class SingleThing:
    """"""A container for a single thing.""""""

    def __init__(self, thing):
        """"""
        Initialize the container.

        thing -- the thing to store
        """"""
        self.thing = thing

    def get_thing(self, _=None):
        """"""Get the thing at the given index.""""""
        return self.thing

    def get_things(self):
        """"""Get the list of things.""""""
        return [self.thing]

    def get_name(self):
        """"""Get the mDNS server name.""""""
        return self.thing.title","class SingleThing:
    '''A container for a single thing.'''

    def __init__(self, thing):
        '''
        Initialize the container.
        thing -- the thing to store
        '''
        pass

    def get_thing(self, _=None):
        '''Get the thing at the given index.'''
        pass

    def get_things(self):
        '''Get the list of things.'''
        pass

    def get_name(self):
        '''Get the mDNS server name.'''
        pass",5,5,4.0,0.0,2.0,2.0,1.0,0.89,0.0,0.0,0.0,0.0,4.0,1.0,4.0,4.0,22.0,5.0,9.0,6.0,4.0,8.0,9.0,6.0,4.0,1.0,0.0,0.0,4.0,snippet_173
259548,mozilla-iot/webthing-python,mozilla-iot_webthing-python/webthing/subscriber.py,webthing.subscriber.Subscriber,"class Subscriber:
    """"""Abstract Subscriber class.""""""

    def update_property(self, property_):
        """"""
        Send an update about a Property.

        :param property_: Property
        """"""
        raise NotImplementedError

    def update_action(self, action):
        """"""
        Send an update about an Action.

        :param action: Action
        """"""
        raise NotImplementedError

    def update_event(self, event):
        """"""
        Send an update about an Event.

        :param event: Event
        """"""
        raise NotImplementedError","class Subscriber:
    '''Abstract Subscriber class.'''

    def update_property(self, property_):
        '''
        Send an update about a Property.
        :param property_: Property
        '''
        pass

    def update_action(self, action):
        '''
        Send an update about an Action.
        :param action: Action
        '''
        pass

    def update_event(self, event):
        '''
        Send an update about an Event.
        :param event: Event
        '''
        pass",4,4,7.0,1.0,2.0,4.0,1.0,1.86,0.0,1.0,0.0,1.0,3.0,0.0,3.0,3.0,26.0,6.0,7.0,4.0,3.0,13.0,7.0,4.0,3.0,1.0,0.0,0.0,3.0,snippet_174
261819,user-cont/conu,user-cont_conu/conu/apidefs/image.py,conu.apidefs.image.S2Image,"class S2Image:
    """"""
    Additional functionality related to s2i-enabled container images
    """"""

    def extend(self, source, new_image_name, s2i_args=None):
        """"""
        extend this s2i-enabled image using provided source, raises ConuException if
        `s2i build` fails

        :param source: str, source used to extend the image, can be path or url
        :param new_image_name: str, name of the new, extended image
        :param s2i_args: list of str, additional options and arguments provided to `s2i build`
        :return: S2Image instance
        """"""
        raise NotImplementedError('extend method is not implemented')

    def usage(self):
        """"""
        Provide output of `s2i usage`

        :return: str
        """"""
        raise NotImplementedError('usage method is not implemented')","class S2Image:
    '''
    Additional functionality related to s2i-enabled container images
    '''

    def extend(self, source, new_image_name, s2i_args=None):
        '''
        extend this s2i-enabled image using provided source, raises ConuException if
        `s2i build` fails
        :param source: str, source used to extend the image, can be path or url
        :param new_image_name: str, name of the new, extended image
        :param s2i_args: list of str, additional options and arguments provided to `s2i build`
        :return: S2Image instance
        '''
        pass

    def usage(self):
        '''
        Provide output of `s2i usage`
        :return: str
        '''
        pass",3,3,9.0,1.0,2.0,6.0,1.0,3.0,0.0,1.0,0.0,1.0,2.0,0.0,2.0,2.0,24.0,4.0,5.0,3.0,2.0,15.0,5.0,3.0,2.0,1.0,0.0,0.0,2.0,snippet_175
264367,materialsproject/custodian,materialsproject_custodian/src/custodian/ansible/interpreter.py,custodian.ansible.interpreter.Modder,"from custodian.ansible.actions import DictActions
import re

class Modder:
    """"""
    Class to modify a dict/file/any object using a mongo-like language.
    Keywords are mostly adopted from mongo's syntax, but instead of $, an
    underscore precedes action keywords. This is so that the modification can
    be inserted into a mongo db easily.

    Allowable actions are supplied as a list of classes as an argument. Refer
    to the action classes on what the actions do. Action classes are in
    pymatpro.ansible.actions.

    Examples:
    >>> modder = Modder()
    >>> dct = {""Hello"": ""World""}
    >>> mod = {'_set': {'Hello':'Universe', 'Bye': 'World'}}
    >>> modder.modify(mod, dct)
    >>> dct['Bye']
    'World'
    >>> dct['Hello']
    'Universe'
    """"""

    def __init__(self, actions=None, strict=True, directory='./') -> None:
        """"""Initialize a Modder from a list of supported actions.

        Args:
            actions ([Action]): A sequence of supported actions. See
                :mod:`custodian.ansible.actions`. Default is None,
                which means only DictActions are supported.
            strict (bool): Indicating whether to use strict mode. In non-strict
                mode, unsupported actions are simply ignored without any
                errors raised. In strict mode, if an unsupported action is
                supplied, a ValueError is raised. Defaults to True.
            directory (str): The directory containing the files to be modified.
                Defaults to ""./"".
        """"""
        self.supported_actions = {}
        actions = actions if actions is not None else [DictActions]
        for action in actions:
            for attr in dir(action):
                if not re.match('__\\w+__', attr) and callable(getattr(action, attr)):
                    self.supported_actions[f'_{attr}'] = getattr(action, attr)
        self.strict = strict
        self.directory = directory

    def modify(self, modification, obj) -> None:
        """"""
        Note that modify makes actual in-place modifications. It does not
        return a copy.

        Args:
            modification (dict): Modification must be {action_keyword :
                settings}. E.g., {'_set': {'Hello':'Universe', 'Bye': 'World'}}
            obj (dict/str/object): Object to modify depending on actions. For
                example, for DictActions, obj will be a dict to be modified.
                For FileActions, obj will be a string with a full pathname to a
                file.
        """"""
        for action, settings in modification.items():
            if action in self.supported_actions:
                self.supported_actions[action](obj, settings, directory=self.directory)
            elif self.strict:
                raise ValueError(f'{action} is not a supported action!')

    def modify_object(self, modification, obj):
        """"""
        Modify an object that supports pymatgen's as_dict() and from_dict API.

        Args:
            modification (dict): Modification must be {action_keyword :
                settings}. E.g., {'_set': {'Hello':'Universe', 'Bye': 'World'}}
            obj (object): Object to modify
        """"""
        dct = obj.as_dict()
        self.modify(modification, dct)
        return obj.from_dict(dct)","
class Modder:
    '''
    Class to modify a dict/file/any object using a mongo-like language.
    Keywords are mostly adopted from mongo's syntax, but instead of $, an
    underscore precedes action keywords. This is so that the modification can
    be inserted into a mongo db easily.
    Allowable actions are supplied as a list of classes as an argument. Refer
    to the action classes on what the actions do. Action classes are in
    pymatpro.ansible.actions.
    Examples:
    >>> modder = Modder()
    >>> dct = {""Hello"": ""World""}
    >>> mod = {'_set': {'Hello':'Universe', 'Bye': 'World'}}
    >>> modder.modify(mod, dct)
    >>> dct['Bye']
    'World'
    >>> dct['Hello']
    'Universe'
    '''

    def __init__(self, actions=None, strict=True, directory='./') -> None:
        '''Initialize a Modder from a list of supported actions.
        Args:
            actions ([Action]): A sequence of supported actions. See
                :mod:`custodian.ansible.actions`. Default is None,
                which means only DictActions are supported.
            strict (bool): Indicating whether to use strict mode. In non-strict
                mode, unsupported actions are simply ignored without any
                errors raised. In strict mode, if an unsupported action is
                supplied, a ValueError is raised. Defaults to True.
            directory (str): The directory containing the files to be modified.
                Defaults to ""./"".
        '''
        pass

    def modify(self, modification, obj) -> None:
        '''
        Note that modify makes actual in-place modifications. It does not
        return a copy.
        Args:
            modification (dict): Modification must be {action_keyword :
                settings}. E.g., {'_set': {'Hello':'Universe', 'Bye': 'World'}}
            obj (dict/str/object): Object to modify depending on actions. For
                example, for DictActions, obj will be a dict to be modified.
                For FileActions, obj will be a string with a full pathname to a
                file.
        '''
        pass

    def modify_object(self, modification, obj):
        '''
        Modify an object that supports pymatgen's as_dict() and from_dict API.
        Args:
            modification (dict): Modification must be {action_keyword :
                settings}. E.g., {'_set': {'Hello':'Universe', 'Bye': 'World'}}
            obj (object): Object to modify
        '''
        pass",4,4,17.0,1.0,6.0,10.0,3.0,2.4,0.0,2.0,1.0,3.0,3.0,3.0,3.0,3.0,76.0,8.0,20.0,11.0,16.0,48.0,19.0,11.0,15.0,5.0,0.0,3.0,10.0,snippet_176
264406,materialsproject/custodian,materialsproject_custodian/src/custodian/utils.py,custodian.utils.tracked_lru_cache,"import functools
from typing import ClassVar

class tracked_lru_cache:
    """"""
    Decorator wrapping the functools.lru_cache adding a tracking of the
    functions that have been wrapped.

    Exposes a method to clear the cache of all the wrapped functions.

    Used to cache the parsed outputs in handlers/validators, to avoid
    multiple parsing of the same file.
    Allows Custodian to clear the cache after all the checks have been performed.
    """"""
    cached_functions: ClassVar = set()

    def __init__(self, func) -> None:
        """"""
        Args:
            func: function to be decorated.
        """"""
        self.func = functools.lru_cache(func)
        functools.update_wrapper(self, func)
        self.cache_info = self.func.cache_info
        self.cache_clear = self.func.cache_clear

    def __call__(self, *args, **kwargs):
        """"""Call the decorated function.""""""
        result = self.func(*args, **kwargs)
        self.cached_functions.add(self.func)
        return result

    @classmethod
    def tracked_cache_clear(cls) -> None:
        """"""Clear the cache of all the decorated functions.""""""
        while cls.cached_functions:
            f = cls.cached_functions.pop()
            f.cache_clear()","
class tracked_lru_cache:
    '''
    Decorator wrapping the functools.lru_cache adding a tracking of the
    functions that have been wrapped.
    Exposes a method to clear the cache of all the wrapped functions.
    Used to cache the parsed outputs in handlers/validators, to avoid
    multiple parsing of the same file.
    Allows Custodian to clear the cache after all the checks have been performed.
    '''

    def __init__(self, func) -> None:
        '''
        Args:
            func: function to be decorated.
        '''
        pass

    def __call__(self, *args, **kwargs):
        '''Call the decorated function.'''
        pass
    @classmethod
    def tracked_cache_clear(cls) -> None:
        '''Clear the cache of all the decorated functions.'''
        pass",4,4,7.0,0.0,4.0,2.0,1.0,0.94,0.0,0.0,0.0,0.0,2.0,3.0,3.0,3.0,38.0,7.0,16.0,11.0,11.0,15.0,15.0,10.0,11.0,2.0,0.0,1.0,4.0,snippet_177
265199,Anaconda-Platform/anaconda-client,Anaconda-Platform_anaconda-client/binstar_client/utils/tables.py,binstar_client.utils.tables.TableCell,"import typing

class TableCell:
    """"""
    General definition of a table cell.

    :param kind: Kind of the cell (used for styling purposes, see :class:`~TableDesign`).
    :param value: Exact content of the cell.
    :param alignment: How text should be aligned in the cell.
    """"""
    __slots__ = ('alignment', 'kind', 'value')

    def __init__(self, kind: str, value: typing.Any, *, alignment: 'Alignment'='<') -> None:
        """"""Initialize new :class:`~TableCell` instance.""""""
        if value is None:
            value = ''
        self.alignment: 'Alignment' = alignment
        self.kind: str = kind
        self.value: str = str(value)

    def __repr__(self) -> str:
        """"""Prepare a string representation of the instance.""""""
        return f'{type(self).__name__}(kind={self.kind!r}, value={self.value!r}, alignment={self.alignment!r})'

    def __str__(self) -> str:
        """"""Prepare a string representation of the instance.""""""
        return self.value","
class TableCell:
    '''
    General definition of a table cell.
    :param kind: Kind of the cell (used for styling purposes, see :class:`~TableDesign`).
    :param value: Exact content of the cell.
    :param alignment: How text should be aligned in the cell.
    '''

    def __init__(self, kind: str, value: typing.Any, *, alignment: 'Alignment'='<') -> None:
        '''Initialize new :class:`~TableCell` instance.'''
        pass

    def __repr__(self) -> str:
        '''Prepare a string representation of the instance.'''
        pass

    def __str__(self) -> str:
        '''Prepare a string representation of the instance.'''
        pass",4,4,7.0,0.0,5.0,1.0,1.0,0.5,0.0,3.0,0.0,0.0,3.0,3.0,3.0,3.0,33.0,6.0,18.0,14.0,8.0,9.0,12.0,8.0,8.0,2.0,0.0,1.0,4.0,snippet_178
265465,mrname/haralyzer,mrname_haralyzer/haralyzer/mixins.py,haralyzer.mixins.GetHeaders,"from typing import Any, Optional
from functools import cached_property

class GetHeaders:
    """"""Mixin to get a header""""""

    def get_header_value(self, name: str) -> Optional[str]:
        """"""
        Returns the header value of the header defined in ``name``

        :param name: Name of the header to get the value of
        :type name: str
        :return: Value of the header
        :rtype: Optional[str]
        """"""
        for header in self.raw_entry['headers']:
            if header['name'].lower() == name.lower():
                return header['value']
        return None

    @cached_property
    def _formatted_headers(self) -> str:
        """"""
        Returns a formatted string of the headers in `KEY: VALUE` format

        :return: string of all headers
        :rtype: str
        """"""
        formatted_headers = ''
        for header in self.raw_entry['headers']:
            name, value = (header['name'], header['value'])
            formatted_headers += f'{name}: {value}\n'
        return formatted_headers","
class GetHeaders:
    '''Mixin to get a header'''

    def get_header_value(self, name: str) -> Optional[str]:
        '''
        Returns the header value of the header defined in ``name``
        :param name: Name of the header to get the value of
        :type name: str
        :return: Value of the header
        :rtype: Optional[str]
        '''
        pass
    @cached_property
    def _formatted_headers(self) -> str:
        '''
        Returns a formatted string of the headers in `KEY: VALUE` format
        :return: string of all headers
        :rtype: str
        '''
        pass",3,3,14.0,2.0,6.0,6.0,3.0,1.08,0.0,1.0,0.0,1.0,2.0,0.0,2.0,2.0,33.0,6.0,13.0,8.0,9.0,14.0,12.0,7.0,9.0,3.0,0.0,2.0,5.0,snippet_179
266878,facelessuser/wcmatch,facelessuser_wcmatch/wcmatch/util.py,wcmatch.util.Immutable,"from typing import Any, Callable, AnyStr, Match, Pattern

class Immutable:
    """"""Immutable.""""""
    __slots__: tuple[Any, ...] = ()

    def __init__(self, **kwargs: Any) -> None:
        """"""Initialize.""""""
        for k, v in kwargs.items():
            super(Immutable, self).__setattr__(k, v)

    def __setattr__(self, name: str, value: Any) -> None:
        """"""Prevent mutability.""""""
        raise AttributeError('Class is immutable!')","
class Immutable:
    '''Immutable.'''

    def __init__(self, **kwargs: Any) -> None:
        '''Initialize.'''
        pass

    def __setattr__(self, name: str, value: Any) -> None:
        '''Prevent mutability.'''
        pass",3,3,5.0,1.0,3.0,2.0,2.0,0.57,0.0,4.0,0.0,1.0,2.0,0.0,2.0,2.0,15.0,5.0,7.0,5.0,4.0,4.0,7.0,5.0,4.0,2.0,0.0,1.0,3.0,snippet_180
269275,CLARIAH/grlc,CLARIAH_grlc/src/fileLoaders.py,src.fileLoaders.BaseLoader,"from grlc.queryTypes import qType, guessQueryType
import json

class BaseLoader:
    """"""Base class for File Loaders""""""

    def getTextForName(self, query_name):
        """"""Return the query text and query type for the given query name.
        Note that file extention is not part of the query name. For example,
        for `query_name='query1'` would return the content of file `query1.rq`
        from the loader's source (assuming such file exists).""""""
        candidateNames = [query_name + '.rq', query_name + '.sparql', query_name + '.tpf', query_name + '.json']
        candidates = [(name, guessQueryType(name)) for name in candidateNames]
        for queryFullName, queryType in candidates:
            queryText = self._getText(queryFullName)
            if queryText:
                if queryType == qType['JSON']:
                    queryText = json.loads(queryText)
                    if 'proto' not in queryText and '@graph' not in queryText:
                        continue
                return (queryText, queryType)
        return ('', None)

    def _getText(self, queryFullName):
        """"""To be implemented by sub-classes.
        Returns None if the file does not exist.""""""
        raise NotImplementedError('Subclasses must override _getText()!')

    def fetchFiles(self):
        """"""To be implemented by sub-classes""""""
        raise NotImplementedError('Subclasses must override fetchFiles()!')","
class BaseLoader:
    '''Base class for File Loaders'''

    def getTextForName(self, query_name):
        '''Return the query text and query type for the given query name.
        Note that file extention is not part of the query name. For example,
        for `query_name='query1'` would return the content of file `query1.rq`
        from the loader's source (assuming such file exists).'''
        pass

    def _getText(self, queryFullName):
        '''To be implemented by sub-classes.
        Returns None if the file does not exist.'''
        pass

    def fetchFiles(self):
        '''To be implemented by sub-classes'''
        pass",4,4,10.0,0.0,7.0,3.0,2.0,0.45,0.0,1.0,0.0,4.0,3.0,0.0,3.0,3.0,36.0,4.0,22.0,8.0,18.0,10.0,17.0,8.0,13.0,5.0,0.0,4.0,7.0,snippet_181
271814,shoebot/shoebot,shoebot_shoebot/lib/photobot/__init__.py,photobot.Blend,"from PIL import Image, ImageChops, ImageFilter, ImageEnhance, ImageOps, ImageDraw, ImageStat

class Blend:
    """"""Layer blending modes.

    Implements additional blending modes to those present in PIL.
    These blending functions can not be used separately from
    the canvas.flatten() method, where the alpha compositing
    of two layers is handled.

    Since these blending are not part of a C library,
    but pure Python, they take forever to process.
    """"""

    def overlay(self, img1, img2):
        """"""Applies the overlay blend mode.

        Overlays image img2 on image img1. The overlay pixel combines
        multiply and screen: it multiplies dark pixels values and screen
        light values. Returns a composite image with the alpha channel
        retained.
        """"""
        p1 = list(img1.getdata())
        p2 = list(img2.getdata())
        for i in range(len(p1)):
            p3 = ()
            for j in range(len(p1[i])):
                a = p1[i][j] / 255.0
                b = p2[i][j] / 255.0
                if j == 3:
                    d = min(a, b)
                elif a > 0.5:
                    d = 2 * (a + b - a * b) - 1
                else:
                    d = 2 * a * b
                p3 += (int(d * 255),)
            p1[i] = p3
        img = Image.new('RGBA', img1.size, 255)
        img.putdata(p1)
        return img

    def hue(self, img1, img2):
        """"""Applies the hue blend mode.

        Hues image img1 with image img2. The hue filter replaces the
        hues of pixels in img1 with the hues of pixels in img2. Returns
        a composite image with the alpha channel retained.
        """"""
        import colorsys
        p1 = list(img1.getdata())
        p2 = list(img2.getdata())
        for i in range(len(p1)):
            r1, g1, b1, a1 = p1[i]
            r1 = r1 / 255.0
            g1 = g1 / 255.0
            b1 = b1 / 255.0
            h1, s1, v1 = colorsys.rgb_to_hsv(r1, g1, b1)
            r2, g2, b2, a2 = p2[i]
            r2 = r2 / 255.0
            g2 = g2 / 255.0
            b2 = b2 / 255.0
            h2, s2, v2 = colorsys.rgb_to_hsv(r2, g2, b2)
            r3, g3, b3 = colorsys.hsv_to_rgb(h2, s1, v1)
            r3 = int(r3 * 255)
            g3 = int(g3 * 255)
            b3 = int(b3 * 255)
            p1[i] = (r3, g3, b3, a1)
        img = Image.new('RGBA', img1.size, 255)
        img.putdata(p1)
        return img

    def color(self, img1, img2):
        """"""Applies the color blend mode.

        Colorize image img1 with image img2. The color filter replaces
        the hue and saturation of pixels in img1 with the hue and
        saturation of pixels in img2. Returns a composite image with the
        alpha channel retained.
        """"""
        import colorsys
        p1 = list(img1.getdata())
        p2 = list(img2.getdata())
        for i in range(len(p1)):
            r1, g1, b1, a1 = p1[i]
            r1 = r1 / 255.0
            g1 = g1 / 255.0
            b1 = b1 / 255.0
            h1, s1, v1 = colorsys.rgb_to_hsv(r1, g1, b1)
            r2, g2, b2, a2 = p2[i]
            r2 = r2 / 255.0
            g2 = g2 / 255.0
            b2 = b2 / 255.0
            h2, s2, v2 = colorsys.rgb_to_hsv(r2, g2, b2)
            r3, g3, b3 = colorsys.hsv_to_rgb(h2, s2, v1)
            r3 = int(r3 * 255)
            g3 = int(g3 * 255)
            b3 = int(b3 * 255)
            p1[i] = (r3, g3, b3, a1)
        img = Image.new('RGBA', img1.size, 255)
        img.putdata(p1)
        return img","
class Blend:
    '''Layer blending modes.
    Implements additional blending modes to those present in PIL.
    These blending functions can not be used separately from
    the canvas.flatten() method, where the alpha compositing
    of two layers is handled.
    Since these blending are not part of a C library,
    but pure Python, they take forever to process.
    '''

    def overlay(self, img1, img2):
        '''Applies the overlay blend mode.
        Overlays image img2 on image img1. The overlay pixel combines
        multiply and screen: it multiplies dark pixels values and screen
        light values. Returns a composite image with the alpha channel
        retained.
        '''
        pass

    def hue(self, img1, img2):
        '''Applies the hue blend mode.
        Hues image img1 with image img2. The hue filter replaces the
        hues of pixels in img1 with the hues of pixels in img2. Returns
        a composite image with the alpha channel retained.
        '''
        pass

    def color(self, img1, img2):
        '''Applies the color blend mode.
        Colorize image img1 with image img2. The color filter replaces
        the hue and saturation of pixels in img1 with the hue and
        saturation of pixels in img2. Returns a composite image with the
        alpha channel retained.
        '''
        pass",4,4,39.0,10.0,22.0,7.0,3.0,0.44,0.0,3.0,0.0,0.0,3.0,0.0,3.0,3.0,131.0,36.0,66.0,33.0,60.0,29.0,64.0,33.0,58.0,5.0,0.0,3.0,9.0,snippet_182
272626,cackharot/suds-py3,cackharot_suds-py3/suds/reader.py,suds.reader.Reader,"from suds.plugin import PluginContainer
import hashlib

class Reader:
    """"""
    The reader provides integration with cache.
    @ivar options: An options object.
    @type options: I{Options}
    """"""

    def __init__(self, options):
        """"""
        @param options: An options object.
        @type options: I{Options}
        """"""
        self.options = options
        self.plugins = PluginContainer(options.plugins)

    def mangle(self, name, x):
        """"""
        Mangle the name by hashing the I{name} and appending I{x}.
        @return: the mangled name.
        """"""
        h = hashlib.sha256(name.encode('utf8')).hexdigest()
        return '%s-%s' % (h, x)","
class Reader:
    '''
    The reader provides integration with cache.
    @ivar options: An options object.
    @type options: I{Options}
    '''

    def __init__(self, options):
        '''
        @param options: An options object.
        @type options: I{Options}
        '''
        pass

    def mangle(self, name, x):
        '''
        Mangle the name by hashing the I{name} and appending I{x}.
        @return: the mangled name.
        '''
        pass",3,3,7.0,0.0,3.0,4.0,1.0,1.86,0.0,1.0,1.0,2.0,2.0,2.0,2.0,2.0,22.0,2.0,7.0,6.0,4.0,13.0,7.0,6.0,4.0,1.0,0.0,0.0,2.0,snippet_183
272646,cackharot/suds-py3,cackharot_suds-py3/suds/sax/enc.py,suds.sax.enc.Encoder,"import re

class Encoder:
    """"""
    An XML special character encoder/decoder.
    @cvar encodings: A mapping of special characters encoding.
    @type encodings: [(str,str)]
    @cvar decodings: A mapping of special characters decoding.
    @type decodings: [(str,str)]
    @cvar special: A list of special characters
    @type special: [char]
    """"""
    encodings = (('&', '&amp;'), ('<', '&lt;'), ('>', '&gt;'), ('""', '&quot;'), (""'"", '&apos;'))
    decodings = (('&lt;', '<'), ('&gt;', '>'), ('&quot;', '""'), ('&apos;', ""'""), ('&amp;', '&'))
    special = ('&', '<', '>', '""', ""'"")

    def needsEncoding(self, s):
        """"""
        Get whether string I{s} contains special characters.
        @param s: A string to check.
        @type s: str
        @return: True if needs encoding.
        @rtype: boolean
        """"""
        if isinstance(s, str):
            for c in self.special:
                if c in s:
                    return True
        return False

    def encode(self, s):
        """"""
        Encode special characters found in string I{s}.
        @param s: A string to encode.
        @type s: str
        @return: The encoded string.
        @rtype: str
        """"""
        if isinstance(s, str) and self.needsEncoding(s):
            for x in self.encodings:
                s = re.sub(x[0], x[1], s)
        return s

    def decode(self, s):
        """"""
        Decode special characters encodings found in string I{s}.
        @param s: A string to decode.
        @type s: str
        @return: The decoded string.
        @rtype: str
        """"""
        if isinstance(s, str) and '&' in s:
            for x in self.decodings:
                s = s.replace(x[0], x[1])
        return s","
class Encoder:
    '''
    An XML special character encoder/decoder.
    @cvar encodings: A mapping of special characters encoding.
    @type encodings: [(str,str)]
    @cvar decodings: A mapping of special characters decoding.
    @type decodings: [(str,str)]
    @cvar special: A list of special characters
    @type special: [char]
    '''

    def needsEncoding(self, s):
        '''
        Get whether string I{s} contains special characters.
        @param s: A string to check.
        @type s: str
        @return: True if needs encoding.
        @rtype: boolean
        '''
        pass

    def encode(self, s):
        '''
        Encode special characters found in string I{s}.
        @param s: A string to encode.
        @type s: str
        @return: The encoded string.
        @rtype: str
        '''
        pass

    def decode(self, s):
        '''
        Decode special characters encodings found in string I{s}.
        @param s: A string to decode.
        @type s: str
        @return: The decoded string.
        @rtype: str
        '''
        pass",4,4,12.0,0.0,5.0,7.0,3.0,0.94,0.0,1.0,0.0,0.0,3.0,0.0,3.0,3.0,66.0,4.0,32.0,10.0,28.0,30.0,20.0,10.0,16.0,4.0,0.0,3.0,10.0,snippet_184
272664,cackharot/suds-py3,cackharot_suds-py3/suds/transport/__init__.py,suds.transport.Transport,"class Transport:
    """"""
    The transport I{interface}.
    """"""

    def __init__(self):
        """"""
        Constructor.
        """"""
        from suds.transport.options import Options
        self.options = Options()
        del Options

    def open(self, request):
        """"""
        Open the url in the specified request.
        @param request: A transport request.
        @type request: L{Request}
        @return: An input stream.
        @rtype: stream
        @raise TransportError: On all transport errors.
        """"""
        raise Exception('not-implemented')

    def send(self, request):
        """"""
        Send soap message.  Implementations are expected to handle:
            - proxies
            - I{http} headers
            - cookies
            - sending message
            - brokering exceptions into L{TransportError}
        @param request: A transport request.
        @type request: L{Request}
        @return: The reply
        @rtype: L{Reply}
        @raise TransportError: On all transport errors.
        """"""
        raise Exception('not-implemented')","class Transport:
    '''
    The transport I{interface}.
    '''

    def __init__(self):
        '''
        Constructor.
        '''
        pass

    def open(self, request):
        '''
        Open the url in the specified request.
        @param request: A transport request.
        @type request: L{Request}
        @return: An input stream.
        @rtype: stream
        @raise TransportError: On all transport errors.
        '''
        pass

    def send(self, request):
        '''
        Send soap message.  Implementations are expected to handle:
            - proxies
            - I{http} headers
            - cookies
            - sending message
            - brokering exceptions into L{TransportError}
        @param request: A transport request.
        @type request: L{Request}
        @return: The reply
        @rtype: L{Reply}
        @raise TransportError: On all transport errors.
        '''
        pass",4,4,11.0,0.0,3.0,8.0,1.0,3.0,0.0,2.0,1.0,1.0,3.0,1.0,3.0,3.0,39.0,3.0,9.0,6.0,4.0,27.0,9.0,6.0,4.0,1.0,0.0,0.0,3.0,snippet_185
272699,cackharot/suds-py3,cackharot_suds-py3/suds/xsd/doctor.py,suds.xsd.doctor.TnsFilter,"class TnsFilter:
    """"""
    Target Namespace filter.
    @ivar tns: A list of target namespaces.
    @type tns: [str,...]
    """"""

    def __init__(self, *tns):
        """"""
        @param tns: A list of target namespaces.
        @type tns: [str,...]
        """"""
        self.tns = []
        self.add(*tns)

    def add(self, *tns):
        """"""
        Add I{targetNamesapces} to be added.
        @param tns: A list of target namespaces.
        @type tns: [str,...]
        """"""
        self.tns += tns

    def match(self, root, ns):
        """"""
        Match by I{targetNamespace} excluding those that
        are equal to the specified namespace to prevent
        adding an import to itself.
        @param root: A schema root.
        @type root: L{Element}
        """"""
        tns = root.get('targetNamespace')
        if len(self.tns):
            matched = tns in self.tns
        else:
            matched = 1
        itself = ns == tns
        return matched and (not itself)","class TnsFilter:
    '''
    Target Namespace filter.
    @ivar tns: A list of target namespaces.
    @type tns: [str,...]
    '''

    def __init__(self, *tns):
        '''
        @param tns: A list of target namespaces.
        @type tns: [str,...]
        '''
        pass

    def add(self, *tns):
        '''
        Add I{targetNamesapces} to be added.
        @param tns: A list of target namespaces.
        @type tns: [str,...]
        '''
        pass

    def match(self, root, ns):
        '''
        Match by I{targetNamespace} excluding those that
        are equal to the specified namespace to prevent
        adding an import to itself.
        @param root: A schema root.
        @type root: L{Element}
        '''
        pass",4,4,10.0,0.0,4.0,5.0,1.0,1.5,0.0,0.0,0.0,0.0,3.0,1.0,3.0,3.0,38.0,3.0,14.0,8.0,10.0,21.0,13.0,8.0,9.0,2.0,0.0,1.0,4.0,snippet_186
272711,cackharot/suds-py3,cackharot_suds-py3/suds/xsd/sxbase.py,suds.xsd.sxbase.NodeFinder,"class NodeFinder:
    """"""
    Find nodes based on flexable criteria.  The I{matcher} is
    may be any object that implements a match(n) method.
    @ivar matcher: An object used as criteria for match.
    @type matcher: I{any}.match(n)
    @ivar limit: Limit the number of matches.  0=unlimited.
    @type limit: int
    """"""

    def __init__(self, matcher, limit=0):
        """"""
        @param matcher: An object used as criteria for match.
        @type matcher: I{any}.match(n)
        @param limit: Limit the number of matches.  0=unlimited.
        @type limit: int
        """"""
        self.matcher = matcher
        self.limit = limit

    def find(self, node, list):
        """"""
        Traverse the tree looking for matches.
        @param node: A node to match on.
        @type node: L{SchemaObject}
        @param list: A list to fill.
        @type list: list
        """"""
        if self.matcher.match(node):
            list.append(node)
            self.limit -= 1
            if self.limit == 0:
                return
        for c in node.rawchildren:
            self.find(c, list)
        return self","class NodeFinder:
    '''
    Find nodes based on flexable criteria.  The I{matcher} is
    may be any object that implements a match(n) method.
    @ivar matcher: An object used as criteria for match.
    @type matcher: I{any}.match(n)
    @ivar limit: Limit the number of matches.  0=unlimited.
    @type limit: int
    '''

    def __init__(self, matcher, limit=0):
        '''
        @param matcher: An object used as criteria for match.
        @type matcher: I{any}.match(n)
        @param limit: Limit the number of matches.  0=unlimited.
        @type limit: int
        '''
        pass

    def find(self, node, list):
        '''
        Traverse the tree looking for matches.
        @param node: A node to match on.
        @type node: L{SchemaObject}
        @param list: A list to fill.
        @type list: list
        '''
        pass",3,3,13.0,0.0,6.0,7.0,3.0,1.62,0.0,0.0,0.0,0.0,2.0,2.0,2.0,2.0,35.0,1.0,13.0,6.0,10.0,21.0,13.0,6.0,10.0,4.0,0.0,2.0,5.0,snippet_187
275837,vanheeringen-lab/gimmemotifs,vanheeringen-lab_gimmemotifs/gimmemotifs/maelstrom/__init__.py,gimmemotifs.maelstrom.MaelstromResult,"from gimmemotifs.motif import read_motifs
from scipy.cluster.hierarchy import dendrogram, linkage
from gimmemotifs.utils import join_max, pfmfile_location
import seaborn as sns
import os
import pandas as pd
from matplotlib.gridspec import GridSpec
from scipy.spatial.distance import pdist
import matplotlib.pyplot as plt
import glob
import numpy as np

class MaelstromResult:
    """"""Class for working with maelstrom output.""""""

    def __init__(self, outdir):
        """"""Initialize a MaelstromResult object from a maelstrom output
        directory.

        Parameters
        ----------
        outdir : str
            Name of a maelstrom output directory.

        See Also
        --------
        maelstrom.run_maelstrom : Run a maelstrom analysis.
        """"""
        if not os.path.exists(outdir):
            raise FileNotFoundError('No such directory: ' + outdir)
        fnames = glob.glob(os.path.join(outdir, 'nonredundant*.p[fw]m'))
        if len(fnames) == 0:
            fnames = glob.glob(os.path.join(outdir, '*.p[fw]m'))
        if len(fnames) > 0:
            pfmfile = fnames[0]
            with open(pfmfile) as fin:
                self.motifs = {m.id: m for m in read_motifs(fin)}
        self.activity = {}
        for fname in glob.glob(os.path.join(outdir, 'activity*txt')):
            _, name, mtype, _, _ = os.path.split(fname)[-1].split('.')
            self.activity[f'{name}.{mtype}'] = pd.read_table(fname, comment='#', index_col=0)
        self.result = pd.read_table(os.path.join(outdir, 'final.out.txt'), comment='#', index_col=0)
        self.correlation = self.result.loc[:, self.result.columns.str.contains('corr')]
        self.percent_match = self.result.loc[:, self.result.columns.str.contains('% with motif')]
        self.result = self.result.loc[:, ~self.result.columns.str.contains('corr') & ~self.result.columns.str.contains('% with motif')]
        self.scores = pd.read_table(os.path.join(outdir, 'motif.score.txt.gz'), index_col=0)
        self.counts = pd.read_table(os.path.join(outdir, 'motif.count.txt.gz'), index_col=0)
        fname = os.path.join(outdir, 'motif.freq.txt')
        if os.path.exists(fname):
            self.freq = pd.read_table(fname, index_col=0)
        try:
            self.input = pd.read_table(os.path.join(outdir, 'input.table.txt'), index_col=0)
            if self.input.shape[1] == 1:
                self.input.columns = ['cluster']
        except Exception:
            pass

    def plot_heatmap(self, kind='final', min_freq=0.01, threshold=2, name=True, indirect=True, figsize=None, max_number_factors=5, aspect=1, cmap='RdBu_r', **kwargs):
        """"""Plot clustered heatmap of predicted motif activity.

        Parameters
        ----------
        kind : str, optional
            Which data type to use for plotting. Default is 'final', which will
            plot the result of the rank aggregation. Other options are 'freq'
            for the motif frequencies, or any of the individual activities such
            as 'rf.score'.

        min_freq : float, optional
            Minimum frequency of motif occurrence.

        threshold : float, optional
            Minimum activity (absolute) of the rank aggregation result.

        name : bool, optional
            Use factor names instead of motif names for plotting.

        indirect : bool, optional
            Include indirect factors (computationally predicted or non-curated). Default is True.

        max_number_factors : int, optional
            Truncate the list of factors to this maximum size.

        figsize : tuple, optional
            Tuple of figure size (width, height).

        aspect : int, optional
            Aspect ratio for tweaking the plot.

        cmap : str, optional
            Color paletter to use, RdBu_r by default.

        kwargs : other keyword arguments
            All other keyword arguments are passed to sns.heatmap

        Returns
        -------
        cg : ClusterGrid
            A seaborn ClusterGrid instance.
        """"""
        filt = np.any(np.abs(self.result) >= threshold, 1)
        if hasattr(self, 'freq'):
            filt = filt & np.any(np.abs(self.freq.T) >= min_freq, 1)
        else:
            filt = filt & (self.counts.sum() / self.counts.shape[0] > min_freq)
        idx = self.result.loc[filt].index
        if idx.shape[0] == 0:
            logger.warning('Empty matrix, try lowering the threshold')
            return
        if idx.shape[0] >= 100:
            logger.warning('The filtered matrix has more than 100 rows.')
            logger.warning('It might be worthwhile to increase the threshold for visualization')
        if kind == 'final':
            data = self.result
        elif kind == 'freq':
            if hasattr(self, 'freq'):
                data = self.freq.T
                cmap = 'Reds'
            else:
                raise ValueError('frequency plot only works with maelstrom output from clusters')
        elif kind in self.activity:
            data = self.activity[kind]
            if kind in ['hypergeom.count', 'mwu.score']:
                cmap = 'Reds'
        else:
            raise ValueError('Unknown dtype')
        m = data.loc[idx]
        if 'vmax' in kwargs:
            vmax = kwargs.pop('vmax')
        else:
            vmax = max(abs(np.percentile(m, 1)), np.percentile(m, 99))
        if 'vmin' in kwargs:
            vmin = kwargs.pop('vmin')
        else:
            vmin = -vmax
        if name:
            m['factors'] = [self.motifs[n].format_factors(max_length=max_number_factors, html=False, include_indirect=indirect, extra_str=',..') for n in m.index]
            m = m.set_index('factors')
        h, w = m.shape
        if figsize is None:
            figsize = (4 + m.shape[1] / 4, 1 + m.shape[0] / 3)
        fig = plt.figure(figsize=figsize)
        npixels = 30
        g = GridSpec(2, 1, height_ratios=(fig.get_figheight() * fig.dpi - npixels, npixels))
        ax1 = fig.add_subplot(g[0, :])
        ax2 = fig.add_subplot(g[1, :])
        ax2.set_title('aggregated z-score')
        dm = pdist(m, metric='correlation')
        hc = linkage(dm, method='ward')
        leaves = dendrogram(hc, no_plot=True)['leaves']
        cg = sns.heatmap(m.iloc[leaves], ax=ax1, cbar_ax=ax2, cbar_kws={'orientation': 'horizontal'}, cmap=cmap, linewidths=1, vmin=vmin, vmax=vmax, **kwargs)
        plt.setp(cg.axes.xaxis.get_majorticklabels(), rotation=90)
        plt.tight_layout()
        return cg

    def plot_scores(self, motifs, name=True, max_len=50):
        """"""Create motif scores boxplot of different clusters.
        Motifs can be specified as either motif or factor names.
        The motif scores will be scaled and plotted as z-scores.

        Parameters
        ----------
        motifs : iterable or str
            List of motif or factor names.

        name : bool, optional
            Use factor names instead of motif names for plotting.

        max_len : int, optional
            Truncate the list of factors to this maximum length.

        Returns
        -------

        g : FacetGrid
            Returns the seaborn FacetGrid object with the plot.
        """"""
        if self.input.shape[1] != 1:
            raise ValueError(""Can't make a categorical plot with real-valued data"")
        if type('') == type(motifs):
            motifs = [motifs]
        plot_motifs = []
        for motif in motifs:
            if motif in self.motifs:
                plot_motifs.append(motif)
            else:
                for m in self.motifs.values():
                    if motif in m.factors:
                        plot_motifs.append(m.id)
        data = self.scores[plot_motifs]
        data[:] = data.scale(data, axix=0)
        if name:
            data = data.T
            data['factors'] = [join_max(self.motifs[n].factors, max_len, ',', suffix=',(...)') for n in plot_motifs]
            data = data.set_index('factors').T
        data = pd.melt(self.input.join(data), id_vars=['cluster'])
        data.columns = ['cluster', 'motif', 'z-score']
        g = sns.catplot(data=data, y='motif', x='z-score', hue='cluster', kind='box', aspect=2)
        return g","
class MaelstromResult:
    '''Class for working with maelstrom output.'''

    def __init__(self, outdir):
        '''Initialize a MaelstromResult object from a maelstrom output
        directory.
        Parameters
        ----------
        outdir : str
            Name of a maelstrom output directory.
        See Also
        --------
        maelstrom.run_maelstrom : Run a maelstrom analysis.
        '''
        pass

    def plot_heatmap(self, kind='final', min_freq=0.01, threshold=2, name=True, indirect=True, figsize=None, max_number_factors=5, aspect=1, cmap='RdBu_r', **kwargs):
        '''Plot clustered heatmap of predicted motif activity.
        Parameters
        ----------
        kind : str, optional
            Which data type to use for plotting. Default is 'final', which will
            plot the result of the rank aggregation. Other options are 'freq'
            for the motif frequencies, or any of the individual activities such
            as 'rf.score'.
        min_freq : float, optional
            Minimum frequency of motif occurrence.
        threshold : float, optional
            Minimum activity (absolute) of the rank aggregation result.
        name : bool, optional
            Use factor names instead of motif names for plotting.
        indirect : bool, optional
            Include indirect factors (computationally predicted or non-curated). Default is True.
        max_number_factors : int, optional
            Truncate the list of factors to this maximum size.
        figsize : tuple, optional
            Tuple of figure size (width, height).
        aspect : int, optional
            Aspect ratio for tweaking the plot.
        cmap : str, optional
            Color paletter to use, RdBu_r by default.
        kwargs : other keyword arguments
            All other keyword arguments are passed to sns.heatmap
        Returns
        -------
        cg : ClusterGrid
            A seaborn ClusterGrid instance.
        '''
        pass

    def plot_scores(self, motifs, name=True, max_len=50):
        '''Create motif scores boxplot of different clusters.
        Motifs can be specified as either motif or factor names.
        The motif scores will be scaled and plotted as z-scores.
        Parameters
        ----------
        motifs : iterable or str
            List of motif or factor names.
        name : bool, optional
            Use factor names instead of motif names for plotting.
        max_len : int, optional
            Truncate the list of factors to this maximum length.
        Returns
        -------
        g : FacetGrid
            Returns the seaborn FacetGrid object with the plot.
        '''
        pass",4,4,89.0,13.0,55.0,22.0,10.0,0.41,0.0,4.0,0.0,0.0,3.0,9.0,3.0,3.0,272.0,42.0,165.0,51.0,149.0,67.0,100.0,38.0,96.0,13.0,0.0,4.0,29.0,snippet_188
278809,OTA-Insight/djangosaml2idp,OTA-Insight_djangosaml2idp/djangosaml2idp/idp.py,djangosaml2idp.idp.IDP,"from django.core.exceptions import ImproperlyConfigured
from saml2.config import IdPConfig
from django.conf import settings
from saml2.metadata import entity_descriptor
import copy
from saml2.server import Server
from django.utils.translation import gettext as _

class IDP:
    """""" Access point for the IDP Server instance
    """"""
    _server_instance: Server = None

    @classmethod
    def construct_metadata(cls, with_local_sp: bool=True) -> dict:
        """""" Get the config including the metadata for all the configured service providers. """"""
        from .models import ServiceProvider
        idp_config = copy.deepcopy(settings.SAML_IDP_CONFIG)
        if idp_config:
            idp_config['metadata'] = {'local': [sp.metadata_path() for sp in ServiceProvider.objects.filter(active=True)] if with_local_sp else []}
        return idp_config

    @classmethod
    def load(cls, force_refresh: bool=False) -> Server:
        """""" Instantiate a IDP Server instance based on the config defined in the SAML_IDP_CONFIG settings.
            Throws an ImproperlyConfigured exception if it could not do so for any reason.
        """"""
        if cls._server_instance is None or force_refresh:
            conf = IdPConfig()
            md = cls.construct_metadata()
            try:
                conf.load(md)
                cls._server_instance = Server(config=conf)
            except Exception as e:
                raise ImproperlyConfigured(_('Could not instantiate an IDP based on the SAML_IDP_CONFIG settings and configured ServiceProviders: {}').format(str(e)))
        return cls._server_instance

    @classmethod
    def metadata(cls) -> str:
        """""" Get the IDP metadata as a string. """"""
        conf = IdPConfig()
        try:
            conf.load(cls.construct_metadata(with_local_sp=False))
            metadata = entity_descriptor(conf)
        except Exception as e:
            raise ImproperlyConfigured(_('Could not instantiate IDP metadata based on the SAML_IDP_CONFIG settings and configured ServiceProviders: {}').format(str(e)))
        return str(metadata)","
class IDP:
    ''' Access point for the IDP Server instance
    '''
    @classmethod
    def construct_metadata(cls, with_local_sp: bool=True) -> dict:
        ''' Get the config including the metadata for all the configured service providers. '''
        pass
    @classmethod
    def load(cls, force_refresh: bool=False) -> Server:
        ''' Instantiate a IDP Server instance based on the config defined in the SAML_IDP_CONFIG settings.
            Throws an ImproperlyConfigured exception if it could not do so for any reason.
        '''
        pass
    @classmethod
    def metadata(cls) -> str:
        ''' Get the IDP metadata as a string. '''
        pass",4,4,11.0,0.0,9.0,2.0,3.0,0.24,0.0,5.0,1.0,0.0,0.0,0.0,3.0,3.0,43.0,3.0,33.0,16.0,25.0,8.0,26.0,11.0,21.0,3.0,0.0,2.0,8.0,snippet_189
279001,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.AES_CTR_Mechanism,"import PyKCS11.LowLevel

class AES_CTR_Mechanism:
    """"""CKM_AES_CTR encryption mechanism""""""

    def __init__(self, counterBits, counterBlock):
        """"""
        :param counterBits: the number of incremented bits in the counter block
        :param counterBlock: a 16-byte initial value of the counter block
        """"""
        self._param = PyKCS11.LowLevel.CK_AES_CTR_PARAMS()
        self._source_cb = ckbytelist(counterBlock)
        self._param.ulCounterBits = counterBits
        self._param.cb = self._source_cb
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_AES_CTR
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_AES_CTR_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class AES_CTR_Mechanism:
    '''CKM_AES_CTR encryption mechanism'''

    def __init__(self, counterBits, counterBlock):
        '''
        :param counterBits: the number of incremented bits in the counter block
        :param counterBlock: a 16-byte initial value of the counter block
        '''
        pass

    def to_native(self):
        '''convert mechanism to native format'''
        pass",3,3,9.0,1.0,6.0,2.0,1.0,0.42,0.0,1.0,1.0,0.0,2.0,3.0,2.0,2.0,21.0,4.0,12.0,6.0,9.0,5.0,12.0,6.0,9.0,1.0,0.0,0.0,2.0,snippet_190
279002,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.AES_GCM_Mechanism,"import PyKCS11.LowLevel

class AES_GCM_Mechanism:
    """"""CKM_AES_GCM warpping mechanism""""""

    def __init__(self, iv, aad, tagBits):
        """"""
        :param iv: initialization vector
        :param aad: additional authentication data
        :param tagBits: length of authentication tag in bits
        """"""
        self._param = PyKCS11.LowLevel.CK_GCM_PARAMS()
        self._source_iv = ckbytelist(iv)
        self._param.pIv = self._source_iv
        self._param.ulIvLen = len(self._source_iv)
        self._source_aad = ckbytelist(aad)
        self._param.pAAD = self._source_aad
        self._param.ulAADLen = len(self._source_aad)
        self._param.ulTagBits = tagBits
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_AES_GCM
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_GCM_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class AES_GCM_Mechanism:
    '''CKM_AES_GCM warpping mechanism'''

    def __init__(self, iv, aad, tagBits):
        '''
        :param iv: initialization vector
        :param aad: additional authentication data
        :param tagBits: length of authentication tag in bits
        '''
        pass

    def to_native(self):
        '''convert mechanism to native format'''
        pass",3,3,12.0,2.0,8.0,3.0,1.0,0.38,0.0,1.0,1.0,0.0,2.0,4.0,2.0,2.0,28.0,6.0,16.0,7.0,13.0,6.0,16.0,7.0,13.0,1.0,0.0,0.0,2.0,snippet_191
279010,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.CONCATENATE_BASE_AND_KEY_Mechanism,"import PyKCS11.LowLevel

class CONCATENATE_BASE_AND_KEY_Mechanism:
    """"""CKM_CONCATENATE_BASE_AND_KEY key derivation mechanism""""""

    def __init__(self, encKey):
        """"""
        :param encKey: a handle of encryption key
        """"""
        self._encKey = encKey
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_CONCATENATE_BASE_AND_KEY
        self._mech.pParameter = self._encKey
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_OBJECT_HANDLE_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class CONCATENATE_BASE_AND_KEY_Mechanism:
    '''CKM_CONCATENATE_BASE_AND_KEY key derivation mechanism'''

    def __init__(self, encKey):
        '''
        :param encKey: a handle of encryption key
        '''
        pass

    def to_native(self):
        '''convert mechanism to native format'''
        pass",3,3,6.0,1.0,4.0,2.0,1.0,0.44,0.0,0.0,0.0,0.0,2.0,2.0,2.0,2.0,16.0,3.0,9.0,5.0,6.0,4.0,9.0,5.0,6.0,1.0,0.0,0.0,2.0,snippet_192
279012,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.CkClass,"class CkClass:
    """"""
    Base class for CK_* classes
    """"""
    flags_dict = {}
    fields = {}
    flags = 0

    def flags2text(self):
        """"""
        parse the `self.flags` field and create a list of `CKF_*` strings
        corresponding to bits set in flags

        :return: a list of strings
        :rtype: list
        """"""
        r = []
        for k, v in self.flags_dict.items():
            if self.flags & k:
                r.append(v)
        return r

    def state2text(self):
        """"""
        Dummy method. Will be overwriden if necessary
        """"""
        return ''

    def to_dict(self):
        """"""
        convert the fields of the object into a dictionnary
        """"""
        dico = {}
        for field in self.fields:
            if field == 'flags':
                dico[field] = self.flags2text()
            elif field == 'state':
                dico[field] = self.state2text()
            else:
                dico[field] = self.__dict__[field]
        return dico

    def __str__(self):
        """"""
        text representation of the object
        """"""
        dico = self.to_dict()
        lines = []
        for key in sorted(dico.keys()):
            ck_type = self.fields[key]
            if ck_type == 'flags':
                flags = ', '.join(dico[key])
                lines.append(f'{key}: {flags}')
            elif ck_type == 'pair':
                p1, p2 = dico[key]
                lines.append(f'{key}: {p1}.{p2}')
            else:
                lines.append(f'{key}: {dico[key]}')
        return '\n'.join(lines)","class CkClass:
    '''
    Base class for CK_* classes
        '''

    def flags2text(self):
        '''
        parse the `self.flags` field and create a list of `CKF_*` strings
        corresponding to bits set in flags
        :return: a list of strings
        :rtype: list
        '''
        pass

    def state2text(self):
        '''
        Dummy method. Will be overwriden if necessary
        '''
        pass

    def to_dict(self):
        '''
        convert the fields of the object into a dictionnary
        '''
        pass

    def __str__(self):
        '''
        text representation of the object
        '''
        pass",5,5,14.0,0.0,9.0,4.0,4.0,0.56,0.0,2.0,0.0,5.0,3.0,0.0,3.0,3.0,57.0,7.0,32.0,15.0,28.0,18.0,28.0,15.0,24.0,4.0,0.0,2.0,11.0,snippet_193
279015,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.EXTRACT_KEY_FROM_KEY_Mechanism,"import PyKCS11.LowLevel

class EXTRACT_KEY_FROM_KEY_Mechanism:
    """"""CKM_EXTRACT_KEY_FROM_KEY key derivation mechanism""""""

    def __init__(self, extractParams):
        """"""
        :param extractParams: the index of the first bit of the original
        key to be used in the newly-derived key.  For example if
        extractParams=5 then the 5 first bits are skipped and not used.
        """"""
        self._param = PyKCS11.LowLevel.CK_EXTRACT_PARAMS()
        self._param.assign(extractParams)
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_EXTRACT_KEY_FROM_KEY
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_EXTRACT_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class EXTRACT_KEY_FROM_KEY_Mechanism:
    '''CKM_EXTRACT_KEY_FROM_KEY key derivation mechanism'''

    def __init__(self, extractParams):
        '''
        :param extractParams: the index of the first bit of the original
        key to be used in the newly-derived key.  For example if
        extractParams=5 then the 5 first bits are skipped and not used.
        '''
        pass

    def to_native(self):
        '''convert mechanism to native format'''
        pass",3,3,7.0,1.0,5.0,2.0,1.0,0.5,0.0,0.0,0.0,0.0,2.0,2.0,2.0,2.0,18.0,3.0,10.0,5.0,7.0,5.0,10.0,5.0,7.0,1.0,0.0,0.0,2.0,snippet_194
279016,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.KEY_DERIVATION_STRING_DATA_MechanismBase,"import PyKCS11.LowLevel

class KEY_DERIVATION_STRING_DATA_MechanismBase:
    """"""Base class for mechanisms using derivation string data""""""

    def __init__(self, data, mechType):
        """"""
        :param data: a byte array to concatenate the key with
        :param mechType: mechanism type
        """"""
        self._param = PyKCS11.LowLevel.CK_KEY_DERIVATION_STRING_DATA()
        self._data = ckbytelist(data)
        self._param.pData = self._data
        self._param.ulLen = len(self._data)
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = mechType
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_KEY_DERIVATION_STRING_DATA_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class KEY_DERIVATION_STRING_DATA_MechanismBase:
    '''Base class for mechanisms using derivation string data'''

    def __init__(self, data, mechType):
        '''
        :param data: a byte array to concatenate the key with
        :param mechType: mechanism type
        '''
        pass

    def to_native(self):
        '''convert mechanism to native format'''
        pass",3,3,10.0,1.0,7.0,2.0,1.0,0.36,0.0,1.0,1.0,3.0,2.0,3.0,2.0,2.0,23.0,4.0,14.0,6.0,11.0,5.0,12.0,6.0,9.0,1.0,0.0,0.0,2.0,snippet_195
279017,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.Mechanism,"import PyKCS11.LowLevel

class Mechanism:
    """"""Wraps CK_MECHANISM""""""

    def __init__(self, mechanism, param=None):
        """"""
        :param mechanism: the mechanism to be used
        :type mechanism: integer, any `CKM_*` value
        :param param: data to be used as crypto operation parameter
          (i.e. the IV for some algorithms)
        :type param: string or list/tuple of bytes

        :see: :func:`Session.decrypt`, :func:`Session.sign`
        """"""
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = mechanism
        self._param = None
        if param:
            self._param = ckbytelist(param)
            self._mech.pParameter = self._param
            self._mech.ulParameterLen = len(param)

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class Mechanism:
    '''Wraps CK_MECHANISM'''

    def __init__(self, mechanism, param=None):
        '''
        :param mechanism: the mechanism to be used
        :type mechanism: integer, any `CKM_*` value
        :param param: data to be used as crypto operation parameter
          (i.e. the IV for some algorithms)
        :type param: string or list/tuple of bytes
        :see: :func:`Session.decrypt`, :func:`Session.sign`
        '''
        pass

    def to_native(self):
        '''convert mechanism to native format'''
        pass",3,3,10.0,1.0,5.0,4.0,2.0,0.82,0.0,1.0,1.0,0.0,2.0,2.0,2.0,2.0,23.0,3.0,11.0,5.0,8.0,9.0,11.0,5.0,8.0,2.0,0.0,1.0,3.0,snippet_196
279020,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.RSAOAEPMechanism,"import PyKCS11.LowLevel

class RSAOAEPMechanism:
    """"""RSA OAEP Wrapping mechanism""""""

    def __init__(self, hashAlg, mgf, label=None):
        """"""
        :param hashAlg: the hash algorithm to use (like `CKM_SHA256`)
        :param mgf: the mask generation function to use (like
          `CKG_MGF1_SHA256`)
        :param label: the (optional) label to use
        """"""
        self._param = PyKCS11.LowLevel.CK_RSA_PKCS_OAEP_PARAMS()
        self._param.hashAlg = hashAlg
        self._param.mgf = mgf
        self._source = None
        self._param.source = CKZ_DATA_SPECIFIED
        if label:
            self._source = ckbytelist(label)
            self._param.ulSourceDataLen = len(self._source)
        else:
            self._param.ulSourceDataLen = 0
        self._param.pSourceData = self._source
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = CKM_RSA_PKCS_OAEP
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_RSA_PKCS_OAEP_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class RSAOAEPMechanism:
    '''RSA OAEP Wrapping mechanism'''

    def __init__(self, hashAlg, mgf, label=None):
        '''
        :param hashAlg: the hash algorithm to use (like `CKM_SHA256`)
        :param mgf: the mask generation function to use (like
          `CKG_MGF1_SHA256`)
        :param label: the (optional) label to use
        '''
        pass

    def to_native(self):
        '''convert mechanism to native format'''
        pass",3,3,12.0,0.0,9.0,3.0,2.0,0.37,0.0,1.0,1.0,0.0,2.0,3.0,2.0,2.0,28.0,2.0,19.0,6.0,16.0,7.0,18.0,6.0,15.0,2.0,0.0,1.0,3.0,snippet_197
279021,LudovicRousseau/PyKCS11,LudovicRousseau_PyKCS11/PyKCS11/__init__.py,PyKCS11.RSA_PSS_Mechanism,"import PyKCS11.LowLevel

class RSA_PSS_Mechanism:
    """"""RSA PSS Wrapping mechanism""""""

    def __init__(self, mecha, hashAlg, mgf, sLen):
        """"""
        :param mecha: the mechanism to use (like
          `CKM_SHA384_RSA_PKCS_PSS`)
        :param hashAlg: the hash algorithm to use (like `CKM_SHA384`)
        :param mgf: the mask generation function to use (like
          `CKG_MGF1_SHA384`)
        :param sLen: length, in bytes, of the salt value used in the PSS
          encoding (like 0 or the message length)
        """"""
        self._param = PyKCS11.LowLevel.CK_RSA_PKCS_PSS_PARAMS()
        self._param.hashAlg = hashAlg
        self._param.mgf = mgf
        self._param.sLen = sLen
        self._mech = PyKCS11.LowLevel.CK_MECHANISM()
        self._mech.mechanism = mecha
        self._mech.pParameter = self._param
        self._mech.ulParameterLen = PyKCS11.LowLevel.CK_RSA_PKCS_PSS_PARAMS_LENGTH

    def to_native(self):
        """"""convert mechanism to native format""""""
        return self._mech","
class RSA_PSS_Mechanism:
    '''RSA PSS Wrapping mechanism'''

    def __init__(self, mecha, hashAlg, mgf, sLen):
        '''
        :param mecha: the mechanism to use (like
          `CKM_SHA384_RSA_PKCS_PSS`)
        :param hashAlg: the hash algorithm to use (like `CKM_SHA384`)
        :param mgf: the mask generation function to use (like
          `CKG_MGF1_SHA384`)
        :param sLen: length, in bytes, of the salt value used in the PSS
          encoding (like 0 or the message length)
        '''
        pass

    def to_native(self):
        '''convert mechanism to native format'''
        pass",3,3,10.0,0.0,6.0,5.0,1.0,0.83,0.0,0.0,0.0,0.0,2.0,2.0,2.0,2.0,24.0,2.0,12.0,5.0,9.0,10.0,12.0,5.0,9.0,1.0,0.0,0.0,2.0,snippet_198
281845,datadotworld/data.world-py,datadotworld_data.world-py/datadotworld/datadotworld.py,datadotworld.datadotworld.UriParam,"class UriParam:
    """"""Represents a URI value as a parameter to a SPARQL query""""""

    def __init__(self, uri):
        """"""
        Initialize the UriParam value
        :param uri: the uri value to wrap
        """"""
        self._uri = uri

    def __repr__(self):
        """"""
        The official string representation for the URI
        :return: the string representation for the URI
        """"""
        return self._uri","class UriParam:
    '''Represents a URI value as a parameter to a SPARQL query'''

    def __init__(self, uri):
        '''
        Initialize the UriParam value
        :param uri: the uri value to wrap
        '''
        pass

    def __repr__(self):
        '''
        The official string representation for the URI
        :return: the string representation for the URI
        '''
        pass",3,3,6.0,0.0,2.0,4.0,1.0,1.8,0.0,0.0,0.0,0.0,2.0,1.0,2.0,2.0,15.0,1.0,5.0,4.0,2.0,9.0,5.0,4.0,2.0,1.0,0.0,0.0,2.0,snippet_199
282529,quora/qcore,quora_qcore/qcore/events.py,qcore.events.EventInterceptor,"class EventInterceptor:
    """"""A context object helping to temporarily intercept
    a set of events on an object exposing a set of event hooks.

    """"""

    def __init__(self, source, **events):
        """"""
        Constructor.

        :param source: the object exposing a set of event hook properies
        :param events: a set of event_hook_name=event_handler pairs specifying
                       which events to intercept.
        """"""
        self.source = source
        self.events = events

    def __enter__(self):
        """"""Starts event interception.""""""
        source = self.source
        for name, handler in self.events.items():
            hook = getattr(source, name)
            hook.subscribe(handler)

    def __exit__(self, typ, value, traceback):
        """"""Stops event interception.""""""
        source = self.source
        for name, handler in self.events.items():
            hook = getattr(source, name)
            hook.unsubscribe(handler)","class EventInterceptor:
    '''A context object helping to temporarily intercept
    a set of events on an object exposing a set of event hooks.
    '''

    def __init__(self, source, **events):
        '''
        Constructor.
        :param source: the object exposing a set of event hook properies
        :param events: a set of event_hook_name=event_handler pairs specifying
                       which events to intercept.
        '''
        pass

    def __enter__(self):
        '''Starts event interception.'''
        pass

    def __exit__(self, typ, value, traceback):
        '''Stops event interception.'''
        pass",4,4,7.0,0.0,4.0,3.0,2.0,0.79,0.0,0.0,0.0,0.0,3.0,2.0,3.0,3.0,30.0,5.0,14.0,12.0,10.0,11.0,14.0,12.0,10.0,2.0,0.0,1.0,5.0,snippet_200
284811,empymod/empymod,empymod/filters.py,empymod.filters.DigitalFilter,"import numpy as np
import os

class DigitalFilter:
    """"""Simple Class for Digital Linear Filters.


    Parameters
    ----------
    name : str
        Name of the DFL.

    savename = str
        Name with which the filter is saved. If None (default) it is set to the
        same value as `name`.

    filter_coeff = list of str
        By default, the following filter coefficients are checked:

            ``filter_coeff = ['j0', 'j1', 'sin', 'cos']``

        This accounts for the standard Hankel and Fourier DLF in CSEM
        modelling. However, additional coefficient names can be provided via
        this parameter (in list format).

    """"""

    def __init__(self, name, savename=None, filter_coeff=None):
        """"""Add filter name.""""""
        self.name = name
        if savename is None:
            self.savename = name
        else:
            self.savename = savename
        self.filter_coeff = ['j0', 'j1', 'sin', 'cos']
        if filter_coeff is not None:
            self.filter_coeff.extend(filter_coeff)

    def tofile(self, path='filters'):
        """"""Save filter values to ASCII-files.

        Store the filter base and the filter coefficients in separate files
        in the directory `path`; `path` can be a relative or absolute path.

        Examples
        --------
        >>> import empymod
        >>> # Load a filter
        >>> filt = empymod.filters.Hankel().wer_201_2018
        >>> # Save it to pure ASCII-files
        >>> filt.tofile()
        >>> # This will save the following three files:
        >>> #    ./filters/wer_201_2018_base.txt
        >>> #    ./filters/wer_201_2018_j0.txt
        >>> #    ./filters/wer_201_2018_j1.txt

        """"""
        name = self.savename
        path = os.path.abspath(path)
        os.makedirs(path, exist_ok=True)
        basefile = os.path.join(path, name + '_base.txt')
        with open(basefile, 'w') as f:
            self.base.tofile(f, sep='\n')
        for val in self.filter_coeff:
            if hasattr(self, val):
                attrfile = os.path.join(path, name + '_' + val + '.txt')
                with open(attrfile, 'w') as f:
                    getattr(self, val).tofile(f, sep='\n')

    def fromfile(self, path='filters'):
        """"""Load filter values from ASCII-files.

        Load filter base and filter coefficients from ASCII files in the
        directory `path`; `path` can be a relative or absolute path.

        Examples
        --------
        >>> import empymod
        >>> # Create an empty filter;
        >>> # Name has to be the base of the text files
        >>> filt = empymod.filters.DigitalFilter('my-filter')
        >>> # Load the ASCII-files
        >>> filt.fromfile()
        >>> # This will load the following three files:
        >>> #    ./filters/my-filter_base.txt
        >>> #    ./filters/my-filter_j0.txt
        >>> #    ./filters/my-filter_j1.txt
        >>> # and store them in filt.base, filt.j0, and filt.j1.

        """"""
        name = self.savename
        path = os.path.abspath(path)
        basefile = os.path.join(path, name + '_base.txt')
        with open(basefile, 'r') as f:
            self.base = np.fromfile(f, sep='\n')
        for val in self.filter_coeff:
            attrfile = os.path.join(path, name + '_' + val + '.txt')
            if os.path.isfile(attrfile):
                with open(attrfile, 'r') as f:
                    setattr(self, val, np.fromfile(f, sep='\n'))
        self.factor = np.around([self.base[1] / self.base[0]], 15)","
class DigitalFilter:
    '''Simple Class for Digital Linear Filters.

    Parameters
    ----------
    name : str
        Name of the DFL.
    savename = str
        Name with which the filter is saved. If None (default) it is set to the
        same value as `name`.
    filter_coeff = list of str
        By default, the following filter coefficients are checked:
            ``filter_coeff = ['j0', 'j1', 'sin', 'cos']``
        This accounts for the standard Hankel and Fourier DLF in CSEM
        modelling. However, additional coefficient names can be provided via
        this parameter (in list format).
    '''

    def __init__(self, name, savename=None, filter_coeff=None):
        '''Add filter name.'''
        pass

    def tofile(self, path='filters'):
        '''Save filter values to ASCII-files.
        Store the filter base and the filter coefficients in separate files
        in the directory `path`; `path` can be a relative or absolute path.
        Examples
        --------
        >>> import empymod
        >>> # Load a filter
        >>> filt = empymod.filters.Hankel().wer_201_2018
        >>> # Save it to pure ASCII-files
        >>> filt.tofile()
        >>> # This will save the following three files:
        >>> #    ./filters/wer_201_2018_base.txt
        >>> #    ./filters/wer_201_2018_j0.txt
        >>> #    ./filters/wer_201_2018_j1.txt
        '''
        pass

    def fromfile(self, path='filters'):
        '''Load filter values from ASCII-files.
        Load filter base and filter coefficients from ASCII files in the
        directory `path`; `path` can be a relative or absolute path.
        Examples
        --------
        >>> import empymod
        >>> # Create an empty filter;
        >>> # Name has to be the base of the text files
        >>> filt = empymod.filters.DigitalFilter('my-filter')
        >>> # Load the ASCII-files
        >>> filt.fromfile()
        >>> # This will load the following three files:
        >>> #    ./filters/my-filter_base.txt
        >>> #    ./filters/my-filter_j0.txt
        >>> #    ./filters/my-filter_j1.txt
        >>> # and store them in filt.base, filt.j0, and filt.j1.
        '''
        pass",4,4,31.0,5.0,11.0,15.0,3.0,1.74,0.0,0.0,0.0,0.0,3.0,5.0,3.0,3.0,118.0,26.0,34.0,19.0,30.0,59.0,33.0,17.0,29.0,3.0,0.0,3.0,9.0,snippet_201
284814,empymod/empymod,empymod/filters.py,empymod.filters._BaseFilter,"import libdlf
import numpy as np

class _BaseFilter:
    """"""Base class for wrappers loading filters from libdlf.""""""

    def __init__(self, ftype):
        """"""Initiate a new wrapper of `ftype` ('hankel' or 'fourier').""""""
        self._ftype = ftype
        self.available = list(FILTERS[ftype].keys())
        for k, v in FILTERS[ftype].items():
            setattr(self, k, v)

    def __getattribute__(self, name):
        """"""Modify to load filter if the attribute is a know filter name.""""""
        ftype = object.__getattribute__(self, '_ftype')
        if name in FILTERS[ftype].keys():
            if FILTERS[ftype][name] is None:
                data = getattr(getattr(libdlf, ftype), name)
                dlf = DigitalFilter(name)
                for i, val in enumerate(['base'] + data.values):
                    setattr(dlf, val, data()[i])
                dlf.factor = np.around([dlf.base[1] / dlf.base[0]], 15)
                FILTERS[ftype][name] = dlf
            return FILTERS[ftype][name]
        else:
            return object.__getattribute__(self, name)","
class _BaseFilter:
    '''Base class for wrappers loading filters from libdlf.'''

    def __init__(self, ftype):
        '''Initiate a new wrapper of `ftype` ('hankel' or 'fourier').'''
        pass

    def __getattribute__(self, name):
        '''Modify to load filter if the attribute is a know filter name.'''
        pass",3,3,18.0,4.0,9.0,5.0,3.0,0.58,0.0,4.0,1.0,2.0,2.0,2.0,2.0,2.0,40.0,10.0,19.0,10.0,16.0,11.0,18.0,10.0,15.0,4.0,0.0,3.0,6.0,snippet_202
285266,Samreay/ChainConsumer,src/chainconsumer/kde.py,chainconsumer.kde.MegKDE,"import numpy as np
from scipy import spatial

class MegKDE:
    """"""Matched Elliptical Gaussian Kernel Density Estimator

    Adapted from the algorithm specified in the BAMBIS's model specified Wolf 2017
    to support weighted samples.
    """"""

    def __init__(self, train: np.ndarray, weights: np.ndarray | None=None, truncation: float=3.0, nmin: int=4, factor: float=1.0):
        """"""
        Args:
            train (np.ndarray): The training data set. Should be a 1D array of samples or a 2D array
                of shape (n_samples, n_dim).
            weights (np.ndarray, optional): An array of weights. If not specified, equal weights are assumed.
            truncation (float, optional): The maximum deviation (in sigma) to use points in the KDE
            nmin (int, optional): The minimum number of points required to estimate the density
            factor (float, optional): Send bandwidth to this factor of the data estimate
        """"""
        self.truncation = truncation
        self.nmin = nmin
        self.train = train
        if len(train.shape) == 1:
            train = np.atleast_2d(train).T
        self.num_points, self.num_dim = train.shape
        if weights is None:
            weights = np.ones(self.num_points)
        self.weights = weights
        self.mean = np.average(train, weights=weights, axis=0)
        dx = train - self.mean
        cov = np.atleast_2d(np.cov(dx.T, aweights=weights))
        self.A = np.linalg.cholesky(np.linalg.inv(cov))
        self.d = np.dot(dx, self.A)
        self.tree = spatial.cKDTree(self.d)
        self.sigma = 2.0 * factor * np.power(self.num_points, -1.0 / (4 + self.num_dim))
        self.sigma_fact = -0.5 / (self.sigma * self.sigma)

    def evaluate(self, data: np.ndarray) -> np.ndarray:
        """"""Estimate un-normalised probability density at target points

        Args:
            data (np.ndarray): 2D array of shape (n_samples, n_dim).

        Returns:
            np.ndarray: A `(n_samples)` length array of estimates

        """"""
        if len(data.shape) == 1 and self.num_dim == 1:
            data = np.atleast_2d(data).T
        _d = np.dot(data - self.mean, self.A)
        neighbors = self.tree.query_ball_point(_d, self.sigma * self.truncation)
        out = []
        for i, n in enumerate(neighbors):
            if len(n) >= self.nmin:
                diff = self.d[n, :] - _d[i]
                distsq = np.sum(diff * diff, axis=1)
            else:
                dist, n = self.tree.query(_d[i], k=self.nmin)
                distsq = dist * dist
            out.append(np.sum(self.weights[n] * np.exp(self.sigma_fact * distsq)))
        return np.array(out)","
class MegKDE:
    '''Matched Elliptical Gaussian Kernel Density Estimator
    Adapted from the algorithm specified in the BAMBIS's model specified Wolf 2017
    to support weighted samples.
    '''

    def __init__(self, train: np.ndarray, weights: np.ndarray | None=None, truncation: float=3.0, nmin: int=4, factor: float=1.0):
        '''
        Args:
            train (np.ndarray): The training data set. Should be a 1D array of samples or a 2D array
                of shape (n_samples, n_dim).
            weights (np.ndarray, optional): An array of weights. If not specified, equal weights are assumed.
            truncation (float, optional): The maximum deviation (in sigma) to use points in the KDE
            nmin (int, optional): The minimum number of points required to estimate the density
            factor (float, optional): Send bandwidth to this factor of the data estimate
        '''
        pass

    def evaluate(self, data: np.ndarray) -> np.ndarray:
        '''Estimate un-normalised probability density at target points
        Args:
            data (np.ndarray): 2D array of shape (n_samples, n_dim).
        Returns:
            np.ndarray: A `(n_samples)` length array of estimates
        '''
        pass",3,3,33.0,5.0,20.0,12.0,4.0,0.76,0.0,3.0,0.0,0.0,2.0,12.0,2.0,2.0,78.0,13.0,41.0,30.0,31.0,31.0,33.0,23.0,30.0,4.0,0.0,2.0,7.0,snippet_203
285844,4Catalyzer/flask-resty,4Catalyzer_flask-resty/flask_resty/authentication.py,flask_resty.authentication.AuthenticationBase,"class AuthenticationBase:
    """"""Base class for API authentication components.

    Authentication components are responsible for extracting the request
    credentials, if any. They should raise a 401 if the credentials are
    invalid, but should provide `None` for unauthenticated users.

    Flask-RESTy provides an implementation using `JSON Web Tokens`_  but you
    can use any authentication component by extending
    :py:class:`AuthenticationBase` and implementing
    :py:meth:`get_request_credentials`.

    .. _JSON Web Tokens: https://jwt.io/
    """"""

    def authenticate_request(self):
        """"""Store the request credentials in the
        :py:class:`flask.ctx.AppContext`.

        .. warning::

            No validation is performed by Flask-RESTy. It is up to the
            implementor to validate the request in
            :py:meth:`get_request_credentials`.
        """"""
        set_request_credentials(self.get_request_credentials())

    def get_request_credentials(self):
        """"""Get the credentials for the current request.

        Typically this is done by inspecting :py:data:`flask.request`.

        .. warning::

            Implementing classes **must** raise an exception on authentication
            failure. A 401 Unauthorized :py:class:`ApiError` is recommended.

        :return: The credentials for the current request.
        """"""
        raise NotImplementedError()","class AuthenticationBase:
    '''Base class for API authentication components.
    Authentication components are responsible for extracting the request
    credentials, if any. They should raise a 401 if the credentials are
    invalid, but should provide `None` for unauthenticated users.
    Flask-RESTy provides an implementation using `JSON Web Tokens`_  but you
    can use any authentication component by extending
    :py:class:`AuthenticationBase` and implementing
    :py:meth:`get_request_credentials`.
    .. _JSON Web Tokens: https://jwt.io/
    '''

    def authenticate_request(self):
        '''Store the request credentials in the
        :py:class:`flask.ctx.AppContext`.
        .. warning::
            No validation is performed by Flask-RESTy. It is up to the
            implementor to validate the request in
            :py:meth:`get_request_credentials`.
        '''
        pass

    def get_request_credentials(self):
        '''Get the credentials for the current request.
        Typically this is done by inspecting :py:data:`flask.request`.
        .. warning::
            Implementing classes **must** raise an exception on authentication
            failure. A 401 Unauthorized :py:class:`ApiError` is recommended.
        :return: The credentials for the current request.
        '''
        pass",3,3,12.0,3.0,2.0,7.0,1.0,4.8,0.0,1.0,0.0,2.0,2.0,0.0,2.0,2.0,40.0,11.0,5.0,3.0,2.0,24.0,5.0,3.0,2.0,1.0,0.0,0.0,2.0,snippet_204
285856,4Catalyzer/flask-resty,4Catalyzer_flask-resty/flask_resty/filtering.py,flask_resty.filtering.ArgFilterBase,"class ArgFilterBase:
    """"""An abstract specification of a filter from a query argument.

    Implementing classes must provide :py:meth:`maybe_set_arg_name` and
    :py:meth:`filter_query`.
    """"""

    def maybe_set_arg_name(self, arg_name):
        """"""Set the name of the argument to which this filter is bound.

        :param str arg_name: The name of the field to filter against.
        :raises: :py:class:`NotImplementedError` if no implementation is
            provided.
        """"""
        raise NotImplementedError()

    def filter_query(self, query, view, arg_value):
        """"""Filter the query.

        :param query: The query to filter.
        :type query: :py:class:`sqlalchemy.orm.query.Query`
        :param view: The view with the model we wish to filter for.
        :type view: :py:class:`ModelView`
        :param str arg_value: The filter specification
        :return: The filtered query
        :rtype: :py:class:`sqlalchemy.orm.query.Query`
        :raises: :py:class:`NotImplementedError` if no implementation is
            provided.
        """"""
        raise NotImplementedError()","class ArgFilterBase:
    '''An abstract specification of a filter from a query argument.
    Implementing classes must provide :py:meth:`maybe_set_arg_name` and
    :py:meth:`filter_query`.
    '''

    def maybe_set_arg_name(self, arg_name):
        '''Set the name of the argument to which this filter is bound.
        :param str arg_name: The name of the field to filter against.
        :raises: :py:class:`NotImplementedError` if no implementation is
            provided.
        '''
        pass

    def filter_query(self, query, view, arg_value):
        '''Filter the query.
        :param query: The query to filter.
        :type query: :py:class:`sqlalchemy.orm.query.Query`
        :param view: The view with the model we wish to filter for.
        :type view: :py:class:`ModelView`
        :param str arg_value: The filter specification
        :return: The filtered query
        :rtype: :py:class:`sqlalchemy.orm.query.Query`
        :raises: :py:class:`NotImplementedError` if no implementation is
            provided.
        '''
        pass",3,3,11.0,1.0,2.0,8.0,1.0,4.0,0.0,1.0,0.0,1.0,2.0,0.0,2.0,2.0,30.0,5.0,5.0,3.0,2.0,20.0,5.0,3.0,2.0,1.0,0.0,0.0,2.0,snippet_205
285958,tcalmant/python-javaobj,tcalmant_python-javaobj/javaobj/constants.py,javaobj.constants.StreamCodeDebug,"class StreamCodeDebug:
    """"""
    Codes utility methods
    """"""

    @staticmethod
    def op_id(op_id):
        """"""
        Returns the name of the given OP Code
        :param op_id: OP Code
        :return: Name of the OP Code
        """"""
        try:
            return TerminalCode(op_id).name
        except ValueError:
            return '<unknown TC:{0}>'.format(op_id)

    @staticmethod
    def type_code(type_id):
        """"""
        Returns the name of the given Type Code
        :param type_id: Type code
        :return: Name of the type code
        """"""
        try:
            return TypeCode(type_id).name
        except ValueError:
            return '<unknown TypeCode:{0}>'.format(type_id)

    @staticmethod
    def flags(flags):
        """"""
        Returns the names of the class description flags found in the given
        integer

        :param flags: A class description flag entry
        :return: The flags names as a single string
        """"""
        names = sorted((key.name for key in ClassDescFlags if key & flags))
        return ', '.join(names)","class StreamCodeDebug:
    '''
    Codes utility methods
    '''
    @staticmethod
    def op_id(op_id):
        '''
        Returns the name of the given OP Code
        :param op_id: OP Code
        :return: Name of the OP Code
        '''
        pass
    @staticmethod
    def type_code(type_id):
        '''
        Returns the name of the given Type Code
        :param type_id: Type code
        :return: Name of the type code
        '''
        pass
    @staticmethod
    def flags(flags):
        '''
        Returns the names of the class description flags found in the given
        integer
        :param flags: A class description flag entry
        :return: The flags names as a single string
        '''
        pass",4,4,11.0,0.0,4.0,6.0,2.0,1.29,0.0,4.0,3.0,0.0,0.0,0.0,3.0,3.0,43.0,4.0,17.0,8.0,10.0,22.0,14.0,5.0,10.0,2.0,0.0,1.0,5.0,snippet_206
285972,tcalmant/python-javaobj,tcalmant_python-javaobj/javaobj/v2/api.py,javaobj.v2.api.IJavaStreamParser,"class IJavaStreamParser:
    """"""
    API of the Java stream parser
    """"""

    def run(self):
        """"""
        Parses the input stream
        """"""
        raise NotImplementedError

    def dump(self, content):
        """"""
        Dumps to a string the given objects
        """"""
        raise NotImplementedError

    def _read_content(self, type_code, block_data, class_desc=None):
        """"""
        Parses the next content. Use with care (use only in a transformer)
        """"""","class IJavaStreamParser:
    '''
    API of the Java stream parser
    '''

    def run(self):
        '''
        Parses the input stream
        '''
        pass

    def dump(self, content):
        '''
        Dumps to a string the given objects
        '''
        pass

    def _read_content(self, type_code, block_data, class_desc=None):
        '''
        Parses the next content. Use with care (use only in a transformer)
        '''
        pass",4,4,6.0,0.0,2.0,4.0,1.0,2.5,0.0,1.0,0.0,1.0,3.0,0.0,3.0,3.0,24.0,3.0,6.0,4.0,2.0,15.0,6.0,4.0,2.0,1.0,0.0,0.0,3.0,snippet_207
287233,materialsvirtuallab/monty,materialsvirtuallab_monty/src/monty/design_patterns.py,monty.design_patterns.NullFile,"import os

class NullFile:
    """"""A file object that is associated to /dev/null.""""""

    def __new__(cls):
        """"""
        Pass through.
        """"""
        return open(os.devnull, 'w')

    def __init__(self):
        """"""no-op""""""","
class NullFile:
    '''A file object that is associated to /dev/null.'''

    def __new__(cls):
        '''
        Pass through.
        '''
        pass

    def __init__(self):
        '''no-op'''
        pass",3,3,4.0,0.0,2.0,3.0,1.0,1.5,0.0,0.0,0.0,0.0,2.0,0.0,2.0,2.0,11.0,2.0,4.0,3.0,1.0,6.0,4.0,3.0,1.0,1.0,0.0,0.0,2.0,snippet_208
287394,common-workflow-language/schema_salad,common-workflow-language_schema_salad/schema_salad/cpp_codegen.py,schema_salad.cpp_codegen.ClassDefinition,"from typing import IO, Any, Optional, Union, cast

class ClassDefinition:
    """"""Prototype of a class.""""""

    def __init__(self, name: str):
        """"""Initialize the class definition with a name.""""""
        self.fullName = name
        self.extends: list[dict[str, str]] = []
        self.specializationTypes: list[str] = []
        self.allfields: list[FieldDefinition] = []
        self.fields: list[FieldDefinition] = []
        self.abstract = False
        self.namespace, self.classname = split_name(name)
        self.namespace = safenamespacename(self.namespace)
        self.classname = safename(self.classname)

    def writeFwdDeclaration(self, target: IO[str], fullInd: str, ind: str) -> None:
        """"""Write forward declaration.""""""
        target.write(f'{fullInd}namespace {self.namespace} {{ struct {self.classname}; }}\n')

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, common_namespace: str) -> None:
        """"""Write definition of the class.""""""
        target.write(f'{fullInd}namespace {self.namespace} {{\n')
        target.write(f'{fullInd}struct {self.classname}')
        extends = list(map(safename2, self.extends))
        override = ''
        virtual = 'virtual '
        if len(self.extends) > 0:
            target.write(f'\n{fullInd}{ind}: ')
            target.write(f'\n{fullInd}{ind}, '.join(extends))
            override = ' override'
            virtual = ''
        target.write(' {\n')
        for field in self.fields:
            field.writeDefinition(target, fullInd + ind, ind, self.namespace)
        if self.abstract:
            target.write(f'{fullInd}{ind}virtual ~{self.classname}() = 0;\n')
        else:
            target.write(f'{fullInd}{ind}{virtual}~{self.classname}(){override} = default;\n')
        target.write(f'{fullInd}{ind}{virtual}auto toYaml([[maybe_unused]] {common_namespace}::store_config const& config) const -> YAML::Node{override};\n')
        target.write(f'{fullInd}{ind}{virtual}void fromYaml(YAML::Node const& n){override};\n')
        target.write(f'{fullInd}}};\n')
        target.write(f'{fullInd}}}\n\n')

    def writeImplDefinition(self, target: IO[str], fullInd: str, ind: str, common_namespace: str) -> None:
        """"""Write definition with implementation.""""""
        extends = list(map(safename2, self.extends))
        if self.abstract:
            target.write(f'{fullInd}inline {self.namespace}::{self.classname}::~{self.classname}() = default;\n')
        target.write(f'{fullInd}inline auto {self.namespace}::{self.classname}::toYaml([[maybe_unused]] ::{common_namespace}::store_config const& config) const -> YAML::Node {{\n{fullInd}{ind}using ::{common_namespace}::toYaml;\n{fullInd}{ind}auto n = YAML::Node{{}};\n{fullInd}{ind}if (config.generateTags) {{\n{fullInd}{ind}{ind}n.SetTag(""{self.classname}"");\n{fullInd}{ind}}}\n')
        for e in extends:
            target.write(f'{fullInd}{ind}n = mergeYaml(n, {e}::toYaml(config));\n')
        for field in self.fields:
            fieldname = safename(field.name)
            target.write(f'{fullInd}{ind}{{\n')
            target.write(f'{fullInd}{ind}{ind} auto member = toYaml(*{fieldname}, config);\n')
            if field.typeDSL:
                target.write(f'{fullInd}{ind}{ind} member = simplifyType(member, config);\n')
            target.write(f'{fullInd}{ind}{ind} member = convertListToMap(member, {q(field.mapSubject)}, {q(field.mapPredicate)}, config);\n')
            target.write(f'{fullInd}{ind}{ind}addYamlField(n, {q(field.name)}, member);\n')
            target.write(f'{fullInd}{ind}}}\n')
        target.write(f'{fullInd}{ind}return n;\n{fullInd}}}\n')
        functionname = f'{self.namespace}::{self.classname}::fromYaml'
        target.write(f'{fullInd}inline void {functionname}([[maybe_unused]] YAML::Node const& n) {{\n{fullInd}{ind}using ::{common_namespace}::fromYaml;\n')
        for e in extends:
            target.write(f'{fullInd}{ind}{e}::fromYaml(n);\n')
        for field in self.fields:
            fieldname = safename(field.name)
            expandType = ''
            if field.typeDSL:
                expandType = 'expandType'
            target.write(f'{fullInd}{ind}{{\n{fullInd}{ind}{ind}auto nodeAsList = convertMapToList(n[{q(field.name)}], {q(field.mapSubject)}, {q(field.mapPredicate)});\n{fullInd}{ind}{ind}auto expandedNode = {expandType}(nodeAsList);\n{fullInd}{ind}{ind}fromYaml(expandedNode, *{fieldname});\n{fullInd}{ind}}}\n')
        target.write(f'{fullInd}}}\n')
        if not self.abstract:
            e = f'::{self.namespace}::{self.classname}'
            target.write(f'namespace {common_namespace} {{\ntemplate <>\nstruct DetectAndExtractFromYaml<{e}> {{\n    auto operator()(YAML::Node const& n) const -> std::optional<{e}> {{\n        if (!n.IsDefined()) return std::nullopt;\n        if (!n.IsMap()) return std::nullopt;\n        auto res = {e}{{}};\n\n')
            for field in self.fields:
                fieldname = safename(field.name)
                target.write(f'        if constexpr (::{common_namespace}::IsConstant<decltype(res.{fieldname})::value_t>::value) try {{\n            fromYaml(n[{q(field.name)}], *res.{fieldname});\n            fromYaml(n, res);\n            return res;\n        }} catch(...) {{}}\n\n')
            target.write('        return std::nullopt;\n    }\n};\n}\n')","
class ClassDefinition:
    '''Prototype of a class.'''

    def __init__(self, name: str):
        '''Initialize the class definition with a name.'''
        pass

    def writeFwdDeclaration(self, target: IO[str], fullInd: str, ind: str) -> None:
        '''Write forward declaration.'''
        pass

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, common_namespace: str) -> None:
        '''Write definition of the class.'''
        pass

    def writeImplDefinition(self, target: IO[str], fullInd: str, ind: str, common_namespace: str) -> None:
        '''Write definition with implementation.'''
        pass",5,5,35.0,4.0,29.0,3.0,4.0,0.1,0.0,7.0,1.0,0.0,4.0,8.0,4.0,4.0,145.0,19.0,115.0,26.0,106.0,11.0,69.0,22.0,64.0,10.0,0.0,2.0,16.0,snippet_209
287396,common-workflow-language/schema_salad,common-workflow-language_schema_salad/schema_salad/cpp_codegen.py,schema_salad.cpp_codegen.EnumDefinition,"from typing import IO, Any, Optional, Union, cast

class EnumDefinition:
    """"""Prototype of a enum.""""""

    def __init__(self, name: str, values: list[str]):
        """"""Initialize enum definition with a name and possible values.""""""
        self.name = name
        self.values = values
        self.namespace, self.classname = split_name(name)
        self.namespace = safenamespacename(self.namespace)
        self.classname = safename(self.classname)

    def writeDefinition(self, target: IO[str], ind: str, common_namespace: str) -> None:
        """"""Write enum definition to output.""""""
        namespace = ''
        if len(self.name.split('#')) == 2:
            namespace, classname = split_name(self.name)
            namespace = safenamespacename(namespace)
            classname = safename(classname)
            name = namespace + '::' + classname
        else:
            name = safename(self.name)
            classname = name
        if len(namespace) > 0:
            target.write(f'namespace {namespace} {{\n')
        target.write(f'enum class {classname} : unsigned int {{\n{ind}')
        target.write(f',\n{ind}'.join(map(safename, self.values)))
        target.write('\n};\n')
        target.write(f'inline auto to_string({classname} v) {{\n')
        target.write(f'{ind}static auto m = std::vector<std::string_view> {{\n')
        target.write(f'{ind}    ""')
        target.write(f'"",\n{ind}    ""'.join(self.values))
        target.write(f'""\n{ind}}};\n')
        target.write(f'{ind}using U = std::underlying_type_t<{name}>;\n')
        target.write(f'{ind}return m.at(static_cast<U>(v));\n}}\n')
        if len(namespace) > 0:
            target.write('}\n')
        target.write(f'inline void to_enum(std::string_view v, {name}& out) {{\n')
        target.write(f'{ind}static auto m = std::map<std::string, {name}, std::less<>> {{\n')
        for v in self.values:
            target.write(f'{ind}{ind}{{{q(v)}, {name}::{safename(v)}}},\n')
        target.write(f'{ind}}};\n{ind}auto iter = m.find(v);\n')
        target.write(f'{ind}if (iter == m.end()) throw bool{{}};\n')
        target.write(f'{ind}out = iter->second;\n}}\n')
        target.write(f'namespace {common_namespace} {{\n')
        target.write(f'inline auto toYaml({name} v, [[maybe_unused]] ::{common_namespace}::store_config const& config) {{\n')
        target.write(f'{ind}auto n = YAML::Node{{std::string{{to_string(v)}}}};\n')
        target.write(f'{ind}if (config.generateTags) n.SetTag(""{name}"");\n')
        target.write(f'{ind}return n;\n}}\n')
        target.write(f'inline void fromYaml(YAML::Node n, {name}& out) {{\n')
        target.write(f'{ind}to_enum(n.as<std::string>(), out);\n}}\n')
        if len(self.values):
            target.write(f'template <> struct IsConstant<{name}> : std::true_type {{}};\n')
        target.write('}\n')
        target.write('\n')","
class EnumDefinition:
    '''Prototype of a enum.'''

    def __init__(self, name: str, values: list[str]):
        '''Initialize enum definition with a name and possible values.'''
        pass

    def writeDefinition(self, target: IO[str], ind: str, common_namespace: str) -> None:
        '''Write enum definition to output.'''
        pass",3,3,33.0,5.0,26.0,3.0,4.0,0.12,0.0,4.0,0.0,0.0,2.0,4.0,2.0,2.0,69.0,12.0,52.0,10.0,49.0,6.0,48.0,10.0,45.0,6.0,0.0,1.0,7.0,snippet_210
287397,common-workflow-language/schema_salad,common-workflow-language_schema_salad/schema_salad/cpp_codegen.py,schema_salad.cpp_codegen.FieldDefinition,"from typing import IO, Any, Optional, Union, cast

class FieldDefinition:
    """"""Prototype of a single field from a class definition.""""""

    def __init__(self, name: str, typeStr: str, optional: bool, mapSubject: str, mapPredicate: str, typeDSL: bool):
        """"""Initialize field definition.

        Creates a new field with name, its type, optional and which field to use to convert
        from list to map (or empty if it is not possible)
        """"""
        self.name = name
        self.typeStr = typeStr
        self.optional = optional
        self.mapSubject = mapSubject
        self.mapPredicate = mapPredicate
        self.typeDSL = typeDSL

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, namespace: str) -> None:
        """"""Write a C++ definition for the class field.""""""
        name = safename(self.name)
        typeStr = self.typeStr.replace(namespace + '::', '')
        target.write(f'{fullInd}heap_object<{typeStr}> {name};\n')","
class FieldDefinition:
    '''Prototype of a single field from a class definition.'''

    def __init__(self, name: str, typeStr: str, optional: bool, mapSubject: str, mapPredicate: str, typeDSL: bool):
        '''Initialize field definition.
        Creates a new field with name, its type, optional and which field to use to convert
        from list to map (or empty if it is not possible)
        '''
        pass

    def writeDefinition(self, target: IO[Any], fullInd: str, ind: str, namespace: str) -> None:
        '''Write a C++ definition for the class field.'''
        pass",3,3,13.0,1.0,10.0,3.0,1.0,0.3,0.0,4.0,0.0,0.0,2.0,6.0,2.0,2.0,29.0,3.0,20.0,19.0,9.0,6.0,12.0,11.0,9.0,1.0,0.0,0.0,2.0,snippet_211
287519,kinegratii/borax,kinegratii_borax/borax/calendars/utils.py,borax.calendars.utils.ThreeNineUtils,"from borax.calendars.lunardate import LunarDate, TextUtils, TermUtils
from collections import OrderedDict
from datetime import date, datetime, timedelta
from typing import Union, Dict

class ThreeNineUtils:
    """"""三伏数九天工具函数
    """"""

    @staticmethod
    def get_39days(year: int) -> Dict[str, date]:
        """"""获取公历year年的三伏数九天对应的公历日期。
        """"""
        day13 = TermUtils.day_start_from_term(year, '夏至', 3, '庚')
        day23 = day13 + timedelta(days=10)
        day33 = TermUtils.day_start_from_term(year, '立秋', 1, '庚')
        day19 = TermUtils.day_start_from_term(year, '冬至', 0)
        days = OrderedDict({'初伏': day13, '中伏': day23, '末伏': day33, '一九': day19})
        for i, dc in enumerate(TextUtils.DAYS_CN[1:10], start=1):
            days[f'{dc}九'] = day19 + timedelta(days=(i - 1) * 9)
        return days

    @staticmethod
    def get_39label(date_obj: Union[date, LunarDate]) -> str:
        """"""返回三伏数九天对应的标签，如果不是，返回空字符串。
        """"""
        if isinstance(date_obj, LunarDate):
            sd = date_obj.to_solar_date()
        else:
            sd = date_obj
        if sd.month in (4, 5, 6, 10, 11):
            return ''
        year = sd.year - bool(sd.month < 4)
        days = ThreeNineUtils.get_39days(year)
        for vs in list(days.items()):
            label, sd = vs
            range_len = -1
            if label in ['初伏', '末伏']:
                range_len = 10
            elif label == '中伏':
                range_len = (days['末伏'] - days['中伏']).days
            elif '九' in label:
                range_len = 9
            offset = (date_obj - sd).days
            if 0 <= offset <= range_len - 1:
                return f'{label}第{offset + 1}天'
        return ''","
class ThreeNineUtils:
    '''三伏数九天工具函数
    '''
    @staticmethod
    def get_39days(year: int) -> Dict[str, date]:
        '''获取公历year年的三伏数九天对应的公历日期。
        '''
        pass
    @staticmethod
    def get_39label(date_obj: Union[date, LunarDate]) -> str:
        '''返回三伏数九天对应的标签，如果不是，返回空字符串。
        '''
        pass",3,3,20.0,0.0,18.0,2.0,5.0,0.15,0.0,11.0,3.0,0.0,0.0,0.0,2.0,2.0,47.0,2.0,39.0,18.0,34.0,6.0,29.0,16.0,26.0,8.0,0.0,2.0,10.0,snippet_212
292643,swistakm/graceful,swistakm_graceful/src/graceful/authentication.py,graceful.authentication.BaseAuthenticationMiddleware,"class BaseAuthenticationMiddleware:
    """"""Base class for all authentication middleware classes.

    Args:
        user_storage (BaseUserStorage): a storage object used to retrieve
            user object using their identifier lookup.
        name (str): custom name of the authentication middleware useful
            for handling custom user storage backends. Defaults to middleware
            class name.

    .. versionadded:: 0.4.0
    """"""
    challenge = None
    only_with_storage = False

    def __init__(self, user_storage=None, name=None):
        """"""Initialize authentication middleware.""""""
        self.user_storage = user_storage
        self.name = name if name else self.__class__.__name__
        if self.only_with_storage and (not isinstance(self.user_storage, BaseUserStorage)):
            raise ValueError('{} authentication middleware requires valid storage. Got {}.'.format(self.__class__.__name__, self.user_storage))

    def process_resource(self, req, resp, resource, uri_kwargs=None):
        """"""Process resource after routing to it.

        This is basic falcon middleware handler.

        Args:
            req (falcon.Request): request object
            resp (falcon.Response): response object
            resource (object): resource object matched by falcon router
            uri_kwargs (dict): additional keyword argument from uri template.
                For ``falcon<1.0.0`` this is always ``None``
        """"""
        if 'user' in req.context:
            return
        identifier = self.identify(req, resp, resource, uri_kwargs)
        user = self.try_storage(identifier, req, resp, resource, uri_kwargs)
        if user is not None:
            req.context['user'] = user
        elif self.challenge is not None:
            req.context.setdefault('challenges', list()).append(self.challenge)

    def identify(self, req, resp, resource, uri_kwargs):
        """"""Identify the user that made the request.

        Args:
            req (falcon.Request): request object
            resp (falcon.Response): response object
            resource (object): resource object matched by falcon router
            uri_kwargs (dict): additional keyword argument from uri template.
                For ``falcon<1.0.0`` this is always ``None``

        Returns:
            object: a user object (preferably a dictionary).
        """"""
        raise NotImplementedError

    def try_storage(self, identifier, req, resp, resource, uri_kwargs):
        """"""Try to find user in configured user storage object.

        Args:
            identifier: User identifier.

        Returns:
            user object.
        """"""
        if identifier is None:
            user = None
        elif self.user_storage is not None:
            user = self.user_storage.get_user(self, identifier, req, resp, resource, uri_kwargs)
        elif self.user_storage is None and (not self.only_with_storage):
            user = {'identified_with': self, 'identifier': identifier}
        else:
            user = None
        return user","class BaseAuthenticationMiddleware:
    '''Base class for all authentication middleware classes.
    Args:
        user_storage (BaseUserStorage): a storage object used to retrieve
            user object using their identifier lookup.
        name (str): custom name of the authentication middleware useful
            for handling custom user storage backends. Defaults to middleware
            class name.
    .. versionadded:: 0.4.0
    '''

    def __init__(self, user_storage=None, name=None):
        '''Initialize authentication middleware.'''
        pass

    def process_resource(self, req, resp, resource, uri_kwargs=None):
        '''Process resource after routing to it.
        This is basic falcon middleware handler.
        Args:
            req (falcon.Request): request object
            resp (falcon.Response): response object
            resource (object): resource object matched by falcon router
            uri_kwargs (dict): additional keyword argument from uri template.
                For ``falcon<1.0.0`` this is always ``None``
        '''
        pass

    def identify(self, req, resp, resource, uri_kwargs):
        '''Identify the user that made the request.
        Args:
            req (falcon.Request): request object
            resp (falcon.Response): response object
            resource (object): resource object matched by falcon router
            uri_kwargs (dict): additional keyword argument from uri template.
                For ``falcon<1.0.0`` this is always ``None``
        Returns:
            object: a user object (preferably a dictionary).
        '''
        pass

    def try_storage(self, identifier, req, resp, resource, uri_kwargs):
        '''Try to find user in configured user storage object.
        Args:
            identifier: User identifier.
        Returns:
            user object.
        '''
        pass",5,5,23.0,4.0,10.0,10.0,3.0,1.16,0.0,4.0,1.0,5.0,4.0,2.0,4.0,4.0,115.0,22.0,44.0,12.0,39.0,51.0,25.0,12.0,20.0,4.0,0.0,1.0,12.0,snippet_213
292661,swistakm/graceful,swistakm_graceful/src/graceful/parameters.py,graceful.parameters.BaseParam,"import inspect

class BaseParam:
    """"""Base parameter class for subclassing.

    To create new parameter type subclass ``BaseParam`` and implement
    ``.value()`` method handler.

    Args:
        details (str): verbose description of parameter. Should contain all
            information that may be important to your API user and will be used
            for describing resource on ``OPTIONS`` requests and ``.describe()``
            call.

        label (str): human readable label for this parameter (it will be used
            for describing resource on OPTIONS requests).

            *Note that it is recomended to use parameter names that are
            self-explanatory intead of relying on param labels.*

        required (bool): if set to ``True`` then all GET, POST, PUT,
            PATCH and DELETE requests will return ``400 Bad Request`` response
            if query param is not provided. Defaults to ``False``.

        default (str): set default value for param if it is not
            provided in request as query parameter. This MUST be a raw string
            value that will be then parsed by ``.value()`` handler.

            If default is set and ``required`` is ``True`` it will raise
            ``ValueError`` as having required parameters with default
            value has no sense.

        many (str): set to ``True`` if multiple occurences of this parameter
            can be included in query string, as a result values for this
            parameter will be always included as a list in params dict.
            Defaults to ``False``. Instead of ``list`` you can use any
            list-compatible data type by overriding the ``container`` class
            attribute. See: :ref:`guide-params-custom-containers`.

            .. versionadded:: 0.1.0

        validators (list): list of validator callables.

            .. versionadded:: 0.2.0

    .. note::
        If ``many=False`` and client inlcudes multiple values for this
        parameter in query string then only one of those values will be
        returned, and it is undefined which one.

    **Example:**

    .. code-block:: python

           class BoolParam(BaseParam):
               def value(self, data):
                   if data in {'true', 'True', 'yes', '1', 'Y'}:
                       return True
                   elif data in {'false', 'False', 'no', '0', 'N'}:
                       return False
                   else:
                       raise ValueError(
                           ""{data} is not valid boolean field"".format(
                               data=data
                           )
                       )

    """"""
    spec = None
    type = None
    container = list

    def __init__(self, details, label=None, required=False, default=None, many=False, validators=None):
        """"""Initialize parameter and verify default/required contraints.""""""
        self.label = label
        self.details = details
        self.required = required
        self.many = many
        self.validators = validators or []
        if not default is None and (not isinstance(default, str)):
            raise TypeError(""value for {cls} 'default' argument must be string instance"".format(cls=self.__class__.__name__))
        if not default is None and required:
            raise ValueError(""{cls}(required={required}, default='{default}'): initialization with both required and default makes no sense"".format(cls=self.__class__.__name__, default=default, required=required))
        self.default = default

    def validated_value(self, raw_value):
        """"""Return parsed parameter value and run validation handlers.

        Error message included in exception will be included in http error
        response

        Args:
            value: raw parameter value to parse validate

        Returns:
            None

        Note:
            Concept of validation for params is understood here as a process
            of checking if data of valid type (successfully parsed/processed by
            ``.value()`` handler) does meet some other constraints
            (lenght, bounds, uniqueness, etc.). It will internally call its
            ``value()`` handler.

        """"""
        value = self.value(raw_value)
        try:
            for validator in self.validators:
                validator(value)
        except:
            raise
        else:
            return value

    def value(self, raw_value):
        """"""Raw value deserialization method handler.

        Args:
            raw_value (str): raw value from GET parameters

        """"""
        raise NotImplementedError('{cls}.value() method not implemented'.format(cls=self.__class__.__name__))

    def describe(self, **kwargs):
        """"""Describe this parameter instance for purpose of self-documentation.

        Args:
            kwargs (dict): dictionary of additional description items for
                extending default description

        Returns:
            dict: dictionary of description items


        Suggested way for overriding description fields or extending it with
        additional items is calling super class method with new/overriden
        fields passed as keyword arguments like following:

        .. code-block:: python

            class DummyParam(BaseParam):
                def description(self, **kwargs):
                    super().describe(is_dummy=True, **kwargs)

        """"""
        description = {'label': self.label, 'details': inspect.cleandoc(self.details), 'required': self.required, 'many': self.many, 'spec': self.spec, 'default': self.default, 'type': self.type or 'unspecified'}
        description.update(kwargs)
        return description","
class BaseParam:
    '''Base parameter class for subclassing.
    To create new parameter type subclass ``BaseParam`` and implement
    ``.value()`` method handler.
    Args:
        details (str): verbose description of parameter. Should contain all
            information that may be important to your API user and will be used
            for describing resource on ``OPTIONS`` requests and ``.describe()``
            call.
        label (str): human readable label for this parameter (it will be used
            for describing resource on OPTIONS requests).
            *Note that it is recomended to use parameter names that are
            self-explanatory intead of relying on param labels.*
        required (bool): if set to ``True`` then all GET, POST, PUT,
            PATCH and DELETE requests will return ``400 Bad Request`` response
            if query param is not provided. Defaults to ``False``.
        default (str): set default value for param if it is not
            provided in request as query parameter. This MUST be a raw string
            value that will be then parsed by ``.value()`` handler.
            If default is set and ``required`` is ``True`` it will raise
            ``ValueError`` as having required parameters with default
            value has no sense.
        many (str): set to ``True`` if multiple occurences of this parameter
            can be included in query string, as a result values for this
            parameter will be always included as a list in params dict.
            Defaults to ``False``. Instead of ``list`` you can use any
            list-compatible data type by overriding the ``container`` class
            attribute. See: :ref:`guide-params-custom-containers`.
            .. versionadded:: 0.1.0
        validators (list): list of validator callables.
            .. versionadded:: 0.2.0
    .. note::
        If ``many=False`` and client inlcudes multiple values for this
        parameter in query string then only one of those values will be
        returned, and it is undefined which one.
    **Example:**
    .. code-block:: python
           class BoolParam(BaseParam):
               def value(self, data):
                   if data in {'true', 'True', 'yes', '1', 'Y'}:
                       return True
                   elif data in {'false', 'False', 'no', '0', 'N'}:
                       return False
                   else:
                       raise ValueError(
                           ""{data} is not valid boolean field"".format(
                               data=data
                           )
                       )
    '''

    def __init__(self, details, label=None, required=False, default=None, many=False, validators=None):
        '''Initialize parameter and verify default/required contraints.'''
        pass

    def validated_value(self, raw_value):
        '''Return parsed parameter value and run validation handlers.
        Error message included in exception will be included in http error
        response
        Args:
            value: raw parameter value to parse validate
        Returns:
            None
        Note:
            Concept of validation for params is understood here as a process
            of checking if data of valid type (successfully parsed/processed by
            ``.value()`` handler) does meet some other constraints
            (lenght, bounds, uniqueness, etc.). It will internally call its
            ``value()`` handler.
        '''
        pass
               def value(self, data):
                   '''Raw value deserialization method handler.
        Args:
            raw_value (str): raw value from GET parameters
                              '''
                              pass

    def describe(self, **kwargs):
        '''Describe this parameter instance for purpose of self-documentation.
        Args:
            kwargs (dict): dictionary of additional description items for
                extending default description
        Returns:
            dict: dictionary of description items

        Suggested way for overriding description fields or extending it with
        additional items is calling super class method with new/overriden
        fields passed as keyword arguments like following:
        .. code-block:: python
            class DummyParam(BaseParam):
                def description(self, **kwargs):
                    super().describe(is_dummy=True, **kwargs)
                                '''
                                pass",5,5,27.0,4.0,14.0,9.0,2.0,1.51,0.0,4.0,0.0,8.0,4.0,6.0,4.0,4.0,193.0,40.0,61.0,25.0,48.0,92.0,30.0,17.0,25.0,3.0,0.0,2.0,8.0,snippet_214
292765,pytroll/pyspectral,pytroll_pyspectral/rsr_convert_scripts/avhrr1_rsr.py,avhrr1_rsr.AvhrrRSR,"import numpy as np
from pyspectral.config import get_config
from xlrd import open_workbook
import os

class AvhrrRSR:
    """"""Container for the NOAA AVHRR-1 RSR data.""""""

    def __init__(self, wavespace='wavelength'):
        """"""Initialize the AVHRR-1 RSR class.""""""
        options = get_config()
        self.avhrr_path = options['avhrr/1'].get('path')
        if not os.path.exists(self.avhrr_path):
            self.avhrr1_path = os.path.join(DATA_PATH, options['avhrr/1'].get('filename'))
        self.output_dir = options.get('rsr_dir', './')
        self.rsr = {}
        for satname in AVHRR1_SATELLITES:
            self.rsr[satname] = {}
            for chname in AVHRR_BAND_NAMES['avhrr/1']:
                self.rsr[satname][chname] = {'wavelength': None, 'response': None}
        self._load()
        self.wavespace = wavespace
        if wavespace not in ['wavelength', 'wavenumber']:
            raise AttributeError('wavespace has to be either ' + ""'wavelength' or 'wavenumber'!"")
        self.unit = 'micrometer'
        if wavespace == 'wavenumber':
            self.convert2wavenumber()

    def _load(self, scale=1.0):
        """"""Load the AVHRR RSR data for the band requested.""""""
        wb_ = open_workbook(self.avhrr_path)
        sheet_names = []
        for sheet in wb_.sheets():
            if sheet.name in ['Kleespies Data']:
                print('Skip sheet...')
                continue
            ch_name = CHANNEL_NAMES.get(sheet.name.strip())
            if not ch_name:
                break
            sheet_names.append(sheet.name.strip())
            header = sheet.col_values(0, start_rowx=0, end_rowx=2)
            platform_name = header[0].strip('# ')
            unit = header[1].split('Wavelength (')[1].strip(')')
            scale = get_scale_from_unit(unit)
            wvl = sheet.col_values(0, start_rowx=2)
            is_comment = True
            idx = 0
            while is_comment:
                item = wvl[::-1][idx]
                if isinstance(item, str):
                    idx = idx + 1
                else:
                    break
            ndim = len(wvl) - idx
            wvl = wvl[0:ndim]
            if platform_name == 'TIROS-N':
                wvl = adjust_typo_avhrr1_srf_only_xls_file(platform_name, wvl)
            response = sheet.col_values(1, start_rowx=2, end_rowx=2 + ndim)
            wavelength = np.array(wvl) * scale
            response = np.array(response)
            self.rsr[platform_name][ch_name]['wavelength'] = wavelength
            self.rsr[platform_name][ch_name]['response'] = response","
class AvhrrRSR:
    '''Container for the NOAA AVHRR-1 RSR data.'''

    def __init__(self, wavespace='wavelength'):
        '''Initialize the AVHRR-1 RSR class.'''
        pass

    def _load(self, scale=1.0):
        '''Load the AVHRR RSR data for the band requested.'''
        pass",3,3,36.0,8.0,27.0,2.0,7.0,0.09,0.0,2.0,0.0,0.0,2.0,6.0,2.0,2.0,76.0,18.0,54.0,26.0,51.0,5.0,51.0,26.0,48.0,7.0,0.0,3.0,13.0,snippet_215
293144,Kentzo/Power,Kentzo_Power/power/common.py,power.common.PowerManagementObserver,"from abc import ABCMeta, abstractmethod

class PowerManagementObserver:
    """"""
    Base class for PowerManagement observers.
    Do not make assumptions in what thread or event loop these methods are called.
    """"""
    __metaclass__ = ABCMeta

    @abstractmethod
    def on_power_sources_change(self, power_management):
        """"""
        @param power_management: Instance of PowerManagement posted notification
        """"""
        pass

    @abstractmethod
    def on_time_remaining_change(self, power_management):
        """"""
        @param power_management: Instance of PowerManagement posted notification
        """"""
        pass","
class PowerManagementObserver:
    '''
    Base class for PowerManagement observers.
    Do not make assumptions in what thread or event loop these methods are called.
    '''
    @abstractmethod
    def on_power_sources_change(self, power_management):
        '''
        @param power_management: Instance of PowerManagement posted notification
        '''
        pass
    @abstractmethod
    def on_time_remaining_change(self, power_management):
        '''
        @param power_management: Instance of PowerManagement posted notification
        '''
        pass",3,3,5.0,0.0,2.0,3.0,1.0,1.25,0.0,0.0,0.0,0.0,2.0,0.0,2.0,2.0,20.0,2.0,8.0,6.0,3.0,10.0,6.0,4.0,3.0,1.0,0.0,0.0,2.0,snippet_216
293849,hasgeek/coaster,hasgeek_coaster/src/coaster/logger.py,coaster.logger.FilteredValueIndicator,"class FilteredValueIndicator:
    """"""Represent a filtered value.""""""

    def __str__(self) -> str:
        """"""Filter str.""""""
        return '[Filtered]'

    def __repr__(self) -> str:
        """"""Filter repr.""""""
        return '[Filtered]'","class FilteredValueIndicator:
    '''Represent a filtered value.'''

    def __str__(self) -> str:
        '''Filter str.'''
        pass

    def __repr__(self) -> str:
        '''Filter repr.'''
        pass",3,3,3.0,0.0,2.0,1.0,1.0,0.6,0.0,1.0,0.0,0.0,2.0,0.0,2.0,2.0,10.0,2.0,5.0,3.0,2.0,3.0,5.0,3.0,2.0,1.0,0.0,0.0,2.0,snippet_217
293851,hasgeek/coaster,hasgeek_coaster/src/coaster/logger.py,coaster.logger.RepeatValueIndicator,"class RepeatValueIndicator:
    """"""Represent a repeating value.""""""

    def __init__(self, key: str) -> None:
        """"""Init with key.""""""
        self.key = key

    def __repr__(self) -> str:
        """"""Return representation.""""""
        return f'<same as prior {self.key!r}>'
    __str__ = __repr__","class RepeatValueIndicator:
    '''Represent a repeating value.'''

    def __init__(self, key: str) -> None:
        '''Init with key.'''
        pass

    def __repr__(self) -> str:
        '''Return representation.'''
        pass",3,3,3.0,0.0,2.0,1.0,1.0,0.5,0.0,1.0,0.0,0.0,2.0,1.0,2.0,2.0,12.0,3.0,6.0,5.0,3.0,3.0,6.0,5.0,3.0,1.0,0.0,0.0,2.0,snippet_218
295540,yunojuno-archive/django-inbound-email,yunojuno-archive_django-inbound-email/inbound_email/backends/__init__.py,inbound_email.backends.RequestParser,"from django.conf import settings

class RequestParser:
    """"""Abstract base class, to be implemented by service-specific classes.""""""

    @property
    def max_file_size(self):
        """"""The maximum file size to process as an attachment (default=10MB).""""""
        return getattr(settings, 'INBOUND_EMAIL_ATTACHMENT_SIZE_MAX', 10000000)

    def parse(self, request):
        """"""Parse a request object into an EmailMultiAlternatives instance.

        This function is where the hard word gets done. The following fields are
        parsed from the request.POST dict:

        * from - the sender
        * to - the recipient list
        * cc - the cc list
        * html - the HTML version of the email
        * text - the text version of the email
        * subject - the subject line
        * attachments

        Inheriting classes should raise RequestParseError if the inbound request
        cannot be converted successfully.

        """"""
        raise NotImplementedError('Must be implemented by inheriting class.')","
class RequestParser:
    '''Abstract base class, to be implemented by service-specific classes.'''
    @property
    def max_file_size(self):
        '''The maximum file size to process as an attachment (default=10MB).'''
        pass

    def parse(self, request):
        '''Parse a request object into an EmailMultiAlternatives instance.
        This function is where the hard word gets done. The following fields are
        parsed from the request.POST dict:
        * from - the sender
        * to - the recipient list
        * cc - the cc list
        * html - the HTML version of the email
        * text - the text version of the email
        * subject - the subject line
        * attachments
        Inheriting classes should raise RequestParseError if the inbound request
        cannot be converted successfully.
        '''
        pass",3,3,11.0,2.0,2.0,7.0,1.0,2.5,0.0,1.0,0.0,3.0,2.0,0.0,2.0,2.0,27.0,6.0,6.0,4.0,2.0,15.0,5.0,3.0,2.0,1.0,0.0,0.0,2.0,snippet_219
295935,cloudsmith-io/cloudsmith-cli,cloudsmith-io_cloudsmith-cli/cloudsmith_cli/core/ratelimits.py,cloudsmith_cli.core.ratelimits.RateLimitsInfo,"import datetime

class RateLimitsInfo:
    """"""Data for rate limits.""""""
    interval = None
    limit = None
    remaining = None
    reset = None
    throttled = None

    def __str__(self):
        """"""Get rate limit information as text.""""""
        return 'Throttled: %(throttled)s, Remaining: %(remaining)d/%(limit)d, Interval: %(interval)f, Reset: %(reset)s' % {'throttled': 'Yes' if self.throttled else 'No', 'remaining': self.remaining, 'limit': self.limit, 'interval': self.interval, 'reset': self.reset}

    @classmethod
    def from_dict(cls, data):
        """"""Create RateLimitsInfo from a dictionary.""""""
        info = RateLimitsInfo()
        if 'interval' in data:
            info.interval = float(data['interval'])
        if 'limit' in data:
            info.limit = int(data['limit'])
        if 'remaining' in data:
            info.remaining = int(data['remaining'])
        if 'reset' in data:
            info.reset = datetime.datetime.utcfromtimestamp(int(data['reset']))
        if 'throtted' in data:
            info.throttled = bool(data['throttled'])
        else:
            info.throttled = info.remaining == 0
        return info

    @classmethod
    def from_headers(cls, headers):
        """"""Create RateLimitsInfo from HTTP headers.""""""
        try:
            data = {'interval': headers['X-RateLimit-Interval'], 'limit': headers['X-RateLimit-Limit'], 'remaining': headers['X-RateLimit-Remaining'], 'reset': headers['X-RateLimit-Reset']}
        except KeyError:
            data = {}
        return cls.from_dict(data)","
class RateLimitsInfo:
    '''Data for rate limits.'''

    def __str__(self):
        '''Get rate limit information as text.'''
        pass
    @classmethod
    def from_dict(cls, data):
        '''Create RateLimitsInfo from a dictionary.'''
        pass
    @classmethod
    def from_headers(cls, headers):
        '''Create RateLimitsInfo from HTTP headers.'''
        pass",4,4,15.0,1.0,13.0,1.0,3.0,0.09,0.0,5.0,0.0,0.0,1.0,0.0,3.0,3.0,57.0,7.0,46.0,13.0,40.0,4.0,28.0,11.0,24.0,6.0,0.0,1.0,10.0,snippet_220
296505,CZ-NIC/python-rt,CZ-NIC_python-rt/rt/rest2.py,rt.rest2.Attachment,"import dataclasses
import base64

@dataclasses.dataclass
class Attachment:
    """"""Dataclass representing an attachment.""""""
    file_name: str
    file_type: str
    file_content: bytes

    def to_dict(self) -> dict[str, str]:
        """"""Convert to a dictionary for submitting to the REST API.""""""
        return {'FileName': self.file_name, 'FileType': self.file_type, 'FileContent': base64.b64encode(self.file_content).decode('utf-8')}

    def multipart_form_element(self) -> tuple[str, bytes, str]:
        """"""Convert to a tuple as required for multipart-form-data submission.""""""
        return (self.file_name, self.file_content, self.file_type)","@dataclasses.dataclass
class Attachment:
    '''Dataclass representing an attachment.'''

    def to_dict(self) -> dict[str, str]:
        '''Convert to a dictionary for submitting to the REST API.'''
        pass

    def multipart_form_element(self) -> tuple[str, bytes, str]:
        '''Convert to a tuple as required for multipart-form-data submission.'''
        pass",3,3,7.0,0.0,6.0,1.0,1.0,0.19,0.0,4.0,0.0,0.0,2.0,0.0,2.0,2.0,22.0,3.0,16.0,3.0,13.0,3.0,8.0,3.0,5.0,1.0,0.0,0.0,2.0,snippet_221
297960,theolind/pymysensors,theolind_pymysensors/mysensors/transport.py,mysensors.transport.Transport,"class Transport:
    """"""Handle gateway transport.

    I/O is allowed in this class. This class should host methods that
    are related to the gateway transport type.
    """"""

    def __init__(self, gateway, connect, timeout=1.0, reconnect_timeout=10.0, **kwargs):
        """"""Set up transport.""""""
        self._connect = connect
        self.can_log = False
        self.connect_task = None
        self.gateway = gateway
        self.protocol = None
        self.reconnect_timeout = reconnect_timeout
        self.timeout = timeout

    def disconnect(self):
        """"""Disconnect from the transport.""""""
        if not self.protocol or not self.protocol.transport:
            self.protocol = None
            return
        _LOGGER.info('Disconnecting from gateway')
        self.protocol.transport.close()
        self.protocol = None

    def send(self, message):
        """"""Write a message to the gateway.""""""
        if not message or not self.protocol or (not self.protocol.transport):
            return
        if not self.can_log:
            _LOGGER.debug('Sending %s', message.strip())
        try:
            self.protocol.transport.write(message.encode())
        except OSError as exc:
            _LOGGER.error('Failed writing to transport %s: %s', self.protocol.transport, exc)
            self.protocol.transport.close()
            self.protocol.conn_lost_callback()","class Transport:
    '''Handle gateway transport.
    I/O is allowed in this class. This class should host methods that
    are related to the gateway transport type.
    '''

    def __init__(self, gateway, connect, timeout=1.0, reconnect_timeout=10.0, **kwargs):
        '''Set up transport.'''
        pass

    def disconnect(self):
        '''Disconnect from the transport.'''
        pass

    def send(self, message):
        '''Write a message to the gateway.'''
        pass",4,4,10.0,0.0,9.0,1.0,2.0,0.31,0.0,1.0,0.0,3.0,3.0,7.0,3.0,3.0,42.0,5.0,29.0,12.0,25.0,9.0,27.0,11.0,23.0,4.0,0.0,1.0,7.0,snippet_222
298148,edx/event-tracking,edx_event-tracking/eventtracking/backends/logger.py,eventtracking.backends.logger.LoggerBackend,"import logging
import json

class LoggerBackend:
    """"""
    Event tracker backend that uses a python logger.

    Events are logged to the INFO level as JSON strings.
    """"""

    def __init__(self, **kwargs):
        """"""
        Event tracker backend that uses a python logger.

        `name` is an identifier for the logger, which should have
            been configured using the default python mechanisms.
        """"""
        name = kwargs.get('name', None)
        self.max_event_size = kwargs.get('max_event_size', MAX_EVENT_SIZE)
        self.event_logger = logging.getLogger(name)
        level = kwargs.get('level', 'info')
        self.log = getattr(self.event_logger, level.lower())

    def send(self, event):
        """"""Send the event to the standard python logger""""""
        event_str = json.dumps(event, cls=DateTimeJSONEncoder)
        if self.max_event_size is None or len(event_str) <= self.max_event_size:
            self.log(event_str)","
class LoggerBackend:
    '''
    Event tracker backend that uses a python logger.
    Events are logged to the INFO level as JSON strings.
    '''

    def __init__(self, **kwargs):
        '''
        Event tracker backend that uses a python logger.
        `name` is an identifier for the logger, which should have
            been configured using the default python mechanisms.
        '''
        pass

    def send(self, event):
        '''Send the event to the standard python logger'''
        pass",3,3,10.0,1.0,5.0,4.0,2.0,1.09,0.0,1.0,1.0,0.0,2.0,3.0,2.0,2.0,28.0,5.0,11.0,9.0,8.0,12.0,11.0,9.0,8.0,2.0,0.0,1.0,3.0,snippet_223
303020,UpCloudLtd/upcloud-python-api,UpCloudLtd_upcloud-python-api/upcloud_api/cloud_manager/host_mixin.py,upcloud_api.cloud_manager.host_mixin.HostManager,"from upcloud_api.api import API
from upcloud_api.host import Host

class HostManager:
    """"""
    Functions for managing hosts. Intended to be used as a mixin for CloudManager.
    """"""
    api: API

    def get_hosts(self):
        """"""
        Returns a list of available hosts, along with basic statistics of them when available.
        """"""
        url = '/host'
        res = self.api.get_request(url)
        return [Host(**host) for host in res['hosts']['host']]

    def get_host(self, id: str) -> Host:
        """"""
        Returns detailed information about a specific host.
        """"""
        url = f'/host/{id}'
        res = self.api.get_request(url)
        return Host(**res['host'])

    def modify_host(self, host: str, description: str) -> Host:
        """"""
        Modifies description of a specific host.
        """"""
        url = f'/host/{host}'
        body = {'host': {'description': description}}
        res = self.api.patch_request(url, body)
        return Host(**res['host'])","
class HostManager:
    '''
    Functions for managing hosts. Intended to be used as a mixin for CloudManager.
    '''

    def get_hosts(self):
        '''
        Returns a list of available hosts, along with basic statistics of them when available.
        '''
        pass

    def get_hosts(self):
        '''
        Returns detailed information about a specific host.
        '''
        pass

    def modify_host(self, host: str, description: str) -> Host:
        '''
        Modifies description of a specific host.
        '''
        pass",4,4,7.0,0.0,4.0,3.0,1.0,0.8,0.0,2.0,1.0,1.0,3.0,0.0,3.0,3.0,31.0,4.0,15.0,11.0,11.0,12.0,15.0,11.0,11.0,1.0,0.0,0.0,3.0,snippet_224
303053,UpCloudLtd/upcloud-python-api,UpCloudLtd_upcloud-python-api/upcloud_api/upcloud_resource.py,upcloud_api.upcloud_resource.UpCloudResource,"class UpCloudResource:
    """"""
    Base class for all API resources.

    ATTRIBUTES is used to define serialization (see: to_dict)
    and defaults (see: __init__ and _reset).

    All UpCloudResources:
    - must define ATTRIBUTES accordingly with https://www.upcloud.com/api/ (doc)
    - must have `to_dict` for JSON serialization
    - must have `_reset` for initializing and refreshing the instance with updated data
    - must call `UpCloudResource.__init__` (that uses `_reset`)
    - optionally implement `sync` for refreshing the instance with new data from API
    """"""
    ATTRIBUTES = {}
    cloud_manager: 'CloudManager'

    def __init__(self, **kwargs) -> None:
        """"""
        Create a resource object from a dict.

        Set attributes from kwargs and any missing defaults from ATTRIBUTES.
        """"""
        self._reset(**kwargs)

    def _reset(self, **kwargs) -> None:
        """"""
        Reset after repopulating from API (or when initializing).
        """"""
        for key in kwargs:
            setattr(self, key, kwargs[key])
        for attr in self.ATTRIBUTES:
            if not hasattr(self, attr) and self.ATTRIBUTES[attr] is not None:
                setattr(self, attr, self.ATTRIBUTES[attr])

    def sync(self):
        """"""
        Sync the object from the API and use the internal resource._reset to
        update fields.
        """"""
        raise NotImplementedError

    def to_dict(self):
        """"""
        Return a dict that can be serialised to JSON and sent to UpCloud's API.
        """"""
        return {attr: getattr(self, attr) for attr in self.ATTRIBUTES if hasattr(self, attr)}","class UpCloudResource:
    '''
    Base class for all API resources.
    ATTRIBUTES is used to define serialization (see: to_dict)
    and defaults (see: __init__ and _reset).
    All UpCloudResources:
    - must define ATTRIBUTES accordingly with https://www.upcloud.com/api/ (doc)
    - must have `to_dict` for JSON serialization
    - must have `_reset` for initializing and refreshing the instance with updated data
    - must call `UpCloudResource.__init__` (that uses `_reset`)
    - optionally implement `sync` for refreshing the instance with new data from API
    '''

    def __init__(self, **kwargs) -> None:
        '''
        Create a resource object from a dict.
        Set attributes from kwargs and any missing defaults from ATTRIBUTES.
        '''
        pass

    def _reset(self, **kwargs) -> None:
        '''
        Reset after repopulating from API (or when initializing).
        '''
        pass

    def sync(self):
        '''
        Sync the object from the API and use the internal resource._reset to
        update fields.
        '''
        pass

    def to_dict(self):
        '''
        Return a dict that can be serialised to JSON and sent to UpCloud's API.
        '''
        pass",5,5,8.0,1.0,3.0,4.0,2.0,1.87,0.0,1.0,0.0,20.0,4.0,0.0,4.0,4.0,52.0,10.0,15.0,8.0,10.0,28.0,15.0,8.0,10.0,4.0,0.0,2.0,7.0,snippet_225
304377,edx/edx-lint,edx_edx-lint/edx_lint/pylint/annotations_check.py,edx_lint.pylint.annotations_check.AnnotationLines,"import re

class AnnotationLines:
    """"""
    AnnotationLines provides utility methods to work with a string in terms of
    lines.  As an example, it can convert a Call node into a list of its contents
    separated by line breaks.
    """"""
    _ANNOTATION_REGEX = re.compile('[\\s]*#[\\s]*\\.\\.[\\s]*(toggle)')

    def __init__(self, module_node):
        """"""
        Arguments:
            module_node: The visited module node.
        """"""
        module_as_binary = module_node.stream().read()
        file_encoding = module_node.file_encoding
        if file_encoding is None:
            file_encoding = 'UTF-8'
        module_as_string = module_as_binary.decode(file_encoding)
        self._list_of_string_lines = module_as_string.split('\n')

    def is_line_annotated(self, line_number):
        """"""
        Checks if the provided line number is annotated.
        """"""
        if line_number < 1 or self._line_count() < line_number:
            return False
        return bool(self._ANNOTATION_REGEX.match(self._get_line_contents(line_number)))

    def _line_count(self):
        """"""
        Gets the number of lines in the string.
        """"""
        return len(self._list_of_string_lines)

    def _get_line_contents(self, line_number):
        """"""
        Gets the line of text designated by the provided line number.
        """"""
        return self._list_of_string_lines[line_number - 1]","
class AnnotationLines:
    '''
    AnnotationLines provides utility methods to work with a string in terms of
    lines.  As an example, it can convert a Call node into a list of its contents
    separated by line breaks.
    '''

    def __init__(self, module_node):
        '''
        Arguments:
            module_node: The visited module node.
        '''
        pass

    def is_line_annotated(self, line_number):
        '''
        Checks if the provided line number is annotated.
        '''
        pass

    def _line_count(self):
        '''
        Gets the number of lines in the string.
        '''
        pass

    def _get_line_contents(self, line_number):
        '''
        Gets the line of text designated by the provided line number.
        '''
        pass",5,5,8.0,1.0,4.0,3.0,2.0,1.18,0.0,1.0,0.0,0.0,4.0,1.0,4.0,4.0,44.0,8.0,17.0,10.0,12.0,20.0,17.0,10.0,12.0,2.0,0.0,1.0,6.0,snippet_226
304970,threeML/astromodels,threeML_astromodels/astromodels/functions/template_model.py,astromodels.functions.template_model.TemplateFile,"import numpy as np
import h5py
from dataclasses import dataclass
import collections
from typing import Dict, List, Optional, Union

@dataclass
class TemplateFile:
    """"""
    simple container to read and write
    the data to an hdf5 file

    """"""
    name: str
    description: str
    grid: np.ndarray
    parameters: Dict[str, np.ndarray]
    parameter_order: List[str]
    energies: np.ndarray
    interpolation_degree: int
    spline_smoothing_factor: float

    def save(self, file_name: str):
        """"""
        serialize the contents to a file

        :param file_name:
        :type file_name: str
        :returns:

        """"""
        with h5py.File(file_name, 'w') as f:
            f.attrs['name'] = self.name
            f.attrs['description'] = self.description
            f.attrs['interpolation_degree'] = self.interpolation_degree
            f.attrs['spline_smoothing_factor'] = self.spline_smoothing_factor
            f.create_dataset('energies', data=self.energies, compression='gzip')
            f.create_dataset('grid', data=self.grid, compression='gzip')
            dt = h5py.special_dtype(vlen=str)
            po = np.array(self.parameter_order, dtype=dt)
            f.create_dataset('parameter_order', data=po)
            par_group = f.create_group('parameters')
            for k in self.parameter_order:
                par_group.create_dataset(k, data=self.parameters[k], compression='gzip')

    @classmethod
    def from_file(cls, file_name: str):
        """"""
        read contents from a file

        :param cls:
        :type cls:
        :param file_name:
        :type file_name: str
        :returns:

        """"""
        with h5py.File(file_name, 'r') as f:
            name = f.attrs['name']
            description = f.attrs['description']
            interpolation_degree = f.attrs['interpolation_degree']
            spline_smoothing_factor = f.attrs['spline_smoothing_factor']
            energies = f['energies'][()]
            parameter_order = f['parameter_order'][()]
            grid = f['grid'][()]
            parameters = collections.OrderedDict()
            for k in parameter_order:
                parameters[k] = f['parameters'][k][()]
        return cls(name=name, description=description, interpolation_degree=interpolation_degree, spline_smoothing_factor=spline_smoothing_factor, energies=energies, parameter_order=parameter_order, parameters=parameters, grid=grid)","@dataclass
class TemplateFile:
    '''
    simple container to read and write
    the data to an hdf5 file
    '''

    def save(self, file_name: str):
        '''
        serialize the contents to a file
        :param file_name:
        :type file_name: str
        :returns:
        '''
        pass
    @classmethod
    def from_file(cls, file_name: str):
        '''
        read contents from a file
        :param cls:
        :type cls:
        :param file_name:
        :type file_name: str
        :returns:
        '''
        pass",3,3,33.0,7.0,19.0,8.0,2.0,0.4,0.0,2.0,0.0,0.0,1.0,0.0,2.0,2.0,84.0,17.0,48.0,19.0,44.0,19.0,36.0,16.0,33.0,2.0,0.0,2.0,4.0,snippet_227
305558,NASA-AMMOS/AIT-Core,ait/core/pcap.py,core.pcap.PCapRolloverStream,"import datetime
from ait.core import log
import math
import calendar

class PCapRolloverStream:
    """"""
    Wraps a PCapStream to rollover to a new filename, based on packet
    times, file size, or number of packets.
    """"""

    def __init__(self, format, nbytes=None, npackets=None, nseconds=None, dryrun=False):
        """"""Creates a new :class:`PCapRolloverStream` with the given
        thresholds.

        A :class:`PCapRolloverStream` behaves like a
        :class:`PCapStream`, except that writing a new packet will
        cause the current file to be closed and a new file to be
        opened when one or more of thresholds (``nbytes``,
        ``npackets``, ``nseconds``) is exceeded.

        The new filename is determined by passing the ``format``
        string through :func:`PCapPacketHeader.timestamp.strftime()`
        for the first packet in the file.

        When segmenting based on time (``nseconds``), for file naming
        and interval calculation purposes ONLY, the timestamp of the
        first packet in the file is rounded down to nearest even
        multiple of the number of seconds.  This yields nice round
        number timestamps for filenames.  For example:

          PCapRolloverStream(format=""%Y%m%dT%H%M%S.pcap"", nseconds=3600)

        If the first packet written to a file has a time of 2017-11-23
        19:28:58, the file will be named:

            20171123T190000.pcap

        And a new file will be started when a packet is written with a
        timestamp that exceeds 2017-11-23 19:59:59.

        :param format:    Output filename in ``strftime(3)`` format
        :param nbytes:    Rollover after writing nbytes
        :param npackets:  Rollover after writing npackets
        :param nseconds:  Rollover after nseconds have elapsed between
                          the first and last packet timestamp in the file.
        :param dryrun:    Simulate file writes and output log messages.
        """"""
        self._dryrun = dryrun
        self._filename = None
        self._format = format
        self._startTime = None
        self._stream = None
        self._threshold = PCapFileStats(nbytes, npackets, nseconds)
        self._total = PCapFileStats(0, 0, 0)

    @property
    def rollover(self):
        """"""Indicates whether or not its time to rollover to a new file.""""""
        rollover = False
        if not rollover and self._threshold.nbytes is not None:
            rollover = self._total.nbytes >= self._threshold.nbytes
        if not rollover and self._threshold.npackets is not None:
            rollover = self._total.npackets >= self._threshold.npackets
        if not rollover and self._threshold.nseconds is not None:
            nseconds = math.ceil(self._total.nseconds)
            rollover = nseconds >= self._threshold.nseconds
        return rollover

    def write(self, bytes, header=None):
        """"""Writes packet ``bytes`` and the optional pcap packet ``header``.

        If the pcap packet ``header`` is not specified, one will be
        generated based on the number of packet ``bytes`` and current
        time.
        """"""
        if header is None:
            header = PCapPacketHeader(orig_len=len(bytes))
        if self._stream is None:
            if self._threshold.nseconds is not None:
                nseconds = self._threshold.nseconds
                remainder = int(math.floor(header.ts % nseconds))
                delta = datetime.timedelta(seconds=remainder)
                timestamp = header.timestamp - delta
            else:
                timestamp = header.timestamp
            self._filename = timestamp.strftime(self._format)
            self._startTime = calendar.timegm(timestamp.replace(microsecond=0).timetuple())
            if self._dryrun:
                self._stream = True
                self._total.nbytes += len(PCapGlobalHeader().pack())
            else:
                self._stream = open(self._filename, 'w')
                self._total.nbytes += len(self._stream.header.pack())
        if not self._dryrun:
            self._stream.write(bytes, header)
        self._total.nbytes += len(bytes) + len(header)
        self._total.npackets += 1
        self._total.nseconds = header.ts - self._startTime
        if self.rollover:
            self.close()
        return header.incl_len

    def close(self):
        """"""Closes this :class:``PCapStream`` by closing the underlying Python
        stream.""""""
        if self._stream:
            values = (self._total.nbytes, self._total.npackets, int(math.ceil(self._total.nseconds)), self._filename)
            if self._dryrun:
                msg = 'Would write {} bytes, {} packets, {} seconds to {}.'
            else:
                msg = 'Wrote {} bytes, {} packets, {} seconds to {}.'
                self._stream.close()
            log.info(msg.format(*values))
            self._filename = None
            self._startTime = None
            self._stream = None
            self._total = PCapFileStats(0, 0, 0)","
class PCapRolloverStream:
    '''
    Wraps a PCapStream to rollover to a new filename, based on packet
    times, file size, or number of packets.
    '''

    def __init__(self, format, nbytes=None, npackets=None, nseconds=None, dryrun=False):
        '''Creates a new :class:`PCapRolloverStream` with the given
        thresholds.
        A :class:`PCapRolloverStream` behaves like a
        :class:`PCapStream`, except that writing a new packet will
        cause the current file to be closed and a new file to be
        opened when one or more of thresholds (``nbytes``,
        ``npackets``, ``nseconds``) is exceeded.
        The new filename is determined by passing the ``format``
        string through :func:`PCapPacketHeader.timestamp.strftime()`
        for the first packet in the file.
        When segmenting based on time (``nseconds``), for file naming
        and interval calculation purposes ONLY, the timestamp of the
        first packet in the file is rounded down to nearest even
        multiple of the number of seconds.  This yields nice round
        number timestamps for filenames.  For example:
          PCapRolloverStream(format=""%Y%m%dT%H%M%S.pcap"", nseconds=3600)
        If the first packet written to a file has a time of 2017-11-23
        19:28:58, the file will be named:
            20171123T190000.pcap
        And a new file will be started when a packet is written with a
        timestamp that exceeds 2017-11-23 19:59:59.
        :param format:    Output filename in ``strftime(3)`` format
        :param nbytes:    Rollover after writing nbytes
        :param npackets:  Rollover after writing npackets
        :param nseconds:  Rollover after nseconds have elapsed between
                          the first and last packet timestamp in the file.
        :param dryrun:    Simulate file writes and output log messages.
        '''
        pass
    @property
    def rollover(self):
        '''Indicates whether or not its time to rollover to a new file.'''
        pass

    def write(self, bytes, header=None):
        '''Writes packet ``bytes`` and the optional pcap packet ``header``.
        If the pcap packet ``header`` is not specified, one will be
        generated based on the number of packet ``bytes`` and current
        time.
        '''
        pass

    def close(self):
        '''Closes this :class:``PCapStream`` by closing the underlying Python
        stream.'''
        pass",5,5,31.0,6.0,16.0,9.0,4.0,0.61,0.0,5.0,3.0,0.0,4.0,7.0,4.0,4.0,135.0,27.0,67.0,21.0,61.0,41.0,56.0,20.0,51.0,7.0,0.0,2.0,15.0,snippet_228
305605,NASA-AMMOS/AIT-Core,ait/core/tlm.py,core.tlm.PacketContext,"class PacketContext:
    """"""PacketContext

    A PacketContext provides a simple wrapper around a Packet so that
    field accesses of the form:

        packet.fieldname

    may also be specified as:

        packet[fieldname]

    This latter syntax allows a PacketContext to be used as a symbol
    table when evaluating PacketExpressions.
    """"""
    __slots__ = ['_packet']

    def __init__(self, packet):
        """"""Creates a new PacketContext for the given Packet.""""""
        self._packet = packet

    def __getitem__(self, name):
        """"""Returns packet[name]""""""
        result = None
        if self._packet._hasattr(name):
            result = self._packet._getattr(name)
        else:
            msg = ""Packet '%s' has no field '%s'""
            values = (self._packet._defn.name, name)
            raise KeyError(msg % values)
        return result","class PacketContext:
    '''PacketContext
    A PacketContext provides a simple wrapper around a Packet so that
    field accesses of the form:
        packet.fieldname
    may also be specified as:
        packet[fieldname]
    This latter syntax allows a PacketContext to be used as a symbol
    table when evaluating PacketExpressions.
    '''

    def __init__(self, packet):
        '''Creates a new PacketContext for the given Packet.'''
        pass

    def __getitem__(self, name):
        '''Returns packet[name]'''
        pass",3,3,8.0,1.0,6.0,1.0,2.0,0.85,0.0,1.0,0.0,0.0,2.0,1.0,2.0,2.0,34.0,10.0,13.0,8.0,10.0,11.0,12.0,8.0,9.0,2.0,0.0,1.0,3.0,snippet_229
305610,NASA-AMMOS/AIT-Core,ait/core/tlm.py,core.tlm.RawPacket,"class RawPacket:
    """"""RawPacket

    Wraps a packet such that:

        packet.raw.fieldname

    returns the value of fieldname as a raw value with no enumeration
    substitutions or DN to EU conversions applied.
    """"""
    __slots__ = ['_packet']

    def __init__(self, packet):
        """"""Creates a new RawPacket based on the given Packet.""""""
        self._packet = packet

    def __getattr__(self, fieldname):
        """"""Returns the value of the given packet fieldname as a raw
        value with no DN to EU conversion applied.
        """"""
        return self._packet._getattr(fieldname, raw=True)","class RawPacket:
    '''RawPacket
    Wraps a packet such that:
        packet.raw.fieldname
    returns the value of fieldname as a raw value with no enumeration
    substitutions or DN to EU conversions applied.
    '''

    def __init__(self, packet):
        '''Creates a new RawPacket based on the given Packet.'''
        pass

    def __getattr__(self, fieldname):
        '''Returns the value of the given packet fieldname as a raw
        value with no DN to EU conversion applied.
        '''
        pass",3,3,4.0,0.0,2.0,2.0,1.0,1.67,0.0,0.0,0.0,0.0,2.0,1.0,2.0,2.0,22.0,6.0,6.0,5.0,3.0,10.0,6.0,5.0,3.0,1.0,0.0,0.0,2.0,snippet_230
305613,NASA-AMMOS/AIT-Core,ait/core/tlm.py,core.tlm.WordArray,"class WordArray:
    """"""WordArrays are somewhat analogous to Python bytearrays, but
    currently much more limited in functionality.  They provide a
    readonly view of a bytearray addressable and iterable as a sequence
    of 16-bit words.  This is convenient for telemetry processing as
    packets are often more naturally addressable on word, as opposed to
    byte, boundaries.
    """"""
    __slots__ = ['_bytes']

    def __init__(self, bytes):
        """"""Creates a new wordarray from the given bytearray.

        The given bytearray should contain an even number of bytes.  If
        odd, the last byte is ignored.
        """"""
        self._bytes = bytes

    def __getitem__(self, key):
        """"""Returns the words in this wordarray at the given Python slice
        or word at the given integer index.""""""
        length = len(self)
        if isinstance(key, slice):
            return [self[n] for n in range(*key.indices(length))]
        elif isinstance(key, int):
            if key < 0:
                key += length
            if key >= length:
                msg = 'wordarray index (%d) is out of range [0 %d].'
                raise IndexError(msg % (key, length - 1))
            index = 2 * key
            return self._bytes[index] << 8 | self._bytes[index + 1]
        else:
            raise TypeError('wordarray indices must be integers.')

    def __len__(self):
        """"""Returns the number of words in this wordarray.""""""
        return len(self._bytes) / 2","class WordArray:
    '''WordArrays are somewhat analogous to Python bytearrays, but
    currently much more limited in functionality.  They provide a
    readonly view of a bytearray addressable and iterable as a sequence
    of 16-bit words.  This is convenient for telemetry processing as
    packets are often more naturally addressable on word, as opposed to
    byte, boundaries.
    '''

    def __init__(self, bytes):
        '''Creates a new wordarray from the given bytearray.
        The given bytearray should contain an even number of bytes.  If
        odd, the last byte is ignored.
        '''
        pass

    def __getitem__(self, key):
        '''Returns the words in this wordarray at the given Python slice
        or word at the given integer index.'''
        pass

    def __len__(self):
        '''Returns the number of words in this wordarray.'''
        pass",4,4,10.0,1.0,6.0,2.0,2.0,0.7,0.0,5.0,0.0,0.0,3.0,1.0,3.0,3.0,42.0,8.0,20.0,9.0,16.0,14.0,18.0,9.0,14.0,5.0,0.0,2.0,7.0,snippet_231
306020,ihmeuw/vivarium,ihmeuw_vivarium/src/vivarium/framework/lookup/interpolation.py,vivarium.framework.lookup.interpolation.Order0Interp,"import numpy as np
import pandas as pd
from collections.abc import Hashable, Sequence

class Order0Interp:
    """"""A callable that returns the result of order 0 interpolation over input data.

    Attributes
    ----------
    data
        The data from which to build the interpolation.
    value_columns
        Columns to be interpolated.
    extrapolate
        Whether or not to extrapolate beyond the edge of supplied bins.
    parameter_bins
        A dictionary where they keys are a tuple of the form
        (column name used in call, column name for left bin edge, column name for right bin edge)
        and the values are dictionaries of the form {""bins"": [ordered left edges of bins],
        ""max"": max right edge (used when extrapolation not allowed)}.

    """"""

    def __init__(self, data: pd.DataFrame, continuous_parameters: Sequence[Sequence[str]], value_columns: list[str], extrapolate: bool, validate: bool):
        """"""
        Parameters
        ----------
        data
            Data frame used to build interpolation.
        continuous_parameters
            Parameter columns. Should be of form (column name used in call,
            column name for left bin edge, column name for right bin edge)
            or column name. Assumes left bin edges are inclusive and
            right exclusive.
        value_columns
            Columns to be interpolated.
        extrapolate
            Whether or not to extrapolate beyond the edge of supplied bins.
        validate
            Whether or not to validate the data.
        """"""
        if validate:
            check_data_complete(data, continuous_parameters)
        self.data = data.copy()
        self.value_columns = value_columns
        self.extrapolate = extrapolate
        self.parameter_bins = {}
        for p in continuous_parameters:
            left_edge = self.data[p[1]].drop_duplicates().sort_values()
            max_right = self.data[p[2]].drop_duplicates().max()
            self.parameter_bins[tuple(p)] = {'bins': left_edge.reset_index(drop=True), 'max': max_right}

    def __call__(self, interpolants: pd.DataFrame) -> pd.DataFrame:
        """"""Find the bins for each parameter for each interpolant in interpolants
        and return the values from data there.

        Parameters
        ----------
        interpolants
            Data frame containing the parameters to interpolate..

        Returns
        -------
            A table with the interpolated values for the given interpolants.
        """"""
        interpolant_bins = pd.DataFrame(index=interpolants.index)
        merge_cols = []
        for cols, d in self.parameter_bins.items():
            bins = d['bins']
            max_right = d['max']
            merge_cols.append(cols[1])
            interpolant_col = interpolants[cols[0]]
            if not self.extrapolate and (interpolant_col.min() < bins[0] or interpolant_col.max() >= max_right):
                raise ValueError(f'Extrapolation outside of bins used to set up interpolation is only allowed when explicitly set in creation of Interpolation. Extrapolation is currently off for this interpolation, and parameter {cols[0]} includes data outside of original bins.')
            bin_indices = np.digitize(interpolant_col, bins.tolist())
            bin_indices[bin_indices > 0] -= 1
            interpolant_bins[cols[1]] = bins.loc[bin_indices].values
        index = interpolant_bins.index
        interp_vals = interpolant_bins.merge(self.data, how='left', on=merge_cols).set_index(index)
        return interp_vals[self.value_columns]","
class Order0Interp:
    '''A callable that returns the result of order 0 interpolation over input data.
    Attributes
    ----------
    data
        The data from which to build the interpolation.
    value_columns
        Columns to be interpolated.
    extrapolate
        Whether or not to extrapolate beyond the edge of supplied bins.
    parameter_bins
        A dictionary where they keys are a tuple of the form
        (column name used in call, column name for left bin edge, column name for right bin edge)
        and the values are dictionaries of the form {""bins"": [ordered left edges of bins],
        ""max"": max right edge (used when extrapolation not allowed)}.
    '''

    def __init__(self, data: pd.DataFrame, continuous_parameters: Sequence[Sequence[str]], value_columns: list[str], extrapolate: bool, validate: bool):
        '''
        Parameters
        ----------
        data
            Data frame used to build interpolation.
        continuous_parameters
            Parameter columns. Should be of form (column name used in call,
            column name for left bin edge, column name for right bin edge)
            or column name. Assumes left bin edges are inclusive and
            right exclusive.
        value_columns
            Columns to be interpolated.
        extrapolate
            Whether or not to extrapolate beyond the edge of supplied bins.
        validate
            Whether or not to validate the data.
        '''
        pass

    def __call__(self, interpolants: pd.DataFrame) -> pd.DataFrame:
        '''Find the bins for each parameter for each interpolant in interpolants
        and return the values from data there.
        Parameters
        ----------
        interpolants
            Data frame containing the parameters to interpolate..
        Returns
        -------
            A table with the interpolated values for the given interpolants.
        '''
        pass",3,3,43.0,5.0,23.0,16.0,3.0,0.98,0.0,6.0,0.0,0.0,2.0,4.0,2.0,2.0,106.0,13.0,47.0,26.0,37.0,46.0,28.0,19.0,25.0,3.0,0.0,2.0,6.0,snippet_232
306151,lago-project/lago,lago-project_lago/lago/templates.py,lago.templates.Template,"class Template:
    """"""
    Disk image template class

    Attributes:
        name (str): Name of this template
        _versions (dict(str:TemplateVersion)): versions for this template
    """"""

    def __init__(self, name, versions):
        """"""
        Args:
            name (str): Name of the template
            versions (dict(str:TemplateVersion)): dictionary with the
                version_name: :class:`TemplateVersion` pairs for this template
        """"""
        self.name = name
        self._versions = versions

    def get_version(self, ver_name=None):
        """"""
        Get the given version for this template, or the latest

        Args:
            ver_name (str or None): Version to retieve, None for the latest

        Returns:
            TemplateVersion: The version matching the given name or the latest
                one
        """"""
        if ver_name is None:
            return self.get_latest_version()
        return self._versions[ver_name]

    def get_latest_version(self):
        """"""
        Retrieves the latest version for this template, the latest being the
        one with the newest timestamp

        Returns:
            TemplateVersion
        """"""
        return max(self._versions.values(), key=lambda x: x.timestamp())","class Template:
    '''
    Disk image template class
    Attributes:
        name (str): Name of this template
        _versions (dict(str:TemplateVersion)): versions for this template
    '''

    def __init__(self, name, versions):
        '''
        Args:
            name (str): Name of the template
            versions (dict(str:TemplateVersion)): dictionary with the
                version_name: :class:`TemplateVersion` pairs for this template
        '''
        pass

    def get_version(self, ver_name=None):
        '''
        Get the given version for this template, or the latest
        Args:
            ver_name (str or None): Version to retieve, None for the latest
        Returns:
            TemplateVersion: The version matching the given name or the latest
                one
        '''
        pass

    def get_latest_version(self):
        '''
        Retrieves the latest version for this template, the latest being the
        one with the newest timestamp
        Returns:
            TemplateVersion
        '''
        pass",4,4,11.0,1.0,3.0,7.0,1.0,2.6,0.0,0.0,0.0,0.0,3.0,2.0,3.0,3.0,43.0,7.0,10.0,6.0,6.0,26.0,10.0,6.0,6.0,2.0,0.0,1.0,4.0,snippet_233
311054,geronimp/graftM,geronimp_graftM/graftm/decorator.py,graftm.decorator.Decorator,"import logging
from graftm.tree_decorator import TreeDecorator
from graftm.rerooter import Rerooter
from dendropy import Tree

class Decorator:
    """"""re-root a tree and decorate it using old taxonomy, for the graftM 
    decorate pipeline.

    ALL trees provided to this class must be in newick format. No other formats 
    are supported. If any taxonomic decoration is already in the tree to be 
    decorated by this class is WILL be overwritten. Of course, bootstrap values 
    will remain untouched.""""""

    def __init__(self, **kwargs):
        """"""
        Parameters
        ----------
        reference_tree_path: str
            Path to the file containing the reference tree, which is used to
            retroot the tree tree provided to tree
        tree_path: str
            Path to the file containing the tree to be re-rooted. This tree will
            be rerooted at the same position as the tree porovided to the 
            reference_tree
        """"""
        reference_tree_path = kwargs.pop('reference_tree_path', None)
        tree_path = kwargs.pop('tree_path')
        logging.debug('Importing old tree from file: %s' % tree_path)
        self.tree = Tree.get(path=tree_path, schema='newick')
        if reference_tree_path:
            logging.debug('Importing reference tree from file: %s' % reference_tree_path)
            self.reference_tree = Tree.get(path=reference_tree_path, schema='newick')
        else:
            self.reference_tree = reference_tree_path
        if len(kwargs) > 0:
            raise Exception('Unexpected arguments provided to Decorator class: %s' % kwargs)

    def _reroot(self):
        """"""Run the re-rooting algorithm in the Rerooter class.""""""
        rerooter = Rerooter()
        self.tree = rerooter.reroot_by_tree(self.reference_tree, self.tree)

    def main(self, taxonomy, output_tree, output_tax, no_unique_tax, decorate, seqinfo):
        """"""Decorate and if necessary, re-root the tree. If an old reference tree
        is provided it is assumed that re-rooting is desired

        Parameters
        ----------
        taxonomy: str
            Path to a GreenGenes formatted, or taxtastic formatted taxonomy 
            file, containing the taxonomy of all or some if the sequences used 
            to construct the tree. This taxonomy will be used by the 
            tree_decorator.TreeDecorator class to decorate self.tree
        output_tree: str
            Path to file to which the decorated tree will be written to.
        output_tax: str
            Path to file to which the decorated taxonomy will be written to.
        seqinfo: str
            path to taxtastic seqinfo file to be used with taxonomy to annotate
            the tree.""""""
        if self.reference_tree:
            self._reroot()
        td = TreeDecorator(self.tree, taxonomy, seqinfo)
        if decorate:
            td.decorate(output_tree, output_tax, no_unique_tax)
        else:
            logging.info('Writing tree to file: %s' % output_tree)
            self.tree.write(output_tree, format='newick')","
class Decorator:
    '''re-root a tree and decorate it using old taxonomy, for the graftM 
    decorate pipeline.
    ALL trees provided to this class must be in newick format. No other formats 
    are supported. If any taxonomic decoration is already in the tree to be 
    decorated by this class is WILL be overwritten. Of course, bootstrap values 
        will remain untouched.'''

    def __init__(self, **kwargs):
        '''
        Parameters
        ----------
        reference_tree_path: str
            Path to the file containing the reference tree, which is used to
            retroot the tree tree provided to tree
        tree_path: str
            Path to the file containing the tree to be re-rooted. This tree will
            be rerooted at the same position as the tree porovided to the 
            reference_tree
        '''
        pass

    def _reroot(self):
        '''Run the re-rooting algorithm in the Rerooter class.'''
        pass

    def main(self, taxonomy, output_tree, output_tax, no_unique_tax, decorate, seqinfo):
        '''Decorate and if necessary, re-root the tree. If an old reference tree
        is provided it is assumed that re-rooting is desired
        Parameters
        ----------
        taxonomy: str
            Path to a GreenGenes formatted, or taxtastic formatted taxonomy 
            file, containing the taxonomy of all or some if the sequences used 
            to construct the tree. This taxonomy will be used by the 
            tree_decorator.TreeDecorator class to decorate self.tree
        output_tree: str
            Path to file to which the decorated tree will be written to.
        output_tax: str
            Path to file to which the decorated taxonomy will be written to.
        seqinfo: str
            path to taxtastic seqinfo file to be used with taxonomy to annotate
        the tree.'''
        pass",4,4,22.0,1.0,12.0,10.0,2.0,1.0,0.0,3.0,2.0,0.0,3.0,2.0,3.0,3.0,78.0,6.0,36.0,11.0,31.0,36.0,23.0,10.0,19.0,3.0,0.0,1.0,7.0,snippet_234
311056,geronimp/graftM,geronimp_graftM/graftm/deduplicator.py,graftm.deduplicator.Deduplicator,"class Deduplicator:
    """"""Deduplicates sequences""""""

    def deduplicate(self, aligned_sequence_objects):
        """"""Sort the given aligned_sequence objects into an array of arrays,
        where input sequences are grouped iff they have the same sequence

        Parameters
        ----------
        aligned_sequence_objects: array of Sequence objects
            input sequences

        Returns
        -------
        Array of arrays of Sequence objects""""""
        sequence_to_groups = {}
        for s in aligned_sequence_objects:
            try:
                sequence_to_groups[s.seq].append(s)
            except KeyError:
                sequence_to_groups[s.seq] = [s]
        return list(sequence_to_groups.values())

    def lca_taxonomy(self, deduplicated_sequences, taxonomy_hash):
        """"""Given a set of deduplicated sequences and a taxonomy hash,
        return the respective LCAs of taxonomy

        Parameters
        ----------
        deduplicated_sequences: Array of arrays of Sequence objects
            as output from deduplicate()
        taxonomy_hash: dictionary 
            of sequence names to taxonomy array (i.e. array of str)

        Returns
        -------
        Array of taxonomy LCAs""""""
        to_return = []
        for dup_group in deduplicated_sequences:
            lca = taxonomy_hash[dup_group[0].name]
            for s in dup_group[1:]:
                for i, tax in enumerate(taxonomy_hash[s.name]):
                    if i >= len(lca) or tax != lca[i]:
                        lca = lca[:i]
                        break
                if len(lca) > len(taxonomy_hash[s.name]):
                    lca = lca[:len(taxonomy_hash[s.name])]
            to_return.append(lca)
        return to_return","class Deduplicator:
    '''Deduplicates sequences'''

    def deduplicate(self, aligned_sequence_objects):
        '''Sort the given aligned_sequence objects into an array of arrays,
        where input sequences are grouped iff they have the same sequence
        Parameters
        ----------
        aligned_sequence_objects: array of Sequence objects
            input sequences
        Returns
        -------
        Array of arrays of Sequence objects'''
        pass

    def lca_taxonomy(self, deduplicated_sequences, taxonomy_hash):
        '''Given a set of deduplicated sequences and a taxonomy hash,
        return the respective LCAs of taxonomy
        Parameters
        ----------
        deduplicated_sequences: Array of arrays of Sequence objects
            as output from deduplicate()
        taxonomy_hash: dictionary 
            of sequence names to taxonomy array (i.e. array of str)
        Returns
        -------
        Array of taxonomy LCAs'''
        pass",3,3,24.0,3.0,11.0,10.0,5.0,0.95,0.0,3.0,0.0,0.0,2.0,0.0,2.0,2.0,51.0,8.0,22.0,10.0,19.0,21.0,22.0,10.0,19.0,6.0,0.0,4.0,9.0,snippet_235
311087,geronimp/graftM,geronimp_graftM/graftm/search_table.py,graftm.search_table.SearchTableWriter,"import logging
from graftm.sequence_search_results import SequenceSearchResult

class SearchTableWriter:
    """"""
    Class for writing the search output OTU table. Basically a summary
    of hits to the HMM/Diamond searched in the following format:

             #ID    Metagenome_1    Metagenome_2    ...
            HMM1    50              6
            HMM2    195             41
            HMM3    2               20120
            ...

    You just need to specify a series of SequenceSearchResult objects, and an
    output path.
    """"""

    def _interpret_hits(self, results_list, base_list):
        """"""Sort reads that hit multiple HMMs to the databases to which they had
        the highest bit score. Return a dictionary containing HMMs as keys, and
        number of hits as the values.

        This function is set up so that the read names could easily be returned
        instead of numbers, for future development of GraftM

        Parameters
        ----------
        results_list: list
            Iterable if SequenceSearchResult objects. e.g.
                [SequenceSearchResult_1, SequenceSearchResult_2, ...]

        base_list: list
            Iterable of the basenames for each sequence file provided to graftM
            e.g.
                [sample_1, sample_2, ...]

        Returns
        -------
        dictionary:
            Contains samples as entries. The value for each sample is another
            dictionary with HMM as the key, and number of hits as values:
                {""sample_1"":{HMM_1: 12
                             HMM_2: 35
                             HMM_3: 1258
                             ...}
                 ...
                }

        """"""
        logging.debug('Sorting reads into HMMs by bit score')
        run_results = {}
        for base, results in zip(base_list, results_list):
            search_results = {}
            for search in results():
                search_list = list(search.each([SequenceSearchResult.QUERY_ID_FIELD, SequenceSearchResult.ALIGNMENT_BIT_SCORE, SequenceSearchResult.HMM_NAME_FIELD]))
                for hit in search_list:
                    if hit[0] in search_results:
                        if float(hit[1]) > search_results[hit[0]][0]:
                            search_results[hit[0]] = [float(hit[1]), hit[2]]
                    else:
                        search_results[hit[0]] = [float(hit[1]), hit[2]]
            run_results[base] = search_results
        db_count = {}
        for run in run_results.keys():
            run_count = {}
            for entry in list(run_results[run].values()):
                key = entry[1]
                if key in run_count:
                    run_count[key] += 1
                else:
                    run_count[key] = 1
            db_count[run] = run_count
        return db_count

    def _write_results(self, db_count, output_path):
        """"""Write the table to the output_path directory

        db_count: dict
            Contains samples as entries. The value for each sample is another
            dictionary with HMM as the key, and number of hits as values:
                {""sample_1"":{HMM_1: 12
                             HMM_2: 35
                             HMM_3: 1258
                             ...}
                 ...
                }

        output_path: str
            Path to output file to which the resultant output file will be
            written to.
        """"""
        logging.debug('Writing search otu table to file: %s' % output_path)
        output_dict = {}
        for idx, value_dict in enumerate(db_count.values()):
            for database, count in value_dict.items():
                if database in output_dict:
                    output_dict[database].append(str(count))
                else:
                    output_dict[database] = ['0'] * idx + [str(count)]
            for key, item in output_dict.items():
                if len(item) == idx:
                    output_dict[key].append('0')
        with open(output_path, 'w') as out:
            out.write('\t'.join(['#ID'] + list(db_count.keys())) + '\n')
            for key, item in output_dict.items():
                out.write('%s\t%s' % (key, '\t'.join(item)) + '\n')

    def build_search_otu_table(self, search_results_list, base_list, output_path):
        """"""
        Build an OTU from SequenceSearchResult objects

        Parameters
        ----------
        search_results_list: list
            Iterable if SequenceSearchResult objects. e.g.
                [SequenceSearchResult_1, SequenceSearchResult_2, ...]
        base_list: list
            Iterable of the basenames for each sequence file provided to graftM
            e.g.
                [sample_1, sample_2, ...]
        output_path: str
            Path to output file to which the resultant output file will be
            written to.
        """"""
        db_count = self._interpret_hits(search_results_list, base_list)
        self._write_results(db_count, output_path)","
class SearchTableWriter:
    '''
    Class for writing the search output OTU table. Basically a summary
    of hits to the HMM/Diamond searched in the following format:
             #ID    Metagenome_1    Metagenome_2    ...
            HMM1    50              6
            HMM2    195             41
            HMM3    2               20120
            ...
    You just need to specify a series of SequenceSearchResult objects, and an
    output path.
    '''

    def _interpret_hits(self, results_list, base_list):
        '''Sort reads that hit multiple HMMs to the databases to which they had
        the highest bit score. Return a dictionary containing HMMs as keys, and
        number of hits as the values.
        This function is set up so that the read names could easily be returned
        instead of numbers, for future development of GraftM
        Parameters
        ----------
        results_list: list
            Iterable if SequenceSearchResult objects. e.g.
                [SequenceSearchResult_1, SequenceSearchResult_2, ...]
        base_list: list
            Iterable of the basenames for each sequence file provided to graftM
            e.g.
                [sample_1, sample_2, ...]
        Returns
        -------
        dictionary:
            Contains samples as entries. The value for each sample is another
            dictionary with HMM as the key, and number of hits as values:
                {""sample_1"":{HMM_1: 12
                             HMM_2: 35
                             HMM_3: 1258
                             ...}
                 ...
                }
        '''
        pass

    def _write_results(self, db_count, output_path):
        '''Write the table to the output_path directory
        db_count: dict
            Contains samples as entries. The value for each sample is another
            dictionary with HMM as the key, and number of hits as values:
                {""sample_1"":{HMM_1: 12
                             HMM_2: 35
                             HMM_3: 1258
                             ...}
                 ...
                }
        output_path: str
            Path to output file to which the resultant output file will be
            written to.
        '''
        pass

    def build_search_otu_table(self, search_results_list, base_list, output_path):
        '''
        Build an OTU from SequenceSearchResult objects
        Parameters
        ----------
        search_results_list: list
            Iterable if SequenceSearchResult objects. e.g.
                [SequenceSearchResult_1, SequenceSearchResult_2, ...]
        base_list: list
            Iterable of the basenames for each sequence file provided to graftM
            e.g.
                [sample_1, sample_2, ...]
        output_path: str
            Path to output file to which the resultant output file will be
            written to.
        '''
        pass",4,4,42.0,6.0,16.0,20.0,6.0,1.44,0.0,6.0,1.0,0.0,3.0,0.0,3.0,3.0,144.0,24.0,50.0,21.0,46.0,72.0,42.0,20.0,38.0,9.0,0.0,5.0,17.0,snippet_236
311100,geronimp/graftM,geronimp_graftM/graftm/tree_decorator.py,graftm.tree_decorator.TreeDecorator,"from graftm.greengenes_taxonomy import GreenGenesTaxonomy, MalformedGreenGenesTaxonomyException
from graftm.getaxnseq import Getaxnseq
import logging
from graftm.taxonomy_cleaner import TaxonomyCleaner

class TreeDecorator:
    """"""
    A class that conservatively decorates trees with taxonomy, or any other
    hierarchical annotation. If all tips descending from a node within the
    provided tree have consistent taxonomy, it will be decorated with that
    taxonomy (or annotation of any type).
    """"""

    def __init__(self, tree, taxonomy, seqinfo=None):
        """"""
        Parameters
        ----------
        tree        : dendropy.Tree

            dendropy.Tree object
        taxonomy    : string
            Path to a file containing taxonomy information about the tree,
            either in Greengenes or taxtastic format (seqinfo file must also
            be provided if taxonomy is in taxtastic format).
        seqinfo     : string
            Path to a seqinfo file. This is a .csv file with the first column
            denoting the sequence name, and the second column, its most resolved
            taxonomic rank.
        """"""
        self.encountered_nodes = {}
        self.encountered_taxonomies = set()
        self.tree = tree
        logging.info('Reading in taxonomy')
        if seqinfo:
            logging.info('Importing taxtastic taxonomy from files: %s and %s' % (taxonomy, seqinfo))
            gtns = Getaxnseq()
            self.taxonomy = gtns.read_taxtastic_taxonomy_and_seqinfo(open(taxonomy), open(seqinfo))
        else:
            try:
                logging.info('Reading Greengenes style taxonomy')
                self.taxonomy = GreenGenesTaxonomy.read_file(taxonomy).taxonomy
            except MalformedGreenGenesTaxonomyException:
                raise Exception('Failed to read taxonomy as a Greengenes                                  formatted file. Was a taxtastic style                                  taxonomy provided with no seqinfo file?')

    def _write_consensus_strings(self, output):
        """"""
        Writes the taxonomy of each leaf to a file. If the leaf has no
        taxonomy, a taxonomy string will be created using the annotations
        provided to the ancestor nodes of that leaf (meaning, it will be
        decorated).

        Parameters
        ----------
        output    : string
            File to which the taxonomy strings for each leaf in the tree will
            be written in Greengenes format, e.g.
                637960147    mcrA; Euryarchaeota_mcrA; Methanomicrobia
                637699780    mcrA; Euryarchaeota_mcrA; Methanomicrobia
        """"""
        logging.info('Writing decorated taxonomy to file: %s' % output)
        with open(output, 'w') as out:
            for tip in self.tree.leaf_nodes():
                tax_name = tip.taxon.label.replace(' ', '_')
                if tip.taxon.label in self.taxonomy:
                    tax_string = '; '.join(self.taxonomy[tax_name])
                else:
                    ancestor_list = []
                    for ancestor in tip.ancestor_iter():
                        if ancestor.label:
                            split_node_name = ancestor.label.split(':')
                            if len(split_node_name) == 2:
                                ancestor_list += list(reversed(split_node_name[1].split('; ')))
                            elif len(split_node_name) == 1:
                                try:
                                    float(split_node_name[0])
                                except ValueError:
                                    ancestor_list += list(reversed(split_node_name[0].split('; ')))
                            else:
                                raise Exception('Malformed node name: %s' % ancestor.label)
                    tax_list = list(reversed(ancestor_list))
                    if len(tax_list) < 1:
                        logging.warning('No taxonomy found for species %s!' % tax_name)
                        tax_string = 'Unknown'
                    else:
                        tax_string = '; '.join(tax_list)
                output_line = '%s\t%s\n' % (tax_name, tax_string)
                out.write(output_line)

    def _rename(self, node, name):
        """"""
        Rename an internal node of the tree. If an annotation is already
        present, append the new annotation to the end of it. If a bootstrap
        value is present, add annotations are added after a "":"" as per standard
        newick format.

        Parameters
        ----------
        node: dendropy.Node
            dendropy.Node object
        name    : string
            Annotation to rename the node with.
        """"""
        if node.label:
            try:
                float(node.label)
                new_label = '%s:%s' % (node.label, name)
            except ValueError:
                new_label = '%s; %s' % (node.label, name)
            node.label = new_label
        else:
            node.label = name

    def decorate(self, output_tree, output_tax, unique_names):
        """"""
        Decorate a tree with taxonomy. This code does not allow inconsistent
        taxonomy within a clade. If one sequence in a clade has a different
        annotation to the rest, it will split the clade. Paraphyletic group
        names are distinguished if unique_names = True using a simple tally of
        each group (see unique_names below).

        Parameters
        ----------
        output_tree        : string
            File to which the decorated tree will be written.
        output_tax         : string
            File to which the taxonomy strings for each tip in the tree will be
            written.
        unique_names       : boolean
            True indicating that a unique number will be appended to the end of
            a taxonomic rank if it is found more than once in the tree
            (i.e. it is paraphyletic in the tree). If false, multiple clades
            may be assigned with the same name.
        """"""
        logging.info('Decorating tree')
        encountered_taxonomies = {}
        tc = TaxonomyCleaner()
        for node in self.tree.preorder_internal_node_iter(exclude_seed_node=True):
            max_tax_string_length = 0
            for tip in node.leaf_nodes():
                tip_label = tip.taxon.label.replace(' ', '_')
                if tip_label in self.taxonomy:
                    tax_string_length = len(self.taxonomy[tip.taxon.label.replace(' ', '_')])
                    if tax_string_length > max_tax_string_length:
                        max_tax_string_length = tax_string_length
            logging.debug('Number of ranks found for node: %i' % max_tax_string_length)
            tax_string_array = []
            for rank in range(max_tax_string_length):
                rank_tax = []
                for tip in node.leaf_nodes():
                    tip_label = tip.taxon.label.replace(' ', '_')
                    if tip_label in self.taxonomy:
                        tip_tax = self.taxonomy[tip_label]
                        if len(tip_tax) > rank:
                            tip_rank = tip_tax[rank]
                            if tip_rank not in rank_tax:
                                rank_tax.append(tip_rank)
                consistent_taxonomy = len(rank_tax) == 1
                if consistent_taxonomy:
                    tax = rank_tax.pop()
                    logging.debug('Consistent taxonomy found for node: %s' % tax)
                    if tax not in tc.meaningless_taxonomic_names:
                        if unique_names:
                            if tax in encountered_taxonomies:
                                encountered_taxonomies[tax] += 0
                                tax = '%s_%i' % (tax, encountered_taxonomies[tax])
                            else:
                                encountered_taxonomies[tax] = 0
                        tax_string_array.append(tax)
            if any(tax_string_array):
                index = 0
                for anc in node.ancestor_iter():
                    try:
                        index += anc.tax
                    except:
                        continue
                tax_string_array = tax_string_array[index:]
                if any(tax_string_array):
                    self._rename(node, '; '.join(tax_string_array))
                node.tax = len(tax_string_array)
        logging.info('Writing decorated tree to file: %s' % output_tree)
        if output_tree:
            self.tree.write(path=output_tree, schema='newick')
        if output_tax:
            self._write_consensus_strings(output_tax)","
class TreeDecorator:
    '''
    A class that conservatively decorates trees with taxonomy, or any other
    hierarchical annotation. If all tips descending from a node within the
    provided tree have consistent taxonomy, it will be decorated with that
    taxonomy (or annotation of any type).
    '''

    def __init__(self, tree, taxonomy, seqinfo=None):
        '''
        Parameters
        ----------
        tree        : dendropy.Tree
            dendropy.Tree object
        taxonomy    : string
            Path to a file containing taxonomy information about the tree,
            either in Greengenes or taxtastic format (seqinfo file must also
            be provided if taxonomy is in taxtastic format).
        seqinfo     : string
            Path to a seqinfo file. This is a .csv file with the first column
            denoting the sequence name, and the second column, its most resolved
            taxonomic rank.
        '''
        pass

    def _write_consensus_strings(self, output):
        '''
        Writes the taxonomy of each leaf to a file. If the leaf has no
        taxonomy, a taxonomy string will be created using the annotations
        provided to the ancestor nodes of that leaf (meaning, it will be
        decorated).
        Parameters
        ----------
        output    : string
            File to which the taxonomy strings for each leaf in the tree will
            be written in Greengenes format, e.g.
                637960147    mcrA; Euryarchaeota_mcrA; Methanomicrobia
                637699780    mcrA; Euryarchaeota_mcrA; Methanomicrobia
        '''
        pass

    def _rename(self, node, name):
        '''
        Rename an internal node of the tree. If an annotation is already
        present, append the new annotation to the end of it. If a bootstrap
        value is present, add annotations are added after a "":"" as per standard
        newick format.
        Parameters
        ----------
        node: dendropy.Node
            dendropy.Node object
        name    : string
            Annotation to rename the node with.
        '''
        pass

    def decorate(self, output_tree, output_tax, unique_names):
        '''
        Decorate a tree with taxonomy. This code does not allow inconsistent
        taxonomy within a clade. If one sequence in a clade has a different
        annotation to the rest, it will split the clade. Paraphyletic group
        names are distinguished if unique_names = True using a simple tally of
        each group (see unique_names below).
        Parameters
        ----------
        output_tree        : string
            File to which the decorated tree will be written.
        output_tax         : string
            File to which the taxonomy strings for each tip in the tree will be
            written.
        unique_names       : boolean
            True indicating that a unique number will be appended to the end of
            a taxonomic rank if it is found more than once in the tree
            (i.e. it is paraphyletic in the tree). If false, multiple clades
            may be assigned with the same name.
        '''
        pass",5,5,48.0,5.0,28.0,15.0,9.0,0.57,0.0,11.0,4.0,0.0,4.0,4.0,4.0,4.0,204.0,25.0,114.0,36.0,109.0,65.0,100.0,35.0,95.0,20.0,0.0,7.0,35.0,snippet_237
311641,abhishek-ram/pyas2-lib,abhishek-ram_pyas2-lib/pyas2lib/as2.py,pyas2lib.as2.Organization,"from pyas2lib.constants import AS2_VERSION, ASYNCHRONOUS_MDN, DIGEST_ALGORITHMS, EDIINT_FEATURES, ENCRYPTION_ALGORITHMS, KEY_ENCRYPTION_ALGORITHMS, MDN_CONFIRM_TEXT, MDN_FAILED_TEXT, MDN_MODES, SIGNATUR_ALGORITHMS, SYNCHRONOUS_MDN
from pyas2lib.utils import canonicalize, extract_first_part, make_mime_boundary, mime_to_bytes, pem_to_der, quote_as2name, split_pem, unquote_as2name, verify_certificate_chain
from pyas2lib.exceptions import AS2Exception, DuplicateDocument, ImproperlyConfigured, InsufficientSecurityError, IntegrityError, MDNNotFound, PartnerNotFound
from oscrypto import asymmetric
from dataclasses import dataclass

@dataclass
class Organization:
    """"""
    Class represents an AS2 organization and defines the certificates and
    settings to be used when sending and receiving messages.

    :param as2_name: The unique AS2 name for this organization

    :param sign_key: A byte string of the pkcs12 encoded key pair
        used for signing outbound messages and MDNs.

    :param sign_key_pass: The password for decrypting the `sign_key`

    :param decrypt_key:  A byte string of the pkcs12 encoded key pair
        used for decrypting inbound messages.

    :param decrypt_key_pass: The password for decrypting the `decrypt_key`

    :param mdn_url: The URL where the receiver is expected to post
        asynchronous MDNs.

    :param domain:
        Optional domain if given provides the portion of the message id
        after the '@'.  It defaults to the locally defined hostname.
    """"""
    as2_name: str
    sign_key: bytes = None
    sign_key_pass: str = None
    decrypt_key: bytes = None
    decrypt_key_pass: str = None
    mdn_url: str = None
    mdn_confirm_text: str = MDN_CONFIRM_TEXT
    domain: str = None

    def __post_init__(self):
        """"""Run the post initialisation checks for this class.""""""
        if self.sign_key:
            self.sign_key = self.load_key(self.sign_key, self.sign_key_pass)
        if self.decrypt_key:
            self.decrypt_key = self.load_key(self.decrypt_key, self.decrypt_key_pass)

    @staticmethod
    def load_key(key_str: bytes, key_pass: str):
        """"""Function to load password protected key file in p12 or pem format.""""""
        try:
            key, cert, _ = asymmetric.load_pkcs12(key_str, key_pass)
        except ValueError as e:
            if e.args[0] == 'Password provided is invalid':
                raise AS2Exception('Password not valid for Private Key.') from e
            key, cert = (None, None)
            for kc in split_pem(key_str):
                try:
                    cert = asymmetric.load_certificate(kc)
                except (ValueError, TypeError) as e:
                    try:
                        key = asymmetric.load_private_key(kc, key_pass)
                    except OSError:
                        raise AS2Exception('Invalid Private Key or password is not correct.') from e
        if not key or not cert:
            raise AS2Exception('Invalid Private key file or Public key not included.')
        return (key, cert)",,3,3,18.0,3.0,13.0,3.0,5.0,0.63,0.0,6.0,1.0,0.0,1.0,0.0,2.0,2.0,72.0,15.0,35.0,14.0,31.0,22.0,32.0,12.0,29.0,7.0,0.0,4.0,10.0,snippet_238
311642,abhishek-ram/pyas2-lib,abhishek-ram_pyas2-lib/pyas2lib/as2.py,pyas2lib.as2.Partner,"from oscrypto import asymmetric
from pyas2lib.exceptions import AS2Exception, DuplicateDocument, ImproperlyConfigured, InsufficientSecurityError, IntegrityError, MDNNotFound, PartnerNotFound
from pyas2lib.utils import canonicalize, extract_first_part, make_mime_boundary, mime_to_bytes, pem_to_der, quote_as2name, split_pem, unquote_as2name, verify_certificate_chain
from pyas2lib.constants import AS2_VERSION, ASYNCHRONOUS_MDN, DIGEST_ALGORITHMS, EDIINT_FEATURES, ENCRYPTION_ALGORITHMS, KEY_ENCRYPTION_ALGORITHMS, MDN_CONFIRM_TEXT, MDN_FAILED_TEXT, MDN_MODES, SIGNATUR_ALGORITHMS, SYNCHRONOUS_MDN
from dataclasses import dataclass

@dataclass
class Partner:
    """"""
    Class represents an AS2 partner and defines the certificates and
    settings to be used when sending and receiving messages.

    :param as2_name: The unique AS2 name for this partner.

    :param verify_cert: A byte string of the certificate to be used for
        verifying signatures of inbound messages and MDNs.

    :param verify_cert_ca: A byte string of the ca certificate if any of
        the verification cert

    :param encrypt_cert: A byte string of the certificate to be used for
        encrypting outbound message.

    :param encrypt_cert_ca: A byte string of the ca certificate if any of
        the encryption cert

    :param validate_certs: Set this flag to `False` to disable validations of
        the encryption and verification certificates. (default `True`)

    :param compress: Set this flag to `True` to compress outgoing
        messages. (default `False`)

    :param sign: Set this flag to `True` to sign outgoing
        messages. (default `False`)

    :param digest_alg: The digest algorithm to be used for generating the
        signature. (default ""sha256"")

    :param encrypt: Set this flag to `True` to encrypt outgoing
        messages. (default `False`)

    :param enc_alg:
        The encryption algorithm to be used. (default `""tripledes_192_cbc""`)

    :param mdn_mode: The mode to be used for receiving the MDN.
        Set to `None` for no MDN, `'SYNC'` for synchronous and `'ASYNC'`
        for asynchronous. (default `None`)

    :param mdn_digest_alg: The digest algorithm to be used by the receiver
        for signing the MDN. Use `None` for unsigned MDN. (default `None`)

    :param mdn_confirm_text: The text to be used in the MDN for successfully
        processed messages received from this partner.

    :param canonicalize_as_binary: force binary canonicalization for this partner

    :param sign_alg: The signing algorithm to be used for generating the
        signature. (default `rsassa_pkcs1v15`)

    :param key_enc_alg: The key encryption algorithm to be used.
        (default `rsaes_pkcs1v15`)

    """"""
    as2_name: str
    verify_cert: bytes = None
    verify_cert_ca: bytes = None
    encrypt_cert: bytes = None
    encrypt_cert_ca: bytes = None
    validate_certs: bool = True
    compress: bool = False
    encrypt: bool = False
    enc_alg: str = 'tripledes_192_cbc'
    sign: bool = False
    digest_alg: str = 'sha256'
    mdn_mode: str = None
    mdn_digest_alg: str = None
    mdn_confirm_text: str = MDN_CONFIRM_TEXT
    ignore_self_signed: bool = True
    canonicalize_as_binary: bool = False
    sign_alg: str = 'rsassa_pkcs1v15'
    key_enc_alg: str = 'rsaes_pkcs1v15'

    def __post_init__(self):
        """"""Run the post initialisation checks for this class.""""""
        if self.digest_alg and self.digest_alg not in DIGEST_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported Digest Algorithm {self.digest_alg}, must be one of {DIGEST_ALGORITHMS}')
        if self.enc_alg and self.enc_alg not in ENCRYPTION_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported Encryption Algorithm {self.enc_alg}, must be one of {ENCRYPTION_ALGORITHMS}')
        if self.mdn_mode and self.mdn_mode not in MDN_MODES:
            raise ImproperlyConfigured(f'Unsupported MDN Mode {self.mdn_mode}, must be one of {MDN_MODES}')
        if self.mdn_digest_alg and self.mdn_digest_alg not in DIGEST_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported MDN Digest Algorithm {self.mdn_digest_alg}, must be one of {DIGEST_ALGORITHMS}')
        if self.sign_alg and self.sign_alg not in SIGNATUR_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported Signature Algorithm {self.sign_alg}, must be one of {SIGNATUR_ALGORITHMS}')
        if self.key_enc_alg and self.key_enc_alg not in KEY_ENCRYPTION_ALGORITHMS:
            raise ImproperlyConfigured(f'Unsupported Key Encryption Algorithm {self.key_enc_alg}, must be one of {KEY_ENCRYPTION_ALGORITHMS}')

    def load_verify_cert(self):
        """"""Load the verification certificate of the partner and returned the parsed cert.""""""
        if self.validate_certs:
            cert = pem_to_der(self.verify_cert, return_multiple=False)
            if self.verify_cert_ca:
                trust_roots = pem_to_der(self.verify_cert_ca)
            else:
                trust_roots = []
            verify_certificate_chain(cert, trust_roots, ignore_self_signed=self.ignore_self_signed)
        return asymmetric.load_certificate(self.verify_cert)

    def load_encrypt_cert(self):
        """"""Load the encryption certificate of the partner and returned the parsed cert.""""""
        if self.validate_certs:
            cert = pem_to_der(self.encrypt_cert, return_multiple=False)
            if self.encrypt_cert_ca:
                trust_roots = pem_to_der(self.encrypt_cert_ca)
            else:
                trust_roots = []
            verify_certificate_chain(cert, trust_roots, ignore_self_signed=self.ignore_self_signed)
        return asymmetric.load_certificate(self.encrypt_cert)","@dataclass
class Partner:
    '''
    Class represents an AS2 partner and defines the certificates and
    settings to be used when sending and receiving messages.
    :param as2_name: The unique AS2 name for this partner.
    :param verify_cert: A byte string of the certificate to be used for
        verifying signatures of inbound messages and MDNs.
    :param verify_cert_ca: A byte string of the ca certificate if any of
        the verification cert
    :param encrypt_cert: A byte string of the certificate to be used for
        encrypting outbound message.
    :param encrypt_cert_ca: A byte string of the ca certificate if any of
        the encryption cert
    :param validate_certs: Set this flag to `False` to disable validations of
        the encryption and verification certificates. (default `True`)
    :param compress: Set this flag to `True` to compress outgoing
        messages. (default `False`)
    :param sign: Set this flag to `True` to sign outgoing
        messages. (default `False`)
    :param digest_alg: The digest algorithm to be used for generating the
        signature. (default ""sha256"")
    :param encrypt: Set this flag to `True` to encrypt outgoing
        messages. (default `False`)
    :param enc_alg:
        The encryption algorithm to be used. (default `""tripledes_192_cbc""`)
    :param mdn_mode: The mode to be used for receiving the MDN.
        Set to `None` for no MDN, `'SYNC'` for synchronous and `'ASYNC'`
        for asynchronous. (default `None`)
    :param mdn_digest_alg: The digest algorithm to be used by the receiver
        for signing the MDN. Use `None` for unsigned MDN. (default `None`)
    :param mdn_confirm_text: The text to be used in the MDN for successfully
        processed messages received from this partner.
    :param canonicalize_as_binary: force binary canonicalization for this partner
    :param sign_alg: The signing algorithm to be used for generating the
        signature. (default `rsassa_pkcs1v15`)
    :param key_enc_alg: The key encryption algorithm to be used.
        (default `rsaes_pkcs1v15`)
    '''

    def __post_init__(self):
        '''Run the post initialisation checks for this class.'''
        pass

    def load_verify_cert(self):
        '''Load the verification certificate of the partner and returned the parsed cert.'''
        pass

    def load_encrypt_cert(self):
        '''Load the encryption certificate of the partner and returned the parsed cert.'''
        pass",4,4,25.0,4.0,17.0,3.0,4.0,0.66,0.0,1.0,1.0,0.0,3.0,0.0,3.0,3.0,152.0,34.0,71.0,25.0,67.0,47.0,48.0,25.0,44.0,7.0,0.0,2.0,13.0,snippet_239
311862,reportportal/client-Python,reportportal_client-Python/reportportal_client/core/rp_issues.py,reportportal_client.core.rp_issues.ExternalIssue,"class ExternalIssue:
    """"""This class represents external(BTS) system issue.""""""

    def __init__(self, bts_url=None, bts_project=None, submit_date=None, ticket_id=None, url=None):
        """"""Initialize instance attributes.

        :param bts_url:     Bug tracker system URL
        :param bts_project: Bug tracker system project
        :param submit_date: Bug submission date
        :param ticket_id:   Unique ID of the ticket at the BTS
        :param url:         URL to the ticket(bug)
        """"""
        self.bts_url = bts_url
        self.bts_project = bts_project
        self.submit_date = submit_date
        self.ticket_id = ticket_id
        self.url = url

    @property
    def payload(self):
        """"""Form the correct dictionary for the BTS issue.""""""
        return {'btsUrl': self.bts_url, 'btsProject': self.bts_project, 'submitDate': self.submit_date, 'ticketId': self.ticket_id, 'url': self.url}","class ExternalIssue:
    '''This class represents external(BTS) system issue.'''

    def __init__(self, bts_url=None, bts_project=None, submit_date=None, ticket_id=None, url=None):
        '''Initialize instance attributes.
        :param bts_url:     Bug tracker system URL
        :param bts_project: Bug tracker system project
        :param submit_date: Bug submission date
        :param ticket_id:   Unique ID of the ticket at the BTS
        :param url:         URL to the ticket(bug)
        '''
        pass
    @property
    def payload(self):
        '''Form the correct dictionary for the BTS issue.'''
        pass",3,3,12.0,1.0,7.0,4.0,1.0,0.56,0.0,0.0,0.0,0.0,2.0,5.0,2.0,2.0,28.0,3.0,16.0,9.0,12.0,9.0,9.0,8.0,6.0,1.0,0.0,0.0,2.0,snippet_240
311863,reportportal/client-Python,reportportal_client-Python/reportportal_client/core/rp_issues.py,reportportal_client.core.rp_issues.Issue,"class Issue:
    """"""This class represents an issue that can be attached to test result.""""""

    def __init__(self, issue_type, comment=None, auto_analyzed=False, ignore_analyzer=True):
        """"""Initialize instance attributes.

        :param issue_type:      Issue type locator. Allowable values: ""pb***"",
                                ""ab***"", ""si***"", ""ti***"", ""nd001"". Where ***
                                is locator id.
        :param comment:         Issue comments
        :param auto_analyzed:   Indicator that the issue has been marked with
                                the RP auto analyzer
        :param ignore_analyzer: Flag that forces RP analyzer to ignore this
                                issue
        """"""
        self._external_issues = []
        self.auto_analyzed = auto_analyzed
        self.comment = comment
        self.ignore_analyzer = ignore_analyzer
        self.issue_type = issue_type

    def external_issue_add(self, issue):
        """"""Add external system issue to the issue.""""""
        self._external_issues.append(issue.payload)

    @property
    def payload(self):
        """"""Form the correct dictionary for the issue.""""""
        return {'autoAnalyzed': self.auto_analyzed, 'comment': self.comment, 'externalSystemIssues': self._external_issues, 'ignoreAnalyzer': self.ignore_analyzer, 'issueType': self.issue_type}","class Issue:
    '''This class represents an issue that can be attached to test result.'''

    def __init__(self, issue_type, comment=None, auto_analyzed=False, ignore_analyzer=True):
        '''Initialize instance attributes.
        :param issue_type:      Issue type locator. Allowable values: ""pb***"",
                                ""ab***"", ""si***"", ""ti***"", ""nd001"". Where ***
                                is locator id.
        :param comment:         Issue comments
        :param auto_analyzed:   Indicator that the issue has been marked with
                                the RP auto analyzer
        :param ignore_analyzer: Flag that forces RP analyzer to ignore this
                                issue
        '''
        pass

    def external_issue_add(self, issue):
        '''Add external system issue to the issue.'''
        pass
    @property
    def payload(self):
        '''Form the correct dictionary for the issue.'''
        pass",4,4,10.0,0.0,5.0,4.0,1.0,0.72,0.0,0.0,0.0,0.0,3.0,5.0,3.0,3.0,35.0,4.0,18.0,10.0,13.0,13.0,11.0,9.0,7.0,1.0,0.0,0.0,3.0,snippet_241
312030,SuperCowPowers/workbench,aws_setup/aws_identity_check.py,aws_identity_check.AWSIdentityCheck,"from workbench.core.cloud_platform.aws.aws_account_clamp import AWSAccountClamp
from workbench.utils.config_manager import ConfigManager
import sys
import logging

class AWSIdentityCheck:
    """"""Just a Utility Script that allows people to check which AWS Identity is active""""""

    def __init__(self):
        """"""AWSIdentityCheck Initialization""""""
        self.log = logging.getLogger('workbench')
        self.aws_clamp = AWSAccountClamp()

    def check(self):
        """"""Check the AWS Identity""""""
        cm = ConfigManager()
        active_profile = cm.get_config('AWS_PROFILE')
        if active_profile:
            self.log.info(f'Workbench AWS_PROFILE: {active_profile}')
        else:
            self.log.info('No AWS_PROFILE set')
            sys.exit(0)
        self.log.info('\n\n*** Caller/Base Identity Check ***')
        self.aws_clamp.check_aws_identity()
        self.log.info('Caller/Base Identity Check Success...')
        self.log.info('\n\n*** AWS Assumed Role Check ***')
        self.aws_clamp.check_assumed_role()
        self.log.info('Assumed Role Check Success...')","
class AWSIdentityCheck:
    '''Just a Utility Script that allows people to check which AWS Identity is active'''

    def __init__(self):
        '''AWSIdentityCheck Initialization'''
        pass

    def check(self):
        '''Check the AWS Identity'''
        pass",3,3,14.0,2.0,9.0,3.0,2.0,0.39,0.0,2.0,2.0,0.0,2.0,2.0,2.0,2.0,31.0,6.0,18.0,7.0,15.0,7.0,17.0,7.0,14.0,2.0,0.0,1.0,3.0,snippet_242
312072,SuperCowPowers/workbench,src/workbench/core/pipelines/pipeline_executor.py,pipeline_executor.PipelineExecutor,"import logging
from workbench.api import DataSource, FeatureSet, Model, Endpoint
from workbench.api.model import ModelType

class PipelineExecutor:
    """"""PipelineExecutor: Internal Class: Executes a Workbench Pipeline

    Common Usage:
        ```python
        my_pipeline = PipelineExecutor(pipeline)
        my_pipeline.execute()  # Execute entire pipeline
        my_pipeline.execute_partial([""data_source"", ""feature_set""])
        my_pipeline.execute_partial([""model"", ""endpoint""])
        ```
    """"""

    def __init__(self, pipeline):
        """"""PipelineExecutor Init Method""""""
        self.log = logging.getLogger('workbench')
        self.pipeline_name = pipeline.name
        self.pipeline = pipeline.pipeline

    def execute(self, subset: list=None):
        """"""Execute the Workbench Pipeline

        Args:
            subset (list): A list of steps to execute. If None, execute the entire pipeline

        Raises:
            RuntimeError: If the pipeline execution fails in any way
        """"""
        self.log.important(f'Executing Pipeline {self.pipeline_name}...')
        if subset:
            self.log.important(f'\tSubset: {subset}')
        workbench_objects = {}
        for class_name, kwargs in self.pipeline.items():
            data_input = kwargs['input']
            del kwargs['input']
            if isinstance(input, str) and data_input == '<<parameter_required>>':
                msg = 'Call set_input() to set the input (DataFrame, file path or S3 path)'
                self.log.critical(msg)
                raise RuntimeError(msg)
            if class_name == 'data_source':
                if not subset or 'data_source' in subset:
                    workbench_objects['data_source'] = DataSource(data_input, **kwargs)
                else:
                    workbench_objects['data_source'] = DataSource(source=kwargs['name'])
            elif class_name == 'feature_set':
                holdout_ids = None
                if 'holdout_ids' in kwargs:
                    if kwargs['holdout_ids'] == '<<parameter_optional>>':
                        self.log.important('Hold out ids are not set, defaulting to 80/20 split')
                        holdout_ids = None
                        id_column = None
                    else:
                        holdout_ids = kwargs['holdout_ids']
                        if 'id_column' not in kwargs or kwargs['id_column'] == '<<parameter_optional>>':
                            self.log.warning('Hold out ids are set, but no id column is provided! Defaulting to 80/20 split')
                            holdout_ids = None
                            id_column = None
                        else:
                            id_column = kwargs['id_column']
                            del kwargs['id_column']
                    del kwargs['holdout_ids']
                if 'data_source' in workbench_objects and (not subset or 'feature_set' in subset):
                    if 'feature_schema' in kwargs:
                        if kwargs['feature_schema'] == 'molecular_descriptors_v1':
                            del kwargs['feature_schema']
                            self.log.error('Feature Schema: molecular_descriptors_v1 not currently implemented')
                        else:
                            raise RuntimeError(f""Unsupported feature schema: {kwargs['feature_schema']}"")
                    else:
                        workbench_objects['data_source'].to_features(**kwargs)
                workbench_objects['feature_set'] = FeatureSet(kwargs['name'])
                if holdout_ids:
                    workbench_objects['feature_set'].set_training_holdouts(id_column, holdout_ids)
            elif class_name == 'model':
                if 'model_type' in kwargs:
                    kwargs['model_type'] = ModelType(kwargs['model_type'])
                if 'feature_set' in workbench_objects and (not subset or 'model' in subset):
                    workbench_objects['feature_set'].to_model(**kwargs)
                if not subset or 'endpoint' in subset:
                    workbench_objects['model'] = Model(kwargs['name'])
                    workbench_objects['model'].set_owner('pipeline')
            elif class_name == 'endpoint':
                if 'model' in workbench_objects and (not subset or 'endpoint' in subset):
                    workbench_objects['model'].to_endpoint(**kwargs)
                    endpoint = Endpoint(kwargs['name'])
                    endpoint.auto_inference(capture=True)
            else:
                raise RuntimeError(f'Unsupported pipeline stage: {class_name}')

    def execute_partial(self, subset: list):
        """"""Execute a partial Pipeline

        Args:
            subset (list): A subset of the pipeline to execute

        Raises:
            RunTimeException: If the pipeline execution fails in any way
        """"""
        self.execute(subset)","
class PipelineExecutor:
    '''PipelineExecutor: Internal Class: Executes a Workbench Pipeline
    Common Usage:
        ```python
        my_pipeline = PipelineExecutor(pipeline)
        my_pipeline.execute()  # Execute entire pipeline
        my_pipeline.execute_partial([""data_source"", ""feature_set""])
        my_pipeline.execute_partial([""model"", ""endpoint""])
        ```
    '''

    def __init__(self, pipeline):
        '''PipelineExecutor Init Method'''
        pass

    def execute(self, subset: list=None):
        '''Execute the Workbench Pipeline
        Args:
            subset (list): A list of steps to execute. If None, execute the entire pipeline
        Raises:
            RuntimeError: If the pipeline execution fails in any way
        '''
        pass

    def execute_partial(self, subset: list):
        '''Execute a partial Pipeline
        Args:
            subset (list): A subset of the pipeline to execute
        Raises:
            RunTimeException: If the pipeline execution fails in any way
        '''
        pass",4,4,39.0,5.0,23.0,11.0,7.0,0.61,0.0,8.0,5.0,0.0,3.0,3.0,3.0,3.0,132.0,19.0,70.0,14.0,66.0,43.0,59.0,14.0,55.0,20.0,0.0,5.0,22.0,snippet_243
312949,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/factory/source/element.py,apidoc.factory.source.element.Element,"from apidoc.lib.util.cast import to_boolean
import collections
from apidoc.object.source_raw import Sampleable, Displayable

class Element:
    """""" Populate Helper Factory
    """"""

    def set_common_datas(self, element, name, datas):
        """"""Populated common data for an element from dictionnary datas
        """"""
        element.name = str(name)
        if 'description' in datas:
            element.description = str(datas['description']).strip()
        if isinstance(element, Sampleable) and element.sample is None and ('sample' in datas):
            element.sample = str(datas['sample']).strip()
        if isinstance(element, Displayable):
            if 'display' in datas:
                element.display = to_boolean(datas['display'])
            if 'label' in datas:
                element.label = datas['label']
            else:
                element.label = element.name

    def create_dictionary_of_element_from_dictionary(self, property_name, datas):
        """"""Populate a dictionary of elements
        """"""
        response = {}
        if property_name in datas and datas[property_name] is not None and isinstance(datas[property_name], collections.Iterable):
            for key, value in datas[property_name].items():
                response[key] = self.create_from_name_and_dictionary(key, value)
        return response

    def create_list_of_element_from_dictionary(self, property_name, datas):
        """"""Populate a list of elements
        """"""
        response = []
        if property_name in datas and datas[property_name] is not None and isinstance(datas[property_name], list):
            for value in datas[property_name]:
                response.append(self.create_from_dictionary(value))
        return response

    def get_enum(self, property, enum, datas):
        """"""Factory enum type
        """"""
        str_property = str(datas[property]).lower()
        if str_property not in enum:
            raise ValueError('Unknow enum ""%s"" for ""%s"".' % (str_property, property))
        return enum(str_property)","
class Element:
    ''' Populate Helper Factory
    '''

    def set_common_datas(self, element, name, datas):
        '''Populated common data for an element from dictionnary datas
        '''
        pass

    def create_dictionary_of_element_from_dictionary(self, property_name, datas):
        '''Populate a dictionary of elements
        '''
        pass

    def create_list_of_element_from_dictionary(self, property_name, datas):
        '''Populate a list of elements
        '''
        pass

    def get_enum(self, property, enum, datas):
        '''Factory enum type
        '''
        pass",5,5,11.0,1.0,8.0,2.0,4.0,0.32,0.0,5.0,2.0,8.0,4.0,0.0,4.0,4.0,50.0,9.0,31.0,10.0,26.0,10.0,30.0,10.0,25.0,6.0,0.0,2.0,14.0,snippet_244
313013,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/object/source_raw.py,apidoc.object.source_raw.Sampleable,"class Sampleable:
    """"""Element who can provide samples
    """"""

    def __init__(self):
        """"""Class instantiation
        """"""
        super().__init__()
        self.sample = None

    def get_sample(self):
        """"""Return the a sample for the element
        """"""
        if self.sample is None:
            return self.get_default_sample()
        return self.sample

    def get_default_sample(self):
        """"""Return default value for the element
        """"""
        return 'my_%s' % self.name","class Sampleable:
    '''Element who can provide samples
    '''

    def __init__(self):
        '''Class instantiation
        '''
        pass

    def get_sample(self):
        '''Return the a sample for the element
        '''
        pass

    def get_default_sample(self):
        '''Return default value for the element
        '''
        pass",4,4,5.0,0.0,3.0,2.0,1.0,0.8,0.0,1.0,0.0,3.0,3.0,1.0,3.0,3.0,22.0,4.0,10.0,5.0,6.0,8.0,10.0,5.0,6.0,2.0,0.0,1.0,4.0,snippet_245
313027,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/service/config.py,apidoc.service.config.Config,"import os
from apidoc.object.config import Config as ConfigObject

class Config:
    """"""Provide tool to managed config
    """"""

    def validate(self, config):
        """"""Validate that the source file is ok
        """"""
        if not isinstance(config, ConfigObject):
            raise Exception('Config object expected')
        if config['output']['componants'] not in ('local', 'remote', 'embedded', 'without'):
            raise ValueError('Unknown componant ""%s"".' % config['output']['componants'])
        if config['output']['layout'] not in ('default', 'content-only'):
            raise ValueError('Unknown layout ""%s"".' % config['output']['layout'])
        if config['input']['locations'] is not None:
            unknown_locations = [x for x in config['input']['locations'] if not os.path.exists(x)]
            if len(unknown_locations) > 0:
                raise ValueError('Location%s ""%s"" does not exists' % ('s' if len(unknown_locations) > 1 else '', '"" and ""'.join(unknown_locations)))
            config['input']['locations'] = [os.path.realpath(x) for x in config['input']['locations']]
        if config['input']['arguments'] is not None:
            if not isinstance(config['input']['arguments'], dict):
                raise ValueError('Sources arguments ""%s"" are not a dict' % config['input']['arguments'])

    def get_template_from_config(self, config):
        """"""Retrieve a template path from the config object
        """"""
        if config['output']['template'] == 'default':
            return os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'template', 'default.html')
        else:
            return os.path.abspath(config['output']['template'])","
class Config:
    '''Provide tool to managed config
    '''

    def validate(self, config):
        '''Validate that the source file is ok
        '''
        pass

    def get_template_from_config(self, config):
        '''Retrieve a template path from the config object
        '''
        pass",3,3,19.0,3.0,15.0,2.0,6.0,0.2,0.0,3.0,0.0,0.0,2.0,0.0,2.0,2.0,44.0,8.0,30.0,4.0,27.0,6.0,20.0,4.0,17.0,9.0,0.0,2.0,11.0,snippet_246
313029,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/service/merger.py,apidoc.service.merger.Merger,"from apidoc.lib.util.cast import to_boolean

class Merger:
    """"""Provide tool to merge elements
    """"""

    def merge_extends(self, target, extends, inherit_key='inherit', inherit=False):
        """"""Merge extended dicts
        """"""
        if isinstance(target, dict):
            if inherit and inherit_key in target and (not to_boolean(target[inherit_key])):
                return
            if not isinstance(extends, dict):
                raise ValueError('Unable to merge: Dictionnary expected')
            for key in extends:
                if key not in target:
                    target[str(key)] = extends[key]
                else:
                    self.merge_extends(target[key], extends[key], inherit_key, True)
        elif isinstance(target, list):
            if not isinstance(extends, list):
                raise ValueError('Unable to merge: List expected')
            target += extends

    def merge_sources(self, datas):
        """"""Merge sources files
        """"""
        datas = [data for data in datas if data is not None]
        if len(datas) == 0:
            raise ValueError('Data missing')
        if len(datas) == 1:
            return datas[0]
        if isinstance(datas[0], list):
            if len([x for x in datas if not isinstance(x, list)]) > 0:
                raise TypeError('Unable to merge: List expected')
            base = []
            for x in datas:
                base = base + x
            return base
        if isinstance(datas[0], dict):
            if len([x for x in datas if not isinstance(x, dict)]) > 0:
                raise TypeError('Unable to merge: Dictionnary expected')
            result = {}
            for element in datas:
                for key in element:
                    if key in result:
                        result[key] = self.merge_sources([result[key], element[key]])
                    else:
                        result[key] = element[key]
            return result
        if len([x for x in datas if isinstance(x, (dict, list))]) > 0:
            raise TypeError('Unable to merge: List not expected')
        raise ValueError('Unable to merge: Conflict')

    def merge_configs(self, config, datas):
        """"""Merge configs files
        """"""
        if not isinstance(config, dict) or len([x for x in datas if not isinstance(x, dict)]) > 0:
            raise TypeError('Unable to merge: Dictionnary expected')
        for key, value in config.items():
            others = [x[key] for x in datas if key in x]
            if len(others) > 0:
                if isinstance(value, dict):
                    config[key] = self.merge_configs(value, others)
                else:
                    config[key] = others[-1]
        return config","
class Merger:
    '''Provide tool to merge elements
    '''

    def merge_extends(self, target, extends, inherit_key='inherit', inherit=False):
        '''Merge extended dicts
        '''
        pass

    def merge_sources(self, datas):
        '''Merge sources files
        '''
        pass

    def merge_configs(self, config, datas):
        '''Merge configs files
        '''
        pass",4,4,22.0,2.0,18.0,2.0,8.0,0.15,0.0,5.0,0.0,0.0,3.0,0.0,3.0,3.0,73.0,11.0,54.0,11.0,50.0,8.0,50.0,11.0,46.0,12.0,0.0,4.0,25.0,snippet_247
313030,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/service/parser.py,apidoc.service.parser.Parser,"import os
import yaml
import json

class Parser:
    """"""Provide tools to parse files
    """"""

    def load_from_file(self, file_path, format=None):
        """"""Return dict from a file config
        """"""
        if format is None:
            base_name, file_extension = os.path.splitext(file_path)
            if file_extension in ('.yaml', '.yml'):
                format = 'yaml'
            elif file_extension in '.json':
                format = 'json'
            else:
                raise ValueError('Config file ""%s"" undetermined' % file_extension)
        if format == 'yaml':
            return yaml.load(open(file_path), Loader=yaml.CSafeLoader if yaml.__with_libyaml__ else yaml.SafeLoader)
        elif format == 'json':
            return json.load(open(file_path))
        else:
            raise ValueError('Format ""%s"" unknwon' % format)

    def load_all_from_directory(self, directory_path):
        """"""Return a list of dict from a directory containing files
        """"""
        datas = []
        for root, folders, files in os.walk(directory_path):
            for f in files:
                datas.append(self.load_from_file(os.path.join(root, f)))
        return datas","
class Parser:
    '''Provide tools to parse files
    '''

    def load_from_file(self, file_path, format=None):
        '''Return dict from a file config
        '''
        pass

    def load_all_from_directory(self, directory_path):
        '''Return a list of dict from a directory containing files
        '''
        pass",3,3,14.0,1.0,11.0,2.0,5.0,0.27,0.0,1.0,0.0,0.0,2.0,0.0,2.0,2.0,32.0,4.0,22.0,7.0,19.0,6.0,18.0,7.0,15.0,7.0,0.0,2.0,10.0,snippet_248
313032,SolutionsCloud/apidoc,SolutionsCloud_apidoc/apidoc/service/template.py,apidoc.service.template.Template,"import sys
import logging
import shutil
import os

class Template:
    """"""Provide tool to managed templates
    """"""

    def __init__(self):
        """"""Class instantiation
        """"""
        self.input = 'default.html'
        self.output = 'stdout'
        self.env = None

    def render(self, sources, config, out=sys.stdout):
        """"""Render the documentation as defined in config Object
        """"""
        logger = logging.getLogger()
        template = self.env.get_template(self.input)
        output = template.render(sources=sources, layout=config['output']['layout'], config=config['output'])
        if self.output == 'stdout':
            out.write(output)
        else:
            dir = os.path.dirname(self.output)
            if dir and (not os.path.exists(dir)):
                try:
                    os.makedirs(dir)
                except IOError as ioerror:
                    logger.error('Error on creating dir ""{}"": {}'.format(dir, str(ioerror)))
                    return
            if config['output']['template'] == 'default':
                if config['output']['componants'] == 'local':
                    for template_dir in self.env.loader.searchpath:
                        files = (os.path.join(template_dir, 'resource', 'js', 'combined.js'), os.path.join(template_dir, 'resource', 'css', 'combined.css'), os.path.join(template_dir, 'resource', 'font', 'apidoc.eot'), os.path.join(template_dir, 'resource', 'font', 'apidoc.woff'), os.path.join(template_dir, 'resource', 'font', 'apidoc.ttf'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.eot'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.woff'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.ttf'))
                        for file in files:
                            filename = os.path.basename(file)
                            dirname = os.path.basename(os.path.dirname(file))
                            if not os.path.exists(os.path.join(dir, dirname)):
                                os.makedirs(os.path.join(dir, dirname))
                            if os.path.exists(file):
                                shutil.copyfile(file, os.path.join(dir, dirname, filename))
                            else:
                                logger.warn('Missing resource file ""%s"". If you run apidoc in virtualenv, run ""%s""' % (filename, 'python setup.py resources'))
                if config['output']['componants'] == 'remote':
                    for template_dir in self.env.loader.searchpath:
                        files = (os.path.join(template_dir, 'resource', 'js', 'combined.js'), os.path.join(template_dir, 'resource', 'css', 'combined-embedded.css'), os.path.join(template_dir, 'resource', 'font', 'apidoc.eot'), os.path.join(template_dir, 'resource', 'font', 'apidoc.woff'), os.path.join(template_dir, 'resource', 'font', 'apidoc.ttf'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.eot'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.woff'), os.path.join(template_dir, 'resource', 'font', 'source-code-pro.ttf'))
                        for file in files:
                            filename = os.path.basename(file)
                            dirname = os.path.basename(os.path.dirname(file))
                            if not os.path.exists(os.path.join(dir, dirname)):
                                os.makedirs(os.path.join(dir, dirname))
                            if os.path.exists(file):
                                shutil.copyfile(file, os.path.join(dir, dirname, filename))
                            else:
                                logger.warn('Missing resource file ""%s"". If you run apidoc in virtualenv, run ""%s""' % (filename, 'python setup.py resources'))
            open(self.output, 'w').write(output)","
class Template:
    '''Provide tool to managed templates
    '''

    def __init__(self):
        '''Class instantiation
        '''
        pass

    def render(self, sources, config, out=sys.stdout):
        '''Render the documentation as defined in config Object
        '''
        pass",3,3,36.0,3.0,31.0,2.0,8.0,0.1,0.0,1.0,0.0,0.0,2.0,3.0,2.0,2.0,78.0,9.0,63.0,15.0,60.0,6.0,42.0,14.0,39.0,15.0,0.0,6.0,16.0,snippet_249
313488,adafruit/Adafruit_CircuitPython_framebuf,adafruit_Adafruit_CircuitPython_framebuf/adafruit_framebuf.py,adafruit_framebuf.MHMSBFormat,"class MHMSBFormat:
    """"""MHMSBFormat""""""

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        """"""Set a given pixel to a color.""""""
        index = (y * framebuf.stride + x) // 8
        offset = 7 - x & 7
        framebuf.buf[index] = framebuf.buf[index] & ~(1 << offset) | (color != 0) << offset

    @staticmethod
    def get_pixel(framebuf, x, y):
        """"""Get the color of a given pixel""""""
        index = (y * framebuf.stride + x) // 8
        offset = 7 - x & 7
        return framebuf.buf[index] >> offset & 1

    @staticmethod
    def fill(framebuf, color):
        """"""completely fill/clear the buffer with a color""""""
        if color:
            fill = 255
        else:
            fill = 0
        for i in range(len(framebuf.buf)):
            framebuf.buf[i] = fill

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        """"""Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.""""""
        for _x in range(x, x + width):
            offset = 7 - _x & 7
            for _y in range(y, y + height):
                index = (_y * framebuf.stride + _x) // 8
                framebuf.buf[index] = framebuf.buf[index] & ~(1 << offset) | (color != 0) << offset","class MHMSBFormat:
    '''MHMSBFormat'''
    @staticmethod
    def set_pixel(framebuf, x, y, color):
        '''Set a given pixel to a color.'''
        pass
    @staticmethod
    def get_pixel(framebuf, x, y):
        '''Get the color of a given pixel'''
        pass
    @staticmethod
    def fill(framebuf, color):
        '''completely fill/clear the buffer with a color'''
        pass
    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        '''Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.'''
        pass",5,5,8.0,0.0,6.0,2.0,2.0,0.27,0.0,1.0,0.0,0.0,0.0,0.0,4.0,4.0,41.0,4.0,30.0,19.0,21.0,8.0,21.0,15.0,16.0,3.0,0.0,2.0,8.0,snippet_250
313489,adafruit/Adafruit_CircuitPython_framebuf,adafruit_Adafruit_CircuitPython_framebuf/adafruit_framebuf.py,adafruit_framebuf.MVLSBFormat,"class MVLSBFormat:
    """"""MVLSBFormat""""""

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        """"""Set a given pixel to a color.""""""
        index = (y >> 3) * framebuf.stride + x
        offset = y & 7
        framebuf.buf[index] = framebuf.buf[index] & ~(1 << offset) | (color != 0) << offset

    @staticmethod
    def get_pixel(framebuf, x, y):
        """"""Get the color of a given pixel""""""
        index = (y >> 3) * framebuf.stride + x
        offset = y & 7
        return framebuf.buf[index] >> offset & 1

    @staticmethod
    def fill(framebuf, color):
        """"""completely fill/clear the buffer with a color""""""
        if color:
            fill = 255
        else:
            fill = 0
        for i in range(len(framebuf.buf)):
            framebuf.buf[i] = fill

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        """"""Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.""""""
        while height > 0:
            index = (y >> 3) * framebuf.stride + x
            offset = y & 7
            for w_w in range(width):
                framebuf.buf[index + w_w] = framebuf.buf[index + w_w] & ~(1 << offset) | (color != 0) << offset
            y += 1
            height -= 1","class MVLSBFormat:
    '''MVLSBFormat'''
    @staticmethod
    def set_pixel(framebuf, x, y, color):
        '''Set a given pixel to a color.'''
        pass
    @staticmethod
    def get_pixel(framebuf, x, y):
        '''Get the color of a given pixel'''
        pass
    @staticmethod
    def fill(framebuf, color):
        '''completely fill/clear the buffer with a color'''
        pass
    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        '''Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.'''
        pass",5,5,8.0,0.0,7.0,2.0,2.0,0.25,0.0,1.0,0.0,0.0,0.0,0.0,4.0,4.0,43.0,4.0,32.0,18.0,23.0,8.0,23.0,14.0,18.0,3.0,0.0,2.0,8.0,snippet_251
313491,adafruit/Adafruit_CircuitPython_framebuf,adafruit_Adafruit_CircuitPython_framebuf/adafruit_framebuf.py,adafruit_framebuf.RGB888Format,"class RGB888Format:
    """"""RGB888Format""""""

    @staticmethod
    def set_pixel(framebuf, x, y, color):
        """"""Set a given pixel to a color.""""""
        index = (y * framebuf.stride + x) * 3
        if isinstance(color, tuple):
            framebuf.buf[index:index + 3] = bytes(color)
        else:
            framebuf.buf[index:index + 3] = bytes((color >> 16 & 255, color >> 8 & 255, color & 255))

    @staticmethod
    def get_pixel(framebuf, x, y):
        """"""Get the color of a given pixel""""""
        index = (y * framebuf.stride + x) * 3
        return framebuf.buf[index] << 16 | framebuf.buf[index + 1] << 8 | framebuf.buf[index + 2]

    @staticmethod
    def fill(framebuf, color):
        """"""completely fill/clear the buffer with a color""""""
        fill = (color >> 16 & 255, color >> 8 & 255, color & 255)
        for i in range(0, len(framebuf.buf), 3):
            framebuf.buf[i:i + 3] = bytes(fill)

    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        """"""Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.""""""
        fill = (color >> 16 & 255, color >> 8 & 255, color & 255)
        for _x in range(x, x + width):
            for _y in range(y, y + height):
                index = (_y * framebuf.stride + _x) * 3
                framebuf.buf[index:index + 3] = bytes(fill)","class RGB888Format:
    '''RGB888Format'''
    @staticmethod
    def set_pixel(framebuf, x, y, color):
        '''Set a given pixel to a color.'''
        pass
    @staticmethod
    def get_pixel(framebuf, x, y):
        '''Get the color of a given pixel'''
        pass
    @staticmethod
    def fill(framebuf, color):
        '''completely fill/clear the buffer with a color'''
        pass
    @staticmethod
    def fill_rect(framebuf, x, y, width, height, color):
        '''Draw a rectangle at the given location, size and color. The ``fill_rect`` method draws
        both the outline and interior.'''
        pass",5,5,8.0,0.0,6.0,2.0,2.0,0.23,0.0,3.0,0.0,0.0,0.0,0.0,4.0,4.0,41.0,4.0,30.0,17.0,21.0,7.0,19.0,13.0,14.0,3.0,0.0,2.0,8.0,snippet_252
313959,Galarzaa90/tibia.py,Galarzaa90_tibia.py/tibiapy/parsers/creature.py,tibiapy.parsers.creature.BoostableBossesParser,"import urllib.parse
import os
from tibiapy.utils import convert_line_breaks, parse_tibiacom_content
from tibiapy.errors import InvalidContentError
from tibiapy.models import BoostableBosses, BoostedCreatures, BossEntry, Creature, CreatureEntry, CreaturesSection

class BoostableBossesParser:
    """"""Parser for the boostable bosses section of Tibia.com.""""""

    @classmethod
    def from_content(cls, content: str) -> BoostableBosses:
        """"""Create an instance of the class from the html content of the boostable bosses library's page.

        Parameters
        ----------
        content:
            The HTML content of the page.

        Returns
        -------
            The Boostable Bosses section.

        Raises
        ------
        InvalidContent
            If content is not the HTML of a creature library's page.

        """"""
        try:
            parsed_content = parse_tibiacom_content(content)
            boosted_creature_table = parsed_content.select_one('div.TableContainer')
            boosted_creature_text = boosted_creature_table.select_one('div.Text')
            if not boosted_creature_text or 'Boosted' not in boosted_creature_text.text:
                raise InvalidContentError('content is not from the boostable bosses section.')
            boosted_boss_tag = boosted_creature_table.select_one('b')
            boosted_boss_image = boosted_creature_table.select_one('img')
            image_url = urllib.parse.urlparse(boosted_boss_image['src'])
            boosted_boss = BossEntry(name=boosted_boss_tag.text, identifier=os.path.basename(image_url.path).replace('.gif', ''))
            list_table = parsed_content.find('div', style=lambda v: v and 'display: table' in v)
            entries_container = list_table.find_all('div', style=lambda v: v and 'float: left' in v)
            entries = []
            for entry_container in entries_container:
                name = entry_container.text.strip()
                image = entry_container.select_one('img')
                image_url = urllib.parse.urlparse(image['src'])
                identifier = os.path.basename(image_url.path).replace('.gif', '')
                entries.append(BossEntry(name=name, identifier=identifier))
            return BoostableBosses(boosted_boss=boosted_boss, bosses=entries)
        except (AttributeError, ValueError) as e:
            raise InvalidContentError(""content is not the boosted boss's library"", e) from e

    @classmethod
    def boosted_boss_from_header(cls, content: str) -> BossEntry:
        """"""Get the boosted boss from any Tibia.com page.

        Parameters
        ----------
        content:
            The HTML content of a Tibia.com page.

        Returns
        -------
            The boosted boss of the day.

        Raises
        ------
        InvalidContent
            If content is not the HTML of a Tibia.com's page.

        """"""
        return BoostedCreaturesParser.from_header(content).boss","
class BoostableBossesParser:
    '''Parser for the boostable bosses section of Tibia.com.'''
    @classmethod
    def from_content(cls, content: str) -> BoostableBosses:
        '''Create an instance of the class from the html content of the boostable bosses library's page.
        Parameters
        ----------
        content:
            The HTML content of the page.
        Returns
        -------
            The Boostable Bosses section.
        Raises
        ------
        InvalidContent
            If content is not the HTML of a creature library's page.
        '''
        pass
    @classmethod
    def boosted_boss_from_header(cls, content: str) -> BossEntry:
        '''Get the boosted boss from any Tibia.com page.
        Parameters
        ----------
        content:
            The HTML content of a Tibia.com page.
        Returns
        -------
            The boosted boss of the day.
        Raises
        ------
        InvalidContent
            If content is not the HTML of a Tibia.com's page.
        '''
        pass",3,3,32.0,6.0,13.0,13.0,3.0,0.93,0.0,7.0,4.0,0.0,0.0,0.0,2.0,2.0,69.0,13.0,29.0,20.0,24.0,27.0,26.0,17.0,23.0,4.0,0.0,2.0,5.0,snippet_253
313961,Galarzaa90/tibia.py,Galarzaa90_tibia.py/tibiapy/parsers/creature.py,tibiapy.parsers.creature.CreatureParser,"from tibiapy.models import BoostableBosses, BoostedCreatures, BossEntry, Creature, CreatureEntry, CreaturesSection
from tibiapy.utils import convert_line_breaks, parse_tibiacom_content
from typing import Optional
from tibiapy.builders import CreatureBuilder

class CreatureParser:
    """"""Parser for creatures.""""""
    _valid_elements = ('ice', 'fire', 'earth', 'poison', 'death', 'holy', 'physical', 'energy')

    @classmethod
    def from_content(cls, content: str) -> Optional[Creature]:
        """"""Create an instance of the class from the html content of the creature library's page.

        Parameters
        ----------
        content:
            The HTML content of the page.

        Returns
        -------
            The creature contained in the page.

        """"""
        try:
            parsed_content = parse_tibiacom_content(content)
            _, content_container = parsed_content.find_all('div', style=lambda v: v and 'position: relative' in v)
            title_container, description_container = content_container.select('div')
            title = title_container.select_one('h2')
            name = title.text.strip()
            img = title_container.select_one('img')
            img_url = img['src']
            race = img_url.split('/')[-1].replace('.gif', '')
            builder = CreatureBuilder().name(name).identifier(race)
            convert_line_breaks(description_container)
            paragraph_tags = description_container.select('p')
            paragraphs = [p.text for p in paragraph_tags]
            builder.description('\n'.join(paragraphs[:-2]).strip())
            hp_text = paragraphs[-2]
            cls._parse_hp_text(builder, hp_text)
            exp_text = paragraphs[-1]
            cls._parse_exp_text(builder, exp_text)
            return builder.build()
        except ValueError:
            return None

    @classmethod
    def _parse_exp_text(cls, builder: CreatureBuilder, exp_text: str) -> None:
        """"""Parse the experience text, containing dropped loot and adds it to the creature.

        Parameters
        ----------
        builder: :class:`CreatureBuilder`
            The builder where data will be stored to.
        exp_text: :class:`str`
            The text containing experience.

        """"""
        if (m := EXP_PATTERN.search(exp_text)):
            builder.experience(int(m.group(1)))
        if (m := LOOT_PATTERN.search(exp_text)):
            builder.loot(m.group(1))

    @classmethod
    def _parse_hp_text(cls, builder: CreatureBuilder, hp_text: str) -> None:
        """"""Parse the text containing the creature's hitpoints, containing weaknesses, immunities and more and adds it.

        Parameters
        ----------
        builder: :class:`CreatureBuilder`
            The builder where data will be stored to.
        hp_text: :class:`str`
            The text containing hitpoints.

        """"""
        m = HP_PATTERN.search(hp_text)
        if m:
            builder.hitpoints(int(m.group(1)))
        m = IMMUNE_PATTERN.search(hp_text)
        immune = []
        if m:
            immune.extend(cls._parse_elements(m.group(1)))
        if 'cannot be paralysed' in hp_text:
            immune.append('paralyze')
        if 'sense invisible' in hp_text:
            immune.append('invisible')
        builder.immune_to(immune)
        if (m := WEAK_PATTERN.search(hp_text)):
            builder.weak_against(cls._parse_elements(m.group(1)))
        if (m := STRONG_PATTERN.search(hp_text)):
            builder.strong_against(cls._parse_elements(m.group(1)))
        if (m := MANA_COST.search(hp_text)):
            builder.mana_cost(int(m.group(1)))
            if 'summon or convince' in hp_text:
                builder.convinceable(True)
                builder.summonable(True)
            if 'cannot be summoned' in hp_text:
                builder.convinceable(True)
            if 'cannot be convinced' in hp_text:
                builder.summonable(True)

    @classmethod
    def _parse_elements(cls, text: str) -> list[str]:
        """"""Parse the elements found in a string, adding them to the collection.

        Parameters
        ----------
        text: :class:`str`
            The text containing the elements.

        """"""
        return [element for element in cls._valid_elements if element in text]","
class CreatureParser:
    '''Parser for creatures.'''
    @classmethod
    def from_content(cls, content: str) -> Optional[Creature]:
        '''Create an instance of the class from the html content of the creature library's page.
        Parameters
        ----------
        content:
            The HTML content of the page.
        Returns
        -------
            The creature contained in the page.
        '''
        pass
    @classmethod
    def _parse_exp_text(cls, builder: CreatureBuilder, exp_text: str) -> None:
        '''Parse the experience text, containing dropped loot and adds it to the creature.
        Parameters
        ----------
        builder: :class:`CreatureBuilder`
            The builder where data will be stored to.
        exp_text: :class:`str`
            The text containing experience.
        '''
        pass
    @classmethod
    def _parse_hp_text(cls, builder: CreatureBuilder, hp_text: str) -> None:
        '''Parse the text containing the creature's hitpoints, containing weaknesses, immunities and more and adds it.
        Parameters
        ----------
        builder: :class:`CreatureBuilder`
            The builder where data will be stored to.
        hp_text: :class:`str`
            The text containing hitpoints.
        '''
        pass
    @classmethod
    def _parse_elements(cls, text: str) -> list[str]:
        '''Parse the elements found in a string, adding them to the collection.
        Parameters
        ----------
        text: :class:`str`
            The text containing the elements.
        '''
        pass",5,5,27.0,5.0,14.0,8.0,4.0,0.51,0.0,6.0,2.0,0.0,0.0,0.0,4.0,4.0,121.0,26.0,63.0,26.0,54.0,32.0,57.0,21.0,52.0,11.0,0.0,2.0,17.0,snippet_254
313962,Galarzaa90/tibia.py,Galarzaa90_tibia.py/tibiapy/parsers/creature.py,tibiapy.parsers.creature.CreaturesSectionParser,"from tibiapy.models import BoostableBosses, BoostedCreatures, BossEntry, Creature, CreatureEntry, CreaturesSection
from tibiapy.errors import InvalidContentError
import urllib.parse
from tibiapy.utils import convert_line_breaks, parse_tibiacom_content

class CreaturesSectionParser:
    """"""Parser for the creatures section in the library from Tibia.com.""""""

    @classmethod
    def boosted_creature_from_header(cls, content: str) -> CreatureEntry:
        """"""Get the boosted creature from any Tibia.com page.

        Parameters
        ----------
        content:
            The HTML content of a Tibia.com page.

        Returns
        -------
            The boosted creature of the day.

        Raises
        ------
        InvalidContent
            If content is not the HTML of a Tibia.com's page.

        """"""
        return BoostedCreaturesParser.from_header(content).creature

    @classmethod
    def from_content(cls, content: str) -> CreaturesSection:
        """"""Create an instance of the class from the html content of the creature library's page.

        Parameters
        ----------
        content:
            The HTML content of the page.

        Returns
        -------
            The creatures section from Tibia.com.

        Raises
        ------
        InvalidContent
            If content is not the HTML of a creature library's page.

        """"""
        try:
            parsed_content = parse_tibiacom_content(content)
            boosted_creature_table = parsed_content.select_one('div.TableContainer')
            boosted_creature_text = boosted_creature_table.select_one('div.Text')
            if not boosted_creature_text or 'Boosted' not in boosted_creature_text.text:
                raise InvalidContentError('content is not from the creatures section.')
            boosted_creature_link = boosted_creature_table.select_one('a')
            url = urllib.parse.urlparse(boosted_creature_link['href'])
            query = urllib.parse.parse_qs(url.query)
            boosted_creature = CreatureEntry(name=boosted_creature_link.text, identifier=query['race'][0])
            list_table = parsed_content.find('div', style=lambda v: v and 'display: table' in v)
            entries_container = list_table.find_all('div', style=lambda v: v and 'float: left' in v)
            entries = []
            for entry_container in entries_container:
                name = entry_container.text.strip()
                link = entry_container.select_one('a')
                url = urllib.parse.urlparse(link['href'])
                query = urllib.parse.parse_qs(url.query)
                entries.append(CreatureEntry(name=name, identifier=query['race'][0]))
            return CreaturesSection(boosted_creature=boosted_creature, creatures=entries)
        except (AttributeError, ValueError) as e:
            raise InvalidContentError(""content is not the creature's library"", e) from e","
class CreaturesSectionParser:
    '''Parser for the creatures section in the library from Tibia.com.'''
    @classmethod
    def boosted_creature_from_header(cls, content: str) -> CreatureEntry:
        '''Get the boosted creature from any Tibia.com page.
        Parameters
        ----------
        content:
            The HTML content of a Tibia.com page.
        Returns
        -------
            The boosted creature of the day.
        Raises
        ------
        InvalidContent
            If content is not the HTML of a Tibia.com's page.
        '''
        pass
    @classmethod
    def from_content(cls, content: str) -> CreaturesSection:
        '''Create an instance of the class from the html content of the creature library's page.
        Parameters
        ----------
        content:
            The HTML content of the page.
        Returns
        -------
            The creatures section from Tibia.com.
        Raises
        ------
        InvalidContent
            If content is not the HTML of a creature library's page.
        '''
        pass",3,3,31.0,6.0,13.0,13.0,3.0,0.96,0.0,7.0,4.0,0.0,0.0,0.0,2.0,2.0,68.0,13.0,28.0,19.0,23.0,27.0,26.0,16.0,23.0,4.0,0.0,2.0,5.0,snippet_255
319441,pysal/spglm,spglm/varfuncs.py,spglm.varfuncs.VarianceFunction,"import numpy as np

class VarianceFunction:
    """"""
    Relates the variance of a random variable to its mean. Defaults to 1.

    Methods
    -------
    call
        Returns an array of ones that is the same shape as `mu`

    Notes
    -----
    After a variance function is initialized, its call method can be used.

    Alias for VarianceFunction:
    constant = VarianceFunction()

    See also
    --------
    statsmodels.family.family
    """"""

    def __call__(self, mu):
        """"""
        Default variance function

        Parameters
        -----------
        mu : array-like
            mean parameters

        Returns
        -------
        v : array
            ones(mu.shape)
        """"""
        mu = np.asarray(mu)
        return np.ones(mu.shape, np.float64)

    def deriv(self, mu):
        """"""
        Derivative of the variance function v'(mu)
        """"""
        from statsmodels.tools.numdiff import approx_fprime_cs
        return np.diag(approx_fprime_cs(mu, self))","
class VarianceFunction:
    '''
    Relates the variance of a random variable to its mean. Defaults to 1.
    Methods
    -------
    call
        Returns an array of ones that is the same shape as `mu`
    Notes
    -----
    After a variance function is initialized, its call method can be used.
    Alias for VarianceFunction:
    constant = VarianceFunction()
    See also
    --------
    statsmodels.family.family
    '''

    def __call__(self, mu):
        '''
        Default variance function
        Parameters
        -----------
        mu : array-like
            mean parameters
        Returns
        -------
        v : array
            ones(mu.shape)
        '''
        pass

    def deriv(self, mu):
        '''
        Derivative of the variance function v'(mu)
        '''
        pass",3,3,12.0,2.0,3.0,8.0,1.0,4.29,0.0,0.0,0.0,0.0,2.0,0.0,2.0,2.0,46.0,9.0,7.0,4.0,3.0,30.0,7.0,4.0,3.0,1.0,0.0,0.0,2.0,snippet_256
320689,optimizely/python-sdk,optimizely_python-sdk/optimizely/user_profile.py,optimizely.user_profile.UserProfileService,"from typing import Any, Optional

class UserProfileService:
    """""" Class encapsulating user profile service functionality.
  Override with your own implementation for storing and retrieving the user profile. """"""

    def lookup(self, user_id: str) -> dict[str, Any]:
        """""" Fetch the user profile dict corresponding to the user ID.

    Args:
      user_id: ID for user whose profile needs to be retrieved.

    Returns:
      Dict representing the user's profile.
    """"""
        return UserProfile(user_id).__dict__

    def save(self, user_profile: dict[str, Any]) -> None:
        """""" Save the user profile dict sent to this method.

    Args:
      user_profile: Dict representing the user's profile.
    """"""
        pass","
class UserProfileService:
    ''' Class encapsulating user profile service functionality.
  Override with your own implementation for storing and retrieving the user profile. '''

    def lookup(self, user_id: str) -> dict[str, Any]:
        ''' Fetch the user profile dict corresponding to the user ID.
    Args:
      user_id: ID for user whose profile needs to be retrieved.
    Returns:
      Dict representing the user's profile.
        '''
        pass

    def save(self, user_profile: dict[str, Any]) -> None:
        ''' Save the user profile dict sent to this method.
    Args:
      user_profile: Dict representing the user's profile.
        '''
        pass",3,3,9.0,2.0,2.0,5.0,1.0,2.4,0.0,4.0,1.0,2.0,2.0,0.0,2.0,2.0,22.0,5.0,5.0,3.0,2.0,12.0,5.0,3.0,2.0,1.0,0.0,0.0,2.0,snippet_257
322746,wilson-eft/wilson,wilson-eft_wilson/wilson/classes.py,wilson.classes.RGsolution,"import numpy as np
from math import log, e

class RGsolution:
    """"""Class representing a continuous (interpolated) solution to the
    SMEFT RGEs to be used for plotting.""""""

    def __init__(self, fun, scale_min, scale_max):
        """"""Initialize.

        Parameters:

        - fun: function of the scale that is expected to return a
        dictionary with the RGE solution and to accept vectorized input.
        - scale_min, scale_max: lower and upper boundaries of the scale
        """"""
        self.fun = fun
        self.scale_min = scale_min
        self.scale_max = scale_max

    def plotdata(self, key, part='re', scale='log', steps=50):
        """"""Return a tuple of arrays x, y that can be fed to plt.plot,
        where x is the scale in GeV and y is the parameter of interest.

        Parameters:

        - key: dicionary key of the parameter to be plotted (e.g. a WCxf
          coefficient name or a SM parameter like 'g')
        - part: plot the real part 're' (default) or the imaginary part 'im'
        - scale: 'log'; make the x steps logarithmically distributed; for
          'linear', linearly distributed
        - steps: steps in x to take (default: 50)
        """"""
        if scale == 'log':
            x = np.logspace(log(self.scale_min), log(self.scale_max), steps, base=e)
        elif scale == 'linear':
            x = np.linspace(self.scale_min, self.scale_max, steps)
        y = self.fun(x)
        y = np.array([d[key] for d in y])
        if part == 're':
            return (x, y.real)
        elif part == 'im':
            return (x, y.imag)

    def plot(self, key, part='re', scale='log', steps=50, legend=True, plotargs={}):
        """"""Plot the RG evolution of parameter `key`.

        Parameters:

        - part, scale, steps: see `plotdata`
        - legend: boolean, show the legend (default: True)
        - plotargs: dictionary of arguments to be passed to plt.plot
        """"""
        try:
            import matplotlib.pyplot as plt
        except ImportError:
            raise ImportError('Please install matplotlib if you want to use the plot method')
        pdat = self.plotdata(key, part=part, scale=scale, steps=steps)
        plt.plot(*pdat, label=key, **plotargs)
        if scale == 'log':
            plt.xscale('log')
        if legend:
            plt.legend()","
class RGsolution:
    '''Class representing a continuous (interpolated) solution to the
    SMEFT RGEs to be used for plotting.'''

    def __init__(self, fun, scale_min, scale_max):
        '''Initialize.
        Parameters:
        - fun: function of the scale that is expected to return a
        dictionary with the RGE solution and to accept vectorized input.
        - scale_min, scale_max: lower and upper boundaries of the scale
        '''
        pass

    def plotdata(self, key, part='re', scale='log', steps=50):
        '''Return a tuple of arrays x, y that can be fed to plt.plot,
        where x is the scale in GeV and y is the parameter of interest.
        Parameters:
        - key: dicionary key of the parameter to be plotted (e.g. a WCxf
          coefficient name or a SM parameter like 'g')
        - part: plot the real part 're' (default) or the imaginary part 'im'
        - scale: 'log'; make the x steps logarithmically distributed; for
          'linear', linearly distributed
        - steps: steps in x to take (default: 50)
        '''
        pass

    def plotdata(self, key, part='re', scale='log', steps=50):
        '''Plot the RG evolution of parameter `key`.
        Parameters:
        - part, scale, steps: see `plotdata`
        - legend: boolean, show the legend (default: True)
        - plotargs: dictionary of arguments to be passed to plt.plot
        '''
        pass",4,4,20.0,2.0,10.0,7.0,3.0,0.75,0.0,1.0,0.0,0.0,3.0,3.0,3.0,3.0,65.0,9.0,32.0,11.0,27.0,24.0,25.0,11.0,20.0,5.0,0.0,1.0,10.0,snippet_258
322803,wilson-eft/wilson,wilson-eft_wilson/wilson/wcxf/classes.py,wilson.wcxf.classes.WCxf,"import yaml

class WCxf:
    """"""Base class for WCxf files (not meant to be used directly).""""""

    @classmethod
    def load(cls, stream, **kwargs):
        """"""Load the object data from a JSON or YAML file.""""""
        wcxf = _load_yaml_json(stream, **kwargs)
        return cls(**wcxf)

    def dump(self, stream=None, fmt='json', **kwargs):
        """"""Dump the object data to a JSON or YAML file.

        Optional arguments:

        - `stream`: if None (default), return a string. Otherwise,
          should be a writable file-like object
        - `fmt`: format, should be 'json' (default) or 'yaml'

        Additional keyword arguments will be passed to the `json.dump(s)`
        or `yaml.dump` methods.
        """"""
        d = {k: v for k, v in self.__dict__.items() if k[0] != '_'}
        if fmt.lower() == 'json':
            indent = kwargs.pop('indent', 2)
            return _dump_json(d, stream=stream, indent=indent, **kwargs)
        elif fmt.lower() == 'yaml':
            default_flow_style = kwargs.pop('default_flow_style', False)
            return yaml.dump(d, stream, default_flow_style=default_flow_style, **kwargs)
        else:
            raise ValueError(f""Format {fmt} unknown: use 'json' or 'yaml'."")","
class WCxf:
    '''Base class for WCxf files (not meant to be used directly).'''
    @classmethod
    def load(cls, stream, **kwargs):
        '''Load the object data from a JSON or YAML file.'''
        pass

    def dump(self, stream=None, fmt='json', **kwargs):
        '''Dump the object data to a JSON or YAML file.
        Optional arguments:
        - `stream`: if None (default), return a string. Otherwise,
          should be a writable file-like object
        - `fmt`: format, should be 'json' (default) or 'yaml'
        Additional keyword arguments will be passed to the `json.dump(s)`
        or `yaml.dump` methods.
        '''
        pass",3,3,16.0,2.0,9.0,6.0,2.0,0.63,0.0,1.0,0.0,3.0,1.0,0.0,2.0,2.0,36.0,5.0,19.0,8.0,15.0,12.0,12.0,7.0,9.0,3.0,0.0,1.0,4.0,snippet_259
322855,jhermann/rituals,jhermann_rituals/src/rituals/util/antglob.py,rituals.util.antglob.Pattern,"class Pattern:
    """"""A single pattern for either inclusion or exclusion.""""""

    def __init__(self, spec, inclusive):
        """"""Create regex-based pattern matcher from glob `spec`.""""""
        self.compiled = compile_glob(spec.rstrip('/'))
        self.inclusive = inclusive
        self.is_dir = spec.endswith('/')

    def __str__(self):
        """"""Return inclusiveness indicator and original glob pattern.""""""
        return ('+' if self.inclusive else '-') + self.compiled.pattern

    def matches(self, path):
        """"""Check this pattern against given `path`.""""""
        return bool(self.compiled.match(path))","class Pattern:
    '''A single pattern for either inclusion or exclusion.'''

    def __init__(self, spec, inclusive):
        '''Create regex-based pattern matcher from glob `spec`.'''
        pass

    def __str__(self):
        '''Return inclusiveness indicator and original glob pattern.'''
        pass

    def matches(self, path):
        '''Check this pattern against given `path`.'''
        pass",4,4,4.0,0.0,3.0,1.0,1.0,0.44,0.0,1.0,0.0,0.0,3.0,3.0,3.0,3.0,16.0,3.0,9.0,7.0,5.0,4.0,9.0,7.0,5.0,2.0,0.0,0.0,4.0,snippet_260
323184,SoftwareDefinedBuildings/XBOS,apps/Data_quality_analysis/Import_Data.py,Data_quality_analysis.Import_Data.Import_Data,"import pandas as pd
import glob
import numpy as np
import os

class Import_Data:
    """""" This class imports data from csv files. """"""

    def __init__(self):
        """""" Constructor: Store the imported data. """"""
        self.data = pd.DataFrame()

    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        """""" Imports csv file(s) and stores the result in data.

        Note
        ----
        1. If folder exists out of current directory, folder_name should contain correct regex
        2. Assuming there's no file called ""\\*.csv""

        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '\\*', i.e. all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.', i.e. current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe

        """"""
        if isinstance(file_name, str) and isinstance(folder_name, str):
            try:
                self.data = self._load_csv(file_name, folder_name, head_row, index_col, convert_col, concat_files)
            except Exception as e:
                raise e
        elif isinstance(file_name, list) and isinstance(folder_name, str):
            for i, file in enumerate(file_name):
                if isinstance(head_row, list):
                    _head_row = head_row[i]
                else:
                    _head_row = head_row
                if isinstance(index_col, list):
                    _index_col = index_col[i]
                else:
                    _index_col = index_col
                try:
                    data_tmp = self._load_csv(file, folder_name, _head_row, _index_col, convert_col, concat_files)
                    if concat_files:
                        self.data = self.data.append(data_tmp, ignore_index=False, verify_integrity=False)
                    else:
                        self.data = self.data.join(data_tmp, how='outer')
                except Exception as e:
                    raise e
        else:
            raise NotImplementedError(""Filename and Folder name can't both be of type list."")

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        """""" Load single csv file.

        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '*' - all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.' - current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe

        Returns
        -------
        pd.DataFrame()
            Dataframe containing csv data

        """"""
        if file_name == '*':
            if not os.path.isdir(folder_name):
                raise OSError('Folder does not exist.')
            else:
                file_name_list = sorted(glob.glob(folder_name + '*.csv'))
                if not file_name_list:
                    raise OSError('Either the folder does not contain any csv files or invalid folder provided.')
                else:
                    self.import_csv(file_name=file_name_list, head_row=head_row, index_col=index_col, convert_col=convert_col, concat_files=concat_files)
                    return self.data
        elif not os.path.isdir(folder_name):
            raise OSError('Folder does not exist.')
        else:
            path = os.path.join(folder_name, file_name)
            if head_row > 0:
                data = pd.read_csv(path, index_col=index_col, skiprows=[i for i in range(head_row - 1)])
            else:
                data = pd.read_csv(path, index_col=index_col)
            try:
                data.index = pd.to_datetime(data.index, format='%m/%d/%y %H:%M')
            except:
                data.index = pd.to_datetime(data.index, dayfirst=False, infer_datetime_format=True)
        if convert_col:
            for col in data.columns:
                if data[col].dtype != np.number:
                    data[col] = pd.to_numeric(data[col], errors='coerce')
        return data","
class Import_Data:
    ''' This class imports data from csv files. '''

    def __init__(self):
        ''' Constructor: Store the imported data. '''
        pass

    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        ''' Imports csv file(s) and stores the result in data.
        Note
        ----
        1. If folder exists out of current directory, folder_name should contain correct regex
        2. Assuming there's no file called ""\*.csv""
        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '\*', i.e. all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.', i.e. current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe
        '''
        pass

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        ''' Load single csv file.
        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '*' - all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.' - current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe
        Returns
        -------
        pd.DataFrame()
            Dataframe containing csv data
        '''
        pass",4,4,44.0,7.0,19.0,18.0,7.0,0.93,0.0,7.0,0.0,1.0,3.0,1.0,3.0,3.0,140.0,26.0,59.0,14.0,55.0,55.0,48.0,13.0,44.0,10.0,0.0,4.0,20.0,snippet_261
323187,SoftwareDefinedBuildings/XBOS,apps/Data_quality_analysis/Plot_Data.py,Data_quality_analysis.Plot_Data.Plot_Data,"import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

class Plot_Data:
    """""" This class contains functions for displaying various plots.

    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.

    """"""
    count = 1

    def __init__(self, figsize=(18, 5)):
        """""" Constructor.

        Parameters
        ----------
        figsize : tuple
            Size of figure.

        """"""
        self.figsize = figsize

    def correlation_plot(self, data):
        """""" Create heatmap of Pearson's correlation coefficient.

        Parameters
        ----------
        data    : pd.DataFrame()
            Data to display.

        Returns
        -------
        matplotlib.figure
            Heatmap.

        """"""
        fig = plt.figure(Plot_Data.count)
        corr = data.corr()
        ax = sns.heatmap(corr)
        Plot_Data.count += 1
        return fig

    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        """""" Create baseline and projection plots.

        Parameters
        ----------
        y_true              : pd.Series()
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.Dataframe()
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.

        Returns
        -------
        matplotlib.figure
            Baseline plot

        """"""
        fig = plt.figure(Plot_Data.count)
        if projection_period:
            nrows = len(baseline_period) + len(projection_period) / 2
        else:
            nrows = len(baseline_period) / 2
        base_df = pd.DataFrame()
        base_df['y_true'] = y_true
        base_df['y_pred'] = y_pred
        ax1 = fig.add_subplot(nrows, 1, 1)
        base_df.plot(ax=ax1, figsize=self.figsize, title='Baseline Period ({}-{}). \nBest Model: {}. \nBaseline Adj R2: {}. \nSite: {}.'.format(baseline_period[0], baseline_period[1], model_name, adj_r2, site))
        if projection_period:
            num_plot = 2
            for i in range(0, len(projection_period), 2):
                ax = fig.add_subplot(nrows, 1, num_plot)
                period = slice(projection_period[i], projection_period[i + 1])
                project_df = pd.DataFrame()
                try:
                    project_df['y_true'] = data.loc[period, output_col]
                    project_df['y_pred'] = model.predict(data.loc[period, input_col])
                    project_df['y_pred'][project_df['y_pred'] < 0] = 0
                    project_df.plot(ax=ax, figsize=self.figsize, title='Projection Period ({}-{})'.format(projection_period[i], projection_period[i + 1]))
                    num_plot += 1
                    fig.tight_layout()
                    Plot_Data.count += 1
                    return (fig, project_df['y_true'], project_df['y_pred'])
                except:
                    raise TypeError('If projecting into the future, please specify project_ind_col that has data available                                         in the future time period requested.')
        return (fig, None, None)","
class Plot_Data:
    ''' This class contains functions for displaying various plots.
    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.
    '''

    def __init__(self, figsize=(18, 5)):
        ''' Constructor.
        Parameters
        ----------
        figsize : tuple
            Size of figure.
        '''
        pass

    def correlation_plot(self, data):
        ''' Create heatmap of Pearson's correlation coefficient.
        Parameters
        ----------
        data    : pd.DataFrame()
            Data to display.
        Returns
        -------
        matplotlib.figure
            Heatmap.
        '''
        pass

    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        ''' Create baseline and projection plots.
        Parameters
        ----------
        y_true              : pd.Series()
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.Dataframe()
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.
        Returns
        -------
        matplotlib.figure
            Baseline plot
        '''
        pass",4,4,38.0,6.0,15.0,17.0,2.0,1.21,0.0,3.0,0.0,0.0,3.0,1.0,3.0,3.0,133.0,29.0,47.0,22.0,39.0,57.0,38.0,18.0,34.0,5.0,0.0,3.0,7.0,snippet_262
323204,SoftwareDefinedBuildings/XBOS,apps/data_analysis/XBOS_data_analytics/Import_Data.py,XBOS_data_analytics.Import_Data.Import_Data,"import os
import pandas as pd
import numpy as np
import glob

class Import_Data:
    """""" This class imports data from csv files """"""

    def __init__(self):
        """""" Constructor.

        This class stores the imported data.

        """"""
        self.data = pd.DataFrame()

    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        """""" Imports csv file(s) and stores the result in data.

        Note
        ----
        1. If folder exists out of current directory, folder_name should contain correct regex
        2. Assuming there's no file called ""\\*.csv""

        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '\\*', i.e. all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.', i.e. current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe

        """"""
        if isinstance(file_name, str) and isinstance(folder_name, str):
            try:
                self.data = self._load_csv(file_name, folder_name, head_row, index_col, convert_col, concat_files)
            except Exception as e:
                raise e
        elif isinstance(file_name, list) and isinstance(folder_name, str):
            for i, file in enumerate(file_name):
                if isinstance(head_row, list):
                    _head_row = head_row[i]
                else:
                    _head_row = head_row
                if isinstance(index_col, list):
                    _index_col = index_col[i]
                else:
                    _index_col = index_col
                try:
                    data_tmp = self._load_csv(file, folder_name, _head_row, _index_col, convert_col, concat_files)
                    if concat_files:
                        self.data = self.data.append(data_tmp, ignore_index=False, verify_integrity=False)
                    else:
                        self.data = self.data.join(data_tmp, how='outer')
                except Exception as e:
                    raise e
        else:
            raise NotImplementedError(""Filename and Folder name can't both be of type list."")

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        """""" Load single csv file.

        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '*' - all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.' - current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe

        Returns
        -------
        pd.DataFrame()
            Dataframe containing csv data

        """"""
        if file_name == '*':
            if not os.path.isdir(folder_name):
                raise OSError('Folder does not exist.')
            else:
                file_name_list = sorted(glob.glob(folder_name + '*.csv'))
                if not file_name_list:
                    raise OSError('Either the folder does not contain any csv files or invalid folder provided.')
                else:
                    self.import_csv(file_name=file_name_list, head_row=head_row, index_col=index_col, convert_col=convert_col, concat_files=concat_files)
                    return self.data
        elif not os.path.isdir(folder_name):
            raise OSError('Folder does not exist.')
        else:
            path = os.path.join(folder_name, file_name)
            if head_row > 0:
                data = pd.read_csv(path, index_col=index_col, skiprows=[i for i in range(head_row - 1)])
            else:
                data = pd.read_csv(path, index_col=index_col)
            try:
                data.index = pd.to_datetime(data.index, format='%m/%d/%y %H:%M')
            except:
                data.index = pd.to_datetime(data.index, dayfirst=False, infer_datetime_format=True)
        if convert_col:
            for col in data.columns:
                if data[col].dtype != np.number:
                    data[col] = pd.to_numeric(data[col], errors='coerce')
        return data","
class Import_Data:
    ''' This class imports data from csv files '''

    def __init__(self):
        ''' Constructor.
        This class stores the imported data.
        '''
        pass

    def import_csv(self, file_name='*', folder_name='.', head_row=0, index_col=0, convert_col=True, concat_files=False):
        ''' Imports csv file(s) and stores the result in data.
        Note
        ----
        1. If folder exists out of current directory, folder_name should contain correct regex
        2. Assuming there's no file called ""\*.csv""
        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '\*', i.e. all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.', i.e. current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe
        '''
        pass

    def _load_csv(self, file_name, folder_name, head_row, index_col, convert_col, concat_files):
        ''' Load single csv file.
        Parameters
        ----------
        file_name       : str
            CSV file to be imported. Defaults to '*' - all csv files in the folder.
        folder_name     : str
            Folder where file resides. Defaults to '.' - current directory.
        head_row        : int
            Skips all rows from 0 to head_row-1
        index_col       : int
            Skips all columns from 0 to index_col-1
        convert_col     : bool
            Convert columns to numeric type
        concat_files    : bool
            Appends data from files to result dataframe
        Returns
        -------
        pd.DataFrame()
            Dataframe containing csv data
        '''
        pass",4,4,45.0,7.0,19.0,19.0,7.0,0.97,0.0,7.0,0.0,2.0,3.0,1.0,3.0,3.0,144.0,28.0,59.0,14.0,55.0,57.0,48.0,13.0,44.0,10.0,0.0,4.0,20.0,snippet_263
323208,SoftwareDefinedBuildings/XBOS,apps/data_analysis/XBOS_data_analytics/Plot_Data.py,XBOS_data_analytics.Plot_Data.Plot_Data,"import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

class Plot_Data:
    """""" This class contains functions for displaying various plots.

    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.

    """"""
    count = 1

    def __init__(self, figsize=(18, 5)):
        """""" Constructor.

        Parameters
        ----------
        figsize : tuple
            Size of figure.

        """"""
        self.figsize = figsize

    def correlation_plot(self, data):
        """""" Create heatmap of Pearson's correlation coefficient.

        Parameters
        ----------
        data    : pd.DataFrame()
            Data to display.

        Returns
        -------
        matplotlib.figure
            Heatmap.

        """"""
        fig = plt.figure(Plot_Data.count)
        corr = data.corr()
        ax = sns.heatmap(corr)
        Plot_Data.count += 1
        return fig

    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        """""" Create baseline and projection plots.

        Parameters
        ----------
        y_true              : pd.Series()
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.Dataframe()
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.

        Returns
        -------
        matplotlib.figure
            Baseline plot

        """"""
        fig = plt.figure(Plot_Data.count)
        if projection_period:
            nrows = len(baseline_period) + len(projection_period) / 2
        else:
            nrows = len(baseline_period) / 2
        base_df = pd.DataFrame()
        base_df['y_true'] = y_true
        base_df['y_pred'] = y_pred
        ax1 = fig.add_subplot(nrows, 1, 1)
        base_df.plot(ax=ax1, figsize=self.figsize, title='Baseline Period ({}-{}). \nBest Model: {}. \nBaseline Adj R2: {}. \nSite: {}.'.format(baseline_period[0], baseline_period[1], model_name, adj_r2, site))
        if projection_period:
            num_plot = 2
            for i in range(0, len(projection_period), 2):
                ax = fig.add_subplot(nrows, 1, num_plot)
                period = slice(projection_period[i], projection_period[i + 1])
                project_df = pd.DataFrame()
                try:
                    project_df['y_true'] = data.loc[period, output_col]
                    project_df['y_pred'] = model.predict(data.loc[period, input_col])
                    project_df['y_pred'][project_df['y_pred'] < 0] = 0
                    project_df.plot(ax=ax, figsize=self.figsize, title='Projection Period ({}-{})'.format(projection_period[i], projection_period[i + 1]))
                    num_plot += 1
                    fig.tight_layout()
                    Plot_Data.count += 1
                    return (fig, project_df['y_true'], project_df['y_pred'])
                except:
                    raise SystemError('If projecting into the future, please specify project_ind_col that has data available                                         in the future time period requested.')
        return (fig, None, None)","
class Plot_Data:
    ''' This class contains functions for displaying various plots.
    Attributes
    ----------
    count    : int
        Keeps track of the number of figures.
    '''

    def __init__(self, figsize=(18, 5)):
        ''' Constructor.
        Parameters
        ----------
        figsize : tuple
            Size of figure.
        '''
        pass

    def correlation_plot(self, data):
        ''' Create heatmap of Pearson's correlation coefficient.
        Parameters
        ----------
        data    : pd.DataFrame()
            Data to display.
        Returns
        -------
        matplotlib.figure
            Heatmap.
        '''
        pass

    def baseline_projection_plot(self, y_true, y_pred, baseline_period, projection_period, model_name, adj_r2, data, input_col, output_col, model, site):
        ''' Create baseline and projection plots.
        Parameters
        ----------
        y_true              : pd.Series()
            Actual y values.
        y_pred              : np.ndarray
            Predicted y values.
        baseline_period     : list(str)
            Baseline period.
        projection_period   : list(str)
            Projection periods.
        model_name          : str
            Optimal model's name.
        adj_r2              : float
            Adjusted R2 score of optimal model.
        data                : pd.Dataframe()
            Data containing real values.
        input_col           : list(str)
            Predictor column(s).
        output_col          : str
            Target column.
        model               : func
            Optimal model.
        Returns
        -------
        matplotlib.figure
            Baseline plot
        '''
        pass",4,4,38.0,6.0,15.0,17.0,2.0,1.21,0.0,3.0,0.0,0.0,3.0,1.0,3.0,3.0,132.0,28.0,47.0,22.0,39.0,57.0,38.0,18.0,34.0,5.0,0.0,3.0,7.0,snippet_264
323615,edx/edx-django-utils,edx_edx-django-utils/edx_django_utils/db/read_replica.py,edx_django_utils.db.read_replica.ReadReplicaRouter,"from django.conf import settings

class ReadReplicaRouter:
    """"""
    A database router that by default, reads from the writer database,
    but can be overridden with a context manager to route all reads
    to the read-replica.

    See https://docs.djangoproject.com/en/2.2/topics/db/multi-db/#automatic-database-routing
    """"""

    def db_for_read(self, model, **hints):
        """"""
        Reads go the active reader name
        """"""
        return _storage.db_name if _storage.db_name in settings.DATABASES else WRITER_NAME

    def db_for_write(self, model, **hints):
        """"""
        Writes always go to the writer.
        """"""
        return WRITER_NAME

    def allow_relation(self, obj1, obj2, **hints):
        """"""
        Relations between objects are allowed if both objects are
        in either the read-replica or the writer.
        """"""
        db_list = (READ_REPLICA_NAME, WRITER_NAME)
        if obj1._state.db in db_list and obj2._state.db in db_list:
            return True
        return None

    def allow_migrate(self, db, app_label, model_name=None, **hints):
        """"""
        All non-auth models end up in this pool.
        """"""
        return True","
class ReadReplicaRouter:
    '''
    A database router that by default, reads from the writer database,
    but can be overridden with a context manager to route all reads
    to the read-replica.
    See https://docs.djangoproject.com/en/2.2/topics/db/multi-db/#automatic-database-routing
    '''

    def db_for_read(self, model, **hints):
        '''
        Reads go the active reader name
        '''
        pass

    def db_for_write(self, model, **hints):
        '''
        Writes always go to the writer.
        '''
        pass

    def allow_relation(self, obj1, obj2, **hints):
        '''
        Relations between objects are allowed if both objects are
        in either the read-replica or the writer.
        '''
        pass

    def allow_migrate(self, db, app_label, model_name=None, **hints):
        '''
        All non-auth models end up in this pool.
        '''
        pass",5,5,8.0,0.0,4.0,4.0,2.0,1.28,0.0,0.0,0.0,0.0,4.0,0.0,4.0,4.0,42.0,5.0,18.0,10.0,9.0,23.0,12.0,6.0,7.0,2.0,0.0,1.0,6.0,snippet_265
323652,edx/edx-django-utils,edx_edx-django-utils/edx_django_utils/plugins/plugin_manager.py,edx_django_utils.plugins.plugin_manager.PluginManager,"from stevedore.extension import ExtensionManager
import functools
from collections import OrderedDict

class PluginManager:
    """"""
    Base class that manages plugins for an IDA.
    """"""

    @classmethod
    @functools.lru_cache(maxsize=None)
    def get_available_plugins(cls, namespace=None):
        """"""
        Returns a dict of all the plugins that have been made available.
        """"""
        plugins = OrderedDict()
        extension_manager = ExtensionManager(namespace=namespace or cls.NAMESPACE)
        for plugin_name in extension_manager.names():
            plugins[plugin_name] = extension_manager[plugin_name].plugin
        return plugins

    @classmethod
    def get_plugin(cls, name, namespace=None):
        """"""
        Returns the plugin with the given name.
        """"""
        plugins = cls.get_available_plugins(namespace)
        if name not in plugins:
            raise PluginError('No such plugin {name} for entry point {namespace}'.format(name=name, namespace=namespace or cls.NAMESPACE))
        return plugins[name]","
class PluginManager:
    '''
    Base class that manages plugins for an IDA.
        '''
    @classmethod
    @functools.lru_cache(maxsize=None)
    def get_available_plugins(cls, namespace=None):
        '''
        Returns a dict of all the plugins that have been made available.
        '''
        pass
    @classmethod
    def get_plugin(cls, name, namespace=None):
        '''
        Returns the plugin with the given name.
        '''
        pass",3,3,14.0,0.0,9.0,5.0,2.0,0.59,0.0,2.0,1.0,1.0,0.0,0.0,2.0,2.0,36.0,2.0,22.0,9.0,16.0,13.0,12.0,7.0,9.0,2.0,0.0,1.0,4.0,snippet_266
327277,edx/django-config-models,edx_django-config-models/config_models/views.py,config_models.views.AtomicMixin,"from django.db import transaction

class AtomicMixin:
    """"""Mixin to provide atomic transaction for as_view.""""""

    @classmethod
    def create_atomic_wrapper(cls, wrapped_func):
        """"""Returns a wrapped function.""""""

        def _create_atomic_wrapper(*args, **kwargs):
            """"""Actual wrapper.""""""
            with transaction.atomic():
                return wrapped_func(*args, **kwargs)
        return _create_atomic_wrapper

    @classmethod
    def as_view(cls, **initkwargs):
        """"""Overrides as_view to add atomic transaction.""""""
        view = super().as_view(**initkwargs)
        return cls.create_atomic_wrapper(view)","
class AtomicMixin:
    '''Mixin to provide atomic transaction for as_view.'''
    @classmethod
    def create_atomic_wrapper(cls, wrapped_func):
        '''Returns a wrapped function.'''
        pass

        def _create_atomic_wrapper(*args, **kwargs):
                '''Actual wrapper.'''
                pass
    @classmethod
    def as_view(cls, **initkwargs):
        '''Overrides as_view to add atomic transaction.'''
        pass",4,4,7.0,0.0,4.0,3.0,1.0,0.64,0.0,1.0,0.0,1.0,0.0,0.0,2.0,2.0,20.0,2.0,11.0,7.0,5.0,7.0,9.0,5.0,5.0,1.0,0.0,1.0,3.0,snippet_267
339242,pawelad/pymonzo,pawelad_pymonzo/src/pymonzo/client.py,pymonzo.client.MonzoAPI,"from pymonzo.exceptions import MonzoAPIError, NoSettingsFile
from authlib.integrations.base_client import OAuthError
from pymonzo.webhooks import WebhooksResource
from pathlib import Path
from pymonzo.attachments import AttachmentsResource
from json import JSONDecodeError
from pymonzo.transactions import TransactionsResource
from pymonzo.pots import PotsResource
from pymonzo.whoami import WhoAmIResource
from pymonzo.utils import get_authorization_response_url
from urllib.parse import urlparse
from typing import Any, Optional
from pymonzo.balance import BalanceResource
import webbrowser
from pymonzo.feed import FeedResource
from pymonzo.accounts import AccountsResource
from authlib.integrations.httpx_client import OAuth2Client
from pymonzo.settings import PyMonzoSettings

class MonzoAPI:
    """"""Monzo public API client.

    To use it, you need to create a new OAuth client in [Monzo Developer Portal].
    The `Redirect URLs` should be set to `http://localhost:6600/pymonzo` and
    `Confidentiality` should be set to `Confidential` if you'd like to automatically
    refresh the access token when it expires.

    You can now use `Client ID` and `Client secret` in [`pymonzo.MonzoAPI.authorize`][]
    to finish the OAuth 2 'Authorization Code Flow' and get the API access token
    (which is by default saved to disk and refreshed when expired).

    [Monzo Developer Portal]: https://developers.monzo.com/

    Note:
        Monzo API docs: https://docs.monzo.com/
    """"""
    api_url = 'https://api.monzo.com'
    authorization_endpoint = 'https://auth.monzo.com/'
    token_endpoint = 'https://api.monzo.com/oauth2/token'
    settings_path = Path.home() / '.pymonzo'

    def __init__(self, access_token: Optional[str]=None) -> None:
        """"""Initialize Monzo API client and mount all resources.

        It expects [`pymonzo.MonzoAPI.authorize`][] to be called beforehand, so
        it can load the local settings file containing the API access token. You
        can also explicitly pass the `access_token`, but it won't be able to
        automatically refresh it once it expires.

        Arguments:
            access_token: OAuth access token. You can obtain it (and by default, save
                it to disk, so it can refresh automatically) by running
                [`pymonzo.MonzoAPI.authorize`][]. Alternatively, you can get a
                temporary access token from the [Monzo Developer Portal].

                [Monzo Developer Portal]: https://developers.monzo.com/

        Raises:
            NoSettingsFile: When the access token wasn't passed explicitly and the
                settings file couldn't be loaded.

        """"""
        if access_token:
            self._settings = PyMonzoSettings(token={'access_token': access_token})
        else:
            try:
                self._settings = PyMonzoSettings.load_from_disk(self.settings_path)
            except (FileNotFoundError, JSONDecodeError) as e:
                raise NoSettingsFile('No settings file found. You need to either run `MonzoAPI.authorize(client_id, client_secret)` to get the authorization token (and save it to disk), or explicitly pass the `access_token`.') from e
        self.session = OAuth2Client(client_id=self._settings.client_id, client_secret=self._settings.client_secret, token=self._settings.token, authorization_endpoint=self.authorization_endpoint, token_endpoint=self.token_endpoint, token_endpoint_auth_method='client_secret_post', update_token=self._update_token, base_url=self.api_url)
        self.whoami = WhoAmIResource(client=self).whoami
        '\n        Mounted Monzo `whoami` endpoint. For more information see\n        [`pymonzo.whoami.WhoAmIResource.whoami`][].\n        '
        self.accounts = AccountsResource(client=self)
        '\n        Mounted Monzo `accounts` resource. For more information see\n        [`pymonzo.accounts.AccountsResource`][].\n        '
        self.attachments = AttachmentsResource(client=self)
        '\n        Mounted Monzo `attachments` resource. For more information see\n        [`pymonzo.attachments.AttachmentsResource`][].\n        '
        self.balance = BalanceResource(client=self)
        '\n        Mounted Monzo `balance` resource. For more information see\n        [`pymonzo.balance.BalanceResource`][].\n        '
        self.feed = FeedResource(client=self)
        '\n        Mounted Monzo `feed` resource. For more information see\n        [`pymonzo.feed.FeedResource`][].\n        '
        self.pots = PotsResource(client=self)
        '\n        Mounted Monzo `pots` resource. For more information see\n        [`pymonzo.pots.PotsResource`][].\n        '
        self.transactions = TransactionsResource(client=self)
        '\n        Mounted Monzo `transactions` resource. For more information see\n        [`pymonzo.transactions.TransactionsResource`][].\n        '
        self.webhooks = WebhooksResource(client=self)
        '\n        Mounted Monzo `webhooks` resource. For more information see\n        [`pymonzo.webhooks.WebhooksResource`][].\n        '

    @classmethod
    def authorize(cls, client_id: str, client_secret: str, *, save_to_disk: bool=True, redirect_uri: str='http://localhost:6600/pymonzo') -> dict:
        """"""Use OAuth 2 'Authorization Code Flow' to get Monzo API access token.

        By default, it also saves the token to disk, so it can be loaded during
        [`pymonzo.MonzoAPI`][] initialization.

        Note:
            Monzo API docs: https://docs.monzo.com/#authentication

        Arguments:
            client_id: OAuth client ID.
            client_secret: OAuth client secret.
            save_to_disk: Whether to save the token to disk.
            redirect_uri: Redirect URI specified in OAuth client.

        Returns:
            OAuth token.
        """"""
        client = OAuth2Client(client_id=client_id, client_secret=client_secret, redirect_uri=redirect_uri, token_endpoint_auth_method='client_secret_post')
        url, state = client.create_authorization_url(cls.authorization_endpoint)
        print(f'Please visit this URL to authorize: {url}')
        webbrowser.open(url)
        parsed_url = urlparse(redirect_uri)
        assert parsed_url.hostname is not None
        assert parsed_url.port is not None
        authorization_response = get_authorization_response_url(host=parsed_url.hostname, port=parsed_url.port)
        try:
            token = client.fetch_token(url=cls.token_endpoint, authorization_response=authorization_response)
        except (OAuthError, JSONDecodeError) as e:
            raise MonzoAPIError('Error while fetching API access token') from e
        if save_to_disk:
            settings = PyMonzoSettings(client_id=client_id, client_secret=client_secret, token=token)
            settings.save_to_disk(cls.settings_path)
        return token

    def _update_token(self, token: dict, **kwargs: Any) -> None:
        """"""Update settings with refreshed access token and save it to disk.

        Arguments:
            token: OAuth access token.
            **kwargs: Extra kwargs.
        """"""
        self._settings.token = token
        if self.settings_path.exists():
            self._settings.save_to_disk(self.settings_path)","
class MonzoAPI:
    '''Monzo public API client.
    To use it, you need to create a new OAuth client in [Monzo Developer Portal].
    The `Redirect URLs` should be set to `http://localhost:6600/pymonzo` and
    `Confidentiality` should be set to `Confidential` if you'd like to automatically
    refresh the access token when it expires.
    You can now use `Client ID` and `Client secret` in [`pymonzo.MonzoAPI.authorize`][]
    to finish the OAuth 2 'Authorization Code Flow' and get the API access token
    (which is by default saved to disk and refreshed when expired).
    [Monzo Developer Portal]: https://developers.monzo.com/
    Note:
        Monzo API docs: https://docs.monzo.com/
    '''

    def __init__(self, access_token: Optional[str]=None) -> None:
        '''Initialize Monzo API client and mount all resources.
        It expects [`pymonzo.MonzoAPI.authorize`][] to be called beforehand, so
        it can load the local settings file containing the API access token. You
        can also explicitly pass the `access_token`, but it won't be able to
        automatically refresh it once it expires.
        Arguments:
            access_token: OAuth access token. You can obtain it (and by default, save
                it to disk, so it can refresh automatically) by running
                [`pymonzo.MonzoAPI.authorize`][]. Alternatively, you can get a
                temporary access token from the [Monzo Developer Portal].
                [Monzo Developer Portal]: https://developers.monzo.com/
        Raises:
            NoSettingsFile: When the access token wasn't passed explicitly and the
                settings file couldn't be loaded.
        '''
        pass
    @classmethod
    def authorize(cls, client_id: str, client_secret: str, *, save_to_disk: bool=True, redirect_uri: str='http://localhost:6600/pymonzo') -> dict:
        '''Use OAuth 2 'Authorization Code Flow' to get Monzo API access token.
        By default, it also saves the token to disk, so it can be loaded during
        [`pymonzo.MonzoAPI`][] initialization.
        Note:
            Monzo API docs: https://docs.monzo.com/#authentication
        Arguments:
            client_id: OAuth client ID.
            client_secret: OAuth client secret.
            save_to_disk: Whether to save the token to disk.
            redirect_uri: Redirect URI specified in OAuth client.
        Returns:
            OAuth token.
        '''
        pass

    def _update_token(self, token: dict, **kwargs: Any) -> None:
        '''Update settings with refreshed access token and save it to disk.
        Arguments:
            token: OAuth access token.
            **kwargs: Extra kwargs.
        '''
        pass",4,4,56.0,8.0,25.0,24.0,3.0,1.02,0.0,17.0,11.0,0.0,2.0,10.0,3.0,3.0,194.0,32.0,82.0,34.0,70.0,84.0,42.0,24.0,38.0,3.0,0.0,2.0,8.0,snippet_268
339313,chaoss/grimoirelab-kingarthur,arthur/tasks.py,arthur.tasks._TaskConfig,"import re
from grimoirelab_toolkit.introspect import find_class_properties

class _TaskConfig:
    """"""Abstract class to store task configuration options.

    This class defines how to store specific task configuration
    arguments such as scheduling or archiving options. It is not
    meant to be instantiated on its own.

    Configuration options must be defined using `property` and `setter`
    decorators. Setters must check whether the given value is valid
    or not. When it is invalid, a `ValueError` exception should be
    raised. The rationale behind this is to use these methods as
    parsers when `from_dict` class method is called. It will create
    a new instance of the subclass passing its properties from a
    dictionary.
    """"""
    KW_ARGS_ERROR_REGEX = re.compile(""^.+ got an unexpected keyword argument '(.+)'$"")

    def to_dict(self):
        """"""Returns a dict with the representation of this task configuration object.""""""
        properties = find_class_properties(self.__class__)
        config = {name: self.__getattribute__(name) for name, _ in properties}
        return config

    @classmethod
    def from_dict(cls, config):
        """"""Create an configuration object from a dictionary.

        Key,value pairs will be used to initialize a task configuration
        object. If 'config' contains invalid configuration parameters
        a `ValueError` exception will be raised.

        :param config: dictionary used to create an instance of this object

        :returns: a task config instance

        :raises ValueError: when an invalid configuration parameter is found
        """"""
        try:
            obj = cls(**config)
        except TypeError as e:
            m = cls.KW_ARGS_ERROR_REGEX.match(str(e))
            if m:
                raise ValueError(""unknown '%s' task config parameter"" % m.group(1))
            else:
                raise e
        else:
            return obj","
class _TaskConfig:
    '''Abstract class to store task configuration options.
    This class defines how to store specific task configuration
    arguments such as scheduling or archiving options. It is not
    meant to be instantiated on its own.
    Configuration options must be defined using `property` and `setter`
    decorators. Setters must check whether the given value is valid
    or not. When it is invalid, a `ValueError` exception should be
    raised. The rationale behind this is to use these methods as
    parsers when `from_dict` class method is called. It will create
    a new instance of the subclass passing its properties from a
    dictionary.
    '''

    def to_dict(self):
        '''Returns a dict with the representation of this task configuration object.'''
        pass
    @classmethod
    def from_dict(cls, config):
        '''Create an configuration object from a dictionary.
        Key,value pairs will be used to initialize a task configuration
        object. If 'config' contains invalid configuration parameters
        a `ValueError` exception will be raised.
        :param config: dictionary used to create an instance of this object
        :returns: a task config instance
        :raises ValueError: when an invalid configuration parameter is found
        '''
        pass",3,3,16.0,3.0,9.0,5.0,2.0,1.05,0.0,3.0,0.0,2.0,1.0,0.0,2.0,2.0,50.0,9.0,20.0,11.0,16.0,21.0,16.0,8.0,13.0,3.0,0.0,2.0,4.0,snippet_269
340679,solvebio/solvebio-python,solvebio_solvebio-python/solvebio/contrib/streamlit/solvebio_streamlit.py,solvebio.contrib.streamlit.solvebio_streamlit.SolveBioStreamlit,"import asyncio
import os
from solvebio_auth import SolveBioOAuth2
import solvebio
import streamlit as st

class SolveBioStreamlit:
    """"""SolveBio OAuth2 wrapper for restricting access to Streamlit apps""""""
    CLIENT_ID = os.environ.get('CLIENT_ID', 'Application (client) Id')
    CLIENT_SECRET = os.environ.get('CLIENT_SECRET', 'Application (client) secret')
    APP_URL = os.environ.get('APP_URL', 'http://localhost:5000')

    def solvebio_login_component(self, authorization_url):
        """"""Streamlit component for logging into SolveBio""""""
        st.title('Secure Streamlit App')
        st.write('\n            <h4>\n                <a target=""_self"" href=""{}"">Log in to SolveBio to continue</a>\n            </h4>\n            This app requires a SolveBio account. <br>\n            <a href=""mailto:support@solvebio.com"">Contact Support</a>\n            '.format(authorization_url), unsafe_allow_html=True)

    def get_token_from_session(self):
        """"""Reads token from streamlit session state.

        If token is in the session state then the user is authorized to use the app and vice versa.
        """"""
        if 'token' not in st.session_state:
            token = None
        else:
            token = st.session_state.token
        return token

    def wrap(self, streamlit_app):
        """"""SolveBio OAuth2 wrapper around streamlit app""""""
        oauth_client = SolveBioOAuth2(self.CLIENT_ID, self.CLIENT_SECRET)
        authorization_url = oauth_client.get_authorization_url(self.APP_URL)
        oauth_token = self.get_token_from_session()
        if oauth_token is None:
            try:
                code = st.experimental_get_query_params()['code']
                params = {}
                st.experimental_set_query_params(**params)
            except:
                self.solvebio_login_component(authorization_url)
            else:
                try:
                    oauth_token = asyncio.run(oauth_client.get_access_token(code, self.APP_URL))
                except:
                    st.error('This account is not allowed or page was refreshed. Please login again.')
                    self.solvebio_login_component(authorization_url)
                else:
                    if oauth_token.is_expired():
                        st.error('Login session has ended. Please login again.')
                        self.solvebio_login_component(authorization_url)
                    else:
                        solvebio_client = solvebio.SolveClient(token=oauth_token['access_token'], token_type=oauth_token['token_type'])
                        st.session_state.solvebio_client = solvebio_client
                        st.session_state.token = oauth_token
                        streamlit_app()
        else:
            streamlit_app()","
class SolveBioStreamlit:
    '''SolveBio OAuth2 wrapper for restricting access to Streamlit apps'''

    def solvebio_login_component(self, authorization_url):
        '''Streamlit component for logging into SolveBio'''
        pass

    def get_token_from_session(self):
        '''Reads token from streamlit session state.
        If token is in the session state then the user is authorized to use the app and vice versa.
        '''
        pass

    def wrap(self, streamlit_app):
        '''SolveBio OAuth2 wrapper around streamlit app'''
        pass",4,4,29.0,4.0,19.0,6.0,3.0,0.31,0.0,2.0,2.0,0.0,3.0,0.0,3.0,3.0,96.0,16.0,61.0,14.0,57.0,19.0,38.0,14.0,34.0,5.0,0.0,4.0,8.0,snippet_270
341459,wright-group/WrightTools,wright-group_WrightTools/WrightTools/diagrams/WMEL.py,WrightTools.diagrams.WMEL.Subplot,"import numpy as np

class Subplot:
    """"""Subplot containing WMEL.""""""

    def __init__(self, ax, energies, number_of_interactions=4, title='', title_font_size=16, state_names=None, virtual=[None], state_font_size=14, state_text_buffer=0.5, label_side='left'):
        """"""Subplot.

        Parameters
        ----------
        ax : matplotlib axis
            The axis.
        energies : 1D array-like
            Energies (scaled between 0 and 1)
        number_of_interactions : integer
            Number of interactions in diagram.
        title : string (optional)
            Title of subplot. Default is empty string.
        state_names: list of str (optional)
            list of the names of the states
        virtual: list of ints (optional)
            list of indexes of any vitual energy states
        state_font_size: numtype (optional)
            font size for the state lables
        state_text_buffer: numtype (optional)
            space between the energy level bars and the state labels
        """"""
        self.ax = ax
        self.energies = energies
        self.interactions = number_of_interactions
        self.state_names = state_names
        for i in range(len(self.energies)):
            if i in virtual:
                linestyle = '--'
            else:
                linestyle = '-'
            self.ax.axhline(self.energies[i], color='k', linewidth=2, ls=linestyle, zorder=5)
        if isinstance(state_names, list):
            for i in range(len(self.energies)):
                if label_side == 'left':
                    ax.text(-state_text_buffer, energies[i], state_names[i], fontsize=state_font_size, verticalalignment='center', horizontalalignment='center')
                elif label_side == 'right':
                    ax.text(1 + state_text_buffer, energies[i], state_names[i], fontsize=state_font_size, verticalalignment='center', horizontalalignment='center')
        self.x_pos = np.linspace(0, 1, number_of_interactions)
        self.ax.set_xlim(-0.1, 1.1)
        self.ax.set_ylim(-0.01, 1.01)
        self.ax.axis('off')
        self.ax.set_title(title, fontsize=title_font_size)

    def add_arrow(self, index, between, kind, label='', head_length=10, head_aspect=1, font_size=14, color='k'):
        """"""Add an arrow to the WMEL diagram.

        Parameters
        ----------
        index : integer
            The interaction, or start and stop interaction for the arrow.
        between : 2-element iterable of integers
            The inital and final state of the arrow
        kind : {'ket', 'bra', 'outbra', 'outket'}
            The kind of interaction.
        label : string (optional)
            Interaction label. Default is empty string.
        head_length: number (optional)
            size of arrow head
        font_size : number (optional)
            Label font size. Default is 14.
        color : matplotlib color (optional)
            Arrow color. Default is black.

        Returns
        -------
        [line,arrow_head,text]
        """"""
        if hasattr(index, 'index'):
            x_pos = list(index)
        else:
            x_pos = [index] * 2
        x_pos = [np.linspace(0, 1, self.interactions)[i] for i in x_pos]
        y_pos = [self.energies[between[0]], self.energies[between[1]]]
        arrow_length = self.energies[between[1]] - self.energies[between[0]]
        arrow_end = self.energies[between[1]]
        if arrow_length > 0:
            direction = 1
        elif arrow_length < 0:
            direction = -1
        else:
            raise ValueError('between invalid!')
        length = abs(y_pos[0] - y_pos[1])
        if kind == 'ket':
            line = self.ax.plot(x_pos, y_pos, linestyle='-', color=color, linewidth=2, zorder=9)
        elif kind == 'bra':
            line = self.ax.plot(x_pos, y_pos, linestyle='--', color=color, linewidth=2, zorder=9)
        elif kind == 'out':
            yi = np.linspace(y_pos[0], y_pos[1], 100)
            xi = np.sin((yi - y_pos[0]) * int(1 / length * 20) * 2 * np.pi * length) / 40 + x_pos[0]
            line = self.ax.plot(xi[:-5], yi[:-5], linestyle='-', color=color, linewidth=2, solid_capstyle='butt', zorder=9)
        elif kind == 'outbra':
            yi = np.linspace(y_pos[0], y_pos[1], 100)
            xi = np.sin((yi - y_pos[0]) * int(1 / length * 20) * 2 * np.pi * length) / 40 + x_pos[0]
            counter = 0
            while counter - 13 <= len(yi):
                subyi = yi[counter:counter + 15]
                subxi = xi[counter:counter + 15]
                line = self.ax.plot(subxi[:-5], subyi[:-5], linestyle='-', color=color, linewidth=2, solid_capstyle='butt', zorder=9)
                counter += 13
        else:
            raise ValueError(""kind is not 'ket', 'bra', 'out' or 'outbra'."")
        dx = x_pos[1] - x_pos[0]
        dy = y_pos[1] - y_pos[0]
        xytext = (x_pos[1] - dx * 0.01, y_pos[1] - dy * 0.01)
        annotation = self.ax.annotate('', xy=(x_pos[1], y_pos[1]), xytext=xytext, arrowprops=dict(fc=color, ec=color, shrink=0, headwidth=head_length * head_aspect, headlength=head_length, linewidth=0, zorder=10), size=25)
        text = self.ax.text(np.mean(x_pos), -0.15, label, fontsize=font_size, horizontalalignment='center')
        return (line, annotation.arrow_patch, text)","
class Subplot:
    '''Subplot containing WMEL.'''

    def __init__(self, ax, energies, number_of_interactions=4, title='', title_font_size=16, state_names=None, virtual=[None], state_font_size=14, state_text_buffer=0.5, label_side='left'):
        '''Subplot.
        Parameters
        ----------
        ax : matplotlib axis
            The axis.
        energies : 1D array-like
            Energies (scaled between 0 and 1)
        number_of_interactions : integer
            Number of interactions in diagram.
        title : string (optional)
            Title of subplot. Default is empty string.
        state_names: list of str (optional)
            list of the names of the states
        virtual: list of ints (optional)
            list of indexes of any vitual energy states
        state_font_size: numtype (optional)
            font size for the state lables
        state_text_buffer: numtype (optional)
            space between the energy level bars and the state labels
        '''
        pass

    def add_arrow(self, index, between, kind, label='', head_length=10, head_aspect=1, font_size=14, color='k'):
        '''Add an arrow to the WMEL diagram.
        Parameters
        ----------
        index : integer
            The interaction, or start and stop interaction for the arrow.
        between : 2-element iterable of integers
            The inital and final state of the arrow
        kind : {'ket', 'bra', 'outbra', 'outket'}
            The kind of interaction.
        label : string (optional)
            Interaction label. Default is empty string.
        head_length: number (optional)
            size of arrow head
        font_size : number (optional)
            Label font size. Default is 14.
        color : matplotlib color (optional)
            Arrow color. Default is black.
        Returns
        -------
        [line,arrow_head,text]
        '''
        pass",3,3,98.0,4.0,69.0,26.0,8.0,0.38,0.0,5.0,0.0,0.0,2.0,5.0,2.0,2.0,199.0,10.0,138.0,49.0,113.0,53.0,55.0,27.0,52.0,9.0,0.0,3.0,16.0,snippet_271
341765,peopledoc/django-agnocomplete,peopledoc_django-agnocomplete/agnocomplete/views.py,agnocomplete.views.UserContextFormViewMixin,"class UserContextFormViewMixin:
    """"""
    This mixin is injecting the context variable into the form kwargs
    """"""

    def get_agnocomplete_context(self):
        """"""
        Return the view current user.

        You may want to change this value by overrding this method.
        """"""
        return self.request.user

    def get_form_kwargs(self):
        """"""
        Return the form kwargs.

        This method injects the context variable, defined in
        :meth:`get_agnocomplete_context`. Override this method to adjust it to
        your needs.
        """"""
        data = super().get_form_kwargs()
        data.update({'user': self.get_agnocomplete_context()})
        return data","class UserContextFormViewMixin:
    '''
    This mixin is injecting the context variable into the form kwargs
    '''

    def get_agnocomplete_context(self):
        '''
        Return the view current user.
        You may want to change this value by overrding this method.
        '''
        pass

    def get_form_kwargs(self):
        '''
        Return the form kwargs.
        This method injects the context variable, defined in
        :meth:`get_agnocomplete_context`. Override this method to adjust it to
        your needs.
        '''
        pass",3,3,10.0,1.0,4.0,5.0,1.0,1.44,0.0,1.0,0.0,2.0,2.0,0.0,2.0,2.0,26.0,4.0,9.0,4.0,6.0,13.0,7.0,4.0,4.0,1.0,0.0,0.0,2.0,snippet_272
343261,danpoland/pyramid-restful-framework,danpoland_pyramid-restful-framework/pyramid_restful/pagination/base.py,pyramid_restful.pagination.base.BasePagination,"class BasePagination:
    """"""
    The base class each Pagination class should implement.
    """"""

    def paginate_query(self, query, request):
        """"""
        :param query: SQLAlchemy ``query``.
        :param request: The request from the view
        :return: The paginated date based on the provided query and request.
        """"""
        raise NotImplementedError('paginate_query() must be implemented.')

    def get_paginated_response(self, data):
        """"""
        :param data: The paginated data.
        :return: A response containing the paginated data.
        """"""
        raise NotImplementedError('get_paginated_response() must be implemented.')","class BasePagination:
    '''
    The base class each Pagination class should implement.
        '''

    def paginate_query(self, query, request):
        '''
        :param query: SQLAlchemy ``query``.
        :param request: The request from the view
        :return: The paginated date based on the provided query and request.
        '''
        pass

    def get_paginated_response(self, data):
        '''
        :param data: The paginated data.
        :return: A response containing the paginated data.
        '''
        pass",3,3,8.0,1.0,2.0,6.0,1.0,2.8,0.0,1.0,0.0,1.0,2.0,0.0,2.0,2.0,21.0,4.0,5.0,3.0,2.0,14.0,5.0,3.0,2.0,1.0,0.0,0.0,2.0,snippet_273
343270,danpoland/pyramid-restful-framework,danpoland_pyramid-restful-framework/pyramid_restful/permissions.py,pyramid_restful.permissions.BasePermission,"class BasePermission:
    """"""
    All permission classes should inherit from this class.
    """"""
    message = None

    def has_permission(self, request, view):
        """"""
        Checked on every request to a view. Return ``True`` if permission is granted else ``False``.

        :param request: The request sent to the view.
        :param view: The instance of the view being accessed.
        :return: Boolean
        """"""
        return True

    def has_object_permission(self, request, view, obj):
        """"""
        Checked when a request is for a specific object. Return ``True`` if permission is granted else ``False``.

        :param request: The request sent to the view.
        :param view:  The instance of the view being accessed.
        :param obj: The object being accessed.
        :return: Boolean
        """"""
        return True","class BasePermission:
    '''
    All permission classes should inherit from this class.
        '''

    def has_permission(self, request, view):
        '''
        Checked on every request to a view. Return ``True`` if permission is granted else ``False``.
        :param request: The request sent to the view.
        :param view: The instance of the view being accessed.
        :return: Boolean
        '''
        pass

    def has_object_permission(self, request, view, obj):
        '''
        Checked when a request is for a specific object. Return ``True`` if permission is granted else ``False``.
        :param request: The request sent to the view.
        :param view:  The instance of the view being accessed.
        :param obj: The object being accessed.
        :return: Boolean
        '''
        pass",3,3,11.0,2.0,2.0,7.0,1.0,2.83,0.0,0.0,0.0,0.0,2.0,0.0,2.0,2.0,30.0,7.0,6.0,4.0,3.0,17.0,6.0,4.0,3.0,1.0,0.0,0.0,2.0,snippet_274
344249,openid/JWTConnect-Python-CryptoJWT,src/cryptojwt/jws/__init__.py,cryptojwt.jws.Signer,"class Signer:
    """"""Abstract base class for signing algorithms.""""""

    def sign(self, msg, key):
        """"""Sign ``msg`` with ``key`` and return the signature.""""""
        raise NotImplementedError()

    def verify(self, msg, sig, key):
        """"""Return True if ``sig`` is a valid signature for ``msg``.""""""
        raise NotImplementedError()","class Signer:
    '''Abstract base class for signing algorithms.'''

    def sign(self, msg, key):
        '''Sign ``msg`` with ``key`` and return the signature.'''
        pass

    def verify(self, msg, sig, key):
        '''Return True if ``sig`` is a valid signature for ``msg``.'''
        pass",3,3,3.0,0.0,2.0,1.0,1.0,0.6,0.0,1.0,0.0,5.0,2.0,0.0,2.0,2.0,10.0,2.0,5.0,3.0,2.0,3.0,5.0,3.0,2.0,1.0,0.0,0.0,2.0,snippet_275
347062,adobe-apiplatform/umapi-client.py,umapi_client/api.py,umapi_client.api.QuerySingle,"class QuerySingle:
    """"""
    Look for a single object
    """"""

    def __init__(self, connection, object_type, url_params=None, query_params=None):
        """"""
        Provide the connection and query parameters when you create the query.

        :param connection: The Connection to run the query against
        :param object_type: The type of object being queried (e.g., ""user"" or ""group"")
        :param url_params: Query qualifiers that go in the URL path (e.g., a group name when querying users)
        :param query_params: Query qualifiers that go in the query string (e.g., a domain name)
        """"""
        self.conn = connection
        self.object_type = object_type
        self.url_params = url_params if url_params else []
        self.query_params = query_params if query_params else {}
        self._result = None

    def reload(self):
        """"""
        Rerun the query (lazily).
        The result will contain a value on the server side that have changed since the last run.
        :return: None
        """"""
        self._result = None

    def _fetch_result(self):
        """"""
        Fetch the queried object.
        """"""
        self._result = self.conn.query_single(self.object_type, self.url_params, self.query_params)

    def result(self):
        """"""
        Fetch the result, if we haven't already or if reload has been called.
        :return: the result object of the query.
        """"""
        if self._result is None:
            self._fetch_result()
        return self._result","class QuerySingle:
    '''
    Look for a single object
    '''

    def __init__(self, connection, object_type, url_params=None, query_params=None):
        '''
        Provide the connection and query parameters when you create the query.
        :param connection: The Connection to run the query against
        :param object_type: The type of object being queried (e.g., ""user"" or ""group"")
        :param url_params: Query qualifiers that go in the URL path (e.g., a group name when querying users)
        :param query_params: Query qualifiers that go in the query string (e.g., a domain name)
        '''
        pass

    def reload(self):
        '''
        Rerun the query (lazily).
        The result will contain a value on the server side that have changed since the last run.
        :return: None
        '''
        pass

    def _fetch_result(self):
        '''
        Fetch the queried object.
        '''
        pass

    def result(self):
        '''
        Fetch the result, if we haven't already or if reload has been called.
        :return: the result object of the query.
        '''
        pass",5,5,9.0,0.0,4.0,5.0,2.0,1.53,0.0,0.0,0.0,1.0,4.0,5.0,4.0,4.0,42.0,4.0,15.0,10.0,10.0,23.0,15.0,10.0,10.0,3.0,0.0,1.0,7.0,snippet_276
347189,facelessuser/backrefs,facelessuser_backrefs/backrefs/util.py,backrefs.util.Immutable,"from typing import Any, Callable, AnyStr

class Immutable:
    """"""Immutable.""""""
    __slots__: tuple[Any, ...] = ()

    def __init__(self, **kwargs: Any) -> None:
        """"""Initialize.""""""
        for k, v in kwargs.items():
            super().__setattr__(k, v)

    def __setattr__(self, name: str, value: Any) -> None:
        """"""Prevent mutability.""""""
        raise AttributeError('Class is immutable!')","
class Immutable:
    '''Immutable.'''

    def __init__(self, **kwargs: Any) -> None:
        '''Initialize.'''
        pass

    def __setattr__(self, name: str, value: Any) -> None:
        '''Prevent mutability.'''
        pass",3,3,5.0,1.0,3.0,1.0,2.0,0.43,0.0,4.0,0.0,4.0,2.0,0.0,2.0,2.0,15.0,5.0,7.0,5.0,4.0,3.0,7.0,5.0,4.0,2.0,0.0,1.0,3.0,snippet_277
347503,inveniosoftware/invenio-records,inveniosoftware_invenio-records/invenio_records/dumpers/base.py,invenio_records.dumpers.base.Dumper,"from copy import deepcopy

class Dumper:
    """"""Interface for dumpers.""""""

    def dump(self, record, data):
        """"""Dump a record that can be used a source document for the search engine.

        The job of this method is to create a Python dictionary from the record
        provided in the argument.

        If you overwrite this method without calling super, then you should
        ensure that you make a deep copy of the record dictionary, to avoid
        that changes to the dump affects the record.

        :param record: The record to dump.
        :param data: The initial dump data passed in by ``record.dumps()``.
        """"""
        data.update(deepcopy(dict(record)))
        return data

    def load(self, data, record_cls):
        """"""Load a record from the source document of a search engine hit.

        The job of this method, is to create a record of type ``record_cls``
        based on the input ``data``.

        :param data: A Python dictionary representing the data to load.
        :param records_cls: The record class to be constructed.
        :returns: A instance of ``record_cls``.
        """"""
        raise NotImplementedError()","
class Dumper:
    '''Interface for dumpers.'''

    def dump(self, record, data):
        '''Dump a record that can be used a source document for the search engine.
        The job of this method is to create a Python dictionary from the record
        provided in the argument.
        If you overwrite this method without calling super, then you should
        ensure that you make a deep copy of the record dictionary, to avoid
        that changes to the dump affects the record.
        :param record: The record to dump.
        :param data: The initial dump data passed in by ``record.dumps()``.
        '''
        pass

    def load(self, data, record_cls):
        '''Load a record from the source document of a search engine hit.
        The job of this method, is to create a record of type ``record_cls``
        based on the input ``data``.
        :param data: A Python dictionary representing the data to load.
        :param records_cls: The record class to be constructed.
        :returns: A instance of ``record_cls``.
        '''
        pass",3,3,16.0,3.0,3.0,11.0,1.0,3.83,0.0,2.0,0.0,1.0,2.0,0.0,2.0,2.0,36.0,7.0,6.0,3.0,3.0,23.0,6.0,3.0,3.0,1.0,0.0,0.0,2.0,snippet_278
347507,inveniosoftware/invenio-records,inveniosoftware_invenio-records/invenio_records/dumpers/search.py,invenio_records.dumpers.search.SearchDumperExt,"class SearchDumperExt:
    """"""Interface for Search dumper extensions.""""""

    def dump(self, record, data):
        """"""Dump the data.""""""

    def load(self, data, record_cls):
        """"""Load the data.

        Reverse the changes made by the dump method.
        """"""","class SearchDumperExt:
    '''Interface for Search dumper extensions.'''

    def dump(self, record, data):
        '''Dump the data.'''
        pass

    def load(self, data, record_cls):
        '''Load the data.
        Reverse the changes made by the dump method.
        '''
        pass",3,3,4.0,1.0,1.0,2.0,1.0,1.67,0.0,0.0,0.0,3.0,2.0,0.0,2.0,2.0,11.0,3.0,3.0,3.0,0.0,5.0,3.0,3.0,0.0,1.0,0.0,0.0,2.0,snippet_279
347521,inveniosoftware/invenio-records,inveniosoftware_invenio-records/invenio_records/systemfields/base.py,invenio_records.systemfields.base.SystemFieldContext,"class SystemFieldContext:
    """"""Base class for a system field context.

    A system field context is created once you access a field's attribute on
    a class. As the system field may be defined on a super class, this context
    allows us to know from which class the field was accessed.

    Normally you should subclass this class, and implement methods the methods
    on it that requires you to know the record class.
    """"""

    def __init__(self, field, record_cls):
        """"""Initialise the field context.""""""
        self._field = field
        self._record_cls = record_cls

    @property
    def field(self):
        """"""Access the field to prevent it from being overwritten.""""""
        return self._field

    @property
    def record_cls(self):
        """"""Record class to prevent it from being overwritten.""""""
        return self._record_cls","class SystemFieldContext:
    '''Base class for a system field context.
    A system field context is created once you access a field's attribute on
    a class. As the system field may be defined on a super class, this context
    allows us to know from which class the field was accessed.
    Normally you should subclass this class, and implement methods the methods
    on it that requires you to know the record class.
        '''

    def __init__(self, field, record_cls):
        '''Initialise the field context.'''
        pass
    @property
    def field(self):
        '''Access the field to prevent it from being overwritten.'''
        pass
    @property
    def record_cls(self):
        '''Record class to prevent it from being overwritten.'''
        pass",4,4,3.0,0.0,2.0,1.0,1.0,1.0,0.0,0.0,0.0,1.0,3.0,2.0,3.0,3.0,25.0,5.0,10.0,8.0,4.0,10.0,8.0,6.0,4.0,1.0,0.0,0.0,3.0,snippet_280
348021,edx/edx-val,edx_edx-val/edxval/models.py,edxval.models.ModelFactoryWithValidation,"class ModelFactoryWithValidation:
    """"""
    A Model mixin that provides validation-based factory methods.
    """"""

    @classmethod
    def create_with_validation(cls, *args, **kwargs):
        """"""
        Factory method that creates and validates the model object before it is saved.
        """"""
        ret_val = cls(*args, **kwargs)
        ret_val.full_clean()
        ret_val.save()
        return ret_val

    @classmethod
    def get_or_create_with_validation(cls, *args, **kwargs):
        """"""
        Factory method that gets or creates-and-validates the model object before it is saved.
        Similar to the get_or_create method on Models, it returns a tuple of (object, created),
        where created is a boolean specifying whether an object was created.
        """"""
        try:
            return (cls.objects.get(*args, **kwargs), False)
        except cls.DoesNotExist:
            return (cls.create_with_validation(*args, **kwargs), True)","class ModelFactoryWithValidation:
    '''
    A Model mixin that provides validation-based factory methods.
    '''
    @classmethod
    def create_with_validation(cls, *args, **kwargs):
        '''
        Factory method that creates and validates the model object before it is saved.
        '''
        pass
    @classmethod
    def get_or_create_with_validation(cls, *args, **kwargs):
        '''
        Factory method that gets or creates-and-validates the model object before it is saved.
        Similar to the get_or_create method on Models, it returns a tuple of (object, created),
        where created is a boolean specifying whether an object was created.
        '''
        pass",3,3,9.0,0.0,5.0,5.0,2.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,2.0,2.0,26.0,2.0,13.0,6.0,8.0,13.0,11.0,4.0,8.0,2.0,0.0,1.0,3.0,snippet_281
348074,edx/edx-val,edx_edx-val/edxval/transcript_utils.py,edxval.transcript_utils.Transcript,"import json
from pysrt import SubRipFile, SubRipItem, SubRipTime
from edxval.exceptions import TranscriptsGenerationException
from pysrt.srtexc import Error

class Transcript:
    """"""
    Container for transcript methods.
    """"""
    SRT = 'srt'
    SJSON = 'sjson'

    @staticmethod
    def generate_sjson_from_srt(srt_subs):
        """"""
        Generate transcripts from sjson to SubRip (*.srt).

        Arguments:
            srt_subs(SubRip): ""SRT"" subs object

        Returns:
            Subs converted to ""SJSON"" format.
        """"""
        sub_starts = []
        sub_ends = []
        sub_texts = []
        for sub in srt_subs:
            sub_starts.append(sub.start.ordinal)
            sub_ends.append(sub.end.ordinal)
            sub_texts.append(sub.text.replace('\n', ' '))
        sjson_subs = {'start': sub_starts, 'end': sub_ends, 'text': sub_texts}
        return sjson_subs

    @staticmethod
    def generate_srt_from_sjson(sjson_subs):
        """"""
        Generate transcripts from sjson to SubRip (*.srt)

        Arguments:
            sjson_subs (dict): `sjson` subs.

        Returns:
            Subtitles in SRT format.
        """"""
        output = ''
        equal_len = len(sjson_subs['start']) == len(sjson_subs['end']) == len(sjson_subs['text'])
        if not equal_len:
            return output
        for i in range(len(sjson_subs['start'])):
            item = SubRipItem(index=i, start=SubRipTime(milliseconds=sjson_subs['start'][i]), end=SubRipTime(milliseconds=sjson_subs['end'][i]), text=sjson_subs['text'][i])
            output += str(item)
            output += '\n'
        return output

    @classmethod
    def convert(cls, content, input_format, output_format):
        """"""
        Convert transcript `content` from `input_format` to `output_format`.

        Arguments:
            content: Transcript content byte-stream.
            input_format: Input transcript format.
            output_format: Output transcript format.

        Accepted input formats: sjson, srt.
        Accepted output format: srt, sjson.

        Raises:
            TranscriptsGenerationException: On parsing the invalid srt
            content during conversion from srt to sjson.
        """"""
        assert input_format in ('srt', 'sjson')
        assert output_format in ('srt', 'sjson')
        try:
            content = content.decode('utf-8-sig')
        except UnicodeDecodeError:
            content = content.decode('latin-1')
        if input_format == output_format:
            return content
        if input_format == 'srt':
            if output_format == 'sjson':
                try:
                    srt_subs = SubRipFile.from_string(content, error_handling=SubRipFile.ERROR_RAISE)
                except Error as ex:
                    raise TranscriptsGenerationException(str(ex)) from ex
                return json.dumps(cls.generate_sjson_from_srt(srt_subs))
        if input_format == 'sjson':
            if output_format == 'srt':
                return cls.generate_srt_from_sjson(json.loads(content))","
class Transcript:
    '''
    Container for transcript methods.
    '''
    @staticmethod
    def generate_sjson_from_srt(srt_subs):
        '''
        Generate transcripts from sjson to SubRip (*.srt).
        Arguments:
            srt_subs(SubRip): ""SRT"" subs object
        Returns:
            Subs converted to ""SJSON"" format.
        '''
        pass
    @staticmethod
    def generate_srt_from_sjson(sjson_subs):
        '''
        Generate transcripts from sjson to SubRip (*.srt)
        Arguments:
            sjson_subs (dict): `sjson` subs.
        Returns:
            Subtitles in SRT format.
        '''
        pass
    @classmethod
    def convert(cls, content, input_format, output_format):
        '''
        Convert transcript `content` from `input_format` to `output_format`.
        Arguments:
            content: Transcript content byte-stream.
            input_format: Input transcript format.
            output_format: Output transcript format.
        Accepted input formats: sjson, srt.
        Accepted output format: srt, sjson.
        Raises:
            TranscriptsGenerationException: On parsing the invalid srt
            content during conversion from srt to sjson.
        '''
        pass",4,4,32.0,6.0,16.0,11.0,4.0,0.67,0.0,4.0,1.0,0.0,0.0,0.0,3.0,3.0,109.0,20.0,54.0,20.0,47.0,36.0,42.0,16.0,38.0,8.0,0.0,3.0,13.0,snippet_282
350782,druids/django-chamber,druids_django-chamber/chamber/commands/__init__.py,chamber.commands.ProgressBarStream,"class ProgressBarStream:
    """"""
    OutputStream wrapper to remove default linebreak at line endings.
    """"""

    def __init__(self, stream):
        """"""
        Wrap the given stream.
        """"""
        self.stream = stream

    def write(self, *args, **kwargs):
        """"""
        Call the stream's write method without linebreaks at line endings.
        """"""
        return self.stream.write(*args, ending='', **kwargs)

    def flush(self):
        """"""
        Call the stream's flush method without any extra arguments.
        """"""
        return self.stream.flush()","class ProgressBarStream:
    '''
    OutputStream wrapper to remove default linebreak at line endings.
        '''

    def __init__(self, stream):
        '''
        Wrap the given stream.
        '''
        pass

    def write(self, *args, **kwargs):
        '''
        Call the stream's write method without linebreaks at line endings.
        '''
        pass

    def flush(self):
        '''
        Call the stream's flush method without any extra arguments.
        '''
        pass",4,4,5.0,0.0,2.0,3.0,1.0,1.71,0.0,0.0,0.0,0.0,3.0,1.0,3.0,3.0,22.0,3.0,7.0,5.0,3.0,12.0,7.0,5.0,3.0,1.0,0.0,0.0,3.0,snippet_283
350920,pytroll/trollimage,pytroll_trollimage/trollimage/_xrimage_rasterio.py,trollimage._xrimage_rasterio.RIODataset,"from rasterio.enums import Resampling
from rasterio.windows import Window

class RIODataset:
    """"""A wrapper for a rasterio dataset.""""""

    def __init__(self, rfile, overviews=None, overviews_resampling=None, overviews_minsize=256):
        """"""Init the rasterio dataset.""""""
        self.rfile = rfile
        self.overviews = overviews
        if overviews_resampling is None:
            overviews_resampling = 'nearest'
        self.overviews_resampling = Resampling[overviews_resampling]
        self.overviews_minsize = overviews_minsize

    def __setitem__(self, key, item):
        """"""Put the data chunk in the image.""""""
        if len(key) == 3:
            indexes = list(range(key[0].start + 1, key[0].stop + 1, key[0].step or 1))
            y = key[1]
            x = key[2]
        else:
            indexes = 1
            y = key[0]
            x = key[1]
        chy_off = y.start
        chy = y.stop - y.start
        chx_off = x.start
        chx = x.stop - x.start
        self.rfile.write(item, window=Window(chx_off, chy_off, chx, chy), indexes=indexes)

    def close(self):
        """"""Close the file.""""""
        if self.overviews is not None:
            overviews = self.overviews
            if len(overviews) == 0:
                from rasterio.rio.overview import get_maximum_overview_level
                width = self.rfile.width
                height = self.rfile.height
                max_level = get_maximum_overview_level(width, height, self.overviews_minsize)
                overviews = [2 ** j for j in range(1, max_level + 1)]
            logger.debug('Building overviews %s with %s resampling', str(overviews), self.overviews_resampling.name)
            self.rfile.build_overviews(overviews, resampling=self.overviews_resampling)
        return self.rfile.close()","
class RIODataset:
    '''A wrapper for a rasterio dataset.'''

    def __init__(self, rfile, overviews=None, overviews_resampling=None, overviews_minsize=256):
        '''Init the rasterio dataset.'''
        pass

    def __setitem__(self, key, item):
        '''Put the data chunk in the image.'''
        pass

    def close(self):
        '''Close the file.'''
        pass",4,4,16.0,1.0,14.0,2.0,2.0,0.14,0.0,3.0,0.0,0.0,3.0,4.0,3.0,3.0,53.0,5.0,42.0,21.0,36.0,6.0,33.0,20.0,28.0,3.0,0.0,2.0,7.0,snippet_284
350922,pytroll/trollimage,pytroll_trollimage/trollimage/_xrimage_rasterio.py,trollimage._xrimage_rasterio.RIOTag,"class RIOTag:
    """"""Rasterio wrapper to allow da.store on tag.""""""

    def __init__(self, rfile, name):
        """"""Init the rasterio tag.""""""
        self.rfile = rfile
        self.name = name

    def __setitem__(self, key, item):
        """"""Put the data in the tag.""""""
        kwargs = {self.name: item.item()}
        self.rfile.update_tags(**kwargs)

    def close(self):
        """"""Close the file.""""""
        return self.rfile.close()","class RIOTag:
    '''Rasterio wrapper to allow da.store on tag.'''

    def __init__(self, rfile, name):
        '''Init the rasterio tag.'''
        pass

    def __setitem__(self, key, item):
        '''Put the data in the tag.'''
        pass

    def close(self):
        '''Close the file.'''
        pass",4,4,4.0,0.0,3.0,1.0,1.0,0.44,0.0,0.0,0.0,0.0,3.0,2.0,3.0,3.0,16.0,3.0,9.0,7.0,5.0,4.0,9.0,7.0,5.0,1.0,0.0,0.0,3.0,snippet_285
353855,inveniosoftware/invenio-communities,inveniosoftware_invenio-communities/invenio_communities/communities/records/systemfields/access.py,invenio_communities.communities.records.systemfields.access.AccessEnumMixin,"class AccessEnumMixin:
    """"""Mixin for enum functionalities.""""""

    @classmethod
    def validate(cls, level):
        """"""Validate a string against the enum values.""""""
        return cls(level) in cls

    def __str__(self):
        """"""Return its value.""""""
        return self.value","class AccessEnumMixin:
    '''Mixin for enum functionalities.'''
    @classmethod
    def validate(cls, level):
        '''Validate a string against the enum values.'''
        pass

    def __str__(self):
        '''Return its value.'''
        pass",3,3,3.0,0.0,2.0,1.0,1.0,0.5,0.0,0.0,0.0,5.0,1.0,0.0,2.0,2.0,11.0,2.0,6.0,4.0,2.0,3.0,5.0,3.0,2.0,1.0,0.0,0.0,2.0,snippet_286
354005,inveniosoftware/invenio-communities,inveniosoftware_invenio-communities/invenio_communities/roles.py,invenio_communities.roles.Role,"from dataclasses import dataclass, field

@dataclass(frozen=True)
class Role:
    """"""Role class.""""""
    name: str = ''
    'Name of the role.'
    title: str = ''
    'Title of the role.'
    description: str = ''
    'Brief description of capabilities of the role.'
    can_manage_roles: list = field(default_factory=list)
    'List of other roles that this role can manage.'
    is_owner: bool = False
    'This role is the owner role (only one can exists).'
    can_manage: bool = False
    'This role has manage permissions.'
    can_curate: bool = False
    'This role has record manage permissions.'
    can_view: bool = False
    'This role has view restricted record permissions.'

    def can_manage_role(self, role_name):
        """"""Determine if this role can manage the role name.""""""
        return role_name in self.can_manage_roles

    def __hash__(self):
        """"""Compute a hash for use with e.g. sets.""""""
        return self.name.__hash__()","@dataclass(frozen=True)
class Role:
    '''Role class.'''

    def can_manage_role(self, role_name):
        '''Determine if this role can manage the role name.'''
        pass

    def __hash__(self):
        '''Compute a hash for use with e.g. sets.'''
        pass",3,3,3.0,0.0,2.0,1.0,1.0,0.85,0.0,0.0,0.0,0.0,2.0,0.0,2.0,2.0,34.0,10.0,13.0,11.0,10.0,11.0,13.0,11.0,10.0,1.0,0.0,0.0,2.0,snippet_287
356596,grycap/RADL,grycap_RADL/radl/radl.py,radl.radl.Aspect,"import copy

class Aspect:
    """"""A network, ansible_host, system, deploy, configure or contextualize element in a RADL.""""""

    def getId(self):
        """"""Return the id of the aspect.""""""
        return id(self)

    def clone(self):
        """"""Return a copy of this aspect.""""""
        return copy.deepcopy(self)","
class Aspect:
    '''A network, ansible_host, system, deploy, configure or contextualize element in a RADL.'''

    def getId(self):
        '''Return the id of the aspect.'''
        pass

    def clone(self):
        '''Return a copy of this aspect.'''
        pass",3,3,4.0,1.0,2.0,1.0,1.0,0.6,0.0,0.0,0.0,7.0,2.0,0.0,2.0,2.0,12.0,4.0,5.0,3.0,2.0,3.0,5.0,3.0,2.0,1.0,0.0,0.0,2.0,snippet_288
357236,pytroll/posttroll,posttroll/backends/zmq/message_broadcaster.py,posttroll.backends.zmq.message_broadcaster.ZMQDesignatedReceiversSender,"import threading
from posttroll.backends.zmq.socket import close_socket, set_up_client_socket
from zmq import LINGER, NOBLOCK, REQ, ZMQError

class ZMQDesignatedReceiversSender:
    """"""Sends message to multiple *receivers* on *port*.""""""

    def __init__(self, default_port, receivers):
        """"""Set up the sender.""""""
        self.default_port = default_port
        self.receivers = receivers
        self._shutdown_event = threading.Event()

    def __call__(self, data):
        """"""Send data.""""""
        for receiver in self.receivers:
            self._send_to_address(receiver, data)

    def _send_to_address(self, address, data, timeout=10):
        """"""Send data to *address* and *port* without verification of response.""""""
        if address.find(':') == -1:
            full_address = 'tcp://%s:%d' % (address, self.default_port)
        else:
            full_address = 'tcp://%s' % address
        options = {LINGER: int(timeout * 1000)}
        socket = set_up_client_socket(REQ, full_address, options)
        try:
            socket.send_string(data)
            while not self._shutdown_event.is_set():
                try:
                    message = socket.recv_string(NOBLOCK)
                except ZMQError:
                    self._shutdown_event.wait(0.1)
                    continue
                if message != 'ok':
                    logger.warning('invalid acknowledge received: %s' % message)
                break
        finally:
            close_socket(socket)

    def close(self):
        """"""Close the sender.""""""
        self._shutdown_event.set()","
class ZMQDesignatedReceiversSender:
    '''Sends message to multiple *receivers* on *port*.'''

    def __init__(self, default_port, receivers):
        '''Set up the sender.'''
        pass

    def __call__(self, data):
        '''Send data.'''
        pass

    def _send_to_address(self, address, data, timeout=10):
        '''Send data to *address* and *port* without verification of response.'''
        pass

    def close(self):
        '''Close the sender.'''
        pass",5,5,9.0,1.0,7.0,1.0,2.0,0.2,0.0,2.0,0.0,0.0,4.0,3.0,4.0,4.0,42.0,6.0,30.0,13.0,25.0,6.0,28.0,13.0,23.0,5.0,0.0,3.0,9.0,snippet_289
357237,pytroll/posttroll,posttroll/backends/zmq/ns.py,posttroll.backends.zmq.ns.ZMQNameServer,"from posttroll.backends.zmq.socket import SocketReceiver, close_socket, set_up_client_socket, set_up_server_socket
from contextlib import suppress
from zmq import LINGER, REP, REQ
from posttroll.ns import get_active_address, get_configured_nameserver_port

class ZMQNameServer:
    """"""The name server.""""""

    def __init__(self):
        """"""Set up the nameserver.""""""
        self.running: bool = True
        self.listener: SocketReceiver | None = None
        self._authenticator = None

    def run(self, address_receiver, address: str | None=None):
        """"""Run the listener and answer to requests.""""""
        port = get_configured_nameserver_port()
        try:
            if not self.running:
                return
            if address is None:
                address = '*'
            address = create_nameserver_address(address)
            self.listener, _, self._authenticator = set_up_server_socket(REP, address)
            logger.debug(f'Nameserver listening on port {port}')
            socket_receiver = SocketReceiver()
            socket_receiver.register(self.listener)
            while self.running:
                try:
                    for msg, _ in socket_receiver.receive(self.listener, timeout=1):
                        logger.debug('Replying to request: ' + str(msg))
                        active_address = get_active_address(msg.data['service'], address_receiver, msg.version)
                        self.listener.send_unicode(str(active_address))
                except TimeoutError:
                    continue
        except KeyboardInterrupt:
            pass
        finally:
            socket_receiver.unregister(self.listener)
            self.close_sockets_and_threads()

    def close_sockets_and_threads(self):
        """"""Close all sockets and threads.""""""
        with suppress(AttributeError):
            close_socket(self.listener)
        with suppress(AttributeError):
            self._authenticator.stop()

    def stop(self):
        """"""Stop the name server.""""""
        self.running = False","
class ZMQNameServer:
    '''The name server.'''

    def __init__(self):
        '''Set up the nameserver.'''
        pass

    def run(self, address_receiver, address: str | None=None):
        '''Run the listener and answer to requests.'''
        pass

    def close_sockets_and_threads(self):
        '''Close all sockets and threads.'''
        pass

    def stop(self):
        '''Stop the name server.'''
        pass",5,5,11.0,0.0,9.0,2.0,3.0,0.18,0.0,7.0,1.0,0.0,4.0,3.0,4.0,4.0,50.0,5.0,38.0,13.0,33.0,7.0,36.0,13.0,31.0,7.0,0.0,4.0,10.0,snippet_290
357239,pytroll/posttroll,posttroll/backends/zmq/socket.py,posttroll.backends.zmq.socket.SocketReceiver,"from posttroll.message import Message
import zmq

class SocketReceiver:
    """"""A receiver for mulitple sockets.""""""

    def __init__(self):
        """"""Set up the receiver.""""""
        self._poller = zmq.Poller()

    def register(self, socket):
        """"""Register the socket.""""""
        self._poller.register(socket, zmq.POLLIN)

    def unregister(self, socket):
        """"""Unregister the socket.""""""
        self._poller.unregister(socket)

    def receive(self, *sockets, timeout=None):
        """"""Timeout is in seconds.""""""
        if timeout:
            timeout *= 1000
        socks = dict(self._poller.poll(timeout=timeout))
        if socks:
            for sock in sockets:
                if socks.get(sock) == zmq.POLLIN:
                    received = sock.recv_string(zmq.NOBLOCK)
                    yield (Message.decode(received), sock)
        else:
            raise TimeoutError('Did not receive anything on sockets.')","
class SocketReceiver:
    '''A receiver for mulitple sockets.'''

    def __init__(self):
        '''Set up the receiver.'''
        pass

    def register(self, socket):
        '''Register the socket.'''
        pass

    def unregister(self, socket):
        '''Unregister the socket.'''
        pass

    def receive(self, *sockets, timeout=None):
        '''Timeout is in seconds.'''
        pass",5,5,5.0,0.0,4.0,1.0,2.0,0.28,0.0,3.0,1.0,0.0,4.0,1.0,4.0,4.0,27.0,4.0,18.0,9.0,13.0,5.0,17.0,9.0,12.0,5.0,0.0,3.0,8.0,snippet_291
357241,pytroll/posttroll,posttroll/bbmcast.py,posttroll.bbmcast.MulticastReceiver,"import struct
from socket import AF_INET, INADDR_ANY, IP_ADD_MEMBERSHIP, IP_MULTICAST_IF, IP_MULTICAST_LOOP, IP_MULTICAST_TTL, IPPROTO_IP, SO_BROADCAST, SO_LINGER, SO_REUSEADDR, SOCK_DGRAM, SOL_IP, SOL_SOCKET, gethostbyname, inet_aton, socket, timeout

class MulticastReceiver:
    """"""Multicast receiver on *port* for an *mcgroup*.""""""
    BUFSIZE = 1024

    def __init__(self, port, mcgroup=None):
        """"""Set up the multicast receiver.""""""
        self.port = port
        self.socket, self.group = mcast_receiver(port, mcgroup)
        logger.info(f'Receiver initialized on group {self.group}.')

    def settimeout(self, tout=None):
        """"""Set timeout.

        A timeout will throw a 'socket.timeout'.
        """"""
        self.socket.settimeout(tout)
        return self

    def __call__(self):
        """"""Receive data from a socket.""""""
        data, sender = self.socket.recvfrom(self.BUFSIZE)
        return (data.decode(), sender)

    def close(self):
        """"""Close the receiver.""""""
        self.socket.setsockopt(SOL_SOCKET, SO_LINGER, struct.pack('ii', 1, 1))
        self.socket.close()","
class MulticastReceiver:
    '''Multicast receiver on *port* for an *mcgroup*.'''

    def __init__(self, port, mcgroup=None):
        '''Set up the multicast receiver.'''
        pass

    def settimeout(self, tout=None):
        '''Set timeout.
        A timeout will throw a 'socket.timeout'.
        '''
        pass

    def __call__(self):
        '''Receive data from a socket.'''
        pass

    def close(self):
        '''Close the receiver.'''
        pass",5,5,5.0,0.0,3.0,2.0,1.0,0.53,0.0,0.0,0.0,0.0,4.0,3.0,4.0,4.0,29.0,6.0,15.0,9.0,10.0,8.0,15.0,9.0,10.0,1.0,0.0,0.0,4.0,snippet_292
357242,pytroll/posttroll,posttroll/bbmcast.py,posttroll.bbmcast.MulticastSender,"class MulticastSender:
    """"""Multicast sender on *port* and *mcgroup*.""""""

    def __init__(self, port, mcgroup=None):
        """"""Set up the multicast sender.""""""
        self.port = port
        self.group = mcgroup
        self.socket, self.group = mcast_sender(mcgroup)
        logger.debug('Started multicast group %s', self.group)

    def __call__(self, data):
        """"""Send data to a socket.""""""
        self.socket.sendto(data.encode(), (self.group, self.port))

    def close(self):
        """"""Close the sender.""""""
        self.socket.close()","class MulticastSender:
    '''Multicast sender on *port* and *mcgroup*.'''

    def __init__(self, port, mcgroup=None):
        '''Set up the multicast sender.'''
        pass

    def __call__(self, data):
        '''Send data to a socket.'''
        pass

    def close(self):
        '''Close the sender.'''
        pass",4,4,4.0,0.0,3.0,1.0,1.0,0.4,0.0,0.0,0.0,0.0,3.0,3.0,3.0,3.0,17.0,3.0,10.0,7.0,6.0,4.0,10.0,7.0,6.0,1.0,0.0,0.0,3.0,snippet_293
357244,pytroll/posttroll,posttroll/listener.py,posttroll.listener.ListenerContainer,"from threading import Thread
from queue import Queue
import logging

class ListenerContainer:
    """"""Container for a listener instance.""""""
    logger = logging.getLogger(__name__ + '.ListenerContainer')

    def __init__(self, topics=None, addresses=None, nameserver='localhost', services=''):
        """"""Initialize the class.""""""
        self.listener = None
        self.output_queue = None
        self.thread = None
        self.addresses = addresses
        self.nameserver = nameserver
        if topics is not None:
            self.output_queue = Queue()
            self.listener = Listener(topics=topics, queue=self.output_queue, addresses=self.addresses, nameserver=self.nameserver, services=services)
            self.thread = Thread(target=self.listener.run, daemon=True)
            self.thread.start()

    def __setstate__(self, state):
        """"""Re-initialize the class.""""""
        self.__init__(**state)

    def restart_listener(self, topics):
        """"""Restart listener after configuration update.""""""
        if self.listener is not None:
            if self.listener.running:
                self.stop()
        self.__init__(topics=topics)

    def stop(self):
        """"""Stop listener.""""""
        self.logger.debug('Stopping listener.')
        self.listener.stop()
        if self.thread is not None:
            self.thread.join()
            self.thread = None
        self.logger.debug('Listener stopped.')","
class ListenerContainer:
    '''Container for a listener instance.'''

    def __init__(self, topics=None, addresses=None, nameserver='localhost', services=''):
        '''Initialize the class.'''
        pass

    def __setstate__(self, state):
        '''Re-initialize the class.'''
        pass

    def restart_listener(self, topics):
        '''Restart listener after configuration update.'''
        pass

    def stop(self):
        '''Stop listener.'''
        pass",5,5,10.0,1.0,7.0,2.0,2.0,0.27,0.0,3.0,1.0,0.0,4.0,5.0,4.0,4.0,46.0,8.0,30.0,11.0,25.0,8.0,27.0,11.0,22.0,3.0,0.0,2.0,8.0,snippet_294
357253,pytroll/posttroll,posttroll/message_broadcaster.py,posttroll.message_broadcaster.DesignatedReceiversSender,"from posttroll import config, message

class DesignatedReceiversSender:
    """"""Sends message to multiple *receivers* on *port*.""""""

    def __init__(self, default_port, receivers):
        """"""Set settings.""""""
        backend = config.get('backend', 'unsecure_zmq')
        if backend == 'unsecure_zmq':
            from posttroll.backends.zmq.message_broadcaster import ZMQDesignatedReceiversSender
            self._sender = ZMQDesignatedReceiversSender(default_port, receivers)
        else:
            raise NotImplementedError()

    def __call__(self, data):
        """"""Send messages from all receivers.""""""
        return self._sender(data)

    def close(self):
        """"""Close the sender.""""""
        return self._sender.close()","
class DesignatedReceiversSender:
    '''Sends message to multiple *receivers* on *port*.'''

    def __init__(self, default_port, receivers):
        '''Set settings.'''
        pass

    def __call__(self, data):
        '''Send messages from all receivers.'''
        pass

    def close(self):
        '''Close the sender.'''
        pass",4,4,5.0,0.0,4.0,1.0,1.0,0.33,0.0,2.0,1.0,0.0,3.0,1.0,3.0,3.0,18.0,2.0,12.0,7.0,7.0,4.0,11.0,7.0,6.0,2.0,0.0,1.0,4.0,snippet_295
357255,pytroll/posttroll,posttroll/ns.py,posttroll.ns.NameServer,"from posttroll.address_receiver import AddressReceiver
import datetime as dt
from posttroll import config
from contextlib import suppress

class NameServer:
    """"""The name server.""""""

    def __init__(self, max_age=None, multicast_enabled=True, restrict_to_localhost=False):
        """"""Initialize nameserver.""""""
        self.loop = True
        self.listener = None
        self._max_age = max_age or dt.timedelta(minutes=10)
        self._multicast_enabled = multicast_enabled
        self._restrict_to_localhost = restrict_to_localhost
        backend = config['backend']
        if backend not in ['unsecure_zmq', 'secure_zmq']:
            raise NotImplementedError(f'Did not recognize backend: {backend}')
        from posttroll.backends.zmq.ns import ZMQNameServer
        self._ns = ZMQNameServer()

    def run(self, address_receiver=None, nameserver_address=None):
        """"""Run the listener and answer to requests.""""""
        if address_receiver is None:
            address_receiver = AddressReceiver(max_age=self._max_age, multicast_enabled=self._multicast_enabled, restrict_to_localhost=self._restrict_to_localhost)
            address_receiver.start()
        try:
            return self._ns.run(address_receiver, nameserver_address)
        finally:
            with suppress(AttributeError):
                address_receiver.stop()

    def stop(self):
        """"""Stop the nameserver.""""""
        return self._ns.stop()","
class NameServer:
    '''The name server.'''

    def __init__(self, max_age=None, multicast_enabled=True, restrict_to_localhost=False):
        '''Initialize nameserver.'''
        pass

    def run(self, address_receiver=None, nameserver_address=None):
        '''Run the listener and answer to requests.'''
        pass

    def stop(self):
        '''Stop the nameserver.'''
        pass",4,4,9.0,0.0,8.0,1.0,2.0,0.16,0.0,6.0,2.0,0.0,3.0,6.0,3.0,3.0,32.0,3.0,25.0,12.0,20.0,4.0,22.0,12.0,17.0,2.0,0.0,2.0,5.0,snippet_296
357257,pytroll/posttroll,posttroll/publisher.py,posttroll.publisher.Publish,"class Publish:
    """"""The publishing context.

    See :class:`Publisher` and :class:`NoisyPublisher` for more information on the arguments.

    The publisher is selected based on the arguments, see :func:`create_publisher_from_dict_config` for
    information how the selection is done.

    Example on how to use the :class:`Publish` context::

            from posttroll.publisher import Publish
            from posttroll.message import Message
            import time

            try:
                with Publish(""my_service"", port=9000) as pub:
                    counter = 0
                    while True:
                        counter += 1
                        message = Message(""/counter"", ""info"", str(counter))
                        print(""publishing"", message)
                        pub.send(message.encode())
                        time.sleep(3)
            except KeyboardInterrupt:
                print(""terminating publisher..."")

    """"""

    def __init__(self, name, port=0, aliases=None, broadcast_interval=2, nameservers=None, min_port=None, max_port=None):
        """"""Initialize the class.""""""
        settings = {'name': name, 'port': port, 'min_port': min_port, 'max_port': max_port, 'aliases': aliases, 'broadcast_interval': broadcast_interval, 'nameservers': nameservers}
        self.publisher = create_publisher_from_dict_config(settings)

    def __enter__(self):
        """"""Enter the context.""""""
        return self.publisher.start()

    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""Exit the context.""""""
        self.publisher.stop()","class Publish:
    '''The publishing context.
    See :class:`Publisher` and :class:`NoisyPublisher` for more information on the arguments.
    The publisher is selected based on the arguments, see :func:`create_publisher_from_dict_config` for
    information how the selection is done.
    Example on how to use the :class:`Publish` context::
            from posttroll.publisher import Publish
            from posttroll.message import Message
            import time
            try:
                with Publish(""my_service"", port=9000) as pub:
                    counter = 0
                    while True:
                        counter += 1
                        message = Message(""/counter"", ""info"", str(counter))
                        print(""publishing"", message)
                        pub.send(message.encode())
                        time.sleep(3)
            except KeyboardInterrupt:
                print(""terminating publisher..."")
    '''

    def __init__(self, name, port=0, aliases=None, broadcast_interval=2, nameservers=None, min_port=None, max_port=None):
        '''Initialize the class.'''
        pass

    def __enter__(self):
        '''Enter the context.'''
        pass

    def __exit__(self, exc_type, exc_val, exc_tb):
        '''Exit the context.'''
        pass",4,4,4.0,0.0,3.0,1.0,1.0,2.09,0.0,0.0,0.0,0.0,3.0,1.0,3.0,3.0,43.0,9.0,11.0,7.0,6.0,23.0,8.0,6.0,4.0,1.0,0.0,0.0,3.0,snippet_297
357261,pytroll/posttroll,posttroll/subscriber.py,posttroll.subscriber.Subscribe,"from posttroll.message import _MAGICK

class Subscribe:
    """"""Subscriber context.

    See :class:`NSSubscriber` and :class:`Subscriber` for initialization parameters.

    The subscriber is selected based on the arguments, see :func:`create_subscriber_from_dict_config` for
    information how the selection is done.

    Example::
            del tmp

        from posttroll.subscriber import Subscribe

        with Subscribe(""a_service"", ""my_topic"",) as sub:
            for msg in sub.recv():
                print(msg)

    """"""

    def __init__(self, services='', topics=_MAGICK, addr_listener=False, addresses=None, timeout=10, translate=False, nameserver='localhost', message_filter=None):
        """"""Initialize the class.""""""
        settings = {'services': services, 'topics': topics, 'message_filter': message_filter, 'translate': translate, 'addr_listener': addr_listener, 'addresses': addresses, 'timeout': timeout, 'nameserver': nameserver}
        self.subscriber = create_subscriber_from_dict_config(settings)

    def __enter__(self):
        """"""Start the subscriber when used as a context manager.""""""
        return self.subscriber

    def __exit__(self, exc_type, exc_val, exc_tb):
        """"""Stop the subscriber when used as a context manager.""""""
        return self.subscriber.stop()","
class Subscribe:
    '''Subscriber context.
    See :class:`NSSubscriber` and :class:`Subscriber` for initialization parameters.
    The subscriber is selected based on the arguments, see :func:`create_subscriber_from_dict_config` for
    information how the selection is done.
    Example::
            del tmp
        from posttroll.subscriber import Subscribe
        with Subscribe(""a_service"", ""my_topic"",) as sub:
            for msg in sub.recv():
                print(msg)
    '''

    def __init__(self, services='', topics=_MAGICK, addr_listener=False, addresses=None, timeout=10, translate=False, nameserver='localhost', message_filter=None):
        '''Initialize the class.'''
        pass

    def __enter__(self):
        '''Start the subscriber when used as a context manager.'''
        pass

    def __exit__(self, exc_type, exc_val, exc_tb):
        '''Stop the subscriber when used as a context manager.'''
        pass",4,4,7.0,0.0,6.0,1.0,1.0,0.74,0.0,0.0,0.0,0.0,3.0,1.0,3.0,3.0,42.0,9.0,19.0,8.0,13.0,14.0,8.0,6.0,4.0,1.0,0.0,0.0,3.0,snippet_298
357263,pytroll/posttroll,posttroll/subscriber.py,posttroll.subscriber._AddressListener,"from posttroll.address_receiver import get_configured_address_port

class _AddressListener:
    """"""Listener for new addresses of interest.""""""

    def __init__(self, subscriber, services='', nameserver='localhost'):
        """"""Initialize address listener.""""""
        if isinstance(services, str):
            services = [services]
        self.services = services
        self.subscriber = subscriber
        address_publish_port = get_configured_address_port()
        self.subscriber.add_hook_sub('tcp://' + nameserver + ':' + str(address_publish_port), ['pytroll://address'], self.handle_msg)

    def handle_msg(self, msg):
        """"""Handle the message *msg*.""""""
        addr_ = msg.data['URI']
        status = msg.data.get('status', True)
        if status:
            msg_services = msg.data.get('service')
            for service in self.services:
                if not service or service in msg_services:
                    LOGGER.debug('Adding address %s %s', str(addr_), str(service))
                    self.subscriber.add(addr_)
                    break
        else:
            LOGGER.debug('Removing address %s', str(addr_))
            self.subscriber.remove(addr_)","
class _AddressListener:
    '''Listener for new addresses of interest.'''

    def __init__(self, subscriber, services='', nameserver='localhost'):
        '''Initialize address listener.'''
        pass

    def handle_msg(self, msg):
        '''Handle the message *msg*.'''
        pass",3,3,13.0,0.0,12.0,1.0,3.0,0.13,0.0,1.0,0.0,0.0,2.0,2.0,2.0,2.0,29.0,2.0,24.0,10.0,21.0,3.0,20.0,10.0,17.0,4.0,0.0,3.0,6.0,snippet_299
360979,reanahub/reana-commons,reanahub_reana-commons/reana_commons/validation/compute_backends.py,reana_commons.validation.compute_backends.ComputeBackendValidatorBase,"from typing import Dict, List, Optional
from reana_commons.errors import REANAValidationError

class ComputeBackendValidatorBase:
    """"""REANA workflow compute backend validation base class.""""""

    def __init__(self, workflow_steps: Optional[List[Dict]]=None, supported_backends: Optional[List[str]]=[]):
        """"""Validate compute backends in REANA workflow steps.

        :param workflow_steps: list of dictionaries which represents different steps involved in workflow.
        :param supported_backends: a list of the supported compute backends.
        """"""
        self.workflow_steps = workflow_steps
        self.supported_backends = supported_backends

    def validate(self) -> None:
        """"""Validate compute backends in REANA workflow.""""""
        raise NotImplementedError

    def raise_error(self, compute_backend: str, step_name: str) -> None:
        """"""Raise validation error.""""""
        raise REANAValidationError(f'''Compute backend ""{compute_backend}"" found in step ""{step_name}"" is not supported. List of supported compute backends: ""{', '.join(self.supported_backends)}""''')","
class ComputeBackendValidatorBase:
    '''REANA workflow compute backend validation base class.'''

    def __init__(self, workflow_steps: Optional[List[Dict]]=None, supported_backends: Optional[List[str]]=[]):
        '''Validate compute backends in REANA workflow steps.
        :param workflow_steps: list of dictionaries which represents different steps involved in workflow.
        :param supported_backends: a list of the supported compute backends.
        '''
        pass

    def validate(self) -> None:
        '''Validate compute backends in REANA workflow.'''
        pass

    def raise_error(self, compute_backend: str, step_name: str) -> None:
        '''Raise validation error.'''
        pass",4,4,7.0,0.0,5.0,2.0,1.0,0.47,0.0,3.0,1.0,4.0,3.0,2.0,3.0,3.0,26.0,4.0,15.0,10.0,7.0,7.0,8.0,6.0,4.0,1.0,0.0,0.0,3.0,snippet_300
361134,raymondEhlers/pachyderm,raymondEhlers_pachyderm/src/pachyderm/fit/base.py,pachyderm.fit.base.BaseFitResult,"import numpy as np
import numpy.typing as npt
from dataclasses import dataclass
from typing import TYPE_CHECKING, Any, TypeVar, cast

@dataclass
class BaseFitResult:
    """"""Base fit result.

    This represents the most basic fit result.

    Attributes:
        parameters: Names of the parameters used in the fit.
        free_parameters: Names of the free parameters used in the fit.
        fixed_parameters: Names of the fixed parameters used in the fit.
        values_at_minimum: Contains the values of the full RP fit function at the minimum. Keys are the
            names of parameters, while values are the numerical values at convergence.
        errors_on_parameters: Contains the values of the errors associated with the parameters
            determined via the fit.
        covariance_matrix: Contains the values of the covariance matrix. Keys are tuples
            with (param_name_a, param_name_b), and the values are covariance between the specified parameters.
            Note that fixed parameters are _not_ included in this matrix.
        errors: Store the errors associated with the component fit function.
    """"""
    parameters: list[str]
    free_parameters: list[str]
    fixed_parameters: list[str]
    values_at_minimum: dict[str, float]
    errors_on_parameters: dict[str, float]
    covariance_matrix: dict[tuple[str, str], float]
    errors: npt.NDArray[Any]

    @property
    def correlation_matrix(self) -> dict[tuple[str, str], float]:
        """"""The correlation matrix of the free parameters.

        These values are derived from the covariance matrix values stored in the fit.

        Note:
            This property caches the correlation matrix value so we don't have to calculate it every time.

        Args:
            None
        Returns:
            The correlation matrix of the fit result.
        """"""
        try:
            return self._correlation_matrix
        except AttributeError:

            def corr(i_name: str, j_name: str) -> float:
                """"""Calculate the correlation matrix (definition from iminuit) from the covariance matrix.""""""
                value = self.covariance_matrix[i_name, j_name] / (np.sqrt(self.covariance_matrix[i_name, i_name] * self.covariance_matrix[j_name, j_name]) + 1e-100)
                return float(value)
            matrix: dict[tuple[str, str], float] = {}
            for i_name in self.free_parameters:
                for j_name in self.free_parameters:
                    matrix[i_name, j_name] = corr(i_name, j_name)
            self._correlation_matrix = matrix
        return self._correlation_matrix","@dataclass
class BaseFitResult:
    '''Base fit result.
    This represents the most basic fit result.
    Attributes:
        parameters: Names of the parameters used in the fit.
        free_parameters: Names of the free parameters used in the fit.
        fixed_parameters: Names of the fixed parameters used in the fit.
        values_at_minimum: Contains the values of the full RP fit function at the minimum. Keys are the
            names of parameters, while values are the numerical values at convergence.
        errors_on_parameters: Contains the values of the errors associated with the parameters
            determined via the fit.
        covariance_matrix: Contains the values of the covariance matrix. Keys are tuples
            with (param_name_a, param_name_b), and the values are covariance between the specified parameters.
            Note that fixed parameters are _not_ included in this matrix.
        errors: Store the errors associated with the component fit function.
    '''
    @property
    def correlation_matrix(self) -> dict[tuple[str, str], float]:
        '''The correlation matrix of the free parameters.
        These values are derived from the covariance matrix values stored in the fit.
        Note:
            This property caches the correlation matrix value so we don't have to calculate it every time.
        Args:
            None
        Returns:
            The correlation matrix of the fit result.
        '''
        pass
    def correlation_matrix(self) -> dict[tuple[str, str], float]:
        '''Calculate the correlation matrix (definition from iminuit) from the covariance matrix.'''
        pass",3,3,24.0,4.0,11.0,9.0,3.0,1.16,0.0,5.0,0.0,1.0,1.0,1.0,1.0,1.0,65.0,11.0,25.0,9.0,21.0,29.0,21.0,8.0,18.0,4.0,0.0,3.0,5.0,snippet_301
361458,AtteqCom/zsl,AtteqCom_zsl/src/zsl/errors.py,zsl.errors.ErrorHandler,"from abc import ABCMeta, abstractmethod

class ErrorHandler:
    """"""
    Custom error handler providing a response on a particular error.
    """"""
    __metaclass__ = ABCMeta

    @abstractmethod
    def can_handle(self, e):
        """"""
        Indicator if the handler is able to handle the given exception `e`.

        :param e: The exception that shall be determined if can be handled by the handler.
        :return: `True` or `False` depending on whether the handler can/should handle the method.
        """"""
        pass

    @abstractmethod
    def handle(self, e):
        """"""
        Handle the exception.

        :param e: The handled exception.
        :return: The error response for the exception.
        """"""
        pass","
class ErrorHandler:
    '''
    Custom error handler providing a response on a particular error.
    '''
    @abstractmethod
    def can_handle(self, e):
        '''
        Indicator if the handler is able to handle the given exception `e`.
        :param e: The exception that shall be determined if can be handled by the handler.
        :return: `True` or `False` depending on whether the handler can/should handle the method.
        '''
        pass
    @abstractmethod
    def handle(self, e):
        '''
        Handle the exception.
        :param e: The handled exception.
        :return: The error response for the exception.
        '''
        pass",3,3,9.0,1.0,2.0,6.0,1.0,1.88,0.0,0.0,0.0,4.0,2.0,0.0,2.0,2.0,27.0,4.0,8.0,6.0,3.0,15.0,6.0,4.0,3.0,1.0,0.0,0.0,2.0,snippet_302
361483,AtteqCom/zsl,AtteqCom_zsl/src/zsl/resource/model_resource.py,zsl.resource.model_resource.ReadOnlyResourceMixin,"class ReadOnlyResourceMixin:
    """"""
    The mixin to be used to forbid the update/delete and create operations.
    Remember the Python's MRO and place this mixin at the right place in the inheritance declaration.

    .. automethod:: create
    .. automethod:: update
    .. automethod:: delete
    """"""
    OPERATION_CREATE = 'create'
    OPERATION_UPDATE = 'update'
    OPERATION_DELETE = 'delete'

    @staticmethod
    def create(params, args, data):
        """"""Raises exception.

        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.

        :raises ReadOnlyResourceUpdateOperationException: when accessed
        """"""
        raise ReadOnlyResourceUpdateOperationException(ReadOnlyResourceMixin.OPERATION_CREATE)

    @staticmethod
    def update(params, args, data):
        """"""Raises exception.

        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.

        :raises ReadOnlyResourceUpdateOperationException: when accessed
        """"""
        raise ReadOnlyResourceUpdateOperationException(ReadOnlyResourceMixin.OPERATION_UPDATE)

    @staticmethod
    def delete(params, args, data):
        """"""Raises exception.

        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.

        :raises ReadOnlyResourceUpdateOperationException: when accessed
        """"""
        raise ReadOnlyResourceUpdateOperationException(ReadOnlyResourceMixin.OPERATION_DELETE)","class ReadOnlyResourceMixin:
    '''
    The mixin to be used to forbid the update/delete and create operations.
    Remember the Python's MRO and place this mixin at the right place in the inheritance declaration.
    .. automethod:: create
    .. automethod:: update
    .. automethod:: delete
    '''
    @staticmethod
    def create(params, args, data):
        '''Raises exception.
        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.
        :raises ReadOnlyResourceUpdateOperationException: when accessed
        '''
        pass
    @staticmethod
    def update(params, args, data):
        '''Raises exception.
        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.
        :raises ReadOnlyResourceUpdateOperationException: when accessed
        '''
        pass
    @staticmethod
    def delete(params, args, data):
        '''Raises exception.
        Just raises ReadOnlyResourceUpdateOperationException to indicate
        that this method is not available.
        :raises ReadOnlyResourceUpdateOperationException: when accessed
        '''
        pass",4,4,9.0,2.0,2.0,5.0,1.0,1.69,0.0,1.0,1.0,1.0,0.0,0.0,3.0,3.0,46.0,11.0,13.0,10.0,6.0,22.0,10.0,7.0,6.0,1.0,0.0,0.0,3.0,snippet_303
361534,AtteqCom/zsl,AtteqCom_zsl/src/zsl/utils/command_dispatcher.py,zsl.utils.command_dispatcher.CommandDispatcher,"import inspect

class CommandDispatcher:
    """"""
    A simple class for command dictionary. A command is a function
    which can take named parameters.
    """"""

    def __init__(self):
        """"""
        Create command dictionary
        """"""
        self.commands = {}

    def command(self, fn):
        """"""
        Add method or function to dispatcher. Can be use as a nice
        decorator.

        :param fn: function or method
        :type fn: function
        :return: the same function
        :rtype: function
        """"""
        self.commands[fn.__name__] = fn
        return fn
    'alias for ``CommandDispatcher.command``'
    add_function = command

    def execute_command(self, command, args=None):
        """"""
        Execute a command

        :param command: name of the command
        :type command: str
        :param args: optional named arguments for command
        :type args: dict
        :return: the result of command
        :raises KeyError: if command is not found
        """"""
        if args is None:
            args = {}
        command_fn = self.commands[command]
        return command_fn(**args)

    def bound(self, instance):
        """"""
        Return a new dispatcher, which will switch all command functions
        with bounded methods of given instance matched by name. It will
        match only regular methods.

        :param instance: object instance
        :type instance: object
        :return: new Dispatcher
        :rtype: CommandDispatcher
        """"""
        bounded_dispatcher = CommandDispatcher()
        bounded_dispatcher.commands = self.commands.copy()
        for name in self.commands:
            method = getattr(instance, name, None)
            if method and inspect.ismethod(method) and (method.__self__ == instance):
                bounded_dispatcher.commands[name] = method
        return bounded_dispatcher","
class CommandDispatcher:
    '''
    A simple class for command dictionary. A command is a function
    which can take named parameters.
    '''

    def __init__(self):
        '''
        Create command dictionary
        '''
        pass

    def command(self, fn):
        '''
        Add method or function to dispatcher. Can be use as a nice
        decorator.
        :param fn: function or method
        :type fn: function
        :return: the same function
        :rtype: function
        '''
        pass

    def execute_command(self, command, args=None):
        '''
        Execute a command
        :param command: name of the command
        :type command: str
        :param args: optional named arguments for command
        :type args: dict
        :return: the result of command
        :raises KeyError: if command is not found
        '''
        pass

    def bound(self, instance):
        '''
        Return a new dispatcher, which will switch all command functions
        with bounded methods of given instance matched by name. It will
        match only regular methods.
        :param instance: object instance
        :type instance: object
        :return: new Dispatcher
        :rtype: CommandDispatcher
        '''
        pass",5,5,15.0,3.0,5.0,7.0,2.0,1.7,0.0,0.0,0.0,0.0,4.0,1.0,4.0,4.0,71.0,17.0,20.0,11.0,15.0,34.0,20.0,11.0,15.0,3.0,0.0,2.0,7.0,snippet_304
362561,reanahub/reana-db,reanahub_reana-db/reana_db/models.py,reana_db.models.QuotaBase,"class QuotaBase:
    """"""Quota base functionality.""""""

    def _get_quota_by_type(self, resource_type):
        """"""Aggregate quota usage by resource type.""""""

        def _get_health_status(usage, limit):
            """"""Calculate quota health status.""""""
            health = QuotaHealth.healthy
            if limit:
                percentage = usage / limit * 100
                if percentage >= 80:
                    if percentage >= 100:
                        health = QuotaHealth.critical
                    else:
                        health = QuotaHealth.warning
            return health.name
        quota_usage = 0
        quota_limit = 0
        unit = None
        for resource in self.resources:
            if resource.resource.type_ == resource_type:
                if unit and unit != resource.resource.unit:
                    raise Exception('Error while calculating quota usage. Not all resources of resource type {} use the same units.'.format(resource_type))
                unit = resource.resource.unit
                quota_usage += resource.quota_used
                if hasattr(resource, 'quota_limit'):
                    quota_limit += resource.quota_limit
        usage_dict = {'usage': {'raw': quota_usage, 'human_readable': ResourceUnit.human_readable_unit(unit, quota_usage)}}
        if quota_limit:
            usage_dict['limit'] = {'raw': quota_limit, 'human_readable': ResourceUnit.human_readable_unit(unit, quota_limit)}
            usage_dict['health'] = _get_health_status(quota_usage, quota_limit)
        return usage_dict

    def get_quota_usage(self):
        """"""Get quota usage information.""""""
        used_resource_types = set((res.resource.type_ for res in self.resources))
        return {resource_type.name: self._get_quota_by_type(resource_type) for resource_type in used_resource_types}","class QuotaBase:
    '''Quota base functionality.'''

    def _get_quota_by_type(self, resource_type):
        '''Aggregate quota usage by resource type.'''
        pass

        def _get_health_status(usage, limit):
                '''Calculate quota health status.'''
                pass

    def get_quota_usage(self):
        '''Get quota usage information.'''
        pass",4,4,22.0,2.0,18.0,2.0,4.0,0.11,0.0,4.0,2.0,3.0,2.0,0.0,2.0,2.0,58.0,7.0,46.0,12.0,42.0,5.0,30.0,12.0,26.0,6.0,0.0,3.0,11.0,snippet_305
