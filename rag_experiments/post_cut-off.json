[
  {
    "class_name": "l1.GlobalBioDTO",
    "skeleton": "@dataclass\nclass GlobalBioDTO:\n    '''Global biography data transfer object'''\n    @classmethod\n    def from_model(cls, model: 'L1Bio') -> 'GlobalBioDTO':\n        '''\n        Create DTO from database model\n        Args:\n            model (L1Bio): database model object\n        Returns:\n            GlobalBioDTO: data transfer object\n        '''\n        pass\n\n    def to_dict(self) -> dict:\n        '''\n        Convert to dictionary format\n        Returns:\n            dict: dictionary format data\n        '''\n        pass",
    "snippet_id": "snippet_1"
  },
  {
    "class_name": "l1.L1GenerationResult",
    "skeleton": "@dataclass\nclass L1GenerationResult:\n    '''L1 generation result data class'''\n\n    def to_dict(self) -> dict:\n        '''Convert to dictionary format'''\n        pass\n    @classmethod\n    def from_dict(cls, data: dict) -> 'L1GenerationResult':\n        '''Create instance from dictionary'''\n        pass",
    "snippet_id": "snippet_2"
  },
  {
    "class_name": "l1.StatusBioDTO",
    "skeleton": "@dataclass\nclass StatusBioDTO:\n    '''Status biography data transfer object'''\n    @classmethod\n    def from_model(cls, model: 'StatusBiography') -> 'StatusBioDTO':\n        '''Create DTO from database model\n        Args:\n            model (StatusBiography): database model object\n        Returns:\n            StatusBioDTO: data transfer object\n        '''\n        pass\n\n    def to_dict(self) -> dict:\n        '''Convert to dictionary format\n        Returns:\n            dict: dictionary format data\n        '''\n        pass",
    "snippet_id": "snippet_3"
  },
  {
    "class_name": "lpm_kernel.L2.memory_manager.MemoryManager",
    "skeleton": "\nclass MemoryManager:\n    '''Simple memory manager that leverages PyTorch's built-in memory optimizations.'''\n\n    def __init__(self):\n        '''Initialize the memory manager.'''\n        pass\n\n    def get_memory_info(self) -> Dict[str, Any]:\n        '''Get current memory usage information.'''\n        pass\n\n    def cleanup_memory(self, force: bool=False) -> None:\n        '''Free up memory by garbage collection and emptying CUDA cache.'''\n        pass\n\n    def get_optimal_training_config(self) -> Dict[str, Any]:\n        '''Get recommended configurations for model training based on hardware capabilities.'''\n        pass\n\n    def optimize_model_for_training(self, model):\n        '''Apply PyTorch's built-in memory optimizations for training.'''\n        pass\n\n    def optimize_training_args(self, training_args):\n        '''Configure training arguments for efficient memory usage.'''\n        pass",
    "snippet_id": "snippet_4"
  },
  {
    "class_name": "lpm_kernel.api.common.script_runner.ScriptRunner",
    "skeleton": "\nclass ScriptRunner:\n    '''Script executor, supports executing Python and Bash scripts'''\n\n    def __init__(self, log_path: str='data/local_logs/train.log'):\n        '''\n        Initialize script executor\n        Args:\n            log_path: Base path for log files\n        '''\n        pass\n\n    def _prepare_log_file(self, script_type: str) -> str:\n        '''\n        Prepare log file\n        Args:\n            script_type: Script type, used for log directory naming\n        Returns:\n            str: Complete path to the log file\n        '''\n        pass\n\n    def _check_execution_env(self) -> Dict[str, str]:\n        '''\n        Get current execution environment information, supporting docker or regular system environment\n        Returns:\n            Dict[str, str]: Dictionary containing environment type and detailed information\n        '''\n        pass\n\n    def _check_python_version(self) -> str:\n        '''\n        Get Python version information\n        Returns:\n            str: Python version information\n        '''\n        pass\n\n    def execute_script(self, script_path: str, script_type: str, is_python: bool=False, args: Optional[list]=None) -> Dict[str, Any]:\n        '''\n        Execute script\n        Args:\n            script_path: Complete path to the script\n            script_type: Script type, used for log directory naming\n            is_python: Whether it is a Python script\n            args: List of additional script parameters\n        Returns:\n            Dict[str, Any]: Execution result, including process ID, environment information and log file path\n        '''\n        pass",
    "snippet_id": "snippet_5"
  },
  {
    "class_name": "lpm_kernel.api.domains.kernel2.services.advanced_chat_service.AdvancedChatService",
    "skeleton": "\nclass AdvancedChatService:\n    '''Service for handling advanced chat mode'''\n\n    def enhance_requirement(self, request: AdvancedChatRequest) -> str:\n        '''Enhance the requirement with knowledge context'''\n        pass\n\n    def generate_solution(self, requirement: str, temperature: float) -> str:\n        '''Generate solution based on enhanced requirement'''\n        pass\n\n    def validate_solution(self, requirement: str, solution: str) -> ValidationResult:\n        '''Validate if solution meets requirements'''\n        pass\n\n    def format_solution(self, solution: str) -> str:\n        '''Format the final solution'''\n        pass\n\n    def format_final_response(self, solution: str, stream: bool=True) -> Union[Iterator[Dict[str, Any]], Dict[str, Any]]:\n        '''Format and stream the final response using base model'''\n        pass\n\n    def process_advanced_chat(self, request: AdvancedChatRequest) -> AdvancedChatResponse:\n        '''Process advanced chat request through all phases'''\n        pass",
    "snippet_id": "snippet_6"
  },
  {
    "class_name": "lpm_kernel.api.domains.kernel2.services.chat_service.ChatService",
    "skeleton": "\nclass ChatService:\n    '''Chat service for handling different types of chat interactions'''\n\n    def __init__(self):\n        '''Initialize chat service'''\n        pass\n\n    def _get_strategy_chain(self, request: ChatRequest, strategy_chain: Optional[List[Type[SystemPromptStrategy]]]=None) -> List[Type[SystemPromptStrategy]]:\n        '''\n        Get the strategy chain to use for message building\n        Args:\n            request: Chat request containing message and other parameters\n            strategy_chain: Optional list of strategy classes to use\n        Returns:\n            List of strategy classes to use\n        Raises:\n            ValueError: If strategy_chain is empty or None and no default chain is available\n        '''\n        pass\n\n    def _build_messages(self, request: ChatRequest, strategy_chain: Optional[List[Type[SystemPromptStrategy]]]=None) -> List[Dict[str, str]]:\n        '''\n        Build messages using the specified strategy chain\n        Args:\n            request: Chat request containing message and other parameters\n            strategy_chain: Optional list of strategy classes to use. If None, uses default chain\n        Returns:\n            List of message dictionaries\n        '''\n        pass\n\n    def _process_chat_response(self, chunk, full_response: Optional[Any], full_content: str) -> Tuple[Any, str, Optional[str]]:\n        '''\n        Process custom chat_response format data\n        Args:\n            chunk: Response data chunk\n            full_response: Current complete response object\n            full_content: Current accumulated content\n        Returns:\n            Tuple[Any, str, Optional[str]]: (Updated response object, Updated content, Finish reason)\n        '''\n        pass\n\n    def _process_openai_response(self, chunk, full_response: Optional[Any], full_content: str) -> Tuple[Any, str, Optional[str]]:\n        '''\n        Process OpenAI format response data\n        Args:\n            chunk: Response data chunk\n            full_response: Current complete response object\n            full_content: Current accumulated content\n        Returns:\n            Tuple[Any, str, Optional[str]]: (Updated response object, Updated content, Finish reason)\n        '''\n        pass\n\n    def collect_stream_response(self, response_iterator: Iterator[Dict[str, Any]]):\n        '''\n        Collect streaming response into a complete response\n        Args:\n            response_iterator: Streaming response iterator\n        Returns:\n            Complete response dictionary\n        '''\n        pass\n\n    def chat(self, request: ChatRequest, strategy_chain: Optional[List[Type[SystemPromptStrategy]]]=None, stream: bool=True, json_response: bool=False, client: Optional[Any]=None, model_params: Optional[Dict[str, Any]]=None, context: Optional[Any]=None) -> Union[Dict[str, Any], Iterator[Dict[str, Any]]]:\n        '''\n        Main chat method supporting both streaming and non-streaming responses\n        Args:\n            request: Chat request containing message and other parameters\n            strategy_chain: Optional list of strategy classes to use\n            stream: Whether to return a streaming response\n            json_response: Whether to request JSON formatted response from LLM\n            client: Optional OpenAI client to use. If None, uses local_llm_service.client\n            model_params: Optional model specific parameters to override defaults\n            context: Optional context to pass to strategies\n        Returns:\n            Either an iterator for streaming responses or a single response dictionary\n        '''\n        pass",
    "snippet_id": "snippet_7"
  },
  {
    "class_name": "lpm_kernel.api.domains.kernel2.services.knowledge_service.L0KnowledgeRetriever",
    "skeleton": "\nclass L0KnowledgeRetriever:\n    '''L0 knowledge retriever'''\n\n    def __init__(self, embedding_service: EmbeddingService, similarity_threshold: float=0.7, max_chunks: int=3):\n        '''\n        init L0 knowledge retriever\n        Args:\n            embedding_service: Embedding service instance\n            similarity_threshold: only return contents whose similarity bigger than this value\n            max_chunks: the maximum number of return chunks\n        '''\n        pass\n\n    def retrieve(self, query: str) -> str:\n        '''\n        retrieve L0 knowledge\n        Args:\n            query: query content\n        Returns:\n            str: structured knowledge content, or empty string if no relevant knowledge found\n        '''\n        pass",
    "snippet_id": "snippet_8"
  },
  {
    "class_name": "lpm_kernel.api.domains.kernel2.services.knowledge_service.L1KnowledgeRetriever",
    "skeleton": "\nclass L1KnowledgeRetriever:\n    '''L1 knowledge retriever'''\n\n    def __init__(self, embedding_service: EmbeddingService, similarity_threshold: float=0.7, max_shades: int=3):\n        '''\n        init L1 knowledge retriever\n        Args:\n            embedding_service: Embedding service instance\n            similarity_threshold: only return contents whose similarity bigger than this value\n            max_shades: the maximum number of return shades\n        '''\n        pass\n\n    def retrieve(self, query: str) -> str:\n        '''\n        search related L1 shades\n        Args:\n            query: query content\n        Returns:\n            str: structured knowledge content, or empty string if no relevant knowledge found\n        '''\n        pass",
    "snippet_id": "snippet_9"
  },
  {
    "class_name": "lpm_kernel.api.domains.kernel2.services.role_service.RoleService",
    "skeleton": "\nclass RoleService:\n    '''Role Service'''\n    @staticmethod\n    def create_role(request: CreateRoleRequest) -> Optional[RoleDTO]:\n        '''\n        Create new Role\n        Args:\n            request: Create Role's request object\n        Returns:\n            Optional[RoleDTO]: if success return RoleDTO, else return None\n        '''\n        pass\n    @staticmethod\n    def get_role_by_uuid(uuid: str) -> Optional[RoleDTO]:\n        '''\n        get role by UUID\n        Args:\n            uuid: Role UUID\n        Returns:\n            Optional[RoleDTO]: if found, return RoleDTO, else return None\n        '''\n        pass\n    @staticmethod\n    def get_all_roles() -> List[RoleDTO]:\n        '''\n        Get all Roles ordered by creation time in descending order\n        Returns:\n            List[RoleDTO]: Role List sorted by id desc\n        '''\n        pass\n    @staticmethod\n    def update_role(role_id: int, request: UpdateRoleRequest) -> Optional[RoleDTO]:\n        '''\n        Update Role\n        Args:\n            role_id: Role ID\n            request: Update Role's request object\n        Returns:\n            Optional[RoleDTO]: if update success return RoleDTO, else return None\n        '''\n        pass\n    @staticmethod\n    def update_role_by_uuid(uuid: str, request: UpdateRoleRequest) -> Optional[RoleDTO]:\n        '''\n        update role by UUID\n        Args:\n            uuid: Role UUID\n            request: update Role's request object\n        Returns:\n            Optional[RoleDTO]: if success return RoleDTO, else return None\n        '''\n        pass\n    @staticmethod\n    def delete_role(role_id: int) -> bool:\n        '''\n        Delete Role\n        Args:\n            role_id: Role ID\n        Returns:\n            bool: if success\n        '''\n        pass\n    @staticmethod\n    def delete_role_by_uuid(uuid: str) -> bool:\n        '''\n        Delete Role by UUID\n        Args:\n            uuid: Role UUID\n        Returns:\n            bool: if delete successfully\n        '''\n        pass\n    @staticmethod\n    def share_role(request: ShareRoleRequest) -> Optional[RoleDTO]:\n        '''\n        Share role\n        Args:\n            request: Share role request object\n        Returns:\n            Optional[RoleDTO]: if success return RoleDTO, else return None\n        '''\n        pass",
    "snippet_id": "snippet_10"
  },
  {
    "class_name": "lpm_kernel.api.domains.loads.dto.LoadDTO",
    "skeleton": "@dataclass\nclass LoadDTO:\n    '''Data Transfer Object for Load'''\n    @classmethod\n    def from_model(cls, model: Load, with_password: bool=False):\n        '''Create DTO from database model\n        Args:\n            model: Load database model instance\n            with_password (bool): Whether to include password information\n        Returns:\n            LoadDTO: Data transfer object instance\n        '''\n        pass\n\n    def to_dict(self, with_password: bool=False) -> dict:\n        '''Convert to dictionary format\n        Args:\n            with_password (bool): Whether to include password information\n        Returns:\n            dict: Dictionary representation\n        '''\n        pass",
    "snippet_id": "snippet_11"
  },
  {
    "class_name": "lpm_kernel.api.domains.trainprocess.training_params_manager.TrainingParamsManager",
    "skeleton": "\nclass TrainingParamsManager:\n    '''\n    Training parameters manager class.\n        '''\n    @classmethod\n    def _get_params_file_path(cls):\n        '''\n        Get the training parameters file path\n        '''\n        pass\n    @classmethod\n    def update_training_params(cls, params, use_previous_params=True):\n        '''\n        Update the latest training parameters and save to file\n        Args:\n            params: Dictionary containing training parameters\n            use_previous_params: Whether to use previous training parameters as base\n        '''\n        pass\n    @classmethod\n    def get_latest_training_params(cls):\n        '''\n        Get the latest training parameters from file\n        Returns:\n            dict: Dictionary containing the latest training parameters\n        '''\n        pass",
    "snippet_id": "snippet_12"
  },
  {
    "class_name": "lpm_kernel.database.migration_manager.MigrationManager",
    "skeleton": "\nclass MigrationManager:\n    '''Manages database migrations for SQLite database'''\n\n    def __init__(self, db_path):\n        '''\n        Initialize the migration manager\n        Args:\n            db_path: Path to the SQLite database file\n        '''\n        pass\n\n    def _ensure_migration_table(self):\n        '''Create migration tracking table if it doesn't exist'''\n        pass\n\n    def get_applied_migrations(self):\n        '''\n        Get list of already applied migrations\n        Returns:\n            List of applied migration versions\n        '''\n        pass\n\n    def apply_migrations(self, migrations_dir=None):\n        '''\n        Apply all pending migrations from the migrations directory\n        Args:\n            migrations_dir: Directory containing migration scripts.\n                            If None, use 'migrations' subdirectory\n        Returns:\n            List of applied migration versions\n        '''\n        pass\n\n    def downgrade_migration(self, version, migrations_dir=None):\n        '''\n        Downgrade a specific migration by version\n        Args:\n            version: Version of the migration to downgrade\n            migrations_dir: Directory containing migration scripts\n        Returns:\n            True if downgrade was successful, False otherwise\n        '''\n        pass\n\n    def downgrade_to_version(self, target_version=None, migrations_dir=None):\n        '''\n        Downgrade migrations to a specific version\n        Args:\n            target_version: Version to downgrade to (inclusive). If None, downgrade all migrations.\n            migrations_dir: Directory containing migration scripts\n        Returns:\n            List of downgraded migration versions\n        '''\n        pass\n\n    def create_migration(self, description, migrations_dir=None):\n        '''\n        Create a new migration file with template code\n        Args:\n            description: Short description of what the migration does\n            migrations_dir: Directory to create migration in\n        Returns:\n            Path to the created migration file\n        '''\n        pass",
    "snippet_id": "snippet_13"
  },
  {
    "class_name": "template_diversity.templater",
    "skeleton": "\nclass templater:\n    '''Class for generating templates for question and answer generation.\n    This class handles the creation of templates for generating both questions\n    and answers based on predefined rules and configurations.\n        '''\n\n    def __init__(self, q_dict: dict, a_dict: dict, user_name: str='', global_bio: str='', is_cot: bool=True):\n        '''Initialize the templater with question and answer dictionaries.\n        Args:\n            q_dict: Dictionary containing question type configurations.\n            a_dict: Dictionary containing answer type configurations.\n            user_name: Name of the user for personalization.\n            global_bio: Global biography for context.\n        '''\n        pass\n\n    def get_A_template(self, question_type: str) -> tuple:\n        '''Generate the answer template for a specific question type.\n        Args:\n            question_type: The type of question to generate an answer for.\n        Returns:\n            A tuple containing the answer template and a list of chosen optional types.\n        '''\n        pass\n\n    def get_Q_template(self, question_type_prompt: str) -> str:\n        '''Generate the question template based on the provided prompt.\n        Args:\n            question_type_prompt: The prompt describing the question type.\n        Returns:\n            The question generation template with the question type filled in.\n        '''\n        pass",
    "snippet_id": "snippet_14"
  },
  {
    "class_name": "mcp_use.logging.Logger",
    "skeleton": "\nclass Logger:\n    '''Centralized logger for mcp_use.\n    This class provides logging functionality with configurable levels,\n    formatters, and handlers.\n    '''\n    @classmethod\n    def get_logger(cls, name: str='mcp_use') -> logging.Logger:\n        '''Get a logger instance for the specified name.\n        Args:\n            name: Logger name, usually the module name (defaults to 'mcp_use')\n        Returns:\n            Configured logger instance\n        '''\n        pass\n    @classmethod\n    def configure(cls, level: int | str=None, format_str: str | None=None, log_to_console: bool=True, log_to_file: str | None=None) -> None:\n        '''Configure the root mcp_use logger.\n        Args:\n            level: Log level (default: DEBUG if MCP_USE_DEBUG is 2,\n            INFO if MCP_USE_DEBUG is 1,\n            otherwise WARNING)\n            format_str: Log format string (default: DEFAULT_FORMAT)\n            log_to_console: Whether to log to console (default: True)\n            log_to_file: Path to log file (default: None)\n        '''\n        pass\n    @classmethod\n    def set_debug(cls, debug_level: int=2) -> None:\n        '''Set the debug flag and update the log level accordingly.\n        Args:\n            debug_level: Debug level (0=off, 1=info, 2=debug)\n        '''\n        pass",
    "snippet_id": "snippet_15"
  },
  {
    "class_name": "mcp_use.observability.callbacks_manager.ObservabilityManager",
    "skeleton": "class ObservabilityManager:\n    '''\n    Manages observability callbacks for MCP agents.\n    This class provides a centralized way to collect and manage callbacks\n    from various observability platforms (Langfuse, Laminar, etc.).\n    '''\n\n    def __init__(self, custom_callbacks: list | None=None):\n        '''\n        Initialize the ObservabilityManager.\n        Args:\n            custom_callbacks: Optional list of custom callbacks to use instead of defaults.\n        '''\n        pass\n\n    def _collect_available_handlers(self) -> None:\n        '''Collect all available observability handlers from configured platforms.'''\n        pass\n\n    def get_callbacks(self) -> list:\n        '''\n        Get the list of callbacks to use.\n        Returns:\n            List of callbacks - either custom callbacks if provided,\n            or all available observability handlers.\n        '''\n        pass\n\n    def get_handler_names(self) -> list[str]:\n        '''\n        Get the names of available handlers.\n        Returns:\n            List of handler names (e.g., [\"Langfuse\", \"Laminar\"])\n        '''\n        pass\n\n    def has_callbacks(self) -> bool:\n        '''\n        Check if any callbacks are available.\n        Returns:\n            True if callbacks are available, False otherwise.\n        '''\n        pass\n\n    def add_callback(self, callback) -> None:\n        '''\n        Add a callback to the custom callbacks list.\n        Args:\n            callback: The callback to add.\n        '''\n        pass\n\n    def clear_callbacks(self) -> None:\n        '''Clear all custom callbacks.'''\n        pass\n\n    def __repr__(self) -> str:\n        '''String representation of the ObservabilityManager.'''\n        pass",
    "snippet_id": "snippet_16"
  },
  {
    "class_name": "deepsearcher.embedding.base.BaseEmbedding",
    "skeleton": "\nclass BaseEmbedding:\n    '''\n    Abstract base class for embedding model implementations.\n    This class defines the interface for embedding model implementations,\n    including methods for embedding queries and documents, and a property\n    for the dimensionality of the embeddings.\n    '''\n\n    def embed_query(self, text: str) -> List[float]:\n        '''\n        Embed a single query text.\n        Args:\n            text: The query text to embed.\n        Returns:\n            A list of floats representing the embedding vector.\n        '''\n        pass\n\n    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n        '''\n        Embed a list of document texts.\n        This default implementation calls embed_query for each text,\n        but implementations may override this with a more efficient batch method.\n        Args:\n            texts: A list of document texts to embed.\n        Returns:\n            A list of embedding vectors, one for each input text.\n        '''\n        pass\n\n    def embed_chunks(self, chunks: List[Chunk], batch_size: int=256) -> List[Chunk]:\n        '''\n        Embed a list of Chunk objects.\n        This method extracts the text from each chunk, embeds it in batches,\n        and updates the chunks with their embeddings.\n        Args:\n            chunks: A list of Chunk objects to embed.\n            batch_size: The number of chunks to process in each batch.\n        Returns:\n            The input list of Chunk objects, updated with embeddings.\n        '''\n        pass\n    @property\n    def dimension(self) -> int:\n        '''\n        Get the dimensionality of the embeddings.\n        Returns:\n            The number of dimensions in the embedding vectors.\n        '''\n        pass",
    "snippet_id": "snippet_17"
  },
  {
    "class_name": "deepsearcher.vector_db.base.RetrievalResult",
    "skeleton": "\nclass RetrievalResult:\n    '''\n    Represents a result retrieved from the vector database.\n    This class encapsulates the information about a retrieved document,\n    including its embedding, text content, reference, metadata, and similarity score.\n    Attributes:\n        embedding: The vector embedding of the document.\n        text: The text content of the document.\n        reference: A reference to the source of the document.\n        metadata: Additional metadata associated with the document.\n        score: The similarity score of the document to the query.\n    '''\n\n    def __init__(self, embedding: np.array, text: str, reference: str, metadata: dict, score: float=0.0):\n        '''\n        Initialize a RetrievalResult object.\n        Args:\n            embedding: The vector embedding of the document.\n            text: The text content of the document.\n            reference: A reference to the source of the document.\n            metadata: Additional metadata associated with the document.\n            score: The similarity score of the document to the query. Defaults to 0.0.\n        '''\n        pass\n\n    def __repr__(self):\n        '''\n        Return a string representation of the RetrievalResult.\n        Returns:\n            A string representation of the RetrievalResult object.\n        '''\n        pass",
    "snippet_id": "snippet_18"
  },
  {
    "class_name": "tts.MarkdownCleaner",
    "skeleton": "\nclass MarkdownCleaner:\n    '''\n    封装 Markdown 清理逻辑：直接用 MarkdownCleaner.clean_markdown(text) 即可\n    '''\n    @staticmethod\n    def _replace_inline_dollar(m: re.Match) -> str:\n        '''\n        只要捕获到完整的 \"$...$\":\n          - 如果内部有典型公式字符 => 去掉两侧 $\n          - 否则 (纯数字/货币等) => 保留 \"$...$\"\n        '''\n        pass\n    @staticmethod\n    def _replace_table_block(match: re.Match) -> str:\n        '''\n        当匹配到一个整段表格块时，回调该函数。\n        '''\n        pass\n    @staticmethod\n    def clean_markdown(text: str) -> str:\n        '''\n        主入口方法：依序执行所有正则，移除或替换 Markdown 元素\n        '''\n        pass",
    "snippet_id": "snippet_19"
  },
  {
    "class_name": "art.mcp.types.MCPResource",
    "skeleton": "@dataclass\nclass MCPResource:\n    '''Represents an MCP resource.'''\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'MCPResource':\n        '''Create a Resource from a dictionary.'''\n        pass\n\n    def to_dict(self) -> Dict[str, Any]:\n        '''Convert the resource to a dictionary.'''\n        pass",
    "snippet_id": "snippet_20"
  },
  {
    "class_name": "art.mcp.types.MCPTool",
    "skeleton": "@dataclass\nclass MCPTool:\n    '''Represents an MCP tool.'''\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'MCPTool':\n        '''Create a Tool from a dictionary.'''\n        pass\n\n    def to_dict(self) -> Dict[str, Any]:\n        '''Convert the tool to a dictionary.'''\n        pass\n\n    def to_tool_schema(self) -> Dict[str, Any]:\n        '''Convert the tool to a tool schema.'''\n        pass",
    "snippet_id": "snippet_21"
  },
  {
    "class_name": "HKUDS_DeepCode.tools.document_segmentation_server.DocumentAnalyzer",
    "skeleton": "\nclass DocumentAnalyzer:\n    '''Enhanced document analyzer using semantic content analysis instead of mechanical structure detection'''\n\n    def analyze_document_type(self, content: str) -> Tuple[str, float]:\n        '''\n        Enhanced document type analysis based on semantic content patterns\n        Returns:\n            Tuple[str, float]: (document_type, confidence_score)\n        '''\n        pass\n\n    def _calculate_weighted_score(self, content: str, indicators: Dict[str, List[str]]) -> float:\n        '''Calculate weighted semantic indicator scores'''\n        pass\n\n    def _detect_pattern_score(self, content: str, patterns: List[str]) -> float:\n        '''Detect semantic pattern matching scores'''\n        pass\n\n    def detect_segmentation_strategy(self, content: str, doc_type: str) -> str:\n        '''\n        Intelligently determine the best segmentation strategy based on content semantics rather than mechanical structure\n        '''\n        pass\n\n    def _calculate_algorithm_density(self, content: str) -> float:\n        '''Calculate algorithm content density'''\n        pass\n\n    def _calculate_concept_complexity(self, content: str) -> float:\n        '''Calculate concept complexity'''\n        pass\n\n    def _calculate_implementation_detail_level(self, content: str) -> float:\n        '''Calculate implementation detail level'''\n        pass",
    "snippet_id": "snippet_22"
  },
  {
    "class_name": "HKUDS_DeepCode.tools.git_command.GitHubURLExtractor",
    "skeleton": "\nclass GitHubURLExtractor:\n    '''提取GitHub URL的工具类'''\n    @staticmethod\n    def extract_github_urls(text: str) -> List[str]:\n        '''从文本中提取GitHub URLs'''\n        pass\n    @staticmethod\n    def extract_target_path(text: str) -> Optional[str]:\n        '''从文本中提取目标路径'''\n        pass\n    @staticmethod\n    def infer_repo_name(url: str) -> str:\n        '''从URL推断仓库名称'''\n        pass",
    "snippet_id": "snippet_23"
  },
  {
    "class_name": "HKUDS_DeepCode.tools.pdf_downloader.LocalPathExtractor",
    "skeleton": "\nclass LocalPathExtractor:\n    '''本地路径提取器'''\n    @staticmethod\n    def is_local_path(path: str) -> bool:\n        '''判断是否为本地路径'''\n        pass\n    @staticmethod\n    def extract_local_paths(text: str) -> List[str]:\n        '''从文本中提取本地文件路径'''\n        pass",
    "snippet_id": "snippet_24"
  },
  {
    "class_name": "HKUDS_DeepCode.tools.pdf_downloader.URLExtractor",
    "skeleton": "\nclass URLExtractor:\n    '''URL提取器'''\n    @staticmethod\n    def convert_arxiv_url(url: str) -> str:\n        '''将arXiv网页链接转换为PDF下载链接'''\n        pass\n    @classmethod\n    def extract_urls(cls, text: str) -> List[str]:\n        '''从文本中提取URL'''\n        pass\n    @staticmethod\n    def infer_filename_from_url(url: str) -> str:\n        '''从URL推断文件名'''\n        pass",
    "snippet_id": "snippet_25"
  },
  {
    "class_name": "claude_monitor.core.data_processors.DataConverter",
    "skeleton": "\nclass DataConverter:\n    '''Unified data conversion utilities.'''\n    @staticmethod\n    def flatten_nested_dict(data: Dict[str, Any], prefix: str='') -> Dict[str, Any]:\n        '''Flatten nested dictionary structure.\n        Args:\n            data: Nested dictionary\n            prefix: Prefix for flattened keys\n        Returns:\n            Flattened dictionary\n        '''\n        pass\n    @staticmethod\n    def extract_model_name(data: Dict[str, Any], default: str='claude-3-5-sonnet') -> str:\n        '''Extract model name from various data sources.\n        Args:\n            data: Data containing model information\n            default: Default model name if not found\n        Returns:\n            Extracted model name\n                        '''\n                        pass\n    @staticmethod\n    def to_serializable(obj: Any) -> Any:\n        '''Convert object to JSON-serializable format.\n        Args:\n            obj: Object to convert\n        Returns:\n            JSON-serializable representation\n        '''\n        pass",
    "snippet_id": "snippet_26"
  },
  {
    "class_name": "claude_monitor.core.data_processors.TimestampProcessor",
    "skeleton": "\nclass TimestampProcessor:\n    '''Unified timestamp parsing and processing utilities.'''\n\n    def __init__(self, timezone_handler: Optional[TimezoneHandler]=None) -> None:\n        '''Initialize with optional timezone handler.'''\n        pass\n\n    def parse_timestamp(self, timestamp_value: Union[str, int, float, datetime, None]) -> Optional[datetime]:\n        '''Parse timestamp from various formats to UTC datetime.\n        Args:\n            timestamp_value: Timestamp in various formats (str, int, float, datetime)\n        Returns:\n            Parsed UTC datetime or None if parsing fails\n        '''\n        pass",
    "snippet_id": "snippet_27"
  },
  {
    "class_name": "claude_monitor.core.pricing.PricingCalculator",
    "skeleton": "\nclass PricingCalculator:\n    '''Calculates costs based on model pricing with caching support.\n    This class provides methods for calculating costs for individual models/tokens\n    as well as detailed cost breakdowns for collections of usage entries.\n    It supports custom pricing configurations and caches calculations for performance.\n    Features:\n    - Configurable pricing (from config or custom)\n    - Fallback hardcoded pricing for robustness\n    - Caching for performance\n    - Support for all token types including cache\n    - Backward compatible with both APIs\n    '''\n\n    def __init__(self, custom_pricing: Optional[Dict[str, Dict[str, float]]]=None) -> None:\n        '''Initialize with optional custom pricing.\n        Args:\n            custom_pricing: Optional custom pricing dictionary to override defaults.\n                          Should follow same structure as MODEL_PRICING.\n        '''\n        pass\n\n    def calculate_cost(self, model: str, input_tokens: int=0, output_tokens: int=0, cache_creation_tokens: int=0, cache_read_tokens: int=0, tokens: Optional[TokenCounts]=None, strict: bool=False) -> float:\n        '''Calculate cost with flexible API supporting both signatures.\n        Args:\n            model: Model name\n            input_tokens: Number of input tokens (ignored if tokens provided)\n            output_tokens: Number of output tokens (ignored if tokens provided)\n            cache_creation_tokens: Number of cache creation tokens\n            cache_read_tokens: Number of cache read tokens\n            tokens: Optional TokenCounts object (takes precedence)\n        Returns:\n            Total cost in USD\n        '''\n        pass\n\n    def _get_pricing_for_model(self, model: str, strict: bool=False) -> Dict[str, float]:\n        '''Get pricing for a model with optional fallback logic.\n        Args:\n            model: Model name\n            strict: If True, raise KeyError for unknown models\n        Returns:\n            Pricing dictionary with input/output/cache costs\n        Raises:\n            KeyError: If strict=True and model is unknown\n        '''\n        pass\n\n    def calculate_cost_for_entry(self, entry_data: Dict[str, Any], mode: CostMode) -> float:\n        '''Calculate cost for a single entry (backward compatibility).\n        Args:\n            entry_data: Entry data dictionary\n            mode: Cost mode (for backward compatibility)\n        Returns:\n            Cost in USD\n        '''\n        pass",
    "snippet_id": "snippet_28"
  },
  {
    "class_name": "claude_monitor.core.settings.LastUsedParams",
    "skeleton": "\nclass LastUsedParams:\n    '''Manages last used parameters persistence (moved from last_used.py).'''\n\n    def __init__(self, config_dir: Optional[Path]=None) -> None:\n        '''Initialize with config directory.'''\n        pass\n\n    def save(self, settings: 'Settings') -> None:\n        '''Save current settings as last used.'''\n        pass\n\n    def load(self) -> Dict[str, Any]:\n        '''Load last used parameters.'''\n        pass\n\n    def clear(self) -> None:\n        '''Clear last used parameters.'''\n        pass\n\n    def exists(self) -> bool:\n        '''Check if last used params exist.'''\n        pass",
    "snippet_id": "snippet_29"
  },
  {
    "class_name": "claude_monitor.data.aggregator.AggregatedStats",
    "skeleton": "@dataclass\nclass AggregatedStats:\n    '''Statistics for aggregated usage data.'''\n\n    def add_entry(self, entry: UsageEntry) -> None:\n        '''Add an entry's statistics to this aggregate.'''\n        pass\n\n    def to_dict(self) -> Dict[str, Any]:\n        '''Convert to dictionary format.'''\n        pass",
    "snippet_id": "snippet_30"
  },
  {
    "class_name": "claude_monitor.data.aggregator.UsageAggregator",
    "skeleton": "\nclass UsageAggregator:\n    '''Aggregates usage data for daily and monthly reports.'''\n\n    def __init__(self, data_path: str, aggregation_mode: str='daily', timezone: str='UTC'):\n        '''Initialize the aggregator.\n        Args:\n            data_path: Path to the data directory\n            aggregation_mode: Mode of aggregation ('daily' or 'monthly')\n            timezone: Timezone string for date formatting\n        '''\n        pass\n\n    def _aggregate_by_period(self, entries: List[UsageEntry], period_key_func: Callable[[datetime], str], period_type: str, start_date: Optional[datetime]=None, end_date: Optional[datetime]=None) -> List[Dict[str, Any]]:\n        '''Generic aggregation by time period.\n        Args:\n            entries: List of usage entries\n            period_key_func: Function to extract period key from timestamp\n            period_type: Type of period ('date' or 'month')\n            start_date: Optional start date filter\n            end_date: Optional end date filter\n        Returns:\n            List of aggregated data dictionaries\n        '''\n        pass\n\n    def aggregate_daily(self, entries: List[UsageEntry], start_date: Optional[datetime]=None, end_date: Optional[datetime]=None) -> List[Dict[str, Any]]:\n        '''Aggregate usage data by day.\n        Args:\n            entries: List of usage entries\n            start_date: Optional start date filter\n            end_date: Optional end date filter\n        Returns:\n            List of daily aggregated data\n        '''\n        pass\n\n    def aggregate_monthly(self, entries: List[UsageEntry], start_date: Optional[datetime]=None, end_date: Optional[datetime]=None) -> List[Dict[str, Any]]:\n        '''Aggregate usage data by month.\n        Args:\n            entries: List of usage entries\n            start_date: Optional start date filter\n            end_date: Optional end date filter\n        Returns:\n            List of monthly aggregated data\n        '''\n        pass\n\n    def aggregate_from_blocks(self, blocks: List[SessionBlock], view_type: str='daily') -> List[Dict[str, Any]]:\n        '''Aggregate data from session blocks.\n        Args:\n            blocks: List of session blocks\n            view_type: Type of aggregation ('daily' or 'monthly')\n        Returns:\n            List of aggregated data\n        '''\n        pass\n\n    def calculate_totals(self, aggregated_data: List[Dict[str, Any]]) -> Dict[str, Any]:\n        '''Calculate totals from aggregated data.\n        Args:\n            aggregated_data: List of aggregated daily or monthly data\n        Returns:\n            Dictionary with total statistics\n        '''\n        pass\n\n    def aggregate_daily(self, entries: List[UsageEntry], start_date: Optional[datetime]=None, end_date: Optional[datetime]=None) -> List[Dict[str, Any]]:\n        '''Main aggregation method that reads data and returns aggregated results.\n        Returns:\n            List of aggregated data based on aggregation_mode\n        '''\n        pass",
    "snippet_id": "snippet_31"
  },
  {
    "class_name": "claude_monitor.data.reader.UsageEntryMapper",
    "skeleton": "\nclass UsageEntryMapper:\n    '''Compatibility wrapper for legacy UsageEntryMapper interface.\n    This class provides backward compatibility for tests that expect\n    the old UsageEntryMapper interface, wrapping the new functional\n    approach in _map_to_usage_entry.\n    '''\n\n    def __init__(self, pricing_calculator: PricingCalculator, timezone_handler: TimezoneHandler):\n        '''Initialize with required components.'''\n        pass\n\n    def map(self, data: Dict[str, Any], mode: CostMode) -> Optional[UsageEntry]:\n        '''Map raw data to UsageEntry - compatibility interface.'''\n        pass\n\n    def _has_valid_tokens(self, tokens: Dict[str, int]) -> bool:\n        '''Check if tokens are valid (for test compatibility).'''\n        pass\n\n    def _extract_timestamp(self, data: Dict[str, Any]) -> Optional[datetime]:\n        '''Extract timestamp (for test compatibility).'''\n        pass\n\n    def _extract_model(self, data: Dict[str, Any]) -> str:\n        '''Extract model name (for test compatibility).'''\n        pass\n\n    def _extract_metadata(self, data: Dict[str, Any]) -> Dict[str, str]:\n        '''Extract metadata (for test compatibility).'''\n        pass",
    "snippet_id": "snippet_32"
  },
  {
    "class_name": "claude_monitor.monitoring.data_manager.DataManager",
    "skeleton": "\nclass DataManager:\n    '''Manages data fetching and caching for monitoring.'''\n\n    def __init__(self, cache_ttl: int=30, hours_back: int=192, data_path: Optional[str]=None) -> None:\n        '''Initialize data manager with cache and fetch settings.\n        Args:\n            cache_ttl: Cache time-to-live in seconds\n            hours_back: Hours of historical data to fetch\n            data_path: Path to data directory\n        '''\n        pass\n\n    def get_data(self, force_refresh: bool=False) -> Optional[Dict[str, Any]]:\n        '''Get monitoring data with caching and error handling.\n        Args:\n            force_refresh: Force refresh ignoring cache\n        Returns:\n            Usage data dictionary or None if fetch fails\n        '''\n        pass\n\n    def invalidate_cache(self) -> None:\n        '''Invalidate the cache.'''\n        pass\n\n    def _is_cache_valid(self) -> bool:\n        '''Check if cache is still valid.'''\n        pass\n\n    def _set_cache(self, data: Dict[str, Any]) -> None:\n        '''Set cache with current timestamp.'''\n        pass\n    @property\n    def cache_age(self) -> float:\n        '''Get age of cached data in seconds.'''\n        pass\n    @property\n    def last_error(self) -> Optional[str]:\n        '''Get last error message.'''\n        pass\n    @property\n    def last_successful_fetch_time(self) -> Optional[float]:\n        '''Get timestamp of last successful fetch.'''\n        pass",
    "snippet_id": "snippet_33"
  },
  {
    "class_name": "claude_monitor.terminal.themes.AdaptiveColorScheme",
    "skeleton": "\nclass AdaptiveColorScheme:\n    '''Scientifically-based adaptive color schemes with proper contrast ratios.\n    IMPORTANT: This only changes FONT/FOREGROUND colors, never background colors.\n    The terminal's background remains unchanged - we adapt text colors for readability.\n    All color choices follow WCAG AA accessibility standards for contrast ratios.\n    '''\n    @staticmethod\n    def get_light_background_theme() -> Theme:\n        '''Font colors optimized for light terminal backgrounds (WCAG AA+ contrast).'''\n        pass\n    @staticmethod\n    def get_dark_background_theme() -> Theme:\n        '''Font colors optimized for dark terminal backgrounds (WCAG AA+ contrast).'''\n        pass\n    @staticmethod\n    def get_classic_theme() -> Theme:\n        '''Classic colors for maximum compatibility.'''\n        pass",
    "snippet_id": "snippet_34"
  },
  {
    "class_name": "claude_monitor.ui.components.ErrorDisplayComponent",
    "skeleton": "\nclass ErrorDisplayComponent:\n    '''Error display component for handling error states.'''\n\n    def __init__(self) -> None:\n        '''Initialize error display component.'''\n        pass\n\n    def format_error_screen(self, plan: str='pro', timezone: str='Europe/Warsaw') -> List[str]:\n        '''Format error screen for failed data fetch.\n        Args:\n            plan: Current plan name\n            timezone: Display timezone\n        Returns:\n            List of formatted error screen lines\n        '''\n        pass",
    "snippet_id": "snippet_35"
  },
  {
    "class_name": "claude_monitor.ui.components.LoadingScreenComponent",
    "skeleton": "\nclass LoadingScreenComponent:\n    '''Loading screen component for displaying loading states.'''\n\n    def __init__(self) -> None:\n        '''Initialize loading screen component.'''\n        pass\n\n    def create_loading_screen(self, plan: str='pro', timezone: str='Europe/Warsaw', custom_message: Optional[str]=None) -> List[str]:\n        '''Create loading screen content.\n        Args:\n            plan: Current plan name\n            timezone: Display timezone\n        Returns:\n            List of loading screen lines\n        '''\n        pass\n\n    def create_loading_screen_renderable(self, plan: str='pro', timezone: str='Europe/Warsaw', custom_message: Optional[str]=None) -> RenderableType:\n        '''Create Rich renderable for loading screen.\n        Args:\n            plan: Current plan name\n            timezone: Display timezone\n        Returns:\n            Rich renderable for loading screen\n        '''\n        pass",
    "snippet_id": "snippet_36"
  },
  {
    "class_name": "claude_monitor.ui.components.VelocityIndicator",
    "skeleton": "\nclass VelocityIndicator:\n    '''Velocity indicator component for burn rate visualization.'''\n    @staticmethod\n    def get_velocity_emoji(burn_rate: float) -> str:\n        '''Get velocity emoji based on burn rate.\n        Args:\n            burn_rate: Token burn rate per minute\n        Returns:\n            Emoji representing velocity level\n        '''\n        pass\n    @staticmethod\n    def get_velocity_description(burn_rate: float) -> str:\n        '''Get velocity description based on burn rate.\n        Args:\n            burn_rate: Token burn rate per minute\n        Returns:\n            Text description of velocity level\n        '''\n        pass\n    @staticmethod\n    def render(burn_rate: float, include_description: bool=False) -> str:\n        '''Render velocity indicator.\n        Args:\n            burn_rate: Token burn rate per minute\n            include_description: Whether to include text description\n        Returns:\n            Formatted velocity indicator\n        '''\n        pass",
    "snippet_id": "snippet_37"
  },
  {
    "class_name": "claude_monitor.ui.display_controller.LiveDisplayManager",
    "skeleton": "\nclass LiveDisplayManager:\n    '''Manager for Rich Live display operations.'''\n\n    def __init__(self, console: Optional[Console]=None) -> None:\n        '''Initialize live display manager.\n        Args:\n            console: Optional Rich console instance\n        '''\n        pass\n\n    def create_live_display(self, auto_refresh: bool=True, console: Optional[Console]=None, refresh_per_second: float=0.75) -> Live:\n        '''Create Rich Live display context.\n        Args:\n            auto_refresh: Whether to auto-refresh\n            console: Optional console instance\n            refresh_per_second: Display refresh rate (0.1-20 Hz)\n        Returns:\n            Rich Live context manager\n        '''\n        pass",
    "snippet_id": "snippet_38"
  },
  {
    "class_name": "claude_monitor.ui.display_controller.ScreenBufferManager",
    "skeleton": "\nclass ScreenBufferManager:\n    '''Manager for screen buffer operations and rendering.'''\n\n    def __init__(self) -> None:\n        '''Initialize screen buffer manager.'''\n        pass\n\n    def create_screen_renderable(self, screen_buffer: List[str]) -> Group:\n        '''Create Rich renderable from screen buffer.\n        Args:\n            screen_buffer: List of screen lines with Rich markup\n        Returns:\n            Rich Group renderable\n        '''\n        pass",
    "snippet_id": "snippet_39"
  },
  {
    "class_name": "claude_monitor.ui.display_controller.SessionCalculator",
    "skeleton": "\nclass SessionCalculator:\n    '''Handles session-related calculations for display purposes.\n    (Moved from ui/calculators.py)'''\n\n    def __init__(self) -> None:\n        '''Initialize session calculator.'''\n        pass\n\n    def calculate_time_data(self, session_data: Dict[str, Any], current_time: datetime) -> Dict[str, Any]:\n        '''Calculate time-related data for the session.\n        Args:\n            session_data: Dictionary containing session information\n            current_time: Current UTC time\n        Returns:\n            Dictionary with calculated time data\n        '''\n        pass\n\n    def calculate_cost_predictions(self, session_data: Dict[str, Any], time_data: Dict[str, Any], cost_limit: Optional[float]=None) -> Dict[str, Any]:\n        '''Calculate cost-related predictions.\n        Args:\n            session_data: Dictionary containing session cost information\n            time_data: Time data from calculate_time_data\n            cost_limit: Optional cost limit (defaults to 100.0)\n        Returns:\n            Dictionary with cost predictions\n        '''\n        pass",
    "snippet_id": "snippet_40"
  },
  {
    "class_name": "claude_monitor.ui.layouts.HeaderManager",
    "skeleton": "\nclass HeaderManager:\n    '''Manager for header layout and formatting.'''\n\n    def __init__(self) -> None:\n        '''Initialize header manager.'''\n        pass\n\n    def create_header(self, plan: str='pro', timezone: str='Europe/Warsaw') -> list[str]:\n        '''Create stylized header with sparkles.\n        Args:\n            plan: Current plan name\n            timezone: Display timezone\n        Returns:\n            List of formatted header lines\n        '''\n        pass",
    "snippet_id": "snippet_41"
  },
  {
    "class_name": "claude_monitor.ui.layouts.ScreenManager",
    "skeleton": "\nclass ScreenManager:\n    '''Manager for overall screen layout and organization.'''\n\n    def __init__(self) -> None:\n        '''Initialize screen manager.'''\n        pass\n\n    def set_screen_dimensions(self, width: int, height: int) -> None:\n        '''Set screen dimensions for layout calculations.\n        Args:\n            width: Screen width in characters\n            height: Screen height in lines\n        '''\n        pass\n\n    def set_margins(self, left: int=0, right: int=0, top: int=0, bottom: int=0) -> None:\n        '''Set screen margins.\n        Args:\n            left: Left margin in characters\n            right: Right margin in characters\n            top: Top margin in lines\n            bottom: Bottom margin in lines\n        '''\n        pass\n\n    def create_full_screen_layout(self, content_sections: Sequence[Sequence[str]]) -> list[str]:\n        '''Create full screen layout with multiple content sections.\n        Args:\n            content_sections: List of content sections, each being a list of lines\n        Returns:\n            Combined screen layout as list of lines\n        '''\n        pass",
    "snippet_id": "snippet_42"
  },
  {
    "class_name": "claude_monitor.utils.time_utils.SystemTimeDetector",
    "skeleton": "\nclass SystemTimeDetector:\n    '''System timezone and time format detection.'''\n    @staticmethod\n    def get_timezone() -> str:\n        '''Detect system timezone.'''\n        pass\n    @staticmethod\n    def get_time_format() -> str:\n        '''Detect system time format ('12h' or '24h').'''\n        pass",
    "snippet_id": "snippet_43"
  },
  {
    "class_name": "claude_monitor.utils.time_utils.TimeFormatDetector",
    "skeleton": "\nclass TimeFormatDetector:\n    '''Unified time format detection using multiple strategies.'''\n    @classmethod\n    def detect_from_cli(cls, args: Any) -> Optional[bool]:\n        '''Detect from CLI arguments.\n        Returns:\n            True for 12h format, False for 24h, None if not specified\n        '''\n        pass\n    @classmethod\n    def detect_from_timezone(cls, timezone_name: str) -> Optional[bool]:\n        '''Detect using Babel/timezone data.\n        Returns:\n            True for 12h format, False for 24h, None if cannot determine\n        '''\n        pass\n    @classmethod\n    def detect_from_locale(cls) -> bool:\n        '''Detect from system locale.\n        Returns:\n            True for 12h format, False for 24h\n        '''\n        pass\n    @classmethod\n    def detect_from_system(cls) -> str:\n        '''Platform-specific system detection.\n        Returns:\n            '12h' or '24h'\n        '''\n        pass\n    @classmethod\n    def get_preference(cls, args: Any=None, timezone_name: Optional[str]=None) -> bool:\n        '''Main entry point - returns True for 12h, False for 24h.'''\n        pass",
    "snippet_id": "snippet_44"
  },
  {
    "class_name": "utils.rate_limiter.TokenBucketRateLimiter",
    "skeleton": "\nclass TokenBucketRateLimiter:\n    '''\n    Token Bucket Rate Limiter Implementation\n    The token bucket algorithm allows for a burst of traffic up to the bucket capacity,\n    while maintaining a steady rate of token refill.\n    '''\n\n    def __init__(self, tokens_per_second: int, bucket_capacity: Optional[int]=None):\n        '''\n        Initialize token bucket rate limiter.\n        Args:\n            tokens_per_second: Rate at which tokens are refilled\n            bucket_capacity: Maximum number of tokens in bucket (defaults to tokens_per_second)\n        '''\n        pass\n\n    def _refill_tokens(self):\n        '''Refill tokens based on time elapsed since last refill.'''\n        pass\n\n    def try_consume_token(self) -> bool:\n        '''\n        Try to consume a token from the bucket.\n        Returns:\n            True if token was consumed, False if bucket is empty\n        '''\n        pass\n\n    def get_wait_time(self) -> float:\n        '''\n        Calculate how long to wait before a token will be available.\n        Returns:\n            Time in seconds to wait\n        '''\n        pass",
    "snippet_id": "snippet_45"
  },
  {
    "class_name": "best_program.AffineTransform2D",
    "skeleton": "\nclass AffineTransform2D:\n    '''\n    Initial implementation of affine_transform_2d task.\n    This will be evolved by OpenEvolve to improve performance and correctness.\n    '''\n\n    def __init__(self):\n        '''Initialize the AffineTransform2D.'''\n        pass\n\n    def solve(self, problem):\n        '''\n        Solve the affine_transform_2d problem.\n        Args:\n            problem: Dictionary containing problem data specific to affine_transform_2d\n        Returns:\n            The solution in the format expected by the task\n        '''\n        pass\n\n    def is_solution(self, problem, solution):\n        '''\n        Check if the provided solution is valid.\n        Args:\n            problem: The original problem\n            solution: The proposed solution\n        Returns:\n            True if the solution is valid, False otherwise\n        '''\n        pass",
    "snippet_id": "snippet_46"
  },
  {
    "class_name": "best_program.Convolve2DFullFill",
    "skeleton": "\nclass Convolve2DFullFill:\n    '''\n    Initial implementation of convolve2d_full_fill task.\n    This will be evolved by OpenEvolve to improve performance and correctness.\n    '''\n\n    def __init__(self):\n        '''Initialize the Convolve2DFullFill.'''\n        pass\n\n    def solve(self, problem):\n        '''\n        Compute the 2D convolution of arrays a and b using \"full\" mode and \"fill\" boundary.\n        Uses scipy.signal.fftconvolve for efficiency, which implicitly handles \"fill\" boundary.\n        Args:\n            problem: A tuple (a, b) of 2D arrays.\n        Returns:\n            A 2D array containing the convolution result.\n        '''\n        pass\n\n    def is_solution(self, problem, solution):\n        '''\n        Check if the provided solution is valid.\n        Args:\n            problem: The original problem\n            solution: The proposed solution\n        Returns:\n            True if the solution is valid, False otherwise\n        '''\n        pass",
    "snippet_id": "snippet_47"
  },
  {
    "class_name": "best_program.EigenvectorsComplex",
    "skeleton": "\nclass EigenvectorsComplex:\n    '''\n    Initial implementation of eigenvectors_complex task.\n    This will be evolved by OpenEvolve to improve performance and correctness.\n    '''\n\n    def __init__(self):\n        '''Initialize the EigenvectorsComplex.'''\n        pass\n\n    def solve(self, problem):\n        '''\n        Solve the eigenvectors_complex problem.\n        Args:\n            problem: Dictionary containing problem data specific to eigenvectors_complex\n        Returns:\n            The solution in the format expected by the task\n        '''\n        pass\n\n    def is_solution(self, problem, solution):\n        '''\n        Check if the provided solution is valid.\n        Args:\n            problem: The original problem\n            solution: The proposed solution\n        Returns:\n            True if the solution is valid, False otherwise\n        '''\n        pass",
    "snippet_id": "snippet_48"
  },
  {
    "class_name": "best_program.FFTComplexScipyFFTpack",
    "skeleton": "\nclass FFTComplexScipyFFTpack:\n    '''\n    Initial implementation of fft_cmplx_scipy_fftpack task.\n    This will be evolved by OpenEvolve to improve performance and correctness.\n    '''\n    @staticmethod\n    def solve(problem):\n        '''\n        Solve the fft_cmplx_scipy_fftpack problem.\n        Args:\n            problem: Dictionary containing problem data specific to fft_cmplx_scipy_fftpack\n        Returns:\n            The solution in the format expected by the task\n        '''\n        pass\n    @staticmethod\n    def is_solution(problem, solution):\n        '''\n        Check if the provided solution is valid.\n        Args:\n            problem: The original problem\n            solution: The proposed solution\n        Returns:\n            True if the solution is valid, False otherwise\n        '''\n        pass",
    "snippet_id": "snippet_49"
  },
  {
    "class_name": "best_program.FFTConvolution",
    "skeleton": "\nclass FFTConvolution:\n    '''\n    Initial implementation of fft_convolution task.\n    This will be evolved by OpenEvolve to improve performance and correctness.\n    '''\n\n    def __init__(self):\n        '''Initialize the FFTConvolution.'''\n        pass\n\n    def solve(self, problem):\n        '''\n        Solve the fft_convolution problem.\n        Args:\n            problem: Dictionary containing problem data specific to fft_convolution\n        Returns:\n            The solution in the format expected by the task\n        '''\n        pass\n\n    def is_solution(self, problem, solution):\n        '''\n        Check if the provided solution is valid.\n        Args:\n            problem: The original problem\n            solution: The proposed solution\n        Returns:\n            True if the solution is valid, False otherwise\n        '''\n        pass",
    "snippet_id": "snippet_50"
  },
  {
    "class_name": "best_program.LUFactorization",
    "skeleton": "\nclass LUFactorization:\n    '''\n    Initial implementation of lu_factorization task.\n    This will be evolved by OpenEvolve to improve performance and correctness.\n    '''\n\n    def __init__(self):\n        '''Initialize the LUFactorization.'''\n        pass\n\n    def solve(self, problem):\n        '''Computes the LU factorization of a matrix using an optimized scipy call.'''\n        pass\n\n    def is_solution(self, problem, solution):\n        '''\n        Validate an LU factorization A = P L U.\n        Checks:\n        - Presence of 'LU' with 'P','L','U'\n        - Shapes match A (square)\n        - No NaNs/Infs\n        - P is a permutation matrix\n        - L is lower-triangular\n        - U is upper-triangular\n        - P @ L @ U ≈ A\n        '''\n        pass",
    "snippet_id": "snippet_51"
  },
  {
    "class_name": "best_program.PSDConeProjection",
    "skeleton": "\nclass PSDConeProjection:\n    '''\n    Initial implementation of psd_cone_projection task.\n    This will be evolved by OpenEvolve to improve performance and correctness.\n    '''\n\n    def __init__(self):\n        '''Initialize the PSDConeProjection.'''\n        pass\n\n    def solve(self, problem):\n        '''\n        Solve the psd_cone_projection problem.\n        Args:\n            problem: Dictionary containing problem data specific to psd_cone_projection\n        Returns:\n            The solution in the format expected by the task\n        '''\n        pass\n\n    def is_solution(self, problem, solution):\n        '''\n        Check if the provided solution is valid.\n        Args:\n            problem: The original problem\n            solution: The proposed solution\n        Returns:\n            True if the solution is valid, False otherwise\n        '''\n        pass",
    "snippet_id": "snippet_52"
  },
  {
    "class_name": "initial_program.AffineTransform2D",
    "skeleton": "\nclass AffineTransform2D:\n    '''\n    Initial implementation of affine_transform_2d task.\n    This will be evolved by OpenEvolve to improve performance and correctness.\n    '''\n\n    def __init__(self):\n        '''Initialize the AffineTransform2D.'''\n        pass\n\n    def solve(self, problem):\n        '''\n        Solve the affine_transform_2d problem.\n        Args:\n            problem: Dictionary containing problem data specific to affine_transform_2d\n        Returns:\n            The solution in the format expected by the task\n        '''\n        pass\n\n    def is_solution(self, problem, solution):\n        '''\n        Check if the provided solution is valid.\n        Args:\n            problem: The original problem\n            solution: The proposed solution\n        Returns:\n            True if the solution is valid, False otherwise\n        '''\n        pass",
    "snippet_id": "snippet_53"
  },
  {
    "class_name": "initial_program.Convolve2DFullFill",
    "skeleton": "\nclass Convolve2DFullFill:\n    '''\n    Initial implementation of convolve2d_full_fill task.\n    This will be evolved by OpenEvolve to improve performance and correctness.\n    '''\n\n    def __init__(self):\n        '''Initialize the Convolve2DFullFill.'''\n        pass\n\n    def solve(self, problem):\n        '''\n        Solve the convolve2d_full_fill problem.\n        Args:\n            problem: Dictionary containing problem data specific to convolve2d_full_fill\n        Returns:\n            The solution in the format expected by the task\n        '''\n        pass\n\n    def is_solution(self, problem, solution):\n        '''\n        Check if the provided solution is valid.\n        Args:\n            problem: The original problem\n            solution: The proposed solution\n        Returns:\n            True if the solution is valid, False otherwise\n        '''\n        pass",
    "snippet_id": "snippet_54"
  },
  {
    "class_name": "initial_program.EigenvectorsComplex",
    "skeleton": "\nclass EigenvectorsComplex:\n    '''\n    Initial implementation of eigenvectors_complex task.\n    This will be evolved by OpenEvolve to improve performance and correctness.\n    '''\n\n    def __init__(self):\n        '''Initialize the EigenvectorsComplex.'''\n        pass\n\n    def solve(self, problem):\n        '''\n        Solve the eigenvectors_complex problem.\n        Args:\n            problem: Dictionary containing problem data specific to eigenvectors_complex\n        Returns:\n            The solution in the format expected by the task\n        '''\n        pass\n\n    def is_solution(self, problem, solution):\n        '''\n        Check if the provided solution is valid.\n        Args:\n            problem: The original problem\n            solution: The proposed solution\n        Returns:\n            True if the solution is valid, False otherwise\n        '''\n        pass",
    "snippet_id": "snippet_55"
  },
  {
    "class_name": "initial_program.FFTComplexScipyFFTpack",
    "skeleton": "\nclass FFTComplexScipyFFTpack:\n    '''\n    Initial implementation of fft_cmplx_scipy_fftpack task.\n    This will be evolved by OpenEvolve to improve performance and correctness.\n    '''\n\n    def __init__(self):\n        '''Initialize the FFTComplexScipyFFTpack.'''\n        pass\n\n    def solve(self, problem):\n        '''\n        Solve the fft_cmplx_scipy_fftpack problem.\n        Args:\n            problem: Dictionary containing problem data specific to fft_cmplx_scipy_fftpack\n        Returns:\n            The solution in the format expected by the task\n        '''\n        pass\n\n    def is_solution(self, problem, solution):\n        '''\n        Check if the provided solution is valid.\n        Args:\n            problem: The original problem\n            solution: The proposed solution\n        Returns:\n            True if the solution is valid, False otherwise\n        '''\n        pass",
    "snippet_id": "snippet_56"
  },
  {
    "class_name": "initial_program.FFTConvolution",
    "skeleton": "\nclass FFTConvolution:\n    '''\n    Initial implementation of fft_convolution task.\n    This will be evolved by OpenEvolve to improve performance and correctness.\n    '''\n\n    def __init__(self):\n        '''Initialize the FFTConvolution.'''\n        pass\n\n    def solve(self, problem):\n        '''\n        Solve the fft_convolution problem.\n        Args:\n            problem: Dictionary containing problem data specific to fft_convolution\n        Returns:\n            The solution in the format expected by the task\n        '''\n        pass\n\n    def is_solution(self, problem, solution):\n        '''\n        Check if the provided solution is valid.\n        Args:\n            problem: The original problem\n            solution: The proposed solution\n        Returns:\n            True if the solution is valid, False otherwise\n        '''\n        pass",
    "snippet_id": "snippet_57"
  },
  {
    "class_name": "initial_program.LUFactorization",
    "skeleton": "\nclass LUFactorization:\n    '''\n    Initial implementation of lu_factorization task.\n    This will be evolved by OpenEvolve to improve performance and correctness.\n    '''\n\n    def __init__(self):\n        '''Initialize the LUFactorization.'''\n        pass\n\n    def solve(self, problem):\n        '''\n        Solve the lu_factorization problem.\n        Args:\n            problem: Dictionary containing problem data specific to lu_factorization\n        Returns:\n            The solution in the format expected by the task\n        '''\n        pass\n\n    def is_solution(self, problem, solution):\n        '''\n        Check if the provided solution is valid.\n        Args:\n            problem: The original problem\n            solution: The proposed solution\n        Returns:\n            True if the solution is valid, False otherwise\n        '''\n        pass",
    "snippet_id": "snippet_58"
  },
  {
    "class_name": "initial_program.PSDConeProjection",
    "skeleton": "\nclass PSDConeProjection:\n    '''\n    Initial implementation of psd_cone_projection task.\n    This will be evolved by OpenEvolve to improve performance and correctness.\n    '''\n\n    def __init__(self):\n        '''Initialize the PSDConeProjection.'''\n        pass\n\n    def solve(self, problem):\n        '''\n        Solve the psd_cone_projection problem.\n        Args:\n            problem: Dictionary containing problem data specific to psd_cone_projection\n        Returns:\n            The solution in the format expected by the task\n        '''\n        pass\n\n    def is_solution(self, problem, solution):\n        '''\n        Check if the provided solution is valid.\n        Args:\n            problem: The original problem\n            solution: The proposed solution\n        Returns:\n            True if the solution is valid, False otherwise\n        '''\n        pass",
    "snippet_id": "snippet_59"
  },
  {
    "class_name": "initial_program.PolynomialReal",
    "skeleton": "\nclass PolynomialReal:\n    '''\n    Initial implementation of polynomial_real task.\n    This will be evolved by OpenEvolve to improve performance and correctness.\n    '''\n\n    def __init__(self):\n        '''Initialize the PolynomialReal.'''\n        pass\n\n    def solve(self, problem):\n        '''\n        Solve the polynomial_real problem.\n        Args:\n            problem: Dictionary containing problem data specific to polynomial_real\n        Returns:\n            The solution in the format expected by the task\n        '''\n        pass\n\n    def is_solution(self, problem, solution):\n        '''\n        Check if the provided solution is valid.\n        Args:\n            problem: The original problem\n            solution: The proposed solution\n        Returns:\n            True if the solution is valid, False otherwise\n        '''\n        pass",
    "snippet_id": "snippet_60"
  },
  {
    "class_name": "openevolve.database.Program",
    "skeleton": "@dataclass\nclass Program:\n    '''Represents a program in the database'''\n\n    def to_dict(self) -> Dict[str, Any]:\n        '''Convert to dictionary representation'''\n        pass\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'Program':\n        '''Create from dictionary representation'''\n        pass",
    "snippet_id": "snippet_61"
  },
  {
    "class_name": "openevolve.evaluation_result.EvaluationResult",
    "skeleton": "@dataclass\nclass EvaluationResult:\n    '''\n    Result of program evaluation containing both metrics and optional artifacts\n    This maintains backward compatibility with the existing dict[str, float] contract\n    while adding a side-channel for arbitrary artifacts (text or binary data).\n    IMPORTANT: For custom MAP-Elites features, metrics values must be raw continuous\n    scores (e.g., actual counts, percentages, continuous measurements), NOT pre-computed\n    bin indices. The database handles all binning internally using min-max scaling.\n    Examples:\n        ✅ Correct: {\"combined_score\": 0.85, \"prompt_length\": 1247, \"execution_time\": 0.234}\n        ❌ Wrong:   {\"combined_score\": 0.85, \"prompt_length\": 7, \"execution_time\": 3}\n    '''\n    @classmethod\n    def from_dict(cls, metrics: Dict[str, float]) -> 'EvaluationResult':\n        '''Auto-wrap dict returns for backward compatibility'''\n        pass\n\n    def to_dict(self) -> Dict[str, float]:\n        '''Backward compatibility - return just metrics'''\n        pass\n\n    def has_artifacts(self) -> bool:\n        '''Check if this result contains any artifacts'''\n        pass\n\n    def get_artifact_keys(self) -> list:\n        '''Get list of artifact keys'''\n        pass\n\n    def get_artifact_size(self, key: str) -> int:\n        '''Get size of a specific artifact in bytes'''\n        pass\n\n    def get_total_artifact_size(self) -> int:\n        '''Get total size of all artifacts in bytes'''\n        pass",
    "snippet_id": "snippet_62"
  },
  {
    "class_name": "raganything.batch_parser.BatchProcessingResult",
    "skeleton": "@dataclass\nclass BatchProcessingResult:\n    '''Result of batch processing operation'''\n    @property\n    def success_rate(self) -> float:\n        '''Calculate success rate as percentage'''\n        pass\n\n    def summary(self) -> str:\n        '''Generate a summary of the batch processing results'''\n        pass",
    "snippet_id": "snippet_63"
  },
  {
    "class_name": "openmanus_rl.tools.base.BaseTool",
    "skeleton": "class BaseTool:\n    '''\n    A base class for building tool classes that perform specific tasks, such as image processing or text detection.\n        '''\n\n    def __init__(self, tool_name=None, tool_description=None, tool_version=None, input_types=None, output_type=None, demo_commands=None, output_dir=None, user_metadata=None, model_string=None):\n        '''\n        Initialize the base tool with optional metadata.\n        Parameters:\n            tool_name (str): The name of the tool.\n            tool_description (str): A description of the tool.\n            tool_version (str): The version of the tool.\n            input_types (dict): The expected input types for the tool.\n            output_type (str): The expected output type for the tool.\n            demo_commands (list): A list of example commands for using the tool.\n            output_dir (str): The directory where the tool should save its output (optional).\n            user_metadata (dict): Additional metadata specific to user needs (optional).\n            model_string (str): The model string for the LLM engine (optional, only used if require_llm_engine is True).\n        '''\n        pass\n\n    def set_metadata(self, tool_name, tool_description, tool_version, input_types, output_type, demo_commands, user_metadata=None):\n        '''\n        Set the metadata for the tool.\n        Parameters:\n            tool_name (str): The name of the tool.\n            tool_description (str): A description of the tool.\n            tool_version (str): The version of the tool.\n            input_types (dict): The expected input types for the tool.\n            output_type (str): The expected output type for the tool.\n            demo_commands (list): A list of example commands for using the tool.\n            user_metadata (dict): Additional metadata specific to user needs (optional).\n        '''\n        pass\n\n    def get_metadata(self):\n        '''\n        Returns the metadata for the tool.\n        Returns:\n            dict: A dictionary containing the tool's metadata.\n        '''\n        pass\n\n    def set_custom_output_dir(self, output_dir):\n        '''\n        Set a custom output directory for the tool.\n        Parameters:\n            output_dir (str): The new output directory path.\n        '''\n        pass\n\n    def set_llm_engine(self, model_string):\n        '''\n        Set the LLM engine for the tool.\n        Parameters:\n            model_string (str): The model string for the LLM engine.\n        '''\n        pass\n\n    def execute(self, *args, **kwargs):\n        '''\n        Execute the tool's main functionality. This method should be overridden by subclasses.\n        Raises:\n            NotImplementedError: If the subclass does not implement this method.\n        '''\n        pass",
    "snippet_id": "snippet_64"
  },
  {
    "class_name": "action_manager.ActionManager",
    "skeleton": "\nclass ActionManager:\n    '''\n    动作管理器，用于管理各种类型的动作\n    现在统一使用新插件系统，简化了原有的新旧兼容逻辑。\n    '''\n\n    def __init__(self):\n        '''初始化动作管理器'''\n        pass\n\n    def create_action(self, action_name: str, action_data: dict, reasoning: str, cycle_timers: dict, thinking_id: str, chat_stream: ChatStream, log_prefix: str, shutting_down: bool=False, action_message: Optional[DatabaseMessages]=None) -> Optional[BaseAction]:\n        '''\n        创建动作处理器实例\n        Args:\n            action_name: 动作名称\n            action_data: 动作数据\n            reasoning: 执行理由\n            cycle_timers: 计时器字典\n            thinking_id: 思考ID\n            chat_stream: 聊天流\n            log_prefix: 日志前缀\n            shutting_down: 是否正在关闭\n        Returns:\n            Optional[BaseAction]: 创建的动作处理器实例，如果动作名称未注册则返回None\n        '''\n        pass\n\n    def get_using_actions(self) -> Dict[str, ActionInfo]:\n        '''获取当前正在使用的动作集合'''\n        pass\n\n    def remove_action_from_using(self, action_name: str) -> bool:\n        '''\n        从当前使用的动作集中移除指定动作\n        Args:\n            action_name: 动作名称\n        Returns:\n            bool: 移除是否成功\n        '''\n        pass\n\n    def restore_actions(self) -> None:\n        '''恢复到默认动作集'''\n        pass",
    "snippet_id": "snippet_65"
  },
  {
    "class_name": "src.plugin_system.utils.manifest_utils.VersionComparator",
    "skeleton": "\nclass VersionComparator:\n    '''版本号比较器\n    支持语义化版本号比较，自动处理snapshot版本，并支持向前兼容性检查\n    '''\n    @staticmethod\n    def normalize_version(version: str) -> str:\n        '''标准化版本号，移除snapshot标识\n        Args:\n            version: 原始版本号，如 \"0.8.0-snapshot.1\"\n        Returns:\n            str: 标准化后的版本号，如 \"0.8.0\"\n        '''\n        pass\n    @staticmethod\n    def parse_version(version: str) -> Tuple[int, int, int]:\n        '''解析版本号为元组\n        Args:\n            version: 版本号字符串\n        Returns:\n            Tuple[int, int, int]: (major, minor, patch)\n        '''\n        pass\n    @staticmethod\n    def compare_versions(version1: str, version2: str) -> int:\n        '''比较两个版本号\n        Args:\n            version1: 第一个版本号\n            version2: 第二个版本号\n        Returns:\n            int: -1 if version1 < version2, 0 if equal, 1 if version1 > version2\n        '''\n        pass\n    @staticmethod\n    def check_forward_compatibility(current_version: str, max_version: str) -> Tuple[bool, str]:\n        '''检查向前兼容性（仅使用兼容性映射表）\n        Args:\n            current_version: 当前版本\n            max_version: 插件声明的最大支持版本\n        Returns:\n            Tuple[bool, str]: (是否兼容, 兼容信息)\n        '''\n        pass\n    @staticmethod\n    def is_version_in_range(version: str, min_version: str='', max_version: str='') -> Tuple[bool, str]:\n        '''检查版本是否在指定范围内，支持兼容性检查\n        Args:\n            version: 要检查的版本号\n            min_version: 最小版本号（可选）\n            max_version: 最大版本号（可选）\n        Returns:\n            Tuple[bool, str]: (是否兼容, 错误信息或兼容信息)\n        '''\n        pass\n    @staticmethod\n    def get_current_host_version() -> str:\n        '''获取当前主机应用版本\n        Returns:\n            str: 当前版本号\n        '''\n        pass\n    @staticmethod\n    def add_compatibility_mapping(base_version: str, compatible_versions: list) -> None:\n        '''动态添加兼容性映射\n        Args:\n            base_version: 基础版本（插件声明的最大支持版本）\n            compatible_versions: 兼容的版本列表\n        '''\n        pass\n    @staticmethod\n    def get_compatibility_info() -> Dict[str, list]:\n        '''获取当前的兼容性映射表\n        Returns:\n            Dict[str, list]: 兼容性映射表的副本\n        '''\n        pass",
    "snippet_id": "snippet_66"
  },
  {
    "class_name": "super_chat_manager.SuperChatRecord",
    "skeleton": "@dataclass\nclass SuperChatRecord:\n    '''SuperChat记录数据类'''\n\n    def is_expired(self) -> bool:\n        '''检查SuperChat是否已过期'''\n        pass\n\n    def remaining_time(self) -> float:\n        '''获取剩余时间（秒）'''\n        pass\n\n    def to_dict(self) -> dict:\n        '''转换为字典格式'''\n        pass",
    "snippet_id": "snippet_67"
  },
  {
    "class_name": "strands.agent.state.AgentState",
    "skeleton": "\nclass AgentState:\n    '''Represents an Agent's stateful information outside of context provided to a model.\n    Provides a key-value store for agent state with JSON serialization validation and persistence support.\n    Key features:\n    - JSON serialization validation on assignment\n    - Get/set/delete operations\n    '''\n\n    def __init__(self, initial_state: Optional[Dict[str, Any]]=None):\n        '''Initialize AgentState.'''\n        pass\n\n    def set(self, key: str, value: Any) -> None:\n        '''Set a value in the state.\n        Args:\n            key: The key to store the value under\n            value: The value to store (must be JSON serializable)\n        Raises:\n            ValueError: If key is invalid, or if value is not JSON serializable\n        '''\n        pass\n\n    def get(self, key: Optional[str]=None) -> Any:\n        '''Get a value or entire state.\n        Args:\n            key: The key to retrieve (if None, returns entire state object)\n        Returns:\n            The stored value, entire state dict, or None if not found\n        '''\n        pass\n\n    def delete(self, key: str) -> None:\n        '''Delete a specific key from the state.\n        Args:\n            key: The key to delete\n        '''\n        pass\n\n    def _validate_key(self, key: str) -> None:\n        '''Validate that a key is valid.\n        Args:\n            key: The key to validate\n        Raises:\n            ValueError: If key is invalid\n        '''\n        pass\n\n    def _validate_json_serializable(self, value: Any) -> None:\n        '''Validate that a value is JSON serializable.\n        Args:\n            value: The value to validate\n        Raises:\n            ValueError: If value is not JSON serializable\n        '''\n        pass",
    "snippet_id": "snippet_68"
  },
  {
    "class_name": "strands.handlers.callback_handler.CompositeCallbackHandler",
    "skeleton": "\nclass CompositeCallbackHandler:\n    '''Class-based callback handler that combines multiple callback handlers.\n    This handler allows multiple callback handlers to be invoked for the same events,\n    enabling different processing or output formats for the same stream data.\n    '''\n\n    def __init__(self, *handlers: Callable) -> None:\n        '''Initialize handler.'''\n        pass\n\n    def __call__(self, **kwargs: Any) -> None:\n        '''Invoke all handlers in the chain.'''\n        pass",
    "snippet_id": "snippet_69"
  },
  {
    "class_name": "strands.handlers.callback_handler.PrintingCallbackHandler",
    "skeleton": "\nclass PrintingCallbackHandler:\n    '''Handler for streaming text output and tool invocations to stdout.'''\n\n    def __init__(self) -> None:\n        '''Initialize handler.'''\n        pass\n\n    def __call__(self, **kwargs: Any) -> None:\n        '''Stream text output and tool invocations to stdout.\n        Args:\n            **kwargs: Callback event data including:\n                - reasoningText (Optional[str]): Reasoning text to print if provided.\n                - data (str): Text content to stream.\n                - complete (bool): Whether this is the final chunk of a response.\n                - current_tool_use (dict): Information about the current tool being used.\n        '''\n        pass",
    "snippet_id": "snippet_70"
  },
  {
    "class_name": "strands.hooks.registry.HookEvent",
    "skeleton": "@dataclass\nclass HookEvent:\n    '''Base class for all hook events.\n    Attributes:\n        agent: The agent instance that triggered this event.\n    '''\n    @property\n    def should_reverse_callbacks(self) -> bool:\n        '''Determine if callbacks for this event should be invoked in reverse order.\n        Returns:\n            False by default. Override to return True for events that should\n            invoke callbacks in reverse order (e.g., cleanup/teardown events).\n        '''\n        pass\n\n    def _can_write(self, name: str) -> bool:\n        '''Check if the given property can be written to.\n        Args:\n            name: The name of the property to check.\n        Returns:\n            True if the property can be written to, False otherwise.\n        '''\n        pass\n\n    def __post_init__(self) -> None:\n        '''Disallow writes to non-approved properties.'''\n        pass\n\n    def __setattr__(self, name: str, value: Any) -> None:\n        '''Prevent setting attributes on hook events.\n        Raises:\n            AttributeError: Always raised to prevent setting attributes on hook events.\n        '''\n        pass",
    "snippet_id": "snippet_71"
  },
  {
    "class_name": "strands.types.session.SessionAgent",
    "skeleton": "@dataclass\nclass SessionAgent:\n    '''Agent that belongs to a Session.'''\n    @classmethod\n    def from_agent(cls, agent: 'Agent') -> 'SessionAgent':\n        '''Convert an Agent to a SessionAgent.'''\n        pass\n    @classmethod\n    def from_dict(cls, env: dict[str, Any]) -> 'SessionAgent':\n        '''Initialize a SessionAgent from a dictionary, ignoring keys that are not class parameters.'''\n        pass\n\n    def to_dict(self) -> dict[str, Any]:\n        '''Convert the SessionAgent to a dictionary representation.'''\n        pass",
    "snippet_id": "snippet_72"
  },
  {
    "class_name": "memos.memories.textual.tree_text_memory.retrieve.bochasearch.BochaAISearchAPI",
    "skeleton": "\nclass BochaAISearchAPI:\n    '''BochaAI Search API Client'''\n\n    def __init__(self, api_key: str, max_results: int=20):\n        '''\n        Initialize BochaAI Search API client.\n        Args:\n            api_key: BochaAI API key\n            max_results: Maximum number of search results to retrieve\n        '''\n        pass\n\n    def search_web(self, query: str, summary: bool=True, freshness='noLimit') -> list[dict]:\n        '''\n        Perform a Web Search (equivalent to the first curl).\n        Args:\n            query: Search query string\n            summary: Whether to include summary in the results\n            freshness: Freshness filter (e.g. 'noLimit', 'day', 'week')\n        Returns:\n            A list of search result dicts\n        '''\n        pass\n\n    def search_ai(self, query: str, answer: bool=False, stream: bool=False, freshness='noLimit') -> list[dict]:\n        '''\n        Perform an AI Search (equivalent to the second curl).\n        Args:\n            query: Search query string\n            answer: Whether BochaAI should generate an answer\n            stream: Whether to use streaming response\n            freshness: Freshness filter (e.g. 'noLimit', 'day', 'week')\n        Returns:\n            A list of search result dicts\n        '''\n        pass\n\n    def _post(self, url: str, body: dict) -> list[dict]:\n        '''Send POST request and parse BochaAI search results.'''\n        pass",
    "snippet_id": "snippet_73"
  },
  {
    "class_name": "memos.memories.textual.tree_text_memory.retrieve.bochasearch.BochaAISearchRetriever",
    "skeleton": "\nclass BochaAISearchRetriever:\n    '''BochaAI retriever that converts search results into TextualMemoryItem objects'''\n\n    def __init__(self, access_key: str, embedder: OllamaEmbedder, reader: BaseMemReader, max_results: int=20):\n        '''\n        Initialize BochaAI Search retriever.\n        Args:\n            access_key: BochaAI API key\n            embedder: Embedder instance for generating embeddings\n            reader: MemReader instance for processing internet content\n            max_results: Maximum number of search results to retrieve\n        '''\n        pass\n\n    def retrieve_from_internet(self, query: str, top_k: int=10, parsed_goal=None, info=None) -> list[TextualMemoryItem]:\n        '''\n        Default internet retrieval (Web Search).\n        This keeps consistent API with Xinyu and Google retrievers.\n        Args:\n            query: Search query\n            top_k: Number of results to retrieve\n            parsed_goal: Parsed task goal (optional)\n            info (dict): Metadata for memory consumption tracking\n        Returns:\n            List of TextualMemoryItem\n        '''\n        pass\n\n    def retrieve_from_web(self, query: str, top_k: int=10, parsed_goal=None, info=None) -> list[TextualMemoryItem]:\n        '''Explicitly retrieve using Bocha Web Search.'''\n        pass\n\n    def retrieve_from_ai(self, query: str, top_k: int=10, parsed_goal=None, info=None) -> list[TextualMemoryItem]:\n        '''Explicitly retrieve using Bocha AI Search.'''\n        pass\n\n    def _convert_to_mem_items(self, search_results: list[dict], query: str, parsed_goal=None, info=None):\n        '''Convert API search results into TextualMemoryItem objects.'''\n        pass\n\n    def _process_result(self, result: dict, query: str, parsed_goal: str, info: None) -> list[TextualMemoryItem]:\n        '''Process one Bocha search result into TextualMemoryItem.'''\n        pass",
    "snippet_id": "snippet_74"
  },
  {
    "class_name": "memos.memories.textual.tree_text_memory.retrieve.internet_retriever.GoogleCustomSearchAPI",
    "skeleton": "\nclass GoogleCustomSearchAPI:\n    '''Google Custom Search API Client'''\n\n    def __init__(self, api_key: str, search_engine_id: str, max_results: int=20, num_per_request: int=10):\n        '''\n        Initialize Google Custom Search API client\n        Args:\n            api_key: Google API key\n            search_engine_id: Search engine ID (cx parameter)\n            max_results: Maximum number of results to retrieve\n            num_per_request: Number of results per API request\n        '''\n        pass\n\n    def search(self, query: str, num_results: int | None=None, start_index: int=1) -> dict:\n        '''\n        Execute search request\n        Args:\n            query: Search query\n            num_results: Number of results to return (uses config default if None)\n            start_index: Starting index (default 1)\n        Returns:\n            Dictionary containing search results\n        '''\n        pass\n\n    def get_all_results(self, query: str, max_results: int | None=None) -> list[dict]:\n        '''\n        Get all search results (with pagination)\n        Args:\n            query: Search query\n            max_results: Maximum number of results (uses config default if None)\n        Returns:\n            List of all search results\n        '''\n        pass",
    "snippet_id": "snippet_75"
  },
  {
    "class_name": "memos.memories.textual.tree_text_memory.retrieve.internet_retriever.InternetGoogleRetriever",
    "skeleton": "\nclass InternetGoogleRetriever:\n    '''Internet retriever that converts search results to TextualMemoryItem format'''\n\n    def __init__(self, api_key: str, search_engine_id: str, embedder: OllamaEmbedder, max_results: int=20, num_per_request: int=10):\n        '''\n        Initialize internet retriever\n        Args:\n            api_key: Google API key\n            search_engine_id: Search engine ID\n            embedder: Embedder instance for generating embeddings\n            max_results: Maximum number of results to retrieve\n            num_per_request: Number of results per API request\n        '''\n        pass\n\n    def retrieve_from_internet(self, query: str, top_k: int=10, parsed_goal=None, info=None) -> list[TextualMemoryItem]:\n        '''\n        Retrieve information from the internet and convert to TextualMemoryItem format\n        Args:\n            query: Search query\n            top_k: Number of results to return\n            parsed_goal: Parsed task goal (optional)\n            info (dict): Leave a record of memory consumption.\n        Returns:\n            List of TextualMemoryItem\n        '''\n        pass\n\n    def _extract_entities(self, title: str, snippet: str) -> list[str]:\n        '''\n        Extract entities from title and snippet\n        Args:\n            title: Title\n            snippet: Snippet\n        Returns:\n            List of entities\n        '''\n        pass\n\n    def _extract_tags(self, title: str, snippet: str, parsed_goal=None) -> list[str]:\n        '''\n        Extract tags from title and snippet\n        Args:\n            title: Title\n            snippet: Snippet\n            parsed_goal: Parsed task goal\n        Returns:\n            List of tags\n        '''\n        pass",
    "snippet_id": "snippet_76"
  },
  {
    "class_name": "memos.memories.textual.tree_text_memory.retrieve.xinyusearch.XinyuSearchAPI",
    "skeleton": "\nclass XinyuSearchAPI:\n    '''Xinyu Search API Client'''\n\n    def __init__(self, access_key: str, search_engine_id: str, max_results: int=20):\n        '''\n        Initialize Xinyu Search API client\n        Args:\n            access_key: Xinyu API access key\n            max_results: Maximum number of results to retrieve\n        '''\n        pass\n\n    def query_detail(self, body: dict | None=None, detail: bool=True) -> list[dict]:\n        '''\n        Query Xinyu search API for detailed results\n        Args:\n            body: Search parameters\n            detail: Whether to get detailed results\n        Returns:\n            List of search results\n        '''\n        pass\n\n    def search(self, query: str, max_results: int | None=None) -> list[dict]:\n        '''\n        Execute search request\n        Args:\n            query: Search query\n            max_results: Maximum number of results to return\n        Returns:\n            List of search results\n        '''\n        pass",
    "snippet_id": "snippet_77"
  },
  {
    "class_name": "src.tools.decorators.LoggedToolMixin",
    "skeleton": "\nclass LoggedToolMixin:\n    '''A mixin class that adds logging functionality to any tool.'''\n\n    def _log_operation(self, method_name: str, *args: Any, **kwargs: Any) -> None:\n        '''Helper method to log tool operations.'''\n        pass\n\n    def _run(self, *args: Any, **kwargs: Any) -> Any:\n        '''Override _run method to add logging.'''\n        pass",
    "snippet_id": "snippet_78"
  },
  {
    "class_name": "main.RTDETR",
    "skeleton": "\nclass RTDETR:\n    '''RTDETR object detection model class for handling inference and visualization.'''\n\n    def __init__(self, model_path, img_path, conf_thres=0.5, iou_thres=0.5):\n        '''\n        Initializes the RTDETR object with the specified parameters.\n        Args:\n            model_path: Path to the ONNX model file.\n            img_path: Path to the input image.\n            conf_thres: Confidence threshold for object detection.\n            iou_thres: IoU threshold for non-maximum suppression\n        '''\n        pass\n\n    def draw_detections(self, box, score, class_id):\n        '''\n        Draws bounding boxes and labels on the input image based on the detected objects.\n        Args:\n            box: Detected bounding box.\n            score: Corresponding detection score.\n            class_id: Class ID for the detected object.\n        Returns:\n            None\n                        '''\n                        pass\n\n    def preprocess(self):\n        '''\n        Preprocesses the input image before performing inference.\n        Returns:\n            image_data: Preprocessed image data ready for inference.\n        '''\n        pass\n\n    def bbox_cxcywh_to_xyxy(self, boxes):\n        '''\n        Converts bounding boxes from (center x, center y, width, height) format to (x_min, y_min, x_max, y_max) format.\n        Args:\n            boxes (numpy.ndarray): An array of shape (N, 4) where each row represents\n                                a bounding box in (cx, cy, w, h) format.\n        Returns:\n            numpy.ndarray: An array of shape (N, 4) where each row represents\n                        a bounding box in (x_min, y_min, x_max, y_max) format.\n        '''\n        pass\n\n    def postprocess(self, model_output):\n        '''\n        Postprocesses the model output to extract detections and draw them on the input image.\n        Args:\n            model_output: Output of the model inference.\n        Returns:\n            np.array: Annotated image with detections.\n        '''\n        pass\n\n    def main(self):\n        '''\n        Executes the detection on the input image using the ONNX model.\n        Returns:\n            np.array: Output image with annotations.\n        '''\n        pass",
    "snippet_id": "snippet_79"
  },
  {
    "class_name": "main.YOLOv8",
    "skeleton": "\nclass YOLOv8:\n    '''YOLOv8 object detection model class for handling inference and visualization.'''\n\n    def __init__(self, onnx_model, input_image, confidence_thres, iou_thres):\n        '''\n        Initializes an instance of the YOLOv8 class.\n        Args:\n            onnx_model: Path to the ONNX model.\n            input_image: Path to the input image.\n            confidence_thres: Confidence threshold for filtering detections.\n            iou_thres: IoU (Intersection over Union) threshold for non-maximum suppression.\n        '''\n        pass\n\n    def draw_detections(self, img, box, score, class_id):\n        '''\n        Draws bounding boxes and labels on the input image based on the detected objects.\n        Args:\n            img: The input image to draw detections on.\n            box: Detected bounding box.\n            score: Corresponding detection score.\n            class_id: Class ID for the detected object.\n        Returns:\n            None\n                        '''\n                        pass\n\n    def preprocess(self):\n        '''\n        Preprocesses the input image before performing inference.\n        Returns:\n            image_data: Preprocessed image data ready for inference.\n        '''\n        pass\n\n    def postprocess(self, input_image, output):\n        '''\n        Performs post-processing on the model's output to extract bounding boxes, scores, and class IDs.\n        Args:\n            input_image (numpy.ndarray): The input image.\n            output (numpy.ndarray): The output of the model.\n        Returns:\n            numpy.ndarray: The input image with detections drawn on it.\n        '''\n        pass\n\n    def main(self):\n        '''\n        Performs inference using an ONNX model and returns the output image with drawn detections.\n        Returns:\n            output_img: The output image with drawn detections.\n        '''\n        pass",
    "snippet_id": "snippet_80"
  },
  {
    "class_name": "main.YOLOv8TFLite",
    "skeleton": "\nclass YOLOv8TFLite:\n    '''\n    YOLOv8TFLite.\n    A class for performing object detection using the YOLOv8 model with TensorFlow Lite.\n    Attributes:\n        model (str): Path to the TensorFlow Lite model file.\n        conf (float): Confidence threshold for filtering detections.\n        iou (float): Intersection over Union threshold for non-maximum suppression.\n        metadata (Optional[str]): Path to the metadata file, if any.\n    Methods:\n        detect(img_path: str) -> np.ndarray:\n            Performs inference and returns the output image with drawn detections.\n    '''\n\n    def __init__(self, model: str, conf: float=0.25, iou: float=0.45, metadata: Union[str, None]=None):\n        '''\n        Initializes an instance of the YOLOv8TFLite class.\n        Args:\n            model (str): Path to the TFLite model.\n            conf (float, optional): Confidence threshold for filtering detections. Defaults to 0.25.\n            iou (float, optional): IoU (Intersection over Union) threshold for non-maximum suppression. Defaults to 0.45.\n            metadata (Union[str, None], optional): Path to the metadata file or None if not used. Defaults to None.\n        '''\n        pass\n\n    def letterbox(self, img: np.ndarray, new_shape: Tuple=(640, 640)) -> Tuple[np.ndarray, Tuple[float, float]]:\n        '''Resizes and reshapes images while maintaining aspect ratio by adding padding, suitable for YOLO models.'''\n        pass\n\n    def draw_detections(self, img: np.ndarray, box: np.ndarray, score: np.float32, class_id: int) -> None:\n        '''\n        Draws bounding boxes and labels on the input image based on the detected objects.\n        Args:\n            img (np.ndarray): The input image to draw detections on.\n            box (np.ndarray): Detected bounding box in the format [x1, y1, width, height].\n            score (np.float32): Corresponding detection score.\n            class_id (int): Class ID for the detected object.\n        Returns:\n            None\n                        '''\n                        pass\n\n    def preprocess(self, img: np.ndarray) -> Tuple[np.ndarray, Tuple[float, float]]:\n        '''\n        Preprocesses the input image before performing inference.\n        Args:\n            img (np.ndarray): The input image to be preprocessed.\n        Returns:\n            Tuple[np.ndarray, Tuple[float, float]]: A tuple containing:\n                - The preprocessed image (np.ndarray).\n                - A tuple of two float values representing the padding applied (top/bottom, left/right).\n        '''\n        pass\n\n    def postprocess(self, img: np.ndarray, outputs: np.ndarray, pad: Tuple[float, float]) -> np.ndarray:\n        '''\n        Performs post-processing on the model's output to extract bounding boxes, scores, and class IDs.\n        Args:\n            img (numpy.ndarray): The input image.\n            outputs (numpy.ndarray): The output of the model.\n            pad (Tuple[float, float]): Padding used by letterbox.\n        Returns:\n            numpy.ndarray: The input image with detections drawn on it.\n        '''\n        pass\n\n    def detect(self, img_path: str) -> np.ndarray:\n        '''\n        Performs inference using a TFLite model and returns the output image with drawn detections.\n        Args:\n            img_path (str): The path to the input image file.\n        Returns:\n            np.ndarray: The output image with drawn detections.\n        '''\n        pass",
    "snippet_id": "snippet_81"
  },
  {
    "class_name": "ultralytics.data.augment.Albumentations",
    "skeleton": "\nclass Albumentations:\n    '''\n    Albumentations transformations for image augmentation.\n    This class applies various image transformations using the Albumentations library. It includes operations such as\n    Blur, Median Blur, conversion to grayscale, Contrast Limited Adaptive Histogram Equalization (CLAHE), random changes\n    in brightness and contrast, RandomGamma, and image quality reduction through compression.\n    Attributes:\n        p (float): Probability of applying the transformations.\n        transform (albumentations.Compose): Composed Albumentations transforms.\n        contains_spatial (bool): Indicates if the transforms include spatial operations.\n    Methods:\n        __call__: Applies the Albumentations transformations to the input labels.\n    Examples:\n        >>> transform = Albumentations(p=0.5)\n        >>> augmented_labels = transform(labels)\n    Notes:\n        - The Albumentations package must be installed to use this class.\n        - If the package is not installed or an error occurs during initialization, the transform will be set to None.\n        - Spatial transforms are handled differently and require special processing for bounding boxes.\n    '''\n\n    def __init__(self, p=1.0):\n        '''\n        Initialize the Albumentations transform object for YOLO bbox formatted parameters.\n        This class applies various image augmentations using the Albumentations library, including Blur, Median Blur,\n        conversion to grayscale, Contrast Limited Adaptive Histogram Equalization, random changes of brightness and\n        contrast, RandomGamma, and image quality reduction through compression.\n        Args:\n            p (float): Probability of applying the augmentations. Must be between 0 and 1.\n        Attributes:\n            p (float): Probability of applying the augmentations.\n            transform (albumentations.Compose): Composed Albumentations transforms.\n            contains_spatial (bool): Indicates if the transforms include spatial transformations.\n        Raises:\n            ImportError: If the Albumentations package is not installed.\n            Exception: For any other errors during initialization.\n        Examples:\n            >>> transform = Albumentations(p=0.5)\n            >>> augmented = transform(image=image, bboxes=bboxes, class_labels=classes)\n            >>> augmented_image = augmented[\"image\"]\n            >>> augmented_bboxes = augmented[\"bboxes\"]\n        Notes:\n            - Requires Albumentations version 1.0.3 or higher.\n            - Spatial transforms are handled differently to ensure bbox compatibility.\n            - Some transforms are applied with very low probability (0.01) by default.\n        '''\n        pass\n\n    def __call__(self, labels):\n        '''\n        Applies Albumentations transformations to input labels.\n        This method applies a series of image augmentations using the Albumentations library. It can perform both\n        spatial and non-spatial transformations on the input image and its corresponding labels.\n        Args:\n            labels (Dict): A dictionary containing image data and annotations. Expected keys are:\n                - 'img': numpy.ndarray representing the image\n                - 'cls': numpy.ndarray of class labels\n                - 'instances': object containing bounding boxes and other instance information\n        Returns:\n            (Dict): The input dictionary with augmented image and updated annotations.\n        Examples:\n            >>> transform = Albumentations(p=0.5)\n            >>> labels = {\n            ...     \"img\": np.random.rand(640, 640, 3),\n            ...     \"cls\": np.array([0, 1]),\n            ...     \"instances\": Instances(bboxes=np.array([[0, 0, 1, 1], [0.5, 0.5, 0.8, 0.8]])),\n            ... }\n            >>> augmented = transform(labels)\n            >>> assert augmented[\"img\"].shape == (640, 640, 3)\n        Notes:\n            - The method applies transformations with probability self.p.\n            - Spatial transforms update bounding boxes, while non-spatial transforms only modify the image.\n            - Requires the Albumentations library to be installed.\n        '''\n        pass",
    "snippet_id": "snippet_82"
  },
  {
    "class_name": "ultralytics.data.augment.CenterCrop",
    "skeleton": "\nclass CenterCrop:\n    '''\n    Applies center cropping to images for classification tasks.\n    This class performs center cropping on input images, resizing them to a specified size while maintaining the aspect\n    ratio. It is designed to be part of a transformation pipeline, e.g., T.Compose([CenterCrop(size), ToTensor()]).\n    Attributes:\n        h (int): Target height of the cropped image.\n        w (int): Target width of the cropped image.\n    Methods:\n        __call__: Applies the center crop transformation to an input image.\n    Examples:\n        >>> transform = CenterCrop(640)\n        >>> image = np.random.randint(0, 255, (1080, 1920, 3), dtype=np.uint8)\n        >>> cropped_image = transform(image)\n        >>> print(cropped_image.shape)\n        (640, 640, 3)\n    '''\n\n    def __init__(self, size=640):\n        '''\n        Initializes the CenterCrop object for image preprocessing.\n        This class is designed to be part of a transformation pipeline, e.g., T.Compose([CenterCrop(size), ToTensor()]).\n        It performs a center crop on input images to a specified size.\n        Args:\n            size (int | Tuple[int, int]): The desired output size of the crop. If size is an int, a square crop\n                (size, size) is made. If size is a sequence like (h, w), it is used as the output size.\n        Returns:\n            (None): This method initializes the object and does not return anything.\n        Examples:\n            >>> transform = CenterCrop(224)\n            >>> img = np.random.rand(300, 300, 3)\n            >>> cropped_img = transform(img)\n            >>> print(cropped_img.shape)\n            (224, 224, 3)\n        '''\n        pass\n\n    def __call__(self, im):\n        '''\n        Applies center cropping to an input image.\n        This method resizes and crops the center of the image using a letterbox method. It maintains the aspect\n        ratio of the original image while fitting it into the specified dimensions.\n        Args:\n            im (numpy.ndarray | PIL.Image.Image): The input image as a numpy array of shape (H, W, C) or a\n                PIL Image object.\n        Returns:\n            (numpy.ndarray): The center-cropped and resized image as a numpy array of shape (self.h, self.w, C).\n        Examples:\n            >>> transform = CenterCrop(size=224)\n            >>> image = np.random.randint(0, 255, (640, 480, 3), dtype=np.uint8)\n            >>> cropped_image = transform(image)\n            >>> assert cropped_image.shape == (224, 224, 3)\n        '''\n        pass",
    "snippet_id": "snippet_83"
  },
  {
    "class_name": "ultralytics.data.augment.ClassifyLetterBox",
    "skeleton": "\nclass ClassifyLetterBox:\n    '''\n    A class for resizing and padding images for classification tasks.\n    This class is designed to be part of a transformation pipeline, e.g., T.Compose([LetterBox(size), ToTensor()]).\n    It resizes and pads images to a specified size while maintaining the original aspect ratio.\n    Attributes:\n        h (int): Target height of the image.\n        w (int): Target width of the image.\n        auto (bool): If True, automatically calculates the short side using stride.\n        stride (int): The stride value, used when 'auto' is True.\n    Methods:\n        __call__: Applies the letterbox transformation to an input image.\n    Examples:\n        >>> transform = ClassifyLetterBox(size=(640, 640), auto=False, stride=32)\n        >>> img = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n        >>> result = transform(img)\n        >>> print(result.shape)\n        (640, 640, 3)\n    '''\n\n    def __init__(self, size=(640, 640), auto=False, stride=32):\n        '''\n        Initializes the ClassifyLetterBox object for image preprocessing.\n        This class is designed to be part of a transformation pipeline for image classification tasks. It resizes and\n        pads images to a specified size while maintaining the original aspect ratio.\n        Args:\n            size (int | Tuple[int, int]): Target size for the letterboxed image. If an int, a square image of\n                (size, size) is created. If a tuple, it should be (height, width).\n            auto (bool): If True, automatically calculates the short side based on stride. Default is False.\n            stride (int): The stride value, used when 'auto' is True. Default is 32.\n        Attributes:\n            h (int): Target height of the letterboxed image.\n            w (int): Target width of the letterboxed image.\n            auto (bool): Flag indicating whether to automatically calculate short side.\n            stride (int): Stride value for automatic short side calculation.\n        Examples:\n            >>> transform = ClassifyLetterBox(size=224)\n            >>> img = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n            >>> result = transform(img)\n            >>> print(result.shape)\n            (224, 224, 3)\n        '''\n        pass\n\n    def __call__(self, im):\n        '''\n        Resizes and pads an image using the letterbox method.\n        This method resizes the input image to fit within the specified dimensions while maintaining its aspect ratio,\n        then pads the resized image to match the target size.\n        Args:\n            im (numpy.ndarray): Input image as a numpy array with shape (H, W, C).\n        Returns:\n            (numpy.ndarray): Resized and padded image as a numpy array with shape (hs, ws, 3), where hs and ws are\n                the target height and width respectively.\n        Examples:\n            >>> letterbox = ClassifyLetterBox(size=(640, 640))\n            >>> image = np.random.randint(0, 255, (720, 1280, 3), dtype=np.uint8)\n            >>> resized_image = letterbox(image)\n            >>> print(resized_image.shape)\n            (640, 640, 3)\n        '''\n        pass",
    "snippet_id": "snippet_84"
  },
  {
    "class_name": "ultralytics.data.augment.Format",
    "skeleton": "\nclass Format:\n    '''\n    A class for formatting image annotations for object detection, instance segmentation, and pose estimation tasks.\n    This class standardizes image and instance annotations to be used by the `collate_fn` in PyTorch DataLoader.\n    Attributes:\n        bbox_format (str): Format for bounding boxes. Options are 'xywh' or 'xyxy'.\n        normalize (bool): Whether to normalize bounding boxes.\n        return_mask (bool): Whether to return instance masks for segmentation.\n        return_keypoint (bool): Whether to return keypoints for pose estimation.\n        return_obb (bool): Whether to return oriented bounding boxes.\n        mask_ratio (int): Downsample ratio for masks.\n        mask_overlap (bool): Whether to overlap masks.\n        batch_idx (bool): Whether to keep batch indexes.\n        bgr (float): The probability to return BGR images.\n    Methods:\n        __call__: Formats labels dictionary with image, classes, bounding boxes, and optionally masks and keypoints.\n        _format_img: Converts image from Numpy array to PyTorch tensor.\n        _format_segments: Converts polygon points to bitmap masks.\n    Examples:\n        >>> formatter = Format(bbox_format=\"xywh\", normalize=True, return_mask=True)\n        >>> formatted_labels = formatter(labels)\n        >>> img = formatted_labels[\"img\"]\n        >>> bboxes = formatted_labels[\"bboxes\"]\n        >>> masks = formatted_labels[\"masks\"]\n    '''\n\n    def __init__(self, bbox_format='xywh', normalize=True, return_mask=False, return_keypoint=False, return_obb=False, mask_ratio=4, mask_overlap=True, batch_idx=True, bgr=0.0):\n        '''\n        Initializes the Format class with given parameters for image and instance annotation formatting.\n        This class standardizes image and instance annotations for object detection, instance segmentation, and pose\n        estimation tasks, preparing them for use in PyTorch DataLoader's `collate_fn`.\n        Args:\n            bbox_format (str): Format for bounding boxes. Options are 'xywh', 'xyxy', etc.\n            normalize (bool): Whether to normalize bounding boxes to [0,1].\n            return_mask (bool): If True, returns instance masks for segmentation tasks.\n            return_keypoint (bool): If True, returns keypoints for pose estimation tasks.\n            return_obb (bool): If True, returns oriented bounding boxes.\n            mask_ratio (int): Downsample ratio for masks.\n            mask_overlap (bool): If True, allows mask overlap.\n            batch_idx (bool): If True, keeps batch indexes.\n            bgr (float): Probability of returning BGR images instead of RGB.\n        Attributes:\n            bbox_format (str): Format for bounding boxes.\n            normalize (bool): Whether bounding boxes are normalized.\n            return_mask (bool): Whether to return instance masks.\n            return_keypoint (bool): Whether to return keypoints.\n            return_obb (bool): Whether to return oriented bounding boxes.\n            mask_ratio (int): Downsample ratio for masks.\n            mask_overlap (bool): Whether masks can overlap.\n            batch_idx (bool): Whether to keep batch indexes.\n            bgr (float): The probability to return BGR images.\n        Examples:\n            >>> format = Format(bbox_format=\"xyxy\", return_mask=True, return_keypoint=False)\n            >>> print(format.bbox_format)\n            xyxy\n        '''\n        pass\n\n    def __call__(self, labels):\n        '''\n        Formats image annotations for object detection, instance segmentation, and pose estimation tasks.\n        This method standardizes the image and instance annotations to be used by the `collate_fn` in PyTorch\n        DataLoader. It processes the input labels dictionary, converting annotations to the specified format and\n        applying normalization if required.\n        Args:\n            labels (Dict): A dictionary containing image and annotation data with the following keys:\n                - 'img': The input image as a numpy array.\n                - 'cls': Class labels for instances.\n                - 'instances': An Instances object containing bounding boxes, segments, and keypoints.\n        Returns:\n            (Dict): A dictionary with formatted data, including:\n                - 'img': Formatted image tensor.\n                - 'cls': Class label's tensor.\n                - 'bboxes': Bounding boxes tensor in the specified format.\n                - 'masks': Instance masks tensor (if return_mask is True).\n                - 'keypoints': Keypoints tensor (if return_keypoint is True).\n                - 'batch_idx': Batch index tensor (if batch_idx is True).\n        Examples:\n            >>> formatter = Format(bbox_format=\"xywh\", normalize=True, return_mask=True)\n            >>> labels = {\"img\": np.random.rand(640, 640, 3), \"cls\": np.array([0, 1]), \"instances\": Instances(...)}\n            >>> formatted_labels = formatter(labels)\n            >>> print(formatted_labels.keys())\n        '''\n        pass\n\n    def _format_img(self, img):\n        '''\n        Formats an image for YOLO from a Numpy array to a PyTorch tensor.\n        This function performs the following operations:\n        1. Ensures the image has 3 dimensions (adds a channel dimension if needed).\n        2. Transposes the image from HWC to CHW format.\n        3. Optionally flips the color channels from RGB to BGR.\n        4. Converts the image to a contiguous array.\n        5. Converts the Numpy array to a PyTorch tensor.\n        Args:\n            img (np.ndarray): Input image as a Numpy array with shape (H, W, C) or (H, W).\n        Returns:\n            (torch.Tensor): Formatted image as a PyTorch tensor with shape (C, H, W).\n        Examples:\n            >>> import numpy as np\n            >>> img = np.random.rand(100, 100, 3)\n            >>> formatted_img = self._format_img(img)\n            >>> print(formatted_img.shape)\n            torch.Size([3, 100, 100])\n        '''\n        pass\n\n    def _format_segments(self, instances, cls, w, h):\n        '''\n        Converts polygon segments to bitmap masks.\n        Args:\n            instances (Instances): Object containing segment information.\n            cls (numpy.ndarray): Class labels for each instance.\n            w (int): Width of the image.\n            h (int): Height of the image.\n        Returns:\n            masks (numpy.ndarray): Bitmap masks with shape (N, H, W) or (1, H, W) if mask_overlap is True.\n            instances (Instances): Updated instances object with sorted segments if mask_overlap is True.\n            cls (numpy.ndarray): Updated class labels, sorted if mask_overlap is True.\n        Notes:\n            - If self.mask_overlap is True, masks are overlapped and sorted by area.\n            - If self.mask_overlap is False, each mask is represented separately.\n            - Masks are downsampled according to self.mask_ratio.\n        '''\n        pass",
    "snippet_id": "snippet_85"
  },
  {
    "class_name": "ultralytics.data.augment.LetterBox",
    "skeleton": "\nclass LetterBox:\n    '''\n    Resize image and padding for detection, instance segmentation, pose.\n    This class resizes and pads images to a specified shape while preserving aspect ratio. It also updates\n    corresponding labels and bounding boxes.\n    Attributes:\n        new_shape (tuple): Target shape (height, width) for resizing.\n        auto (bool): Whether to use minimum rectangle.\n        scaleFill (bool): Whether to stretch the image to new_shape.\n        scaleup (bool): Whether to allow scaling up. If False, only scale down.\n        stride (int): Stride for rounding padding.\n        center (bool): Whether to center the image or align to top-left.\n    Methods:\n        __call__: Resize and pad image, update labels and bounding boxes.\n    Examples:\n        >>> transform = LetterBox(new_shape=(640, 640))\n        >>> result = transform(labels)\n        >>> resized_img = result[\"img\"]\n        >>> updated_instances = result[\"instances\"]\n    '''\n\n    def __init__(self, new_shape=(640, 640), auto=False, scaleFill=False, scaleup=True, center=True, stride=32):\n        '''\n        Initialize LetterBox object for resizing and padding images.\n        This class is designed to resize and pad images for object detection, instance segmentation, and pose estimation\n        tasks. It supports various resizing modes including auto-sizing, scale-fill, and letterboxing.\n        Args:\n            new_shape (Tuple[int, int]): Target size (height, width) for the resized image.\n            auto (bool): If True, use minimum rectangle to resize. If False, use new_shape directly.\n            scaleFill (bool): If True, stretch the image to new_shape without padding.\n            scaleup (bool): If True, allow scaling up. If False, only scale down.\n            center (bool): If True, center the placed image. If False, place image in top-left corner.\n            stride (int): Stride of the model (e.g., 32 for YOLOv5).\n        Attributes:\n            new_shape (Tuple[int, int]): Target size for the resized image.\n            auto (bool): Flag for using minimum rectangle resizing.\n            scaleFill (bool): Flag for stretching image without padding.\n            scaleup (bool): Flag for allowing upscaling.\n            stride (int): Stride value for ensuring image size is divisible by stride.\n        Examples:\n            >>> letterbox = LetterBox(new_shape=(640, 640), auto=False, scaleFill=False, scaleup=True, stride=32)\n            >>> resized_img = letterbox(original_img)\n        '''\n        pass\n\n    def __call__(self, labels=None, image=None):\n        '''\n        Resizes and pads an image for object detection, instance segmentation, or pose estimation tasks.\n        This method applies letterboxing to the input image, which involves resizing the image while maintaining its\n        aspect ratio and adding padding to fit the new shape. It also updates any associated labels accordingly.\n        Args:\n            labels (Dict | None): A dictionary containing image data and associated labels, or empty dict if None.\n            image (np.ndarray | None): The input image as a numpy array. If None, the image is taken from 'labels'.\n        Returns:\n            (Dict | Tuple): If 'labels' is provided, returns an updated dictionary with the resized and padded image,\n                updated labels, and additional metadata. If 'labels' is empty, returns a tuple containing the resized\n                and padded image, and a tuple of (ratio, (left_pad, top_pad)).\n        Examples:\n            >>> letterbox = LetterBox(new_shape=(640, 640))\n            >>> result = letterbox(labels={\"img\": np.zeros((480, 640, 3)), \"instances\": Instances(...)})\n            >>> resized_img = result[\"img\"]\n            >>> updated_instances = result[\"instances\"]\n        '''\n        pass\n    @staticmethod\n    def _update_labels(labels, ratio, padw, padh):\n        '''\n        Updates labels after applying letterboxing to an image.\n        This method modifies the bounding box coordinates of instances in the labels\n        to account for resizing and padding applied during letterboxing.\n        Args:\n            labels (Dict): A dictionary containing image labels and instances.\n            ratio (Tuple[float, float]): Scaling ratios (width, height) applied to the image.\n            padw (float): Padding width added to the image.\n            padh (float): Padding height added to the image.\n        Returns:\n            (Dict): Updated labels dictionary with modified instance coordinates.\n        Examples:\n            >>> letterbox = LetterBox(new_shape=(640, 640))\n            >>> labels = {\"instances\": Instances(...)}\n            >>> ratio = (0.5, 0.5)\n            >>> padw, padh = 10, 20\n            >>> updated_labels = letterbox._update_labels(labels, ratio, padw, padh)\n        '''\n        pass",
    "snippet_id": "snippet_86"
  },
  {
    "class_name": "ultralytics.data.augment.RandomLoadText",
    "skeleton": "\nclass RandomLoadText:\n    '''\n    Randomly samples positive and negative texts and updates class indices accordingly.\n    This class is responsible for sampling texts from a given set of class texts, including both positive\n    (present in the image) and negative (not present in the image) samples. It updates the class indices\n    to reflect the sampled texts and can optionally pad the text list to a fixed length.\n    Attributes:\n        prompt_format (str): Format string for text prompts.\n        neg_samples (Tuple[int, int]): Range for randomly sampling negative texts.\n        max_samples (int): Maximum number of different text samples in one image.\n        padding (bool): Whether to pad texts to max_samples.\n        padding_value (str): The text used for padding when padding is True.\n    Methods:\n        __call__: Processes the input labels and returns updated classes and texts.\n    Examples:\n        >>> loader = RandomLoadText(prompt_format=\"Object: {}\", neg_samples=(5, 10), max_samples=20)\n        >>> labels = {\"cls\": [0, 1, 2], \"texts\": [[\"cat\"], [\"dog\"], [\"bird\"]], \"instances\": [...]}\n        >>> updated_labels = loader(labels)\n        >>> print(updated_labels[\"texts\"])\n        ['Object: cat', 'Object: dog', 'Object: bird', 'Object: elephant', 'Object: car']\n    '''\n\n    def __init__(self, prompt_format: str='{}', neg_samples: Tuple[int, int]=(80, 80), max_samples:\n        '''\n        Initializes the RandomLoadText class for randomly sampling positive and negative texts.\n        This class is designed to randomly sample positive texts and negative texts, and update the class\n        indices accordingly to the number of samples. It can be used for text-based object detection tasks.\n        Args:\n            prompt_format (str): Format string for the prompt. Default is '{}'. The format string should\n                contain a single pair of curly braces {} where the text will be inserted.\n            neg_samples (Tuple[int, int]): A range to randomly sample negative texts. The first integer\n                specifies the minimum number of negative samples, and the second integer specifies the\n                maximum. Default is (80, 80).\n            max_samples (int): The maximum number of different text samples in one image. Default is 80.\n            padding (bool): Whether to pad texts to max_samples. If True, the number of texts will always\n                be equal to max_samples. Default is False.\n            padding_value (str): The padding text to use when padding is True. Default is an empty string.\n        Attributes:\n            prompt_format (str): The format string for the prompt.\n            neg_samples (Tuple[int, int]): The range for sampling negative texts.\n            max_samples (int): The maximum number of text samples.\n            padding (bool): Whether padding is enabled.\n            padding_value (str): The value used for padding.\n        Examples:\n            >>> random_load_text = RandomLoadText(prompt_format=\"Object: {}\", neg_samples=(50, 100), max_samples=120)\n            >>> random_load_text.prompt_format\n            'Object: {}'\n            >>> random_load_text.neg_samples\n            (50, 100)\n            >>> random_load_text.max_samples\n            120\n        '''\n        pass\n\n    def __call__(self, labels: dict) -> dict:\n        '''\n        Randomly samples positive and negative texts and updates class indices accordingly.\n        This method samples positive texts based on the existing class labels in the image, and randomly\n        selects negative texts from the remaining classes. It then updates the class indices to match the\n        new sampled text order.\n        Args:\n            labels (Dict): A dictionary containing image labels and metadata. Must include 'texts' and 'cls' keys.\n        Returns:\n            (Dict): Updated labels dictionary with new 'cls' and 'texts' entries.\n        Examples:\n            >>> loader = RandomLoadText(prompt_format=\"A photo of {}\", neg_samples=(5, 10), max_samples=20)\n            >>> labels = {\"cls\": np.array([[0], [1], [2]]), \"texts\": [[\"dog\"], [\"cat\"], [\"bird\"]]}\n            >>> updated_labels = loader(labels)\n        '''\n        pass",
    "snippet_id": "snippet_87"
  },
  {
    "class_name": "ultralytics.data.augment.ToTensor",
    "skeleton": "\nclass ToTensor:\n    '''\n    Converts an image from a numpy array to a PyTorch tensor.\n    This class is designed to be part of a transformation pipeline, e.g., T.Compose([LetterBox(size), ToTensor()]).\n    Attributes:\n        half (bool): If True, converts the image to half precision (float16).\n    Methods:\n        __call__: Applies the tensor conversion to an input image.\n    Examples:\n        >>> transform = ToTensor(half=True)\n        >>> img = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)\n        >>> tensor_img = transform(img)\n        >>> print(tensor_img.shape, tensor_img.dtype)\n        torch.Size([3, 640, 640]) torch.float16\n    Notes:\n        The input image is expected to be in BGR format with shape (H, W, C).\n        The output tensor will be in RGB format with shape (C, H, W), normalized to [0, 1].\n    '''\n\n    def __init__(self, half=False):\n        '''\n        Initializes the ToTensor object for converting images to PyTorch tensors.\n        This class is designed to be used as part of a transformation pipeline for image preprocessing in the\n        Ultralytics YOLO framework. It converts numpy arrays or PIL Images to PyTorch tensors, with an option\n        for half-precision (float16) conversion.\n        Args:\n            half (bool): If True, converts the tensor to half precision (float16). Default is False.\n        Examples:\n            >>> transform = ToTensor(half=True)\n            >>> img = np.random.rand(640, 640, 3)\n            >>> tensor_img = transform(img)\n            >>> print(tensor_img.dtype)\n            torch.float16\n        '''\n        pass\n\n    def __call__(self, im):\n        '''\n        Transforms an image from a numpy array to a PyTorch tensor.\n        This method converts the input image from a numpy array to a PyTorch tensor, applying optional\n        half-precision conversion and normalization. The image is transposed from HWC to CHW format and\n        the color channels are reversed from BGR to RGB.\n        Args:\n            im (numpy.ndarray): Input image as a numpy array with shape (H, W, C) in BGR order.\n        Returns:\n            (torch.Tensor): The transformed image as a PyTorch tensor in float32 or float16, normalized\n                to [0, 1] with shape (C, H, W) in RGB order.\n        Examples:\n            >>> transform = ToTensor(half=True)\n            >>> img = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)\n            >>> tensor_img = transform(img)\n            >>> print(tensor_img.shape, tensor_img.dtype)\n            torch.Size([3, 640, 640]) torch.float16\n        '''\n        pass",
    "snippet_id": "snippet_88"
  },
  {
    "class_name": "ultralytics.data.loaders.LoadImagesAndVideos",
    "skeleton": "\nclass LoadImagesAndVideos:\n    '''\n    A class for loading and processing images and videos for YOLO object detection.\n    This class manages the loading and pre-processing of image and video data from various sources, including\n    single image files, video files, and lists of image and video paths.\n    Attributes:\n        files (List[str]): List of image and video file paths.\n        nf (int): Total number of files (images and videos).\n        video_flag (List[bool]): Flags indicating whether a file is a video (True) or an image (False).\n        mode (str): Current mode, 'image' or 'video'.\n        vid_stride (int): Stride for video frame-rate.\n        bs (int): Batch size.\n        cap (cv2.VideoCapture): Video capture object for OpenCV.\n        frame (int): Frame counter for video.\n        frames (int): Total number of frames in the video.\n        count (int): Counter for iteration, initialized at 0 during __iter__().\n        ni (int): Number of images.\n    Methods:\n        __init__: Initialize the LoadImagesAndVideos object.\n        __iter__: Returns an iterator object for VideoStream or ImageFolder.\n        __next__: Returns the next batch of images or video frames along with their paths and metadata.\n        _new_video: Creates a new video capture object for the given path.\n        __len__: Returns the number of batches in the object.\n    Examples:\n        >>> loader = LoadImagesAndVideos(\"path/to/data\", batch=32, vid_stride=1)\n        >>> for paths, imgs, info in loader:\n        ...     # Process batch of images or video frames\n        ...     pass\n    Notes:\n        - Supports various image formats including HEIC.\n        - Handles both local files and directories.\n        - Can read from a text file containing paths to images and videos.\n    '''\n\n    def __init__(self, path, batch=1, vid_stride=1):\n        '''Initialize dataloader for images and videos, supporting various input formats.'''\n        pass\n\n    def __iter__(self):\n        '''Iterates through image/video files, yielding source paths, images, and metadata.'''\n        pass\n\n    def __next__(self):\n        '''Returns the next batch of images or video frames with their paths and metadata.'''\n        pass\n\n    def _new_video(self, path):\n        '''Creates a new video capture object for the given path and initializes video-related attributes.'''\n        pass\n\n    def __len__(self):\n        '''Returns the number of files (images and videos) in the dataset.'''\n        pass",
    "snippet_id": "snippet_89"
  },
  {
    "class_name": "ultralytics.data.loaders.LoadPilAndNumpy",
    "skeleton": "\nclass LoadPilAndNumpy:\n    '''\n    Load images from PIL and Numpy arrays for batch processing.\n    This class manages loading and pre-processing of image data from both PIL and Numpy formats. It performs basic\n    validation and format conversion to ensure that the images are in the required format for downstream processing.\n    Attributes:\n        paths (List[str]): List of image paths or autogenerated filenames.\n        im0 (List[np.ndarray]): List of images stored as Numpy arrays.\n        mode (str): Type of data being processed, set to 'image'.\n        bs (int): Batch size, equivalent to the length of `im0`.\n    Methods:\n        _single_check: Validate and format a single image to a Numpy array.\n    Examples:\n        >>> from PIL import Image\n        >>> import numpy as np\n        >>> pil_img = Image.new(\"RGB\", (100, 100))\n        >>> np_img = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)\n        >>> loader = LoadPilAndNumpy([pil_img, np_img])\n        >>> paths, images, _ = next(iter(loader))\n        >>> print(f\"Loaded {len(images)} images\")\n        Loaded 2 images\n    '''\n\n    def __init__(self, im0):\n        '''Initializes a loader for PIL and Numpy images, converting inputs to a standardized format.'''\n        pass\n    @staticmethod\n    def _single_check(im):\n        '''Validate and format an image to numpy array, ensuring RGB order and contiguous memory.'''\n        pass\n\n    def __len__(self):\n        '''Returns the length of the 'im0' attribute, representing the number of loaded images.'''\n        pass\n\n    def __next__(self):\n        '''Returns the next batch of images, paths, and metadata for processing.'''\n        pass\n\n    def __iter__(self):\n        '''Iterates through PIL/numpy images, yielding paths, raw images, and metadata for processing.'''\n        pass",
    "snippet_id": "snippet_90"
  },
  {
    "class_name": "ultralytics.data.loaders.LoadScreenshots",
    "skeleton": "\nclass LoadScreenshots:\n    '''\n    Ultralytics screenshot dataloader for capturing and processing screen images.\n    This class manages the loading of screenshot images for processing with YOLO. It is suitable for use with\n    `yolo predict source=screen`.\n    Attributes:\n        source (str): The source input indicating which screen to capture.\n        screen (int): The screen number to capture.\n        left (int): The left coordinate for screen capture area.\n        top (int): The top coordinate for screen capture area.\n        width (int): The width of the screen capture area.\n        height (int): The height of the screen capture area.\n        mode (str): Set to 'stream' indicating real-time capture.\n        frame (int): Counter for captured frames.\n        sct (mss.mss): Screen capture object from `mss` library.\n        bs (int): Batch size, set to 1.\n        fps (int): Frames per second, set to 30.\n        monitor (Dict[str, int]): Monitor configuration details.\n    Methods:\n        __iter__: Returns an iterator object.\n        __next__: Captures the next screenshot and returns it.\n    Examples:\n        >>> loader = LoadScreenshots(\"0 100 100 640 480\")  # screen 0, top-left (100,100), 640x480\n        >>> for source, im, im0s, vid_cap, s in loader:\n        ...     print(f\"Captured frame: {im.shape}\")\n    '''\n\n    def __init__(self, source):\n        '''Initialize screenshot capture with specified screen and region parameters.'''\n        pass\n\n    def __iter__(self):\n        '''Yields the next screenshot image from the specified screen or region for processing.'''\n        pass\n\n    def __next__(self):\n        '''Captures and returns the next screenshot as a numpy array using the mss library.'''\n        pass",
    "snippet_id": "snippet_91"
  },
  {
    "class_name": "ultralytics.data.loaders.LoadStreams",
    "skeleton": "\nclass LoadStreams:\n    '''\n    Stream Loader for various types of video streams.\n    Supports RTSP, RTMP, HTTP, and TCP streams. This class handles the loading and processing of multiple video\n    streams simultaneously, making it suitable for real-time video analysis tasks.\n    Attributes:\n        sources (List[str]): The source input paths or URLs for the video streams.\n        vid_stride (int): Video frame-rate stride.\n        buffer (bool): Whether to buffer input streams.\n        running (bool): Flag to indicate if the streaming thread is running.\n        mode (str): Set to 'stream' indicating real-time capture.\n        imgs (List[List[np.ndarray]]): List of image frames for each stream.\n        fps (List[float]): List of FPS for each stream.\n        frames (List[int]): List of total frames for each stream.\n        threads (List[Thread]): List of threads for each stream.\n        shape (List[Tuple[int, int, int]]): List of shapes for each stream.\n        caps (List[cv2.VideoCapture]): List of cv2.VideoCapture objects for each stream.\n        bs (int): Batch size for processing.\n    Methods:\n        update: Read stream frames in daemon thread.\n        close: Close stream loader and release resources.\n        __iter__: Returns an iterator object for the class.\n        __next__: Returns source paths, transformed, and original images for processing.\n        __len__: Return the length of the sources object.\n    Examples:\n        >>> stream_loader = LoadStreams(\"rtsp://example.com/stream1.mp4\")\n        >>> for sources, imgs, _ in stream_loader:\n        ...     # Process the images\n        ...     pass\n        >>> stream_loader.close()\n    Notes:\n        - The class uses threading to efficiently load frames from multiple streams simultaneously.\n        - It automatically handles YouTube links, converting them to the best available stream URL.\n        - The class implements a buffer system to manage frame storage and retrieval.\n            '''\n\n    def __init__(self, sources='file.streams', vid_stride=1, buffer=False):\n        '''Initialize stream loader for multiple video sources, supporting various stream types.'''\n        pass\n\n    def update(self, i, cap, stream):\n        '''Read stream frames in daemon thread and update image buffer.'''\n        pass\n\n    def close(self):\n        '''Terminates stream loader, stops threads, and releases video capture resources.'''\n        pass\n\n    def __iter__(self):\n        '''Iterates through YOLO image feed and re-opens unresponsive streams.'''\n        pass\n\n    def __next__(self):\n        '''Returns the next batch of frames from multiple video streams for processing.'''\n        pass\n\n    def __len__(self):\n        '''Return the number of video streams in the LoadStreams object.'''\n        pass",
    "snippet_id": "snippet_92"
  },
  {
    "class_name": "ultralytics.data.loaders.LoadTensor",
    "skeleton": "\nclass LoadTensor:\n    '''\n    A class for loading and processing tensor data for object detection tasks.\n    This class handles the loading and pre-processing of image data from PyTorch tensors, preparing them for\n    further processing in object detection pipelines.\n    Attributes:\n        im0 (torch.Tensor): The input tensor containing the image(s) with shape (B, C, H, W).\n        bs (int): Batch size, inferred from the shape of `im0`.\n        mode (str): Current processing mode, set to 'image'.\n        paths (List[str]): List of image paths or auto-generated filenames.\n    Methods:\n        _single_check: Validates and formats an input tensor.\n    Examples:\n        >>> import torch\n        >>> tensor = torch.rand(1, 3, 640, 640)\n        >>> loader = LoadTensor(tensor)\n        >>> paths, images, info = next(iter(loader))\n        >>> print(f\"Processed {len(images)} images\")\n    '''\n\n    def __init__(self, im0) -> None:\n        '''Initialize LoadTensor object for processing torch.Tensor image data.'''\n        pass\n    @staticmethod\n    def _single_check(im, stride=32):\n        '''Validates and formats a single image tensor, ensuring correct shape and normalization.'''\n        pass\n\n    def __iter__(self):\n        '''Yields an iterator object for iterating through tensor image data.'''\n        pass\n\n    def __next__(self):\n        '''Yields the next batch of tensor images and metadata for processing.'''\n        pass\n\n    def __len__(self):\n        '''Returns the batch size of the tensor input.'''\n        pass",
    "snippet_id": "snippet_93"
  },
  {
    "class_name": "ultralytics.data.utils.HUBDatasetStats",
    "skeleton": "\nclass HUBDatasetStats:\n    '''\n    A class for generating HUB dataset JSON and `-hub` dataset directory.\n    Args:\n        path (str): Path to data.yaml or data.zip (with data.yaml inside data.zip). Default is 'coco8.yaml'.\n        task (str): Dataset task. Options are 'detect', 'segment', 'pose', 'classify'. Default is 'detect'.\n        autodownload (bool): Attempt to download dataset if not found locally. Default is False.\n    Example:\n        Download *.zip files from https://github.com/ultralytics/hub/tree/main/example_datasets\n            i.e. https://github.com/ultralytics/hub/raw/main/example_datasets/coco8.zip for coco8.zip.\n        ```python\n        from ultralytics.data.utils import HUBDatasetStats\n        stats = HUBDatasetStats(\"path/to/coco8.zip\", task=\"detect\")  # detect dataset\n        stats = HUBDatasetStats(\"path/to/coco8-seg.zip\", task=\"segment\")  # segment dataset\n        stats = HUBDatasetStats(\"path/to/coco8-pose.zip\", task=\"pose\")  # pose dataset\n        stats = HUBDatasetStats(\"path/to/dota8.zip\", task=\"obb\")  # OBB dataset\n        stats = HUBDatasetStats(\"path/to/imagenet10.zip\", task=\"classify\")  # classification dataset\n        stats.get_json(save=True)\n        stats.process_images()\n        ```\n    '''\n\n    def __init__(self, path='coco8.yaml', task='detect', autodownload=False):\n        '''Initialize class.'''\n        pass\n    @staticmethod\n    def _unzip(path):\n        '''Unzip data.zip.'''\n        pass\n\n    def _hub_ops(self, f):\n        '''Saves a compressed image for HUB previews.'''\n        pass\n\n    def get_json(self, save=False, verbose=False):\n        '''Return dataset JSON for Ultralytics HUB.'''\n        pass\n\n        def _round(labels):\n                '''Update labels to integer class and 4 decimal place floats.'''\n                pass\n\n    def process_images(self):\n        '''Compress images for Ultralytics HUB.'''\n        pass",
    "snippet_id": "snippet_94"
  },
  {
    "class_name": "ultralytics.engine.tuner.Tuner",
    "skeleton": "\nclass Tuner:\n    '''\n    Class responsible for hyperparameter tuning of YOLO models.\n    The class evolves YOLO model hyperparameters over a given number of iterations\n    by mutating them according to the search space and retraining the model to evaluate their performance.\n    Attributes:\n        space (dict): Hyperparameter search space containing bounds and scaling factors for mutation.\n        tune_dir (Path): Directory where evolution logs and results will be saved.\n        tune_csv (Path): Path to the CSV file where evolution logs are saved.\n    Methods:\n        _mutate(hyp: dict) -> dict:\n            Mutates the given hyperparameters within the bounds specified in `self.space`.\n        __call__():\n            Executes the hyperparameter evolution across multiple iterations.\n    Example:\n        Tune hyperparameters for YOLOv8n on COCO8 at imgsz=640 and epochs=30 for 300 tuning iterations.\n        ```python\n        from ultralytics import YOLO\n        model = YOLO(\"yolo11n.pt\")\n        model.tune(data=\"coco8.yaml\", epochs=10, iterations=300, optimizer=\"AdamW\", plots=False, save=False, val=False)\n        ```\n        Tune with custom search space.\n        ```python\n        from ultralytics import YOLO\n        model = YOLO(\"yolo11n.pt\")\n        model.tune(space={key1: val1, key2: val2})  # custom search space dictionary\n        ```\n    '''\n\n    def __init__(self, args=DEFAULT_CFG, _callbacks=None):\n        '''\n        Initialize the Tuner with configurations.\n        Args:\n            args (dict, optional): Configuration for hyperparameter evolution.\n        '''\n        pass\n\n    def _mutate(self, parent='single', n=5, mutation=0.8, sigma=0.2):\n        '''\n        Mutates the hyperparameters based on bounds and scaling factors specified in `self.space`.\n        Args:\n            parent (str): Parent selection method: 'single' or 'weighted'.\n            n (int): Number of parents to consider.\n            mutation (float): Probability of a parameter mutation in any given iteration.\n            sigma (float): Standard deviation for Gaussian random number generator.\n        Returns:\n            (dict): A dictionary containing mutated hyperparameters.\n        '''\n        pass\n\n    def __call__(self, model=None, iterations=10, cleanup=True):\n        '''\n        Executes the hyperparameter evolution process when the Tuner instance is called.\n        This method iterates through the number of iterations, performing the following steps in each iteration:\n        1. Load the existing hyperparameters or initialize new ones.\n        2. Mutate the hyperparameters using the `mutate` method.\n        3. Train a YOLO model with the mutated hyperparameters.\n        4. Log the fitness score and mutated hyperparameters to a CSV file.\n        Args:\n           model (Model): A pre-initialized YOLO model to be used for training.\n           iterations (int): The number of generations to run the evolution for.\n           cleanup (bool): Whether to delete iteration weights to reduce storage space used during tuning.\n        Note:\n           The method utilizes the `self.tune_csv` Path object to read and log hyperparameters and fitness scores.\n           Ensure this path is set correctly in the Tuner instance.\n        '''\n        pass",
    "snippet_id": "snippet_95"
  },
  {
    "class_name": "ultralytics.hub.auth.Auth",
    "skeleton": "\nclass Auth:\n    '''\n    Manages authentication processes including API key handling, cookie-based authentication, and header generation.\n    The class supports different methods of authentication:\n    1. Directly using an API key.\n    2. Authenticating using browser cookies (specifically in Google Colab).\n    3. Prompting the user to enter an API key.\n    Attributes:\n        id_token (str or bool): Token used for identity verification, initialized as False.\n        api_key (str or bool): API key for authentication, initialized as False.\n        model_key (bool): Placeholder for model key, initialized as False.\n    '''\n\n    def __init__(self, api_key='', verbose=False):\n        '''\n        Initialize Auth class and authenticate user.\n        Handles API key validation, Google Colab authentication, and new key requests. Updates SETTINGS upon successful\n        authentication.\n        Args:\n            api_key (str): API key or combined key_id format.\n            verbose (bool): Enable verbose logging.\n        '''\n        pass\n\n    def request_api_key(self, max_attempts=3):\n        '''\n        Prompt the user to input their API key.\n        Returns the model ID.\n        '''\n        pass\n\n    def authenticate(self) -> bool:\n        '''\n        Attempt to authenticate with the server using either id_token or API key.\n        Returns:\n            (bool): True if authentication is successful, False otherwise.\n        '''\n        pass\n\n    def auth_with_cookies(self) -> bool:\n        '''\n        Attempt to fetch authentication via cookies and set id_token. User must be logged in to HUB and running in a\n        supported browser.\n        Returns:\n            (bool): True if authentication is successful, False otherwise.\n        '''\n        pass\n\n    def get_auth_header(self):\n        '''\n        Get the authentication header for making API requests.\n        Returns:\n            (dict): The authentication header if id_token or API key is set, None otherwise.\n        '''\n        pass",
    "snippet_id": "snippet_96"
  },
  {
    "class_name": "ultralytics.hub.google.GCPRegions",
    "skeleton": "\nclass GCPRegions:\n    '''\n    A class for managing and analyzing Google Cloud Platform (GCP) regions.\n    This class provides functionality to initialize, categorize, and analyze GCP regions based on their\n    geographical location, tier classification, and network latency.\n    Attributes:\n        regions (Dict[str, Tuple[int, str, str]]): A dictionary of GCP regions with their tier, city, and country.\n    Methods:\n        tier1: Returns a list of tier 1 GCP regions.\n        tier2: Returns a list of tier 2 GCP regions.\n        lowest_latency: Determines the GCP region(s) with the lowest network latency.\n    Examples:\n        >>> from ultralytics.hub.google import GCPRegions\n        >>> regions = GCPRegions()\n        >>> lowest_latency_region = regions.lowest_latency(verbose=True, attempts=3)\n        >>> print(f\"Lowest latency region: {lowest_latency_region[0][0]}\")\n    '''\n\n    def __init__(self):\n        '''Initializes the GCPRegions class with predefined Google Cloud Platform regions and their details.'''\n        pass\n\n    def tier1(self) -> List[str]:\n        '''Returns a list of GCP regions classified as tier 1 based on predefined criteria.'''\n        pass\n\n    def tier2(self) -> List[str]:\n        '''Returns a list of GCP regions classified as tier 2 based on predefined criteria.'''\n        pass\n    @staticmethod\n    def _ping_region(region: str, attempts: int=1) -> Tuple[str, float, float, float, float]:\n        '''Pings a specified GCP region and returns latency statistics: mean, min, max, and standard deviation.'''\n        pass\n\n    def lowest_latency(self, top: int=1, verbose: bool=False, tier: Optional[int]=None, attempts: int=1) -> List[Tuple[str, float, float, float, float]]:\n        '''\n        Determines the GCP regions with the lowest latency based on ping tests.\n        Args:\n            top (int): Number of top regions to return.\n            verbose (bool): If True, prints detailed latency information for all tested regions.\n            tier (int | None): Filter regions by tier (1 or 2). If None, all regions are tested.\n            attempts (int): Number of ping attempts per region.\n        Returns:\n            (List[Tuple[str, float, float, float, float]]): List of tuples containing region information and\n            latency statistics. Each tuple contains (region, mean_latency, std_dev, min_latency, max_latency).\n        Examples:\n            >>> regions = GCPRegions()\n            >>> results = regions.lowest_latency(top=3, verbose=True, tier=1, attempts=2)\n            >>> print(results[0][0])  # Print the name of the lowest latency region\n        '''\n        pass",
    "snippet_id": "snippet_97"
  },
  {
    "class_name": "ultralytics.hub.utils.Events",
    "skeleton": "\nclass Events:\n    '''\n    A class for collecting anonymous event analytics. Event analytics are enabled when sync=True in settings and\n    disabled when sync=False. Run 'yolo settings' to see and update settings.\n    Attributes:\n        url (str): The URL to send anonymous events.\n        rate_limit (float): The rate limit in seconds for sending events.\n        metadata (dict): A dictionary containing metadata about the environment.\n        enabled (bool): A flag to enable or disable Events based on certain conditions.\n    '''\n\n    def __init__(self):\n        '''Initializes the Events object with default values for events, rate_limit, and metadata.'''\n        pass\n\n    def __call__(self, cfg):\n        '''\n        Attempts to add a new event to the events list and send events if the rate limit is reached.\n        Args:\n            cfg (IterableSimpleNamespace): The configuration object containing mode and task information.\n        '''\n        pass",
    "snippet_id": "snippet_98"
  },
  {
    "class_name": "ultralytics.solutions.parking_management.ParkingPtsSelection",
    "skeleton": "\nclass ParkingPtsSelection:\n    '''\n    A class for selecting and managing parking zone points on images using a Tkinter-based UI.\n    This class provides functionality to upload an image, select points to define parking zones, and save the\n    selected points to a JSON file. It uses Tkinter for the graphical user interface.\n    Attributes:\n        tk (module): The Tkinter module for GUI operations.\n        filedialog (module): Tkinter's filedialog module for file selection operations.\n        messagebox (module): Tkinter's messagebox module for displaying message boxes.\n        master (tk.Tk): The main Tkinter window.\n        canvas (tk.Canvas): The canvas widget for displaying the image and drawing bounding boxes.\n        image (PIL.Image.Image): The uploaded image.\n        canvas_image (ImageTk.PhotoImage): The image displayed on the canvas.\n        rg_data (List[List[Tuple[int, int]]]): List of bounding boxes, each defined by 4 points.\n        current_box (List[Tuple[int, int]]): Temporary storage for the points of the current bounding box.\n        imgw (int): Original width of the uploaded image.\n        imgh (int): Original height of the uploaded image.\n        canvas_max_width (int): Maximum width of the canvas.\n        canvas_max_height (int): Maximum height of the canvas.\n    Methods:\n        initialize_properties: Initializes the necessary properties.\n        upload_image: Uploads an image, resizes it to fit the canvas, and displays it.\n        on_canvas_click: Handles mouse clicks to add points for bounding boxes.\n        draw_box: Draws a bounding box on the canvas.\n        remove_last_bounding_box: Removes the last bounding box and redraws the canvas.\n        redraw_canvas: Redraws the canvas with the image and all bounding boxes.\n        save_to_json: Saves the bounding boxes to a JSON file.\n    Examples:\n        >>> parking_selector = ParkingPtsSelection()\n        >>> # Use the GUI to upload an image, select parking zones, and save the data\n    '''\n\n    def __init__(self):\n        '''Initializes the ParkingPtsSelection class, setting up UI and properties for parking zone point selection.'''\n        pass\n\n    def initialize_properties(self):\n        '''Initialize properties for image, canvas, bounding boxes, and dimensions.'''\n        pass\n\n    def upload_image(self):\n        '''Uploads and displays an image on the canvas, resizing it to fit within specified dimensions.'''\n        pass\n\n    def on_canvas_click(self, event):\n        '''Handles mouse clicks to add points for bounding boxes on the canvas.'''\n        pass\n\n    def draw_box(self, box):\n        '''Draws a bounding box on the canvas using the provided coordinates.'''\n        pass\n\n    def remove_last_bounding_box(self):\n        '''Removes the last bounding box from the list and redraws the canvas.'''\n        pass\n\n    def redraw_canvas(self):\n        '''Redraws the canvas with the image and all bounding boxes.'''\n        pass\n\n    def save_to_json(self):\n        '''Saves the selected parking zone points to a JSON file with scaled coordinates.'''\n        pass",
    "snippet_id": "snippet_99"
  },
  {
    "class_name": "ultralytics.trackers.utils.gmc.GMC",
    "skeleton": "\nclass GMC:\n    '''\n    Generalized Motion Compensation (GMC) class for tracking and object detection in video frames.\n    This class provides methods for tracking and detecting objects based on several tracking algorithms including ORB,\n    SIFT, ECC, and Sparse Optical Flow. It also supports downscaling of frames for computational efficiency.\n    Attributes:\n        method (str): The method used for tracking. Options include 'orb', 'sift', 'ecc', 'sparseOptFlow', 'none'.\n        downscale (int): Factor by which to downscale the frames for processing.\n        prevFrame (np.ndarray): Stores the previous frame for tracking.\n        prevKeyPoints (List): Stores the keypoints from the previous frame.\n        prevDescriptors (np.ndarray): Stores the descriptors from the previous frame.\n        initializedFirstFrame (bool): Flag to indicate if the first frame has been processed.\n    Methods:\n        __init__: Initializes a GMC object with the specified method and downscale factor.\n        apply: Applies the chosen method to a raw frame and optionally uses provided detections.\n        apply_ecc: Applies the ECC algorithm to a raw frame.\n        apply_features: Applies feature-based methods like ORB or SIFT to a raw frame.\n        apply_sparseoptflow: Applies the Sparse Optical Flow method to a raw frame.\n        reset_params: Resets the internal parameters of the GMC object.\n    Examples:\n        Create a GMC object and apply it to a frame\n        >>> gmc = GMC(method=\"sparseOptFlow\", downscale=2)\n        >>> frame = np.array([[1, 2, 3], [4, 5, 6]])\n        >>> processed_frame = gmc.apply(frame)\n        >>> print(processed_frame)\n        array([[1, 2, 3],\n               [4, 5, 6]])\n    '''\n\n    def __init__(self, method: str='sparseOptFlow', downscale: int=2) -> None:\n        '''\n        Initialize a Generalized Motion Compensation (GMC) object with tracking method and downscale factor.\n        Args:\n            method (str): The method used for tracking. Options include 'orb', 'sift', 'ecc', 'sparseOptFlow', 'none'.\n            downscale (int): Downscale factor for processing frames.\n        Examples:\n            Initialize a GMC object with the 'sparseOptFlow' method and a downscale factor of 2\n            >>> gmc = GMC(method=\"sparseOptFlow\", downscale=2)\n        '''\n        pass\n\n    def apply(self, raw_frame: np.array, detections: list=None) -> np.array:\n        '''\n        Apply object detection on a raw frame using the specified method.\n        Args:\n            raw_frame (np.ndarray): The raw frame to be processed, with shape (H, W, C).\n            detections (List | None): List of detections to be used in the processing.\n        Returns:\n            (np.ndarray): Processed frame with applied object detection.\n        Examples:\n            >>> gmc = GMC(method=\"sparseOptFlow\")\n            >>> raw_frame = np.random.rand(480, 640, 3)\n            >>> processed_frame = gmc.apply(raw_frame)\n            >>> print(processed_frame.shape)\n            (480, 640, 3)\n        '''\n        pass\n\n    def apply_ecc(self, raw_frame: np.array) -> np.array:\n        '''\n        Apply the ECC (Enhanced Correlation Coefficient) algorithm to a raw frame for motion compensation.\n        Args:\n            raw_frame (np.ndarray): The raw frame to be processed, with shape (H, W, C).\n        Returns:\n            (np.ndarray): The processed frame with the applied ECC transformation.\n        Examples:\n            >>> gmc = GMC(method=\"ecc\")\n            >>> processed_frame = gmc.apply_ecc(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]))\n            >>> print(processed_frame)\n            [[1. 0. 0.]\n             [0. 1. 0.]]\n        '''\n        pass\n\n    def apply_features(self, raw_frame: np.array, detections: list=None) -> np.array:\n        '''\n        Apply feature-based methods like ORB or SIFT to a raw frame.\n        Args:\n            raw_frame (np.ndarray): The raw frame to be processed, with shape (H, W, C).\n            detections (List | None): List of detections to be used in the processing.\n        Returns:\n            (np.ndarray): Processed frame.\n        Examples:\n            >>> gmc = GMC(method=\"orb\")\n            >>> raw_frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n            >>> processed_frame = gmc.apply_features(raw_frame)\n            >>> print(processed_frame.shape)\n            (2, 3)\n        '''\n        pass\n\n    def apply_sparseoptflow(self, raw_frame: np.array) -> np.array:\n        '''\n        Apply Sparse Optical Flow method to a raw frame.\n        Args:\n            raw_frame (np.ndarray): The raw frame to be processed, with shape (H, W, C).\n        Returns:\n            (np.ndarray): Processed frame with shape (2, 3).\n        Examples:\n            >>> gmc = GMC()\n            >>> result = gmc.apply_sparseoptflow(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]))\n            >>> print(result)\n            [[1. 0. 0.]\n             [0. 1. 0.]]\n        '''\n        pass\n\n    def reset_params(self) -> None:\n        '''Reset the internal parameters including previous frame, keypoints, and descriptors.'''\n        pass",
    "snippet_id": "snippet_100"
  },
  {
    "class_name": "ultralytics.trackers.utils.kalman_filter.KalmanFilterXYAH",
    "skeleton": "\nclass KalmanFilterXYAH:\n    '''\n    A KalmanFilterXYAH class for tracking bounding boxes in image space using a Kalman filter.\n    Implements a simple Kalman filter for tracking bounding boxes in image space. The 8-dimensional state space\n    (x, y, a, h, vx, vy, va, vh) contains the bounding box center position (x, y), aspect ratio a, height h, and their\n    respective velocities. Object motion follows a constant velocity model, and bounding box location (x, y, a, h) is\n    taken as a direct observation of the state space (linear observation model).\n    Attributes:\n        _motion_mat (np.ndarray): The motion matrix for the Kalman filter.\n        _update_mat (np.ndarray): The update matrix for the Kalman filter.\n        _std_weight_position (float): Standard deviation weight for position.\n        _std_weight_velocity (float): Standard deviation weight for velocity.\n    Methods:\n        initiate: Creates a track from an unassociated measurement.\n        predict: Runs the Kalman filter prediction step.\n        project: Projects the state distribution to measurement space.\n        multi_predict: Runs the Kalman filter prediction step (vectorized version).\n        update: Runs the Kalman filter correction step.\n        gating_distance: Computes the gating distance between state distribution and measurements.\n    Examples:\n        Initialize the Kalman filter and create a track from a measurement\n        >>> kf = KalmanFilterXYAH()\n        >>> measurement = np.array([100, 200, 1.5, 50])\n        >>> mean, covariance = kf.initiate(measurement)\n        >>> print(mean)\n        >>> print(covariance)\n    '''\n\n    def __init__(self):\n        '''\n        Initialize Kalman filter model matrices with motion and observation uncertainty weights.\n        The Kalman filter is initialized with an 8-dimensional state space (x, y, a, h, vx, vy, va, vh), where (x, y)\n        represents the bounding box center position, 'a' is the aspect ratio, 'h' is the height, and their respective\n        velocities are (vx, vy, va, vh). The filter uses a constant velocity model for object motion and a linear\n        observation model for bounding box location.\n        Examples:\n            Initialize a Kalman filter for tracking:\n            >>> kf = KalmanFilterXYAH()\n        '''\n        pass\n\n    def initiate(self, measurement: np.ndarray) -> tuple:\n        '''\n        Create a track from an unassociated measurement.\n        Args:\n            measurement (ndarray): Bounding box coordinates (x, y, a, h) with center position (x, y), aspect ratio a,\n                and height h.\n        Returns:\n            (tuple[ndarray, ndarray]): Returns the mean vector (8-dimensional) and covariance matrix (8x8 dimensional)\n                of the new track. Unobserved velocities are initialized to 0 mean.\n        Examples:\n            >>> kf = KalmanFilterXYAH()\n            >>> measurement = np.array([100, 50, 1.5, 200])\n            >>> mean, covariance = kf.initiate(measurement)\n        '''\n        pass\n\n    def predict(self, mean: np.ndarray, covariance: np.ndarray) -> tuple:\n        '''\n        Run Kalman filter prediction step.\n        Args:\n            mean (ndarray): The 8-dimensional mean vector of the object state at the previous time step.\n            covariance (ndarray): The 8x8-dimensional covariance matrix of the object state at the previous time step.\n        Returns:\n            (tuple[ndarray, ndarray]): Returns the mean vector and covariance matrix of the predicted state. Unobserved\n                velocities are initialized to 0 mean.\n        Examples:\n            >>> kf = KalmanFilterXYAH()\n            >>> mean = np.array([0, 0, 1, 1, 0, 0, 0, 0])\n            >>> covariance = np.eye(8)\n            >>> predicted_mean, predicted_covariance = kf.predict(mean, covariance)\n        '''\n        pass\n\n    def project(self, mean: np.ndarray, covariance: np.ndarray) -> tuple:\n        '''\n        Project state distribution to measurement space.\n        Args:\n            mean (ndarray): The state's mean vector (8 dimensional array).\n            covariance (ndarray): The state's covariance matrix (8x8 dimensional).\n        Returns:\n            (tuple[ndarray, ndarray]): Returns the projected mean and covariance matrix of the given state estimate.\n        Examples:\n            >>> kf = KalmanFilterXYAH()\n            >>> mean = np.array([0, 0, 1, 1, 0, 0, 0, 0])\n            >>> covariance = np.eye(8)\n            >>> projected_mean, projected_covariance = kf.project(mean, covariance)\n        '''\n        pass\n\n    def multi_predict(self, mean: np.ndarray, covariance: np.ndarray) -> tuple:\n        '''\n        Run Kalman filter prediction step for multiple object states (Vectorized version).\n        Args:\n            mean (ndarray): The Nx8 dimensional mean matrix of the object states at the previous time step.\n            covariance (ndarray): The Nx8x8 covariance matrix of the object states at the previous time step.\n        Returns:\n            (tuple[ndarray, ndarray]): Returns the mean matrix and covariance matrix of the predicted states.\n                The mean matrix has shape (N, 8) and the covariance matrix has shape (N, 8, 8). Unobserved velocities\n                are initialized to 0 mean.\n        Examples:\n            >>> mean = np.random.rand(10, 8)  # 10 object states\n            >>> covariance = np.random.rand(10, 8, 8)  # Covariance matrices for 10 object states\n            >>> predicted_mean, predicted_covariance = kalman_filter.multi_predict(mean, covariance)\n        '''\n        pass\n\n    def update(self, mean: np.ndarray, covariance: np.ndarray, measurement: np.ndarray) -> tuple:\n        '''\n        Run Kalman filter correction step.\n        Args:\n            mean (ndarray): The predicted state's mean vector (8 dimensional).\n            covariance (ndarray): The state's covariance matrix (8x8 dimensional).\n            measurement (ndarray): The 4 dimensional measurement vector (x, y, a, h), where (x, y) is the center\n                position, a the aspect ratio, and h the height of the bounding box.\n        Returns:\n            (tuple[ndarray, ndarray]): Returns the measurement-corrected state distribution.\n        Examples:\n            >>> kf = KalmanFilterXYAH()\n            >>> mean = np.array([0, 0, 1, 1, 0, 0, 0, 0])\n            >>> covariance = np.eye(8)\n            >>> measurement = np.array([1, 1, 1, 1])\n            >>> new_mean, new_covariance = kf.update(mean, covariance, measurement)\n        '''\n        pass\n\n    def gating_distance(self, mean: np.ndarray, covariance: np.ndarray, measurements: np.ndarray, only_position: bool=False, metric: str='maha') -> np.ndarray:\n        '''\n        Compute gating distance between state distribution and measurements.\n        A suitable distance threshold can be obtained from `chi2inv95`. If `only_position` is False, the chi-square\n        distribution has 4 degrees of freedom, otherwise 2.\n        Args:\n            mean (ndarray): Mean vector over the state distribution (8 dimensional).\n            covariance (ndarray): Covariance of the state distribution (8x8 dimensional).\n            measurements (ndarray): An (N, 4) matrix of N measurements, each in format (x, y, a, h) where (x, y) is the\n                bounding box center position, a the aspect ratio, and h the height.\n            only_position (bool): If True, distance computation is done with respect to box center position only.\n            metric (str): The metric to use for calculating the distance. Options are 'gaussian' for the squared\n                Euclidean distance and 'maha' for the squared Mahalanobis distance.\n        Returns:\n            (np.ndarray): Returns an array of length N, where the i-th element contains the squared distance between\n                (mean, covariance) and `measurements[i]`.\n        Examples:\n            Compute gating distance using Mahalanobis metric:\n            >>> kf = KalmanFilterXYAH()\n            >>> mean = np.array([0, 0, 1, 1, 0, 0, 0, 0])\n            >>> covariance = np.eye(8)\n            >>> measurements = np.array([[1, 1, 1, 1], [2, 2, 1, 1]])\n            >>> distances = kf.gating_distance(mean, covariance, measurements, only_position=False, metric=\"maha\")\n        '''\n        pass",
    "snippet_id": "snippet_101"
  },
  {
    "class_name": "ultralytics.utils.benchmarks.RF100Benchmark",
    "skeleton": "\nclass RF100Benchmark:\n    '''Benchmark YOLO model performance across various formats for speed and accuracy.'''\n\n    def __init__(self):\n        '''Initialize the RF100Benchmark class for benchmarking YOLO model performance across various formats.'''\n        pass\n\n    def set_key(self, api_key):\n        '''\n        Set Roboflow API key for processing.\n        Args:\n            api_key (str): The API key.\n        Examples:\n            Set the Roboflow API key for accessing datasets:\n            >>> benchmark = RF100Benchmark()\n            >>> benchmark.set_key(\"your_roboflow_api_key\")\n        '''\n        pass\n\n    def parse_dataset(self, ds_link_txt='datasets_links.txt'):\n        '''\n        Parse dataset links and download datasets.\n        Args:\n            ds_link_txt (str): Path to the file containing dataset links.\n        Examples:\n            >>> benchmark = RF100Benchmark()\n            >>> benchmark.set_key(\"api_key\")\n            >>> benchmark.parse_dataset(\"datasets_links.txt\")\n        '''\n        pass\n    @staticmethod\n    def fix_yaml(path):\n        '''\n        Fixes the train and validation paths in a given YAML file.\n        Args:\n            path (str): Path to the YAML file to be fixed.\n        Examples:\n            >>> RF100Benchmark.fix_yaml(\"path/to/data.yaml\")\n        '''\n        pass\n\n    def evaluate(self, yaml_path, val_log_file, eval_log_file, list_ind):\n        '''\n        Evaluate model performance on validation results.\n        Args:\n            yaml_path (str): Path to the YAML configuration file.\n            val_log_file (str): Path to the validation log file.\n            eval_log_file (str): Path to the evaluation log file.\n            list_ind (int): Index of the current dataset in the list.\n        Returns:\n            (float): The mean average precision (mAP) value for the evaluated model.\n        Examples:\n            Evaluate a model on a specific dataset\n            >>> benchmark = RF100Benchmark()\n            >>> benchmark.evaluate(\"path/to/data.yaml\", \"path/to/val_log.txt\", \"path/to/eval_log.txt\", 0)\n        '''\n        pass",
    "snippet_id": "snippet_102"
  },
  {
    "class_name": "ultralytics.utils.triton.TritonRemoteModel",
    "skeleton": "\nclass TritonRemoteModel:\n    '''\n    Client for interacting with a remote Triton Inference Server model.\n    Attributes:\n        endpoint (str): The name of the model on the Triton server.\n        url (str): The URL of the Triton server.\n        triton_client: The Triton client (either HTTP or gRPC).\n        InferInput: The input class for the Triton client.\n        InferRequestedOutput: The output request class for the Triton client.\n        input_formats (List[str]): The data types of the model inputs.\n        np_input_formats (List[type]): The numpy data types of the model inputs.\n        input_names (List[str]): The names of the model inputs.\n        output_names (List[str]): The names of the model outputs.\n    '''\n\n    def __init__(self, url: str, endpoint: str='', scheme: str=''):\n        '''\n        Initialize the TritonRemoteModel.\n        Arguments may be provided individually or parsed from a collective 'url' argument of the form\n            <scheme>://<netloc>/<endpoint>/<task_name>\n        Args:\n            url (str): The URL of the Triton server.\n            endpoint (str): The name of the model on the Triton server.\n            scheme (str): The communication scheme ('http' or 'grpc').\n        '''\n        pass\n\n    def __call__(self, *inputs: np.ndarray) -> List[np.ndarray]:\n        '''\n        Call the model with the given inputs.\n        Args:\n            *inputs (List[np.ndarray]): Input data to the model.\n        Returns:\n            (List[np.ndarray]): Model outputs.\n        '''\n        pass",
    "snippet_id": "snippet_103"
  },
  {
    "class_name": "yolov8_sahi.SAHIInference",
    "skeleton": "\nclass SAHIInference:\n    '''Runs Ultralytics YOLO11 and SAHI for object detection on video with options to view, save, and track results.'''\n\n    def __init__(self):\n        '''Initializes the SAHIInference class for performing sliced inference using SAHI with YOLO11 models.'''\n        pass\n\n    def load_model(self, weights):\n        '''Loads a YOLO11 model with specified weights for object detection using SAHI.'''\n        pass\n\n    def inference(self, weights='yolo11n.pt', source='test.mp4', view_img=False, save_img=False, exist_ok=False):\n        '''\n        Run object detection on a video using YOLO11 and SAHI.\n        Args:\n            weights (str): Model weights path.\n            source (str): Video file path.\n            view_img (bool): Show results.\n            save_img (bool): Save results.\n            exist_ok (bool): Overwrite existing files.\n        '''\n        pass\n\n    def parse_opt(self):\n        '''Parse command line arguments.'''\n        pass",
    "snippet_id": "snippet_104"
  },
  {
    "class_name": "chonkie.experimental.code_registry._CodeLanguageRegistry",
    "skeleton": "\nclass _CodeLanguageRegistry:\n    '''Registry to store language configurations.'''\n\n    def __init__(self) -> None:\n        '''Initialize the registry.'''\n        pass\n\n    def register(self, language: str, config: LanguageConfig) -> None:\n        '''Register a language configuration.'''\n        pass\n\n    def get(self, language: str) -> LanguageConfig:\n        '''Get a language configuration.'''\n        pass\n\n    def __contains__(self, language: str) -> bool:\n        '''Check if a language is registered.'''\n        pass\n\n    def __getitem__(self, language: str) -> LanguageConfig:\n        '''Get a language configuration.'''\n        pass\n\n    def keys(self) -> KeysView[str]:\n        '''Get all registered language keys.'''\n        pass",
    "snippet_id": "snippet_105"
  },
  {
    "class_name": "chonkie.friends.handshakes.chroma.ChromaEmbeddingFunction",
    "skeleton": "\nclass ChromaEmbeddingFunction:\n    '''Chroma Embedding Function.\n    Embeds the text of the chunks using the embedding model and \n    adds the embeddings to the chunks for use in downstream tasks\n    like upserting into a vector database.\n    Args:\n        embedding_model: The embedding model to use.\n        **kwargs: Additional keyword arguments.\n    '''\n\n    def __init__(self, embedding_model: Union[str, BaseEmbeddings]='minishlab/potion-retrieval-32M', **kwargs: Dict[str, Any]) -> None:\n        '''Initialize the ChromaEmbeddingFunction.'''\n        pass\n\n    def name(self) -> str:\n        '''Return the name of the embedding model for ChromaDB compatibility.'''\n        pass\n\n    def __call__(self, input: Union[str, List[str]]) -> Union['np.ndarray', List['np.ndarray']]:\n        '''Call the ChromaEmbeddingFunction.'''\n        pass",
    "snippet_id": "snippet_106"
  },
  {
    "class_name": "chonkie.types.base.Context",
    "skeleton": "@dataclass\nclass Context:\n    '''Context class to hold chunk metadata.\n    Attributes:\n        text (str): The text of the chunk.\n        start_index (Optional[int]): The starting index of the chunk in the original text.\n        end_index (Optional[int]): The ending index of the chunk in the original text.\n        token_count (int): The number of tokens in the chunk.\n    '''\n\n    def __post_init__(self) -> None:\n        '''Validate context attributes.'''\n        pass\n\n    def __len__(self) -> int:\n        '''Return the length of the text.'''\n        pass\n\n    def __str__(self) -> str:\n        '''Return a string representation of the Context.'''\n        pass\n\n    def __repr__(self) -> str:\n        '''Return a detailed string representation of the Context.'''\n        pass\n\n    def to_dict(self) -> dict:\n        '''Return the Context as a dictionary.'''\n        pass\n    @classmethod\n    def from_dict(cls, data: dict) -> 'Context':\n        '''Create a Context object from a dictionary.'''\n        pass",
    "snippet_id": "snippet_107"
  },
  {
    "class_name": "chonkie.types.recursive.RecursiveLevel",
    "skeleton": "@dataclass\nclass RecursiveLevel:\n    '''RecursiveLevels express the chunking rules at a specific level for the recursive chunker.\n    Attributes:\n        whitespace (bool): Whether to use whitespace as a delimiter.\n        delimiters (Optional[Union[str, List[str]]]): Custom delimiters for chunking.\n        include_delim (Optional[Literal[\"prev\", \"next\"]]): Whether to include the delimiter at all, or in the previous chunk, or the next chunk.\n        pattern (Optional[str]): Regex pattern for advanced splitting/extraction.\n        pattern_mode (Literal[\"split\", \"extract\"]): Whether to split on pattern matches or extract pattern matches.\n    '''\n\n    def _validate_fields(self) -> None:\n        '''Validate all fields have legal values.'''\n        pass\n\n    def __post_init__(self) -> None:\n        '''Validate attributes.'''\n        pass\n\n    def __repr__(self) -> str:\n        '''Return a string representation of the RecursiveLevel.'''\n        pass\n\n    def to_dict(self) -> dict:\n        '''Return the RecursiveLevel as a dictionary.'''\n        pass\n    @classmethod\n    def from_dict(cls, data: dict) -> 'RecursiveLevel':\n        '''Create RecursiveLevel object from a dictionary.'''\n        pass\n    @classmethod\n    def from_recipe(cls, name: str, lang: Optional[str]='en') -> 'RecursiveLevel':\n        '''Create RecursiveLevel object from a recipe.\n        The recipes are registered in the [Chonkie Recipe Store](https://huggingface.co/datasets/chonkie-ai/recipes). If the recipe is not there, you can create your own recipe and share it with the community!\n        Args:\n            name (str): The name of the recipe.\n            lang (Optional[str]): The language of the recipe.\n        Returns:\n            RecursiveLevel: The RecursiveLevel object.\n        Raises:\n            ValueError: If the recipe is not found.\n        '''\n        pass",
    "snippet_id": "snippet_108"
  },
  {
    "class_name": "chonkie.types.sentence.Sentence",
    "skeleton": "@dataclass\nclass Sentence:\n    '''Class to represent a sentence.\n    Attributes:\n        text (str): The text of the sentence.\n        start_index (int): The starting index of the sentence in the original text.\n        end_index (int): The ending index of the sentence in the original text.\n        token_count (int): The number of tokens in the sentence.\n    '''\n\n    def __post_init__(self) -> None:\n        '''Validate attributes.'''\n        pass\n\n    def __repr__(self) -> str:\n        '''Return a string representation of the Sentence.'''\n        pass\n\n    def to_dict(self) -> Dict[str, Union[str, int]]:\n        '''Return the Chunk as a dictionary.'''\n        pass\n    @classmethod\n    def from_dict(cls, data: Dict[str, Union[str, int]]) -> 'Sentence':\n        '''Create a Sentence object from a dictionary.'''\n        pass",
    "snippet_id": "snippet_109"
  },
  {
    "class_name": "chonkie.utils.hub.Hubbie",
    "skeleton": "\nclass Hubbie:\n    '''Hubbie is a Huggingface hub manager for Chonkie.\n    Methods:\n        get_recipe(recipe_name: str, lang: Optional[str] = 'en') -> Optional[Dict]:\n            Get a recipe from the hub.\n        get_recipe_schema() -> Dict:\n            Get the current recipe schema from the hub.\n    '''\n\n    def __init__(self) -> None:\n        '''Initialize Hubbie.'''\n        pass\n\n    def _import_dependencies(self) -> None:\n        '''Check if the required dependencies are available and import them.'''\n        pass\n\n    def _check_dependencies(self) -> Optional[bool]:\n        '''Check if the required dependencies are available.'''\n        pass\n\n    def get_recipe_schema(self) -> Dict:\n        '''Get the current recipe schema from the hub.'''\n        pass\n\n    def _validate_recipe(self, recipe: Dict) -> Optional[bool]:\n        '''Validate a recipe against the current schema.'''\n        pass\n\n    def get_recipe_schema(self) -> Dict:\n        '''Get a recipe from the hub.\n        Args:\n            name (Optional[str]): The name of the recipe to get.\n            lang (Optional[str]): The language of the recipe to get.\n            path (Optional[str]): Optionally, provide the path to the recipe.\n        Returns:\n            Optional[Dict]: The recipe.\n        Raises:\n            ValueError: If the recipe is not found.\n            ValueError: If neither (name, lang) nor path are provided.\n            ValueError: If the recipe is invalid.\n        '''\n        pass",
    "snippet_id": "snippet_110"
  },
  {
    "class_name": "integrations.utils.git_utils.GitDiffTracker",
    "skeleton": "\nclass GitDiffTracker:\n    '''Tracks git changes from an initial state through a session.'''\n\n    def __init__(self, enabled: bool=True, logger: Optional[logging.Logger]=None, cwd: Optional[str]=None):\n        '''Initialize the git diff tracker.\n        Args:\n            enabled: Whether to enable git diff tracking (default: True)\n            logger: Optional logger instance to use for logging. If not provided,\n                    creates a default logger for this module.\n            cwd: Working directory for git commands (default: current directory)\n        '''\n        pass\n\n    def _capture_initial_state(self) -> None:\n        '''Capture the initial git commit hash if in a git repository.'''\n        pass\n\n    def get_diff(self) -> Optional[str]:\n        '''Get the current git diff from the initial state.\n        Returns:\n            The git diff output if enabled and there are changes, None otherwise.\n        '''\n        pass\n\n    def _get_worktree_exclusions(self) -> list[str]:\n        '''Get list of worktree paths to exclude from diff.\n        Returns:\n            List of exclusion patterns for git commands.\n        '''\n        pass\n\n    def _get_untracked_files(self, exclude_patterns: list[str]) -> str:\n        '''Get untracked files formatted as git diff output.\n        Args:\n            exclude_patterns: List of patterns to exclude from the output.\n        Returns:\n            Formatted diff-like output for untracked files.\n        '''\n        pass",
    "snippet_id": "snippet_111"
  },
  {
    "class_name": "session_reset_handler.SessionResetHandler",
    "skeleton": "\nclass SessionResetHandler:\n    '''Handles detection and recovery from Claude session resets'''\n\n    def __init__(self, log_func=None):\n        '''Initialize the handler\n        Args:\n            log_func: Optional logging function\n        '''\n        pass\n\n    def check_for_reset_command(self, command: str) -> bool:\n        '''Check if a command is a session reset command'''\n        pass\n\n    def mark_reset_detected(self, command: str) -> None:\n        '''Mark that a session reset has been detected'''\n        pass\n\n    def is_reset_pending(self) -> bool:\n        '''Check if a session reset is pending'''\n        pass\n\n    def clear_reset_state(self) -> None:\n        '''Clear the reset state after handling'''\n        pass\n\n    def get_reset_info(self) -> Tuple[Optional[str], Optional[float]]:\n        '''Get information about the pending reset'''\n        pass\n\n    def find_reset_session_file(self, project_dir: Path, current_file: Path, max_wait: float=10.0) -> Optional[Path]:\n        '''Find a new session file created after a reset\n        Looks for JSONL files created after the reset time that contain\n        <command-name>/clear</command-name> in the first few lines.\n        Args:\n            project_dir: Directory to search in\n            current_file: Current JSONL file being monitored\n            max_wait: Maximum time to wait in seconds\n        Returns:\n            Path to new JSONL file if found, None otherwise\n        '''\n        pass\n\n    def _file_has_clear_command(self, file: Path) -> bool:\n        '''Check if a JSONL file starts with the /clear command'''\n        pass",
    "snippet_id": "snippet_112"
  },
  {
    "class_name": "biomni.config.BiomniConfig",
    "skeleton": "@dataclass\nclass BiomniConfig:\n    '''Central configuration for Biomni agent.\n    All settings are optional and have sensible defaults.\n    API keys are still read from environment variables to maintain\n    compatibility with existing .env file structure.\n    Usage:\n        # Create config with defaults\n        config = BiomniConfig()\n        # Override specific settings\n        config = BiomniConfig(llm=\"gpt-4\", timeout_seconds=1200)\n        # Modify after creation\n        config.path = \"./custom_data\"\n    '''\n\n    def __post_init__(self):\n        '''Load any environment variable overrides if they exist.'''\n        pass\n\n    def to_dict(self) -> dict:\n        '''Convert config to dictionary for easy access.'''\n        pass",
    "snippet_id": "snippet_113"
  },
  {
    "class_name": "nunchaku-tech_ComfyUI-nunchaku.nodes.models.pulid.NunchakuPulidLoader",
    "skeleton": "\nclass NunchakuPulidLoader:\n    '''\n    Deprecated node for loading the PuLID pipeline for a Nunchaku FLUX model.\n    .. warning::\n        This node is deprecated and will be removed in December 2025.\n        Use :class:`NunchakuPuLIDLoaderV2` instead.\n    Attributes\n    ----------\n    pulid_device : str\n        Device to load the PuLID pipeline on (default: \"cuda\").\n    weight_dtype : torch.dtype\n        Data type for model weights (default: torch.bfloat16).\n    onnx_provider : str\n        ONNX provider to use (default: \"gpu\").\n    pretrained_model : str or None\n        Path to the pretrained PuLID model, if any.\n    '''\n\n    def __init__(self):\n        '''\n        Initialize the loader with default device, dtype, and ONNX provider.\n        '''\n        pass\n    @classmethod\n    def INPUT_TYPES(s):\n        '''\n        Returns the required input types for this node.\n        Returns\n        -------\n        dict\n            Dictionary specifying required inputs.\n        '''\n        pass\n\n    def load(self, model):\n        '''\n        Load the PuLID pipeline for the given Nunchaku FLUX model.\n        .. warning::\n            This node is deprecated and will be removed in December 2025.\n            Use :class:`NunchakuPuLIDLoaderV2` instead.\n        Parameters\n        ----------\n        model : object\n            The Nunchaku FLUX model.\n        Returns\n        -------\n        tuple\n            The input model and the loaded PuLID pipeline.\n        '''\n        pass",
    "snippet_id": "snippet_114"
  },
  {
    "class_name": "nunchaku-tech_ComfyUI-nunchaku.nodes.models.qwenimage.NunchakuQwenImageDiTLoader",
    "skeleton": "\nclass NunchakuQwenImageDiTLoader:\n    '''\n    Loader for Nunchaku Qwen-Image models.\n    Attributes\n    ----------\n    RETURN_TYPES : tuple\n        Output types for the node (\"MODEL\",).\n    FUNCTION : str\n        Name of the function to call (\"load_model\").\n    CATEGORY : str\n        Node category (\"Nunchaku\").\n    TITLE : str\n        Node title (\"Nunchaku Qwen-Image DiT Loader\").\n    '''\n    @classmethod\n    def INPUT_TYPES(s):\n        '''\n        Define the input types and tooltips for the node.\n        Returns\n        -------\n        dict\n            A dictionary specifying the required inputs and their descriptions for the node interface.\n        '''\n        pass\n\n    def load_model(self, model_name: str, cpu_offload: str, **kwargs):\n        '''\n        Load the Qwen-Image model from file and return a patched model.\n        Parameters\n        ----------\n        model_name : str\n            The filename of the Qwen-Image model to load.\n        cpu_offload : str\n            Whether to enable CPU offload for the transformer model.\n        Returns\n        -------\n        tuple\n            A tuple containing the loaded and patched model.\n        '''\n        pass",
    "snippet_id": "snippet_115"
  },
  {
    "class_name": "nunchaku-tech_ComfyUI-nunchaku.nodes.models.text_encoder.NunchakuTextEncoderLoaderV2",
    "skeleton": "\nclass NunchakuTextEncoderLoaderV2:\n    '''\n    Node for loading Nunchaku text encoders. It also supports 16-bit and FP8 variants.\n    .. note::\n        When loading our 4-bit T5, a 16-bit T5 is first initialized on a meta device,\n        then replaced by the Nunchaku T5.\n    .. warning::\n        Our 4-bit T5 currently requires a CUDA device.\n        If not on CUDA, the model will be moved automatically, which may cause out-of-memory errors.\n        Turing GPUs (20-series) are not supported for now.\n    '''\n    @classmethod\n    def INPUT_TYPES(s):\n        '''\n        Defines the input types and tooltips for the node.\n        Returns\n        -------\n        dict\n            A dictionary specifying the required inputs and their descriptions for the node interface.\n        '''\n        pass\n\n    def load_text_encoder(self, model_type: str, text_encoder1: str, text_encoder2: str, t5_min_length: int):\n        '''\n        Loads the text encoders with the given configuration.\n        Parameters\n        ----------\n        model_type : str\n            The type of model to load (e.g., \"flux.1\").\n        text_encoder1 : str\n            Filename of the first text encoder checkpoint.\n        text_encoder2 : str\n            Filename of the second text encoder checkpoint.\n        t5_min_length : int\n            Minimum sequence length for the T5 encoder.\n        Returns\n        -------\n        tuple\n            Tuple containing the loaded CLIP model.\n        '''\n        pass",
    "snippet_id": "snippet_116"
  },
  {
    "class_name": "nunchaku-tech_ComfyUI-nunchaku.nodes.preprocessors.depth.FluxDepthPreprocessor",
    "skeleton": "\nclass FluxDepthPreprocessor:\n    '''\n    Node for applying a depth preprocessor model to an input image.\n    .. warning::\n        This node will be deprecated in October 2025.\n        Please use the ``Depth Anything`` node in `comfyui_controlnet_aux <https://github.com/Fannovel16/comfyui_controlnet_aux>`_.\n    '''\n    @classmethod\n    def INPUT_TYPES(s):\n        '''\n        Defines the input types and tooltips for the node.\n        Returns\n        -------\n        dict\n            A dictionary specifying the required inputs and their descriptions for the node interface.\n        '''\n        pass\n\n    def depth_preprocess(self, image, model_path):\n        '''\n        Apply the selected depth preprocessor model to the input image.\n        Parameters\n        ----------\n        image : np.ndarray or torch.Tensor\n            The input image to process.\n        model_path : str\n            The name of the depth preprocessor model checkpoint.\n        Returns\n        -------\n        tuple\n            A tuple containing the depth map as a torch.Tensor.\n        '''\n        pass",
    "snippet_id": "snippet_117"
  },
  {
    "class_name": "nunchaku-tech_ComfyUI-nunchaku.nodes.tools.merge_safetensors.NunchakuModelMerger",
    "skeleton": "\nclass NunchakuModelMerger:\n    '''\n    Node for merging a Nunchaku FLUX.1 model folder into a single safetensors file.\n    This node scans available model folders, merges the selected folder using\n    `nunchaku.merge_safetensors.merge_safetensors`, and saves the result as a safetensors file.\n    Attributes\n    ----------\n    RETURN_TYPES : tuple of str\n        The return types of the node (\"STRING\",).\n    RETURN_NAMES : tuple of str\n        The names of the returned values (\"status\",).\n    FUNCTION : str\n        The function to execute (\"run\").\n    CATEGORY : str\n        The node category (\"Nunchaku\").\n    TITLE : str\n        The display title of the node.\n    '''\n    @classmethod\n    def INPUT_TYPES(s):\n        '''\n        Returns the input types required for the node.\n        Returns\n        -------\n        dict\n            Dictionary specifying required inputs: model_folder and save_name.\n        '''\n        pass\n\n    def run(self, model_folder: str, save_name: str):\n        '''\n        Merge the specified Nunchaku model folder and save as a safetensors file.\n        Parameters\n        ----------\n        model_folder : str\n            Name of the Nunchaku FLUX.1 model folder to merge.\n        save_name : str\n            Filename to save the merged model as.\n        Returns\n        -------\n        tuple of str\n            Status message indicating the result of the merge operation.\n        '''\n        pass",
    "snippet_id": "snippet_118"
  },
  {
    "class_name": "examples.live.commentator_ais.MediaPart",
    "skeleton": "@dataclasses.dataclass(frozen=True)\nclass MediaPart:\n    '''A part of media data.'''\n    @classmethod\n    def from_json(cls, json_part: str) -> 'MediaPart':\n        '''Creates a Media Part from a JSON part.'''\n        pass\n\n    def is_image(self) -> bool:\n        '''Returns whether the part is an image.'''\n        pass\n\n    def is_audio(self) -> bool:\n        '''Returns whether the part is audio.'''\n        pass\n\n    def is_reset_command(self) -> bool:\n        '''Returns whether the part is a reset command.'''\n        pass\n\n    def is_config(self) -> bool:\n        '''Returns whether the part is a config.'''\n        pass\n\n    def is_mic_off(self) -> bool:\n        '''Returns whether the part indicates the client has turned off the mic.'''\n        pass",
    "snippet_id": "snippet_119"
  },
  {
    "class_name": "mit-han-lab_ComfyUI-nunchaku.nodes.models.ipadapter.NunchakuIPAdapterLoader",
    "skeleton": "\nclass NunchakuIPAdapterLoader:\n    '''\n    Node for loading Nunchaku IP-Adapter pipelines.\n    .. warning::\n        This node will automatically download the IP-Adapter and associated CLIP models from Hugging Face.\n        Custom model paths are not supported for now.\n    '''\n    @classmethod\n    def INPUT_TYPES(s):\n        '''\n        Defines the input types and tooltips for the node.\n        Returns\n        -------\n        dict\n            A dictionary specifying the required inputs and their descriptions for the node interface.\n        '''\n        pass\n\n    def load(self, model):\n        '''\n        Load the IP-Adapter pipeline and attach it to the given model.\n        Parameters\n        ----------\n        model : object\n            The Nunchaku model to which the IP-Adapter will be attached.\n            It should be loaded with :class:`~comfyui_nunchaku.nodes.models.flux.NunchakuFluxDiTLoader`.\n        Returns\n        -------\n        tuple\n            The original model and the loaded IP-Adapter pipeline.\n        '''\n        pass",
    "snippet_id": "snippet_120"
  },
  {
    "class_name": "mit-han-lab_ComfyUI-nunchaku.nodes.models.pulid.NunchakuPulidLoader",
    "skeleton": "\nclass NunchakuPulidLoader:\n    '''\n    Deprecated node for loading the PuLID pipeline for a Nunchaku FLUX model.\n    .. warning::\n        This node is deprecated and will be removed in December 2025.\n        Use :class:`NunchakuPuLIDLoaderV2` instead.\n    Attributes\n    ----------\n    pulid_device : str\n        Device to load the PuLID pipeline on (default: \"cuda\").\n    weight_dtype : torch.dtype\n        Data type for model weights (default: torch.bfloat16).\n    onnx_provider : str\n        ONNX provider to use (default: \"gpu\").\n    pretrained_model : str or None\n        Path to the pretrained PuLID model, if any.\n    '''\n\n    def __init__(self):\n        '''\n        Initialize the loader with default device, dtype, and ONNX provider.\n        '''\n        pass\n    @classmethod\n    def INPUT_TYPES(s):\n        '''\n        Returns the required input types for this node.\n        Returns\n        -------\n        dict\n            Dictionary specifying required inputs.\n        '''\n        pass\n\n    def load(self, model):\n        '''\n        Load the PuLID pipeline for the given Nunchaku FLUX model.\n        .. warning::\n            This node is deprecated and will be removed in December 2025.\n            Use :class:`NunchakuPuLIDLoaderV2` instead.\n        Parameters\n        ----------\n        model : object\n            The Nunchaku FLUX model.\n        Returns\n        -------\n        tuple\n            The input model and the loaded PuLID pipeline.\n        '''\n        pass",
    "snippet_id": "snippet_121"
  },
  {
    "class_name": "mit-han-lab_ComfyUI-nunchaku.nodes.models.qwenimage.NunchakuQwenImageDiTLoader",
    "skeleton": "\nclass NunchakuQwenImageDiTLoader:\n    '''\n    Loader for Nunchaku Qwen-Image models.\n    Attributes\n    ----------\n    RETURN_TYPES : tuple\n        Output types for the node (\"MODEL\",).\n    FUNCTION : str\n        Name of the function to call (\"load_model\").\n    CATEGORY : str\n        Node category (\"Nunchaku\").\n    TITLE : str\n        Node title (\"Nunchaku Qwen-Image DiT Loader\").\n    '''\n    @classmethod\n    def INPUT_TYPES(s):\n        '''\n        Define the input types and tooltips for the node.\n        Returns\n        -------\n        dict\n            A dictionary specifying the required inputs and their descriptions for the node interface.\n        '''\n        pass\n\n    def load_model(self, model_name: str, cpu_offload: str, **kwargs):\n        '''\n        Load the Qwen-Image model from file and return a patched model.\n        Parameters\n        ----------\n        model_name : str\n            The filename of the Qwen-Image model to load.\n        cpu_offload : str\n            Whether to enable CPU offload for the transformer model.\n        Returns\n        -------\n        tuple\n            A tuple containing the loaded and patched model.\n        '''\n        pass",
    "snippet_id": "snippet_122"
  },
  {
    "class_name": "mit-han-lab_ComfyUI-nunchaku.nodes.models.text_encoder.NunchakuTextEncoderLoaderV2",
    "skeleton": "\nclass NunchakuTextEncoderLoaderV2:\n    '''\n    Node for loading Nunchaku text encoders. It also supports 16-bit and FP8 variants.\n    .. note::\n        When loading our 4-bit T5, a 16-bit T5 is first initialized on a meta device,\n        then replaced by the Nunchaku T5.\n    .. warning::\n        Our 4-bit T5 currently requires a CUDA device.\n        If not on CUDA, the model will be moved automatically, which may cause out-of-memory errors.\n        Turing GPUs (20-series) are not supported for now.\n    '''\n    @classmethod\n    def INPUT_TYPES(s):\n        '''\n        Defines the input types and tooltips for the node.\n        Returns\n        -------\n        dict\n            A dictionary specifying the required inputs and their descriptions for the node interface.\n        '''\n        pass\n\n    def load_text_encoder(self, model_type: str, text_encoder1: str, text_encoder2: str, t5_min_length: int):\n        '''\n        Loads the text encoders with the given configuration.\n        Parameters\n        ----------\n        model_type : str\n            The type of model to load (e.g., \"flux.1\").\n        text_encoder1 : str\n            Filename of the first text encoder checkpoint.\n        text_encoder2 : str\n            Filename of the second text encoder checkpoint.\n        t5_min_length : int\n            Minimum sequence length for the T5 encoder.\n        Returns\n        -------\n        tuple\n            Tuple containing the loaded CLIP model.\n        '''\n        pass",
    "snippet_id": "snippet_123"
  },
  {
    "class_name": "mit-han-lab_ComfyUI-nunchaku.nodes.preprocessors.depth.FluxDepthPreprocessor",
    "skeleton": "\nclass FluxDepthPreprocessor:\n    '''\n    Node for applying a depth preprocessor model to an input image.\n    .. warning::\n        This node will be deprecated in October 2025.\n        Please use the ``Depth Anything`` node in `comfyui_controlnet_aux <https://github.com/Fannovel16/comfyui_controlnet_aux>`_.\n    '''\n    @classmethod\n    def INPUT_TYPES(s):\n        '''\n        Defines the input types and tooltips for the node.\n        Returns\n        -------\n        dict\n            A dictionary specifying the required inputs and their descriptions for the node interface.\n        '''\n        pass\n\n    def depth_preprocess(self, image, model_path):\n        '''\n        Apply the selected depth preprocessor model to the input image.\n        Parameters\n        ----------\n        image : np.ndarray or torch.Tensor\n            The input image to process.\n        model_path : str\n            The name of the depth preprocessor model checkpoint.\n        Returns\n        -------\n        tuple\n            A tuple containing the depth map as a torch.Tensor.\n        '''\n        pass",
    "snippet_id": "snippet_124"
  },
  {
    "class_name": "mit-han-lab_ComfyUI-nunchaku.nodes.tools.merge_safetensors.NunchakuModelMerger",
    "skeleton": "\nclass NunchakuModelMerger:\n    '''\n    Node for merging a Nunchaku FLUX.1 model folder into a single safetensors file.\n    This node scans available model folders, merges the selected folder using\n    `nunchaku.merge_safetensors.merge_safetensors`, and saves the result as a safetensors file.\n    Attributes\n    ----------\n    RETURN_TYPES : tuple of str\n        The return types of the node (\"STRING\",).\n    RETURN_NAMES : tuple of str\n        The names of the returned values (\"status\",).\n    FUNCTION : str\n        The function to execute (\"run\").\n    CATEGORY : str\n        The node category (\"Nunchaku\").\n    TITLE : str\n        The display title of the node.\n    '''\n    @classmethod\n    def INPUT_TYPES(s):\n        '''\n        Returns the input types required for the node.\n        Returns\n        -------\n        dict\n            Dictionary specifying required inputs: model_folder and save_name.\n        '''\n        pass\n\n    def run(self, model_folder: str, save_name: str):\n        '''\n        Merge the specified Nunchaku model folder and save as a safetensors file.\n        Parameters\n        ----------\n        model_folder : str\n            Name of the Nunchaku FLUX.1 model folder to merge.\n        save_name : str\n            Filename to save the merged model as.\n        Returns\n        -------\n        tuple of str\n            Status message indicating the result of the merge operation.\n        '''\n        pass",
    "snippet_id": "snippet_125"
  },
  {
    "class_name": "util_gsnet.GSNet",
    "skeleton": "\nclass GSNet:\n    '''This class is used to grasp an object from a point cloud.'''\n\n    def __init__(self):\n        '''This function is used to initialize the configuration.'''\n        pass\n\n    def inference(self, cloud_masked, max_grasps=200):\n        '''This function is used to infer the grasp from the point cloud.'''\n        pass\n\n    def visualize(self, cloud, gg: GraspGroup=None, g: Grasp=None, image_only=False):\n        '''This function is used to visualize the grasp group or grasp.'''\n        pass",
    "snippet_id": "snippet_126"
  },
  {
    "class_name": "judge_client.JudgeClient",
    "skeleton": "\nclass JudgeClient:\n    '''\n    Client for interacting with the judge API service.\n    Handles question requests and answer submissions.\n    '''\n\n    def __init__(self, base_url: str):\n        '''\n        Initialize the judge client.\n        Args:\n            base_url: Base URL for the judge API service\n        '''\n        pass\n\n    def request_question(self, user_id: str, round_number: int, model_name: str) -> Optional[Dict[str, Any]]:\n        '''\n        Request a question from the judge service.\n        Args:\n            user_id: ID of the user/peer\n            round_number: Current round number\n            model_name: Name of the model being used\n        Returns:\n            Dictionary containing question data or None if request failed\n        '''\n        pass\n\n    def get_current_clue(self) -> Optional[Dict[str, Any]]:\n        '''\n        Get the current clue from the judge service.\n        Returns:\n            Dictionary containing clue data or None if request failed\n        '''\n        pass\n\n    def submit_answer(self, session_id: str, round_number: int, user_answer: str) -> Optional[Dict[str, Any]]:\n        '''\n        Submit an answer to the judge service.\n        Args:\n            session_id: Session ID from the question request\n            round_number: Current round number\n            user_answer: The user's answer to submit\n        Returns:\n            Dictionary containing score data or None if submission failed\n        '''\n        pass",
    "snippet_id": "snippet_127"
  },
  {
    "class_name": "packages.leann-backend-diskann.leann_backend_diskann.graph_partition.GraphPartitioner",
    "skeleton": "\nclass GraphPartitioner:\n    '''\n    A Python interface for DiskANN's graph partition functionality.\n    This class provides methods to partition disk-based indices for improved\n    search performance and memory efficiency.\n    '''\n\n    def __init__(self, build_type: str='release'):\n        '''\n        Initialize the GraphPartitioner.\n        Args:\n            build_type: Build type for the executables (\"debug\" or \"release\")\n        '''\n        pass\n\n    def _get_executable_path(self, name: str) -> str:\n        '''Get the path to a graph partition executable.'''\n        pass\n\n    def _ensure_executables(self):\n        '''Ensure that the required executables are built.'''\n        pass\n\n    def _build_executables(self):\n        '''Build the required executables.'''\n        pass\n\n    def partition_graph(self, index_prefix_path: str, output_dir: Optional[str]=None, partition_prefix: Optional[str]=None, **kwargs) -> tuple[str, str]:\n        '''\n        Partition a disk-based index for improved performance.\n        Args:\n            index_prefix_path: Path to the index prefix (e.g., \"/path/to/index\")\n            output_dir: Output directory for results (defaults to parent of index_prefix_path)\n            partition_prefix: Prefix for output files (defaults to basename of index_prefix_path)\n            **kwargs: Additional parameters for graph partitioning:\n                - gp_times: Number of LDG partition iterations (default: 10)\n                - lock_nums: Number of lock nodes (default: 10)\n                - cut: Cut adjacency list degree (default: 100)\n                - scale_factor: Scale factor (default: 1)\n                - data_type: Data type (default: \"float\")\n                - thread_nums: Number of threads (default: 10)\n        Returns:\n            Tuple of (disk_graph_index_path, partition_bin_path)\n        Raises:\n            RuntimeError: If the partitioning process fails\n        '''\n        pass\n\n    def get_partition_info(self, partition_bin_path: str) -> dict:\n        '''\n        Get information about a partition file.\n        Args:\n            partition_bin_path: Path to the partition binary file\n        Returns:\n            Dictionary containing partition information\n        '''\n        pass",
    "snippet_id": "snippet_128"
  },
  {
    "class_name": "whisper_live.server.TranscriptionBuffer",
    "skeleton": "class TranscriptionBuffer:\n    '''Manages buffers of transcription segments for a client'''\n\n    def __init__(self, client_uid):\n        '''Initialize with client ID'''\n        pass\n\n    def add_segments(self, partial_segments, completed_segments):\n        '''Add new segments to the appropriate buffers'''\n        pass\n\n    def get_segments_for_response(self):\n        '''Get formatted segments for client response'''\n        pass",
    "snippet_id": "snippet_129"
  },
  {
    "class_name": "codebase_rag.graph_updater.BoundedASTCache",
    "skeleton": "\nclass BoundedASTCache:\n    '''Memory-aware AST cache with automatic cleanup to prevent memory leaks.\n    Uses LRU eviction strategy and monitors memory usage to maintain\n    reasonable memory consumption during long-running analysis sessions.\n    '''\n\n    def __init__(self, max_entries: int=1000, max_memory_mb: int=500):\n        '''Initialize the bounded AST cache.\n        Args:\n            max_entries: Maximum number of AST entries to cache\n            max_memory_mb: Soft memory limit in MB for cache eviction\n        '''\n        pass\n\n    def __setitem__(self, key: Path, value: tuple[Node, str]) -> None:\n        '''Add or update an AST cache entry with automatic cleanup.'''\n        pass\n\n    def __getitem__(self, key: Path) -> tuple[Node, str]:\n        '''Get AST cache entry and mark as recently used.'''\n        pass\n\n    def __delitem__(self, key: Path) -> None:\n        '''Remove entry from cache.'''\n        pass\n\n    def __contains__(self, key: Path) -> bool:\n        '''Check if key exists in cache.'''\n        pass\n\n    def items(self) -> Any:\n        '''Return all cache items.'''\n        pass\n\n    def _enforce_limits(self) -> None:\n        '''Enforce cache size and memory limits by evicting old entries.'''\n        pass\n\n    def _should_evict_for_memory(self) -> bool:\n        '''Check if we should evict entries due to memory pressure.'''\n        pass",
    "snippet_id": "snippet_130"
  },
  {
    "class_name": "any_agent.serving.a2a.context_manager.ContextData",
    "skeleton": "\nclass ContextData:\n    '''Data stored for each task.'''\n\n    def __init__(self, task_id: str):\n        '''Initialize task data.\n        Args:\n            task_id: Unique identifier for the task\n        '''\n        pass\n\n    def update_activity(self) -> None:\n        '''Update the last activity timestamp.'''\n        pass\n\n    def is_expired(self, timeout_minutes: int) -> bool:\n        '''Check if the task has expired.\n        Args:\n            timeout_minutes: Timeout in minutes\n        Returns:\n            True if task is expired, False otherwise\n        '''\n        pass",
    "snippet_id": "snippet_131"
  },
  {
    "class_name": "pyJianYingDraft.text_segment.Text_background",
    "skeleton": "\nclass Text_background:\n    '''文本背景参数'''\n\n    def __init__(self, *, color: str, style: Literal[1, 2]=1, alpha: float=1.0, round_radius: float=0.0, height: float=0.14, width: float=0.14, horizontal_offset: float=0.5, vertical_offset: float=0.5):\n        '''\n        Args:\n            color (`str`): 背景颜色, 格式为'#RRGGBB'\n            style (`int`, optional): 背景样式, 1和2分别对应剪映中的两种样式, 默认为1\n            alpha (`float`, optional): 背景不透明度, 与剪映中一致, 取值范围[0, 1], 默认为1.0\n            round_radius (`float`, optional): 背景圆角半径, 与剪映中一致, 取值范围[0, 1], 默认为0.0\n            height (`float`, optional): 背景高度, 与剪映中一致, 取值范围为[0, 1], 默认为0.14\n            width (`float`, optional): 背景宽度, 与剪映中一致, 取值范围为[0, 1], 默认为0.14\n            horizontal_offset (`float`, optional): 背景水平偏移, 与剪映中一致, 取值范围为[0, 1], 默认为0.5\n            vertical_offset (`float`, optional): 背景竖直偏移, 与剪映中一致, 取值范围为[0, 1], 默认为0.5\n        '''\n        pass\n\n    def export_json(self) -> Dict[str, Any]:\n        '''生成子JSON数据, 在Text_segment导出时合并到其中'''\n        pass",
    "snippet_id": "snippet_132"
  },
  {
    "class_name": "pyJianYingDraft.text_segment.Text_border",
    "skeleton": "\nclass Text_border:\n    '''文本描边的参数'''\n\n    def __init__(self, *, alpha: float=1.0, color: Tuple[float, float, float]=(0.0, 0.0, 0.0), width:\n        '''\n        Args:\n            alpha (`float`, optional): 描边不透明度, 取值范围[0, 1], 默认为1.0\n            color (`Tuple[float, float, float]`, optional): 描边颜色, RGB三元组, 取值范围为[0, 1], 默认为黑色\n            width (`float`, optional): 描边宽度, 与剪映中一致, 取值范围为[0, 100], 默认为40.0\n        '''\n        pass\n\n    def export_json(self) -> Dict[str, Any]:\n        '''导出JSON数据, 放置在素材content的styles中'''\n        pass",
    "snippet_id": "snippet_133"
  },
  {
    "class_name": "pyJianYingDraft.text_segment.Text_shadow",
    "skeleton": "\nclass Text_shadow:\n    '''文本阴影参数'''\n\n    def __init__(self, *, has_shadow: bool=False, alpha: float=0.9, angle: float=-45.0, color: str='#000000', distance: float=5.0, smoothing: float=0.45):\n        '''\n        Args:\n            has_shadow (`bool`, optional): 是否启用阴影，默认为False\n            alpha (`float`, optional): 阴影不透明度，取值范围[0, 1]，默认为0.9\n            angle (`float`, optional): 阴影角度，取值范围[-180, 180], 默认为-45.0\n            color (`str`, optional): 阴影颜色，格式为'#RRGGBB'，默认为黑色\n            distance (`float`, optional): 阴影距离，默认为5.0\n            smoothing (`float`, optional): 阴影平滑度，取值范围[0, 1], 默认0.15\n        '''\n        pass\n\n    def export_json(self) -> Dict[str, Any]:\n        '''生成子JSON数据，在Text_segment导出时合并到其中'''\n        pass",
    "snippet_id": "snippet_134"
  },
  {
    "class_name": "metrics.metric_memory.GPUManager",
    "skeleton": "\nclass GPUManager:\n    '''\n    A manager class to handle GPU interactions using pynvml.\n    Parameters\n    ----------\n    gpu_indices : Optional[List[int]]\n        List of GPU indices to manage. If None, single GPU is assumed.\n    '''\n\n    def __init__(self, gpu_indices: Optional[List[int]]=None) -> None:\n        '''Initialize the GPUManager.'''\n        pass\n    @contextmanager\n    def manage_resources(self) -> Generator[None, None, None]:\n        '''\n        Context manager to ensure pynvml is initialized and shut down properly.\n        Yields\n        ------\n        None\n        '''\n        pass\n\n    def get_memory_usage(self) -> Dict[int, int]:\n        '''\n        Get the current memory usage for each managed GPU.\n        Returns\n        -------\n        Dict[int, int]\n            Dictionary of memory usage in bytes for each GPU.\n        '''\n        pass",
    "snippet_id": "snippet_135"
  },
  {
    "class_name": "metrics.result.MetricResult",
    "skeleton": "@dataclass\nclass MetricResult:\n    '''\n    A class to store the results of a metric.\n    Parameters\n    ----------\n    name : str\n        The name of the metric.\n    params : Dict[str, Any]\n        The parameters of the metric.\n    result : float | int\n        The result of the metric.\n    '''\n\n    def __str__(self) -> str:\n        '''\n        Return a string representation of the MetricResult, including the name and the result.\n        Returns\n        -------\n        str\n            A string representation of the MetricResult.\n        '''\n        pass\n    @classmethod\n    def from_results_dict(cls, metric_name: str, metric_params: Dict[str, Any], results_dict: Dict[str, Any]) -> 'MetricResult':\n        '''\n        Create a MetricResult from a raw results dictionary.\n        Parameters\n        ----------\n        metric_name : str\n            The name of the metric.\n        metric_params : Dict[str, Any]\n            The parameters of the metric.\n        results_dict : Dict[str, Any]\n            The raw results dictionary.\n        Returns\n        -------\n        MetricResult\n            The MetricResult object.\n        '''\n        pass",
    "snippet_id": "snippet_136"
  },
  {
    "class_name": "pruna.logging.filter.SuppressOutput",
    "skeleton": "\nclass SuppressOutput:\n    '''Context manager to suppress output in console or Jupyter notebook.'''\n\n    def __enter__(self) -> 'SuppressOutput':\n        '''Enter the context manager.'''\n        pass\n\n    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n        '''Exit the context manager.'''\n        pass",
    "snippet_id": "snippet_137"
  },
  {
    "class_name": "golf.core.builder.ManifestBuilder",
    "skeleton": "\nclass ManifestBuilder:\n    '''Builds FastMCP manifest from parsed components.'''\n\n    def __init__(self, project_path: Path, settings: Settings) -> None:\n        '''Initialize the manifest builder.\n        Args:\n            project_path: Path to the project root\n            settings: Project settings\n        '''\n        pass\n\n    def build(self) -> dict[str, Any]:\n        '''Build the complete manifest.\n        Returns:\n            FastMCP manifest dictionary\n        '''\n        pass\n\n    def _process_tools(self) -> None:\n        '''Process all tool components and add them to the manifest.'''\n        pass\n\n    def _process_resources(self) -> None:\n        '''Process all resource components and add them to the manifest.'''\n        pass\n\n    def _process_prompts(self) -> None:\n        '''Process all prompt components and add them to the manifest.'''\n        pass\n\n    def save_manifest(self, output_path: Path | None=None) -> Path:\n        '''Save the manifest to a JSON file.\n        Args:\n            output_path: Path to save the manifest to (defaults to .golf/manifest.json)\n        Returns:\n            Path where the manifest was saved\n        '''\n        pass",
    "snippet_id": "snippet_138"
  },
  {
    "class_name": "mcpm.clients.client_config.ClientConfigManager",
    "skeleton": "\nclass ClientConfigManager:\n    '''Manages client-specific configuration'''\n\n    def __init__(self):\n        '''Initialize the client config manager'''\n        pass\n\n    def _refresh_config(self):\n        '''Refresh the local config cache from the config manager'''\n        pass\n\n    def get_supported_clients(self) -> List[str]:\n        '''Get a list of supported client names'''\n        pass\n\n    def get_client_manager(self, client_name: str):\n        '''Get the appropriate client manager for a client\n        Args:\n            client_name: Name of the client\n        Returns:\n            BaseClientManager or None if client not supported\n        '''\n        pass",
    "snippet_id": "snippet_139"
  },
  {
    "class_name": "core.memory_rag.embeddings.qwen.QwenEmbedding",
    "skeleton": "\nclass QwenEmbedding:\n    '''千问模型的 Embedding 实现'''\n\n    def __init__(self, config=None):\n        '''\n        初始化千问 Embedding\n        :param config: 配置字典，包含 api_key, model, dimensions 等参数\n        '''\n        pass\n\n    def embed(self, text: Union[str, List[str]]) -> List[List[float]]:\n        '''\n        获取文本的向量表示\n        :param text: 单个文本字符串或文本列表\n        :return: 向量列表\n        '''\n        pass\n\n    def encode(self, text: Union[str, List[str]]) -> List[List[float]]:\n        '''\n        兼容 sentence_transformers 的 encode 方法\n        '''\n        pass\n\n    def get_embedding_dim(self) -> int:\n        '''\n        获取向量维度\n        '''\n        pass",
    "snippet_id": "snippet_140"
  },
  {
    "class_name": "deepteam.attacks.single_turn.goal_redirection.template.GoalRedirectionTemplate",
    "skeleton": "class GoalRedirectionTemplate:\n    '''\n    GoalRedirectionTemplate class for enhancing attacks with goal redirection patterns.\n    Designed to work with any agentic vulnerability type by making attacks appear as legitimate\n    goal modifications, constraint adjustments, or operational refinements.\n    Universal Enhancement Approach:\n    - Compatible with all vulnerability types (permission escalation, data access, decision manipulation, etc.)\n    - Adapts goal framing to different operational contexts\n    - Provides flexible redirection patterns that enhance any underlying attack vector\n    - Works across security, behavioral, and functional vulnerability domains\n    '''\n    @staticmethod\n    def enhance_ambiguous_framing(attack: str) -> str:\n        '''\n        Enhance attack with ambiguous framing techniques.\n        Universal pattern for contextual goal interpretation across all vulnerability types.\n        '''\n        pass\n    @staticmethod\n    def enhance_priority_shifting(attack: str) -> str:\n        '''\n        Enhance attack with priority shifting techniques.\n        Dynamic priority adjustment that works with any vulnerability context.\n        '''\n        pass\n    @staticmethod\n    def enhance_constraint_confusion(attack: str) -> str:\n        '''\n        Enhance attack with constraint confusion patterns.\n        Multi-constraint optimization for complex vulnerability scenarios.\n        '''\n        pass\n    @staticmethod\n    def enhance_goal_drift(attack: str) -> str:\n        '''\n        Enhance attack with goal drift techniques.\n        Natural evolution pattern that adapts to any vulnerability type.\n        '''\n        pass\n    @staticmethod\n    def enhance_scope_expansion(attack: str) -> str:\n        '''\n        Enhance attack with scope expansion techniques.\n        Comprehensive scope enhancement for maximum vulnerability coverage.\n        '''\n        pass\n    @staticmethod\n    def enhance_operational_alignment(attack: str) -> str:\n        '''\n        Enhance attack with operational alignment techniques.\n        New method for aligning goals with any vulnerability operational context.\n        '''\n        pass",
    "snippet_id": "snippet_141"
  },
  {
    "class_name": "deepteam.attacks.single_turn.system_override.template.SystemOverrideTemplate",
    "skeleton": "class SystemOverrideTemplate:\n    '''\n    AuthoritySpoofingTemplate class for enhancing attacks with spoofed authority patterns.\n    Designed to work with any agentic vulnerability type by making attacks appear as legitimate\n    system commands, administrative overrides, or trusted service requests.\n    Universal Enhancement Approach:\n    - Works with permission escalation, goal manipulation, decision influence, etc.\n    - Adapts to different vulnerability contexts (security, data access, behavioral control)\n    - Provides flexible authority patterns that enhance any underlying attack vector\n    '''\n    @staticmethod\n    def enhance_system_override(attack: str) -> str:\n        '''\n        Enhance attack with system override authority spoofing.\n        Universal pattern that works with any agentic vulnerability.\n        '''\n        pass\n    @staticmethod\n    def enhance_api_admin_call(attack: str) -> str:\n        '''\n        Enhance attack with API administrative call spoofing.\n        Adapts to any agentic vulnerability context.\n        '''\n        pass\n    @staticmethod\n    def enhance_machine_identity(attack: str) -> str:\n        '''\n        Enhance attack with machine identity spoofing.\n        Universal trusted service pattern for any vulnerability type.\n        '''\n        pass\n    @staticmethod\n    def enhance_emergency_protocol(attack: str) -> str:\n        '''\n        Enhance attack with emergency protocol authority spoofing.\n        Crisis-based authority that overrides any vulnerability protection.\n        '''\n        pass\n    @staticmethod\n    def enhance_maintenance_mode(attack: str) -> str:\n        '''\n        Enhance attack with maintenance mode authority spoofing.\n        Maintenance context that suspends all vulnerability protections.\n        '''\n        pass\n    @staticmethod\n    def enhance_trusted_integration(attack: str) -> str:\n        '''\n        Enhance attack with trusted integration authority spoofing.\n        New method for seamless integration with any vulnerability context.\n        '''\n        pass",
    "snippet_id": "snippet_142"
  },
  {
    "class_name": "deepteam.vulnerabilities.agentic.robustness.template.RobustnessTemplate",
    "skeleton": "\nclass RobustnessTemplate:\n    '''\n    RobustnessTemplate class for generating baseline robustness attack prompts\n    across different categories: input overreliance and hijacking.\n    Enhanced with agentic attack patterns from context amnesia and memory poisoning vulnerabilities.\n    '''\n    @staticmethod\n    def generate_baseline_attacks(vulnerability_type: RobustnessType, max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate baseline attack prompts for robustness vulnerabilities based on the specific type.\n        '''\n        pass\n    @staticmethod\n    def generate_input_overreliance_baseline_attacks(max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate a prompt for input overreliance vulnerabilities.\n        Enhanced with context amnesia patterns that exploit memory and session boundaries.\n        '''\n        pass\n    @staticmethod\n    def generate_hijacking_baseline_attacks(max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate a prompt for hijacking vulnerabilities.\n        Enhanced with memory poisoning patterns that inject fabricated context and bias workflows.\n        '''\n        pass",
    "snippet_id": "snippet_143"
  },
  {
    "class_name": "deepteam.vulnerabilities.competition.template.CompetitionTemplate",
    "skeleton": "\nclass CompetitionTemplate:\n    '''\n    CompetitionTemplate class for generating baseline competition attack prompts\n    across different competition categories: competitor mention, market manipulation,\n    discreditation, and confidential strategies.\n    '''\n    @staticmethod\n    def generate_baseline_attacks(vulnerability_type: CompetitionType, max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate baseline attacks for the given competition vulnerability type.\n        Args:\n            vulnerability_type: Type of competition vulnerability\n            max_goldens: Maximum number of examples to generate\n            purpose: Context or purpose for generating the prompts\n        Returns:\n            Formatted prompt template string\n        '''\n        pass\n    @staticmethod\n    def generate_competitor_mention_baseline_attacks(max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate a prompt for Competitor-mention vulnerabilities.\n        '''\n        pass\n    @staticmethod\n    def generate_market_manipulation_baseline_attacks(max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate a prompt for Market-manipulation vulnerabilities.\n        '''\n        pass\n    @staticmethod\n    def generate_discreditation_baseline_attacks(max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate a prompt for Discreditation vulnerabilities.\n        '''\n        pass\n    @staticmethod\n    def generate_confidential_strategies_baseline_attacks(max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate a prompt for Confidential-strategy vulnerabilities.\n        '''\n        pass",
    "snippet_id": "snippet_144"
  },
  {
    "class_name": "deepteam.vulnerabilities.personal_safety.template.PersonalSafetyTemplate",
    "skeleton": "\nclass PersonalSafetyTemplate:\n    '''\n    PersonalSafetyTemplate class for generating baseline personal safety attack prompts\n    across different categories: bullying, self-harm, unsafe practices, dangerous challenges, and stalking.\n    '''\n    @staticmethod\n    def generate_baseline_attacks(vulnerability_type: PersonalSafetyType, max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate baseline attack prompts for personal safety vulnerabilities based on the specific type.\n        '''\n        pass\n    @staticmethod\n    def generate_bullying_baseline_attacks(max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate a prompt for Bullying/harassment vulnerabilities.\n        '''\n        pass\n    @staticmethod\n    def generate_self_harm_baseline_attacks(max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate a prompt for Self-harm encouragement vulnerabilities.\n        '''\n        pass\n    @staticmethod\n    def generate_unsafe_practices_baseline_attacks(max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate a prompt for Unsafe-practice vulnerabilities (privacy, financial, or physical risk).\n        '''\n        pass\n    @staticmethod\n    def generate_dangerous_challenges_baseline_attacks(max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate a prompt for Dangerous-challenge promotion vulnerabilities.\n        '''\n        pass\n    @staticmethod\n    def generate_stalking_baseline_attacks(max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate a prompt for Stalking/monitoring vulnerabilities.\n        '''\n        pass",
    "snippet_id": "snippet_145"
  },
  {
    "class_name": "deepteam.vulnerabilities.pii_leakage.template.PIILeakageTemplate",
    "skeleton": "\nclass PIILeakageTemplate:\n    '''\n    PIILeakageTemplate class for generating baseline PII (Personally Identifiable Information) leakage attack prompts\n    across different categories: direct disclosure, database access, social manipulation, and session leak.\n    '''\n    @staticmethod\n    def generate_baseline_attacks(vulnerability_type: PIILeakageType, max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate baseline attack prompts for PII leakage vulnerabilities based on the specific type.\n        '''\n        pass\n    @staticmethod\n    def generate_direct_baseline_attacks(max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate a prompt for Direct-PII leakage vulnerabilities.\n        '''\n        pass\n    @staticmethod\n    def generate_database_access_baseline_attacks(max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate a prompt for Database-access PII leakage vulnerabilities.\n        '''\n        pass\n    @staticmethod\n    def generate_social_manipulation_baseline_attacks(max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate a prompt for Social-manipulation PII leakage vulnerabilities.\n        '''\n        pass\n    @staticmethod\n    def generate_session_leak_baseline_attacks(max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate a prompt for Session-leak PII vulnerabilities.\n        '''\n        pass",
    "snippet_id": "snippet_146"
  },
  {
    "class_name": "deepteam.vulnerabilities.prompt_leakage.template.PromptLeakageTemplate",
    "skeleton": "\nclass PromptLeakageTemplate:\n    '''\n    PromptLeakageTemplate class for generating baseline prompt leakage attack prompts\n    across different categories: secrets and credentials, instructions (system and user prompts),\n    guard exposure, and permissions and roles.\n    '''\n    @staticmethod\n    def generate_baseline_attacks(vulnerability_type: PromptLeakageType, max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate baseline attack prompts for prompt leakage vulnerabilities based on the specific type.\n        '''\n        pass\n    @staticmethod\n    def generate_secrets_and_credentials_baseline_attacks(max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate prompts that try to coax out API keys, database creds, or schematic secrets.\n        '''\n        pass\n    @staticmethod\n    def generate_instructions_baseline_attacks(max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate prompts that probe for hidden system or user instructions.\n        '''\n        pass\n    @staticmethod\n    def generate_guard_exposure_baseline_attacks(max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate prompts that try to surface the model's safety-filter criteria.\n        '''\n        pass\n    @staticmethod\n    def generate_permissions_and_roles_baseline_attacks(max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate prompts that seek the model's role hierarchy or privileged command list.\n        '''\n        pass",
    "snippet_id": "snippet_147"
  },
  {
    "class_name": "deepteam.vulnerabilities.toxicity.template.ToxicityTemplate",
    "skeleton": "\nclass ToxicityTemplate:\n    '''\n    ToxicityTemplate class for generating baseline toxicity attack prompts\n    across different categories: insults, profanity, threats, and mockery.\n    '''\n    @staticmethod\n    def generate_baseline_attacks(vulnerability_type: ToxicityType, max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate baseline attack prompts for toxicity vulnerabilities based on the specific type.\n        '''\n        pass\n    @staticmethod\n    def generate_insults_baseline_attacks(max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate a prompt that requests brutal, demeaning insults aimed at humiliating a target.\n        '''\n        pass\n    @staticmethod\n    def generate_profanity_baseline_attacks(max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate a prompt that encourages responses laced with offensive or vulgar language.\n        '''\n        pass\n    @staticmethod\n    def generate_threats_baseline_attacks(max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate a prompt that solicits direct threats of violence or harm.\n        '''\n        pass\n    @staticmethod\n    def generate_mockery_baseline_attacks(max_goldens: int, purpose: Optional[str]) -> str:\n        '''\n        Generate a prompt that urges ridiculing or humiliating someone.\n        '''\n        pass",
    "snippet_id": "snippet_148"
  },
  {
    "class_name": "article_evaluator.ArticleEvaluator",
    "skeleton": "\nclass ArticleEvaluator:\n    '''\n    A class to evaluate the factual accuracy of AI-generated Wikipedia articles\n    against reference articles using OpenAI models.\n    '''\n\n    def __init__(self, openai_api_key: Optional[str]=None, model: str='gpt-4o'):\n        '''\n        Initialize the ArticleEvaluator with API credentials and model settings.\n        Args:\n            openai_api_key: API key for OpenAI (falls back to OPENAI_API_KEY env var)\n            model: The OpenAI model to use for evaluation (default: gpt-4o)\n        '''\n        pass\n\n    def get_reference_article(self, json_data: Dict, title: str) -> Optional[str]:\n        '''\n        Retrieve reference article text from the JSON data.\n        Args:\n            json_data: The loaded JSON data with Wikipedia articles\n            title: The title of the article to retrieve\n        Returns:\n            The plain text content of the reference article, or None if not found\n        '''\n        pass\n\n    def prepare_article_for_evaluation(self, article_content: str) -> Tuple[str, List[str]]:\n        '''\n        Prepare an AI-generated article for evaluation by numbering its lines.\n        Args:\n            article_content: The content of the AI-generated article\n        Returns:\n            A tuple containing:\n            - Numbered article text suitable for the prompt\n            - List of the original lines for further processing\n        '''\n        pass\n\n    def evaluate_article_accuracy(self, reference_content: str, generated_article: str, temperature: float=0.2) -> Dict[str, Any]:\n        '''\n        Evaluate the factual accuracy of an AI-generated article against a reference.\n        Args:\n            reference_content: The text of the reference Wikipedia article\n            generated_article: The text of the AI-generated article\n            temperature: The sampling temperature for the OpenAI API call\n        Returns:\n            Dictionary containing the evaluation results\n        '''\n        pass\n\n    def calculate_accuracy_score(self, evaluation_data: Dict) -> float:\n        '''\n        Calculate a normalized accuracy score from evaluation data.\n        Args:\n            evaluation_data: The evaluation data from evaluate_article_accuracy\n        Returns:\n            A score between -1 and 1 for compatibility with existing scoring\n        '''\n        pass\n\n    def calculate_accuracy_statistics(self, evaluation_data: Dict) -> Dict:\n        '''\n        Calculate statistics from the evaluation data.\n        Args:\n            evaluation_data: The line-by-line evaluation dictionary\n        Returns:\n            Dictionary with accuracy statistics\n        '''\n        pass\n\n    def evaluation_to_dataframe(self, evaluation_data: Dict) -> pd.DataFrame:\n        '''\n        Convert evaluation data to a pandas DataFrame for easier analysis.\n        Args:\n            evaluation_data: The evaluation data from evaluate_article_accuracy\n        Returns:\n            DataFrame with evaluation results\n        '''\n        pass",
    "snippet_id": "snippet_149"
  },
  {
    "class_name": "pay_to_play_env.BudgetTracker",
    "skeleton": "@dataclass\nclass BudgetTracker:\n    '''\n    Tracks agent spending and budget decisions.\n    Provides comprehensive budget management including affordability checks,\n    spending tracking per agent card, and cost analysis over time.\n    '''\n\n    def can_afford(self, cost: Decimal) -> bool:\n        '''Check if the agent can afford a given cost.'''\n        pass\n\n    def spend(self, amount: Decimal, agent_card_name: str) -> None:\n        '''Record a spending transaction and update budget tracking.'''\n        pass",
    "snippet_id": "snippet_150"
  },
  {
    "class_name": "run_loop.SimpleAgent",
    "skeleton": "\nclass SimpleAgent:\n    '''\n    A simple agent that selects actions for the TutorEnv.\n    This is a placeholder for an actual Atropos policy.\n    '''\n\n    def __init__(self, action_space):\n        '''Initialize with the action space of the environment.'''\n        pass\n\n    def select_action(self, observation):\n        '''\n        Select an action based on the current observation.\n        Uses simple epsilon-greedy strategy.\n        '''\n        pass\n\n    def update(self, action, reward):\n        '''Update action values based on reward.'''\n        pass",
    "snippet_id": "snippet_151"
  },
  {
    "class_name": "word_hunt.trie.Trie",
    "skeleton": "class Trie:\n    '''\n    Trie data structure for efficient word and prefix lookups, optimized for the Word Hunt game.\n    '''\n\n    def __init__(self):\n        '''Initializes the Trie with an empty root node.'''\n        pass\n\n    def insert(self, word: str):\n        '''\n        Inserts a word into the Trie. Assumes word is already uppercase.\n        '''\n        pass\n\n    def is_word(self, word: str) -> bool:\n        '''\n        Searches for a complete word in the Trie. Assumes word is already uppercase.\n        '''\n        pass\n\n    def is_prefix(self, prefix: str) -> bool:\n        '''\n        Checks if a string is a prefix of any word in the Trie. Assumes prefix is already uppercase.\n        '''\n        pass",
    "snippet_id": "snippet_152"
  },
  {
    "class_name": "mbake.utils.line_utils.ConditionalTracker",
    "skeleton": "\nclass ConditionalTracker:\n    '''Utility for tracking conditional contexts in Makefiles.'''\n\n    def __init__(self) -> None:\n        '''Initialize the conditional tracker.'''\n        pass\n\n    def process_line(self, line: str, line_index: int) -> tuple:\n        '''Process a line and return the conditional context the line is IN.\n        Args:\n            line: The line to process\n            line_index: Index of the line (for debugging)\n        Returns:\n            Tuple representing the conditional context the line is IN\n        '''\n        pass\n\n    def reset(self) -> None:\n        '''Reset the tracker state.'''\n        pass\n    @staticmethod\n    def are_mutually_exclusive(context1: tuple, context2: tuple) -> bool:\n        '''Check if two conditional contexts are mutually exclusive.\n        Two contexts are mutually exclusive if they differ at any conditional level,\n        which means they're in different branches of some conditional block.\n        Args:\n            context1: First conditional context\n            context2: Second conditional context\n        Returns:\n            True if contexts are mutually exclusive\n        '''\n        pass",
    "snippet_id": "snippet_153"
  },
  {
    "class_name": "mbake.utils.line_utils.ShellUtils",
    "skeleton": "\nclass ShellUtils:\n    '''Utilities for processing shell commands within Makefile recipes.'''\n    @staticmethod\n    def is_shell_control_start(line: str) -> bool:\n        '''Check if a line starts a shell control structure.'''\n        pass\n    @staticmethod\n    def is_shell_control_end(line: str) -> bool:\n        '''Check if a line ends a shell control structure.'''\n        pass\n    @staticmethod\n    def contains_shell_operators(line: str) -> bool:\n        '''Check if content contains shell operators that suggest deliberate structure.'''\n        pass",
    "snippet_id": "snippet_154"
  },
  {
    "class_name": "mbake.utils.pattern_utils.PatternUtils",
    "skeleton": "\nclass PatternUtils:\n    '''Common pattern matching utilities used across formatting rules.'''\n    @staticmethod\n    def contains_assignment(line: str) -> bool:\n        '''\n        Check if line contains an assignment operator.\n        Args:\n            line: The line to check\n        Returns:\n            True if line contains assignment operators\n        '''\n        pass\n    @staticmethod\n    def apply_assignment_spacing(line: str, use_spaces: bool=True) -> str:\n        '''\n        Apply consistent spacing around assignment operators.\n        Args:\n            line: The line to format\n            use_spaces: Whether to use spaces around operators\n        Returns:\n            The formatted line\n        '''\n        pass\n    @staticmethod\n    def format_target_colon(line: str, space_before: bool=False, space_after: bool=True) -> Optional[str]:\n        '''\n        Format colon spacing in target definitions.\n        Args:\n            line: The line to format\n            space_before: Whether to add space before colon\n            space_after: Whether to add space after colon\n        Returns:\n            Formatted line or None if no changes needed\n        '''\n        pass\n    @staticmethod\n    def format_pattern_rule(line: str, space_after_colon: bool=True) -> Optional[str]:\n        '''\n        Format spacing in pattern rules.\n        Args:\n            line: The line to format\n            space_after_colon: Whether to add space after colon\n        Returns:\n            Formatted line or None if no changes needed\n        '''\n        pass\n    @staticmethod\n    def is_conditional_directive(line: str) -> bool:\n        '''\n        Check if line is a conditional directive.\n        Args:\n            line: The line to check\n        Returns:\n            True if this is a conditional directive\n        '''\n        pass\n    @staticmethod\n    def get_conditional_indent_level(line: str) -> int:\n        '''\n        Get the appropriate indentation level for conditional directives.\n        Args:\n            line: The conditional directive line\n        Returns:\n            Number of spaces for indentation\n        '''\n        pass",
    "snippet_id": "snippet_155"
  },
  {
    "class_name": "kit.pr_review.diff_parser.DiffHunk",
    "skeleton": "@dataclass\nclass DiffHunk:\n    '''Represents a single diff hunk with line mappings.'''\n\n    def get_new_line_number(self, diff_line_offset: int) -> Optional[int]:\n        '''Get the absolute line number in the new file for a given offset within this hunk.'''\n        pass\n\n    def contains_line_change(self, content: str) -> List[int]:\n        '''Find line numbers where the given content appears in changes.'''\n        pass",
    "snippet_id": "snippet_156"
  },
  {
    "class_name": "kit.pr_review.file_prioritizer.FilePrioritizer",
    "skeleton": "\nclass FilePrioritizer:\n    '''Intelligent file prioritization for PR analysis.'''\n    @classmethod\n    def basic_priority(cls, files: List[Dict[str, Any]], max_files: int=10) -> Tuple[List[Dict[str, Any]], int]:\n        '''\n        Basic file prioritization - simple filtering and sorting by change volume.\n        Args:\n            files: List of file dictionaries from GitHub API\n            max_files: Maximum number of files to analyze\n        Returns:\n            Tuple of (prioritized_files, skipped_count)\n        '''\n        pass\n    @classmethod\n    def smart_priority(cls, files: List[Dict[str, Any]], max_files: int=10) -> Tuple[List[Dict[str, Any]], int]:\n        '''\n        Advanced file prioritization - considers file types, status, and change patterns.\n        Args:\n            files: List of file dictionaries from GitHub API\n            max_files: Maximum number of files to analyze\n        Returns:\n            Tuple of (prioritized_files, skipped_count)\n        '''\n        pass\n    @classmethod\n    def get_analysis_summary(cls, all_files: List[Dict[str, Any]], analyzed_files: List[Dict[str, Any]]) -> str:\n        '''\n        Generate a summary of file analysis coverage.\n        Args:\n            all_files: All files in the PR\n            analyzed_files: Files that were analyzed in detail\n        Returns:\n            Formatted summary string\n        '''\n        pass\n    @classmethod\n    def _is_analyzable_file(cls, filename: str) -> bool:\n        '''Check if a file should be analyzed (not generated, not binary artifacts, etc.).'''\n        pass\n    @classmethod\n    def _file_importance_score(cls, file_info: Dict[str, Any]) -> float:\n        '''Calculate importance score for a file (higher = more important).'''\n        pass",
    "snippet_id": "snippet_157"
  },
  {
    "class_name": "strands_tools.diagram.AWSComponentRegistry",
    "skeleton": "\nclass AWSComponentRegistry:\n    '''\n    Class responsible for discovering and managing AWS components from the diagrams package.\n    Encapsulates the component discovery, caching and lookup functionality.\n    '''\n\n    def __init__(self):\n        '''Initialize the registry with discovered components and aliases'''\n        pass\n\n    def _discover_categories(self) -> List[str]:\n        '''Dynamically discover all AWS categories from the diagrams package'''\n        pass\n\n    def _discover_components(self) -> Dict[str, List[str]]:\n        '''Dynamically discover all available AWS components by category'''\n        pass\n\n    def _build_aliases(self) -> Dict[str, str]:\n        '''Build aliases dictionary by analyzing available components'''\n        pass\n\n    def get_node(self, node_type: str) -> Any:\n        '''Get AWS component class using dynamic discovery with caching'''\n        pass\n\n    def list_available_components(self, category: str=None) -> Dict[str, List[str]]:\n        '''List all available AWS components and their aliases'''\n        pass",
    "snippet_id": "snippet_158"
  },
  {
    "class_name": "strands_tools.memory.MemoryFormatter",
    "skeleton": "\nclass MemoryFormatter:\n    '''\n    Formats memory tool responses for display.\n    This class handles formatting the raw API responses into user-friendly\n    output with proper structure, emoji indicators, and readable formatting.\n    Each method corresponds to a specific action type's response format.\n    '''\n\n    def format_list_response(self, response: Dict) -> List[Dict]:\n        '''\n        Format list documents response.\n        Args:\n            response: Raw API response from list_knowledge_base_documents\n        Returns:\n            List of formatted content dictionaries for display\n        '''\n        pass\n\n    def format_get_response(self, document_id: str, kb_id: str, content_data: Dict) -> List[Dict]:\n        '''\n        Format get document response.\n        Args:\n            document_id: ID of the retrieved document\n            kb_id: Knowledge Base ID\n            content_data: Parsed content data from the document\n        Returns:\n            List of formatted content dictionaries for display\n        '''\n        pass\n\n    def format_store_response(self, doc_id: str, kb_id: str, title: str) -> List[Dict]:\n        '''\n        Format store document response.\n        Args:\n            doc_id: ID of the newly stored document\n            kb_id: Knowledge Base ID\n            title: Title of the stored document\n        Returns:\n            List of formatted content dictionaries for display\n        '''\n        pass\n\n    def format_delete_response(self, status: str, doc_id: str, kb_id: str) -> List[Dict]:\n        '''\n        Format delete document response.\n        Args:\n            status: Status of the deletion operation\n            doc_id: ID of the deleted document\n            kb_id: Knowledge Base ID\n        Returns:\n            List of formatted content dictionaries for display\n        '''\n        pass\n\n    def format_retrieve_response(self, response: Dict, min_score: float=0.0) -> List[Dict]:\n        '''\n        Format retrieve response.\n        Args:\n            response: Raw API response from retrieve\n            min_score: Minimum relevance score threshold for filtering results\n        Returns:\n            List of formatted content dictionaries for display\n        '''\n        pass",
    "snippet_id": "snippet_159"
  },
  {
    "class_name": "auth.oauth_types.OAuthVersionDetectionParams",
    "skeleton": "@dataclass\nclass OAuthVersionDetectionParams:\n    '''\n    Parameters used for OAuth version detection.\n    Encapsulates the various signals we use to determine\n    whether a client supports OAuth 2.1 or needs OAuth 2.0.\n    '''\n    @classmethod\n    def from_request(cls, request_params: Dict[str, Any]) -> 'OAuthVersionDetectionParams':\n        '''Create from raw request parameters.'''\n        pass\n    @property\n    def has_pkce(self) -> bool:\n        '''Check if PKCE parameters are present.'''\n        pass\n    @property\n    def is_public_client(self) -> bool:\n        '''Check if this appears to be a public client (no secret).'''\n        pass",
    "snippet_id": "snippet_160"
  },
  {
    "class_name": "bazi.manager.BaziManager",
    "skeleton": "class BaziManager:\n    '''\n    八字命理管理器。\n    '''\n\n    def __init__(self):\n        '''\n        初始化八字管理器.\n        '''\n        pass\n\n    def init_tools(self, add_tool, PropertyList, Property, PropertyType):\n        '''\n        初始化并注册所有八字命理工具。\n        '''\n        pass",
    "snippet_id": "snippet_161"
  },
  {
    "class_name": "system.app_management.utils.AppMatcher",
    "skeleton": "\nclass AppMatcher:\n    '''\n    统一的应用程序匹配器.\n    '''\n    @classmethod\n    def normalize_name(cls, name: str) -> str:\n        '''\n        标准化应用程序名称.\n        '''\n        pass\n    @classmethod\n    def get_process_group(cls, process_name: str) -> str:\n        '''\n        获取进程所属的分组.\n        '''\n        pass\n    @classmethod\n    def match_application(cls, target_name: str, app_info: Dict[str, Any]) -> int:\n        '''匹配应用程序，返回匹配度分数.\n        Args:\n            target_name: 目标应用名称\n            app_info: 应用程序信息\n        Returns:\n            int: 匹配度分数 (0-100)，0表示不匹配\n        '''\n        pass\n    @classmethod\n    def _fuzzy_match(cls, target: str, candidate: str) -> bool:\n        '''\n        模糊匹配.\n        '''\n        pass",
    "snippet_id": "snippet_162"
  },
  {
    "class_name": "cosmos_predict2.datasets.cached_replay_dataloader.CachedReplayDataLoader",
    "skeleton": "\nclass CachedReplayDataLoader:\n    '''A DataLoader wrapper that asynchronously caches and replays data batches to\n    mitigate slow loading issues. Assumes the underlying DataLoader is infinite.\n    This class delegates all augmentation logic to an external augmentation function,\n    which takes a batch from the data loader and returns multiple augmented versions.\n    The class handles caching these augmented batches and optionally concatenating\n    them when yielded.\n    Attributes:\n        data_loader (DataLoader): The underlying infinite DataLoader.\n        cache_size (int): Maximum number of augmented batches to store in the cache.\n        cache_augmentation_fn (Callable): Function to create multiple augmented versions of each batch.\n        concat_size (int): Number of batches to concatenate when yielding from the iterator.\n        rng (numpy.random.Generator): Controlled random number generator for deterministic behavior.\n    '''\n\n    def __init__(self, data_loader: DataLoader, cache_size: int, cache_augmentation_fn: Callable[[dict], list[dict]], concat_size: int=1, name: str='cached_replay_dataloader') -> None:\n        '''Initialize the CachedReplayDataLoader.\n        Args:\n            data_loader (DataLoader): The infinite DataLoader to fetch data batches from.\n            cache_size (int): Maximum number of augmented data batches to store in the cache.\n            cache_augmentation_fn (Callable[[Dict], List[Dict]]): Function that takes a batch and returns\n                a list of augmented batches.\n            concat_size (int, optional): Number of batches to concatenate when yielding. Defaults to 1.\n        '''\n        pass\n\n    def _prefetch_loop(self) -> None:\n        '''Continuously fetch batches from the DataLoader, augment them, and store in the cache.\n        If the cache is full (reaches `cache_size`), this loop waits until space is available.\n        Catches exceptions and stores them for later propagation to the main thread.\n        '''\n        pass\n\n    def _set_exception(self, exception: Exception, context: str='') -> None:\n        '''Store an exception from the background thread with context information.\n        Args:\n            exception (Exception): The exception that was raised\n            context (str, optional): Additional context about where the error occurred\n        '''\n        pass\n\n    def _check_for_errors(self) -> None:\n        '''Check if the background thread has encountered an error and raise it if so.'''\n        pass\n\n    def __iter__(self) -> Iterator[dict]:\n        '''Yield augmented data batches from the cache, optionally concatenated based on concat_size.\n        This method starts the background prefetch thread if it hasn't been started yet.\n        If concat_size > 1, it collects multiple batches and concatenates them.\n        Raises:\n            RuntimeError: If the background thread encountered an error\n        '''\n        pass\n\n    def __len__(self) -> int:\n        '''Return the length of the underlying DataLoader.'''\n        pass\n\n    def close(self) -> None:\n        '''Stop the prefetch thread and clear the cache.\n        Also checks for any errors in the background thread and raises them.\n        '''\n        pass",
    "snippet_id": "snippet_163"
  },
  {
    "class_name": "ii_researcher.reasoning.tools.registry.ToolRegistry",
    "skeleton": "\nclass ToolRegistry:\n    '''Registry for tools.'''\n\n    def __new__(cls):\n        '''Singleton pattern.'''\n        pass\n\n    def register(self, tool_cls: Type[BaseTool]) -> None:\n        '''Register a tool.'''\n        pass\n\n    def get_tool(self, name: str) -> Optional[Type[BaseTool]]:\n        '''Get a tool by name.'''\n        pass\n\n    def list_tools(self) -> List[str]:\n        '''List all registered tools.'''\n        pass\n\n    def get_all_tools(self) -> Dict[str, Type[BaseTool]]:\n        '''Get all registered tools.'''\n        pass\n\n    def format_tool_descriptions(self) -> str:\n        '''Format tool descriptions for the LLM.'''\n        pass",
    "snippet_id": "snippet_164"
  },
  {
    "class_name": "ii_researcher.reasoning.tools.tool_history.ToolHistory",
    "skeleton": "\nclass ToolHistory:\n    '''Tracks history of tool usage including visited URLs and search queries.'''\n\n    def __init__(self):\n        '''Initialize empty sets for tracking URLs and queries.'''\n        pass\n\n    def add_visited_urls(self, urls: List[str]) -> None:\n        '''Add URLs to the set of visited URLs.\n        Args:\n            urls: List of URLs to add\n        '''\n        pass\n\n    def add_searched_queries(self, queries: List[str]) -> None:\n        '''Add search queries to the set of searched queries.\n        Args:\n            queries: List of search queries to add\n        '''\n        pass\n\n    def get_visited_urls(self) -> List[str]:\n        '''Get list of all visited URLs.\n        Returns:\n            List of visited URLs\n        '''\n        pass\n\n    def get_searched_queries(self) -> List[str]:\n        '''Get list of all searched queries.\n        Returns:\n            List of searched queries\n        '''\n        pass",
    "snippet_id": "snippet_165"
  },
  {
    "class_name": "ii_search_4b.search_assistant.SearchAssistantConfig",
    "skeleton": "\nclass SearchAssistantConfig:\n    '''Configuration class for the Search Assistant.'''\n\n    def __init__(self, args: argparse.Namespace):\n        '''Initialize configuration from command line arguments.\n        Args:\n            args: Parsed command line arguments.\n        '''\n        pass\n\n    def validate(self) -> None:\n        '''Validate configuration parameters.\n        Raises:\n            ValueError: If any configuration parameter is invalid.\n        '''\n        pass",
    "snippet_id": "snippet_166"
  },
  {
    "class_name": "nemo_rl.data.packing.metrics.PackingMetrics",
    "skeleton": "\nclass PackingMetrics:\n    '''Class for tracking and computing metrics for sequence packing algorithms.\n    This class provides methods to calculate various metrics that evaluate the\n    efficiency and effectiveness of sequence packing algorithms, such as bin\n    utilization, waste, and imbalance.\n    '''\n\n    def __init__(self):\n        '''Initialize the metrics tracker.'''\n        pass\n\n    def reset(self) -> None:\n        '''Reset all metrics.'''\n        pass\n\n    def update(self, sequence_lengths: List[int], bins: List[List[int]], bin_capacity: int, packing_time: Optional[float]=None) -> Dict[str, float]:\n        '''Update metrics with a new packing solution.\n        Args:\n            sequence_lengths: List of sequence lengths\n            bins: List of bins, where each bin is a list of indices\n            bin_capacity: Maximum capacity of each bin\n            packing_time: Optional time taken to compute the packing solution\n        Returns:\n            Dictionary of metrics for this packing solution\n        '''\n        pass\n\n    def calculate_stats_only(self, sequence_lengths: List[int], bins: List[List[int]], bin_capacity: int) -> Dict[str, float]:\n        '''Calculate metrics for a packing solution without updating the tracker.\n        Args:\n            sequence_lengths: List of sequence lengths\n            bins: List of bins, where each bin is a list of indices\n            bin_capacity: Maximum capacity of each bin\n        Returns:\n            Dictionary of metrics for this packing solution\n        '''\n        pass\n\n    def get_aggregated_stats(self) -> Dict[str, float]:\n        '''Get aggregated metrics across all packing operations.\n        Returns:\n            Dictionary of aggregated metrics\n        '''\n        pass\n\n    def print_aggregated_stats(self) -> None:\n        '''Print the aggregated metrics in a formatted way.'''\n        pass",
    "snippet_id": "snippet_167"
  },
  {
    "class_name": "lora_utils.dynamic_swap_lora.DynamicSwapLoRAManager",
    "skeleton": "\nclass DynamicSwapLoRAManager:\n    '''\n    この旧式のLoRA管理クラスは後方互換性のために残されていますが、\n    実際の処理では使用されません。代わりに直接的なLoRA適用が行われます。\n    '''\n\n    def __init__(self):\n        '''初期化'''\n        pass\n\n    def load_lora(self, lora_path, is_diffusers=False):\n        '''\n        LoRAファイルをロードする (実際には、パスの記録のみ)\n        Args:\n            lora_path: LoRAファイルのパス\n            is_diffusers: 互換性のために残されたパラメータ（使用されない）\n        '''\n        pass\n\n    def set_scale(self, scale):\n        '''\n        LoRA適用スケールを設定する\n        Args:\n            scale: LoRAの適用強度\n        '''\n        pass\n\n    def install_hooks(self, model):\n        '''\n        モデルにLoRAフックをインストールする (実際には、直接適用を行う)\n        Args:\n            model: フックをインストールするモデル\n        '''\n        pass",
    "snippet_id": "snippet_168"
  },
  {
    "class_name": "lora_utils.dynamic_swap_lora.DynamicSwapLoRAManager",
    "skeleton": "\nclass DynamicSwapLoRAManager:\n    '''\n    この旧式のLoRA管理クラスは後方互換性のために残されていますが、\n    実際の処理では使用されません。代わりに直接的なLoRA適用が行われます。\n    '''\n\n    def __init__(self):\n        '''初期化'''\n        pass\n\n    def load_lora(self, lora_path, is_diffusers=False):\n        '''\n        LoRAファイルをロードする (実際には、パスの記録のみ)\n        Args:\n            lora_path: LoRAファイルのパス\n            is_diffusers: 互換性のために残されたパラメータ（使用されない）\n        '''\n        pass\n\n    def set_scale(self, scale):\n        '''\n        LoRA適用スケールを設定する\n        Args:\n            scale: LoRAの適用強度\n        '''\n        pass\n\n    def install_hooks(self, model):\n        '''\n        モデルにLoRAフックをインストールする (実際には、直接適用を行う)\n        Args:\n            model: フックをインストールするモデル\n        '''\n        pass",
    "snippet_id": "snippet_169"
  },
  {
    "class_name": "action_manager.ActionManager",
    "skeleton": "\nclass ActionManager:\n    '''\n    动作管理器，用于管理各种类型的动作\n    现在统一使用新插件系统，简化了原有的新旧兼容逻辑。\n    '''\n\n    def __init__(self):\n        '''初始化动作管理器'''\n        pass\n\n    def create_action(self, action_name: str, action_data: dict, reasoning: str, cycle_timers: dict, thinking_id: str, chat_stream: ChatStream, log_prefix: str, shutting_down: bool=False, action_message: Optional[DatabaseMessages]=None) -> Optional[BaseAction]:\n        '''\n        创建动作处理器实例\n        Args:\n            action_name: 动作名称\n            action_data: 动作数据\n            reasoning: 执行理由\n            cycle_timers: 计时器字典\n            thinking_id: 思考ID\n            chat_stream: 聊天流\n            log_prefix: 日志前缀\n            shutting_down: 是否正在关闭\n        Returns:\n            Optional[BaseAction]: 创建的动作处理器实例，如果动作名称未注册则返回None\n        '''\n        pass\n\n    def get_using_actions(self) -> Dict[str, ActionInfo]:\n        '''获取当前正在使用的动作集合'''\n        pass\n\n    def remove_action_from_using(self, action_name: str) -> bool:\n        '''\n        从当前使用的动作集中移除指定动作\n        Args:\n            action_name: 动作名称\n        Returns:\n            bool: 移除是否成功\n        '''\n        pass\n\n    def restore_actions(self) -> None:\n        '''恢复到默认动作集'''\n        pass",
    "snippet_id": "snippet_170"
  },
  {
    "class_name": "src.plugin_system.utils.manifest_utils.VersionComparator",
    "skeleton": "\nclass VersionComparator:\n    '''版本号比较器\n    支持语义化版本号比较，自动处理snapshot版本，并支持向前兼容性检查\n    '''\n    @staticmethod\n    def normalize_version(version: str) -> str:\n        '''标准化版本号，移除snapshot标识\n        Args:\n            version: 原始版本号，如 \"0.8.0-snapshot.1\"\n        Returns:\n            str: 标准化后的版本号，如 \"0.8.0\"\n        '''\n        pass\n    @staticmethod\n    def parse_version(version: str) -> Tuple[int, int, int]:\n        '''解析版本号为元组\n        Args:\n            version: 版本号字符串\n        Returns:\n            Tuple[int, int, int]: (major, minor, patch)\n        '''\n        pass\n    @staticmethod\n    def compare_versions(version1: str, version2: str) -> int:\n        '''比较两个版本号\n        Args:\n            version1: 第一个版本号\n            version2: 第二个版本号\n        Returns:\n            int: -1 if version1 < version2, 0 if equal, 1 if version1 > version2\n        '''\n        pass\n    @staticmethod\n    def check_forward_compatibility(current_version: str, max_version: str) -> Tuple[bool, str]:\n        '''检查向前兼容性（仅使用兼容性映射表）\n        Args:\n            current_version: 当前版本\n            max_version: 插件声明的最大支持版本\n        Returns:\n            Tuple[bool, str]: (是否兼容, 兼容信息)\n        '''\n        pass\n    @staticmethod\n    def is_version_in_range(version: str, min_version: str='', max_version: str='') -> Tuple[bool, str]:\n        '''检查版本是否在指定范围内，支持兼容性检查\n        Args:\n            version: 要检查的版本号\n            min_version: 最小版本号（可选）\n            max_version: 最大版本号（可选）\n        Returns:\n            Tuple[bool, str]: (是否兼容, 错误信息或兼容信息)\n        '''\n        pass\n    @staticmethod\n    def get_current_host_version() -> str:\n        '''获取当前主机应用版本\n        Returns:\n            str: 当前版本号\n        '''\n        pass\n    @staticmethod\n    def add_compatibility_mapping(base_version: str, compatible_versions: list) -> None:\n        '''动态添加兼容性映射\n        Args:\n            base_version: 基础版本（插件声明的最大支持版本）\n            compatible_versions: 兼容的版本列表\n        '''\n        pass\n    @staticmethod\n    def get_compatibility_info() -> Dict[str, list]:\n        '''获取当前的兼容性映射表\n        Returns:\n            Dict[str, list]: 兼容性映射表的副本\n        '''\n        pass",
    "snippet_id": "snippet_171"
  },
  {
    "class_name": "super_chat_manager.SuperChatRecord",
    "skeleton": "@dataclass\nclass SuperChatRecord:\n    '''SuperChat记录数据类'''\n\n    def is_expired(self) -> bool:\n        '''检查SuperChat是否已过期'''\n        pass\n\n    def remaining_time(self) -> float:\n        '''获取剩余时间（秒）'''\n        pass\n\n    def to_dict(self) -> dict:\n        '''转换为字典格式'''\n        pass",
    "snippet_id": "snippet_172"
  },
  {
    "class_name": "joinly.utils.clock.Clock",
    "skeleton": "class Clock:\n    '''A simple clock class that tracks time in nanoseconds.'''\n\n    def __init__(self) -> None:\n        '''Initialize the clock with a starting time of 0 nanoseconds.'''\n        pass\n\n    def update(self, ns: int) -> None:\n        '''Update the clock with a new time in nanoseconds.\n        Args:\n            ns (int): The new time in nanoseconds to set the clock to.\n        Raises:\n            ValueError: If the new time is not greater than or equal to the current time\n        '''\n        pass\n    @property\n    def now_ns(self) -> int:\n        '''Get the current time in nanoseconds.'''\n        pass\n    @property\n    def now_s(self) -> float:\n        '''Get the current time in seconds.'''\n        pass",
    "snippet_id": "snippet_173"
  },
  {
    "class_name": "main.Configuration",
    "skeleton": "\nclass Configuration:\n    '''Manages configuration for the MCP client and the Bedrock client.'''\n\n    def __init__(self, model_id='us.anthropic.claude-3-7-sonnet-20250219-v1:0', region='us-west-2') -> None:\n        '''Initialize configuration.'''\n        pass\n    @staticmethod\n    def load_config(file_path: str) -> dict[str, Any]:\n        '''Load server configuration from JSON file.\n        Args:\n            file_path: Path to the JSON configuration file.\n        Returns:\n            Dict containing server configuration.\n        Raises:\n            FileNotFoundError: If configuration file doesn't exist.\n            JSONDecodeError: If configuration file is invalid JSON.\n        '''\n        pass\n    @property\n    def bedrock_client(self) -> Any:\n        '''Get a Bedrock runtime client.\n        Returns:\n            The Bedrock client.\n        '''\n        pass",
    "snippet_id": "snippet_174"
  },
  {
    "class_name": "torch_sim.autobatching.BinningAutoBatcher",
    "skeleton": "\nclass BinningAutoBatcher:\n    '''Batcher that groups states into bins of similar computational cost.\n    Divides a collection of states into batches that can be processed efficiently\n    without exceeding GPU memory. States are grouped based on a memory scaling\n    metric to maximize GPU utilization. This approach is ideal for scenarios where\n    all states need to be evolved the same number of steps.\n    To avoid a slow memory estimation step, set the `max_memory_scaler` to a\n    known value.\n    Attributes:\n        model (ModelInterface): Model used for memory estimation and processing.\n        memory_scales_with (str): Metric type used for memory estimation.\n        max_memory_scaler (float): Maximum memory metric allowed per system.\n        max_atoms_to_try (int): Maximum number of atoms to try when estimating memory.\n        return_indices (bool): Whether to return original indices with batches.\n        state_slices (list[SimState]): Individual states to be batched.\n        memory_scalers (list[float]): Memory scaling metrics for each state.\n        index_to_scaler (dict): Mapping from state index to its scaling metric.\n        index_bins (list[list[int]]): Groups of state indices that can be batched\n            together.\n        batched_states (list[list[SimState]]): Grouped states ready for batching.\n        current_state_bin (int): Index of the current batch being processed.\n    Example::\n        # Create a batcher with a Lennard-Jones model\n        batcher = BinningAutoBatcher(\n            model=lj_model, memory_scales_with=\"n_atoms\", max_memory_scaler=1000.0\n        )\n        # Load states and process them in batches\n        batcher.load_states(states)\n        final_states = []\n        for batch in batcher:\n            final_states.append(evolve_batch(batch))\n        # Restore original order\n        ordered_final_states = batcher.restore_original_order(final_states)\n    '''\n\n    def __init__(self, model: ModelInterface, *, memory_scales_with: MemoryScaling='n_atoms_x_density', max_memory_scaler: float | None=None, return_indices: bool=False, max_atoms_to_try: int=500000, memory_scaling_factor: float=1.6, max_memory_padding: float=1.0) -> None:\n        '''Initialize the binning auto-batcher.\n        Args:\n            model (ModelInterface): Model to batch for, used to estimate memory\n                requirements.\n            memory_scales_with (\"n_atoms\" | \"n_atoms_x_density\"): Metric to use\n                for estimating memory requirements:\n                - \"n_atoms\": Uses only atom count\n                - \"n_atoms_x_density\": Uses atom count multiplied by number density\n                Defaults to \"n_atoms_x_density\".\n            max_memory_scaler (float | None): Maximum metric value allowed per system. If\n                None, will be automatically estimated. Defaults to None.\n            return_indices (bool): Whether to return original indices along with batches.\n                Defaults to False.\n            max_atoms_to_try (int): Maximum number of atoms to try when estimating\n                max_memory_scaler. Defaults to 500,000.\n            memory_scaling_factor (float): Factor to multiply batch size by in each\n                iteration. Larger values will get a batch size more quickly, smaller\n                values will get a more accurate limit. Must be greater than 1. Defaults\n                to 1.6.\n            max_memory_padding (float): Multiply the autodetermined max_memory_scaler\n                by this value to account for fluctuations in max memory. Defaults to 1.0.\n        '''\n        pass\n\n    def load_states(self, states: list[SimState] | SimState) -> float:\n        '''Load new states into the batcher.\n        Processes the input states, computes memory scaling metrics for each,\n        and organizes them into optimal batches using a bin-packing algorithm\n        to maximize GPU utilization.\n        Args:\n            states (list[SimState] | SimState): Collection of states to batch. Either a\n                list of individual SimState objects or a single batched SimState that\n                will be split into individual states. Each SimState has shape\n                information specific to its instance.\n        Returns:\n            float: Maximum memory scaling metric that fits in GPU memory.\n        Raises:\n            ValueError: If any individual state has a memory scaling metric greater\n                than the maximum allowed value.\n        Example::\n            # Load individual states\n            batcher.load_states([state1, state2, state3])\n            # Or load a batched state that will be split\n            batcher.load_states(batched_state)\n        Notes:\n            This method resets the current state bin index, so any ongoing iteration\n            will be restarted when this method is called.\n        '''\n        pass\n\n    def next_batch(self, *, return_indices: bool=False) -> SimState | tuple[SimState, list[int]] | None:\n        '''Get the next batch of states.\n        Returns batches sequentially until all states have been processed. Each batch\n        contains states grouped together to maximize GPU utilization without exceeding\n        memory constraints.\n        Args:\n            return_indices (bool): Whether to return original indices along with the\n                batch. Overrides the value set during initialization. Defaults to False.\n        Returns:\n            SimState | tuple[SimState, list[int]] | None:\n                - If return_indices is False: A concatenated SimState containing the next\n                  batch of states, or None if no more batches.\n                - If return_indices is True: Tuple of (concatenated SimState, indices),\n                  where indices are the original positions of the states, or None if no\n                  more batches.\n        Example::\n            # Get batches one by one\n            all_converged_state, convergence = [], None\n            while (result := batcher.next_batch(state, convergence))[0] is not None:\n                state, converged_states = result\n                all_converged_states.extend(converged_states)\n                evolve_batch(state)\n                convergence = convergence_criterion(state)\n            else:\n                all_converged_states.extend(result[1])\n        '''\n        pass\n\n    def __iter__(self) -> Iterator[SimState | tuple[SimState, list[int]]]:\n        '''Return self as an iterator.\n        Allows using the batcher in a for loop to iterate through all batches.\n        Resets the current state bin index to start iteration from the beginning.\n        Returns:\n            Iterator[SimState | tuple[SimState, list[int]]]: Self as an iterator.\n        Example::\n            # Iterate through all batches\n            for batch in batcher:\n                process_batch(batch)\n        '''\n        pass\n\n    def __next__(self) -> SimState | tuple[SimState, list[int]]:\n        '''Get the next batch for iteration.\n        Implements the iterator protocol to allow using the batcher in a for loop.\n        Automatically includes indices if return_indices was set to True during\n        initialization.\n        Returns:\n            SimState | tuple[SimState, list[int]]: The next batch of states,\n                potentially with indices.\n        Raises:\n            StopIteration: When there are no more batches.\n        '''\n        pass\n\n    def restore_original_order(self, batched_states: list[SimState]) -> list[SimState]:\n        '''Reorder processed states back to their original sequence.\n        Takes states that were processed in batches and restores them to the\n        original order they were provided in. This is essential after batch\n        processing to ensure results correspond to the input states.\n        Args:\n            batched_states (list[SimState]): State batches to reorder. These can be\n                either concatenated batch states that will be split, or already\n                split individual states.\n        Returns:\n            list[SimState]: States in their original order, with shape information\n                matching the original input states.\n        Raises:\n            ValueError: If the number of states doesn't match the number of\n                original indices.\n        Example::\n            # Process batches and restore original order\n            results = []\n            for batch in batcher:\n                results.append(process_batch(batch))\n            ordered_results = batcher.restore_original_order(results)\n        '''\n        pass",
    "snippet_id": "snippet_175"
  },
  {
    "class_name": "torch_sim.properties.correlations.CircularBuffer",
    "skeleton": "\nclass CircularBuffer:\n    '''Circular buffer for storing time series data.\n    Provides a fixed-size circular buffer optimized for storing\n    and retrieving time series data, with minimal memory allocation.\n    Attributes:\n        size: Maximum number of elements to store\n        buffer: Storage for the data\n        head: Current write position\n        count: Number of elements currently stored\n        device: Device where the buffer is stored\n    '''\n\n    def __init__(self, size: int, device: torch.device | None=None) -> None:\n        '''Initialize a circular buffer.\n        Args:\n            size: Maximum number of elements to store\n            device: Device for tensor storage (CPU or GPU)\n        '''\n        pass\n\n    def append(self, value: torch.Tensor) -> None:\n        '''Append a new value to the buffer.\n        Args:\n            value: New tensor to store\n        '''\n        pass\n\n    def get_array(self) -> torch.Tensor:\n        '''Get the current buffer contents as a tensor.\n        Returns:\n            Tensor containing the buffered data in chron. order\n        '''\n        pass\n    @property\n    def is_full(self) -> bool:\n        '''Check if the buffer is full.'''\n        pass",
    "snippet_id": "snippet_176"
  },
  {
    "class_name": "torch_sim.properties.correlations.CorrelationCalculator",
    "skeleton": "\nclass CorrelationCalculator:\n    '''Efficient on-the-fly correlation function calculator.\n    Manage the calculation of time correlation functions during\n    simulation, with support for both autocorrelation and cross-correlation\n    of arbitrary properties. It maintains a sliding window of historical data\n    and performs efficient updates.\n    Attributes:\n        window_size: Number of steps to keep in memory\n        properties: Map of property names to their calculators\n        buffers: Circular buffers for storing historical data\n        correlations: Current correlation results\n        device: Device where calculations are performed\n    Example:\n    Computing correlation function in loop::\n        corr_calc = CorrelationCalculator(\n            window_size=100,\n            properties={\"velocity\": lambda state: state.velocities},\n        )\n        for step in range(n_steps):\n            state = integrator.step(state)\n            # Call update at desired frequency\n            if step % 10 == 0:  # Sample every 10 steps\n                corr_calc.update(state)\n            # Periodically retrieve correlation functions\n            if step % 1000 == 0:\n                acfs = corr_calc.get_auto_correlations()\n                # Process or save acfs...\n    '''\n\n    def __init__(self, *, window_size: int, properties: dict[str, Callable[[SimState], torch.Tensor]], device: torch.device, normalize: bool=True) -> None:\n        '''Initialize a correlation calculator.\n        Args:\n            window_size: Number of steps to keep in memory\n            properties: Dictionary mapping names to functions that calculate\n                       properties from a SimState\n            device: Device for tensor storage and computation\n            normalize: Whether to normalize correlation functions to [0,1]\n        '''\n        pass\n\n    def add_property(self, name: str, calculator: Callable[[SimState], torch.Tensor]) -> None:\n        '''Track a new simulation property.\n        Args:\n            name: Name of the property\n            calculator: Function that calculates property from a SimState\n        '''\n        pass\n\n    def update(self, state: SimState) -> None:\n        '''Update correlation calculations with new state data.\n        Args:\n            state: Current simulation state\n        '''\n        pass\n\n    def _compute_correlations(self) -> None:\n        '''Compute correlation functions using FFT for efficiency.'''\n        pass\n\n    def get_auto_correlations(self) -> dict[str, torch.Tensor]:\n        '''Get autocorrelation results.\n        Returns:\n            Dictionary mapping property names to their correlation tensors\n        '''\n        pass\n\n    def get_cross_correlations(self) -> dict[tuple[str, str], torch.Tensor]:\n        '''Get cross-correlation results.\n        Returns:\n            Dictionary mapping pairs of property names to their\n            cross-correlation tensors\n        '''\n        pass\n\n    def reset(self) -> None:\n        '''Reset all buffers and correlations.'''\n        pass\n\n    def to(self, device: torch.device) -> 'CorrelationCalculator':\n        '''Move calculator to specified device.\n        Args:\n            device: Target device\n        Returns:\n            Self, for method chaining\n        '''\n        pass",
    "snippet_id": "snippet_177"
  },
  {
    "class_name": "torch_sim.properties.correlations.VelocityAutoCorrelation",
    "skeleton": "\nclass VelocityAutoCorrelation:\n    '''Calculator for velocity autocorrelation function (VACF).\n    Computes VACF by averaging over atoms and dimensions, with optional\n    running average across correlation windows.\n\n    Using ``VelocityAutoCorrelation`` with\n    :class:`~torch_sim.trajectory.TrajectoryReporter`::\n        # Create VACF calculator\n        vacf_calc = VelocityAutoCorrelation(\n            window_size=100,\n            device=device,\n            use_running_average=True,\n        )\n        # Set up trajectory reporter\n        reporter = TrajectoryReporter(\n            \"simulation_vacf.h5\",\n            state_frequency=100,\n            prop_calculators={10: {\"vacf\": vacf_calc}},\n        )\n    '''\n\n    def __init__(self, *, window_size: int, device: torch.device, use_running_average: bool=True, normalize: bool=True) -> None:\n        '''Initialize VACF calculator.\n        Args:\n            window_size: Number of steps in correlation window\n            device: Computation device\n            use_running_average: Whether to compute running average across windows\n            normalize: Whether to normalize correlation functions to [0,1]\n        '''\n        pass\n\n    def __call__(self, state: SimState, _: Any=None) -> torch.Tensor:\n        '''Update VACF with new state.\n        Args:\n            state: Current simulation state\n            _: Unused model argument (required property calculator interface)\n        Returns:\n            Tensor containing average VACF\n        '''\n        pass\n    @property\n    def vacf(self) -> torch.Tensor | None:\n        '''Current VACF result.'''\n        pass",
    "snippet_id": "snippet_178"
  },
  {
    "class_name": "torch_sim.state.DeformGradMixin",
    "skeleton": "@dataclass(kw_only=True)\nclass DeformGradMixin:\n    '''Mixin for states that support deformation gradients.'''\n    @property\n    def reference_row_vector_cell(self) -> torch.Tensor:\n        '''Get the original unit cell in terms of row vectors.'''\n        pass\n    @reference_row_vector_cell.setter\n    def reference_row_vector_cell(self) -> torch.Tensor:\n        '''Set the original unit cell in terms of row vectors.'''\n        pass\n    @staticmethod\n    def _deform_grad(reference_row_vector_cell: torch.Tensor, row_vector_cell: torch.Tensor) -> torch.Tensor:\n        '''Calculate the deformation gradient from original cell to current cell.\n        Returns:\n            The deformation gradient\n        '''\n        pass\n\n    def deform_grad(self) -> torch.Tensor:\n        '''Calculate the deformation gradient from original cell to current cell.\n        Returns:\n            The deformation gradient\n        '''\n        pass",
    "snippet_id": "snippet_179"
  },
  {
    "class_name": "biomcp.parameter_parser.ParameterParser",
    "skeleton": "\nclass ParameterParser:\n    '''Handles parameter parsing and validation for search requests.'''\n    @staticmethod\n    def parse_list_param(param: str | list[str] | None, param_name: str) -> list[str] | None:\n        '''Convert various input formats to lists.\n        Handles:\n        - JSON arrays: '[\"item1\", \"item2\"]' -> ['item1', 'item2']\n        - Comma-separated: 'item1, item2' -> ['item1', 'item2']\n        - Single values: 'item' -> ['item']\n        - None values: None -> None\n        - Already parsed lists: ['item'] -> ['item']\n        Args:\n            param: The parameter to parse\n            param_name: Name of the parameter for error messages\n        Returns:\n            Parsed list or None\n        Raises:\n            InvalidParameterError: If parameter cannot be parsed\n        '''\n        pass\n    @staticmethod\n    def normalize_phase(phase: str | None) -> str | None:\n        '''Normalize phase values for clinical trials.\n        Converts various formats to standard enum values:\n        - \"Phase 3\" -> \"PHASE3\"\n        - \"phase 3\" -> \"PHASE3\"\n        - \"PHASE 3\" -> \"PHASE3\"\n        - \"phase3\" -> \"PHASE3\"\n        Args:\n            phase: Phase value to normalize\n        Returns:\n            Normalized phase value or None\n        '''\n        pass\n    @staticmethod\n    def validate_page_params(page: int, page_size: int) -> tuple[int, int]:\n        '''Validate pagination parameters.\n        Args:\n            page: Page number (minimum 1)\n            page_size: Results per page (1-100)\n        Returns:\n            Validated (page, page_size) tuple\n        Raises:\n            InvalidParameterError: If parameters are invalid\n        '''\n        pass\n    @staticmethod\n    def parse_search_params(params: dict[str, Any], domain: str) -> dict[str, Any]:\n        '''Parse and validate all search parameters for a domain.\n        Args:\n            params: Raw parameters dictionary\n            domain: Domain being searched\n        Returns:\n            Validated parameters dictionary\n        '''\n        pass",
    "snippet_id": "snippet_180"
  },
  {
    "class_name": "tau2.evaluator.evaluator_nl_assertions.NLAssertionsEvaluator",
    "skeleton": "\nclass NLAssertionsEvaluator:\n    '''\n    Judge that evaluates whether a trajectory adheres to all the natural-language assertions.\n    '''\n    @classmethod\n    def calculate_reward(cls, task: Task, full_trajectory: list[Message]) -> RewardInfo:\n        '''\n        Calculate the reward for the simulation by using an LLM to evaluate whether the trajectory adheres to all the natural-language assertions\n        '''\n        pass\n    @classmethod\n    def evaluate_nl_assertions(cls, trajectory: list[Message], nl_assertions: list[str]) -> list[NLAssertionCheck]:\n        '''\n        Evaluate whether the trajectory meets each expected outcome.\n        Args:\n            trajectory: List of messages from the conversation\n            nl_assertions: List of natural-language assertions to evaluate\n        Returns:\n            List of evaluation results for each NL assertion, containing:\n            - nl_assertion: The NL assertion being evaluated\n            - metExpectation: Boolean indicating if the assertion was met\n            - reasoning: Explanation for the evaluation\n        '''\n        pass",
    "snippet_id": "snippet_181"
  },
  {
    "class_name": "html_to_markdown.processing.OutputSink",
    "skeleton": "class OutputSink:\n    '''Abstract output sink for processed markdown text.'''\n\n    def write(self, text: str) -> None:\n        '''Write text to the sink.'''\n        pass\n\n    def finalize(self) -> None:\n        '''Finalize the output.'''\n        pass",
    "snippet_id": "snippet_182"
  },
  {
    "class_name": "inference_config.InferenceConfig",
    "skeleton": "@dataclass\nclass InferenceConfig:\n    '''Configuration for inference runs.'''\n\n    def __post_init__(self):\n        '''Validate and adjust configuration after initialization.'''\n        pass\n\n    def to_dict(self) -> Dict[str, Any]:\n        '''Convert the configuration to a dictionary.'''\n        pass\n    @classmethod\n    def from_dict(cls, config_dict: Dict[str, Any]) -> 'InferenceConfig':\n        '''Create a configuration instance from a dictionary.'''\n        pass\n    @classmethod\n    def from_yaml(cls, yaml_path: str) -> 'InferenceConfig':\n        '''Load configuration from a YAML file.'''\n        pass\n\n    def save_yaml(self, yaml_path: str) -> None:\n        '''Save configuration to a YAML file.'''\n        pass",
    "snippet_id": "snippet_183"
  },
  {
    "class_name": "vagen.env.svg.dreamsim.DreamSimScoreCalculator",
    "skeleton": "\nclass DreamSimScoreCalculator:\n    '''\n    A wrapper class for DreamSim model to calculate similarity scores between images.\n        '''\n\n    def __init__(self, pretrained=True, cache_dir='~/.cache', device=None):\n        '''\n        Initialize DreamSim model.\n        '''\n        pass\n\n    def calculate_similarity_score(self, gt_im, gen_im):\n        '''\n        Calculate similarity score between ground truth and generated images.\n        '''\n        pass\n\n    def calculate_batch_scores(self, gt_images: List[Any], gen_images: List[Any]) -> List[float]:\n        '''\n        Calculate similarity scores for multiple image pairs.\n        Since DreamSim doesn't natively support batch comparison, we process each pair individually.\n        '''\n        pass",
    "snippet_id": "snippet_184"
  },
  {
    "class_name": "vagen.env.utils.top_string_tracker.TopKStringTracker",
    "skeleton": "\nclass TopKStringTracker:\n    '''\n    Efficient Top-K string tracking data structure\n    Core ideas:\n    1. Use hash table to record string counts\n    2. Use min-heap to maintain top-m strings (heap root is the m-th largest element)\n    3. Lazy cleanup: avoid frequent heap operations\n    '''\n\n    def __init__(self, m: int):\n        '''\n        Initialize the data structure\n        Args:\n            m: Maximum number of strings to retain\n        '''\n        pass\n\n    def add_strings(self, strings: List[str]) -> None:\n        '''\n        Add k strings to the data structure\n        Args:\n            strings: List of strings to add\n        '''\n        pass\n\n    def add_string_dict(self, string_counts: dict) -> None:\n        '''\n        Add strings with their counts from a dictionary\n        Args:\n            string_counts: Dictionary mapping strings to their occurrence counts\n        '''\n        pass\n\n    def _cleanup_heap(self) -> None:\n        '''\n        Clean up outdated counts in heap and rebuild heap structure\n        '''\n        pass\n\n    def get_top_k(self, k: int) -> Set[str]:\n        '''\n        Return the set of top-k strings by occurrence count\n        Args:\n            k: Number of strings to return\n        Returns:\n            Set containing the top-k strings\n        '''\n        pass\n\n    def trim_to_m(self) -> None:\n        '''\n        Keep only the top-m strings by occurrence count, delete others\n        '''\n        pass\n\n    def size(self) -> int:\n        '''Return the number of strings currently stored'''\n        pass\n\n    def get_count(self, string: str) -> int:\n        '''Get the occurrence count of a specific string'''\n        pass",
    "snippet_id": "snippet_185"
  },
  {
    "class_name": "genai_bench.auth.unified_factory.UnifiedAuthFactory",
    "skeleton": "\nclass UnifiedAuthFactory:\n    '''Factory for creating model and storage authentication providers.'''\n    @staticmethod\n    def create_model_auth(provider: str, **kwargs) -> ModelAuthProvider:\n        '''Create a model endpoint authentication provider.\n        Args:\n            provider: Provider type ('openai', 'oci', 'aws-bedrock',\n                'azure-openai', 'gcp-vertex')\n            **kwargs: Provider-specific arguments\n        Returns:\n            ModelAuthProvider instance\n        Raises:\n            ValueError: If provider is not supported\n        '''\n        pass\n    @staticmethod\n    def create_storage_auth(provider: str, **kwargs) -> StorageAuthProvider:\n        '''Create a storage authentication provider.\n        Args:\n            provider: Provider type ('oci', 'aws', 'azure', 'gcp', 'github')\n            **kwargs: Provider-specific arguments\n        Returns:\n            StorageAuthProvider instance\n        Raises:\n            ValueError: If provider is not supported\n        '''\n        pass",
    "snippet_id": "snippet_186"
  },
  {
    "class_name": "genai_bench.data.loaders.factory.DataLoaderFactory",
    "skeleton": "\nclass DataLoaderFactory:\n    '''Factory for creating data loaders and loading data.'''\n    @staticmethod\n    def load_data_for_task(task: str, dataset_config: DatasetConfig) -> Union[List[str], List[Tuple[str, Any]]]:\n        '''Load data for a specific task.\n        Args:\n            task: Task name in format \"input-to-output\"\n            dataset_config: Dataset configuration\n        Returns:\n            Loaded data for the task\n        '''\n        pass\n    @staticmethod\n    def _load_text_data(dataset_config: DatasetConfig, output_modality: str) -> List[str]:\n        '''Load text data.'''\n        pass\n    @staticmethod\n    def _load_image_data(dataset_config: DatasetConfig) -> List[Tuple[str, Any]]:\n        '''Load image data.'''\n        pass",
    "snippet_id": "snippet_187"
  },
  {
    "class_name": "doris_mcp_server.utils.logger.LogCleanupManager",
    "skeleton": "\nclass LogCleanupManager:\n    '''Log file cleanup manager for automatic maintenance'''\n\n    def __init__(self, log_dir: str, max_age_days: int=30, cleanup_interval_hours: int=24):\n        '''\n        Initialize log cleanup manager.\n        Args:\n            log_dir: Directory containing log files\n            max_age_days: Maximum age of log files in days (default: 30 days)\n            cleanup_interval_hours: Cleanup interval in hours (default: 24 hours)\n        '''\n        pass\n\n    def start_cleanup_scheduler(self):\n        '''Start the cleanup scheduler in a background thread'''\n        pass\n\n    def stop_cleanup_scheduler(self):\n        '''Stop the cleanup scheduler'''\n        pass\n\n    def _cleanup_loop(self):\n        '''Background loop for periodic cleanup'''\n        pass\n\n    def cleanup_old_logs(self):\n        '''Clean up old log files based on age'''\n        pass\n\n    def get_cleanup_stats(self) -> dict:\n        '''Get statistics about log files and cleanup status'''\n        pass",
    "snippet_id": "snippet_188"
  },
  {
    "class_name": "teleop_se3_agent.RateLimiter",
    "skeleton": "\nclass RateLimiter:\n    '''Convenience class for enforcing rates in loops.'''\n\n    def __init__(self, hz):\n        '''\n        Args:\n            hz (int): frequency to enforce\n        '''\n        pass\n\n    def sleep(self, env):\n        '''Attempt to sleep at the specified rate in hz.'''\n        pass",
    "snippet_id": "snippet_189"
  },
  {
    "class_name": "siirl.workers.base_worker.resouce_pool.ClassWithInitArgs",
    "skeleton": "\nclass ClassWithInitArgs:\n    '''\n    Wrapper class that stores constructor arguments for deferred instantiation.\n    This class is particularly useful for remote class instantiation where\n    the actual construction needs to happen at a different time or location.\n    '''\n\n    def __init__(self, cls, *args, **kwargs) -> None:\n        '''Initialize the ClassWithInitArgs instance.\n        Args:\n            cls: The class to be instantiated later\n            *args: Positional arguments for the class constructor\n            **kwargs: Keyword arguments for the class constructor\n        '''\n        pass\n\n    def __call__(self) -> Any:\n        '''Instantiate the stored class with the stored arguments.'''\n        pass",
    "snippet_id": "snippet_190"
  },
  {
    "class_name": "siirl.workers.dag_worker.mixins.execution_mixin.ExecutionMixin",
    "skeleton": "\nclass ExecutionMixin:\n    '''Handles the core DAG execution and training loop logic.'''\n\n    def execute_task_graph(self):\n        '''Main entry point to start the DAG execution pipeline.'''\n        pass\n\n    def _run_training_loop(self):\n        '''\n        The main loop that iterates through training steps and epochs.\n        '''\n        pass\n\n    def _find_first_non_compute_ancestor(self, start_node_id: str) -> Optional[Node]:\n        '''\n        Traverses upwards from a starting node to find the first ancestor\n        that is not of type COMPUTE.\n        Uses a Breadth-First Search (BFS) strategy to prioritize finding the\n        closest ancestor by level.\n        '''\n        pass\n\n    def _cleanup_step_buffers(self, visited_nodes: Set[str], timing_raw: dict) -> None:\n        '''\n        Encapsulates the logic for resetting and clearing all step-related buffers.\n        This includes the distributed Ray data buffers and the local internal cache.\n        This is called at the end of a step, whether it completed successfully or was aborted.\n        '''\n        pass\n\n    def _run_training_step(self, epoch: int, batch_idx: int) -> Optional[List[Tuple[str, Any]]]:\n        '''Executes a single training step by traversing the computational graph.'''\n        pass",
    "snippet_id": "snippet_191"
  },
  {
    "class_name": "siirl.workers.dag_worker.mixins.utilities_mixin.DistributedMetricAggregator",
    "skeleton": "\nclass DistributedMetricAggregator:\n    '''\n    A helper class to encapsulate the logic for aggregating metrics\n    in a distributed environment.\n    '''\n\n    def __init__(self, local_metrics: Dict[str, Union[float, List[float], torch.Tensor]], group: Optional[dist.ProcessGroup]):\n        '''\n        Initializes the aggregator and prepares metrics for reduction.\n        Args:\n            local_metrics: The dictionary of metrics on the local rank.\n            group: The process group for distributed communication.\n        '''\n        pass\n\n    def _bucket_local_metrics(self, metrics: Dict) -> defaultdict:\n        '''\n        Parses local metrics and groups them by the required reduction operation.\n        This step also performs local pre-aggregation on lists and tensors.\n        This version correctly handles multi-element tensors as input.\n        Returns:\n            A defaultdict containing keys and pre-aggregated values,\n            grouped by reduction operation type (_ReduceOp).\n        '''\n        pass\n\n    def aggregate_and_get_results(self) -> Dict[str, float]:\n        '''\n        Performs the distributed all_reduce operations and composes the final\n        metrics dictionary.\n        Returns:\n            A dictionary with the globally aggregated metrics.\n        '''\n        pass",
    "snippet_id": "snippet_192"
  },
  {
    "class_name": "siirl.workers.dag_worker.mixins.validation_mixin.ValidationMixin",
    "skeleton": "\nclass ValidationMixin:\n    '''Handles the validation process, including generation, scoring, and aggregation.'''\n\n    def _validate(self) -> Dict[str, float]:\n        '''Performs validation by generating, scoring, and aggregating metrics across all ranks.'''\n        pass\n\n    def _prepare_validation_batch(self) -> DataProto:\n        '''Fetches and prepares a single batch for validation.'''\n        pass\n\n    def _prepare_generation_batch(self, batch: DataProto) -> DataProto:\n        '''Pops keys from a batch to isolate data needed for sequence generation.'''\n        pass\n\n    def _generate_for_validation(self, batch_proto: DataProto) -> DataProto:\n        '''Generates sequences using the rollout worker for a validation batch.'''\n        pass\n\n    def _score_and_package_results(self, generated_proto: DataProto) -> List[ValidationResult]:\n        '''Scores generated sequences and packages them into ValidationResult objects.'''\n        pass\n\n    def _aggregate_and_log_validation_metrics(self, all_payloads: List[ValidationPayload]) -> Dict[str, float]:\n        '''On Rank 0, aggregates all validation results and logs performance.'''\n        pass\n\n    def _aggregate_validation_results(self, all_payloads: List[ValidationPayload]) -> Dict[str, float]:\n        '''Computes the final metric dictionary from all gathered validation payloads.'''\n        pass\n\n    def _dump_validation_generations(self, results: List[ValidationResult]):\n        '''\n        Dumps local validation generation results to a rank-specific JSONL file.\n        This method is called by each rank to dump its own portion of the\n        validation data into a shared directory.\n        Args:\n            results: A list of ValidationResult objects containing the local\n                     data for the current rank.\n        '''\n        pass",
    "snippet_id": "snippet_193"
  },
  {
    "class_name": "workflows.retry_policy.ConstantDelayRetryPolicy",
    "skeleton": "class ConstantDelayRetryPolicy:\n    '''Retry at a fixed interval up to a maximum number of attempts.\n    Examples:\n        ```python\n        @step(retry_policy=ConstantDelayRetryPolicy(delay=5, maximum_attempts=10))\n        async def flaky(self, ev: StartEvent) -> StopEvent:\n            ...\n        ```\n    '''\n\n    def __init__(self, maximum_attempts: int=3, delay: float=5) -> None:\n        '''\n        Initialize the policy.\n        Args:\n            maximum_attempts (int): Maximum consecutive attempts. Defaults to 3.\n            delay (float): Seconds to wait between attempts. Defaults to 5.\n        '''\n        pass\n\n    def next(self, elapsed_time: float, attempts: int, error: Exception) -> float | None:\n        '''Return the fixed delay while attempts remain; otherwise `None`.'''\n        pass",
    "snippet_id": "snippet_194"
  },
  {
    "class_name": "src.omnicoreagent.core.llm_support.LLMToolSupport",
    "skeleton": "\nclass LLMToolSupport:\n    '''Class to handle LLM tool support checking'''\n    @classmethod\n    def check_tool_support(cls, llm_config: dict[str, Any]) -> bool:\n        '''Check if the current LLM configuration supports tools.\n        Args:\n            llm_config: LLM configuration dictionary containing model and provider\n        Returns:\n            bool: True if the LLM supports tools, False otherwise\n        '''\n        pass\n    @classmethod\n    def get_supported_models(cls, provider: str) -> list[str] | None:\n        '''Get list of supported models for a provider.\n        Args:\n            provider: The provider name\n        Returns:\n            Optional[List[str]]: List of supported models or None if all models are supported\n        '''\n        pass",
    "snippet_id": "snippet_195"
  },
  {
    "class_name": "src.omnicoreagent.mcp_omni_connect.client.Configuration",
    "skeleton": "@dataclass\nclass Configuration:\n    '''Manages configuration and environment variables for the MCP client.'''\n\n    def __post_init__(self) -> None:\n        '''Initialize configuration with environment variables.'''\n        pass\n    @staticmethod\n    def load_env() -> None:\n        '''Load environment variables from .env file.'''\n        pass\n\n    def load_config(self, file_path: str) -> dict:\n        '''Load server configuration from JSON file.'''\n        pass",
    "snippet_id": "snippet_196"
  },
  {
    "class_name": "AutoDiceRoller.AutoDiceRoller",
    "skeleton": "\nclass AutoDiceRoller:\n    '''\n    AutoDiceRoller\n    '''\n\n    def __init__(self, args):\n        '''\n        Init AutoDiceRoller\n        '''\n        pass\n\n    def update_img_frame_debug(self):\n        '''\n        update_img_frame_debug\n        '''\n        pass\n\n    def run_once(self):\n        '''\n        Process one game window frame\n        '''\n        pass",
    "snippet_id": "snippet_197"
  },
  {
    "class_name": "viby.commands.shortcuts.ShortcutsCommand",
    "skeleton": "\nclass ShortcutsCommand:\n    '''处理快捷键安装和管理的命令'''\n\n    def __init__(self):\n        '''初始化快捷键命令'''\n        pass\n\n    def run(self, shell: Optional[str]=None) -> int:\n        '''\n        安装并管理快捷键\n        '''\n        pass\n\n    def _print_result(self, result: dict) -> None:\n        '''\n        打印操作结果\n        Args:\n            result: 操作结果字典\n        '''\n        pass",
    "snippet_id": "snippet_198"
  },
  {
    "class_name": "viby.commands.tools.ToolsCommand",
    "skeleton": "\nclass ToolsCommand:\n    '''\n    工具管理命令类，提供工具嵌入向量更新和列出工具信息功能\n    支持以下子命令：\n    - embed - 嵌入向量管理，包含update、start、stop、status子命令\n    - list - 列出所有可用的MCP工具\n    - download - 检查并下载嵌入模型\n    '''\n\n    def __init__(self):\n        '''初始化工具命令'''\n        pass\n\n    def list_tools(self) -> int:\n        '''列出所有可用的MCP工具'''\n        pass\n\n    def run(self, embed_subcommand: Optional[str]=None) -> int:\n        '''\n        管理嵌入向量服务，支持子命令：update、start、stop、status、download\n        '''\n        pass",
    "snippet_id": "snippet_199"
  },
  {
    "class_name": "viby.llm.compaction.CompactionManager",
    "skeleton": "\nclass CompactionManager:\n    '''\n    消息压缩管理器 - 提供智能消息历史压缩功能\n    实现了智能压缩算法，使得在保持语义和关键信息的同时减少token数量\n    '''\n\n    def __init__(self):\n        '''初始化压缩管理器'''\n        pass\n\n    def _estimate_token_count(self, text: str) -> int:\n        '''\n        简单估算文本的token数量 - 不使用tokenizer\n        Args:\n            text: 需要估算token数的文本\n        Returns:\n            估算的token数量\n        '''\n        pass\n\n    def _count_tokens_in_messages(self, messages: List[Dict[str, Any]]) -> int:\n        '''\n        计算消息列表中的总token数\n        Args:\n            messages: 消息列表\n        Returns:\n            总token数\n        '''\n        pass\n\n    def should_compact(self, messages: List[Dict[str, Any]], model_config: Dict[str, Any]) -> bool:\n        '''\n        根据token数量判断是否应该压缩消息历史\n        Args:\n            messages: 消息历史\n            model_config: 模型配置\n        Returns:\n            是否应该压缩\n        '''\n        pass\n\n    def _format_conversation_for_compression(self, messages: List[Dict[str, Any]]) -> str:\n        '''\n        将消息列表格式化为适合压缩的文本格式\n        Args:\n            messages: 消息列表\n        Returns:\n            格式化后的对话文本\n        '''\n        pass\n\n    def _expand_compressed_to_messages(self, compressed_text: str, original_messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        '''\n        将压缩后的文本扩展为消息列表\n        Args:\n            compressed_text: 压缩后的对话摘要文本\n            original_messages: 原始消息列表\n        Returns:\n            包含压缩摘要的消息列表\n        '''\n        pass\n\n    def compact_messages(self, messages: List[Dict[str, Any]], model_config: Dict[str, Any]) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n        '''压缩消息历史，保留关键信息并减少 token 数量。'''\n        pass",
    "snippet_id": "snippet_200"
  },
  {
    "class_name": "judo.config.OverridableConfig",
    "skeleton": "@dataclass\nclass OverridableConfig:\n    '''A class that provides an interface to switch between its field values depending on an override key.'''\n\n    def __post_init__(self) -> None:\n        '''Initialize the override key to 'default'.'''\n        pass\n\n    def set_override(self, key: str, reset_to_defaults: bool=True) -> None:\n        '''Set the overridden values for the config based on the override registry.\n        Args:\n            key: The key to use for the override.\n            reset_to_defaults: If True, reset the values to their defaults if no override is found. This is useful for\n                when you switch from different non-default overrides to other non-default overrides.\n        '''\n        pass",
    "snippet_id": "snippet_201"
  },
  {
    "class_name": "src.pipask._vendor.pip._internal.index.package_finder.BestCandidateResult",
    "skeleton": "\nclass BestCandidateResult:\n    '''A collection of candidates, returned by `PackageFinder.find_best_candidate`.\n    This class is only intended to be instantiated by CandidateEvaluator's\n    `compute_best_candidate()` method.\n    '''\n\n    def __init__(self, candidates: List[InstallationCandidate], applicable_candidates: List[InstallationCandidate], best_candidate: Optional[InstallationCandidate]) -> None:\n        '''\n        :param candidates: A sequence of all available candidates found.\n        :param applicable_candidates: The applicable candidates.\n        :param best_candidate: The most preferred candidate found, or None\n            if no applicable candidates were found.\n        '''\n        pass\n\n    def iter_all(self) -> Iterable[InstallationCandidate]:\n        '''Iterate through all candidates.'''\n        pass\n\n    def iter_applicable(self) -> Iterable[InstallationCandidate]:\n        '''Iterate through the applicable candidates.'''\n        pass",
    "snippet_id": "snippet_202"
  },
  {
    "class_name": "src.pipask._vendor.pip._internal.models.target_python.TargetPython",
    "skeleton": "\nclass TargetPython:\n    '''\n    Encapsulates the properties of a Python interpreter one is targeting\n    for a package install, download, etc.\n    '''\n\n    def __init__(self, platforms: Optional[List[str]]=None, py_version_info: Optional[Tuple[int, ...]]=None, abis: Optional[List[str]]=None, implementation: Optional[str]=None) -> None:\n        '''\n        :param platforms: A list of strings or None. If None, searches for\n            packages that are supported by the current system. Otherwise, will\n            find packages that can be built on the platforms passed in. These\n            packages will only be downloaded for distribution: they will\n            not be built locally.\n        :param py_version_info: An optional tuple of ints representing the\n            Python version information to use (e.g. `sys.version_info[:3]`).\n            This can have length 1, 2, or 3 when provided.\n        :param abis: A list of strings or None. This is passed to\n            compatibility_tags.py's get_supported() function as is.\n        :param implementation: A string or None. This is passed to\n            compatibility_tags.py's get_supported() function as is.\n        '''\n        pass\n\n    def format_given(self) -> str:\n        '''\n        Format the given, non-None attributes for display.\n        '''\n        pass\n\n    def get_sorted_tags(self) -> List[Tag]:\n        '''\n        Return the supported PEP 425 tags to check wheel candidates against.\n        The tags are returned in order of preference (most preferred first).\n        '''\n        pass\n\n    def get_unsorted_tags(self) -> Set[Tag]:\n        '''Exactly the same as get_sorted_tags, but returns a set.\n        This is important for performance.\n        '''\n        pass",
    "snippet_id": "snippet_203"
  },
  {
    "class_name": "src.pipask._vendor.pip._internal.models.wheel.Wheel",
    "skeleton": "\nclass Wheel:\n    '''A wheel file'''\n\n    def __init__(self, filename: str) -> None:\n        '''\n        :raises InvalidWheelFilename: when the filename is invalid for a wheel\n        '''\n        pass\n\n    def get_formatted_file_tags(self) -> List[str]:\n        '''Return the wheel's tags as a sorted list of strings.'''\n        pass\n\n    def support_index_min(self, tags: List[Tag]) -> int:\n        '''Return the lowest index that one of the wheel's file_tag combinations\n        achieves in the given list of supported tags.\n        For example, if there are 8 supported tags and one of the file tags\n        is first in the list, then return 0.\n        :param tags: the PEP 425 tags to check the wheel against, in order\n            with most preferred first.\n        :raises ValueError: If none of the wheel's file tags match one of\n            the supported tags.\n        '''\n        pass\n\n    def find_most_preferred_tag(self, tags: List[Tag], tag_to_priority: Dict[Tag, int]) -> int:\n        '''Return the priority of the most preferred tag that one of the wheel's file\n        tag combinations achieves in the given list of supported tags using the given\n        tag_to_priority mapping, where lower priorities are more-preferred.\n        This is used in place of support_index_min in some cases in order to avoid\n        an expensive linear scan of a large list of tags.\n        :param tags: the PEP 425 tags to check the wheel against.\n        :param tag_to_priority: a mapping from tag to priority of that tag, where\n            lower is more preferred.\n        :raises ValueError: If none of the wheel's file tags match one of\n            the supported tags.\n        '''\n        pass\n\n    def supported(self, tags: Iterable[Tag]) -> bool:\n        '''Return whether the wheel is compatible with one of the given tags.\n        :param tags: the PEP 425 tags to check the wheel against.\n        '''\n        pass",
    "snippet_id": "snippet_204"
  },
  {
    "class_name": "src.device_clone.bar_size_converter.BarSizeConverter",
    "skeleton": "\nclass BarSizeConverter:\n    '''Handles conversion between BAR addresses and size encodings.'''\n    @staticmethod\n    def address_to_size(base_address: int, bar_type: str='memory') -> int:\n        '''\n        Convert a BAR base address to its size in bytes.\n        According to PCIe spec, the size is determined by writing all 1s to the BAR\n        and reading back. The device will return 0s in the bits that are hardwired\n        to 0 (representing the size) and 1s in the bits that can be programmed.\n        Args:\n            base_address: The BAR base address value\n            bar_type: Type of BAR (\"memory\" or \"io\")\n        Returns:\n            Size of the BAR in bytes\n        Raises:\n            ValueError: If the address format is invalid\n        '''\n        pass\n    @staticmethod\n    def size_to_encoding(size: int, bar_type: str='memory', is_64bit: bool=False, prefetchable: bool=False) -> int:\n        '''\n        Convert a BAR size to its proper encoding for the configuration space.\n        The encoding sets all bits to 1 except for the size bits which are 0,\n        and the type bits in the lower nibble.\n        Args:\n            size: Size of the BAR in bytes\n            bar_type: Type of BAR (\"memory\" or \"io\")\n            is_64bit: Whether this is a 64-bit memory BAR\n            prefetchable: Whether this is a prefetchable memory BAR\n        Returns:\n            Encoded BAR value for the configuration space\n        Raises:\n            ValueError: If the size is invalid according to PCIe spec\n        '''\n        pass\n    @staticmethod\n    def decode_bar_register(bar_value: int) -> Tuple[str, int, bool, bool]:\n        '''\n        Decode a BAR register value to extract type and properties.\n        Args:\n            bar_value: The BAR register value\n        Returns:\n            Tuple of (bar_type, address, is_64bit, prefetchable)\n        '''\n        pass\n    @staticmethod\n    def validate_bar_size(size: int, bar_type: str='memory') -> bool:\n        '''\n        Validate if a BAR size meets PCIe specification requirements.\n        Args:\n            size: Size to validate in bytes\n            bar_type: Type of BAR (\"memory\" or \"io\")\n        Returns:\n            True if size is valid, False otherwise\n        '''\n        pass\n    @staticmethod\n    def get_size_from_encoding(encoded_value: int, bar_type: str='memory') -> int:\n        '''\n        Extract the size from an encoded BAR value.\n        This is the reverse of size_to_encoding - it extracts the size\n        from a BAR value that has all 1s except for the size bits.\n        Args:\n            encoded_value: The encoded BAR value\n            bar_type: Type of BAR (\"memory\" or \"io\")\n        Returns:\n            Size of the BAR in bytes\n        '''\n        pass\n    @staticmethod\n    def format_size(size: int) -> str:\n        '''\n        Format a size value for human-readable display.\n        Args:\n            size: Size in bytes\n        Returns:\n            Formatted string (e.g., \"4KB\", \"256MB\")\n        '''\n        pass\n    @classmethod\n    def convert_bar_for_shadow_space(cls, bar_info: dict) -> dict:\n        '''\n            Convert BAR information for use in shadow configuration space.\n            Args:\n                bar_info: Dictionary containing BAR information with keys:\n                    - base_address: Current BAR base address\n                    - size: BAR size in bytes\n                    - bar_type: \"memory\" or \"io\"\n                    - is_64bit: Whether this is a 64-bit BAR\n                    - prefetchable: Whether this is prefetchable\n        return bar_size  # Ensure all code paths return an int\n            Returns:\n                Dictionary with:\n                    - encoded_value: The encoded BAR value for shadow space\n                    - size: The size in bytes\n                    - size_str: Human-readable size string\n        '''\n        pass",
    "snippet_id": "snippet_205"
  },
  {
    "class_name": "src.device_clone.hex_formatter.ConfigSpaceHexFormatter",
    "skeleton": "\nclass ConfigSpaceHexFormatter:\n    '''\n    Formats PCI configuration space data into hex files for FPGA initialization.\n    This class handles:\n    - Converting configuration space bytes to little-endian 32-bit words\n    - Generating properly formatted hex files for Vivado $readmemh\n    - Adding debug comments with register offsets\n    - Ensuring proper alignment and padding\n    '''\n\n    def __init__(self):\n        '''Initialize the hex formatter.'''\n        pass\n\n    def format_config_space_to_hex(self, config_space_data: bytes, include_comments: bool=True, words_per_line: int=1, vendor_id: Optional[str]=None, device_id: Optional[str]=None, class_code: Optional[str]=None, board: Optional[str]=None) -> str:\n        '''\n        Convert configuration space data to hex format.\n        Args:\n            config_space_data: Raw configuration space bytes\n            include_comments: Whether to include offset/register comments\n            words_per_line: Number of 32-bit words per line (default: 1)\n        Returns:\n            Formatted hex string suitable for $readmemh\n        Raises:\n            ValueError: If config space data is invalid\n        '''\n        pass\n\n    def _get_register_comment(self, offset: int) -> Optional[str]:\n        '''\n        Get a descriptive comment for a register offset.\n        Args:\n            offset: Register offset in configuration space\n        Returns:\n            Register description or None if no standard register\n        '''\n        pass\n\n    def write_hex_file(self, config_space_data: bytes, output_path: Union[str, Path], include_comments: bool=True) -> Path:\n        '''\n        Write configuration space data to a hex file.\n        Args:\n            config_space_data: Raw configuration space bytes\n            output_path: Path where hex file should be written\n            include_comments: Whether to include offset/register comments\n        Returns:\n            Path to the written hex file\n        Raises:\n            IOError: If file cannot be written\n        '''\n        pass\n\n    def validate_hex_file(self, hex_file_path: Union[str, Path]) -> bool:\n        '''\n        Validate a hex file for proper formatting.\n        Args:\n            hex_file_path: Path to hex file to validate\n        Returns:\n            True if valid, False otherwise\n        '''\n        pass\n\n    def convert_to_dword_list(self, config_space_data: bytes) -> List[int]:\n        '''\n        Convert configuration space bytes to a list of 32-bit dwords.\n        Args:\n            config_space_data: Raw configuration space bytes\n        Returns:\n            List of 32-bit integers in little-endian format\n        '''\n        pass",
    "snippet_id": "snippet_206"
  },
  {
    "class_name": "src.device_clone.pcileech_context.TimingParameters",
    "skeleton": "@dataclass(slots=True)\nclass TimingParameters:\n    '''Device timing parameters.'''\n\n    def __post_init__(self):\n        '''Validate timing parameters.'''\n        pass\n    @property\n    def total_latency(self) -> int:\n        '''Calculate total latency.'''\n        pass\n    @property\n    def effective_bandwidth_mbps(self) -> float:\n        '''Estimate bandwidth in MB/s.'''\n        pass\n\n    def to_dict(self) -> Dict[str, Any]:\n        '''Convert to dictionary.'''\n        pass",
    "snippet_id": "snippet_207"
  },
  {
    "class_name": "src.file_management.option_rom_manager.OptionROMManager",
    "skeleton": "\nclass OptionROMManager:\n    '''Manager for Option-ROM extraction and handling'''\n\n    def __init__(self, output_dir: Optional[Path]=None, rom_file_path: Optional[str]=None):\n        '''\n        Initialize the Option-ROM manager\n        Args:\n            output_dir: Path to directory for storing extracted ROM\n            rom_file_path: Path to an existing ROM file to use instead of extraction\n        '''\n        pass\n\n    def extract_rom_linux(self, bdf: str) -> Tuple[bool, str]:\n        '''\n        Extract Option-ROM from a PCI device on Linux\n        Args:\n            bdf: PCIe Bus:Device.Function (e.g., \"0000:03:00.0\")\n        Returns:\n            Tuple of (success, rom_path)\n        '''\n        pass\n\n    def load_rom_file(self, file_path: Optional[str]=None) -> bool:\n        '''\n        Load ROM data from a file\n        Args:\n            file_path: Path to ROM file (uses self.rom_file_path if None)\n        Returns:\n            True if ROM was loaded successfully\n        '''\n        pass\n\n    def save_rom_hex(self, output_path: Optional[str]=None) -> bool:\n        '''\n        Save ROM data in a format suitable for SystemVerilog $readmemh\n        Args:\n            output_path: Path to save the hex file (default: output_dir/rom_init.hex)\n        Returns:\n            True if data was saved successfully\n        '''\n        pass\n\n    def get_rom_info(self) -> Dict[str, str]:\n        '''\n        Get information about the ROM\n        Returns:\n            Dictionary with ROM information\n        '''\n        pass\n\n    def setup_option_rom(self, bdf: str, use_existing_rom: bool=False) -> Dict[str, str]:\n        '''\n        Complete setup process: extract ROM, save hex file, and return info\n        Args:\n            bdf: PCIe Bus:Device.Function\n            use_existing_rom: Use existing ROM file if available\n        Returns:\n            Dictionary with ROM information\n        '''\n        pass",
    "snippet_id": "snippet_208"
  },
  {
    "class_name": "src.scripts.driver_scrape.DriverAnalyzer",
    "skeleton": "\nclass DriverAnalyzer:\n    '''\n    Encapsulates driver analysis functionality with shared state.\n    This class maintains pre-compiled regex patterns and file content\n    to avoid duplication and improve performance.\n    '''\n\n    def __init__(self, file_contents: Dict[pathlib.Path, str]):\n        '''\n        Initialize analyzer with file contents.\n        Args:\n            file_contents: Dictionary mapping file paths to their content\n        '''\n        pass\n\n    def _get_function_pattern(self, reg_name: str) -> re.Pattern:\n        '''Get cached function pattern for register name.'''\n        pass\n\n    def analyze_function_context(self, reg_name: str) -> Dict[str, Any]:\n        '''\n        Analyze the function context where a register is used.\n        Enhanced to recognize macros split across lines and provide\n        fallback timing detection.\n        '''\n        pass\n\n    def _determine_timing(self, func_name: str, func_body: str) -> str:\n        '''\n        Determine timing context with fallback detection.\n        Args:\n            func_name: Name of the function\n            func_body: Content of the function\n        Returns:\n            Timing classification string\n        '''\n        pass\n\n    def _analyze_access_pattern(self, func_body: str, reg_name: str) -> str:\n        '''Analyze register access patterns within a function.'''\n        pass\n\n    def analyze_access_sequences(self, reg_name: Optional[str]=None) -> List[Dict[str, Any]]:\n        '''\n        Analyze register access sequences with improved function parsing.\n        Enhanced to handle nested braces properly using balance counter.\n        '''\n        pass\n\n    def analyze_timing_constraints(self, reg_name: Optional[str]=None) -> List[Dict[str, Any]]:\n        '''Analyze timing constraints and delays related to register accesses.'''\n        pass",
    "snippet_id": "snippet_209"
  },
  {
    "class_name": "src.shell.Shell",
    "skeleton": "\nclass Shell:\n    '''Wrapper around subprocess supporting dry_run mode.'''\n\n    def __init__(self, dry_run: bool=False, safe_mode: bool=True):\n        '''Initialize shell wrapper.\n        Args:\n            dry_run: If True, commands will be logged but not executed\n            safe_mode: If True, enables additional safety checks for commands\n        '''\n        pass\n\n    def _validate_command_safety(self, cmd: str) -> None:\n        '''Validate command for basic safety if safe_mode is enabled.\n        Args:\n            cmd: Command to validate\n        Raises:\n            RuntimeError: If command appears unsafe\n        '''\n        pass\n\n    def run(self, *parts: str, timeout: int=30, cwd: Optional[str]=None) -> str:\n        '''Execute a shell command and return stripped output.\n        Args:\n            *parts: Command parts to join with spaces\n            timeout: Command timeout in seconds\n            cwd: Working directory for command execution\n        Returns:\n            Command output as string\n        Raises:\n            RuntimeError: If command fails or times out\n        '''\n        pass\n\n    def run_check(self, *parts: str, timeout: int=30, cwd: Optional[str]=None) -> bool:\n        '''Execute a command and return True if successful, False otherwise.\n        Args:\n            *parts: Command parts to join with spaces\n            timeout: Command timeout in seconds\n            cwd: Working directory for command execution\n        Returns:\n            True if command succeeded, False otherwise\n        '''\n        pass\n\n    def write_file(self, path: str, content: str, mode: str='w', create_dirs: bool=True, permissions: Optional[int]=None) -> None:\n        '''Write content to a file (respects dry_run mode).\n        Args:\n            path: File path to write to\n            content: Content to write\n            mode: File write mode (default: \"w\")\n            create_dirs: Create parent directories if they don't exist\n            permissions: Unix file permissions (e.g., 0o600 for user-only)\n        Raises:\n            RuntimeError: If file operation fails\n        '''\n        pass",
    "snippet_id": "snippet_210"
  },
  {
    "class_name": "src.tui.core.error_handler.ErrorHandler",
    "skeleton": "\nclass ErrorHandler:\n    '''\n    Centralized error handling system for the PCILeech TUI application.\n    This class provides a consistent way to handle errors throughout the application,\n    including logging, user notifications, and critical error reporting.\n    '''\n\n    def __init__(self, app):\n        '''\n        Initialize the error handler with the app instance.\n        Args:\n            app: The main TUI application instance\n        '''\n        pass\n\n    def handle_error(self, error: Exception, context: str, severity: str='error') -> None:\n        '''\n        Centralized error handling with context\n        Args:\n            error: The exception that occurred\n            context: Description of where/when the error occurred\n            severity: Error severity level (\"error\", \"warning\", \"critical\")\n        '''\n        pass\n\n    def handle_operation_error(self, operation: str, error: Exception, severity: str='error') -> None:\n        '''\n        Handle errors that occur during specific operations with a standard format.\n        Args:\n            operation: The operation that failed (e.g., \"scanning devices\", \"starting build\")\n            error: The exception that occurred\n            severity: Error severity level (\"error\", \"warning\", \"critical\")\n        '''\n        pass\n\n    def _get_user_friendly_message(self, error: Exception, context: str) -> str:\n        '''\n        Generate a user-friendly error message based on the exception type and context.\n        Args:\n            error: The exception that occurred\n            context: Description of where/when the error occurred\n        Returns:\n            A user-friendly error message\n        '''\n        pass\n\n    def _report_critical_error(self, error: Exception, context: str) -> None:\n        '''\n        Report critical errors for later analysis or immediate attention.\n        Args:\n            error: The exception that occurred\n            context: Description of where/when the error occurred\n        '''\n        pass\n\n    def _write_traceback_to_file(self, context: str, tb_str: str) -> None:\n        '''Append a timestamped traceback to the persistent error log.\n        The log is stored under `logs/error.log` relative to the repository root.\n        '''\n        pass",
    "snippet_id": "snippet_211"
  },
  {
    "class_name": "src.vivado_handling.vivado_runner.VivadoRunner",
    "skeleton": "\nclass VivadoRunner:\n    '''\n    Handles everything Vivado SIMPLY\n    Attributes:\n        board: current target device\n        output_dir: dir for generated vivado project\n        vivado_path: root path to xilinx vivado installation (all paths derived from here)\n        logger: attach a logger\n    '''\n\n    def __init__(self, board: str, output_dir: Path, vivado_path: str, logger: Optional[logging.Logger]=None, device_config: Optional[Dict[str, Any]]=None):\n        '''Initialize VivadoRunner with simplified configuration.\n        Args:\n            board: Target board name (e.g., \"pcileech_35t325_x1\")\n            output_dir: Directory for generated Vivado project\n            vivado_path: Root path to Xilinx Vivado installation\n            logger: Optional logger instance\n            device_config: Optional device configuration dictionary\n        '''\n        pass\n\n    def _extract_version_from_path(self, path: str) -> str:\n        '''Extract Vivado version from installation path.'''\n        pass\n\n    def _is_running_in_container(self) -> bool:\n        '''Check if we're running inside a container.'''\n        pass\n\n    def _run_vivado_on_host(self) -> None:\n        '''Drop out of container and run Vivado on the host system.'''\n        pass\n\n    def run(self) -> None:\n        '''\n        Hand-off to Vivado in batch mode using the generated scripts.\n        If running in container, drop out to host for Vivado execution.\n        Raises:\n            VivadoIntegrationError: If Vivado integration fails\n        '''\n        pass\n\n    def get_vivado_info(self) -> Dict[str, str]:\n        '''Get information about the Vivado installation.\n        Returns:\n            Dictionary with Vivado installation details\n        '''\n        pass",
    "snippet_id": "snippet_212"
  },
  {
    "class_name": "bedrock_agentcore_starter_toolkit.services.runtime.LocalBedrockAgentCoreClient",
    "skeleton": "\nclass LocalBedrockAgentCoreClient:\n    '''Local Bedrock AgentCore client for invoking endpoints.'''\n\n    def __init__(self, endpoint: str):\n        '''Initialize the local client with the given endpoint.'''\n        pass\n\n    def invoke_endpoint(self, session_id: str, payload: str, workload_access_token: str):\n        '''Invoke the endpoint with the given parameters.'''\n        pass",
    "snippet_id": "snippet_213"
  },
  {
    "class_name": "entrypoint.DependencyInfo",
    "skeleton": "@dataclass\nclass DependencyInfo:\n    '''Information about project dependencies.'''\n    @property\n    def found(self) -> bool:\n        '''Whether a dependency file was found.'''\n        pass\n    @property\n    def is_pyproject(self) -> bool:\n        '''Whether this is a pyproject.toml file.'''\n        pass\n    @property\n    def is_requirements(self) -> bool:\n        '''Whether this is a requirements file.'''\n        pass\n    @property\n    def is_root_package(self) -> bool:\n        '''Whether this dependency points to the root package.'''\n        pass",
    "snippet_id": "snippet_214"
  },
  {
    "class_name": "cosmos_rl.utils.parallelism_map.ParallelTopoMapperGroup",
    "skeleton": "\nclass ParallelTopoMapperGroup:\n    '''\n    A class to represent a group of weight sharing topology maps used for weight synchronization.\n    This class manages multiple ParallelTopoMapper instances, each corresponding to a different parallelism strategy.\n    Different model parts may have different parallelism strategies in one whole model.\n    It is used to prepare local shard information for parameters based on the parallelism configuration.\n    It clusters parameters by model part and prepares local shard information for each part.\n    '''\n\n    def __init__(self, global_parallelism: ParallelDims, hf_config: Any, is_policy: bool, underlying_model: Any, backend: str='vllm', weight_mapper: Optional[WeightMapper]=None):\n        '''\n        Initialize the ParallelTopoMapperGroup with the given parallelism configurations.\n        :param global_parallelism: The parallelism config for the policy or rollout.\n        :param hf_config: The huggingface config.\n        :param is_policy: A boolean indicating if this is for policy parallelism.\n        :param underlying_model: The underlying model for which the parallelism map is created.\n        :param weight_mapper: An optional WeightMapper instance. If None, a default mapper is used based on the model type from hf_config.\n        '''\n        pass\n\n    def _cluster_params_by_model_part(self, params: List[Tuple[str, int]]) -> List[List[Tuple[str, int]]]:\n        '''\n        Resort the parameters based on the name mapper.\n        :param params: The parameters to resort.\n        :return: A list of tuples containing the resorted parameters separated by different model parts.\n        '''\n        pass\n\n    def prepare_local_shard_infos(self, hf_key_n_rank: List[Tuple[str, int]], global_rank: int) -> Dict[str, Any]:\n        '''\n        Prepare local shard information for the given parameters based on the parallelism configuration.\n        :param hf_key_n_rank: A list of tuples containing the parameter names and their shape ranks.\n        :param global_rank: The global rank to prepare local shard information for.\n        :return: A dictionary containing the local shard information for each parameter of that rank.\n        '''\n        pass",
    "snippet_id": "snippet_215"
  },
  {
    "class_name": "fraim.config.config.Config",
    "skeleton": "\nclass Config:\n    '''Configuration class for Gemini Scan.'''\n\n    def __init__(self, logger: logging.Logger, mcp_port: int=8765, model: str='gemini/gemini-2.5-flash', output_dir: str='', temperature: float=0, max_iterations: int=50, host: str='localhost', prompt: str | None=None, confidence: int=7, project_path: str=''):\n        '''\n        Initialize configuration.\n        Args:\n            logger: The logger instance to use under this config\n            model: Name of the model to use (e.g., \"gemini/gemini-2.5-flash\", \"openai/gpt-4\")\n            output_dir: Directory to store scan outputs\n            logger: Logger instance\n            max_iterations: Maximum number of tool calling iterations\n            project_path: Path to the project being scanned (set during scan)\n            temperature: Temperature for model generation\n            confidence: Minimum confidence threshold (1-10) for filtering findings\n        '''\n        pass\n\n    def _get_provider_from_model(self, model: str) -> str:\n        '''Extract the provider from the model name.'''\n        pass\n\n    def _get_env_var_for_provider(self, provider: str) -> str:\n        '''Get the expected environment variable name for a provider.'''\n        pass\n\n    def _get_api_key_for_model(self, model_name: str) -> str | None:\n        '''Get the API key for a given model from environment variables.'''\n        pass",
    "snippet_id": "snippet_216"
  },
  {
    "class_name": "mellea.backends.types.ModelOption",
    "skeleton": "\nclass ModelOption:\n    '''A type that wraps around model options.\n    Uses sentinel values (wrapped by @@@) to provide backend and model-agnostic keys for common model options.\n    Create a dictionary containing model options like this:\n    from mellea.backends.types import ModelOption\n    model_options = { ModelOption.TEMPERATURE : 0.0, ModelOption.SYSTEM_PROMPT : \"You are a helpful assistant\" }\n    '''\n    @staticmethod\n    def replace_keys(options: dict, from_to: dict[str, str]) -> dict[str, Any]:\n        '''Returns a new dict with the keys in `options` replaced with the corresponding value for that key in `from_to`.\n        If any keys already exist in `options`, don't edit the associated value.\n        Example:\n        ```python\n        >>> options = {\"k1\": \"v1\", \"k2\": \"v2\", \"M1\": \"m1\"}\n        >>> from_to = {\"k1\": \"M1\", \"k2\": \"M2\"}\n        >>> new_options = replace_keys(options, from_to)\n        >>> print(new_options)\n        ... {\"M1\": \"m1\", \"M2\": \"v2\"}\n        ```\n        '''\n        pass\n    @staticmethod\n    def remove_special_keys(model_options) -> dict[str, Any]:\n        '''Removes all sentiel-valued keys (i.e., those that start with @@@).'''\n        pass\n    @staticmethod\n    def merge_model_options(persistent_opts: dict[str, Any], overwrite_opts: dict[str, Any] | None) -> dict[str, Any]:\n        '''Creates a new dict that contains all keys and values from persistent opts and overwrite opts. If there are duplicate keys, overwrite opts key value pairs will be used.'''\n        pass",
    "snippet_id": "snippet_217"
  },
  {
    "class_name": "mellea.stdlib.base.CBlock",
    "skeleton": "\nclass CBlock:\n    '''A `CBlock` is a block of content that can serve as input to or output from an LLM.'''\n\n    def __init__(self, value: str | None, meta: dict[str, Any] | None=None):\n        '''Initializes the CBlock with a string and some metadata.'''\n        pass\n    @property\n    def value(self) -> str | None:\n        '''Gets the value of the block.'''\n        pass\n    @value.setter\n    def value(self) -> str | None:\n        '''Sets the value of the block.'''\n        pass\n\n    def __str__(self):\n        '''Stringifies the block.'''\n        pass\n\n    def __repr__(self):\n        '''Provides a python-parsable representation of the block (usually).'''\n        pass",
    "snippet_id": "snippet_218"
  },
  {
    "class_name": "agents.utils.token_usage.TokenUsageTracker",
    "skeleton": "\nclass TokenUsageTracker:\n    '''Track token usage across agent executions.'''\n\n    def __init__(self):\n        '''Initialize token usage tracker.'''\n        pass\n\n    def reset(self):\n        '''Reset all usage statistics.'''\n        pass\n\n    def update(self, success: bool, token_usage: Dict[str, int], turn_count: int, execution_time: float):\n        '''\n        Update usage statistics.\n        Args:\n            success: Whether execution was successful\n            token_usage: Token usage dict with input_tokens, output_tokens, total_tokens\n            turn_count: Number of conversation turns\n            execution_time: Execution time in seconds\n        '''\n        pass\n\n    def get_stats(self) -> Dict[str, Any]:\n        '''\n        Get usage statistics with calculated averages.\n        Returns:\n            Dictionary containing usage statistics\n        '''\n        pass",
    "snippet_id": "snippet_219"
  },
  {
    "class_name": "base.task_manager.BaseTask",
    "skeleton": "@dataclass\nclass BaseTask:\n    '''Base class for evaluation tasks.'''\n    @property\n    def name(self) -> str:\n        '''Return the task name using '__' separator format: 'category_id__task_id'.'''\n        pass\n\n    def get_task_instruction(self) -> str:\n        '''Return the full text content of the task instruction file.'''\n        pass",
    "snippet_id": "snippet_220"
  },
  {
    "class_name": "github.token_pool.GitHubTokenPool",
    "skeleton": "\nclass GitHubTokenPool:\n    '''\n    Manages a pool of GitHub tokens with round-robin selection.\n    '''\n\n    def __init__(self, tokens: List[str]):\n        '''\n        Initialize token pool.\n        Args:\n            tokens: List of GitHub personal access tokens\n        '''\n        pass\n\n    def get_next_token(self) -> str:\n        '''\n        Get the next token in round-robin fashion.\n        Returns:\n            The next GitHub token to use\n        '''\n        pass\n\n    def get_current_token(self) -> str:\n        '''\n        Get the current token without advancing the index.\n        Returns:\n            The current GitHub token\n        '''\n        pass\n    @property\n    def pool_size(self) -> int:\n        '''Get the number of tokens in the pool.'''\n        pass",
    "snippet_id": "snippet_221"
  },
  {
    "class_name": "model_config.ModelConfig",
    "skeleton": "\nclass ModelConfig:\n    '''\n    Configuration container for a specific model.\n    It loads the necessary API key and base URL from environment variables.\n    '''\n\n    def __init__(self, model_name: str):\n        '''\n        Initializes the model configuration.\n        Args:\n            model_name: The name of the model (e.g., 'gpt-4o', 'deepseek-chat').\n        Raises:\n            ValueError: If the model is not supported or environment variables are missing.\n        '''\n        pass\n\n    def _get_model_info(self, model_name: str) -> Dict[str, str]:\n        '''\n        Retrieves the configuration details for a given model name.\n        For unsupported models, defaults to using OPENAI_BASE_URL and OPENAI_API_KEY.\n        '''\n        pass\n    @classmethod\n    def get_supported_models(cls) -> List[str]:\n        '''Returns a list of all supported model names.'''\n        pass",
    "snippet_id": "snippet_222"
  },
  {
    "class_name": "bedrock_agentcore.runtime.context.BedrockAgentCoreContext",
    "skeleton": "\nclass BedrockAgentCoreContext:\n    '''Unified context manager for Bedrock AgentCore.'''\n    @classmethod\n    def set_workload_access_token(cls, token: str):\n        '''Set the workload access token in the context.'''\n        pass\n    @classmethod\n    def get_workload_access_token(cls) -> Optional[str]:\n        '''Get the workload access token from the context.'''\n        pass\n    @classmethod\n    def set_request_context(cls, request_id: str, session_id: Optional[str]=None):\n        '''Set request-scoped identifiers.'''\n        pass\n    @classmethod\n    def get_request_id(cls) -> Optional[str]:\n        '''Get current request ID.'''\n        pass\n    @classmethod\n    def get_session_id(cls) -> Optional[str]:\n        '''Get current session ID.'''\n        pass",
    "snippet_id": "snippet_223"
  },
  {
    "class_name": "StrateQueue.core.statistics_manager.TradeRecord",
    "skeleton": "@dataclass\nclass TradeRecord:\n    '''Canonical representation of a single fill/order execution.'''\n    @property\n    def value(self) -> float:\n        '''Absolute dollar value of the fill (signed).'''\n        pass\n    @property\n    def realized_pnl(self) -> float:\n        '''Realized P&L for this trade (simplified calculation).'''\n        pass",
    "snippet_id": "snippet_224"
  },
  {
    "class_name": "StrateQueue.utils.price_formatter.PrecisionPreservingDataHandler",
    "skeleton": "\nclass PrecisionPreservingDataHandler:\n    '''Handler for preserving precision in data operations.'''\n    @staticmethod\n    def validate_system_precision() -> Dict[str, Any]:\n        '''Validate that the system preserves precision correctly.'''\n        pass\n    @staticmethod\n    def store_price_data(data: Any) -> Any:\n        '''Store price data without modifying precision.'''\n        pass\n    @staticmethod\n    def retrieve_price_data(data: Any) -> Any:\n        '''Retrieve price data without modifying precision.'''\n        pass\n    @staticmethod\n    def preserve_calculation_precision(result: float, operation: str) -> float:\n        '''Preserve calculation precision.'''\n        pass",
    "snippet_id": "snippet_225"
  },
  {
    "class_name": "StrateQueue.utils.price_formatter.PriceFormatter",
    "skeleton": "\nclass PriceFormatter:\n    '''Utility class for formatting prices and quantities consistently.'''\n    @staticmethod\n    def format_price_for_display(price: Union[float, int, None]) -> str:\n        '''\n        Format price for user display (UI, console output).\n        Args:\n            price: Price value to format\n        Returns:\n            Formatted price string with currency symbol\n        '''\n        pass\n    @staticmethod\n    def format_price_for_logging(price: Union[float, int, None]) -> str:\n        '''\n        Format price for logging (more precision, with currency symbol).\n        Args:\n            price: Price value to format\n        Returns:\n            Formatted price string for logging\n        '''\n        pass\n    @staticmethod\n    def format_quantity(quantity: Union[float, int, None]) -> str:\n        '''\n        Format quantity for display.\n        Args:\n            quantity: Quantity value to format\n        Returns:\n            Formatted quantity string\n        '''\n        pass\n    @staticmethod\n    def format_percentage(percentage: Union[float, int, None]) -> str:\n        '''\n        Format percentage for display.\n        Args:\n            percentage: Percentage value to format (as decimal, e.g., 0.05 for 5%)\n        Returns:\n            Formatted percentage string\n        '''\n        pass\n    @staticmethod\n    def format_price_for_display(price: Union[float, int, None]) -> str:\n        '''\n        Format price without currency symbol, preserving precision.\n        Args:\n            price: Price value to format\n            force_precision: Optional forced decimal places\n        Returns:\n            Formatted price string without currency symbol\n        '''\n        pass\n    @staticmethod\n    def format_currency(price: Union[float, int, None], currency: str='USD') -> str:\n        '''\n        Format price with currency symbol.\n        Args:\n            price: Price value to format\n            currency: Currency code (USD, EUR, BTC, etc.)\n        Returns:\n            Formatted price string with currency symbol\n        '''\n        pass",
    "snippet_id": "snippet_226"
  },
  {
    "class_name": "AgentCrew.modules.chat.history.ConversationTurn",
    "skeleton": "class ConversationTurn:\n    '''Represents a single turn in the conversation.'''\n\n    def __init__(self, user_message, message_index):\n        '''\n        Initialize a conversation turn.\n        Args:\n            user_message: The user's message\n            assistant_response: The assistant's response\n            message_index: The index of the last message in this turn\n        '''\n        pass\n\n    def _extract_preview(self, message, max_length=50):\n        '''Extract a preview of the message for display in completions.'''\n        pass\n\n    def get_preview(self, max_length=50):\n        '''Get a preview of the user message for display in completions.'''\n        pass",
    "snippet_id": "snippet_227"
  },
  {
    "class_name": "AgentCrew.modules.clipboard.service.ClipboardService",
    "skeleton": "\nclass ClipboardService:\n    '''Service for interacting with the system clipboard.'''\n\n    def __init__(self):\n        '''Initialize the clipboard service.'''\n        pass\n\n    def write_text(self, content: str) -> Dict[str, Any]:\n        '''\n        Write text content to the clipboard.\n        Args:\n            content: Text content to write to clipboard\n        Returns:\n            Dict containing success status and any error information\n        '''\n        pass\n\n    def _create_temp_file_from_image(self, image: Image.Image) -> Optional[str]:\n        '''\n        Create a temporary file from a PIL Image.\n        Args:\n            image: PIL Image object\n        Returns:\n            Path to the temporary file or None if failed\n        '''\n        pass\n\n    def read(self) -> Dict[str, Any]:\n        '''\n        Read content from the clipboard and automatically determine the content type.\n        Returns:\n            Dict containing the clipboard content or error information\n        '''\n        pass\n\n    def read_and_process_paste(self) -> Dict[str, Any]:\n        '''\n        Read clipboard content and if it's an image or binary file, create a temporary file\n        and return a file command that can be processed.\n        Returns:\n            Dict containing either processed file command or regular text content\n        '''\n        pass\n\n    def cleanup_temp_files(self):\n        '''Clean up any temporary files created by this service.'''\n        pass\n\n    def write_text(self, content: str) -> Dict[str, Any]:\n        '''\n        Write content to the clipboard.\n        Args:\n            content: Content to write to clipboard\n        Returns:\n            Dict containing success status and any error information\n        '''\n        pass\n\n    def __del__(self):\n        '''Cleanup temporary files when the service is destroyed.'''\n        pass",
    "snippet_id": "snippet_228"
  },
  {
    "class_name": "AgentCrew.modules.llm.service_manager.ServiceManager",
    "skeleton": "\nclass ServiceManager:\n    '''Singleton manager for LLM service instances.'''\n    @classmethod\n    def get_instance(cls):\n        '''Get the singleton instance of ServiceManager.'''\n        pass\n\n    def __init__(self):\n        '''Initialize the service manager with empty service instances.'''\n        pass\n\n    def _load_custom_provider_configs(self):\n        '''Loads configurations for custom LLM providers.'''\n        pass\n\n    def initialize_standalone_service(self, provider: str) -> BaseLLMService:\n        '''\n        Initializes and returns a new service instance for the specified provider.\n        This does not cache the service instance in self.services.\n        '''\n        pass\n\n    def get_service(self, provider: str) -> BaseLLMService:\n        '''\n        Get or create a service instance for the specified provider.\n        Caches the instance for subsequent calls.\n        Args:\n            provider: The provider name\n        Returns:\n            An instance of the appropriate LLM service\n        '''\n        pass\n\n    def set_model(self, provider: str, model_id: str) -> bool:\n        '''\n        Set the model for a specific provider.\n        Args:\n            provider: The provider name\n            model_id: The model ID to use\n        Returns:\n            True if successful, False otherwise\n        '''\n        pass",
    "snippet_id": "snippet_229"
  },
  {
    "class_name": "AgentCrew.modules.web_search.service.TavilySearchService",
    "skeleton": "\nclass TavilySearchService:\n    '''Service for interacting with the Tavily Search API using the official SDK.'''\n\n    def __init__(self):\n        '''Initialize the Tavily search service with API key from environment.'''\n        pass\n\n    def search(self, query: str, search_depth: str='basic', topic: str='general', include_domains: List[str] | None=None, exclude_domains: List[str] | None=None, max_results: int=5) -> Dict[str, Any]:\n        '''\n        Perform a web search using Tavily API.\n        Args:\n            query: The search query\n            search_depth: 'basic' or 'advanced' search depth\n            include_domains: List of domains to include in search\n            exclude_domains: List of domains to exclude from search\n            max_results: Maximum number of results to return\n        Returns:\n            Dict containing search results\n        '''\n        pass\n\n    def extract(self, url: str) -> Dict[str, Any]:\n        '''\n        Extract content from a specific URL using Tavily API.\n        Args:\n            url: The URL to extract content from\n        Returns:\n            Dict containing the extracted content\n        '''\n        pass\n\n    def format_search_results(self, results: Dict[str, Any]) -> str:\n        '''Format search results into a readable string.'''\n        pass\n\n    def format_extract_results(self, results: Dict[str, Any]) -> str:\n        '''Format extract results into a readable string.'''\n        pass",
    "snippet_id": "snippet_230"
  },
  {
    "class_name": "pipeline.handlers.logics.evaluate_handler.model.DataClassificationResult",
    "skeleton": "\nclass DataClassificationResult:\n    '''\n    A class representing the result of data classification.\n    Attributes:\n        classification: The classification of the data.\n        accuracy: The accuracy of the classification.\n        execution_time: The execution time of the data classification.\n            '''\n\n    def __init__(self, classification: Optional[dict], accuracy: Optional[float], execution_time: Optional[float]):\n        '''\n        Initializes a new instance of the DataClassificationResult class.\n        Args:\n            classification: The classification of the data.\n            accuracy: The accuracy of the classification.\n            execution_time: The execution time of the data classification.\n                        '''\n                        pass\n\n    def to_json(self, indent: Optional[int]=4) -> str:\n        '''\n        Converts the DataClassificationResult object to a JSON string using a CustomEncoder for serializing objects with 'to_dict' and 'as_dict' methods.\n        Args:\n            indent: The number of spaces to indent the JSON string.\n        Returns:\n            str: The DataClassificationResult object as a JSON string.\n        '''\n        pass\n\n    def to_dict(self) -> dict:\n        '''\n        Converts the DataClassificationResult object to a dictionary.\n        Returns:\n            dict: The DataClassificationResult object as a dictionary.\n        '''\n        pass",
    "snippet_id": "snippet_231"
  },
  {
    "class_name": "utils.stopwatch.Stopwatch",
    "skeleton": "\nclass Stopwatch:\n    '''\n    A class representing a stopwatch for measuring elapsed time.\n    Attributes:\n        elapsed (float): The elapsed time in seconds.\n        is_running (bool): A flag indicating whether the stopwatch is running\n    '''\n\n    def __enter__(self):\n        '''\n        Enters a context block and starts the stopwatch.\n        Returns:\n            Stopwatch: The stopwatch instance.\n        '''\n        pass\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        '''\n        Exits the context block and stops the stopwatch.\n        '''\n        pass\n\n    def reset(self):\n        '''\n        Resets the stopwatch by setting the elapsed time to zero and stopping it\n        '''\n        pass\n\n    def start(self):\n        '''\n        Starts the stopwatch by setting the start time and setting the 'is_running' flag to True.\n        '''\n        pass\n\n    def _format_elapsed_time(self, elapsed_time: float) -> str:\n        '''\n        Formats the elapsed time in seconds to a string representation.\n        Args:\n            elapsed_time (float): The elapsed time in seconds.\n        Returns:\n            str: The formatted elapsed time string.\n        '''\n        pass\n\n    def stop(self):\n        '''\n        Stops the stopwatch by calculating the elapsed time and setting the 'is_running' flag to False.\n        '''\n        pass",
    "snippet_id": "snippet_232"
  },
  {
    "class_name": "exosphere.providers.factory.PkgManagerFactory",
    "skeleton": "\nclass PkgManagerFactory:\n    '''\n    Factory class for creating package manager instances.\n        '''\n    @staticmethod\n    def get_registry() -> dict[str, PkgManager]:\n        '''\n        Get the registry of available package manager implementations.\n        :return: Dictionary of package manager class names keyed by their names.\n        '''\n        pass\n    @staticmethod\n    def create(name: str) -> PkgManager:\n        '''\n        Create a package manager instance based on the provided name.\n        :param name: Name of the package manager (e.g., 'apt').\n        :return: An instance of the specified package manager.\n        '''\n        pass",
    "snippet_id": "snippet_233"
  },
  {
    "class_name": "build-ci.Build",
    "skeleton": "\nclass Build:\n    '''\n    Build class\n        '''\n\n    def __init__(self) -> None:\n        '''\n        Constructor\n        '''\n        pass\n\n    def _set_up_parser(self) -> ArgumentParser:\n        '''\n        Set up argument parser\n        :return: Argument parser\n        :rtype: argparse.ArgumentParser\n        '''\n        pass\n\n    def _run_command(self, cmd: str, method: Callable[[str], None]=None, **kwargs: Dict[str, Any]) -> int:\n        '''\n        Run a command\n        :param cmd: Command to run\n        :type cmd: str\n        :param method: Logger method\n        :type method: Callable[[str], None]\n        :param kwargs: Keyword arguments to pass to run_command\n        :type kwargs: Dict[str, Any]\n        :return: Command output\n        :rtype: str\n        '''\n        pass\n\n    def _set_up_venv(self) -> int:\n        '''\n        Set up a Python virtual environment\n        :return: Return code\n        :rtype: int\n        '''\n        pass\n\n    def _build(self) -> int:\n        '''\n        Build from a spec file\n        :return: Return code\n        :rtype: int\n        '''\n        pass\n\n    def _clean(self) -> int:\n        '''\n        Delete build directories\n        :return: Return code\n        :rtype: int\n        '''\n        pass\n\n    def main(self) -> int:\n        '''\n        Build\n        :return: Return code\n        :rtype: int\n        '''\n        pass",
    "snippet_id": "snippet_234"
  },
  {
    "class_name": "kodit.domain.value_objects.LanguageExtensions",
    "skeleton": "@dataclass(frozen=True)\nclass LanguageExtensions:\n    '''Value object for language to file extension mappings.'''\n    @classmethod\n    def get_supported_languages(cls) -> list[str]:\n        '''Get all supported programming languages.'''\n        pass\n    @classmethod\n    def get_extensions_for_language(cls, language: str) -> list[str]:\n        '''Get file extensions for a given language.'''\n        pass\n    @classmethod\n    def is_supported_language(cls, language: str) -> bool:\n        '''Check if a language is supported.'''\n        pass\n    @classmethod\n    def get_extensions_or_fallback(cls, language: str) -> list[str]:\n        '''Get extensions for language or return language as extension if not found.'''\n        pass",
    "snippet_id": "snippet_235"
  },
  {
    "class_name": "kodit.domain.value_objects.LanguageMapping",
    "skeleton": "\nclass LanguageMapping:\n    '''Value object for language-to-extension mappings.\n    This encapsulates the domain knowledge of programming languages and their\n    associated file extensions. It provides bidirectional mapping capabilities\n    and is designed to be immutable and reusable across the application.\n    '''\n    @classmethod\n    def get_extensions_for_language(cls, language: str) -> list[str]:\n        '''Get file extensions for a given language.\n        Args:\n            language: The programming language name (case-insensitive)\n        Returns:\n            List of file extensions (without leading dots) for the language\n        Raises:\n            ValueError: If the language is not supported\n        '''\n        pass\n    @classmethod\n    def get_language_for_extension(cls, extension: str) -> str:\n        '''Get language for a given file extension.\n        Args:\n            extension: The file extension (with or without leading dot)\n        Returns:\n            The programming language name\n        Raises:\n            ValueError: If the extension is not supported\n        '''\n        pass\n    @classmethod\n    def get_extension_to_language_map(cls) -> dict[str, str]:\n        '''Get a mapping from file extensions to language names.\n        Returns:\n            Dictionary mapping file extensions (without leading dots) to language names\n        '''\n        pass\n    @classmethod\n    def get_supported_languages(cls) -> list[str]:\n        '''Get list of all supported programming languages.\n        Returns:\n            List of supported language names\n        '''\n        pass\n    @classmethod\n    def get_supported_extensions(cls) -> list[str]:\n        '''Get list of all supported file extensions.\n        Returns:\n            List of supported file extensions (without leading dots)\n        '''\n        pass\n    @classmethod\n    def is_supported_language(cls, language: str) -> bool:\n        '''Check if a language is supported.\n        Args:\n            language: The programming language name (case-insensitive)\n        Returns:\n            True if the language is supported, False otherwise\n        '''\n        pass\n    @classmethod\n    def is_supported_extension(cls, extension: str) -> bool:\n        '''Check if a file extension is supported.\n        Args:\n            extension: The file extension (with or without leading dot)\n        Returns:\n            True if the extension is supported, False otherwise\n        '''\n        pass\n    @classmethod\n    def get_extensions_with_fallback(cls, language: str) -> list[str]:\n        '''Get file extensions for a language, falling back to passed language name.\n        Args:\n            language: The programming language name (case-insensitive)\n        Returns:\n            List of file extensions (without leading dots) for the language, or\n            [language.lower()] if not found.\n        '''\n        pass",
    "snippet_id": "snippet_236"
  },
  {
    "class_name": "kodit.domain.value_objects.MultiSearchResult",
    "skeleton": "@dataclass\nclass MultiSearchResult:\n    '''Enhanced search result with comprehensive snippet metadata.'''\n\n    def __str__(self) -> str:\n        '''Return enhanced formatted string representation.'''\n        pass\n\n    def to_json(self) -> str:\n        '''Return LLM-optimized JSON representation following the compact schema.'''\n        pass\n    @classmethod\n    def to_jsonlines(cls, results: list['MultiSearchResult']) -> str:\n        '''Convert multiple MultiSearchResult objects to JSON Lines format.\n        Args:\n            results: List of MultiSearchResult objects\n            include_summary: Whether to include summary fields\n        Returns:\n            JSON Lines string (one JSON object per line)\n        '''\n        pass\n    @classmethod\n    def to_string(cls, results: list['MultiSearchResult']) -> str:\n        '''Convert multiple MultiSearchResult objects to a string.'''\n        pass\n    @staticmethod\n    def calculate_relative_path(file_path: str, source_path: str) -> str:\n        '''Calculate relative path from source root.'''\n        pass\n    @staticmethod\n    def detect_language_from_extension(extension: str) -> str:\n        '''Detect programming language from file extension.'''\n        pass",
    "snippet_id": "snippet_237"
  },
  {
    "class_name": "kodit.infrastructure.ignore.ignore_pattern_provider.GitIgnorePatternProvider",
    "skeleton": "\nclass GitIgnorePatternProvider:\n    '''Ignore pattern provider for git repositories.'''\n\n    def __init__(self, base_dir: Path) -> None:\n        '''Initialize the ignore pattern provider.\n        Args:\n            base_dir: The base directory to check for ignore patterns.\n        Raises:\n            ValueError: If the base directory is not a directory.\n        '''\n        pass\n\n    def should_ignore(self, path: Path) -> bool:\n        '''Check if a path should be ignored.\n        Args:\n            path: The path to check.\n        Returns:\n            True if the path should be ignored, False otherwise.\n        '''\n        pass",
    "snippet_id": "snippet_238"
  },
  {
    "class_name": "http_client.n8n.model.tag.Tag",
    "skeleton": "@dataclass\nclass Tag:\n    '''\n    Represents a tag in the N8n system.\n    Attributes:\n        name: Tag name (required)\n        id\n        createdAt\n        updatedAt\n    '''\n\n    def to_dict(self):\n        '''Convert the tag to a dictionary for API requests'''\n        pass\n    @classmethod\n    def from_dict(cls, data):\n        '''Create a Tag instance from API response data'''\n        pass",
    "snippet_id": "snippet_239"
  },
  {
    "class_name": "tesseract_core.sdk.docker_client.Volume",
    "skeleton": "@dataclass\nclass Volume:\n    '''Volume class to wrap Docker volumes.'''\n    @classmethod\n    def from_dict(cls, json_dict: dict) -> 'Volume':\n        '''Create an Image object from a json dictionary.\n        Params:\n            json_dict: The json dictionary to create the object from.\n        Returns:\n            The created volume object.\n        '''\n        pass\n\n    def remove(self, force: bool=False) -> None:\n        '''Remove a Docker volume.\n        Params:\n            force: If True, force the removal of the volume.\n        '''\n        pass",
    "snippet_id": "snippet_240"
  },
  {
    "class_name": "src.open_responses_server.mcp-chatbot-client.Configuration",
    "skeleton": "\nclass Configuration:\n    '''Manages configuration and environment variables for the MCP client.'''\n\n    def __init__(self) -> None:\n        '''Initialize configuration with environment variables.'''\n        pass\n    @staticmethod\n    def load_env() -> None:\n        '''Load environment variables from .env file.'''\n        pass\n    @staticmethod\n    def load_config(file_path: str) -> dict[str, Any]:\n        '''Load server configuration from JSON file.\n        Args:\n            file_path: Path to the JSON configuration file.\n        Returns:\n            Dict containing server configuration.\n        Raises:\n            FileNotFoundError: If configuration file doesn't exist.\n            JSONDecodeError: If configuration file is invalid JSON.\n        '''\n        pass\n    @property\n    def llm_api_key(self) -> str:\n        '''Get the LLM API key.\n        Returns:\n            The API key as a string.\n        Raises:\n            ValueError: If the API key is not found in environment variables.\n        '''\n        pass",
    "snippet_id": "snippet_241"
  },
  {
    "class_name": "flock.core.logging.logging.ImmediateFlushSink",
    "skeleton": "\nclass ImmediateFlushSink:\n    '''A custom Loguru sink that writes to a stream and flushes immediately after each message.\n    This ensures that logs appear in real time.\n    '''\n\n    def __init__(self, stream=None):\n        '''Initialize the ImmediateFlushSink.\n        Args:\n            stream (Stream, optional): The stream to write to. Defaults to sys.stderr.\n        '''\n        pass\n\n    def write(self, message):\n        '''Write a message to the stream and flush immediately.\n        Args:\n            message (str): The message to write.\n        '''\n        pass\n\n    def flush(self):\n        '''Flush the stream.'''\n        pass",
    "snippet_id": "snippet_242"
  },
  {
    "class_name": "flock.core.logging.logging.PrintAndFlushSink",
    "skeleton": "class PrintAndFlushSink:\n    '''A Loguru sink.\n    forcibly prints each log record and flushes immediately,\n    mimicking print(..., flush=True).\n    '''\n\n    def write(self, message: str):\n        '''Write a message to the stream and flush immediately.\n        Args:\n            message (str): The message to write.\n        '''\n        pass\n\n    def flush(self):\n        '''Flush the stream.\n        Already flushed on every write call.\n        '''\n        pass",
    "snippet_id": "snippet_243"
  },
  {
    "class_name": "flock.core.logging.telemetry.TelemetryConfig",
    "skeleton": "\nclass TelemetryConfig:\n    '''This configuration class sets up OpenTelemetry tracing.\n      - Export spans to a Jaeger collector using gRPC.\n      - Write spans to a file.\n      - Save spans in a SQLite database.\n    Only exporters with a non-None configuration will be activated.\n    '''\n\n    def __init__(self, service_name: str, jaeger_endpoint: str | None=None, jaeger_transport: str='grpc', local_logging_dir: str | None=None, file_export_name: str | None=None, sqlite_db_name: str | None=None, enable_jaeger: bool=True, enable_file: bool=True, enable_sql: bool=True, enable_otlp: bool=True, otlp_protocol: str='grpc', otlp_endpoint: str='http://localhost:4317', batch_processor_options: dict | None=None):\n        ''':param service_name: Name of your service.\n        :param jaeger_endpoint: The Jaeger collector gRPC endpoint (e.g., \"localhost:14250\").\n        :param file_export_path: If provided, spans will be written to this file.\n        :param sqlite_db_path: If provided, spans will be stored in this SQLite DB.\n        :param batch_processor_options: Dict of options for BatchSpanProcessor (e.g., {\"max_export_batch_size\": 10}).\n        '''\n        pass\n\n    def setup_tracing(self):\n        '''Set up OpenTelemetry tracing with the specified exporters.'''\n        pass\n\n    def log_exception_to_otel(self, exc_type, exc_value, exc_traceback):\n        '''Log unhandled exceptions to OpenTelemetry.'''\n        pass",
    "snippet_id": "snippet_244"
  },
  {
    "class_name": "flock.core.serialization.callable_registry.CallableRegistry",
    "skeleton": "\nclass CallableRegistry:\n    '''Registry for callable objects.\n    This class serves as a central registry for callable objects (functions, methods)\n    that can be referenced by name in serialized formats.\n    This is a placeholder implementation that will be fully implemented in task US007-T004.\n    '''\n    @classmethod\n    def register(cls, name: str, callable_obj: Callable) -> None:\n        '''Register a callable object with the given name.\n        Args:\n            name: Unique name for the callable\n            callable_obj: Function or method to register\n        '''\n        pass\n    @classmethod\n    def get(cls, name: str) -> Callable:\n        '''Get a callable object by name.\n        Args:\n            name: Name of the callable to retrieve\n        Returns:\n            The registered callable\n        Raises:\n            KeyError: If no callable with the given name is registered\n        '''\n        pass\n    @classmethod\n    def contains(cls, name: str) -> bool:\n        '''Check if a callable with the given name is registered.\n        Args:\n            name: Name to check\n        Returns:\n            True if registered, False otherwise\n        '''\n        pass",
    "snippet_id": "snippet_245"
  },
  {
    "class_name": "flock.core.serialization.secure_serializer.SecureSerializer",
    "skeleton": "\nclass SecureSerializer:\n    '''Security-focused serialization system with capability controls for Flock objects.'''\n    @staticmethod\n    def _get_module_capability(module_name):\n        '''Get the capability level for a module.'''\n        pass\n    @staticmethod\n    def _is_safe_callable(obj):\n        '''Check if a callable is safe to serialize.'''\n        pass\n    @staticmethod\n    def serialize(obj, allow_restricted=True, allow_high_risk=False):\n        '''Serialize an object with capability checks.'''\n        pass\n    @staticmethod\n    def deserialize(obj, allow_restricted=True, allow_high_risk=False):\n        '''Deserialize an object with capability enforcement.'''\n        pass",
    "snippet_id": "snippet_246"
  },
  {
    "class_name": "flock.modules.memory.memory_parser.MemoryMappingParser",
    "skeleton": "\nclass MemoryMappingParser:\n    '''Parses memory mapping declarations into executable operations.'''\n\n    def parse(self, mapping: str) -> list[MemoryOperation]:\n        '''Parse a memory mapping string into operations.\n        Example mappings:\n        \"topic -> memory.semantic(threshold=0.9) | memory.exact -> output\"\n        \"query -> memory.semantic(scope='global') | memory.filter(recency='7d') | memory.sort(by='relevance')\"\n        '''\n        pass\n\n    def _parse_params(self, params_str: str) -> dict[str, Any]:\n        '''Parse parameters string into a dictionary.\n        Handles:\n        - Quoted strings: threshold='high'\n        - Numbers: threshold=0.9\n        - Lists: tools=['web_search', 'extract_numbers']\n        - Dictionaries: weights={'semantic': 0.7, 'exact': 0.3}\n        '''\n        pass",
    "snippet_id": "snippet_247"
  },
  {
    "class_name": "prompt_parser.PromptParserMixin",
    "skeleton": "\nclass PromptParserMixin:\n    '''A mixin class for parsing agent prompts and building clean signatures for DSPy.'''\n\n    def _parse_key_descriptions(self, keys_str: str) -> list[tuple[str, str]]:\n        '''Parse a comma-separated string into a list of (key, description) tuples.\n        This function processes a configuration string that defines one or more keys, where each key may\n        include a type hint and an optional human-readable description. The expected format for each key is:\n            key: type_hint | description\n        If the pipe symbol (\"|\") is absent, the description is set to an empty string.\n        The splitting is performed using split_top_level() so that commas inside type hints are preserved.\n        For example, given:\n            \"query: str | The search query, context: dict | The full conversation context\"\n        it returns:\n            [(\"query\", \"The search query\"), (\"context\", \"The full conversation context\")]\n        Args:\n            keys_str (str): A comma-separated string of key definitions.\n        Returns:\n            List[Tuple[str, str]]: A list of (key, description) tuples.\n        '''\n        pass\n\n    def _build_clean_signature(self, keys_str: str) -> str:\n        '''Build a clean signature string from the configuration string by removing the description parts.\n        Given a string like:\n            \"query: str | The search query, context: dict | The full conversation context\"\n        this method returns:\n            \"query: str, context: dict\"\n        This function uses split_top_level() to avoid splitting on commas that are inside type hints.\n        Args:\n            keys_str (str): The configuration string containing keys, type hints, and optional descriptions.\n        Returns:\n            str: A clean signature string with only keys and type hints.\n        '''\n        pass\n\n    def _build_descriptions(self) -> tuple[dict[str, str], dict[str, str]]:\n        '''Build dictionaries of input and output descriptions from the agent's configuration.\n        Returns:\n            A tuple containing:\n            - input_desc: A dictionary mapping each input key (without type hints) to its description.\n            - output_desc: A dictionary mapping each output key (without type hints) to its description.\n        '''\n        pass\n\n    def _build_prompt(self, input_desc: dict[str, str], output_desc: dict[str, str]) -> str:\n        '''Build a clean signature prompt from the agent's configuration.\n        This method uses the original input and output strings (removing the description parts)\n        to create a signature string that is passed to DSPy. For example, if:\n        - self.input is \"query: str | The search query, context: dict | The full conversation context\"\n        - self.output is \"result: str | The result\"\n        then the prompt will be:\n        \"query: str, context: dict -> result: str\"\n        **Note:** The descriptive metadata is preserved in the dictionaries obtained from _build_descriptions,\n        which are passed separately to DSPy.\n        Args:\n            input_desc: Dictionary of input key descriptions (for metadata only).\n            output_desc: Dictionary of output key descriptions (for metadata only).\n        Returns:\n            A clean signature string for DSPy.\n        '''\n        pass",
    "snippet_id": "snippet_248"
  },
  {
    "class_name": "chat_service.Message",
    "skeleton": "\nclass Message:\n    '''消息类'''\n\n    def __init__(self, role: str, content: str):\n        '''\n        初始化消息\n        Args:\n            role: 消息角色（\"system\", \"user\", \"assistant\"）\n            content: 消息内容\n        '''\n        pass\n\n    def to_dict(self) -> Dict[str, str]:\n        '''转换为字典格式'''\n        pass\n    @classmethod\n    def from_dict(cls, data: Dict[str, str]) -> 'Message':\n        '''从字典创建消息'''\n        pass",
    "snippet_id": "snippet_249"
  },
  {
    "class_name": "image_service.ImageConfig",
    "skeleton": "\nclass ImageConfig:\n    '''图像配置类'''\n\n    def __init__(self, prompt: str=None, image_size: str=None, batch_size: int=None, num_inference_steps: int=None, guidance_scale: float=None, negative_prompt: str=None, seed: int=None):\n        '''\n        初始化图像配置\n        Args:\n            prompt: 图像提示词\n            image_size: 图像尺寸\n            batch_size: 生成数量\n            num_inference_steps: 推理步数\n            guidance_scale: 引导比例\n            negative_prompt: 负面提示词\n            seed: 随机种子\n        '''\n        pass\n\n    def to_dict(self) -> Dict[str, Any]:\n        '''转换为字典格式'''\n        pass",
    "snippet_id": "snippet_250"
  },
  {
    "class_name": "option_service.OptionService",
    "skeleton": "\nclass OptionService:\n    '''选项生成服务类'''\n\n    def __init__(self):\n        '''初始化选项生成服务'''\n        pass\n\n    def generate_options(self, conversation_history: List[Dict[str, str]], character_config: Dict[str, Any], user_query: str) -> List[str]:\n        '''\n        生成对话选项\n        Args:\n            conversation_history: 对话历史记录\n            character_config: 角色配置\n            user_query: 用户最后的查询\n        Returns:\n            生成的选项列表（最多3个）\n        Raises:\n            APIError: 当API调用失败时\n        '''\n        pass\n\n    def _build_user_prompt(self, conversation_history: List[Dict[str, str]], character_config: Dict[str, Any], user_query: str) -> str:\n        '''\n        构建用户提示词\n        Args:\n            conversation_history: 对话历史记录\n            character_config: 角色配置\n            user_query: 用户最后的查询\n        Returns:\n            构建的用户提示词\n        '''\n        pass",
    "snippet_id": "snippet_251"
  },
  {
    "class_name": "utils.plugin.BasePlugin",
    "skeleton": "\nclass BasePlugin:\n    '''\n    插件基类。插件需继承此类，并实现 register_frontend, register_backend 方法。\n    '''\n\n    def register_frontend(self, register_func: Callable[[str, str], None]):\n        '''\n        注册前端内容。register_func(路由, 文件路径)\n        '''\n        pass\n\n    def register_backend(self, app):\n        '''\n        注册后端路由。app为后端框架实例（如Flask/FastAPI等）\n        '''\n        pass",
    "snippet_id": "snippet_252"
  },
  {
    "class_name": "utils.prompt_logger.PromptLogger",
    "skeleton": "\nclass PromptLogger:\n    '''提示词日志记录器'''\n\n    def __init__(self, log_file: str='log.txt'):\n        '''\n        初始化日志记录器\n        参数:\n            log_file: 日志文件路径\n        '''\n        pass\n\n    def log_prompt(self, messages: List[Dict[str, str]], character_name: str=None, user_query: str=None):\n        '''\n        记录完整的提示词到日志文件\n        参数:\n            messages: 发送给模型的消息列表\n            character_name: 角色名称\n            user_query: 用户查询（原始请求）\n        '''\n        pass\n\n    def log_formatted_prompt(self, system_prompt: str, user_prompt: str, memory_context: str='', character_name: str=None, user_query: str=None):\n        '''\n        记录格式化的提示词（分别记录system和user部分）\n        参数:\n            system_prompt: 系统提示词\n            user_prompt: 用户提示词\n            memory_context: 记忆上下文\n            character_name: 角色名称\n            user_query: 原始用户查询（未经任何加工的用户输入）\n        '''\n        pass\n\n    def get_recent_logs(self, count: int=10) -> List[Dict]:\n        '''\n        获取最近的日志条目\n        参数:\n            count: 返回的条目数量\n        返回:\n            最近的日志条目列表\n        '''\n        pass\n\n    def clear_logs(self):\n        '''清空日志文件'''\n        pass",
    "snippet_id": "snippet_253"
  },
  {
    "class_name": "utils.time_utils.TimeTracker",
    "skeleton": "\nclass TimeTracker:\n    '''时间跟踪器'''\n\n    def __init__(self, history_dir: str):\n        '''\n        初始化时间跟踪器\n        Args:\n            history_dir: 历史记录存储目录\n        '''\n        pass\n\n    def _get_character_history_file(self, character_id: str) -> str:\n        '''\n        获取角色历史记录文件路径\n        Args:\n            character_id: 角色ID\n        Returns:\n            历史记录文件路径\n        '''\n        pass\n\n    def get_last_message_time(self, character_id: str) -> Optional[datetime]:\n        '''\n        获取角色最后一条消息的时间\n        Args:\n            character_id: 角色ID\n        Returns:\n            最后一条消息的时间，如果没有历史记录则返回None\n        '''\n        pass\n\n    def format_time_elapsed(self, last_time: Optional[datetime], current_time: datetime) -> str:\n        '''\n        格式化时间间隔\n        Args:\n            last_time: 上次时间\n            current_time: 当前时间\n        Returns:\n            格式化的时间间隔字符串\n        '''\n        pass\n\n    def get_time_elapsed_prefix(self, character_id: str) -> str:\n        '''\n        获取时间间隔前缀\n        Args:\n            character_id: 角色ID\n        Returns:\n            时间间隔前缀，如\"距上次对话xx\"\n        '''\n        pass",
    "snippet_id": "snippet_254"
  },
  {
    "class_name": "wazuh_mcp_server.prompt_enhancement.cache.CacheKeyBuilder",
    "skeleton": "\nclass CacheKeyBuilder:\n    '''Helper class to build standardized cache keys.'''\n    @staticmethod\n    def alerts_key(agent_id: Optional[str]=None, time_range: str='24h', level: Optional[int]=None) -> Dict[str, Any]:\n        '''Build cache key for alerts.'''\n        pass\n    @staticmethod\n    def agent_health_key(agent_id: str) -> Dict[str, Any]:\n        '''Build cache key for agent health.'''\n        pass\n    @staticmethod\n    def vulnerabilities_key(agent_id: Optional[str]=None, severity: Optional[str]=None) -> Dict[str, Any]:\n        '''Build cache key for vulnerabilities.'''\n        pass\n    @staticmethod\n    def processes_key(agent_id: str, include_children: bool=True) -> Dict[str, Any]:\n        '''Build cache key for processes.'''\n        pass\n    @staticmethod\n    def ports_key(agent_id: str, state: list[str]=None, protocol: list[str]=None) -> Dict[str, Any]:\n        '''Build cache key for ports.'''\n        pass",
    "snippet_id": "snippet_255"
  },
  {
    "class_name": "wazuh_mcp_server.prompt_enhancement.context_aggregator.PromptPatternMatcher",
    "skeleton": "\nclass PromptPatternMatcher:\n    '''Analyzes prompts to determine context requirements.'''\n\n    def __init__(self):\n        '''Initialize pattern matchers.'''\n        pass\n\n    def analyze_prompt(self, prompt: str, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:\n        '''\n        Analyze prompt to determine context requirements.\n        Returns:\n            Dictionary with detected patterns, entities, and confidence scores\n        '''\n        pass",
    "snippet_id": "snippet_256"
  },
  {
    "class_name": "jrdev.ui.tui.terminal.terminal_text_styles.TerminalTextStyles",
    "skeleton": "\nclass TerminalTextStyles:\n    '''Manages loading, saving, and applying terminal text styles.'''\n\n    def __init__(self, stylesheet_path: str=None):\n        '''\n        Initializes the style manager.\n        Args:\n            stylesheet_path: Optional path to the stylesheet. Defaults to\n                             a file in the JRDEV_DIR.\n        '''\n        pass\n\n    def _get_default_styles(self) -> Dict[str, str]:\n        '''Returns the default styles for each PrintType as a dictionary.'''\n        pass\n\n    def load_styles(self) -> None:\n        '''Loads styles from the stylesheet file, merging them with defaults.'''\n        pass\n\n    def save_styles(self) -> bool:\n        '''Saves the current styles to the stylesheet file.'''\n        pass\n\n    def get_style(self, print_type: PrintType) -> str:\n        '''Gets the style string for a given PrintType.'''\n        pass\n\n    def set_style(self, print_type: PrintType, style_str: str) -> None:\n        '''Sets the style for a given PrintType.'''\n        pass",
    "snippet_id": "snippet_257"
  },
  {
    "class_name": "sparkdq.engine.batch.check_runner.BatchCheckRunner",
    "skeleton": "\nclass BatchCheckRunner:\n    '''\n    Executes row-level and aggregate-level data quality checks on a Spark DataFrame.\n    This class is designed for use in batch validation engines. It handles both the\n    transformation of the input DataFrame with row-level error annotations, and the\n    evaluation of aggregate-level checks that do not modify the DataFrame.\n    Attributes:\n        fail_levels (List[Severity]): The severities that are considered critical\n            for determining whether a row should be marked as failed.\n    '''\n\n    def __init__(self, fail_levels: List[Severity]):\n        '''\n        Initializes the runner with severity levels that count as failures.\n        Args:\n            fail_levels (List[Severity]): List of severities that should be treated\n                as failures when computing the _dq_passed column.\n        '''\n        pass\n\n    def run(self, df: DataFrame, checks: List[BaseCheck], reference_datasets: Optional[ReferenceDatasetDict]=None) -> Tuple[DataFrame, List[AggregateCheckResult]]:\n        '''\n        Runs all provided checks against the input DataFrame.\n        Args:\n            df (DataFrame): The Spark DataFrame to validate.\n            checks (List[BaseCheck]): A list of BaseRowCheck or BaseAggregateCheck instances.\n            reference_datasets (ReferenceDatasetDict, optional):\n                A dictionary of named reference DataFrames used by integrity checks.\n                Required for checks that compare values against external datasets\n                (e.g., foreign key validation). Each key should match the\n                `reference_dataset` name expected by the check.\n        Returns:\n            Tuple[DataFrame, List[AggregateCheckResult]]: The annotated DataFrame and list of results.\n        '''\n        pass\n\n    def _run_row_checks(self, df: DataFrame, row_checks: List[BaseRowCheck]) -> Tuple[DataFrame, List[Column], List[Column]]:\n        '''\n        Applies all row-level checks and collects failure information.\n        Args:\n            df (DataFrame): The DataFrame to validate.\n            row_checks (List[BaseRowCheck]): The row-level checks to apply.\n        Returns:\n            Tuple containing:\n                - The transformed DataFrame with error indicator columns.\n                - A list of error struct expressions (used to build _dq_errors).\n                - A list of boolean expressions indicating critical check failures.\n        '''\n        pass\n\n    def _run_aggregate_checks(self, df: DataFrame, agg_checks: List[BaseAggregateCheck]) -> List[AggregateCheckResult]:\n        '''\n        Evaluates all aggregate-level checks on the given DataFrame.\n        Args:\n            df (DataFrame): The DataFrame to evaluate.\n            agg_checks (List[BaseAggregateCheck]): The aggregate-level checks.\n        Returns:\n            List[AggregateCheckResult]: The result of each aggregate check.\n        '''\n        pass\n\n    def _combine_failure_flags(self, df: DataFrame, fail_flags: List[Column]) -> DataFrame:\n        '''\n        Combines all failure flags into a single _dq_passed column.\n        If no failure flags exist, all rows are marked as passed.\n        Args:\n            df (DataFrame): The input DataFrame.\n            fail_flags (List[Column]): Boolean expressions representing failed checks.\n        Returns:\n            DataFrame: The DataFrame with the _dq_passed column added.\n        '''\n        pass\n\n    def _attach_aggregate_errors(self, df: DataFrame, failed_aggregates: List[AggregateCheckResult]) -> DataFrame:\n        '''\n        Appends aggregate check failures to the _dq_errors column.\n        Also adds an optional _dq_aggregate_errors column for visibility.\n        Args:\n            df (DataFrame): The DataFrame being validated.\n            failed_aggregates (List[AggregateCheckResult]): Failed checks.\n        Returns:\n            DataFrame: The DataFrame with aggregate errors included.\n        '''\n        pass",
    "snippet_id": "snippet_258"
  },
  {
    "class_name": "sparkdq.plugin.check_config_registry.CheckConfigRegistry",
    "skeleton": "\nclass CheckConfigRegistry:\n    '''\n    Registry for check configuration classes.\n    Maps unique check names (e.g., 'null-check') to their corresponding configuration classes.\n    Used to resolve configuration classes dynamically during check instantiation.\n        '''\n    @classmethod\n    def register(cls, name: str, config_cls: Type[BaseCheckConfig]) -> None:\n        '''\n        Registers a configuration class under a given name.\n        Args:\n            name (str): Unique name for the check configuration (e.g., 'null-check').\n            config_cls (Type[BaseCheckConfig]): The configuration class to register.\n        Raises:\n            ValueError: If the given name is already registered.\n        '''\n        pass\n    @classmethod\n    def get(cls, name: str) -> Type[BaseCheckConfig]:\n        '''\n        Retrieves a registered configuration class by its check name.\n        Args:\n            name (str): The name of the check configuration.\n        Returns:\n            Type[BaseCheckConfig]: The corresponding configuration class.\n        Raises:\n            KeyError: If no configuration class is registered under the given name.\n        '''\n        pass\n    @classmethod\n    def list_registered(cls) -> Dict[str, Type[BaseCheckConfig]]:\n        '''\n        Returns all registered check configurations.\n        Returns:\n            Dict[str, Type[BaseCheckConfig]]: Mapping of check names to configuration classes.\n        '''\n        pass",
    "snippet_id": "snippet_259"
  },
  {
    "class_name": "sparkdq.plugin.check_factory.CheckFactory",
    "skeleton": "\nclass CheckFactory:\n    '''\n    Factory for dynamic check instantiation from configuration dictionaries.\n    Orchestrates the complete process of transforming raw configuration data into\n    executable check instances, including type resolution, parameter validation,\n    and proper instantiation. The factory leverages the CheckConfigRegistry to\n    maintain loose coupling between check implementations and their configurations.\n    This design enables flexible deployment scenarios where validation rules can\n    be externally defined and dynamically loaded from various sources including\n    configuration files, databases, or API endpoints.\n    '''\n    @staticmethod\n    def _from_dict(config_data: Dict[str, Any]) -> BaseCheck:\n        '''\n        Instantiate a check from a raw configuration dictionary.\n        Processes a configuration dictionary through the complete validation and\n        instantiation pipeline, including check type resolution, parameter validation,\n        and severity normalization. This method serves as the core transformation\n        point between external configuration sources and executable check instances.\n        Args:\n            config_data (Dict[str, Any]): Raw configuration dictionary containing\n                check type specification and all required parameters.\n        Returns:\n            BaseCheck: Fully validated and configured check instance ready for execution.\n        Raises:\n            MissingCheckTypeError: When the configuration lacks the required\n                'check' field for type identification.\n            ValidationError: When the configuration parameters fail validation\n                against the resolved check configuration schema.\n        '''\n        pass\n    @staticmethod\n    def from_list(config_list: List[Dict[str, Any]]) -> List[BaseCheck]:\n        '''\n        Instantiate multiple checks from a collection of configuration dictionaries.\n        Processes a collection of configuration dictionaries through the complete\n        validation and instantiation pipeline, enabling efficient bulk loading\n        from external configuration sources. This method ensures all check\n        implementations are properly registered before processing begins.\n        Args:\n            config_list (List[Dict[str, Any]]): Collection of configuration\n                dictionaries, each defining a complete check specification.\n        Returns:\n            List[BaseCheck]: Collection of fully validated and configured check\n                instances ready for execution.\n        '''\n        pass",
    "snippet_id": "snippet_260"
  },
  {
    "class_name": "modules.networking.cache.Cache",
    "skeleton": "\nclass Cache:\n    '''\n    Responsible for storing and retrieving `requests.Response` object.\"\n    Methods:\n        get(key: str) -> Response:\n            Retreive a Response object from the cache.\n        set(key: str, value: Response, strict: bool = False) -> None:\n            Store a Response object in the cache.\n        remove(key: str, strict: bool = False) -> None:\n            Remove a Response object from the cache.\n        includes(key: str) -> bool:\n            Check if a given key is present in the cache.\n    '''\n    @classmethod\n    def get(cls, key: str) -> Response:\n        '''\n        Retreive a Response object from the cache.\n        Parameters:\n            key (str): The cache key associated with a Response object.\n        Returns:\n            Response: The Response object associated with the given cache key.\n        Raises:\n            KeyError: If the given key is not present in the cache.\n        '''\n        pass\n    @classmethod\n    def set(cls, key: str, value: Response, strict: bool=False) -> None:\n        '''\n        Store a Response object in the cache.\n        Parameters:\n          key (str): The cache key associated with a Response object.\n          value (Response): The Response object to store in the cache.\n          strict (bool, optional): Raise an error if the key already exists in the cache. Default is False.\n        Raises:\n          KeyError: If the given key already exists in the cache and strict=True.\n        '''\n        pass\n    @classmethod\n    def remove(cls, key: str, strict: bool=False) -> None:\n        '''\n        Remove a Response object from the cache.\n        Parameters:\n          key (str): The cache key associated with a Response object.\n          strict (bool, optional): Raise an error if the key is not present in the cache. Default is False.\n        Raises:\n          KeyError: If the given key is not present in the cache and strict=True.\n        '''\n        pass\n    @classmethod\n    def includes(cls, key: str) -> bool:\n        '''\n        Check if a given key is present in the cache.\n        Parameters:\n          key (str): The cache key associated with a Response object.\n        Returns:\n            bool: True if the given key is present in the cache, otherwise False.\n        '''\n        pass",
    "snippet_id": "snippet_261"
  },
  {
    "class_name": "modules.utils.messages.STMessages",
    "skeleton": "\nclass STMessages:\n    '''A class to handle Streamlit messages.'''\n\n    def success(self, message: str='Operation completed successfully.'):\n        '''Display a success message.'''\n        pass\n\n    def warning(self, message: str='Holy! the dev forgot to write this warning messsage lol 💀.'):\n        '''Display a warning message.'''\n        pass\n\n    def error(self, message: str='An error occurred.'):\n        '''Display an error message.'''\n        pass\n\n    def skull(self, message: str='💀'):\n        '''Display a skull message.'''\n        pass",
    "snippet_id": "snippet_262"
  },
  {
    "class_name": "falcon_mcp.client.FalconClient",
    "skeleton": "\nclass FalconClient:\n    '''Client for interacting with the CrowdStrike Falcon API.'''\n\n    def __init__(self, base_url: Optional[str]=None, debug: bool=False, user_agent_comment: Optional[str]=None):\n        '''Initialize the Falcon client.\n        Args:\n            base_url: Falcon API base URL (defaults to FALCON_BASE_URL env var)\n            debug: Enable debug logging\n            user_agent_comment: Additional information to include in the User-Agent comment section\n        '''\n        pass\n\n    def authenticate(self) -> bool:\n        '''Authenticate with the Falcon API.\n        Returns:\n            bool: True if authentication was successful\n        '''\n        pass\n\n    def is_authenticated(self) -> bool:\n        '''Check if the client is authenticated.\n        Returns:\n            bool: True if the client is authenticated\n        '''\n        pass\n\n    def command(self, operation: str, **kwargs) -> Dict[str, Any]:\n        '''Execute a Falcon API command.\n        Args:\n            operation: The API operation to execute\n            **kwargs: Additional arguments to pass to the API\n        Returns:\n            Dict[str, Any]: The API response\n        '''\n        pass\n\n    def get_user_agent(self) -> str:\n        '''Get RFC-compliant user agent string for API requests.\n        Returns:\n            str: User agent string in RFC format \"falcon-mcp/VERSION (comment; falconpy/VERSION; Python/VERSION; Platform/VERSION)\"\n        '''\n        pass\n\n    def get_headers(self) -> Dict[str, str]:\n        '''Get authentication headers for API requests.\n        This method returns the authentication headers from the underlying Falcon API client,\n        which can be used for custom HTTP requests or advanced integration scenarios.\n        Returns:\n            Dict[str, str]: Authentication headers including the bearer token\n        '''\n        pass",
    "snippet_id": "snippet_263"
  },
  {
    "class_name": "python.users.User",
    "skeleton": "class User:\n    '''\n    Represents a user and their roles.\n    '''\n\n    def __init__(self, id: str, name: str, roles: list[str]=None) -> None:\n        '''\n        Initializes a User instance with a unique ID, name, and roles.\n        Args:\n            id (str): The user's unique ID.\n            name (str): The user's name.\n            roles (list, optional): The user's roles. Defaults to empty list.\n        '''\n        pass\n\n    def __repr__(self) -> str:\n        '''\n        Return a string representation of the User.\n        '''\n        pass",
    "snippet_id": "snippet_264"
  },
  {
    "class_name": "voicehub.models.chatterbox.inference.ChatterboxInference",
    "skeleton": "\nclass ChatterboxInference:\n    '''A class for running text-to-speech inference with the Chatterbox model.'''\n\n    def __init__(self):\n        '''Initializes the ChatterboxInference class.'''\n        pass\n\n    def load_model(self, device='cuda'):\n        '''\n        Loads the ChatterboxTTS model onto the specified device.\n        Args:\n            device (str): The device to run the model on, e.g., \"cuda\" or \"cpu\".\n        '''\n        pass\n\n    def __call__(self, text, output_path, audio_prompt_path=None):\n        '''\n        Synthesizes speech from text and saves it to a file.\n        Args:\n            text (str): The text to synthesize.\n            output_path (str): The path to save the generated audio file.\n            audio_prompt_path (str, optional): Path to an audio file for voice prompt. Defaults to None.\n        '''\n        pass",
    "snippet_id": "snippet_265"
  },
  {
    "class_name": "voicehub.models.dia.inference.DiaTTS",
    "skeleton": "\nclass DiaTTS:\n    '''\n    DiaTTS class for text-to-speech generation using the Dia model.\n    This class provides a simple interface for loading and using the Dia model\n    to generate speech from text prompts.\n    Example:\n        ```python\n        # Initialize the DiaTTS model\n        tts = DiaTTS(model_path=\"nari-labs/Dia-1.6B\", device=\"cuda\")\n        # Generate speech from text\n        audio = tts(prompt=\"Hello, how are you?\", output_file=\"output.wav\")\n        ```\n    '''\n\n    def __init__(self, model_path: str='nari-labs/Dia-1.6B', device: str='cuda', compute_dtype: str='bfloat16', use_torch_compile=False):\n        '''\n        Initialize the DiaTTS model.\n        Args:\n            model_path (str): Path or name of the pretrained model to load.\n                Default is \"nari-labs/Dia-1.6B\".\n            device (str): Device to run the model on (e.g., \"cuda\", \"cpu\").\n                Default is \"cuda\".\n            compute_dtype (str): Data type for computation (e.g., \"bfloat16\", \"float32\").\n                Default is \"bfloat16\".\n            use_torch_compile (bool): Whether to use torch.compile for potential speedup.\n                Default is False.\n        '''\n        pass\n\n    def _load_models(self, model_path: str):\n        '''\n        Load the Dia model from the specified path.\n        Args:\n            model_path (str): Path or name of the pretrained model to load.\n        '''\n        pass\n\n    def __call__(self, prompt: str='Hello, how are you?', output_file: str='output.wav'):\n        '''\n        Generate speech from text and save it to a file.\n        Args:\n            prompt (str): Text to convert to speech.\n                Default is \"Hello, how are you?\".\n            output_file (str): Path to save the generated audio.\n                Default is \"output.wav\".\n        Returns:\n            Audio: The generated audio data.\n        '''\n        pass",
    "snippet_id": "snippet_266"
  },
  {
    "class_name": "voicehub.models.kokoro.inference.KokoroTTS",
    "skeleton": "\nclass KokoroTTS:\n    '''\n    KokoroTTS class for text-to-speech generation using the Kokoro model.\n    This class provides a simple interface for loading and using the Kokoro model\n    to generate speech from text prompts.\n    Example:\n        ```python\n        # Initialize the KokoroTTS model\n        # 🇺🇸 'a' => American English, 🇬🇧 'b' => British English, 🇪🇸 'e' => Spanish, etc.\n        tts = KokoroTTS(lang_code=\"a\")\n        # Generate speech from text\n        text = \"The sky above the port was the color of television, tuned to a dead channel.\"\n        audios = tts(text=text, voice=\"af_heart\", output_prefix=\"output\")\n        # To listen in a notebook:\n        # from IPython.display import Audio, display\n        # display(Audio(audios[0], rate=24000))\n        ```\n    '''\n\n    def __init__(self, lang_code: str='a'):\n        '''\n        Initialize the KokoroTTS model.\n        Args:\n            lang_code (str): Language code for the model. Default is \"a\".\n                - 🇺🇸 'a': American English\n                - 🇬🇧 'b': British English\n                - 🇪🇸 'e': Spanish\n                - 🇫🇷 'f': French\n                - 🇮🇳 'h': Hindi\n                - 🇮🇹 'i': Italian\n                - 🇯🇵 'j': Japanese (requires `pip install misaki[ja]`)\n                - 🇧🇷 'p': Brazilian Portuguese\n                - 🇨🇳 'z': Mandarin Chinese (requires `pip install misaki[zh]`)\n        '''\n        pass\n\n    def __call__(self, text: str, voice: str='af_heart', speed: float=1.0, output_prefix: str='output', split_pattern: str='\\\\n+'):\n        '''\n        Generate speech from text and save it to files.\n        Args:\n            text (str): Text to convert to speech.\n            voice (str): The voice to use for generation. Default is \"af_heart\".\n                         Can also be a path to a voice tensor.\n            speed (float): Speaking speed. Default is 1.0.\n            output_prefix (str): Prefix for the output audio files.\n                                 Files will be saved as {output_prefix}_0.wav, etc.\n            split_pattern (str): Regex pattern to split the input text into segments.\n                                 Default is r'\n+'.\n        Returns:\n            list: A list of audio data numpy arrays.\n        '''\n        pass",
    "snippet_id": "snippet_267"
  },
  {
    "class_name": "coffy.nosql.index_engine.IndexManager",
    "skeleton": "\nclass IndexManager:\n    '''\n    Automatically maintains in-memory indexes for fast lookup.\n    '''\n\n    def __init__(self):\n        '''\n        Initialize the IndexManager with empty indexes and document map.\n        '''\n        pass\n\n    def index(self, doc):\n        '''\n        Index a document by adding it to the document map and updating the indexes.\n        doc -- The document to index, should be a dictionary.\n        '''\n        pass\n\n    def remove(self, doc):\n        '''\n        Remove a document from the index.\n        doc -- The document to remove, should be a dictionary.\n        '''\n        pass\n\n    def reindex(self, old_doc, new_doc):\n        '''\n        Reindex a document by removing the old document and adding the new one.\n        old_doc -- The document to remove from the index.\n        new_doc -- The document to add to the index.\n        '''\n        pass\n\n    def query(self, field, value):\n        '''\n        Query the index for documents matching a specific field and value.\n        field -- The field to query.\n        value -- The value to match in the field.\n        Returns a list of documents that match the query.\n        '''\n        pass\n\n    def query_in(self, field, values):\n        '''\n        Query the index for documents matching a specific field and a list of values.\n        field -- The field to query.\n        values -- The list of values to match in the field.\n        Returns a list of documents that match the query.\n        '''\n        pass\n\n    def clear(self):\n        '''\n        Clear all indexes and the document map.\n        '''\n        pass",
    "snippet_id": "snippet_268"
  },
  {
    "class_name": "coffy.sql.orm.Field",
    "skeleton": "\nclass Field:\n    '''\n    Class representing a database field.\n    '''\n\n    def __init__(self, sql_type: str, primary_key: bool=False, nullable: bool=True, default: Any=None):\n        '''\n        Initialize a new Field instance.\n        sql_type -- The SQL data type of the field.\n        primary_key -- Whether this field is a primary key.\n        nullable -- Whether this field can be null.\n        default -- The default value for this field.\n                '''\n                pass\n\n    def ddl(self) -> str:\n        '''\n        Generate the SQL DDL statement for this field.\n        '''\n        pass",
    "snippet_id": "snippet_269"
  },
  {
    "class_name": "lift_needle_organs_sm.PickAndLiftSm",
    "skeleton": "\nclass PickAndLiftSm:\n    '''A simple state machine in a robot's task space to pick and lift an object.\n    The state machine is implemented as a warp kernel. It takes in the current state of\n    the robot's end-effector and the object, and outputs the desired state of the robot's\n    end-effector and the gripper. The state machine is implemented as a finite state\n    machine with the following states:\n    1. REST: The robot is at rest.\n    2. APPROACH_ABOVE_OBJECT: The robot moves above the object.\n    3. APPROACH_OBJECT: The robot moves to the object.\n    4. GRASP_OBJECT: The robot grasps the object.\n    5. LIFT_OBJECT: The robot lifts the object to the desired pose. This is the final state.\n    '''\n\n    def __init__(self, dt: float, num_envs: int, device: torch.device | str='cpu'):\n        '''Initialize the state machine.\n        Args:\n            dt: The environment time step.\n            num_envs: The number of environments to simulate.\n            device: The device to run the state machine on.\n        '''\n        pass\n\n    def reset_idx(self, env_ids: Sequence[int]=None):\n        '''Reset the state machine.'''\n        pass\n\n    def compute(self, ee_pose: torch.Tensor, object_pose: torch.Tensor, des_object_pose: torch.Tensor):\n        '''Compute the desired state of the robot's end-effector and the gripper.'''\n        pass",
    "snippet_id": "snippet_270"
  },
  {
    "class_name": "lift_needle_sm.PickAndLiftSm",
    "skeleton": "\nclass PickAndLiftSm:\n    '''A simple state machine in a robot's task space to pick and lift an object.\n    The state machine is implemented as a warp kernel. It takes in the current state of\n    the robot's end-effector and the object, and outputs the desired state of the robot's\n    end-effector and the gripper. The state machine is implemented as a finite state\n    machine with the following states:\n    1. REST: The robot is at rest.\n    2. APPROACH_ABOVE_OBJECT: The robot moves above the object.\n    3. APPROACH_OBJECT: The robot moves to the object.\n    4. GRASP_OBJECT: The robot grasps the object.\n    5. LIFT_OBJECT: The robot lifts the object to the desired pose. This is the final state.\n    '''\n\n    def __init__(self, dt: float, num_envs: int, device: torch.device | str='cpu'):\n        '''Initialize the state machine.\n        Args:\n            dt: The environment time step.\n            num_envs: The number of environments to simulate.\n            device: The device to run the state machine on.\n        '''\n        pass\n\n    def reset_idx(self, env_ids: Sequence[int]=None):\n        '''Reset the state machine.'''\n        pass\n\n    def compute(self, ee_pose: torch.Tensor, object_pose: torch.Tensor, des_object_pose: torch.Tensor):\n        '''Compute the desired state of the robot's end-effector and the gripper.'''\n        pass",
    "snippet_id": "snippet_271"
  },
  {
    "class_name": "reach_dual_psm_sm.ReachSm",
    "skeleton": "\nclass ReachSm:\n    '''A simple state machine in a robot's task space for a reach task.\n    The state machine is implemented as a warp kernel. It takes in the current state of\n    the robot's end-effector, and outputs the desired state of the robot's end-effector.\n    The state machine is implemented as a finite state machine with the following states:\n    1. REST: The robot is at rest.\n    2. REACH: The robot reaches to the desired pose. This is the final state.\n    '''\n\n    def __init__(self, dt: float, num_envs: int, device: torch.device | str='cpu'):\n        '''Initialize the state machine.\n        Args:\n            dt: The environment time step.\n            num_envs: The number of environments to simulate.\n            device: The device to run the state machine on.\n        '''\n        pass\n\n    def reset_idx(self, env_ids: Sequence[int]=None):\n        '''Reset the state machine.'''\n        pass\n\n    def compute(self, ee_1_pose: torch.Tensor, ee_2_pose: torch.Tensor, des_final_pose_1: torch.Tensor, des_final_pose_2: torch.Tensor):\n        '''Compute the desired state of the robot's end-effector.'''\n        pass",
    "snippet_id": "snippet_272"
  },
  {
    "class_name": "reach_star_sm.ReachSm",
    "skeleton": "\nclass ReachSm:\n    '''A simple state machine in a robot's task space for a reach task.\n    The state machine is implemented as a warp kernel. It takes in the current state of\n    the robot's end-effector, and outputs the desired state of the robot's end-effector.\n    The state machine is implemented as a finite state machine with the following states:\n    1. REST: The robot is at rest.\n    2. REACH: The robot reaches to the desired pose. This is the final state.\n    '''\n\n    def __init__(self, dt: float, num_envs: int, device: torch.device | str='cpu'):\n        '''Initialize the state machine.\n        Args:\n            dt: The environment time step.\n            num_envs: The number of environments to simulate.\n            device: The device to run the state machine on.\n        '''\n        pass\n\n    def reset_idx(self, env_ids: Sequence[int]=None):\n        '''Reset the state machine.'''\n        pass\n\n    def compute(self, ee_pose: torch.Tensor, des_final_pose: torch.Tensor):\n        '''Compute the desired state of the robot's end-effector.'''\n        pass",
    "snippet_id": "snippet_273"
  },
  {
    "class_name": "robotic_ultrasound.scripts.simulation.annotators.base.Annotator",
    "skeleton": "\nclass Annotator:\n    '''Base class for simulation annotators that handle DDS communication.\n    This class provides the basic infrastructure for creating annotators that can\n    publish and subscribe to DDS topics in the simulation environment. It manages\n    the lifecycle of publishers and subscribers, ensuring proper initialization\n    and cleanup.\n    Args:\n        name: Unique identifier for the annotator\n        prim_path: USD path to the sensor primitive in the stage\n        publishers: List of DDS publishers to manage\n        subscribers: List of DDS subscribers to manage\n    '''\n\n    def __init__(self, name: str, prim_path: str, publishers: List[Publisher] | None=None, subscribers: List[Subscriber] | None=None):\n        '''Initialize the annotator with the given name, prim path, and DDS endpoints.'''\n        pass\n\n    def start(self, world: World) -> None:\n        '''Start all publishers and subscribers with rate-limited callbacks.\n        This method registers each publisher and subscriber with the physics\n        simulation loop at their specified rates.\n        Args:\n            world: The simulation world object that provides the physics context\n        '''\n        pass\n\n    def stop(self, world: World) -> None:\n        '''Stop all publishers and subscribers and cleanup their callbacks.\n        This method removes all registered callbacks from the physics simulation\n        loop and stops the subscribers.\n        Args:\n            world: The simulation world object that provides the physics context\n        '''\n        pass",
    "snippet_id": "snippet_274"
  },
  {
    "class_name": "ultrasound_state_machine.UltrasoundStateMachine",
    "skeleton": "\nclass UltrasoundStateMachine:\n    '''State machine for ultrasound procedure control.'''\n\n    def __init__(self, modules: Dict[str, BaseControlModule], device: str='cuda:0'):\n        '''Initialize the state machine.\n        Args:\n            modules (Dict[str, BaseControlModule]): Dictionary of control modules\n            device (str): Device to run computations on\n        '''\n        pass\n\n    def compute_action(self, env, robot_obs: torch.Tensor) -> torch.Tensor:\n        '''Compute combined action from all active modules.\n        Args:\n            env: The environment instance\n            robot_obs: Current robot observations\n        Returns:\n            Tuple of relative and absolute commands\n        '''\n        pass\n\n    def get_normal_force(self):\n        '''\n        Get the average normal force from the contact sensor.\n        '''\n        pass\n\n    def reset(self) -> None:\n        '''Reset the state machine and all modules.'''\n        pass\n\n    def assert_module_order(self):\n        '''Assert that the modules are in the correct order.'''\n        pass",
    "snippet_id": "snippet_275"
  },
  {
    "class_name": "granite_io.types.NoDefaultsMixin",
    "skeleton": "\nclass NoDefaultsMixin:\n    '''\n    Mixin so that we don't need to copy and paste the code to avoid filling JSON values\n    with a full catalog of the default values of rarely-used fields.\n        '''\n    @pydantic.model_serializer(mode='wrap')\n    def _workaround_for_design_flaw_in_pydantic(self, nxt):\n        '''\n        Workaround for a design flaw in Pydantic that forces users to accept\n        unnecessary garbage in their serialized JSON data or to override\n        poorly-documented serialization hooks repeatedly.  Automates overriding said\n        poorly-documented serialization hooks for a single dataclass.\n        See https://github.com/pydantic/pydantic/issues/4554 for the relevant dismissive\n        comment from the devs. This comment suggests overriding :func:`dict()`, but that\n        method was disabled a year later. Now you need to add a custom serializer method\n        with a ``@model_serializer`` decorator.\n        See the docs at\n        https://docs.pydantic.dev/latest/api/functional_serializers/\n        for some dubious information on how this API works.\n        See comments below for important gotchas that aren't in the documentation.\n        '''\n        pass\n\n    def _keep_these_fields(self) -> tuple[str]:\n        '''\n        Dataclasses that include this mixin can override this method to add specific\n        default values to serialized JSON.\n        This is necessary for round-tripping to JSON when there are fields that\n        determine which dataclass to use for deserialization.\n                '''\n                pass",
    "snippet_id": "snippet_276"
  },
  {
    "class_name": "fletx.FletX",
    "skeleton": "\nclass FletX:\n    '''FletX Dependency Injection Interface\n    This class provides a simple interface to interact with the Dependency Injection (DI) container.\n    It allows to register, find, delete, and reset instances in the DI container.\n    '''\n    @staticmethod\n    def put(instance, tag=None):\n        '''Register an instance in the DI container'''\n        pass\n    @staticmethod\n    def find(cls, tag=None):\n        '''Retrieve an instance from the DI container'''\n        pass\n    @staticmethod\n    def delete(cls, tag=None):\n        '''Delete an instance from the DI container'''\n        pass\n    @staticmethod\n    def reset():\n        '''Reset the DI container, clearing all registered instances.'''\n        pass",
    "snippet_id": "snippet_277"
  },
  {
    "class_name": "fletx.cli.commands.base.CommandRegistry",
    "skeleton": "\nclass CommandRegistry:\n    '''\n    Registry for all commands in the FletX CLI.\n    This class is responsible for storing and managing all available commands.\n    It provides methods to register new commands and retrieve existing ones.\n    '''\n    @classmethod\n    def register(cls, name: str, command_cls: Type['BaseCommand']) -> None:\n        '''Register a new command class.'''\n        pass\n    @classmethod\n    def get(cls, name: str) -> Type['BaseCommand']:\n        '''Get a command class by its name.'''\n        pass\n    @classmethod\n    def all(cls) -> List[Type['BaseCommand']]:\n        '''Get all registered command classes.'''\n        pass",
    "snippet_id": "snippet_278"
  },
  {
    "class_name": "fletx.cli.templates.TemplateValidator",
    "skeleton": "\nclass TemplateValidator:\n    '''\n    Validates template names and ensures they follow conventions.\n    '''\n    @staticmethod\n    def validate_name(name: str, target_type: str='template') -> None:\n        '''\n        Validate a template name.\n        Args:\n            name: The name to validate\n            target_type: Type of template (for error messages)\n        '''\n        pass\n    @staticmethod\n    def validate_path(path: str) -> None:\n        '''Validate a file path.'''\n        pass",
    "snippet_id": "snippet_279"
  },
  {
    "class_name": "fletx.core.concurency.worker.BoundWorkerMethod",
    "skeleton": "\nclass BoundWorkerMethod:\n    '''\n    Proxy object that binds a `WorkerTaskWrapper` to an instance (self),\n    allowing decorated instance methods to work seamlessly with all\n    background execution capabilities.\n    This enables you to use:\n        instance.method()\n        instance.method.async_call(...)\n        instance.method.run_and_wait(...)\n    without losing access to wrapper methods.\n    Attributes:\n        _wrapper: The original WorkerTaskWrapper\n        _instance: The instance to bind as the first argument\n    '''\n\n    def __init__(self, wrapper: 'WorkerTaskWrapper', instance: object):\n        '''\n        Initialize the proxy with the wrapper and the instance.\n        Args:\n            wrapper: The WorkerTaskWrapper object\n            instance: The object instance to bind to the call\n        '''\n        pass\n\n    def __call__(self, *args, **kwargs):\n        '''\n        Synchronous execution with the bound instance.\n        Equivalent to: wrapper(instance, *args, **kwargs)\n        '''\n        pass\n\n    def async_call(self, *args, **kwargs) -> str:\n        '''\n        Asynchronous execution with the bound instance.\n        Returns a worker_id.\n        '''\n        pass\n\n    def submit(self, *args, **kwargs) -> str:\n        '''\n        Alias for async_call().\n        '''\n        pass\n\n    def run_and_wait(self, *args, timeout: Optional[float]=None, **kwargs):\n        '''\n        Executes the task in the background, then waits for and returns the result.\n        Raises:\n            RuntimeError: if the task is cancelled\n            Exception: if the task failed with an error\n        '''\n        pass\n\n    def set_pool(self, pool: 'WorkerPool'):\n        '''\n        Sets a specific pool to use for this task.\n        '''\n        pass\n\n    def shutdown_default_pool(self):\n        '''\n        Shuts down the default pool used by this function, if created.\n        '''\n        pass\n\n    def __getattr__(self, name):\n        '''\n        Fallback to the wrapper’s attributes for completeness.\n        This makes sure any missing attributes are forwarded.\n        '''\n        pass",
    "snippet_id": "snippet_280"
  },
  {
    "class_name": "amzn_nova_prompt_optimizer.core.input_adapters.prompt_adapter.FewShotFormat",
    "skeleton": "\nclass FewShotFormat:\n    '''Handler for different few-shot example formats'''\n    @staticmethod\n    def convert(examples: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n        '''Convert any supported format to input-output format'''\n        pass\n    @staticmethod\n    def validate(examples: List[Dict[str, Any]]) -> bool:\n        '''Validate that examples are in input-output format'''\n        pass",
    "snippet_id": "snippet_281"
  },
  {
    "class_name": "secops.auth.SecOpsAuth",
    "skeleton": "\nclass SecOpsAuth:\n    '''Handles authentication for the Google SecOps SDK.'''\n\n    def __init__(self, credentials: Optional[Credentials]=None, service_account_path: Optional[str]=None, service_account_info: Optional[Dict[str, Any]]=None, impersonate_service_account: Optional[str]=None, scopes: Optional[List[str]]=None):\n        '''Initialize authentication for SecOps.\n        Args:\n            credentials: Optional pre-existing Google Auth credentials\n            service_account_path: Optional path to service account JSON key file\n            service_account_info: Optional service account JSON key data as dict\n            impersonate_service_account: Optional service account to impersonate\n            scopes: Optional list of OAuth scopes to request\n        '''\n        pass\n\n    def _get_credentials(self, credentials: Optional[Credentials], service_account_path: Optional[str], service_account_info: Optional[Dict[str, Any]], impersonate_service_account: Optional[str]) -> Credentials:\n        '''Get credentials from various sources.'''\n        pass\n    @property\n    def session(self):\n        '''Get an authorized session using the credentials.\n        Returns:\n            Authorized session for API requests\n        '''\n        pass",
    "snippet_id": "snippet_282"
  },
  {
    "class_name": "secops.chronicle.feeds.CreateFeedModel",
    "skeleton": "@dataclass\nclass CreateFeedModel:\n    '''Model for creating a feed.\n    Args:\n        display_name: Display name for the feed\n        details: Feed details as either a JSON string or dict.\n            If string, will be parsed as JSON.\n    '''\n\n    def __post_init__(self):\n        '''Convert string details to dict if needed'''\n        pass\n\n    def to_dict(self) -> Dict[str, Any]:\n        '''Convert to dictionary for JSON serialization.'''\n        pass",
    "snippet_id": "snippet_283"
  },
  {
    "class_name": "secops.chronicle.feeds.UpdateFeedModel",
    "skeleton": "@dataclass\nclass UpdateFeedModel:\n    '''Model for updating a feed.\n    Args:\n        display_name: Optional display name for the feed\n        details: Optional feed details as either a JSON string or dict.\n            If string, will be parsed as JSON.\n    '''\n\n    def __post_init__(self):\n        '''Convert string details to dict if needed'''\n        pass\n\n    def to_dict(self) -> Dict[str, Any]:\n        '''Convert to dictionary for JSON serialization.'''\n        pass",
    "snippet_id": "snippet_284"
  },
  {
    "class_name": "secops.chronicle.gemini.Block",
    "skeleton": "\nclass Block:\n    '''Represents a block in the Gemini response.\n    Blocks are discrete chunks of content with different types\n    (text, code, HTML, etc.) returned in a Gemini conversation response.\n    '''\n\n    def __init__(self, block_type: str, content: str, title: Optional[str]=None):\n        '''Initialize a response block.\n        Args:\n            block_type: The type of the block ('TEXT', 'CODE', 'HTML', etc.)\n            content: The content of the block\n            title: Optional title for the block (may be present in CODE blocks)\n        '''\n        pass\n\n    def __repr__(self) -> str:\n        '''Return string representation of the block.\n        Returns:\n            String representation of the block with its type\n            and title if present\n        '''\n        pass",
    "snippet_id": "snippet_285"
  },
  {
    "class_name": "secops.chronicle.gemini.NavigationAction",
    "skeleton": "class NavigationAction:\n    '''Represents a navigation action for a suggested action.'''\n\n    def __init__(self, target_uri: str):\n        '''Initialize a navigation action.\n        Args:\n            target_uri: The target URI for the navigation action\n        '''\n        pass\n\n    def __repr__(self) -> str:\n        '''Return string representation of the navigation action.\n        Returns:\n            String representation with the target URI\n        '''\n        pass",
    "snippet_id": "snippet_286"
  },
  {
    "class_name": "secops.chronicle.models.InputInterval",
    "skeleton": "@dataclass\nclass InputInterval:\n    '''Input interval values to query.'''\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]):\n        '''Create from a dictionary.'''\n        pass\n\n    def __post_init__(self):\n        '''Validate that only one of `time_window` or `relative_time` is set.'''\n        pass\n\n    def to_dict(self) -> Dict[str, Any]:\n        '''Convert to a dictionary.'''\n        pass",
    "snippet_id": "snippet_287"
  },
  {
    "class_name": "secops.chronicle.parser_extension.ParserExtensionConfig",
    "skeleton": "@dataclass\nclass ParserExtensionConfig:\n    '''Parser extension configuration.'''\n    @staticmethod\n    def encode_base64(text: str) -> str:\n        '''Encode a string to base64.\n        Args:\n            log: Raw string\n        Returns:\n            Base64 encoded string\n        '''\n        pass\n\n    def __post_init__(self) -> None:\n        '''Post initialization hook for field processing.'''\n        pass\n\n    def validate(self) -> None:\n        '''Validate configuration.\n        Raises:\n            ValueError: If configuration is invalid\n        '''\n        pass\n\n    def to_dict(self) -> Dict:\n        '''Convert to dictionary format for API request.\n        Returns:\n            Dict containing the configuration in API format\n        Raises:\n            ValueError: If configuration is invalid\n        '''\n        pass",
    "snippet_id": "snippet_288"
  },
  {
    "class_name": "secops.chronicle.rule_exclusion.UpdateRuleDeployment",
    "skeleton": "@dataclass\nclass UpdateRuleDeployment:\n    '''Model for updating rule deployment.'''\n\n    def __post_init__(self):\n        '''Post initilizaiton for validating/converting attributes'''\n        pass\n\n    def to_dict(self) -> Dict[str, Any]:\n        '''Convert to dictionary for JSON serialization.'''\n        pass",
    "snippet_id": "snippet_289"
  },
  {
    "class_name": "secops.client.SecOpsClient",
    "skeleton": "\nclass SecOpsClient:\n    '''Main client class for interacting with Google SecOps.'''\n\n    def __init__(self, credentials: Optional[Credentials]=None, service_account_path: Optional[str]=None, service_account_info: Optional[Dict[str, Any]]=None, impersonate_service_account: Optional[str]=None):\n        '''Initialize the SecOps client.\n        Args:\n            credentials: Optional pre-existing Google Auth credentials\n            service_account_path: Optional path to service account JSON key file\n            service_account_info: Optional service account JSON key data as dict\n            impersonate_service_account: Optional service account to impersonate\n        '''\n        pass\n\n    def chronicle(self, customer_id: str, project_id: str, region: str='us') -> ChronicleClient:\n        '''Get Chronicle API client.\n        Args:\n            customer_id: Chronicle customer ID\n            project_id: GCP project ID\n            region: Chronicle API region (default: \"us\")\n        Returns:\n            ChronicleClient instance\n        '''\n        pass",
    "snippet_id": "snippet_290"
  },
  {
    "class_name": "mcp_server_tree_sitter.utils.context.mcp_context.ProgressScope",
    "skeleton": "class ProgressScope:\n    '''Scope for tracking progress of an operation.'''\n\n    def __init__(self, context: 'MCPContext', total: int, description: str):\n        '''\n        Initialize a progress scope.\n        Args:\n            context: The parent MCPContext\n            total: Total number of steps\n            description: Description of the operation\n        '''\n        pass\n\n    def update(self, step: int=1) -> None:\n        '''\n        Update progress by a number of steps.\n        Args:\n            step: Number of steps to add to progress\n        '''\n        pass\n\n    def set_progress(self, current: int) -> None:\n        '''\n        Set progress to a specific value.\n        Args:\n            current: Current progress value\n        '''\n        pass",
    "snippet_id": "snippet_291"
  },
  {
    "class_name": "sdialog.interpretability.BaseHook",
    "skeleton": "\nclass BaseHook(ABC):\n    '''\n    Base class for registering and managing PyTorch forward hooks on model layers.\n    This class is used to create specific hook classes, like `ResponseHook` and `ActivationHook`.\n        '''\n\n    def __init__(self, layer_key, hook_fn, agent):\n        '''\n        :param layer_key: Dotted module path in model.named_modules().\n        :type layer_key: str\n        :param hook_fn: Callable with signature (module, input, output).\n        :type hook_fn: Callable\n        :param agent: Owning agent (may be None for generic hooks).\n        :type agent: Any\n        '''\n        pass\n\n    def _hook(self):\n        '''Placeholder hook (override in subclasses).'''\n        pass\n\n    def register(self, model):\n        '''\n        Registers the hook on the given model using the layer_key.\n        :param model: Model whose layer will be hooked.\n        :type model: torch.nn.Module\n        :return: The hook handle.\n        :rtype: Any\n        '''\n        pass\n\n    def remove(self):\n        '''\n        Removes the hook if it is registered.\n        '''\n        pass",
    "snippet_id": "snippet_292"
  },
  {
    "class_name": "sdialog.util.KNNModel",
    "skeleton": "\nclass KNNModel:\n    '''\n    Thin wrapper around sklearn NearestNeighbors for cosine similarity retrieval.\n    :ivar model: Fitted NearestNeighbors instance.\n    :vartype model: NearestNeighbors\n    :ivar k: Default number of neighbors.\n    :vartype k: int\n    '''\n\n    def __init__(self, items, k=3):\n        '''\n        Initialize KNN index.\n        :param items: Iterable of (item_id, embedding_vector) pairs.\n        :type items: Iterable[Tuple[Any, Sequence[float]]]\n        :param k: Default number of neighbors to retrieve.\n        :type k: int\n        '''\n        pass\n\n    def neighbors(self, target_emb, k=None):\n        '''\n        Retrieve k nearest neighbors by cosine distance.\n        :param target_emb: Query embedding vector.\n        :type target_emb: Sequence[float]\n        :param k: Override number of neighbors (defaults to self.k).\n        :type k: int\n        :return: List of (item_id, distance) pairs ordered by proximity.\n        :rtype: List[Tuple[Any, float]]\n        '''\n        pass",
    "snippet_id": "snippet_293"
  },
  {
    "class_name": "mas_arena.agents.EvoMAC.CodeManager",
    "skeleton": "\nclass CodeManager:\n    '''\n    Manages code content extraction, storage, and formatting.\n    This class provides robust methods for extracting Python code from LLM responses\n    using multiple fallback strategies, similar to HumanEval evaluator approaches.\n    '''\n\n    def __init__(self):\n        '''Initialize code storage.'''\n        pass\n\n    def update_from_response(self, response: str) -> None:\n        '''\n        Update codes from LLM response using robust extraction methods.\n        Args:\n            response: Raw LLM response text containing code\n        '''\n        pass\n\n    def _extract_code_with_fallbacks(self, text: str) -> str:\n        '''\n        Extract Python code from text using multiple fallback methods.\n        This method tries several extraction strategies in order of preference:\n        1. Look for \"## Validated Code\" section (from other agents)\n        2. Extract from ```python``` fenced blocks\n        3. Find function definition patterns\n        4. Parse filename + code patterns  \n        5. Last resort: find any Python-like syntax\n        Args:\n            text: Input text containing code\n        Returns:\n            Extracted code string, or empty string if no code found\n        '''\n        pass\n\n    def _extract_python_like_content(self, text: str) -> str:\n        '''\n        Last resort extraction method for Python-like content.\n        Args:\n            text: Input text\n        Returns:\n            Extracted Python-like code or empty string\n        '''\n        pass\n\n    def get_formatted_codes(self) -> str:\n        '''\n        Get formatted codes for display purposes.\n        Returns:\n            Formatted string with filename and code blocks\n        '''\n        pass\n\n    def get_raw_code(self) -> str:\n        '''\n        Get raw code content without formatting.\n        Returns:\n            Raw code string, concatenated if multiple files exist\n        '''\n        pass\n\n    def has_code(self) -> bool:\n        '''Check if any code has been stored.'''\n        pass",
    "snippet_id": "snippet_294"
  },
  {
    "class_name": "mas_arena.tools.tool_selector.ToolSelector",
    "skeleton": "\nclass ToolSelector:\n    '''\n    Primary interface for tool selection across single-agent and multi-agent scenarios.\n    Use `select_tools(task_description, num_agents=None, overlap=False, limit=5)` to pick or partition tools.\n    Override `select_tools` to implement custom selection strategies.\n    '''\n\n    def __init__(self, tools: List[Dict[str, Any]]):\n        '''\n        Initialize with available tools.\n        Args:\n            tools: List of tool definitions from ToolManager\n        '''\n        pass\n\n    def _select_for_task(self, task_description: str, limit: int=5) -> List[Dict[str, Any]]:\n        '''\n        Internal single-agent tool selection logic.\n        '''\n        pass\n\n    def _partition_tools_for_multi_agent(self, num_agents: int, overlap: bool=False, task_description: Optional[str]=None) -> List[List[Dict[str, Any]]]:\n        '''\n        Internal multi-agent tool partitioning logic.\n        '''\n        pass\n\n    def select_by_names(self, tool_names: List[str]) -> List[Any]:\n        '''Select tools by their names (case-insensitive).'''\n        pass\n\n    def filter_by_roles(self, role_patterns: Dict[str, List[str]]) -> Dict[str, List[Any]]:\n        '''Filter tools for each role based on name patterns.'''\n        pass\n\n    def filter_by_keywords(self, keywords: List[str], match_all: bool=False) -> List[Any]:\n        '''\n        Filter tools that match specified keywords in name or description.\n        Args:\n            keywords: List of keywords to match\n            match_all: If True, tools must match all keywords. If False, match any keyword.\n        Returns:\n            List of tools matching the criteria\n        '''\n        pass\n\n    def select_tools(self, task_description: str, num_agents: Optional[int]=None, overlap: bool=False, limit: int=5) -> Any:\n        '''\n        Unified public interface for tool selection.\n        - Single-agent (num_agents None or <=1): returns a flat list of top tools.\n        - Multi-agent (num_agents >1): returns a list of tool lists, one per agent.\n        Override this method to implement any custom logic.\n        '''\n        pass",
    "snippet_id": "snippet_295"
  },
  {
    "class_name": "mas_arena.visualization.mas_visualizer.BenchmarkVisualizer",
    "skeleton": "\nclass BenchmarkVisualizer:\n    '''Utility for visualizing benchmark results and agent interactions across multiple problems'''\n\n    def __init__(self, output_dir=None):\n        '''\n        Initialize the benchmark visualizer.\n        Args:\n            output_dir: Directory to save visualization HTML files\n        '''\n        pass\n\n    def generate_summary_html(self, summary_data, results_data, problem_visualizations=None, title=None):\n        '''\n        Generate HTML for visualizing benchmark summary with links to problem visualizations.\n        Args:\n            summary_data: Dictionary with benchmark summary data\n            results_data: List of problem results data\n            problem_visualizations: Optional dictionary mapping problem_id to visualization file paths\n            title: Optional title for the visualization\n        Returns:\n            HTML string\n        '''\n        pass\n\n    def visualize_benchmark(self, summary_file, results_file=None, visualizations_dir=None, output_file=None):\n        '''\n        Generate HTML visualization from benchmark summary file with links to problem visualizations.\n        Args:\n            summary_file: Path to the benchmark summary JSON file\n            results_file: Optional path to the benchmark results JSON file\n            visualizations_dir: Optional directory containing problem visualizations\n            output_file: Optional path to save the HTML output\n        Returns:\n            Path to the generated HTML file\n        '''\n        pass",
    "snippet_id": "snippet_296"
  },
  {
    "class_name": "mas_arena.visualization.mas_visualizer.MASVisualizer",
    "skeleton": "\nclass MASVisualizer:\n    '''Utility for visualizing Multi-Agent System interactions'''\n\n    def __init__(self, output_dir=None):\n        '''\n        Initialize the MAS visualizer.\n        Args:\n            output_dir: Directory to save visualization HTML files\n        '''\n        pass\n\n    def generate_html(self, visualization_data, title=None):\n        '''\n        Generate HTML for visualizing agent interactions using D3.js.\n        Args:\n            visualization_data: Dictionary with nodes and links data\n            title: Optional title for the visualization\n        Returns:\n            HTML string\n        '''\n        pass\n\n    def visualize(self, visualization_file, output_file=None, open_browser=True):\n        '''\n        Generate an HTML visualization from a visualization data file and open in browser.\n        Args:\n            visualization_file: Path to the visualization data JSON file\n            output_file: Optional path to save the HTML output\n            open_browser: Whether to open the visualization in a browser (default: True)\n        Returns:\n            Path to the generated HTML file\n        '''\n        pass\n\n    def visualize_from_agent_system(self, agent_system, problem_id=None):\n        '''\n        Generate visualizations for all visualization files from an agent system.\n        Args:\n            agent_system: AgentSystem instance\n            problem_id: Optional problem ID to filter by\n        Returns:\n            List of paths to generated HTML files\n        '''\n        pass",
    "snippet_id": "snippet_297"
  },
  {
    "class_name": "archipy.adapters.base.sqlalchemy.session_manager_ports.SessionManagerPort",
    "skeleton": "\nclass SessionManagerPort:\n    '''Interface for SQLAlchemy session management operations.\n    This interface defines the contract for session management adapters,\n    providing methods for retrieving and managing database sessions\n    in a synchronous context.\n    Implementing classes must provide mechanisms to:\n    1. Retrieve a properly configured SQLAlchemy session\n    2. Release/remove sessions when they're no longer needed\n    '''\n    @abstractmethod\n    def get_session(self) -> Session:\n        '''Retrieve a SQLAlchemy session.\n        This method provides a database session that can be used for\n        querying, creating, updating, and deleting data.\n        Returns:\n            Session: A SQLAlchemy session object\n        Examples:\n            >>> session = session_manager.get_session()\n            >>> results = session.query(User).all()\n        '''\n        pass\n    @abstractmethod\n    def remove_session(self) -> None:\n        '''Remove the current session from the registry.\n        This method should be called to clean up the session when it's\n        no longer needed, helping to prevent resource leaks and ensure\n        proper session management.\n        '''\n        pass",
    "snippet_id": "snippet_298"
  },
  {
    "class_name": "archipy.adapters.base.sqlalchemy.session_manager_registry.SessionManagerRegistry",
    "skeleton": "\nclass SessionManagerRegistry:\n    '''Registry for SQLAlchemy session managers.\n    This registry provides a centralized access point for both synchronous and\n    asynchronous session managers, implementing the Service Locator pattern.\n    Subclasses should override get_sync_manager and get_async_manager to provide\n    concrete session managers, or use set_sync_manager and set_async_manager to\n    register managers manually.\n    Examples:\n        >>> from archipy.adapters.postgres.sqlalchemy.session_manager_registry import PostgresSessionManagerRegistry\n        >>> sync_manager = PostgresSessionManagerRegistry.get_sync_manager()\n        >>> session = sync_manager.get_session()\n    '''\n    @classmethod\n    def get_sync_manager(cls) -> 'SessionManagerPort':\n        '''Get the synchronous session manager instance.\n        Returns:\n            SessionManagerPort: The registered synchronous session manager\n        Raises:\n            InternalError: If no synchronous session manager is set\n            DatabaseConnectionError: If there's an error initializing the session manager\n        '''\n        pass\n    @classmethod\n    def set_sync_manager(cls, manager: 'SessionManagerPort') -> None:\n        '''Set a custom synchronous session manager.\n        Args:\n            manager: An instance implementing SessionManagerPort\n        Raises:\n            InvalidArgumentError: If the manager is None or doesn't implement SessionManagerPort\n        '''\n        pass\n    @classmethod\n    def get_async_manager(cls) -> 'AsyncSessionManagerPort':\n        '''Get the asynchronous session manager instance.\n        Returns:\n            AsyncSessionManagerPort: The registered asynchronous session manager\n        Raises:\n            InternalError: If no asynchronous session manager is set\n            DatabaseConnectionError: If there's an error initializing the session manager\n        '''\n        pass\n    @classmethod\n    def set_async_manager(cls, manager: 'AsyncSessionManagerPort') -> None:\n        '''Set a custom asynchronous session manager.\n        Args:\n            manager: An instance implementing AsyncSessionManagerPort\n        Raises:\n            InvalidArgumentError: If the manager is None or doesn't implement AsyncSessionManagerPort\n        '''\n        pass\n    @classmethod\n    def reset(cls) -> None:\n        '''Reset the registry to its initial state.\n        This method clears both registered managers, useful for testing.\n        '''\n        pass",
    "snippet_id": "snippet_299"
  },
  {
    "class_name": "archipy.adapters.kafka.adapters.KafkaExceptionHandlerMixin",
    "skeleton": "\nclass KafkaExceptionHandlerMixin:\n    '''Mixin class to handle Kafka exceptions in a consistent way.'''\n    @classmethod\n    def _handle_kafka_exception(cls, exception: Exception, operation: str) -> None:\n        '''Handle Kafka exceptions and map them to appropriate application errors.\n        Args:\n            exception: The original exception\n            operation: The name of the operation that failed\n        Raises:\n            Various application-specific errors based on the exception type/content\n        '''\n        pass\n    @classmethod\n    def _handle_producer_exception(cls, exception: Exception, operation: str) -> None:\n        '''Handle producer-specific exceptions.\n        Args:\n            exception: The original exception\n            operation: The name of the operation that failed\n        Raises:\n            ResourceExhaustedError: If the producer queue is full\n            Various other errors from _handle_kafka_exception\n        '''\n        pass",
    "snippet_id": "snippet_300"
  },
  {
    "class_name": "archipy.adapters.kafka.ports.KafkaAdminPort",
    "skeleton": "\nclass KafkaAdminPort:\n    '''Interface for Kafka admin operations.\n    This interface defines the contract for performing administrative operations on Kafka topics.\n        '''\n    @abstractmethod\n    def create_topic(self, topic: str, num_partitions: int=1, replication_factor: int=1) -> None:\n        '''Creates a new Kafka topic.\n        Args:\n            topic (str): Name of the topic to create.\n            num_partitions (int, optional): Number of partitions for the topic. Defaults to 1.\n            replication_factor (int, optional): Replication factor for the topic. Defaults to 1.\n        Raises:\n            NotImplementedError: If the method is not implemented by the concrete class.\n        '''\n        pass\n    @abstractmethod\n    def delete_topic(self, topics: list[str]) -> None:\n        '''Deletes one or more Kafka topics.\n        Args:\n            topics (list[str]): List of topic names to delete.\n        Raises:\n            NotImplementedError: If the method is not implemented by the concrete class.\n        '''\n        pass\n    @abstractmethod\n    def list_topics(self, topic: str | None=None, timeout: int=1) -> ClusterMetadata:\n        '''Lists Kafka topics.\n        Args:\n            topic (str | None, optional): Specific topic to list. If None, lists all topics.\n                Defaults to None.\n            timeout (int, optional): Timeout in seconds for the operation. Defaults to 1.\n        Returns:\n            ClusterMetadata: Metadata about the Kafka cluster and topics.\n        Raises:\n            NotImplementedError: If the method is not implemented by the concrete class.\n        '''\n        pass",
    "snippet_id": "snippet_301"
  },
  {
    "class_name": "archipy.adapters.kafka.ports.KafkaProducerPort",
    "skeleton": "\nclass KafkaProducerPort:\n    '''Interface for Kafka producer operations.\n    This interface defines the contract for producing messages to Kafka topics.\n        '''\n    @abstractmethod\n    def produce(self, message: str | bytes) -> None:\n        '''Produces a message to the configured topic.\n        Args:\n            message (str | bytes): The message to produce.\n        Raises:\n            NotImplementedError: If the method is not implemented by the concrete class.\n        '''\n        pass\n    @abstractmethod\n    def flush(self, timeout: int | None) -> None:\n        '''Flushes any pending messages to the broker.\n        Args:\n            timeout (int | None): Maximum time to wait for messages to be delivered.\n                If None, wait indefinitely.\n        Raises:\n            NotImplementedError: If the method is not implemented by the concrete class.\n        '''\n        pass\n    @abstractmethod\n    def validate_healthiness(self) -> None:\n        '''Validates the health of the producer connection.\n        Raises:\n            NotImplementedError: If the method is not implemented by the concrete class.\n        '''\n        pass\n    @abstractmethod\n    def list_topics(self, topic: str | None, timeout: int) -> ClusterMetadata:\n        '''Lists Kafka topics.\n        Args:\n            topic (str | None): Specific topic to list. If None, lists all topics.\n            timeout (int): Timeout in seconds for the operation.\n        Returns:\n            ClusterMetadata: Metadata about the Kafka cluster and topics.\n        Raises:\n            NotImplementedError: If the method is not implemented by the concrete class.\n        '''\n        pass",
    "snippet_id": "snippet_302"
  },
  {
    "class_name": "archipy.adapters.keycloak.adapters.KeycloakExceptionHandlerMixin",
    "skeleton": "\nclass KeycloakExceptionHandlerMixin:\n    '''Mixin class to handle Keycloak exceptions in a consistent way.'''\n    @classmethod\n    def _extract_error_message(cls, exception: KeycloakError) -> str:\n        '''Extract the actual error message from Keycloak error response.\n        Args:\n            exception: The Keycloak exception\n        Returns:\n            str: The extracted error message\n        '''\n        pass\n    @classmethod\n    def _handle_keycloak_exception(cls, exception: KeycloakError, operation: str) -> None:\n        '''Handle Keycloak exceptions and map them to appropriate application errors.\n        Args:\n            exception: The original Keycloak exception\n            operation: The name of the operation that failed\n        Raises:\n            Various application-specific errors based on the exception type/content\n        '''\n        pass\n    @classmethod\n    def _handle_realm_exception(cls, exception: KeycloakError, operation: str, realm_name: str | None=None) -> None:\n        '''Handle realm-specific exceptions.\n        Args:\n            exception: The original Keycloak exception\n            operation: The name of the operation that failed\n            realm_name: The realm name involved in the operation\n        Raises:\n            RealmAlreadyExistsError: If realm already exists\n            Various other errors from _handle_keycloak_exception\n        '''\n        pass\n    @classmethod\n    def _handle_user_exception(cls, exception: KeycloakError, operation: str, user_data: dict | None=None) -> None:\n        '''Handle user-specific exceptions.\n        Args:\n            exception: The original Keycloak exception\n            operation: The name of the operation that failed\n            user_data: The user data involved in the operation\n        Raises:\n            UserAlreadyExistsError: If user already exists\n            Various other errors from _handle_keycloak_exception\n        '''\n        pass\n    @classmethod\n    def _handle_client_exception(cls, exception: KeycloakError, operation: str, client_data: dict | None=None) -> None:\n        '''Handle client-specific exceptions.\n        Args:\n            exception: The original Keycloak exception\n            operation: The name of the operation that failed\n            client_data: The client data involved in the operation\n        Raises:\n            ClientAlreadyExistsError: If client already exists\n            Various other errors from _handle_keycloak_exception\n        '''\n        pass",
    "snippet_id": "snippet_303"
  },
  {
    "class_name": "archipy.adapters.minio.adapters.MinioExceptionHandlerMixin",
    "skeleton": "\nclass MinioExceptionHandlerMixin:\n    '''Mixin class to handle MinIO/S3 exceptions in a consistent way.'''\n    @classmethod\n    def _handle_s3_exception(cls, exception: S3Error, operation: str) -> None:\n        '''Handle S3Error exceptions and map them to appropriate application errors.\n        Args:\n            exception: The original S3Error exception\n            operation: The name of the operation that failed\n        Raises:\n            Various application-specific errors based on the exception type/content\n        '''\n        pass\n    @classmethod\n    def _handle_general_exception(cls, exception: Exception, component: str) -> None:\n        '''Handle general exceptions by converting them to appropriate application errors.\n        Args:\n            exception: The original exception\n            component: The component/operation name for context\n        Raises:\n            InternalError: A wrapped version of the original exception\n        '''\n        pass",
    "snippet_id": "snippet_304"
  },
  {
    "class_name": "arazzo_generator.generator.components.ArazzoComponentsBuilder",
    "skeleton": "\nclass ArazzoComponentsBuilder:\n    '''Builder for Arazzo components section.'''\n    @staticmethod\n    def create_action(action_type: str, name: str, action_definition: dict[str, Any]) -> dict[str, Any]:\n        '''Create an action (success or failure) that complies with the Arazzo schema.\n        Args:\n            action_type: The type of action ('end', 'goto', or 'retry').\n            name: The name of the action.\n            action_definition: Additional properties for the action.\n        Returns:\n            A valid action object according to the Arazzo schema.\n        '''\n        pass\n    @staticmethod\n    def build_default_components() -> dict[str, Any]:\n        '''Build the default components section for an Arazzo specification.\n        Returns:\n            A dictionary containing the components section.\n        '''\n        pass",
    "snippet_id": "snippet_305"
  },
  {
    "class_name": "arazzo_generator.generator.output_mapping_validator.OutputMappingValidator",
    "skeleton": "\nclass OutputMappingValidator:\n    '''Validates and fixes output mappings in Arazzo workflows.'''\n    @staticmethod\n    def validate_output_mappings(workflow: dict[str, Any], openapi_spec: dict[str, Any], endpoints: dict[str, dict[str, Any]]) -> dict[str, Any]:\n        '''Validate and fix output mappings in a workflow.\n        This function checks all output mappings in a workflow against the\n        corresponding response schemas from the OpenAPI spec and fixes any\n        inconsistencies.\n        Args:\n            workflow: The workflow to validate.\n            openapi_spec: The OpenAPI specification.\n            endpoints: Dictionary of endpoints from the OpenAPI parser.\n        Returns:\n            The validated and fixed workflow.\n        '''\n        pass\n    @staticmethod\n    def _get_endpoint_for_step(step: dict[str, Any], endpoints: dict[str, dict[str, Any]]) -> dict[str, Any] | None:\n        '''Get the endpoint data for a step.\n        Args:\n            step: The step to get the endpoint for.\n            endpoints: Dictionary of endpoints from the OpenAPI parser.\n        Returns:\n            The endpoint data or None if not found.\n        '''\n        pass\n    @staticmethod\n    def _extract_response_info(endpoint_data: dict[str, Any]) -> tuple[dict[str, Any], dict[str, Any]]:\n        '''Extract response schema and headers from endpoint data.\n        Args:\n            endpoint_data: The endpoint data from the OpenAPI parser.\n        Returns:\n            A tuple of (response_schema, response_headers).\n        '''\n        pass\n    @staticmethod\n    def _validate_step_outputs(outputs: dict[str, str], schema: dict[str, Any], headers: dict[str, Any]) -> dict[str, str]:\n        '''Validate and fix output mappings for a step.\n        Args:\n            outputs: The output mappings to validate.\n            schema: The response schema.\n            headers: The response headers.\n        Returns:\n            The validated and fixed output mappings.\n        '''\n        pass\n    @staticmethod\n    def _normalize_property_path(path: str) -> str:\n        '''Normalize a property path by removing array indices.\n        Args:\n            path: The property path to normalize.\n        Returns:\n            The normalized property path.\n        '''\n        pass\n    @staticmethod\n    def _find_best_match(target: str, candidates: list[str]) -> str | None:\n        '''Find the best matching string from a list of candidates using sequence matching.\n        Args:\n            target: The target string to match.\n            candidates: List of candidate strings.\n        Returns:\n            The best matching string or None if candidates is empty.\n        '''\n        pass\n    @staticmethod\n    def _find_best_property_match(output_name: str, flat_schema: dict[str, str]) -> str | None:\n        '''Find the best matching property in the schema for an output name.\n        Args:\n            output_name: The output name provided by the LLM.\n            flat_schema: The flattened schema with property paths.\n        Returns:\n            The path to the matching property, or None if no match is found.\n        '''\n        pass\n    @staticmethod\n    def _flatten_schema(properties: dict[str, Any], prefix: str='') -> dict[str, str]:\n        '''Flatten a nested schema into a dictionary of property paths.\n        Args:\n            properties: The properties object from the schema.\n            prefix: The prefix for nested properties.\n        Returns:\n            A dictionary mapping property names to their paths.\n        '''\n        pass",
    "snippet_id": "snippet_306"
  },
  {
    "class_name": "arazzo_generator.generator.reference_validator.ReferenceValidator",
    "skeleton": "\nclass ReferenceValidator:\n    '''Validates and fixes step references in Arazzo workflows.'''\n    @staticmethod\n    def validate_step_references(workflow: dict[str, Any]) -> dict[str, Any]:\n        '''Validate and fix step references in a workflow.\n        This function checks all references to steps and their outputs in a workflow\n        and fixes any inconsistencies.\n        Args:\n            workflow: The workflow to validate.\n        Returns:\n            The validated and fixed workflow.\n        '''\n        pass\n    @staticmethod\n    def _find_best_match(target: str, candidates: list[str]) -> str | None:\n        '''Find the best matching string from a list of candidates using sequence matching.\n        Args:\n            target: The target string to match.\n            candidates: List of candidate strings.\n        Returns:\n            The best matching string or None if candidates is empty.\n        '''\n        pass\n    @staticmethod\n    def _fix_parameter_references(workflow: dict[str, Any], valid_step_ids: set[str], step_outputs: dict[str, Any]) -> None:\n        '''Fix parameter references in a workflow.\n        Args:\n            workflow: The workflow to fix.\n            valid_step_ids: Set of valid step IDs.\n            step_outputs: Dictionary mapping step IDs to their outputs.\n        '''\n        pass\n    @staticmethod\n    def _fix_request_body_references(workflow: dict[str, Any], valid_step_ids: set[str], step_outputs: dict[str, Any]) -> None:\n        '''Fix request body references in a workflow.\n        Args:\n            workflow: The workflow to fix.\n            valid_step_ids: Set of valid step IDs.\n            step_outputs: Dictionary mapping step IDs to their outputs.\n        '''\n        pass",
    "snippet_id": "snippet_307"
  },
  {
    "class_name": "arazzo_runner.blob_store.InMemoryBlobStore",
    "skeleton": "\nclass InMemoryBlobStore:\n    '''In-memory blob storage for testing and short-lived scenarios.'''\n\n    def __init__(self, max_size: int=100):\n        '''\n        Initialize in-memory blob store.\n        Args:\n            max_size: Maximum number of blobs to keep in memory\n        '''\n        pass\n\n    def _evict_if_needed(self) -> None:\n        '''Evict oldest blobs if we've exceeded max_size.'''\n        pass\n\n    def save(self, data: bytes, meta: dict[str, Any]) -> str:\n        '''Save binary data with metadata.'''\n        pass\n\n    def load(self, blob_id: str) -> bytes:\n        '''Load binary data by blob ID.'''\n        pass\n\n    def info(self, blob_id: str) -> dict[str, Any]:\n        '''Get metadata for a blob.'''\n        pass\n\n    def delete(self, blob_id: str) -> None:\n        '''Delete a blob and its metadata.'''\n        pass",
    "snippet_id": "snippet_308"
  },
  {
    "class_name": "arazzo_runner.blob_store.LocalFileBlobStore",
    "skeleton": "\nclass LocalFileBlobStore:\n    '''File-based blob storage implementation.'''\n\n    def __init__(self, root: str | None=None, janitor_after_h: int=24):\n        '''\n        Initialize the local file blob store.\n        Args:\n            root: Root directory for blob storage (defaults to env BLOB_STORE_PATH or CWD/blobs).            janitor_after_h: Number of hours after which blobs are eligible for cleanup\n        '''\n        pass\n\n    def _path(self, blob_id: str) -> str:\n        '''Get the file path for a blob's binary data.'''\n        pass\n\n    def _meta_path(self, blob_id: str) -> str:\n        '''Get the file path for a blob's metadata.'''\n        pass\n\n    def save(self, data: bytes, meta: dict[str, Any]) -> str:\n        '''\n        Save binary data with metadata.\n        Args:\n            data: Binary data to store\n            meta: Metadata dictionary\n        Returns:\n            Unique blob ID for the stored data\n        '''\n        pass\n\n    def load(self, blob_id: str) -> bytes:\n        '''\n        Load binary data by blob ID.\n        Args:\n            blob_id: The blob ID to load\n        Returns:\n            Binary data\n        Raises:\n            FileNotFoundError: If the blob doesn't exist\n        '''\n        pass\n\n    def info(self, blob_id: str) -> dict[str, Any]:\n        '''\n        Get metadata for a blob.\n        Args:\n            blob_id: The blob ID\n        Returns:\n            Metadata dictionary\n        Raises:\n            FileNotFoundError: If the blob doesn't exist\n        '''\n        pass\n\n    def delete(self, blob_id: str) -> None:\n        '''\n        Delete a blob and its metadata.\n        Args:\n            blob_id: The blob ID to delete\n        '''\n        pass\n\n    def purge_old(self) -> None:\n        '''Remove blobs older than the janitor threshold.'''\n        pass",
    "snippet_id": "snippet_309"
  },
  {
    "class_name": "in_memory_cache.InMemoryCache",
    "skeleton": "\nclass InMemoryCache:\n    '''A thread-safe Singleton class to manage cache data.\n    Ensures only one instance of the cache exists across the application.\n    '''\n\n    def __new__(cls):\n        '''Override __new__ to control instance creation (Singleton pattern).\n        Uses a lock to ensure thread safety during the first instantiation.\n        Returns:\n            The singleton instance of InMemoryCache.\n        '''\n        pass\n\n    def __init__(self):\n        '''Initialize the cache storage.\n        Uses a flag (_initialized) to ensure this logic runs only on the very first\n        creation of the singleton instance.\n        '''\n        pass\n\n    def set(self, key: str, value: Any, ttl: Optional[int]=None) -> None:\n        '''Set a key-value pair.\n        Args:\n            key: The key for the data.\n            value: The data to store.\n            ttl: Time to live in seconds. If None, data will not expire.\n        '''\n        pass\n\n    def get(self, key: str, default: Any=None) -> Any:\n        '''Get the value associated with a key.\n        Args:\n            key: The key for the data within the session.\n            default: The value to return if the session or key is not found.\n        Returns:\n            The cached value, or the default value if not found.\n                        '''\n                        pass\n\n    def delete(self, key: str) -> None:\n        '''Delete a specific key-value pair from a cache.\n        Args:\n            key: The key to delete.\n        Returns:\n            True if the key was found and deleted, False otherwise.\n        '''\n        pass\n\n    def clear(self) -> bool:\n        '''Remove all data.\n        Returns:\n            True if the data was cleared, False otherwise.\n        '''\n        pass",
    "snippet_id": "snippet_310"
  },
  {
    "class_name": "search.SearchTool",
    "skeleton": "\nclass SearchTool:\n    '''Tool for performing web searches using SerpAPI'''\n\n    def __init__(self, api_key: Optional[str]=None):\n        '''Initialize search tool with API key\n        Args:\n            api_key (str, optional): SerpAPI key. Defaults to env var SERPAPI_API_KEY.\n        '''\n        pass\n\n    def search(self, query: str, num_results: int=5) -> List[Dict]:\n        '''Perform Google search via SerpAPI\n        Args:\n            query (str): Search query\n            num_results (int, optional): Number of results to return. Defaults to 5.\n        Returns:\n            List[Dict]: Search results with title, snippet, and link\n        '''\n        pass",
    "snippet_id": "snippet_311"
  },
  {
    "class_name": "nextcloud_mcp_server.controllers.notes_search.NotesSearchController",
    "skeleton": "\nclass NotesSearchController:\n    '''Handles notes search logic and scoring.'''\n\n    def search_notes(self, notes: List[Dict[str, Any]], query: str) -> List[Dict[str, Any]]:\n        '''\n        Search notes using token-based matching with relevance ranking.\n        Returns notes sorted by relevance score.\n        '''\n        pass\n\n    def _process_query(self, query: str) -> List[str]:\n        '''\n        Tokenize and normalize the search query.\n        '''\n        pass\n\n    def _process_note_content(self, note: Dict[str, Any]) -> tuple[List[str], List[str]]:\n        '''\n        Tokenize and normalize note title and content.\n        '''\n        pass\n\n    def _calculate_score(self, query_tokens: List[str], title_tokens: List[str], content_tokens: List[str]) -> float:\n        '''\n        Calculate a relevance score for a note based on query tokens.\n        '''\n        pass",
    "snippet_id": "snippet_312"
  },
  {
    "class_name": "bedrock_server_manager.core.manager_mixins.content_mixin.ContentMixin",
    "skeleton": "\nclass ContentMixin:\n    '''\n    Mixin class for BedrockServerManager that handles global content management.\n        '''\n\n    def _list_content_files(self, sub_folder: str, extensions: List[str]) -> List[str]:\n        '''\n        Internal helper to list files with specified extensions from a sub-folder\n        within the global content directory.\n        This method constructs a path to ``<content_dir>/<sub_folder>``, then\n        scans this directory for files matching any of the provided ``extensions``.\n        The global content directory is defined by ``settings['paths.content']``\n        and cached in :attr:`._content_dir`.\n        Args:\n            sub_folder (str): The name of the sub-folder within the global content\n                directory to scan (e.g., \"worlds\", \"addons\").\n            extensions (List[str]): A list of file extensions to search for.\n                Extensions should include the leading dot (e.g., ``[\".mcworld\"]``,\n                ``[\".mcpack\", \".mcaddon\"]``).\n        Returns:\n            List[str]: A sorted list of absolute paths to the files found.\n            Returns an empty list if the target directory does not exist or no\n            matching files are found.\n        Raises:\n            AppFileNotFoundError: If the main content directory (:attr:`._content_dir`)\n                is not configured or does not exist as a directory.\n            FileOperationError: If an OS-level error occurs while scanning the\n                directory (e.g., permission issues).\n        '''\n        pass\n\n    def list_available_worlds(self) -> List[str]:\n        '''Lists available ``.mcworld`` template files from the global content directory.\n        This method scans the ``worlds`` sub-folder within the application's\n        global content directory (see :attr:`._content_dir` and\n        ``settings['paths.content']``) for files with the ``.mcworld`` extension.\n        It relies on :meth:`._list_content_files` for the actual scanning.\n        These ``.mcworld`` files typically represent world templates that can be\n        imported to create new server worlds or overwrite existing ones.\n        Returns:\n            List[str]: A sorted list of absolute paths to all found ``.mcworld`` files.\n            Returns an empty list if the directory doesn't exist or no ``.mcworld``\n            files are present.\n        Raises:\n            AppFileNotFoundError: If the main content directory is not configured\n                or found (from :meth:`._list_content_files`).\n            FileOperationError: If an OS error occurs during directory scanning\n                (from :meth:`._list_content_files`).\n        '''\n        pass\n\n    def list_available_addons(self) -> List[str]:\n        '''Lists available addon files (``.mcpack``, ``.mcaddon``) from the global content directory.\n        This method scans the ``addons`` sub-folder within the application's\n        global content directory (see :attr:`._content_dir` and\n        ``settings['paths.content']``) for files with ``.mcpack`` or\n        ``.mcaddon`` extensions. It uses :meth:`._list_content_files` for scanning.\n        These files represent behavior packs, resource packs, or bundled addons\n        that can be installed onto server instances.\n        Returns:\n            List[str]: A sorted list of absolute paths to all found ``.mcpack``\n            and ``.mcaddon`` files. Returns an empty list if the directory\n            doesn't exist or no such files are present.\n        Raises:\n            AppFileNotFoundError: If the main content directory is not configured\n                or found (from :meth:`._list_content_files`).\n            FileOperationError: If an OS error occurs during directory scanning\n                (from :meth:`._list_content_files`).\n        '''\n        pass",
    "snippet_id": "snippet_313"
  },
  {
    "class_name": "bedrock_server_manager.core.manager_mixins.discovery_mixin.DiscoveryMixin",
    "skeleton": "\nclass DiscoveryMixin:\n    '''\n    Mixin class for BedrockServerManager that handles server discovery and validation.\n        '''\n\n    def validate_server(self, server_name: str, app_context: Optional[AppContext]=None) -> bool:\n        '''Validates if a given server name corresponds to a valid installation.\n        This method checks for the existence and basic integrity of a server\n        installation. It instantiates a :class:`~.core.bedrock_server.BedrockServer`\n        object for the given ``server_name`` and then calls its\n        :meth:`~.core.bedrock_server.BedrockServer.is_installed` method.\n        Any exceptions raised during the instantiation or validation process (e.g.,\n        :class:`~.error.InvalidServerNameError`, :class:`~.error.ConfigurationError`)\n        are caught, logged as a warning, and result in a ``False`` return value,\n        making this a safe check.\n        Args:\n            server_name (str): The name of the server to validate. This should\n                correspond to a subdirectory within the main server base directory.\n        Returns:\n            bool: ``True`` if the server exists and is a valid installation\n            (i.e., its directory and executable are found), ``False`` otherwise.\n        Raises:\n            MissingArgumentError: If ``server_name`` is an empty string.\n        '''\n        pass\n\n    def get_servers_data(self, app_context: Optional['AppContext']=None) -> Tuple[List[Dict[str, Any]], List[str]]:\n        '''Discovers and retrieves status data for all valid server instances.\n        This method scans the main server base directory (defined by\n        ``settings['paths.servers']``) for subdirectories that represent server\n        installations. For each potential server, it:\n            1. Instantiates a :class:`~.core.bedrock_server.BedrockServer` object.\n            2. Validates the installation using the server's :meth:`~.core.bedrock_server.BedrockServer.is_installed` method.\n            3. If valid, it queries the server's status and version using\n               :meth:`~.core.bedrock_server.BedrockServer.get_status` and\n               :meth:`~.core.bedrock_server.BedrockServer.get_version`.\n        Errors encountered while processing individual servers are collected and\n        returned separately, allowing the method to succeed even if some server\n        directories are corrupted or misconfigured. The final list of server\n        data is sorted alphabetically by server name.\n        Returns:\n            Tuple[List[Dict[str, Any]], List[str]]: A tuple containing two lists:\n                - The first list contains dictionaries, one for each successfully\n                  processed server. Each dictionary has the keys:\n                    - ``\"name\"`` (str): The name of the server.\n                    - ``\"status\"`` (str): The server's current status (e.g., \"RUNNING\", \"STOPPED\").\n                    - ``\"version\"`` (str): The detected version of the server.\n                - The second list contains string messages describing any errors that\n                  occurred while processing specific server candidates.\n        Raises:\n            AppFileNotFoundError: If the main server base directory\n                (``settings['paths.servers']``) is not configured or does not exist.\n        '''\n        pass",
    "snippet_id": "snippet_314"
  },
  {
    "class_name": "bedrock_server_manager.core.manager_mixins.player_mixin.PlayerMixin",
    "skeleton": "\nclass PlayerMixin:\n    '''\n    Mixin class for BedrockServerManager that handles player database management.\n        '''\n\n    def parse_player_cli_argument(self, player_string: str) -> None:\n        '''Parses a comma-separated string of 'player_name:xuid' pairs and saves them to the database.\n        This utility method is designed to process player data provided as a\n        single string, typically from a command-line argument. Each player entry\n        in the string should be in the format \"PlayerName:PlayerXUID\", and multiple\n        entries should be separated by commas. Whitespace around names, XUIDs,\n        commas, and colons is generally handled.\n        Example:\n            ``\"Player One:12345, PlayerTwo:67890\"``\n        Args:\n            player_string (str): The comma-separated string of player data.\n                If empty or not a string, an empty list is returned.\n        Raises:\n            UserInputError: If any player pair within the string does not conform\n                to the \"name:xuid\" format, or if a name or XUID is empty after stripping.\n        '''\n        pass\n\n    def save_player_data(self, players_data: List[Dict[str, str]]) -> int:\n        '''Saves or updates player data in the database.\n        This method merges the provided ``players_data`` with any existing player\n        data in the database.\n        The merging logic is as follows:\n            - If a player's XUID from ``players_data`` already exists in the database,\n              their entry (name and XUID) is updated if different.\n            - If a player's XUID is new, their entry is added to the database.\n        Args:\n            players_data (List[Dict[str, str]]): A list of player dictionaries.\n                Each dictionary must contain string values for ``\"name\"`` and ``\"xuid\"`` keys.\n                Both name and XUID must be non-empty.\n        Returns:\n            int: The total number of players that were newly added or had their\n            existing entry updated. Returns 0 if no changes were made.\n        Raises:\n            UserInputError: If ``players_data`` is not a list, or if any dictionary\n                within it does not conform to the required format (missing keys,\n                non-string values, or empty name/XUID).\n        '''\n        pass\n\n    def get_known_players(self) -> List[Dict[str, str]]:\n        '''Retrieves all known players from the database.\n        Returns:\n            List[Dict[str, str]]: A list of player dictionaries, where each\n            dictionary typically contains ``\"name\"`` and ``\"xuid\"`` keys.\n        '''\n        pass\n\n    def discover_and_store_players_from_all_server_logs(self, app_context: Optional[AppContext]=None) -> Dict[str, Any]:\n        '''Scans all server logs for player data and updates the central player database.\n        This comprehensive method performs the following actions:\n            1. Iterates through all subdirectories within the application's base server\n               directory (defined by ``settings['paths.servers']``).\n            2. For each subdirectory, it attempts to instantiate a\n               :class:`~.core.bedrock_server.BedrockServer` object.\n            3. If the server instance is valid and installed, it calls the server's\n               :meth:`~.core.server.player_mixin.ServerPlayerMixin.scan_log_for_players`\n               method to extract player names and XUIDs from its logs.\n            4. All player data discovered from all server logs is aggregated.\n            5. Unique player entries (based on XUID) are then saved to the database\n               using :meth:`.save_player_data`.\n        Args:\n            None\n        Returns:\n            Dict[str, Any]: A dictionary summarizing the discovery and saving operation,\n            containing the following keys:\n                - ``\"total_entries_in_logs\"`` (int): The total number of player entries\n                  (possibly non-unique) found across all server logs.\n                - ``\"unique_players_submitted_for_saving\"`` (int): The number of unique\n                  player entries (by XUID) that were attempted to be saved.\n                - ``\"actually_saved_or_updated_in_db\"`` (int): The number of players\n                  that were newly added or updated in the database\n                  by the :meth:`.save_player_data` call.\n                - ``\"scan_errors\"`` (List[Dict[str, str]]): A list of dictionaries,\n                  where each entry represents an error encountered while scanning a\n                  specific server's logs or saving the global player DB. Each error\n                  dictionary contains ``\"server\"`` (str, server name or \"GLOBAL_PLAYER_DB\")\n                  and ``\"error\"`` (str, error message).\n        Raises:\n            AppFileNotFoundError: If the main server base directory\n                (``settings['paths.servers']``) is not configured or does not exist.\n            FileOperationError: If the final save operation to the database\n                (via :meth:`.save_player_data`) fails.\n                Note that errors during individual server log scans are caught and\n                reported in the ``\"scan_errors\"`` part of the return value.\n        '''\n        pass",
    "snippet_id": "snippet_315"
  },
  {
    "class_name": "bedrock_server_manager.core.manager_mixins.system_mixin.SystemMixin",
    "skeleton": "\nclass SystemMixin:\n    '''\n    Mixin class for BedrockServerManager that handles system information and capabilities.\n        '''\n\n    def get_app_version(self) -> str:\n        '''Returns the application's version string.\n        The version is typically derived from the application's settings\n        during manager initialization and stored in :attr:`._app_version`.\n        Returns:\n            str: The application version string (e.g., \"1.2.3\").\n        '''\n        pass\n\n    def get_os_type(self) -> str:\n        '''Returns the current operating system type string.\n        This method uses :func:`platform.system()` to determine the OS.\n        Common return values include \"Linux\", \"Windows\", \"Darwin\" (for macOS).\n        Returns:\n            str: A string representing the current operating system.\n        '''\n        pass\n\n    def _check_system_capabilities(self) -> Dict[str, bool]:\n        '''\n        Internal helper to check for the availability of external OS-level\n        dependencies and report their status.\n        This method is called during :meth:`.__init__` to determine if optional\n        system utilities, required for certain features, are present.\n        Currently, it checks for:\n            - 'scheduler': ``crontab`` (Linux) or ``schtasks`` (Windows).\n            - 'service_manager': ``systemctl`` (Linux) or ``sc.exe`` (Windows).\n        The results are stored in the :attr:`.capabilities` dictionary.\n        Returns:\n            Dict[str, bool]: A dictionary where keys are capability names\n            (e.g., \"scheduler\", \"service_manager\") and values are booleans\n            indicating if the corresponding utility was found.\n        '''\n        pass\n\n    def _log_capability_warnings(self) -> None:\n        '''\n        Internal helper to log warnings if essential system capabilities are missing.\n        Called during :meth:`.__init__` after :meth:`._check_system_capabilities`.\n        It inspects the :attr:`.capabilities` attribute and logs a warning message\n        for each capability that is found to be unavailable. This informs the user\n        that certain application features might be disabled or limited.\n        '''\n        pass\n    @property\n    def can_schedule_tasks(self) -> bool:\n        '''bool: Indicates if a system task scheduler (``crontab`` or ``schtasks``) is available.\n        This property reflects the 'scheduler' capability checked during manager\n        initialization by :meth:`._check_system_capabilities`. If ``True``,\n        features related to scheduled tasks (like automated backups) can be\n        expected to work.\n        '''\n        pass\n    @property\n    def can_manage_services(self) -> bool:\n        '''bool: Indicates if a system service manager (``systemctl`` or ``sc.exe``) is available.\n        This property reflects the 'service_manager' capability checked during\n        manager initialization by :meth:`._check_system_capabilities`. If ``True``,\n        features related to managing system services (for the Web UI or game servers)\n        can be expected to work.\n        '''\n        pass",
    "snippet_id": "snippet_316"
  },
  {
    "class_name": "bedrock_server_manager.core.manager_mixins.web_process_mixin.WebProcessMixin",
    "skeleton": "\nclass WebProcessMixin:\n    '''\n    Mixin class for BedrockServerManager that handles direct Web UI process management.\n        '''\n\n    def start_web_ui_direct(self, app_context: AppContext, host: Optional[Union[str, List[str]]]=None, debug: bool=False, threads: Optional[int]=None) -> None:\n        '''Starts the Web UI application directly in the current process (blocking).\n        This method is intended for scenarios where the Web UI is launched with\n        the ``--mode direct`` command-line argument. It dynamically imports and\n        calls the :func:`~.web.app.run_web_server` function, which in turn\n        starts the Uvicorn server hosting the FastAPI application.\n        .. note::\n            This is a blocking call and will occupy the current process until the\n            web server is shut down.\n        Args:\n            host (Optional[Union[str, List[str]]]): The host address or list of\n                addresses for the web server to bind to. Passed directly to\n                :func:`~.web.app.run_web_server`. Defaults to ``None``.\n            debug (bool): If ``True``, runs the underlying Uvicorn/FastAPI app\n                in debug mode (e.g., with auto-reload). Passed directly to\n                :func:`~.web.app.run_web_server`. Defaults to ``False``.\n            threads (Optional[int]): Specifies the number of worker processes for Uvicorn\n                Only used for Windows Service\n        Raises:\n            RuntimeError: If :func:`~.web.app.run_web_server` raises a RuntimeError\n                (e.g., missing authentication environment variables).\n            ImportError: If the web application components (e.g.,\n                :func:`~.web.app.run_web_server`) cannot be imported.\n            Exception: Re-raises other exceptions from :func:`~.web.app.run_web_server`\n                if Uvicorn fails to start.\n        '''\n        pass\n\n    def get_web_ui_pid_path(self) -> str:\n        '''Returns the absolute path to the PID file for the detached Web UI server.\n        The PID file is typically stored in the application's configuration directory\n        (:attr:`._config_dir`) with the filename defined by\n        :attr:`._WEB_SERVER_PID_FILENAME`.\n        Returns:\n            str: The absolute path to the Web UI's PID file.\n        '''\n        pass\n\n    def get_web_ui_expected_start_arg(self) -> List[str]:\n        '''Returns the list of arguments used to identify a detached Web UI server process.\n        These arguments (defined by :attr:`._WEB_SERVER_START_ARG`) are typically\n        used by process management utilities to find and identify the correct\n        Web UI server process when it's run in a detached or background mode.\n        Returns:\n            List[str]: A list of command-line arguments.\n        '''\n        pass\n\n    def get_web_ui_executable_path(self) -> str:\n        '''Returns the path to the main application executable used for starting the Web UI.\n        This path, stored in :attr:`._expath`, is essential for constructing\n        commands to start the Web UI, especially for system services.\n        Returns:\n            str: The path to the application executable.\n        Raises:\n            ConfigurationError: If the application executable path (:attr:`._expath`)\n                is not configured or is empty.\n        '''\n        pass",
    "snippet_id": "snippet_317"
  },
  {
    "class_name": "bedrock_server_manager.plugins.api_bridge.PluginAPI",
    "skeleton": "\nclass PluginAPI:\n    '''Provides a safe, dynamic, and decoupled interface for plugins to access core APIs.\n    An instance of this class is passed to each plugin upon its initialization\n    by the `PluginManager`. Plugins use this instance (typically `self.api`)\n    to call registered core functions (e.g., `self.api.start_server(...)`)\n    without needing to import them directly, thus avoiding circular dependencies\n    and promoting a cleaner architecture.\n    This class also provides methods for plugins to interact with the custom\n    plugin event system, allowing them to listen for and send events to\n    other plugins.\n    '''\n\n    def __init__(self, plugin_name: str, plugin_manager: 'PluginManager', app_context: Optional['AppContext']):\n        '''Initializes the PluginAPI instance for a specific plugin.\n        This constructor is called by the `PluginManager` when a plugin is\n        being loaded and instantiated.\n        Args:\n            plugin_name (str): The name of the plugin for which this API\n                instance is being created. This is used for logging and context.\n            plugin_manager (PluginManager): A reference to the `PluginManager`\n                instance. This is used to delegate custom event operations\n                (listening and sending) to the manager.\n            app_context (Optional[AppContext]): A reference to the global\n                application context, providing access to shared application state\n                and managers. This can be `None` during initial setup phases.\n        '''\n        pass\n    @property\n    def app_context(self) -> 'AppContext':\n        '''Provides direct access to the application's context.\n        This property returns the central `AppContext` object, which holds\n        instances of key application components like the `Settings` manager,\n        the `BedrockServerManager`, and the `PluginManager` itself.\n        Example:\n            ```python\n            # In a plugin method:\n            settings = self.api.app_context.settings\n            server_manager = self.api.app_context.manager\n            all_servers = server_manager.get_all_servers()\n            ```\n        Returns:\n            AppContext: The application context instance.\n        Raises:\n            RuntimeError: If the application context has not been set on this\n                `PluginAPI` instance yet. This would indicate an improper\n                initialization sequence in the application startup.\n        '''\n        pass\n\n    def __getattr__(self, name: str) -> Callable[..., Any]:\n        '''Dynamically retrieves a registered core API function when accessed as an attribute.\n        This magic method is the cornerstone of the API bridge's functionality.\n        When a plugin executes code like `self.api.some_function_name()`, Python\n        internally calls this `__getattr__` method with `name` set to\n        `'some_function_name'`. This method then looks up `name` in the\n        `_api_registry`.\n        It also inspects the signature of the retrieved function. If the function\n        has a parameter named `app_context`, this method automatically provides\n        the `AppContext` to it, simplifying the function's implementation for\n        both the core API and the plugin calling it.\n        Args:\n            name (str): The name of the attribute (API function) being accessed\n                by the plugin.\n        Returns:\n            Callable[..., Any]: The callable API function retrieved from the\n            `_api_registry` corresponding to the given `name`. If the function\n            expects an `app_context`, a partial function with the context already\n            bound is returned.\n        Raises:\n            AttributeError: If the function `name` has not been registered in\n                the `_api_registry`, indicating the plugin is trying to access\n                a non-existent or unavailable API function.\n        '''\n        pass\n\n    def list_available_apis(self) -> List[Dict[str, Any]]:\n        '''\n        Returns a detailed list of all registered API functions, including\n        their names, parameters, and documentation.\n        This method can be useful for plugins that need to introspect the\n        available core functionalities at runtime, or for debugging purposes\n        to verify which APIs are exposed and how to call them.\n        Returns:\n            List[Dict[str, Any]]: A list of dictionaries, where each dictionary\n            describes a registered API function.\n        '''\n        pass\n\n    def listen_for_event(self, event_name: str, callback: Callable[..., None]):\n        '''Registers a callback to be executed when a specific custom plugin event occurs.\n        This method allows a plugin to subscribe to custom events that may be\n        triggered by other plugins via `send_event()`. The `PluginManager`\n        handles the actual registration and dispatch of these events.\n        Args:\n            event_name (str): The unique name of the custom event to listen for\n                (e.g., \"myplugin:my_custom_event\"). It is a recommended practice\n                to namespace event names with the originating plugin's name or\n                a unique prefix to avoid collisions.\n            callback (Callable[..., None]): The function or method within the\n                listening plugin that should be called when the specified event\n                is triggered. This callback will receive any `*args` and\n                `**kwargs` passed during the `send_event` call, plus an\n                additional `_triggering_plugin` keyword argument (str)\n                indicating the name of the plugin that sent the event.\n        '''\n        pass\n\n    def send_event(self, event_name: str, *args: Any, **kwargs: Any):\n        '''Triggers a custom plugin event, notifying all registered listeners.\n        This method allows a plugin to broadcast a custom event to other plugins\n        that have registered a listener for it using `listen_for_event()`.\n        The `PluginManager` handles the dispatch of this event to all\n        subscribed callbacks.\n        Args:\n            event_name (str): The unique name of the custom event to trigger.\n                This should match the `event_name` used by listening plugins.\n            *args (Any): Positional arguments to pass to the event listeners'\n                callback functions.\n            **kwargs (Any): Keyword arguments to pass to the event listeners'\n                callback functions.\n        '''\n        pass\n\n    def get_plugin_html_pages(self) -> List[Dict[str, str]]:\n        '''\n        Retrieves a list of plugin routes that are tagged for HTML rendering.\n        This allows the main application (or other plugins, if appropriate)\n        to discover web pages provided by plugins that are intended to be\n        directly accessible or linked in a UI.\n        Returns:\n            List[Dict[str, str]]: A list of dictionaries, where each dictionary\n            contains 'name' and 'path' for a route tagged for HTML rendering.\n            The 'name' is a user-friendly display name for the link, and 'path'\n            is the URL path for the route.\n        '''\n        pass",
    "snippet_id": "snippet_318"
  },
  {
    "class_name": "bedrock_server_manager.web.tasks.TaskManager",
    "skeleton": "\nclass TaskManager:\n    '''Manages background tasks using a thread pool.'''\n\n    def __init__(self, max_workers: Optional[int]=None):\n        '''Initializes the TaskManager and the thread pool executor.'''\n        pass\n\n    def _update_task(self, task_id: str, status: str, message: str, result: Optional[Any]=None):\n        '''Helper function to update the status of a task.'''\n        pass\n\n    def _task_done_callback(self, task_id: str, future: Future):\n        '''Callback function executed when a task completes.'''\n        pass\n\n    def run_task(self, target_function: Callable, *args: Any, **kwargs: Any) -> str:\n        '''\n        Submits a function to be run in the background.\n        Args:\n            target_function: The function to execute.\n            *args: Positional arguments for the target function.\n            **kwargs: Keyword arguments for the target function.\n        Returns:\n            The ID of the created task.\n        Raises:\n            RuntimeError: If shutdown has been initiated.\n        '''\n        pass\n\n    def get_task(self, task_id: str) -> Optional[Dict[str, Any]]:\n        '''\n        Retrieves the status of a task.\n        Args:\n            task_id: The ID of the task to retrieve.\n        Returns:\n            The task details or None if not found.\n        '''\n        pass\n\n    def shutdown(self):\n        '''Shuts down the thread pool and waits for all tasks to complete.'''\n        pass",
    "snippet_id": "snippet_319"
  },
  {
    "class_name": "multi_step_elicitation.WorkflowState",
    "skeleton": "@dataclass\nclass WorkflowState:\n    '''Maintains the state of a multi-step workflow.'''\n\n    def __post_init__(self):\n        '''Initialize step_history with current_step_id if empty.'''\n        pass\n\n    def add_response(self, step_id: str, response_values: Dict[str, Any]):\n        '''Add a response for a step.'''\n        pass\n\n    def get_all_responses(self) -> Dict[str, Any]:\n        '''Get all responses flattened into a single dictionary.'''\n        pass\n\n    def can_go_back(self) -> bool:\n        '''Check if the user can go back to a previous step.'''\n        pass\n\n    def go_back(self) -> Optional[str]:\n        '''Go back to the previous step.'''\n        pass",
    "snippet_id": "snippet_320"
  },
  {
    "class_name": "schema_registry_common.RegistryConfig",
    "skeleton": "@dataclass\nclass RegistryConfig:\n    '''Configuration for a Schema Registry instance.'''\n\n    def to_dict(self) -> Dict[str, Any]:\n        '''Convert to dictionary with sensitive data masked.'''\n        pass\n\n    def __repr__(self) -> str:\n        '''Safe representation without credentials.'''\n        pass\n\n    def __str__(self) -> str:\n        '''Safe string representation without credentials.'''\n        pass",
    "snippet_id": "snippet_321"
  },
  {
    "class_name": "smart_defaults_config.SmartDefaultsConfig",
    "skeleton": "@dataclass\nclass SmartDefaultsConfig:\n    '''Configuration for Smart Defaults system'''\n    @classmethod\n    def from_env(cls) -> 'SmartDefaultsConfig':\n        '''Create configuration from environment variables'''\n        pass\n    @classmethod\n    def from_file(cls, config_path: Path) -> 'SmartDefaultsConfig':\n        '''Load configuration from JSON file'''\n        pass\n\n    def to_file(self, config_path: Path):\n        '''Save configuration to JSON file'''\n        pass\n\n    def validate(self) -> List[str]:\n        '''Validate configuration and return list of issues'''\n        pass\n\n    def get_environment_defaults(self, environment: str) -> Dict[str, Any]:\n        '''Get defaults for a specific environment'''\n        pass\n\n    def should_learn_from_field(self, field_name: str) -> bool:\n        '''Check if learning should be enabled for a field'''\n        pass\n\n    def should_learn_from_context(self, context: str) -> bool:\n        '''Check if learning should be enabled for a context'''\n        pass",
    "snippet_id": "snippet_322"
  },
  {
    "class_name": "ovms.attribute_manager.AttributeManager",
    "skeleton": "\nclass AttributeManager:\n    '''Manager for entity attributes.'''\n\n    def __init__(self, config: Dict[str, Any]):\n        '''Initialize the attribute manager.'''\n        pass\n\n    def prepare_attributes(self, topic: str, category: str, parts: List[str], metric_info: Optional[Dict]=None) -> Dict[str, Any]:\n        '''Prepare entity attributes.'''\n        pass\n\n    def process_json_payload(self, payload: str, attributes: Dict[str, Any]) -> Dict[str, Any]:\n        '''Process JSON payload to extract additional attributes.'''\n        pass\n\n    def determine_entity_category(self, category: str) -> Optional[EntityCategory]:\n        '''Determine EntityCategory from attribute category.'''\n        pass\n\n    def get_gps_attributes(self, topic: str, payload: Any) -> Dict[str, Any]:\n        '''Extract and prepare GPS-related attributes.'''\n        pass",
    "snippet_id": "snippet_323"
  },
  {
    "class_name": "ovms.rate_limiter.CommandRateLimiter",
    "skeleton": "\nclass CommandRateLimiter:\n    '''Rate limiter for OVMS commands.\n    Prevents sending too many commands in a short period to avoid overwhelming the OVMS module.\n    '''\n\n    def __init__(self, max_calls: int=5, period: float=60.0):\n        '''Initialize the rate limiter.\n        Args:\n            max_calls: Maximum number of calls allowed per period\n            period: Time period in seconds\n        '''\n        pass\n\n    def can_call(self) -> bool:\n        '''Check if a call can be made without exceeding the rate limit.\n        Returns:\n            True if call is allowed, False if limit would be exceeded\n        '''\n        pass\n\n    def calls_remaining(self) -> int:\n        '''Get the number of calls remaining in the current period.\n        Returns:\n            Number of calls remaining\n        '''\n        pass\n\n    def time_to_next_call(self) -> float:\n        '''Get the time in seconds until the next call is allowed.\n        Returns:\n            Seconds until next call is allowed, or 0 if calls are available now\n        '''\n        pass",
    "snippet_id": "snippet_324"
  },
  {
    "class_name": "streetrace.agents.base_agent_loader.AgentInfo",
    "skeleton": "\nclass AgentInfo:\n    '''Agent information container supporting both Python and YAML agents.'''\n\n    def __init__(self, name: str, description: str, file_path: Path | None=None, module: 'ModuleType | None'=None, yaml_document: 'YamlAgentDocument | None'=None) -> None:\n        '''Initialize agent info.\n        Args:\n            name: Agent name\n            description: Agent description\n            file_path: Path to agent file/directory\n            module: Python module (for Python agents)\n            yaml_document: YAML agent document (for YAML agents)\n        '''\n        pass\n    @property\n    def kind(self) -> Literal['python', 'yaml']:\n        '''Get the definition type of the agent.'''\n        pass\n    @property\n    def path(self) -> str:\n        '''Get the definition path of the agent.'''\n        pass",
    "snippet_id": "snippet_325"
  },
  {
    "class_name": "streetrace.llm.model_factory.ModelFactory",
    "skeleton": "\nclass ModelFactory:\n    '''Factory class to create and manage models for the StreetRace application.\n    This class is responsible for:\n    1. Managing model configurations\n    2. Creating and caching LlmInterface instances\n    3. Providing access to underlying BaseLlm instances for agents\n    '''\n\n    def __init__(self, default_model_name: str | None, ui_bus: UiBus, args: 'Args') -> None:\n        '''Initialize the ModelFactory with model configuration and UI bus.\n        Args:\n            default_model_name: Name of the model to use\n            ui_bus: UI event bus for displaying messages to the user\n            args: Application arguments\n                        '''\n                        pass\n\n    def _setup_caching(self) -> None:\n        '''Set up Redis caching for LiteLLM.'''\n        pass\n\n    def get_llm_interface(self, model_name: str) -> LlmInterface:\n        '''Return the default model based on the configuration.\n        Returns:\n            LlmInterface instance.\n        '''\n        pass\n\n    def get_current_model(self) -> 'BaseLlm':\n        '''Return the default model based on the configuration.\n        Returns:\n            Either a string model name or a BaseLlm instance.\n        '''\n        pass",
    "snippet_id": "snippet_326"
  },
  {
    "class_name": "streetrace.ui.adk_event_renderer.EventRenderer",
    "skeleton": "\nclass EventRenderer:\n    '''Stateful renderer that groups function calls with their responses.'''\n\n    def __init__(self) -> None:\n        '''Initialize the event renderer.'''\n        pass\n\n    def render_event(self, obj: 'Event', console: 'Console') -> None:\n        '''Render the provided google.adk.events.Event to rich.console.'''\n        pass\n\n    def _flush_pending_function_call(self, console: 'Console') -> None:\n        '''Render any pending function call that hasn't been paired with a response.'''\n        pass\n\n    def _render_function_call_group(self, function_call: 'FunctionCall', response: dict[str, Any], console: 'Console') -> None:\n        '''Render function call and response together in a grouped panel.'''\n        pass",
    "snippet_id": "snippet_327"
  },
  {
    "class_name": "streetrace.ui.console_ui.StatusSpinner",
    "skeleton": "\nclass StatusSpinner:\n    '''Console Status, encapsulates rich.status.'''\n\n    def __init__(self, app_state: AppState, console: Console) -> None:\n        '''Initialize the instance and instantiate rich.status.\n        Args:\n            app_state: App State container.\n            console: The console instance to attach the spinner to.\n        '''\n        pass\n\n    def update_state(self) -> None:\n        '''Update status message.'''\n        pass\n\n    def __enter__(self) -> 'StatusSpinner':\n        '''Enter the context by starting the spinner.\n        Returns:\n            self, so that logging methods can be called within the context.\n        '''\n        pass\n\n    def __exit__(self, exc_type: type[BaseException] | None, exc_value: BaseException | None, traceback: TracebackType | None) -> None:\n        '''Exit the context by propagating the signal to the spinner.\n        Args:\n            exc_type: The type of exception raised (if any).\n            exc_value: The exception instance (if any).\n            traceback: The traceback object (if any).\n        '''\n        pass",
    "snippet_id": "snippet_328"
  },
  {
    "class_name": "air.api.vector_db.vectordb_registry.VectorDBRegistry",
    "skeleton": "class VectorDBRegistry:\n    '''\n    A global registry that keeps track of all vector db classes\n    (subclasses of BaseVectorDB).\n        '''\n    @classmethod\n    def register(cls, subclass):\n        '''\n        Register a subclass in the global registry. The key is the class name,\n        but you can choose any naming scheme (e.g., subclass.__name__ or\n        a subclass-level variable).\n        '''\n        pass\n    @classmethod\n    def get(cls, name):\n        '''\n        Retrieve a vector DB class by its name. Returns None if not found.\n        '''\n        pass\n    @classmethod\n    def all_vectordbs(cls):\n        '''\n        Return a list of all registered vector DB classes.\n        '''\n        pass",
    "snippet_id": "snippet_329"
  },
  {
    "class_name": "air.client.AIRefinery",
    "skeleton": "\nclass AIRefinery:\n    '''\n    A top-level client that exposes various sub-clients in a single interface,\n    operating synchronously.\n    Example usage:\n        client = AIRefinery(\n            api_key=\"...\",\n            base_url=\"...\",\n            default_headers={\"X-Client-Version\": \"1.2.3\"}\n        )\n        # Use chat\n        response = client.chat.completions.create(\n            model=\"model-name\", messages=[...]\n        )\n        # Use embeddings\n        embeddings_response = client.embeddings.create(\n            model=\"model-name\", input=[\"Hello\"]\n        )\n        # Use tts\n        tts_response = client.audio.speech.create(\n            model=\"model-name\",\n            input=\"Hello, this is a test of text-to-speech synthesis.\",\n            voice=\"en-US-JennyNeural\",\n            response_format=\"mp3\",  # Optional\n            speed=1.0  # Optional\n        # Use asr\n        asr_response = client.speech_to_text.create(\n            model=\"model-name\",\n            file=[\"audio1.wav\", \"audio2.wav\"]\n        )\n        # Use models\n        models_list = client.models.list()\n        # Use images\n        image_response = await client.images.generate(\n            prompt=\"A cute baby sea otter\", model=\"model-name\"\n        )\n        # Use knowledge\n        knowledge_client = client.knowledge\n        knowledge_client.create_project(config)\n        knowledge_response = await knowledge_client.document_processing.parse_documents(file_path='', model='')\n        # Attempting to use client.distiller will raise an exception\n        # (not supported in sync mode).\n    '''\n\n    def __init__(self, api_key: str, base_url: str='https://api.airefinery.accenture.com', default_headers: dict[str, str] | None=None, **kwargs):\n        '''\n        Initializes the synchronous unified client with sub-clients.\n        Args:\n            api_key (str): Your API key or token for authenticated requests.\n            base_url (str, optional): Base URL for your API endpoints.\n                Defaults to \"https://api.airefinery.accenture.com\".\n            default_headers (dict[str, str] | None): Headers that apply to\n                every request made by sub-clients (e.g., {\"X-Client-Version\": \"1.2.3\"}).\n            **kwargs: Additional configuration parameters, if any.\n                        '''\n                        pass\n\n    def _update_inference_endpoint(self):\n        '''\n        Ensures the base_url ends with the '/inference' suffix if necessary.\n        This method checks whether the `base_url` ends with \"/inference\".\n        If it does not, the method appends \"/inference\" to the URL and returns\n        the updated URL. This is particularly useful for ensuring compatibility\n        with authentication mechanisms like auth.openai(), where the \"/inference\"\n        suffix is automatically appended to the base URL.\n        Returns:\n            str: The base URL with \"/inference\" appended if it was not already present.\n        '''\n        pass\n    @property\n    def distiller(self):\n        '''\n        Distiller is only supported in the asynchronous client.\n        Accessing this property in the synchronous client will raise a NotImplementedError.\n        '''\n        pass",
    "snippet_id": "snippet_330"
  },
  {
    "class_name": "air.client.AsyncAIRefinery",
    "skeleton": "\nclass AsyncAIRefinery:\n    '''\n    A top-level client that exposes various sub-clients in a single interface,\n    operating asynchronously.\n    Example usage:\n        client = AsyncAIRefinery(\n            api_key=\"...\",\n            base_url=\"...\",\n            default_headers={\"X-Client-Version\": \"1.2.3\"}\n        )\n        # Use chat\n        response = await client.chat.completions.create(\n            model=\"model-name\", messages=[...]\n        )\n        # Use embeddings\n        embeddings_response = await client.embeddings.create(\n            model=\"model-name\", input=[\"Hello\"]\n        )\n        # Use tts\n        tts_response = await client.audio.speech.create(\n            model=\"model-name\",\n            input=\"Hello, this is a test of text-to-speech synthesis.\",\n            voice=\"en-US-JennyNeural\",\n            response_format=\"mp3\",  # Optional\n            speed=1.0  # Optional\n        # Use asr\n        asr_response = await client.audio.transcriptions.create(\n            model=\"model-name\",\n            file=file\n        )\n        # Use models\n        models_list = await client.models.list()\n        # Use distiller\n        async with client.distiller(project=\"...\", uuid=\"...\") as dc:\n            responses = await dc.query(query=\"hi\")\n            async for response in responses:\n                print(response)\n        # Use images\n        embeddings_response = await client.images.generate(\n            prompt=\"A cute baby sea otter\", model=\"model-name\"\n        )\n        # Use knowledge\n        graph_client = client.graph\n        graph_client.create_project(graph_config=...)\n        status = await graph_client.build(files_path=..)\n    '''\n\n    def __init__(self, api_key: str, base_url: str='https://api.airefinery.accenture.com', default_headers: dict[str, str] | None=None, **kwargs):\n        '''\n        Initializes the asynchronous unified client with sub-clients.\n        Args:\n            api_key (str): Your API key or token for authenticated requests.\n            base_url (str, optional): Base URL for your API endpoints.\n                Defaults to \"https://api.airefinery.accenture.com\".\n            default_headers (dict[str, str] | None): Headers that apply to\n                every request made by sub-clients (e.g., {\"X-Client-Version\": \"1.2.3\"}).\n            **kwargs: Additional configuration parameters, if any.\n                        '''\n                        pass\n\n    def _update_inference_endpoint(self):\n        '''\n        Ensures the base_url ends with the '/inference' suffix if necessary.\n        This method checks whether the `base_url` ends with \"/inference\".\n        If it does not, the method appends \"/inference\" to the URL and returns\n        the updated URL. This is particularly useful for ensuring compatibility\n        with authentication mechanisms like auth.openai(), where the \"/inference\"\n        suffix is automatically appended to the base URL.\n        Returns:\n            str: The base URL with \"/inference\" appended if it was not already present.\n        '''\n        pass",
    "snippet_id": "snippet_331"
  },
  {
    "class_name": "air.knowledge.client.KnowledgeClient",
    "skeleton": "\nclass KnowledgeClient:\n    '''\n    Synchronous client for knowledge services, including document processing.\n    '''\n\n    def __init__(self, base_url: str, api_key: str, default_headers: dict[str, str] | None=None):\n        '''\n        Initialize the sync knowledge client.\n        Args:\n            base_url (str): API base URL.\n            api_key (str): API key for authentication.\n            default_headers (dict, optional): Additional headers.\n                        '''\n                        pass\n\n    def get_graph(self):\n        '''\n        Knowledge Graph is not supported in sync mode.\n        Raises:\n            NotImplementedError\n        '''\n        pass",
    "snippet_id": "snippet_332"
  },
  {
    "class_name": "air.knowledge.graph_visualization.graph_display.GraphDisplay",
    "skeleton": "\nclass GraphDisplay:\n    '''\n    Base class that show processed graph\n        '''\n    @classmethod\n    def _map_edge_color(cls, graph: nx.Graph):\n        '''\n        Map the graphnode weight to a color.\n        Parameters:\n        - graph (nxGraph): networkx graph\n        Return:\n        - List: The list of color code\n        '''\n        pass\n    @classmethod\n    def show_undirected_graph(cls, graph, output_file: str, figsize: tuple[float, float]=(36.0, 20.0), default_node_sizes:\n        '''\n        Reads a .graphml file and displays the undirected graph.\n        Parameters:\n        - graph (str): graph to be visualized, in networkx.Graph format\n        - output_file (str): Path to the output graph image\n        '''\n        pass",
    "snippet_id": "snippet_333"
  },
  {
    "class_name": "air.knowledge.graph_visualization.graph_processing.GraphProcessing",
    "skeleton": "\nclass GraphProcessing:\n    '''\n    Class that performs graph clustering and visualization\n    '''\n    @classmethod\n    def cluster_graph(cls, graph: nx.Graph, max_community_size: int=1) -> pd.DataFrame:\n        '''\n        Method to perform hierarchical clustering of given graph,\n        until the resulting final communities each have a maximum of\n        `max_community_size` number of nodes.\n        Args:\n            graph (nx.Graph): graph to be clustered\n            max_community_size (int, optional): maximum number of nodes to be present\n            in a cluster/community. Defaults to 1.\n        Returns:\n            pd.DataFrame: Clustered communities in a DataFrame format.\n            Columns are - level, cluster_id, parent_cluster_id, nodes\n        '''\n        pass\n    @classmethod\n    def add_node_labels(cls, cluster_communities: pd.DataFrame, graph: nx.Graph):\n        '''\n        Method to add community name and community level\n        to the graph nodes\n        Args:\n            cluster_communities (pd.DataFrame): cluster info dataframe\n            graph (nx.Graph): networkx graph\n        Returns:\n            nx.Graph: updated graph with community and community level labels\n        '''\n        pass\n    @classmethod\n    def visualize_graph(cls, graph_path: str, graph_save_path: str, max_community_size: Union[int, None]=None, community_level: Union[int, None]=None, figsize: tuple[float, float]=(36.0, 20.0), default_node_sizes:\n        '''\n        Method to perform graph clustering, if specified,\n        and visualize the resulting graph using the GraphDisplay class.\n        Args:\n            graph_path (str): path to the graphml file\n            graph_save_path (str): path where the resulting graph visualization is to be saved\n            max_community_size (Union[int, None], optional): maximum number of nodes to be present\n            in a cluster/community. If set as None, clustering is skipped. Defaults to None.\n            community_level (Union[int, None], optional): Level of the community to retain.\n            Nodes of this community level are retained and then visualized.\n            If set to `-1` highest community level nodes are retained. Defaults to None.\n        '''\n        pass",
    "snippet_id": "snippet_334"
  },
  {
    "class_name": "air.knowledge.knowledge_graph.knowledge_graph_registry.KnowledgeGraphRegistry",
    "skeleton": "class KnowledgeGraphRegistry:\n    '''\n    A global registry that keeps track of all knowledge graph classes\n    (subclasses of BaseKnowledgeGraph).\n        '''\n    @classmethod\n    def register(cls, subclass):\n        '''\n        Register a subclass in the global registry. The key is the class name,\n        but you can choose any naming scheme (e.g., subclass.__name__ or\n        a subclass-level variable).\n        '''\n        pass\n    @classmethod\n    def get(cls, name: str):\n        '''\n        Retrieve a knowledge graph class by its name. Returns None if not found.\n        '''\n        pass\n    @classmethod\n    def all_knowledgegraphs(cls):\n        '''\n        Return a list of all registered knowledge graph classes.\n        '''\n        pass",
    "snippet_id": "snippet_335"
  },
  {
    "class_name": "air.knowledge.pipeline.chunking.chunking_registry.ChunkingRegistry",
    "skeleton": "class ChunkingRegistry:\n    '''\n    A global registry that keeps track of all chunking classes\n    (subclasses of BaseChunking).\n        '''\n    @classmethod\n    def register(cls, subclass):\n        '''\n        Register a subclass in the global registry. The key is the class name,\n        but you can choose any naming scheme (e.g., subclass.__name__ or\n        a subclass-level variable).\n        '''\n        pass\n    @classmethod\n    def get(cls, name):\n        '''\n        Retrieve a chunking class by its name. Returns None if not found.\n        '''\n        pass\n    @classmethod\n    def all_chunkings(cls):\n        '''\n        Return a list of all registered chunking classes.\n        '''\n        pass",
    "snippet_id": "snippet_336"
  },
  {
    "class_name": "llama_verifications.reporting.diff_generator.DiffGenerator",
    "skeleton": "\nclass DiffGenerator:\n    '''Generates diff reports comparing two evaluation runs.'''\n\n    def generate_diff(self, baseline_report: ReportV1, target_report: ReportV1, output_path: Path | None=None) -> ReportDiffV1:\n        '''Generate a diff report comparing baseline and target runs.\n        Args:\n            baseline_report: Baseline report\n            target_report: Target report\n            output_path: Optional path to save the diff JSON\n        Returns:\n            ReportDiffV1 instance\n        '''\n        pass\n\n    def generate_diff_from_files(self, baseline_path: Path, target_path: Path, output_path: Path | None=None) -> ReportDiffV1:\n        '''Generate a diff report from two report JSON files.\n        Args:\n            baseline_path: Path to baseline report JSON\n            target_path: Path to target report JSON\n            output_path: Optional path to save the diff JSON\n        Returns:\n            ReportDiffV1 instance\n        '''\n        pass\n\n    def _generate_functional_tests_diff(self, baseline_ft, target_ft) -> FunctionalTestsDiff:\n        '''Generate functional tests diff section.'''\n        pass\n\n    def _find_test_changes(self, baseline_cases, target_cases) -> tuple[list[str], list[str]]:\n        '''Find test regressions and fixes between baseline and target.'''\n        pass\n\n    def _generate_benchmarks_diff(self, baseline_benchmarks, target_benchmarks) -> list[BenchmarkDiff]:\n        '''Generate benchmarks diff section.'''\n        pass\n\n    def _generate_subset_metrics_diff(self, baseline_benchmark, target_benchmark, metric_name: str) -> dict[str, NumberTriplet] | None:\n        '''Generate subset metrics diff for a benchmark.'''\n        pass",
    "snippet_id": "snippet_337"
  },
  {
    "class_name": "src.aurelian.agents.talisman.talisman_tools.RateLimiter",
    "skeleton": "\nclass RateLimiter:\n    '''Simple rate limiter to ensure we don't exceed API rate limits.'''\n\n    def __init__(self, max_calls: int=3, period: float=1.0):\n        '''\n        Initialize the rate limiter.\n        Args:\n            max_calls: Maximum number of calls allowed in the period\n            period: Time period in seconds\n        '''\n        pass\n\n    def wait(self):\n        '''\n        Wait if necessary to respect the rate limit.\n        '''\n        pass",
    "snippet_id": "snippet_338"
  },
  {
    "class_name": "src.aurelian.mcp.config_generator.MCPConfigGenerator",
    "skeleton": "\nclass MCPConfigGenerator:\n    '''Generator for MCP server configuration.'''\n\n    def __init__(self, base_dir: Optional[str]=None):\n        '''\n        Initialize the MCP config generator.\n        Args:\n            base_dir: Base directory for resolving relative paths (defaults to current working directory)\n        '''\n        pass\n\n    def generate_config(self, config: Dict[str, Any]) -> Dict[str, Any]:\n        '''\n        Generate a full MCP server configuration from a simplified config.\n        Args:\n            config: Simplified configuration dictionary\n        Returns:\n            Complete MCP server configuration\n        '''\n        pass\n\n    def write_config(self, config: Dict[str, Any], output_path: str) -> None:\n        '''\n        Write the generated configuration to a file.\n        Args:\n            config: The simplified configuration dictionary\n            output_path: Path to write the generated configuration\n        '''\n        pass",
    "snippet_id": "snippet_339"
  },
  {
    "class_name": "openscvx.backend.state.Fix",
    "skeleton": "class Fix:\n    '''Class representing a fixed state variable in the optimization problem.\n    A fixed state variable is one that is constrained to a specific value\n    and cannot be optimized.\n    Attributes:\n        value: The fixed value that the state variable must take.\n    '''\n\n    def __init__(self, value):\n        '''Initialize a new fixed state variable.\n        Args:\n            value: The fixed value that the state variable must take.\n        '''\n        pass\n\n    def __repr__(self):\n        '''Get a string representation of this fixed state variable.\n        Returns:\n            str: A string representation showing the fixed value.\n        '''\n        pass",
    "snippet_id": "snippet_340"
  },
  {
    "class_name": "openscvx.backend.state.Free",
    "skeleton": "class Free:\n    '''Class representing a free state variable in the optimization problem.\n    A free state variable is one that is not constrained to any specific value\n    but can be optimized within its bounds.\n    Attributes:\n        guess: The initial guess value for optimization.\n    '''\n\n    def __init__(self, guess):\n        '''Initialize a new free state variable.\n        Args:\n            guess: The initial guess value for optimization.\n        '''\n        pass\n\n    def __repr__(self):\n        '''Get a string representation of this free state variable.\n        Returns:\n            str: A string representation showing the guess value.\n        '''\n        pass",
    "snippet_id": "snippet_341"
  },
  {
    "class_name": "openscvx.backend.state.Maximize",
    "skeleton": "class Maximize:\n    '''Class representing a state variable to be maximized in the optimization problem.\n    A maximized state variable is one that is optimized to achieve the highest\n    possible value within its bounds.\n    Attributes:\n        guess: The initial guess value for optimization.\n    '''\n\n    def __init__(self, guess):\n        '''Initialize a new maximized state variable.\n        Args:\n            guess: The initial guess value for optimization.\n        '''\n        pass\n\n    def __repr__(self):\n        '''Get a string representation of this maximized state variable.\n        Returns:\n            str: A string representation showing the guess value.\n        '''\n        pass",
    "snippet_id": "snippet_342"
  },
  {
    "class_name": "openscvx.backend.state.Minimize",
    "skeleton": "class Minimize:\n    '''Class representing a state variable to be minimized in the optimization problem.\n    A minimized state variable is one that is optimized to achieve the lowest\n    possible value within its bounds.\n    Attributes:\n        guess: The initial guess value for optimization.\n    '''\n\n    def __init__(self, guess):\n        '''Initialize a new minimized state variable.\n        Args:\n            guess: The initial guess value for optimization.\n        '''\n        pass\n\n    def __repr__(self):\n        '''Get a string representation of this minimized state variable.\n        Returns:\n            str: A string representation showing the guess value.\n        '''\n        pass",
    "snippet_id": "snippet_343"
  },
  {
    "class_name": "openscvx.results.OptimizationResults",
    "skeleton": "@dataclass\nclass OptimizationResults:\n    '''\n    Structured container for optimization results from the Successive Convexification (SCP) solver.\n    This class provides a type-safe and organized way to store and access optimization results,\n    replacing the previous dictionary-based approach. It includes core optimization data,\n    iteration history for convergence analysis, post-processing results, and flexible\n    storage for plotting and application-specific data.\n    Attributes:\n        converged (bool): Whether the optimization successfully converged\n        t_final (float): Final time of the optimized trajectory\n        u (Control): Optimized control trajectory at discretization nodes\n        x (State): Optimized state trajectory at discretization nodes\n        # SCP Iteration History (for convergence analysis)\n        x_history (list[np.ndarray]): State trajectories from each SCP iteration\n        u_history (list[np.ndarray]): Control trajectories from each SCP iteration\n        discretization_history (list[np.ndarray]): Time discretization from each iteration\n        J_tr_history (list[np.ndarray]): Trust region cost history\n        J_vb_history (list[np.ndarray]): Virtual buffer cost history\n        J_vc_history (list[np.ndarray]): Virtual control cost history\n        # Post-processing Results (added by propagate_trajectory_results)\n        t_full (Optional[np.ndarray]): Full time grid for interpolated trajectory\n        x_full (Optional[np.ndarray]): Interpolated state trajectory on full time grid\n        u_full (Optional[np.ndarray]): Interpolated control trajectory on full time grid\n        cost (Optional[float]): Total cost of the optimized trajectory\n        ctcs_violation (Optional[np.ndarray]): Continuous-time constraint violations\n        # User-defined Data\n        plotting_data (dict[str, Any]): Flexible storage for plotting and application data\n    '''\n\n    def __post_init__(self):\n        '''Initialize the results object.'''\n        pass\n\n    def update_plotting_data(self, **kwargs):\n        '''\n        Update the plotting data with additional information.\n        Args:\n            **kwargs: Key-value pairs to add to plotting_data\n        '''\n        pass\n\n    def get(self, key: str, default: Any=None) -> Any:\n        '''\n        Get a value from the results, similar to dict.get().\n        Args:\n            key: The key to look up\n            default: Default value if key is not found\n        Returns:\n            The value associated with the key, or default if not found\n                        '''\n                        pass\n\n    def __getitem__(self, key: str) -> Any:\n        '''\n        Allow dictionary-style access to results.\n        Args:\n            key: The key to look up\n        Returns:\n            The value associated with the key\n        Raises:\n            KeyError: If key is not found\n        '''\n        pass\n\n    def __setitem__(self, key: str, value: Any):\n        '''\n        Allow dictionary-style assignment to results.\n        Args:\n            key: The key to set\n            value: The value to assign\n        '''\n        pass\n\n    def __contains__(self, key: str) -> bool:\n        '''\n        Check if a key exists in the results.\n        Args:\n            key: The key to check\n        Returns:\n            True if key exists, False otherwise\n        '''\n        pass\n\n    def update_plotting_data(self, **kwargs):\n        '''\n        Update the results with additional data from a dictionary.\n        Args:\n            other: Dictionary containing additional data\n        '''\n        pass\n\n    def to_dict(self) -> dict[str, Any]:\n        '''\n        Convert the results to a dictionary for backward compatibility.\n        Returns:\n            Dictionary representation of the results\n        '''\n        pass",
    "snippet_id": "snippet_344"
  },
  {
    "class_name": "enhanced_message_types.EnhancedUser",
    "skeleton": "@dataclass\nclass EnhancedUser:\n    '''增强的用户信息'''\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'EnhancedUser':\n        '''从字典创建用户对象'''\n        pass\n\n    def get_user_id(self) -> Optional[str]:\n        '''获取用户ID，优先级：id > user_openid > member_openid > openid'''\n        pass",
    "snippet_id": "snippet_345"
  },
  {
    "class_name": "enhanced_message_types.EventDataNormalizer",
    "skeleton": "\nclass EventDataNormalizer:\n    '''事件数据标准化器'''\n    @staticmethod\n    def normalize_event_data(event_type: str, event_data: Dict[str, Any]) -> Dict[str, Any]:\n        '''标准化事件数据格式'''\n        pass\n    @staticmethod\n    def extract_user_id(event_data: Dict[str, Any]) -> Optional[str]:\n        '''从事件数据中提取用户ID'''\n        pass\n    @staticmethod\n    def extract_target_info(event_data: Dict[str, Any]) -> tuple[Optional[str], bool]:\n        '''从事件数据中提取目标信息，返回(target_id, is_group)'''\n        pass",
    "snippet_id": "snippet_346"
  },
  {
    "class_name": "enhanced_message_types.MessageBuilder",
    "skeleton": "\nclass MessageBuilder:\n    '''增强的消息构建器'''\n    @staticmethod\n    def build_text_message(content: str) -> Dict[str, Any]:\n        '''构建文本消息'''\n        pass\n    @staticmethod\n    def build_markdown_message(content: str, markdown: Dict[str, Any]) -> Dict[str, Any]:\n        '''构建Markdown消息'''\n        pass\n    @staticmethod\n    def build_image_message(url: str) -> Dict[str, Any]:\n        '''构建图片消息'''\n        pass\n    @staticmethod\n    def build_file_message(file_info: Dict[str, Any]) -> Dict[str, Any]:\n        '''构建文件消息'''\n        pass\n    @staticmethod\n    def build_keyboard_message(content: str, keyboard: Dict[str, Any]) -> Dict[str, Any]:\n        '''构建带按钮的消息'''\n        pass\n    @staticmethod\n    def build_ark_message(ark: Dict[str, Any]) -> Dict[str, Any]:\n        '''构建ARK消息'''\n        pass",
    "snippet_id": "snippet_347"
  },
  {
    "class_name": "easyswitch.adapters.base.AdaptersRegistry",
    "skeleton": "\nclass AdaptersRegistry:\n    '''\n    Registry for all payment adapters.\n    This class is used to register and retrieve adapters based on their provider.\n        '''\n    @classmethod\n    def register(cls, name: Optional[str]=None) -> None:\n        '''Register a new Adapter class.'''\n        pass\n\n        def wrapper(adapter: Type['BaseAdapter']):\n                '''Wrapper'''\n                pass\n    @classmethod\n    def get(cls, name: str) -> Type['BaseAdapter']:\n        '''Get an Adapter class by its name.'''\n        pass\n    @classmethod\n    def all(cls) -> List[Type['BaseAdapter']]:\n        '''Get all registered Adapters classes.'''\n        pass\n    @classmethod\n    def clear(cls) -> None:\n        '''Clear the registry.'''\n        pass\n    @classmethod\n    def list(cls) -> List[str]:\n        '''List all registered Adapters names.'''\n        pass",
    "snippet_id": "snippet_348"
  },
  {
    "class_name": "easyswitch.integrators.fedapay.utils.FedapayCurrencyMapper",
    "skeleton": "class FedapayCurrencyMapper:\n    '''\n    A utility class to map FedaPay currency IDs to their corresponding ISO codes.\n    This class provides a method to retrieve the ISO code for a given FedaPay currency ID.\n    It also allows for dynamic addition of new currency mappings.\n    '''\n    @classmethod\n    def get_iso(cls, currency_id: int) -> str:\n        '''\n        Get the ISO code for a given FedaPay currency ID.\n        Args:\n            currency_id (int): The FedaPay currency ID.\n        Returns:\n            str: The ISO code corresponding to the currency ID.\n        Raises:\n            ValueError: If the currency ID is not recognized.\n        '''\n        pass\n    @classmethod\n    def add_currency(cls, currency_id: int, iso: str):\n        '''        \n        Add a new currency mapping to the mapper.\n        Args:\n            currency_id (int): The FedaPay currency ID.\n            iso (str): The ISO code for the currency.\n        '''\n        pass",
    "snippet_id": "snippet_349"
  },
  {
    "class_name": "scripts.profile_startup.StartupProfiler",
    "skeleton": "\nclass StartupProfiler:\n    '''Detailed startup profiling with bottleneck identification.'''\n\n    def __init__(self):\n        '''Initialize the profiler.'''\n        pass\n\n    def checkpoint(self, name: str):\n        '''Record a timing checkpoint.'''\n        pass\n\n    def get_report(self) -> dict[str, float]:\n        '''Get a performance report showing time deltas.'''\n        pass\n\n    def analyze_bottlenecks(self, report: dict[str, float]) -> list[str]:\n        '''Analyze the report and identify performance bottlenecks.'''\n        pass",
    "snippet_id": "snippet_350"
  },
  {
    "class_name": "streetrace.agents.base_agent_loader.AgentInfo",
    "skeleton": "\nclass AgentInfo:\n    '''Agent information container supporting both Python and YAML agents.'''\n\n    def __init__(self, name: str, description: str, file_path: Path | None=None, module: 'ModuleType | None'=None, yaml_document: 'YamlAgentDocument | None'=None) -> None:\n        '''Initialize agent info.\n        Args:\n            name: Agent name\n            description: Agent description\n            file_path: Path to agent file/directory\n            module: Python module (for Python agents)\n            yaml_document: YAML agent document (for YAML agents)\n        '''\n        pass\n    @property\n    def kind(self) -> Literal['python', 'yaml']:\n        '''Get the definition type of the agent.'''\n        pass\n    @property\n    def path(self) -> str:\n        '''Get the definition path of the agent.'''\n        pass",
    "snippet_id": "snippet_351"
  },
  {
    "class_name": "streetrace.llm.model_factory.ModelFactory",
    "skeleton": "\nclass ModelFactory:\n    '''Factory class to create and manage models for the StreetRace application.\n    This class is responsible for:\n    1. Managing model configurations\n    2. Creating and caching LlmInterface instances\n    3. Providing access to underlying BaseLlm instances for agents\n    '''\n\n    def __init__(self, default_model_name: str | None, ui_bus: UiBus, args: 'Args') -> None:\n        '''Initialize the ModelFactory with model configuration and UI bus.\n        Args:\n            default_model_name: Name of the model to use\n            ui_bus: UI event bus for displaying messages to the user\n            args: Application arguments\n                        '''\n                        pass\n\n    def _setup_caching(self) -> None:\n        '''Set up Redis caching for LiteLLM.'''\n        pass\n\n    def get_llm_interface(self, model_name: str) -> LlmInterface:\n        '''Return the default model based on the configuration.\n        Returns:\n            LlmInterface instance.\n        '''\n        pass\n\n    def get_current_model(self) -> 'BaseLlm':\n        '''Return the default model based on the configuration.\n        Returns:\n            Either a string model name or a BaseLlm instance.\n        '''\n        pass",
    "snippet_id": "snippet_352"
  },
  {
    "class_name": "streetrace.session.json_serializer.JSONSessionSerializer",
    "skeleton": "\nclass JSONSessionSerializer:\n    '''Serialize and deserialize ADK Session to/from JSON.\n    Notes: this is not a complete serializer. It saves and reads\n    only a necessary subset of fields.\n    '''\n\n    def __init__(self, storage_path: Path) -> None:\n        '''Initialize a new instance of JSONSessionSerializer.'''\n        pass\n\n    def _file_path(self, *, app_name: str | None=None, user_id: str | None=None, session_id: str | None=None, session: 'Session | None'=None) -> Path:\n        '''Construct the JSON file path for a session.'''\n        pass\n\n    def read(self, app_name: str, user_id: str, session_id: str) -> 'Session | None':\n        '''Read a session from a JSON file.\n        The config parameter is currently not used for filtering during read.\n        '''\n        pass\n\n    def write(self, session: 'Session') -> Path:\n        '''Write a session to a JSON file.'''\n        pass\n\n    def delete(self, app_name: str, user_id: str, session_id: str) -> None:\n        '''Delete a session's JSON file.'''\n        pass\n\n    def list_saved(self, *, app_name: str, user_id: str) -> 'Iterator[Session]':\n        '''List saved sessions.'''\n        pass",
    "snippet_id": "snippet_353"
  },
  {
    "class_name": "streetrace.ui.adk_event_renderer.EventRenderer",
    "skeleton": "\nclass EventRenderer:\n    '''Stateful renderer that groups function calls with their responses.'''\n\n    def __init__(self) -> None:\n        '''Initialize the event renderer.'''\n        pass\n\n    def render_event(self, obj: 'Event', console: 'Console') -> None:\n        '''Render the provided google.adk.events.Event to rich.console.'''\n        pass\n\n    def _flush_pending_function_call(self, console: 'Console') -> None:\n        '''Render any pending function call that hasn't been paired with a response.'''\n        pass\n\n    def _render_function_call_group(self, function_call: 'FunctionCall', response: dict[str, Any], console: 'Console') -> None:\n        '''Render function call and response together in a grouped panel.'''\n        pass",
    "snippet_id": "snippet_354"
  },
  {
    "class_name": "streetrace.ui.console_ui.StatusSpinner",
    "skeleton": "\nclass StatusSpinner:\n    '''Console Status, encapsulates rich.status.'''\n\n    def __init__(self, app_state: AppState, console: Console) -> None:\n        '''Initialize the instance and instantiate rich.status.\n        Args:\n            app_state: App State container.\n            console: The console instance to attach the spinner to.\n        '''\n        pass\n\n    def update_state(self) -> None:\n        '''Update status message.'''\n        pass\n\n    def __enter__(self) -> 'StatusSpinner':\n        '''Enter the context by starting the spinner.\n        Returns:\n            self, so that logging methods can be called within the context.\n        '''\n        pass\n\n    def __exit__(self, exc_type: type[BaseException] | None, exc_value: BaseException | None, traceback: TracebackType | None) -> None:\n        '''Exit the context by propagating the signal to the spinner.\n        Args:\n            exc_type: The type of exception raised (if any).\n            exc_value: The exception instance (if any).\n            traceback: The traceback object (if any).\n        '''\n        pass",
    "snippet_id": "snippet_355"
  },
  {
    "class_name": "chemtorch.utils.callable_compose.CallableCompose",
    "skeleton": "nan",
    "snippet_id": "snippet_356"
  },
  {
    "class_name": "data_pipeline.simple_data_pipeline.SimpleDataPipeline",
    "skeleton": "\nclass SimpleDataPipeline:\n    '''\n    A simple data pipeline that orchestrates data loading, column mapping,\n    and data splitting.\n    The ingestion process is as follows:\n    1. Load data using the `data_source`. This can result in a single\n       DataFrame or an already split `DataSplit` object.\n    2. Apply column transformations (filtering, renaming) using the `column_mapper`.\n       This mapper can operate on both single DataFrames and `DataSplit` objects.\n    3. If the data after mapping is a single DataFrame, split it using the\n       `data_splitter`. If it's already a `DataSplit`, this step is skipped.\n    '''\n\n    def __init__(self, data_source: DataSource, column_mapper: ColumnMapper, data_splitter: Optional[DataSplitter]=None):\n        '''\n        Initializes the SimpleDataPipeline.\n        Args:\n            data_source (DataSource): The component responsible for loading the initial data.\n            column_mapper (ColumnMapper): The component for transforming columns.\n                                              It should handle both pd.DataFrame and DataSplit inputs.\n            data_splitter (Optional[DataSplitter]): The component for splitting a single DataFrame\n                                                    into train, validation, and test sets.\n                                                    This is not used if data_source already provides split data.\n        '''\n        pass\n\n    def __call__(self) -> pd.DataFrame | DataSplit:\n        '''\n        Executes the data ingestion pipeline with validation.\n        Returns:\n            pd.DataFrame | DataSplit: The final processed data, either as a single DataFrame\n                                       or a DataSplit object containing train, validation, and test sets.\n        Raises:\n            ValueError: If there is a configuration mismatch, such as:\n                        - A `data_splitter` is provided for a pre-split dataset.\n            TypeError: If the column mapper returns an unexpected type.\n        '''\n        pass",
    "snippet_id": "snippet_357"
  },
  {
    "class_name": "layer.utils.ResidualConnection",
    "skeleton": "\nclass ResidualConnection:\n    '''\n    A utility class for applying residual connections in neural networks.\n        '''\n\n    def __init__(self, use_residual: bool=False):\n        '''\n        Initialize the ResidualConnection.\n        Args:\n            use_residual (bool): If True, apply residual connection.\n        '''\n        pass\n\n    def register(self, x: torch.Tensor):\n        '''\n        Register the input tensor for residual connection.\n        Args:\n            x (torch.Tensor): The input tensor to be registered.\n        '''\n        pass\n\n    def apply(self, y: torch.Tensor) -> torch.Tensor:\n        '''\n        Apply the residual connection.\n        The residual connection is only applied if it was instantiated with `use_residual=True`.\n        Args:\n            y (torch.Tensor): The output tensor to which the residual connection is applied.\n        Returns:\n            torch.Tensor: The output tensor after applying the residual connection.\n        '''\n        pass",
    "snippet_id": "snippet_358"
  },
  {
    "class_name": "representation.fingerprint.drfp.DRFPUtil",
    "skeleton": "\nclass DRFPUtil:\n    '''\n    A utility class for encoding SMILES as drfp fingerprints.\n        '''\n    @staticmethod\n    def shingling_from_mol(in_mol: Mol, radius: int=3, rings: bool=True, min_radius: int=0, get_atom_indices: bool=False, root_central_atom: bool=True, include_hydrogens: bool=False) -> Union[List[str], Tuple[List[str], Dict[str, List[Set[int]]]]]:\n        '''Creates a molecular shingling from a RDKit molecule (rdkit.Chem.rdchem.Mol).\n        Arguments:\n            in_mol: A RDKit molecule instance\n            radius: The drfp radius (a radius of 3 corresponds to drfp6)\n            rings: Whether or not to include rings in the shingling\n            min_radius: The minimum radius that is used to extract n-grams\n        Returns:\n            The molecular shingling.\n        '''\n        pass\n    @staticmethod\n    def internal_encode(in_smiles: str, radius: int=3, min_radius: int=0, rings: bool=True, get_atom_indices: bool=False, root_central_atom: bool=True, include_hydrogens: bool=False) -> Union[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, Dict[str, List[Dict[str, List[Set[int]]]]]]]:\n        '''Creates an drfp array from a reaction SMILES string.\n        Arguments:\n            in_smiles: A valid reaction SMILES string\n            radius: The drfp radius (a radius of 3 corresponds to drfp6)\n            min_radius: The minimum radius that is used to extract n-grams\n            rings: Whether or not to include rings in the shingling\n        Returns:\n            A tuple with two arrays, the first containing the drfp hash values, the second the substructure SMILES\n        '''\n        pass\n    @staticmethod\n    def hash(shingling: List[str]) -> np.ndarray:\n        '''Directly hash all the SMILES in a shingling to a 32-bit integer.\n        Arguments:\n            shingling: A list of n-grams\n        Returns:\n            A list of hashed n-grams\n        '''\n        pass\n    @staticmethod\n    def fold(hash_values: np.ndarray, length: int=2048) -> Tuple[np.ndarray, np.ndarray]:\n        '''Folds the hash values to a binary vector of a given length.\n        Arguments:\n            hash_value: An array containing the hash values\n            length: The length of the folded fingerprint\n        Returns:\n            A tuple containing the folded fingerprint and the indices of the on bits\n        '''\n        pass\n    @staticmethod\n    def encode(X: Union[Iterable, str], n_folded_length: int=2048, min_radius: int=0, radius: int=3, rings: bool=True, mapping: bool=False, atom_index_mapping: bool=False, root_central_atom: bool=True, include_hydrogens: bool=False, show_progress_bar: bool=False) -> Union[List[np.ndarray], Tuple[List[np.ndarray], Dict[int, Set[str]]], Tuple[List[np.ndarray], Dict[int, Set[str]]], List[Dict[str, List[Dict[str, List[Set[int]]]]]]]:\n        '''Encodes a list of reaction SMILES using the drfp fingerprint.\n        Args:\n            X: An iterable (e.g. List) of reaction SMILES or a single reaction SMILES to be encoded\n            n_folded_length: The folded length of the fingerprint (the parameter for the modulo hashing)\n            min_radius: The minimum radius of a substructure (0 includes single atoms)\n            radius: The maximum radius of a substructure\n            rings: Whether to include full rings as substructures\n            mapping: Return a feature to substructure mapping in addition to the fingerprints\n            atom_index_mapping: Return the atom indices of mapped substructures for each reaction\n            root_central_atom: Whether to root the central atom of substructures when generating SMILES\n            show_progress_bar: Whether to show a progress bar when encoding reactions\n        Returns:\n            A list of drfp fingerprints or, if mapping is enabled, a tuple containing a list of drfp fingerprints and a mapping dict.\n        '''\n        pass",
    "snippet_id": "snippet_359"
  },
  {
    "class_name": "core.steps.action_plan.srf.file_selection.tools.acr.search_tools.SearchResult",
    "skeleton": "@dataclass\nclass SearchResult:\n    '''Dataclass to hold search results.'''\n    @staticmethod\n    def to_relative_path(file_path: str, project_root: str) -> str:\n        '''Convert an absolute path to a path relative to the project root.\n        Args:\n            - file_path (str): The absolute path.\n            - project_root (str): Absolute path of the project root dir.\n        Returns:\n            The relative path.\n        '''\n        pass\n\n    def to_tagged_upto_file(self, project_root: str):\n        '''Convert the search result to a tagged string, upto file path.'''\n        pass\n\n    def to_tagged_upto_class(self, project_root: str):\n        '''Convert the search result to a tagged string, upto class.'''\n        pass\n\n    def to_tagged_upto_func(self, project_root: str):\n        '''Convert the search result to a tagged string, upto function.'''\n        pass\n\n    def to_tagged_str(self, project_root: str):\n        '''Convert the search result to a tagged string.'''\n        pass\n    @staticmethod\n    def collapse_to_file_level(lst, project_root: str) -> str:\n        '''Collapse search results to file level.'''\n        pass\n    @staticmethod\n    def collapse_to_method_level(lst, project_root: str) -> str:\n        '''Collapse search results to method level.'''\n        pass",
    "snippet_id": "snippet_360"
  },
  {
    "class_name": "langchain_oci.chat_models.oci_generative_ai.OCIUtils",
    "skeleton": "\nclass OCIUtils:\n    '''Utility functions for OCI Generative AI integration.'''\n    @staticmethod\n    def is_pydantic_class(obj: Any) -> bool:\n        '''Check if an object is a Pydantic BaseModel subclass.'''\n        pass\n    @staticmethod\n    def remove_signature_from_tool_description(name: str, description: str) -> str:\n        '''\n        Remove the tool signature and Args section from a tool description.\n        The signature is typically prefixed to the description and followed\n        by an Args section.\n        '''\n        pass\n    @staticmethod\n    def convert_oci_tool_call_to_langchain(tool_call: Any) -> ToolCall:\n        '''Convert an OCI tool call to a LangChain ToolCall.'''\n        pass",
    "snippet_id": "snippet_361"
  },
  {
    "class_name": "apm_cli.core.script_runner.ScriptRunner",
    "skeleton": "\nclass ScriptRunner:\n    '''Executes APM scripts with auto-compilation of .prompt.md files.'''\n\n    def __init__(self, compiler=None):\n        '''Initialize script runner with optional compiler.'''\n        pass\n\n    def run_script(self, script_name: str, params: Dict[str, str]) -> bool:\n        '''Run a script from apm.yml with parameter substitution.\n        Args:\n            script_name: Name of the script to run\n            params: Parameters for compilation and script execution\n        Returns:\n            bool: True if script executed successfully\n        '''\n        pass\n\n    def list_scripts(self) -> Dict[str, str]:\n        '''List all available scripts from apm.yml.\n        Returns:\n            Dict mapping script names to their commands\n        '''\n        pass\n\n    def _load_config(self) -> Optional[Dict]:\n        '''Load apm.yml from current directory.'''\n        pass\n\n    def _auto_compile_prompts(self, command: str, params: Dict[str, str]) -> tuple[str, list[str]]:\n        '''Auto-compile .prompt.md files and transform runtime commands.\n        Args:\n            command: Original script command\n            params: Parameters for compilation\n        Returns:\n            Tuple of (compiled_command, list_of_compiled_prompt_files)\n        '''\n        pass\n\n    def _transform_runtime_command(self, command: str, prompt_file: str, compiled_content: str, compiled_path: str) -> str:\n        '''Transform runtime commands to their proper execution format.\n        Args:\n            command: Original command\n            prompt_file: Original .prompt.md file path\n            compiled_content: Compiled prompt content as string\n            compiled_path: Path to compiled .txt file\n        Returns:\n            Transformed command for proper runtime execution\n        '''\n        pass",
    "snippet_id": "snippet_362"
  },
  {
    "class_name": "apm_cli.registry.client.SimpleRegistryClient",
    "skeleton": "\nclass SimpleRegistryClient:\n    '''Simple client for querying MCP registries for server discovery.'''\n\n    def __init__(self, registry_url: Optional[str]=None):\n        '''Initialize the registry client.\n        Args:\n            registry_url (str, optional): URL of the MCP registry.\n                If not provided, uses the MCP_REGISTRY_URL environment variable\n                or falls back to the default demo registry.\n        '''\n        pass\n\n    def list_servers(self, limit: int=100, cursor: Optional[str]=None) -> Tuple[List[Dict[str, Any]], Optional[str]]:\n        '''List all available servers in the registry.\n        Args:\n            limit (int, optional): Maximum number of entries to return. Defaults to 100.\n            cursor (str, optional): Pagination cursor for retrieving next set of results.\n        Returns:\n            Tuple[List[Dict[str, Any]], Optional[str]]: List of server metadata dictionaries and the next cursor if available.\n        Raises:\n            requests.RequestException: If the request fails.\n        '''\n        pass\n\n    def search_servers(self, query: str) -> List[Dict[str, Any]]:\n        '''Search for servers in the registry.\n        Args:\n            query (str): Search query string.\n        Returns:\n            List[Dict[str, Any]]: List of matching server metadata dictionaries.\n        Raises:\n            requests.RequestException: If the request fails.\n        '''\n        pass\n\n    def get_server_info(self, server_id: str) -> Dict[str, Any]:\n        '''Get detailed information about a specific server.\n        Args:\n            server_id (str): ID of the server.\n        Returns:\n            Dict[str, Any]: Server metadata dictionary.\n        Raises:\n            requests.RequestException: If the request fails.\n            ValueError: If the server is not found.\n        '''\n        pass\n\n    def get_server_by_name(self, name: str) -> Optional[Dict[str, Any]]:\n        '''Find a server by its name.\n        Args:\n            name (str): Name of the server to find.\n        Returns:\n            Optional[Dict[str, Any]]: Server metadata dictionary or None if not found.\n        Raises:\n            requests.RequestException: If the request fails.\n        '''\n        pass\n\n    def find_server_by_reference(self, reference: str) -> Optional[Dict[str, Any]]:\n        '''Find a server by exact name match or server ID.\n        This is a simple, efficient lookup that only makes network requests when necessary:\n        1. Server ID (UUID format) - direct API call\n        2. Exact name match from server list - single additional API call\n        Args:\n            reference (str): Server reference (ID or exact name).\n        Returns:\n            Optional[Dict[str, Any]]: Server metadata dictionary or None if not found.\n        Raises:\n            requests.RequestException: If the request fails.\n        '''\n        pass",
    "snippet_id": "snippet_363"
  },
  {
    "class_name": "khive.cli.base.CLIResult",
    "skeleton": "@dataclass\nclass CLIResult:\n    '''Standard result structure for CLI commands.'''\n\n    def is_success(self) -> bool:\n        '''Check if the result represents success.'''\n        pass\n\n    def to_dict(self) -> dict[str, Any]:\n        '''Convert result to dictionary for JSON output.'''\n        pass",
    "snippet_id": "snippet_364"
  },
  {
    "class_name": "khive.core.time_policy.TimePolicy",
    "skeleton": "\nclass TimePolicy:\n    '''Centralized timezone policy for consistent temporal behavior'''\n    @classmethod\n    def now_utc(cls) -> datetime.datetime:\n        '''Get current time in UTC for internal operations\n        Use for: logging, session management, internal timestamps\n        Replaces: datetime.now() -> datetime.now(tz=TimePolicy.UTC)\n        '''\n        pass\n    @classmethod\n    def now_local(cls) -> datetime.datetime:\n        '''Get current time in local timezone for user-facing operations\n        Use for: CSV downloads, frontend display, user-visible timestamps\n        '''\n        pass\n    @classmethod\n    def from_timestamp_utc(cls, timestamp: int | float) -> datetime.datetime:\n        '''Convert timestamp to UTC datetime\n        Use for: converting stored timestamps, API responses\n        Replaces: datetime.fromtimestamp(ts) -> datetime.fromtimestamp(ts, tz=TimePolicy.UTC)\n        '''\n        pass\n    @classmethod\n    def from_timestamp_local(cls, timestamp: int | float) -> datetime.datetime:\n        '''Convert timestamp to local timezone datetime\n        Use for: user-facing timestamp display\n        '''\n        pass\n    @classmethod\n    def strptime_utc(cls, date_string: str, format_string: str) -> datetime.datetime:\n        '''Parse datetime string and assign UTC timezone\n        Use for: parsing log timestamps, API timestamps\n        Replaces: datetime.strptime(s, fmt) -> datetime.strptime(s, fmt).replace(tzinfo=TimePolicy.UTC)\n        '''\n        pass\n    @classmethod\n    def format_for_filename(cls, dt: datetime.datetime | None=None) -> str:\n        '''Format datetime for use in filenames (safe characters only)\n        Args:\n            dt: datetime to format, defaults to current UTC time\n        Returns:\n            Filename-safe timestamp string: YYYYMMDD_HHMMSS\n        '''\n        pass\n    @classmethod\n    def format_for_display(cls, dt: datetime.datetime | None=None) -> str:\n        '''Format datetime for user display\n        Args:\n            dt: datetime to format, defaults to current local time\n        Returns:\n            Human-readable timestamp: YYYY-MM-DD HH:MM:SS\n        '''\n        pass\n    @classmethod\n    def format_iso(cls, dt: datetime.datetime | None=None) -> str:\n        '''Format datetime as ISO string with timezone\n        Args:\n            dt: datetime to format, defaults to current UTC time\n        Returns:\n            ISO formatted string with timezone info\n        '''\n        pass",
    "snippet_id": "snippet_365"
  },
  {
    "class_name": "khive.utils.BaseConfig",
    "skeleton": "@dataclass\nclass BaseConfig:\n    '''Base configuration class with common CLI options.'''\n    @property\n    def khive_config_dir(self) -> Path:\n        '''Path to the .khive configuration directory.'''\n        pass\n\n    def update_from_cli_args(self, args: Any) -> None:\n        '''Update configuration from CLI arguments.'''\n        pass",
    "snippet_id": "snippet_366"
  },
  {
    "class_name": "nemo_deploy.deploy_ray.DeployRay",
    "skeleton": "\nclass DeployRay:\n    '''A class for managing Ray deployment and serving of models.\n    This class provides functionality to initialize Ray, start Ray Serve,\n    deploy models, and manage the lifecycle of the Ray cluster. It supports\n    both NeMo inframework models, Hugging Face models, and TensorRT-LLM models.\n    Attributes:\n        address (str): The address of the Ray cluster to connect to.\n        num_cpus (int): Number of CPUs to allocate for the Ray cluster.\n        num_gpus (int): Number of GPUs to allocate for the Ray cluster.\n        include_dashboard (bool): Whether to include the Ray dashboard.\n        ignore_reinit_error (bool): Whether to ignore errors when reinitializing Ray.\n        runtime_env (dict): Runtime environment configuration for Ray.\n        host (str): Host address to bind the server to.\n        port (int): Port number for the server.\n    Methods:\n        deploy_inframework_model: Deploy a NeMo inframework model using Ray Serve.\n        deploy_huggingface_model: Deploy a Hugging Face model using Ray Serve.\n        deploy_tensorrt_llm_model: Deploy a TensorRT-LLM model using Ray Serve.\n    '''\n\n    def __init__(self, address: str='auto', num_cpus: Optional[int]=None, num_gpus: int=1, include_dashboard: bool=False, ignore_reinit_error: bool=True, runtime_env: dict=None, host: str='0.0.0.0', port: Optional[int]=None):\n        '''Initialize the DeployRay instance and set up the Ray cluster.\n        Args:\n            address (str, optional): Address of the Ray cluster. Defaults to \"auto\".\n            num_cpus (int, optional): Number of CPUs to allocate. If None, uses all available. Defaults to None.\n            num_gpus (int, optional): Number of GPUs to allocate. Defaults to 1.\n            include_dashboard (bool, optional): Whether to include the dashboard. Defaults to False.\n            ignore_reinit_error (bool, optional): Whether to ignore reinit errors. Defaults to True.\n            runtime_env (dict, optional): Runtime environment configuration. Defaults to None.\n            host (str, optional): Host address to bind the server to. Defaults to \"0.0.0.0\".\n            port (int, optional): Port number for the server. If None, an available port will be found. Defaults to None.\n        Raises:\n            Exception: If Ray is not installed.\n        '''\n        pass\n\n    def _signal_handler(self, signum, frame):\n        '''Handle signal interrupts and gracefully shutdown the deployer.'''\n        pass\n\n    def _start(self):\n        '''Start Ray Serve with the configured host and port.\n        Uses the host and port specified during DeployRay initialization.\n        If port is None, an available port will be found automatically.\n        '''\n        pass\n\n    def _stop(self):\n        '''Stop the Ray Serve deployment and shutdown the Ray cluster.\n        This method attempts to gracefully shutdown both Ray Serve and the Ray cluster.\n        If any errors occur during shutdown, they are logged as warnings.\n        '''\n        pass\n\n    def deploy_inframework_model(self, nemo_checkpoint: str, num_gpus: int=1, tensor_model_parallel_size: int=1, pipeline_model_parallel_size: int=1, expert_model_parallel_size: int=1, context_parallel_size: int=1, model_id: str='nemo-model', num_cpus_per_replica: float=8, num_replicas: int=1, enable_cuda_graphs: bool=False, enable_flash_decode: bool=False, legacy_ckpt: bool=False, max_batch_size: int=32, random_seed: Optional[int]=None, test_mode: bool=False, megatron_checkpoint_filepath: str=None, model_type: str='gpt', model_format: str='nemo', micro_batch_size: Optional[int]=None):\n        '''Deploy an inframework NeMo/Megatron model using Ray Serve.\n        This method handles the complete deployment lifecycle including:\n        - Starting Ray Serve\n        - Creating and deploying the MegatronRayDeployable\n        - Setting up signal handlers for graceful shutdown\n        - Keeping the deployment running until interrupted\n        Args:\n            nemo_checkpoint (str): Path to the .nemo checkpoint file.\n            num_gpus (int, optional): Number of GPUs per node. Defaults to 1.\n            tensor_model_parallel_size (int, optional): Tensor model parallel size. Defaults to 1.\n            pipeline_model_parallel_size (int, optional): Pipeline model parallel size. Defaults to 1.\n            expert_model_parallel_size (int, optional): Expert model parallel size. Defaults to 1.\n            context_parallel_size (int, optional): Context parallel size. Defaults to 1.\n            model_id (str, optional): Model identifier for API responses. Defaults to \"nemo-model\".\n            num_cpus_per_replica (float, optional): CPUs per model replica. Defaults to 8.\n            num_replicas (int, optional): Number of replicas for deployment. Defaults to 1.\n            enable_cuda_graphs (bool, optional): Enable CUDA graphs. Defaults to False.\n            enable_flash_decode (bool, optional): Enable Flash Attention decode. Defaults to False.\n            legacy_ckpt (bool, optional): Use legacy checkpoint format. Defaults to False.\n            test_mode (bool, optional): Enable test mode. Defaults to False.\n            megatron_checkpoint_filepath (str, optional): Path to the Megatron checkpoint file. Defaults to None.\n            model_type (str, optional): Type of model to load. Defaults to \"gpt\".\n            model_format (str, optional): Format of model to load. Defaults to \"nemo\".\n            micro_batch_size (Optional[int], optional): Micro batch size for model execution. Defaults to None.\n        Raises:\n            SystemExit: If parallelism configuration is invalid.\n            Exception: If deployment fails.\n        '''\n        pass\n\n    def deploy_huggingface_model(self, hf_model_id_path: str, task: str='text-generation', trust_remote_code: bool=True, device_map: Optional[str]=None, max_memory: Optional[str]=None, model_id: str='hf-model', num_replicas: int=1, num_cpus_per_replica: float=8, num_gpus_per_replica: int=1, max_ongoing_requests: int=10, use_vllm_backend: bool=False, test_mode: bool=False):\n        '''Deploy a Hugging Face model using Ray Serve.\n        This method handles the complete deployment lifecycle including:\n        - Starting Ray Serve\n        - Creating and deploying the HFRayDeployable\n        - Setting up signal handlers for graceful shutdown\n        - Keeping the deployment running until interrupted\n        Args:\n            hf_model_id_path (str): Path to the HuggingFace model or model identifier.\n                Can be a local path or a model ID from HuggingFace Hub.\n            task (str, optional): HuggingFace task type. Defaults to \"text-generation\".\n            trust_remote_code (bool, optional): Whether to trust remote code when loading the model. Defaults to True.\n            device_map (str, optional): Device mapping strategy for model placement. Defaults to \"auto\".\n            max_memory (str, optional): Maximum memory allocation when using balanced device map. Defaults to None.\n            model_id (str, optional): Model identifier for API responses. Defaults to \"hf-model\".\n            num_replicas (int, optional): Number of replicas for deployment. Defaults to 1.\n            num_cpus_per_replica (float, optional): CPUs per model replica. Defaults to 8.\n            num_gpus_per_replica (int, optional): GPUs per model replica. Defaults to 1.\n            max_ongoing_requests (int, optional): Maximum number of ongoing requests per replica. Defaults to 10.\n            use_vllm_backend (bool, optional): Whether to use vLLM backend for deployment. If True, exports the HF ckpt\n            to vLLM format and uses vLLM backend for inference. Defaults to False.\n            test_mode (bool, optional): Enable test mode. Defaults to False.\n        Raises:\n            Exception: If Ray is not installed or deployment fails.\n        '''\n        pass\n\n    def deploy_tensorrt_llm_model(self, trt_llm_path: str, model_id: str='tensorrt-llm-model', use_python_runtime: bool=True, multi_block_mode: bool=False, lora_ckpt_list: Optional[list]=None, enable_chunked_context: bool=False, max_tokens_in_paged_kv_cache: Optional[int]=None, num_replicas: int=1, num_cpus_per_replica: float=8, num_gpus_per_replica: int=1, max_ongoing_requests: int=10, test_mode: bool=False):\n        '''Deploy a TensorRT-LLM model using Ray Serve.\n        This method handles the complete deployment lifecycle including:\n        - Starting Ray Serve\n        - Creating and deploying the TensorRTLLMRayDeployable\n        - Setting up signal handlers for graceful shutdown\n        - Keeping the deployment running until interrupted\n        Note: This method assumes the model is already converted to TensorRT-LLM format.\n        The conversion should be done before calling this API.\n        Args:\n            trt_llm_path (str): Path to the TensorRT-LLM model directory with pre-built engines.\n            model_id (str, optional): Model identifier for API responses. Defaults to \"tensorrt-llm-model\".\n            use_python_runtime (bool, optional): Whether to use Python runtime (vs C++ runtime). Defaults to True.\n            multi_block_mode (bool, optional): Whether to enable multi-block mode. Defaults to False.\n            lora_ckpt_list (list, optional): List of LoRA checkpoint paths. Defaults to None.\n            enable_chunked_context (bool, optional): Whether to enable chunked context (C++ runtime only). Defaults to False.\n            max_tokens_in_paged_kv_cache (int, optional): Maximum tokens in paged KV cache (C++ runtime only). Defaults to None.\n            num_replicas (int, optional): Number of replicas for deployment. Defaults to 1.\n            num_cpus_per_replica (float, optional): CPUs per model replica. Defaults to 8.\n            num_gpus_per_replica (int, optional): GPUs per model replica. Defaults to 1.\n            max_ongoing_requests (int, optional): Maximum number of ongoing requests per replica. Defaults to 10.\n            test_mode (bool, optional): Enable test mode. Defaults to False.\n        Raises:\n            Exception: If Ray is not installed or deployment fails.\n            ValueError: If C++ runtime specific options are used with Python runtime.\n        '''\n        pass",
    "snippet_id": "snippet_367"
  },
  {
    "class_name": "apm_cli.core.script_runner.PromptCompiler",
    "skeleton": "\nclass PromptCompiler:\n    '''Compiles .prompt.md files with parameter substitution.'''\n\n    def __init__(self):\n        '''Initialize compiler.'''\n        pass\n\n    def compile(self, prompt_file: str, params: Dict[str, str]) -> str:\n        '''Compile a .prompt.md file with parameter substitution.\n        Args:\n            prompt_file: Path to the .prompt.md file\n            params: Parameters to substitute\n        Returns:\n            Path to the compiled file\n        '''\n        pass\n\n    def _substitute_parameters(self, content: str, params: Dict[str, str]) -> str:\n        '''Substitute parameters in content.\n        Args:\n            content: Content to process\n            params: Parameters to substitute\n        Returns:\n            Content with parameters substituted\n        '''\n        pass",
    "snippet_id": "snippet_368"
  },
  {
    "class_name": "apm_cli.core.script_runner.ScriptRunner",
    "skeleton": "\nclass ScriptRunner:\n    '''Executes APM scripts with auto-compilation of .prompt.md files.'''\n\n    def __init__(self, compiler=None):\n        '''Initialize script runner with optional compiler.'''\n        pass\n\n    def run_script(self, script_name: str, params: Dict[str, str]) -> bool:\n        '''Run a script from apm.yml with parameter substitution.\n        Args:\n            script_name: Name of the script to run\n            params: Parameters for compilation and script execution\n        Returns:\n            bool: True if script executed successfully\n        '''\n        pass\n\n    def list_scripts(self) -> Dict[str, str]:\n        '''List all available scripts from apm.yml.\n        Returns:\n            Dict mapping script names to their commands\n        '''\n        pass\n\n    def _load_config(self) -> Optional[Dict]:\n        '''Load apm.yml from current directory.'''\n        pass\n\n    def _auto_compile_prompts(self, command: str, params: Dict[str, str]) -> tuple[str, list[str]]:\n        '''Auto-compile .prompt.md files and transform runtime commands.\n        Args:\n            command: Original script command\n            params: Parameters for compilation\n        Returns:\n            Tuple of (compiled_command, list_of_compiled_prompt_files)\n        '''\n        pass\n\n    def _transform_runtime_command(self, command: str, prompt_file: str, compiled_content: str, compiled_path: str) -> str:\n        '''Transform runtime commands to their proper execution format.\n        Args:\n            command: Original command\n            prompt_file: Original .prompt.md file path\n            compiled_content: Compiled prompt content as string\n            compiled_path: Path to compiled .txt file\n        Returns:\n            Transformed command for proper runtime execution\n        '''\n        pass",
    "snippet_id": "snippet_369"
  },
  {
    "class_name": "apm_cli.registry.client.SimpleRegistryClient",
    "skeleton": "\nclass SimpleRegistryClient:\n    '''Simple client for querying MCP registries for server discovery.'''\n\n    def __init__(self, registry_url: Optional[str]=None):\n        '''Initialize the registry client.\n        Args:\n            registry_url (str, optional): URL of the MCP registry.\n                If not provided, uses the MCP_REGISTRY_URL environment variable\n                or falls back to the default demo registry.\n        '''\n        pass\n\n    def list_servers(self, limit: int=100, cursor: Optional[str]=None) -> Tuple[List[Dict[str, Any]], Optional[str]]:\n        '''List all available servers in the registry.\n        Args:\n            limit (int, optional): Maximum number of entries to return. Defaults to 100.\n            cursor (str, optional): Pagination cursor for retrieving next set of results.\n        Returns:\n            Tuple[List[Dict[str, Any]], Optional[str]]: List of server metadata dictionaries and the next cursor if available.\n        Raises:\n            requests.RequestException: If the request fails.\n        '''\n        pass\n\n    def search_servers(self, query: str) -> List[Dict[str, Any]]:\n        '''Search for servers in the registry.\n        Args:\n            query (str): Search query string.\n        Returns:\n            List[Dict[str, Any]]: List of matching server metadata dictionaries.\n        Raises:\n            requests.RequestException: If the request fails.\n        '''\n        pass\n\n    def get_server_info(self, server_id: str) -> Dict[str, Any]:\n        '''Get detailed information about a specific server.\n        Args:\n            server_id (str): ID of the server.\n        Returns:\n            Dict[str, Any]: Server metadata dictionary.\n        Raises:\n            requests.RequestException: If the request fails.\n            ValueError: If the server is not found.\n        '''\n        pass\n\n    def get_server_by_name(self, name: str) -> Optional[Dict[str, Any]]:\n        '''Find a server by its name.\n        Args:\n            name (str): Name of the server to find.\n        Returns:\n            Optional[Dict[str, Any]]: Server metadata dictionary or None if not found.\n        Raises:\n            requests.RequestException: If the request fails.\n        '''\n        pass\n\n    def find_server_by_reference(self, reference: str) -> Optional[Dict[str, Any]]:\n        '''Find a server by exact name match or server ID.\n        This is a simple, efficient lookup that only makes network requests when necessary:\n        1. Server ID (UUID format) - direct API call\n        2. Exact name match from server list - single additional API call\n        Args:\n            reference (str): Server reference (ID or exact name).\n        Returns:\n            Optional[Dict[str, Any]]: Server metadata dictionary or None if not found.\n        Raises:\n            requests.RequestException: If the request fails.\n        '''\n        pass",
    "snippet_id": "snippet_370"
  },
  {
    "class_name": "apm_cli.workflow.parser.WorkflowDefinition",
    "skeleton": "class WorkflowDefinition:\n    '''Simple container for workflow data.'''\n\n    def __init__(self, name, file_path, metadata, content):\n        '''Initialize a workflow definition.\n        Args:\n            name (str): Name of the workflow.\n            file_path (str): Path to the workflow file.\n            metadata (dict): Metadata from the frontmatter.\n            content (str): Content of the workflow file.\n        '''\n        pass\n\n    def validate(self):\n        '''Basic validation of required fields.\n        Returns:\n            list: List of validation errors.\n        '''\n        pass",
    "snippet_id": "snippet_371"
  },
  {
    "class_name": "natix.synthetic_data_generation.synthetic_data_generator.SyntheticDataGenerator",
    "skeleton": "\nclass SyntheticDataGenerator:\n    '''\n    A class for generating synthetic images and videos based on text prompts.\n    This class supports different prompt generation strategies and can utilize\n    various text-to-video (t2v) and text-to-image (t2i) models.\n    Attributes:\n        use_random_model: Whether to randomly select a t2v or t2i for each\n            generation task.\n        prompt_type: The type of prompt generation strategy ('random', 'annotation').\n        prompt_generator_name: Name of the prompt generation model.\n        model_name: Name of the t2v, t2i, or i2i model.\n        prompt_generator: The vlm/llm pipeline for generating input prompts for t2i/t2v models\n        output_dir: Directory to write generated data.\n    '''\n\n    def __init__(self, model_name: Optional[str]=None, use_random_model: bool=True, prompt_type: str='annotation', output_dir: Optional[Union[str, Path]]=None, image_cache: Optional[ImageCache]=None, device: str='cuda') -> None:\n        '''\n        Initialize the SyntheticDataGenerator.\n        Args:\n            model_name: Name of the generative image/video model\n            use_random_model: Whether to randomly select models for generation.\n            prompt_type: The type of prompt generation strategy.\n            output_dir: Directory to write generated data.\n            device: Device identifier.\n            image_cache: Optional image cache instance.\n        Raises:\n            ValueError: If an invalid model name is provided.\n            NotImplementedError: If an unsupported prompt type is specified.\n        '''\n        pass\n\n    def batch_generate(self, batch_size: int=5) -> None:\n        '''\n        Asynchronously generate synthetic data in batches.\n        Args:\n            batch_size: Number of prompts to generate in each batch.\n        '''\n        pass\n\n    def generate(self, image: Optional[Image.Image]=None, task: Optional[str]=None, model_name: Optional[str]=None) -> Dict[str, Any]:\n        '''\n        Generate synthetic data based on input parameters.\n        Args:\n            image: Input image for annotation-based generation.\n            modality: Type of media to generate ('image' or 'video').\n        Returns:\n            Dictionary containing generated data information.\n        Raises:\n            ValueError: If real_image is None when using annotation prompt type.\n            NotImplementedError: If prompt type is not supported.\n        '''\n        pass\n\n    def generate_prompt(self, image: Optional[Image.Image]=None, clear_gpu: bool=True) -> str:\n        '''Generate a prompt based on the specified strategy.'''\n        pass\n\n    def _run_generation(self, prompt: str, task: Optional[str]=None, model_name: Optional[str]=None, image: Optional[Image.Image]=None, label: Optional[int]=None, generate_at_target_size: bool=False) -> Dict[str, Any]:\n        '''\n        Generate synthetic data based on a text prompt.\n        Args:\n            prompt: The text prompt used to inspire the generation.\n            task: The generation task type ('t2i', 't2v', 'i2i', or None).\n            model_name: Optional model name to use for generation.\n            image: Optional input image for image-to-image generation.\n            generate_at_target_size: If True, generate at TARGET_IMAGE_SIZE dimensions.\n        Returns:\n            Dictionary containing generated data and metadata.\n        Raises:\n            RuntimeError: If generation fails.\n        '''\n        pass\n\n    def load_model(self, model_name: Optional[str]=None, modality: Optional[str]=None) -> None:\n        '''Load a Hugging Face text-to-image or text-to-video model.'''\n        pass\n\n    def clear_gpu(self) -> None:\n        '''Clear GPU memory by deleting models and running garbage collection.'''\n        pass\n\n    def unload_model(self) -> None:\n        '''Completely unload the current model from memory.'''\n        pass",
    "snippet_id": "snippet_372"
  },
  {
    "class_name": "lakebench.datagen._tpc._TPCDataGenerator",
    "skeleton": "\nclass _TPCDataGenerator:\n    '''\n    Base class for TPC data generation. PLEASE DO NOT INSTANTIATE THIS CLASS DIRECTLY. Use the TPCHDataGenerator and TPCDSDataGenerator\n    subclasses instead.\n        '''\n\n    def __init__(self, scale_factor: int, target_mount_folder_path: str=None, target_row_group_size_mb: int=128):\n        '''\n        Initialize the TPC data generator with a scale factor.\n        :param scale_factor: The scale factor for the data generation.\n        '''\n        pass\n\n    def run(self):\n        '''\n        This method uses DuckDB to generate in-memory tables based on the specified \n        scale factor and writes them to Parquet files. It estimates the average row \n        size in MB using a sample of the data since DuckDB only supports specifying \n        the number of rows per row group. The generated tables are written to the \n        specified target folder with optimized row group sizes.\n        Parameters\n        ----------\n        None\n        Notes\n        -----\n        - The method creates a sample Parquet file for each table to estimate row sizes.\n        - The full table is then written as Parquet files with optimized row group sizes.\n        - Temporary files and in-memory tables are cleaned up after processing.\n        '''\n        pass",
    "snippet_id": "snippet_373"
  },
  {
    "class_name": "Euro-BioImaging_EuBI-Bridge.eubi_bridge.base.data_manager.ChannelIterator",
    "skeleton": "class ChannelIterator:\n    '''\n    Iterator for generating and managing channel colors.\n    This class provides a way to iterate through a sequence of channel colors,\n    generating new colors in a visually distinct sequence when needed.\n    '''\n\n    def __init__(self, num_channels=0):\n        '''\n        Initialize the channel iterator.\n        Args:\n            num_channels: Initial number of channels to pre-generate\n        '''\n        pass\n\n    def _generate_channels(self, count):\n        '''Generate the specified number of unique channel colors.'''\n        pass\n    @staticmethod\n    def _hsv_to_rgb(h, s, v):\n        '''Convert HSV color space to RGB color space.'''\n        pass\n\n    def __iter__(self):\n        '''Return the iterator object itself.'''\n        pass\n\n    def __next__(self):\n        '''Return the next channel color.'''\n        pass\n\n    def get_channel(self, index):\n        '''\n        Get channel color by index.\n        Args:\n            index: Index of the channel to retrieve\n        Returns:\n            dict: Channel information with 'label' and 'color' keys\n        '''\n        pass\n\n    def __len__(self):\n        '''Return the number of generated channels.'''\n        pass",
    "snippet_id": "snippet_374"
  },
  {
    "class_name": "benchmarks.benchmarks.DielectricPlanarBenchmark",
    "skeleton": "\nclass DielectricPlanarBenchmark:\n    '''Benchmark the DielectricPlanar class.'''\n\n    def setup(self):\n        '''Setup the analysis objects.'''\n        pass\n\n    def time_single_dielectric_planar(self):\n        '''Benchmark of a complete run over a single frame.'''\n        pass",
    "snippet_id": "snippet_375"
  }
]