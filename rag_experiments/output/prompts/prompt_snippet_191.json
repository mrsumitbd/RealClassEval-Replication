[
  {
    "role": "assistant",
    "content": "You are an expert Python programmer who can correctly implement complete Python classes based on the provided class skeleton."
  },
  {
    "role": "user",
    "content": "For your understanding, following are some (skeleton, code) examples:\n\nExample skeleton:\n\n\nclass MultithreadedJobExecutor(JobExecutor):\n    '''\n    Experimental multi-threaded CWL executor.\n    Does simple resource accounting, will not start a job unless it\n    has cores / ram available, but does not make any attempt to\n    optimize usage.\n    '''\n\n    def __init__(self) -> None:\n        '''Initialize.'''\n        pass\n\n    def select_resources(self, request: dict[str, Union[int, float]], runtime_context: RuntimeContext) -> dict[str, Union[int, float]]:\n        '''Naïve check for available cpu cores and memory.'''\n        pass\n\n    def _runner(self, job: Union[JobBase, WorkflowJob, CallbackJob, ExpressionJob], runtime_context: RuntimeContext, TMPDIR_LOCK: threading.Lock) -> None:\n        '''Job running thread.'''\n        pass\n\n    def run_job(self, job: Optional[JobsType], runtime_context: RuntimeContext) -> None:\n        '''Execute a single Job in a separate thread.'''\n        pass\n\n    def wait_for_next_completion(self, runtime_context: RuntimeContext) -> None:\n        '''Wait for jobs to finish.'''\n        pass\n\n    def run_jobs(self, process: Process, job_order_object: CWLObjectType, logger: logging.Logger, runtime_context: RuntimeContext) -> None:\n        pass\nCorresponding class implementation:\n\nfrom .task_queue import TaskQueue\nimport logging\nfrom .errors import WorkflowException\nfrom .context import RuntimeContext, getdefault\nfrom .loghandler import _logger\nimport threading\nfrom .command_line_tool import CallbackJob, ExpressionJob\nfrom .process import Process, cleanIntermediate, relocateOutputs\nfrom .workflow_job import WorkflowJob, WorkflowJobStep\nimport math\nfrom typing import Optional, Union, cast\nfrom .job import JobBase\nimport psutil\nfrom .cuda import cuda_version_and_device_count\nfrom .utils import CWLObjectType, JobsType\nimport functools\n\nclass MultithreadedJobExecutor(JobExecutor):\n    \"\"\"\n    Experimental multi-threaded CWL executor.\n\n    Does simple resource accounting, will not start a job unless it\n    has cores / ram available, but does not make any attempt to\n    optimize usage.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize.\"\"\"\n        super().__init__()\n        self.exceptions: list[WorkflowException] = []\n        self.pending_jobs: list[JobsType] = []\n        self.pending_jobs_lock = threading.Lock()\n        self.max_ram = int(psutil.virtual_memory().available / 2 ** 20)\n        self.max_cores = float(psutil.cpu_count() or 1)\n        self.max_cuda = cuda_version_and_device_count()[1]\n        self.allocated_ram = float(0)\n        self.allocated_cores = float(0)\n        self.allocated_cuda: int = 0\n\n    def select_resources(self, request: dict[str, Union[int, float]], runtime_context: RuntimeContext) -> dict[str, Union[int, float]]:\n        \"\"\"Naïve check for available cpu cores and memory.\"\"\"\n        result: dict[str, Union[int, float]] = {}\n        maxrsc = {'cores': self.max_cores, 'ram': self.max_ram}\n        resources_types = {'cores', 'ram'}\n        if 'cudaDeviceCountMin' in request or 'cudaDeviceCountMax' in request:\n            maxrsc['cudaDeviceCount'] = self.max_cuda\n            resources_types.add('cudaDeviceCount')\n        for rsc in resources_types:\n            rsc_min = request[rsc + 'Min']\n            if rsc_min > maxrsc[rsc]:\n                raise WorkflowException(f'Requested at least {rsc_min} {rsc} but only {maxrsc[rsc]} available')\n            rsc_max = request[rsc + 'Max']\n            if rsc_max < maxrsc[rsc]:\n                result[rsc] = math.ceil(rsc_max)\n            else:\n                result[rsc] = maxrsc[rsc]\n        result['tmpdirSize'] = math.ceil(request['tmpdirMin'])\n        result['outdirSize'] = math.ceil(request['outdirMin'])\n        return result\n\n    def _runner(self, job: Union[JobBase, WorkflowJob, CallbackJob, ExpressionJob], runtime_context: RuntimeContext, TMPDIR_LOCK: threading.Lock) -> None:\n        \"\"\"Job running thread.\"\"\"\n        try:\n            _logger.debug('job: {}, runtime_context: {}, TMPDIR_LOCK: {}'.format(job, runtime_context, TMPDIR_LOCK))\n            job.run(runtime_context, TMPDIR_LOCK)\n        except WorkflowException as err:\n            _logger.exception(f'Got workflow error: {err}')\n            self.exceptions.append(err)\n        except Exception as err:\n            _logger.exception(f'Got workflow error: {err}')\n            wf_exc = WorkflowException(str(err))\n            wf_exc.__cause__ = err\n            wf_exc.__suppress_context__ = True\n            self.exceptions.append(wf_exc)\n        finally:\n            if runtime_context.workflow_eval_lock:\n                with runtime_context.workflow_eval_lock:\n                    if isinstance(job, JobBase):\n                        ram = job.builder.resources['ram']\n                        self.allocated_ram -= ram\n                        cores = job.builder.resources['cores']\n                        self.allocated_cores -= cores\n                        cudaDevices: int = cast(int, job.builder.resources.get('cudaDeviceCount', 0))\n                        self.allocated_cuda -= cudaDevices\n                    runtime_context.workflow_eval_lock.notify_all()\n\n    def run_job(self, job: Optional[JobsType], runtime_context: RuntimeContext) -> None:\n        \"\"\"Execute a single Job in a separate thread.\"\"\"\n        if job is not None:\n            with self.pending_jobs_lock:\n                self.pending_jobs.append(job)\n        with self.pending_jobs_lock:\n            n = 0\n            while n + 1 <= len(self.pending_jobs):\n                job = self.pending_jobs[n]\n                if isinstance(job, JobBase):\n                    ram = job.builder.resources['ram']\n                    cores = job.builder.resources['cores']\n                    cudaDevices = cast(int, job.builder.resources.get('cudaDeviceCount', 0))\n                    if ram > self.max_ram or cores > self.max_cores or cudaDevices > self.max_cuda:\n                        _logger.error('Job \"%s\" cannot be run, requests more resources (%s) than available on this host (already allocated ram is %d, allocated cores is %d, allocated CUDA is %d, max ram %d, max cores %d, max CUDA %d).', job.name, job.builder.resources, self.allocated_ram, self.allocated_cores, self.allocated_cuda, self.max_ram, self.max_cores, self.max_cuda)\n                        self.pending_jobs.remove(job)\n                        return\n                    if self.allocated_ram + ram > self.max_ram or self.allocated_cores + cores > self.max_cores or self.allocated_cuda + cudaDevices > self.max_cuda:\n                        _logger.debug('Job \"%s\" cannot run yet, resources (%s) are not available (already allocated ram is %d, allocated cores is %d, allocated CUDA devices is %d, max ram %d, max cores %d, max CUDA %d).', job.name, job.builder.resources, self.allocated_ram, self.allocated_cores, self.allocated_cuda, self.max_ram, self.max_cores, self.max_cuda)\n                        n += 1\n                        continue\n                if isinstance(job, JobBase):\n                    ram = job.builder.resources['ram']\n                    self.allocated_ram += ram\n                    cores = job.builder.resources['cores']\n                    self.allocated_cores += cores\n                    cuda = cast(int, job.builder.resources.get('cudaDevices', 0))\n                    self.allocated_cuda += cuda\n                self.taskqueue.add(functools.partial(self._runner, job, runtime_context, TMPDIR_LOCK), runtime_context.workflow_eval_lock)\n                self.pending_jobs.remove(job)\n\n    def wait_for_next_completion(self, runtime_context: RuntimeContext) -> None:\n        \"\"\"Wait for jobs to finish.\"\"\"\n        if runtime_context.workflow_eval_lock is not None:\n            runtime_context.workflow_eval_lock.wait(timeout=3)\n        if self.exceptions:\n            raise self.exceptions[0]\n\n    def run_jobs(self, process: Process, job_order_object: CWLObjectType, logger: logging.Logger, runtime_context: RuntimeContext) -> None:\n        self.taskqueue: TaskQueue = TaskQueue(threading.Lock(), int(math.ceil(self.max_cores)))\n        try:\n            jobiter = process.job(job_order_object, self.output_callback, runtime_context)\n            if runtime_context.workflow_eval_lock is None:\n                raise WorkflowException('runtimeContext.workflow_eval_lock must not be None')\n            runtime_context.workflow_eval_lock.acquire()\n            for job in jobiter:\n                if job is not None:\n                    if isinstance(job, JobBase):\n                        job.builder = runtime_context.builder or job.builder\n                        if job.outdir is not None:\n                            self.output_dirs.add(job.outdir)\n                self.run_job(job, runtime_context)\n                if job is None:\n                    if self.taskqueue.in_flight > 0:\n                        self.wait_for_next_completion(runtime_context)\n                    else:\n                        logger.error('Workflow cannot make any more progress.')\n                        break\n            self.run_job(None, runtime_context)\n            while self.taskqueue.in_flight > 0:\n                self.wait_for_next_completion(runtime_context)\n                self.run_job(None, runtime_context)\n            runtime_context.workflow_eval_lock.release()\n        finally:\n            self.taskqueue.drain()\n            self.taskqueue.join()\n\nExample skeleton:\n\n\nclass TestCase(NcpSupportMixin, unittest.TestCase):\n    '''The base class for all thread certification test cases.\n    The `topology` member of sub-class is used to create test topology.\n        '''\n\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def skipTest(self, reason: Any) -> None:\n        pass\n\n    def setUp(self):\n        pass\n\n    def _setUp(self):\n        '''Create simulator, nodes and apply configurations.\n        '''\n        pass\n\n    def _set_up_active_dataset(self, node, params):\n        pass\n\n    def inspect(self):\n        pass\n\n    def tearDown(self):\n        '''Destroy nodes and simulator.\n        '''\n        pass\n\n    def flush_all(self):\n        '''Flush away all captured messages of all nodes.\n        '''\n        pass\n\n    def flush_nodes(self, nodes):\n        '''Flush away all captured messages of specified nodes.\n        Args:\n            nodes (list): nodes whose messages to flush.\n        '''\n        pass\n\n    def _clean_up_tmp(self):\n        '''\n        Clean up node files in tmp directory\n        '''\n        pass\n\n    def _verify_packets(self, test_info_path: str):\n        pass\n    @property\n    def test_name(self):\n        pass\n\n    def collect_ipaddrs(self):\n        pass\n\n    def collect_rloc16s(self):\n        pass\n\n    def collect_rlocs(self):\n        pass\n\n    def collect_omrs(self):\n        pass\n\n    def collect_duas(self):\n        pass\n\n    def collect_leader_aloc(self, node):\n        pass\n\n    def collect_extra_vars(self, **vars):\n        pass\n\n    def _collect_test_info_after_setup(self):\n        '''\n        Collect test info after setUp\n        '''\n        pass\n\n    def _construct_backbone_network_name(self, backbone_network_id) -> str:\n        '''\n        Construct the name of the backbone network based on the given backbone network id from TOPOLOGY. If the\n        backbone_network_id is not defined in TOPOLOGY, use the default backbone network id.\n        '''\n        pass\n\n    def _output_test_info(self):\n        '''\n        Output test info to json file after tearDown\n        '''\n        pass\n\n    def _get_thread_pcap_filename(self):\n        pass\n\n    def assure_run_ok(self, cmd, shell=False):\n        pass\n\n    def _parse_params(self, params: Optional[dict]) -> dict:\n        pass\n\n    def _has_backbone_traffic(self):\n        pass\n\n    def _prepare_backbone_network(self):\n        '''\n        Creates one or more backbone networks (Docker bridge networks) based on the TOPOLOGY definition.\n        * If `backbone_network_id` is defined in the TOPOLOGY:\n            * Network name:   `backbone{PORT_OFFSET}.{backbone_network_id}`      (e.g., \"backbone0.0\", \"backbone0.1\")\n            * Network prefix: `backbone{PORT_OFFSET}:{backbone_network_id}::/64` (e.g., \"9100:0::/64\", \"9100:1::/64\")\n        * If `backbone_network_id` is undefined:\n            * Network name:   `backbone{PORT_OFFSET}.0`    (e.g., \"backbone0.0\")\n            * Network prefix: `backbone{PORT_OFFSET}::/64` (e.g., \"9100::/64\")\n        '''\n        pass\n\n    def _remove_backbone_network(self):\n        pass\n\n    def _start_backbone_sniffer(self):\n        pass\n\n    def _get_backbone_pcap_filename(self):\n        pass\n\n    def _get_merged_pcap_filename(self):\n        pass\n\n    def _stop_backbone_sniffer(self):\n        pass\n\n    def _merge_thread_backbone_pcaps(self):\n        pass\n\n    def wait_until(self, cond: Callable[[], bool], timeout: int, go_interval: int=1):\n        pass\n\n    def wait_node_state(self, node: Union[int, Node], state: str, timeout: int):\n        pass\n\n    def wait_route_established(self, node1: int, node2: int, timeout=10):\n        pass\n\n    def assertDictIncludes(self, actual: Mapping[str, str], expected: Mapping[str, str]):\n        ''' Asserts the `actual` dict includes the `expected` dict.\n        Args:\n            actual: A dict for checking.\n            expected: The expected items that the actual dict should contains.\n        '''\n        pass\nCorresponding class implementation:\n\nimport subprocess\nimport config\nimport os\nimport sys\nimport logging\nfrom pktverify import utils as pvutils\nimport json\nfrom typing import Optional, Callable, Union, Mapping, Any\nimport stat\nimport signal\nimport unittest\nimport debug\nimport traceback\nimport time\nfrom node import Node, OtbrNode, HostNode\n\nclass TestCase(NcpSupportMixin, unittest.TestCase):\n    \"\"\"The base class for all thread certification test cases.\n\n    The `topology` member of sub-class is used to create test topology.\n    \"\"\"\n    USE_MESSAGE_FACTORY = True\n    TOPOLOGY = {}\n    CASE_WIRESHARK_PREFS = None\n    SUPPORT_THREAD_1_1 = True\n    PACKET_VERIFICATION = config.PACKET_VERIFICATION_DEFAULT\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n        self._start_time = None\n        self._do_packet_verification = PACKET_VERIFICATION and hasattr(self, 'verify') and (self.PACKET_VERIFICATION == PACKET_VERIFICATION)\n        self._backbone_network_names = []\n\n    def skipTest(self, reason: Any) -> None:\n        self._testSkipped = True\n        super(TestCase, self).skipTest(reason)\n\n    def setUp(self):\n        self._testSkipped = False\n        if ENV_THREAD_VERSION == '1.1' and (not self.SUPPORT_THREAD_1_1):\n            self.skipTest('Thread 1.1 not supported.')\n        try:\n            self._setUp()\n        except:\n            traceback.print_exc()\n            for node in list(self.nodes.values()):\n                try:\n                    node.destroy()\n                except Exception:\n                    traceback.print_exc()\n            raise\n\n    def _setUp(self):\n        \"\"\"Create simulator, nodes and apply configurations.\n        \"\"\"\n        self._clean_up_tmp()\n        key_manager = config.create_default_thread_key_manager()\n        self.simulator = config.create_default_simulator(config.create_default_thread_message_factory(key_manager) if self.USE_MESSAGE_FACTORY else None)\n        self.nodes = {}\n        os.environ['LD_LIBRARY_PATH'] = '/tmp/thread-wireshark'\n        if self._has_backbone_traffic():\n            self._prepare_backbone_network()\n            self._start_backbone_sniffer()\n        self._initial_topology = initial_topology = {}\n        for i, params in self.TOPOLOGY.items():\n            params = self._parse_params(params)\n            initial_topology[i] = params\n            backbone_network_name = self._construct_backbone_network_name(params.get('backbone_network_id')) if self._has_backbone_traffic() else None\n            logging.info('Creating node %d: %r', i, params)\n            logging.info('Backbone network: %s', backbone_network_name)\n            if params['is_otbr']:\n                nodeclass = OtbrNode\n            elif params['is_host']:\n                nodeclass = HostNode\n            else:\n                nodeclass = Node\n            node = nodeclass(i, is_mtd=params['is_mtd'], simulator=self.simulator, name=params.get('name'), version=params['version'], is_bbr=params['is_bbr'], backbone_network=backbone_network_name)\n            if 'boot_delay' in params:\n                self.simulator.go(params['boot_delay'])\n            self.nodes[i] = node\n            if node.is_host:\n                continue\n            self.nodes[i].set_mode(params['mode'])\n            if 'partition_id' in params:\n                self.nodes[i].set_preferred_partition_id(params['partition_id'])\n            if params['is_ftd']:\n                self.nodes[i].set_router_selection_jitter(params['router_selection_jitter'])\n            if 'router_upgrade_threshold' in params:\n                self.nodes[i].set_router_upgrade_threshold(params['router_upgrade_threshold'])\n            if 'router_downgrade_threshold' in params:\n                self.nodes[i].set_router_downgrade_threshold(params['router_downgrade_threshold'])\n            if 'router_eligible' in params:\n                self.nodes[i].set_router_eligible(params['router_eligible'])\n            if 'prefer_router_id' in params:\n                self.nodes[i].prefer_router_id(params['prefer_router_id'])\n            if 'timeout' in params:\n                self.nodes[i].set_timeout(params['timeout'])\n            self._set_up_active_dataset(self.nodes[i], params)\n            if 'pending_dataset' in params:\n                self.nodes[i].set_pending_dataset(params['pending_dataset']['pendingtimestamp'], params['pending_dataset']['activetimestamp'], panid=params['pending_dataset'].get('panid'), channel=params['pending_dataset'].get('channel'), delay=params['pending_dataset'].get('delay'))\n            if 'key_sequence_counter' in params:\n                self.nodes[i].set_key_sequence_counter(params['key_sequence_counter'])\n            if 'network_id_timeout' in params:\n                self.nodes[i].set_network_id_timeout(params['network_id_timeout'])\n            if 'context_reuse_delay' in params:\n                self.nodes[i].set_context_reuse_delay(params['context_reuse_delay'])\n            if 'max_children' in params:\n                self.nodes[i].set_max_children(params['max_children'])\n            if 'bbr_registration_jitter' in params:\n                self.nodes[i].set_bbr_registration_jitter(params['bbr_registration_jitter'])\n            if 'router_id_range' in params:\n                self.nodes[i].set_router_id_range(params['router_id_range'][0], params['router_id_range'][1])\n        for i, params in initial_topology.items():\n            allowlist = params['allowlist']\n            if allowlist is None:\n                continue\n            for j in allowlist:\n                rssi = None\n                if isinstance(j, tuple):\n                    j, rssi = j\n                self.nodes[i].add_allowlist(self.nodes[j].get_addr64(), rssi=rssi)\n            self.nodes[i].enable_allowlist()\n        self._inspector = debug.Inspector(self)\n        self._collect_test_info_after_setup()\n\n    def _set_up_active_dataset(self, node, params):\n        dataset = {'timestamp': 1, 'channel': config.CHANNEL, 'channel_mask': config.CHANNEL_MASK, 'extended_panid': config.EXTENDED_PANID, 'mesh_local_prefix': config.MESH_LOCAL_PREFIX.split('/')[0], 'network_key': config.DEFAULT_NETWORK_KEY, 'network_name': config.NETWORK_NAME, 'panid': config.PANID, 'pskc': config.PSKC, 'security_policy': config.SECURITY_POLICY}\n        if 'channel' in params:\n            dataset['channel'] = params['channel']\n        if 'network_key' in params:\n            dataset['network_key'] = params['network_key']\n        if 'network_name' in params:\n            dataset['network_name'] = params['network_name']\n        if 'panid' in params:\n            dataset['panid'] = params['panid']\n        if 'active_dataset' in params:\n            dataset.update(params['active_dataset'])\n        node.set_active_dataset(**dataset)\n\n    def inspect(self):\n        self._inspector.inspect()\n\n    def tearDown(self):\n        \"\"\"Destroy nodes and simulator.\n        \"\"\"\n        if self._do_packet_verification and os.uname().sysname != 'Linux':\n            raise NotImplementedError(f'{self.test_name}: Packet Verification not available on {os.uname().sysname} (Linux only).')\n        if self._do_packet_verification:\n            self.simulator.go(3)\n        if self._has_backbone_traffic():\n            self._stop_backbone_sniffer()\n        for node in list(self.nodes.values()):\n            try:\n                node.stop()\n            except:\n                traceback.print_exc()\n            finally:\n                node.destroy()\n        self.simulator.stop()\n        if self._has_backbone_traffic():\n            self._remove_backbone_network()\n        if self._do_packet_verification:\n            if self._has_backbone_traffic():\n                pcap_filename = self._merge_thread_backbone_pcaps()\n            else:\n                pcap_filename = self._get_thread_pcap_filename()\n            self._test_info['pcap'] = pcap_filename\n            test_info_path = self._output_test_info()\n            if not self._testSkipped:\n                self._verify_packets(test_info_path)\n\n    def flush_all(self):\n        \"\"\"Flush away all captured messages of all nodes.\n        \"\"\"\n        for i in list(self.nodes.keys()):\n            self.simulator.get_messages_sent_by(i)\n\n    def flush_nodes(self, nodes):\n        \"\"\"Flush away all captured messages of specified nodes.\n\n        Args:\n            nodes (list): nodes whose messages to flush.\n\n        \"\"\"\n        for i in nodes:\n            if i in list(self.nodes.keys()):\n                self.simulator.get_messages_sent_by(i)\n\n    def _clean_up_tmp(self):\n        \"\"\"\n        Clean up node files in tmp directory\n        \"\"\"\n        os.system(f'rm -f tmp/{PORT_OFFSET}_*.flash tmp/{PORT_OFFSET}_*.data tmp/{PORT_OFFSET}_*.swap')\n\n    def _verify_packets(self, test_info_path: str):\n        pv = PacketVerifier(test_info_path, self.CASE_WIRESHARK_PREFS)\n        pv.add_common_vars()\n        pv.pkts.filter_thread_unallowed_icmpv6().must_not_next()\n        self.verify(pv)\n        print('Packet verification passed: %s' % test_info_path, file=sys.stderr)\n\n    @property\n    def test_name(self):\n        return os.getenv('TEST_NAME', 'current')\n\n    def collect_ipaddrs(self):\n        if not self._do_packet_verification:\n            return\n        test_info = self._test_info\n        for i, node in self.nodes.items():\n            ipaddrs = node.get_addrs()\n            if hasattr(node, 'get_ether_addrs'):\n                ipaddrs += node.get_ether_addrs()\n            test_info['ipaddrs'][i] = ipaddrs\n            if not node.is_host:\n                mleid = node.get_mleid()\n                test_info['mleids'][i] = mleid\n\n    def collect_rloc16s(self):\n        if not self._do_packet_verification:\n            return\n        test_info = self._test_info\n        test_info['rloc16s'] = {}\n        for i, node in self.nodes.items():\n            if not node.is_host:\n                test_info['rloc16s'][i] = '0x%04x' % node.get_addr16()\n\n    def collect_rlocs(self):\n        if not self._do_packet_verification:\n            return\n        test_info = self._test_info\n        test_info['rlocs'] = {}\n        for i, node in self.nodes.items():\n            if node.is_host:\n                continue\n            test_info['rlocs'][i] = node.get_rloc()\n\n    def collect_omrs(self):\n        if not self._do_packet_verification:\n            return\n        test_info = self._test_info\n        test_info['omrs'] = {}\n        for i, node in self.nodes.items():\n            if node.is_host:\n                continue\n            test_info['omrs'][i] = node.get_ip6_address(config.ADDRESS_TYPE.OMR)\n\n    def collect_duas(self):\n        if not self._do_packet_verification:\n            return\n        test_info = self._test_info\n        test_info['duas'] = {}\n        for i, node in self.nodes.items():\n            if node.is_host:\n                continue\n            test_info['duas'][i] = node.get_ip6_address(config.ADDRESS_TYPE.DUA)\n\n    def collect_leader_aloc(self, node):\n        if not self._do_packet_verification:\n            return\n        test_info = self._test_info\n        test_info['leader_aloc'] = self.nodes[node].get_addr_leader_aloc()\n\n    def collect_extra_vars(self, **vars):\n        if not self._do_packet_verification:\n            return\n        for k in vars.keys():\n            assert isinstance(k, str), k\n        test_vars = self._test_info.setdefault('extra_vars', {})\n        test_vars.update(vars)\n\n    def _collect_test_info_after_setup(self):\n        \"\"\"\n        Collect test info after setUp\n        \"\"\"\n        if not self._do_packet_verification:\n            return\n        test_info = self._test_info = {'script': os.path.abspath(sys.argv[0]), 'testcase': self.test_name, 'start_time': time.ctime(self._start_time), 'pcap': '', 'extaddrs': {}, 'ethaddrs': {}, 'ipaddrs': {}, 'mleids': {}, 'topology': self._initial_topology, 'backbone': {'interface': config.BACKBONE_DOCKER_NETWORK_NAME, 'prefix': config.BACKBONE_PREFIX}, 'domain_prefix': config.DOMAIN_PREFIX, 'env': {'PORT_OFFSET': config.PORT_OFFSET}}\n        for i, node in self.nodes.items():\n            if not node.is_host:\n                extaddr = node.get_addr64()\n                test_info['extaddrs'][i] = ExtAddr(extaddr).format_octets()\n            if node.is_host or node.is_otbr:\n                ethaddr = node.get_ether_mac()\n                test_info['ethaddrs'][i] = EthAddr(ethaddr).format_octets()\n\n    def _construct_backbone_network_name(self, backbone_network_id) -> str:\n        \"\"\"\n        Construct the name of the backbone network based on the given backbone network id from TOPOLOGY. If the\n        backbone_network_id is not defined in TOPOLOGY, use the default backbone network id.\n        \"\"\"\n        id = backbone_network_id if backbone_network_id is not None else config.BACKBONE_DOCKER_NETWORK_DEFAULT_ID\n        backbone_name = f'{config.BACKBONE_DOCKER_NETWORK_NAME}.{id}'\n        assert backbone_name in self._backbone_network_names\n        return backbone_name\n\n    def _output_test_info(self):\n        \"\"\"\n        Output test info to json file after tearDown\n        \"\"\"\n        filename = f'{self.test_name}.json'\n        with open(filename, 'wt') as ofd:\n            ofd.write(json.dumps(self._test_info, indent=1, sort_keys=True))\n        return filename\n\n    def _get_thread_pcap_filename(self):\n        current_pcap = self.test_name + '.pcap'\n        return os.path.abspath(current_pcap)\n\n    def assure_run_ok(self, cmd, shell=False):\n        if not shell and isinstance(cmd, str):\n            cmd = cmd.split()\n        proc = subprocess.run(cmd, stdout=sys.stdout, stderr=sys.stderr, shell=shell)\n        print('>>> %s => %d' % (cmd, proc.returncode), file=sys.stderr)\n        proc.check_returncode()\n\n    def _parse_params(self, params: Optional[dict]) -> dict:\n        params = params or {}\n        if params.get('is_bbr') or params.get('is_otbr'):\n            version = params.get('version', '1.4')\n            assert version != '1.1', params\n            params['version'] = version\n            params.setdefault('bbr_registration_jitter', config.DEFAULT_BBR_REGISTRATION_JITTER)\n        elif params.get('is_host'):\n            assert params.get('version', '') == '', params\n            params['version'] = ''\n        if params.get('version') == '1.2':\n            params['version'] = '1.4'\n        is_ftd = not params.get('is_mtd') and (not params.get('is_host'))\n        effective_params = DEFAULT_PARAMS.copy()\n        if is_ftd:\n            effective_params.update(FTD_DEFAULT_PARAMS)\n        effective_params.update(params)\n        return effective_params\n\n    def _has_backbone_traffic(self):\n        for param in self.TOPOLOGY.values():\n            if param and (param.get('is_otbr') or param.get('is_host')):\n                return True\n        return False\n\n    def _prepare_backbone_network(self):\n        \"\"\"\n        Creates one or more backbone networks (Docker bridge networks) based on the TOPOLOGY definition.\n\n        * If `backbone_network_id` is defined in the TOPOLOGY:\n            * Network name:   `backbone{PORT_OFFSET}.{backbone_network_id}`      (e.g., \"backbone0.0\", \"backbone0.1\")\n            * Network prefix: `backbone{PORT_OFFSET}:{backbone_network_id}::/64` (e.g., \"9100:0::/64\", \"9100:1::/64\")\n\n        * If `backbone_network_id` is undefined:\n            * Network name:   `backbone{PORT_OFFSET}.0`    (e.g., \"backbone0.0\")\n            * Network prefix: `backbone{PORT_OFFSET}::/64` (e.g., \"9100::/64\")\n        \"\"\"\n        backbone_id_set = set()\n        for node in self.TOPOLOGY:\n            id = self.TOPOLOGY[node].get('backbone_network_id')\n            if id is not None:\n                backbone_id_set.add(id)\n        if not backbone_id_set:\n            backbone_id_set.add(config.BACKBONE_DOCKER_NETWORK_DEFAULT_ID)\n        for id in backbone_id_set:\n            backbone = f'{config.BACKBONE_DOCKER_NETWORK_NAME}.{id}'\n            backbone_prefix = f'{config.BACKBONE_IPV6_ADDR_START}:{id}::/64'\n            self._backbone_network_names.append(backbone)\n            self.assure_run_ok(f'docker network create --driver bridge --ipv6 --subnet {backbone_prefix} -o \"com.docker.network.bridge.name\"=\"{backbone}\" {backbone} || true', shell=True)\n\n    def _remove_backbone_network(self):\n        for network_name in self._backbone_network_names:\n            self.assure_run_ok(f'docker network rm {network_name}', shell=True)\n\n    def _start_backbone_sniffer(self):\n        assert self._backbone_network_names, 'Internal Error: self._backbone_network_names is empty'\n        sniffer_interface = self._backbone_network_names[0]\n        pcap_file = self._get_backbone_pcap_filename()\n        try:\n            os.remove(pcap_file)\n        except FileNotFoundError:\n            pass\n        dumpcap = pvutils.which_dumpcap()\n        self._dumpcap_proc = subprocess.Popen([dumpcap, '-i', sniffer_interface, '-w', pcap_file], stdout=sys.stdout, stderr=sys.stderr)\n        time.sleep(0.2)\n        assert self._dumpcap_proc.poll() is None, 'tshark terminated unexpectedly'\n        logging.info('Backbone sniffer launched successfully on interface %s, pid=%s', sniffer_interface, self._dumpcap_proc.pid)\n        os.chmod(pcap_file, stat.S_IWUSR | stat.S_IRUSR | stat.S_IRGRP | stat.S_IROTH)\n\n    def _get_backbone_pcap_filename(self):\n        backbone_pcap = self.test_name + '_backbone.pcap'\n        return os.path.abspath(backbone_pcap)\n\n    def _get_merged_pcap_filename(self):\n        backbone_pcap = self.test_name + '_merged.pcap'\n        return os.path.abspath(backbone_pcap)\n\n    def _stop_backbone_sniffer(self):\n        self._dumpcap_proc.send_signal(signal.SIGTERM)\n        self._dumpcap_proc.__exit__(None, None, None)\n        logging.info('Backbone sniffer terminated successfully: pid=%s' % self._dumpcap_proc.pid)\n\n    def _merge_thread_backbone_pcaps(self):\n        thread_pcap = self._get_thread_pcap_filename()\n        backbone_pcap = self._get_backbone_pcap_filename()\n        merged_pcap = self._get_merged_pcap_filename()\n        mergecap = pvutils.which_mergecap()\n        self.assure_run_ok(f'{mergecap} -w {merged_pcap} {thread_pcap} {backbone_pcap}', shell=True)\n        return merged_pcap\n\n    def wait_until(self, cond: Callable[[], bool], timeout: int, go_interval: int=1):\n        while True:\n            self.simulator.go(go_interval)\n            if cond():\n                break\n            timeout -= go_interval\n            if timeout <= 0:\n                raise RuntimeError(f'wait failed after {timeout} seconds')\n\n    def wait_node_state(self, node: Union[int, Node], state: str, timeout: int):\n        node = self.nodes[node] if isinstance(node, int) else node\n        self.wait_until(lambda: node.get_state() == state, timeout)\n\n    def wait_route_established(self, node1: int, node2: int, timeout=10):\n        node2_addr = self.nodes[node2].get_ip6_address(config.ADDRESS_TYPE.RLOC)\n        while timeout > 0:\n            if self.nodes[node1].ping(node2_addr):\n                break\n            self.simulator.go(1)\n            timeout -= 1\n        else:\n            raise Exception('Route between node %d and %d is not established' % (node1, node2))\n\n    def assertDictIncludes(self, actual: Mapping[str, str], expected: Mapping[str, str]):\n        \"\"\" Asserts the `actual` dict includes the `expected` dict.\n\n        Args:\n            actual: A dict for checking.\n            expected: The expected items that the actual dict should contains.\n        \"\"\"\n        for k, v in expected.items():\n            if k not in actual:\n                raise AssertionError(f'key {k} is not found in first dict')\n            if v != actual[k]:\n                raise AssertionError(f'{repr(actual[k])} != {repr(v)} for key {k}')\n\nNow, implement the following class. Do not explain the code. The given class skeleton is as follows:\n\nclass ExecutionMixin:\n    '''Handles the core DAG execution and training loop logic.'''\n\n    def execute_task_graph(self):\n        '''Main entry point to start the DAG execution pipeline.'''\n        pass\n\n    def _run_training_loop(self):\n        '''\n        The main loop that iterates through training steps and epochs.\n        '''\n        pass\n\n    def _find_first_non_compute_ancestor(self, start_node_id: str) -> Optional[Node]:\n        '''\n        Traverses upwards from a starting node to find the first ancestor\n        that is not of type COMPUTE.\n        Uses a Breadth-First Search (BFS) strategy to prioritize finding the\n        closest ancestor by level.\n        '''\n        pass\n\n    def _cleanup_step_buffers(self, visited_nodes: Set[str], timing_raw: dict) -> None:\n        '''\n        Encapsulates the logic for resetting and clearing all step-related buffers.\n        This includes the distributed Ray data buffers and the local internal cache.\n        This is called at the end of a step, whether it completed successfully or was aborted.\n        '''\n        pass\n\n    def _run_training_step(self, epoch: int, batch_idx: int) -> Optional[List[Tuple[str, Any]]]:\n        '''Executes a single training step by traversing the computational graph.'''\n        pass"
  }
]