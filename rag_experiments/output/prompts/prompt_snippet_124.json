[
  {
    "role": "assistant",
    "content": "You are an expert Python programmer who can correctly implement complete Python classes based on the provided class skeleton."
  },
  {
    "role": "user",
    "content": "For your understanding, following are some (skeleton, code) examples:\n\nExample skeleton:\n\n\nclass SparseConvFunction(Function):\n    '''Sparse Convolution.\n    Please refer to `SECOND <https://www.mdpi.com/1424-8220/18/10/3337>`_ for\n    more details.\n    '''\n    @staticmethod\n    def forward(ctx: Any, features: torch.Tensor, filters: torch.nn.Parameter, indice_pairs: torch.Tensor, indice_pair_num: torch.Tensor, num_activate_out: torch.Tensor) -> torch.Tensor:\n        '''\n        Args:\n            features (torch.Tensor): Features that needs to convolute.\n            filters (torch.nn.parameter.Parameter): Convolution filters.\n            indice_pairs (torch.Tensor): Indice pairs between inputs locations\n                and outputs locations.\n            indice_pair_num (torch.Tensor): Indice pairs num.\n            num_activate_out (torch.Tensor): Output channels num.\n        Returns:\n            torch.Tensor: Output features from gather-gemm-scatter.\n        '''\n        pass\n    @staticmethod\n    def backward(ctx: Any, grad_output: torch.Tensor) -> tuple:\n        pass\nCorresponding class implementation:\n\nfrom typing import Any\nimport torch\nfrom . import sparse_ops as ops\nfrom torch.autograd import Function\n\nclass SparseConvFunction(Function):\n    \"\"\"Sparse Convolution.\n\n    Please refer to `SECOND <https://www.mdpi.com/1424-8220/18/10/3337>`_ for\n    more details.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx: Any, features: torch.Tensor, filters: torch.nn.Parameter, indice_pairs: torch.Tensor, indice_pair_num: torch.Tensor, num_activate_out: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            features (torch.Tensor): Features that needs to convolute.\n            filters (torch.nn.parameter.Parameter): Convolution filters.\n            indice_pairs (torch.Tensor): Indice pairs between inputs locations\n                and outputs locations.\n            indice_pair_num (torch.Tensor): Indice pairs num.\n            num_activate_out (torch.Tensor): Output channels num.\n\n        Returns:\n            torch.Tensor: Output features from gather-gemm-scatter.\n        \"\"\"\n        ctx.save_for_backward(indice_pairs, indice_pair_num, features, filters)\n        return ops.indice_conv(features, filters, indice_pairs, indice_pair_num, num_activate_out, False)\n\n    @staticmethod\n    def backward(ctx: Any, grad_output: torch.Tensor) -> tuple:\n        indice_pairs, indice_pair_num, features, filters = ctx.saved_tensors\n        input_bp, filters_bp = ops.indice_conv_backward(features, filters, grad_output, indice_pairs, indice_pair_num, False)\n        return (input_bp, filters_bp, None, None, None)\n\nExample skeleton:\n\n\nclass FastSpeech2ConformerEncoderLayer(nn.Module):\n\n    def __init__(self, config: FastSpeech2ConformerConfig, module_config):\n        pass\n\n    def forward(self, hidden_states: torch.Tensor, pos_emb: Optional[torch.Tensor]=None, attention_mask: Optional[torch.Tensor]=None, output_attentions: Optional[torch.Tensor]=False):\n        '''\n        Compute encoded features.\n        Args:\n            hidden_states (`torch.Tensor` of shape `(batch, time, size)`): Input tensor.\n            pos_emb (`torch.Tensor` of shape `(1, time, size)`): Positional embeddings tensor.\n            attention_mask (`torch.Tensor` of shape `(batch, time)`): Attention mask tensor for the input.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n        Returns:\n            `torch.Tensor`: Output tensor of shape `(batch, time, size)`.\n        '''\n        pass\nCorresponding class implementation:\n\nimport torch\nfrom .configuration_fastspeech2_conformer import FastSpeech2ConformerConfig, FastSpeech2ConformerHifiGanConfig, FastSpeech2ConformerWithHifiGanConfig\nfrom typing import Optional, Union\nfrom torch import nn\n\nclass FastSpeech2ConformerEncoderLayer(nn.Module):\n\n    def __init__(self, config: FastSpeech2ConformerConfig, module_config):\n        super().__init__()\n        self.self_attn = FastSpeech2ConformerAttention(config, module_config)\n        self.feed_forward = FastSpeech2ConformerMultiLayeredConv1d(config, module_config)\n        self.macaron_style = config.use_macaron_style_in_conformer\n        if self.macaron_style:\n            self.feed_forward_macaron = FastSpeech2ConformerMultiLayeredConv1d(config, module_config)\n            self.ff_macaron_layer_norm = nn.LayerNorm(config.hidden_size)\n            self.ff_scale = 0.5\n        else:\n            self.ff_scale = 1.0\n        self.use_cnn_module = config.use_cnn_in_conformer\n        if self.use_cnn_module:\n            self.conv_module = FastSpeech2ConformerConvolutionModule(config, module_config)\n            self.conv_layer_norm = nn.LayerNorm(config.hidden_size)\n            self.final_layer_norm = nn.LayerNorm(config.hidden_size)\n        self.ff_layer_norm = nn.LayerNorm(config.hidden_size)\n        self.self_attn_layer_norm = nn.LayerNorm(config.hidden_size)\n        self.dropout = nn.Dropout(module_config['dropout_rate'])\n        self.size = config.hidden_size\n        self.normalize_before = module_config['normalize_before']\n        self.concat_after = module_config['concat_after']\n        if self.concat_after:\n            self.concat_linear = nn.Linear(config.hidden_size + config.hidden_size, config.hidden_size)\n\n    def forward(self, hidden_states: torch.Tensor, pos_emb: Optional[torch.Tensor]=None, attention_mask: Optional[torch.Tensor]=None, output_attentions: Optional[torch.Tensor]=False):\n        \"\"\"\n        Compute encoded features.\n\n        Args:\n            hidden_states (`torch.Tensor` of shape `(batch, time, size)`): Input tensor.\n            pos_emb (`torch.Tensor` of shape `(1, time, size)`): Positional embeddings tensor.\n            attention_mask (`torch.Tensor` of shape `(batch, time)`): Attention mask tensor for the input.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n        Returns:\n            `torch.Tensor`: Output tensor of shape `(batch, time, size)`.\n\n        \"\"\"\n        if self.macaron_style:\n            residual = hidden_states\n            if self.normalize_before:\n                hidden_states = self.ff_macaron_layer_norm(hidden_states)\n            hidden_states = residual + self.ff_scale * self.dropout(self.feed_forward_macaron(hidden_states))\n            if not self.normalize_before:\n                hidden_states = self.ff_macaron_layer_norm(hidden_states)\n        residual = hidden_states\n        if self.normalize_before:\n            hidden_states = self.self_attn_layer_norm(hidden_states)\n        attention_output, attention_scores = self.self_attn(hidden_states, attention_mask=attention_mask, pos_emb=pos_emb, output_attentions=output_attentions)\n        if self.concat_after:\n            x_concat = torch.cat((hidden_states, attention_output), dim=-1)\n            hidden_states = self.concat_linear(x_concat)\n            hidden_states = residual + hidden_states\n        else:\n            hidden_states = self.dropout(attention_output)\n            hidden_states = residual + hidden_states\n        if not self.normalize_before:\n            hidden_states = self.self_attn_layer_norm(hidden_states)\n        if self.use_cnn_module:\n            residual = hidden_states\n            if self.normalize_before:\n                hidden_states = self.conv_layer_norm(hidden_states)\n            hidden_states = self.conv_module(hidden_states)\n            hidden_states = self.dropout(hidden_states)\n            hidden_states = residual + hidden_states\n            if not self.normalize_before:\n                hidden_states = self.conv_layer_norm(hidden_states)\n        residual = hidden_states\n        if self.normalize_before:\n            hidden_states = self.ff_layer_norm(hidden_states)\n        hidden_states = self.feed_forward(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = residual + self.ff_scale * hidden_states\n        if not self.normalize_before:\n            hidden_states = self.ff_layer_norm(hidden_states)\n        if self.conv_module is not None:\n            hidden_states = self.final_layer_norm(hidden_states)\n        outputs = (hidden_states,)\n        if output_attentions:\n            outputs += (attention_scores,)\n        return outputs\n\nNow, implement the following class. Do not explain the code. The given class skeleton is as follows:\n\nclass FluxDepthPreprocessor:\n    '''\n    Node for applying a depth preprocessor model to an input image.\n    .. warning::\n        This node will be deprecated in October 2025.\n        Please use the ``Depth Anything`` node in `comfyui_controlnet_aux <https://github.com/Fannovel16/comfyui_controlnet_aux>`_.\n    '''\n    @classmethod\n    def INPUT_TYPES(s):\n        '''\n        Defines the input types and tooltips for the node.\n        Returns\n        -------\n        dict\n            A dictionary specifying the required inputs and their descriptions for the node interface.\n        '''\n        pass\n\n    def depth_preprocess(self, image, model_path):\n        '''\n        Apply the selected depth preprocessor model to the input image.\n        Parameters\n        ----------\n        image : np.ndarray or torch.Tensor\n            The input image to process.\n        model_path : str\n            The name of the depth preprocessor model checkpoint.\n        Returns\n        -------\n        tuple\n            A tuple containing the depth map as a torch.Tensor.\n        '''\n        pass"
  }
]