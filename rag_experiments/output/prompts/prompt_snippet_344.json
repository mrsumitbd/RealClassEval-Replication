[
  {
    "role": "assistant",
    "content": "You are an expert Python programmer who can correctly implement complete Python classes based on the provided class skeleton."
  },
  {
    "role": "user",
    "content": "For your understanding, following are some (skeleton, code) examples:\n\nExample skeleton:\n\n\nclass TensorforceAgent(Agent):\n    '''\n    Tensorforce agent (specification key: `tensorforce`).\n    Highly configurable agent and basis for a broad class of deep reinforcement learning agents,\n    which act according to a policy parametrized by a neural network, leverage a memory module for\n    periodic updates based on batches of experience, and optionally employ a baseline/critic/target\n    policy for improved reward estimation.\n    Args:\n        states (specification): States specification\n            (<span style=\"color:#C00000\"><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create()`), arbitrarily nested dictionary of state\n            descriptions (usually taken from `Environment.states()`) with the following attributes:\n            <ul>\n            <li><b>type</b> (<i>\"bool\" | \"int\" | \"float\"</i>) &ndash; state data type\n            (<span style=\"color:#00C000\"><b>default</b></span>: \"float\").</li>\n            <li><b>shape</b> (<i>int | iter[int]</i>) &ndash; state shape\n            (<span style=\"color:#C00000\"><b>required</b></span>).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete state values\n            (<span style=\"color:#C00000\"><b>required</b></span> for type \"int\").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum state value\n            (<span style=\"color:#00C000\"><b>optional</b></span> for type \"float\").</li>\n            </ul>\n        actions (specification): Actions specification\n            (<span style=\"color:#C00000\"><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create()`), arbitrarily nested dictionary of\n            action descriptions (usually taken from `Environment.actions()`) with the following\n            attributes:\n            <ul>\n            <li><b>type</b> (<i>\"bool\" | \"int\" | \"float\"</i>) &ndash; action data type\n            (<span style=\"color:#C00000\"><b>required</b></span>).</li>\n            <li><b>shape</b> (<i>int > 0 | iter[int > 0]</i>) &ndash; action shape\n            (<span style=\"color:#00C000\"><b>default</b></span>: scalar).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete action values\n            (<span style=\"color:#C00000\"><b>required</b></span> for type \"int\").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum action value\n            (<span style=\"color:#00C000\"><b>optional</b></span> for type \"float\").</li>\n            </ul>\n        max_episode_timesteps (int > 0): Upper bound for numer of timesteps per episode\n            (<span style=\"color:#00C000\"><b>default</b></span>: not given, better implicitly\n            specified via `environment` argument for `Agent.create()`).\n        policy (specification): Policy configuration, see [networks](../modules/networks.html) and\n            [policies documentation](../modules/policies.html)\n            (<span style=\"color:#00C000\"><b>default</b></span>: action distributions or value\n            functions parametrized by an automatically configured network).\n        memory (int | specification): Replay memory capacity, or memory configuration, see the\n            [memories documentation](../modules/memories.html)\n            (<span style=\"color:#00C000\"><b>default</b></span>: minimum capacity recent memory).\n        update (int | specification): Model update configuration with the following attributes\n            (<span style=\"color:#C00000\"><b>required</b>,\n            <span style=\"color:#00C000\"><b>default</b></span>: timesteps batch size</span>):\n            <ul>\n            <li><b>unit</b> (<i>\"timesteps\" | \"episodes\"</i>) &ndash; unit for update attributes\n            (<span style=\"color:#C00000\"><b>required</b></span>).</li>\n            <li><b>batch_size</b>\n            (<i><a href=\"../modules/parameters.html\">parameter</a>, int > 0</i>) &ndash;\n            size of update batch in number of units\n            (<span style=\"color:#C00000\"><b>required</b></span>).</li>\n            <li><b>frequency</b>\n            (<i>\"never\" | <a href=\"../modules/parameters.html\">parameter</a>, int > 0 | 0.0 < float <= 1.0</i>) &ndash;\n            frequency of updates, relative to batch_size if float\n            (<span style=\"color:#00C000\"><b>default</b></span>: batch_size).</li>\n            <li><b>start</b>\n            (<i><a href=\"../modules/parameters.html\">parameter</a>, int >= batch_size</i>) &ndash;\n            number of units before first update\n            (<span style=\"color:#00C000\"><b>default</b></span>: none).</li>\n            </ul>\n        optimizer (specification): Optimizer configuration, see the\n            [optimizers documentation](../modules/optimizers.html)\n            (<span style=\"color:#00C000\"><b>default</b></span>: Adam optimizer).\n        objective (specification): Optimization objective configuration, see the\n            [objectives documentation](../modules/objectives.html)\n            (<span style=\"color:#C00000\"><b>required</b></span>).\n        reward_estimation (specification): Reward estimation configuration with the following\n            attributes (<span style=\"color:#C00000\"><b>required</b></span>):\n            <ul>\n            <li><b>horizon</b>\n            (<i>\"episode\" | <a href=\"../modules/parameters.html\">parameter</a>, int >= 1</i>)\n            &ndash; Horizon of discounted-sum return estimation\n            (<span style=\"color:#C00000\"><b>required</b></span>).</li>\n            <li><b>discount</b>\n            (<i><a href=\"../modules/parameters.html\">parameter</a>, 0.0 <= float <= 1.0</i>) &ndash;\n            Discount factor of future rewards for discounted-sum return estimation\n            (<span style=\"color:#00C000\"><b>default</b></span>: 1.0).</li>\n            <li><b>predict_horizon_values</b> (<i>false | \"early\" | \"late\"</i>) &ndash; Whether to\n            include a baseline prediction of the horizon value as part of the return estimation, and\n            if so, whether to compute the horizon value prediction \"early\" when experiences are\n            stored to memory, or \"late\" when batches of experience are retrieved for the update\n            (<span style=\"color:#00C000\"><b>default</b></span>: \"late\" if baseline_policy or\n            baseline_objective are specified, else false).</li>\n            <li><b>estimate_advantage</b> (<i>False | \"early\" | \"late\"</i>) &ndash; Whether to use\n            an estimate of the advantage (return minus baseline value prediction) instead of the\n            return as learning signal, and whether to do so late after the baseline update\n            (default) or early before the baseline update\n            (<span style=\"color:#00C000\"><b>default</b></span>: false, unless baseline_policy is\n            specified but baseline_objective/optimizer are not).</li>\n            <li><b>predict_action_values</b> (<i>bool</i>) &ndash; Whether to predict state-action-\n            instead of state-values as horizon values and for advantage estimation\n            (<span style=\"color:#00C000\"><b>default</b></span>: false).</li>\n            <li><b>reward_processing</b> (<i>specification)</i>) &ndash; Reward preprocessing as\n            layer or list of layers, see the\n            [preprocessing documentation](../modules/preprocessing.html)\n            (<span style=\"color:#00C000\"><b>default</b></span>: no reward processing).</li>\n            <li><b>return_processing</b> (<i>specification</i>) &ndash; Return processing as layer\n            or list of layers, see the [preprocessing documentation](../modules/preprocessing.html)\n            (<span style=\"color:#00C000\"><b>default</b></span>: no return processing).</li>\n            <li><b>advantage_processing</b> (<i>specification</i>) &ndash; Advantage processing as\n            layer or list of layers, see the [preprocessing documentation](../modules/preprocessing.html)\n            (<span style=\"color:#00C000\"><b>default</b></span>: no advantage processing).</li>\n            <li><b>predict_terminal_values</b> (<i>bool</i>) &ndash; Whether to predict the value\n            of terminal states, usually not required since max_episode_timesteps terminals are\n            handled separately\n            (<span style=\"color:#00C000\"><b>default</b></span>: false).</li>\n            </ul>\n        baseline (specification): Baseline configuration, policy will be used as baseline if none,\n            see [networks](../modules/networks.html) and potentially\n            [policies documentation](../modules/policies.html)\n            (<span style=\"color:#00C000\"><b>default</b></span>: none).\n        baseline_optimizer (specification | <a href=\"../modules/parameters.html\">parameter</a>, float > 0.0):\n            Baseline optimizer configuration, see the\n            [optimizers documentation](../modules/optimizers.html),\n            main optimizer will be used for baseline if none, a float implies none and specifies a\n            custom weight for the baseline loss\n            (<span style=\"color:#00C000\"><b>default</b></span>: none).\n        baseline_objective (specification): Baseline optimization objective configuration, see the\n            [objectives documentation](../modules/objectives.html),\n            required if baseline optimizer is specified, main objective will be used for baseline if\n            baseline objective and optimizer are not specified\n            (<span style=\"color:#00C000\"><b>default</b></span>: none).\n        l2_regularization (<a href=\"../modules/parameters.html\">parameter</a>, float >= 0.0):\n            L2 regularization loss weight\n            (<span style=\"color:#00C000\"><b>default</b></span>: no L2 regularization).\n        entropy_regularization (<a href=\"../modules/parameters.html\">parameter</a>, float >= 0.0):\n            Entropy regularization loss weight, to discourage the policy distribution from being\n            \"too certain\"\n            (<span style=\"color:#00C000\"><b>default</b></span>: no entropy regularization).\n        state_preprocessing (dict[specification]): State preprocessing as layer or list of layers,\n            see the [preprocessing documentation](../modules/preprocessing.html),\n            specified per state-type or -name\n            (<span style=\"color:#00C000\"><b>default</b></span>: linear normalization of bounded\n            float states to [-2.0, 2.0]).\n        exploration (<a href=\"../modules/parameters.html\">parameter</a> | dict[<a href=\"../modules/parameters.html\">parameter</a>], float >= 0.0):\n            Exploration, defined as the probability for uniformly random output in case of `bool`\n            and `int` actions, and the standard deviation of Gaussian noise added to every output in\n            case of `float` actions, specified globally or per action-type or -name\n            (<span style=\"color:#00C000\"><b>default</b></span>: no exploration).\n        variable_noise (<a href=\"../modules/parameters.html\">parameter</a>, float >= 0.0):\n            Add Gaussian noise with given standard deviation to all trainable variables, as\n            alternative exploration mechanism\n            (<span style=\"color:#00C000\"><b>default</b></span>: no variable noise).\n        parallel_interactions (int > 0): Maximum number of parallel interactions to support,\n            for instance, to enable multiple parallel episodes, environments or agents within an\n            environment\n            (<span style=\"color:#00C000\"><b>default</b></span>: 1).\n        config (specification): Additional configuration options:\n            <ul>\n            <li><b>name</b> (<i>string</i>) &ndash; Agent name, used e.g. for TensorFlow scopes and\n            saver default filename\n            (<span style=\"color:#00C000\"><b>default</b></span>: \"agent\").\n            <li><b>device</b> (<i>string</i>) &ndash; Device name\n            (<span style=\"color:#00C000\"><b>default</b></span>: CPU). Different from (un)supervised\n            deep learning, RL does not always benefit from running on a GPU, depending on\n            environment and agent configuration. In particular for RL-typical environments with\n            low-dimensional state spaces (i.e., no images), one usually gets better performance by\n            running on CPU only. Consequently, Tensorforce is configured to run on CPU by default,\n            which can be changed, for instance, by setting this value to 'GPU' instead.\n            <li><b>seed</b> (<i>int</i>) &ndash; Random seed to set for Python, NumPy (both set\n            globally!) and TensorFlow, environment seed may have to be set separately for fully\n            deterministic execution, generally not recommended since results in a fully\n            deterministic setting are less meaningful/representative\n            (<span style=\"color:#00C000\"><b>default</b></span>: none).</li>\n            <li><b>buffer_observe</b> (<i>false | \"episode\" | int > 0</i>) &ndash; Number of\n            timesteps within an episode to buffer before calling the internal observe function, to\n            reduce calls to TensorFlow for improved performance\n            (<span style=\"color:#00C000\"><b>default</b></span>: configuration-specific maximum\n            number which can be buffered without affecting performance).</li>\n            <li><b>enable_int_action_masking</b> (<i>bool</i>) &ndash; Whether int action options\n            can be masked via an optional \"[ACTION-NAME]_mask\" state input\n            (<span style=\"color:#00C000\"><b>default</b></span>: true).</li>\n            <li><b>create_tf_assertions</b> (<i>bool</i>) &ndash; Whether to create internal\n            TensorFlow assertion operations\n            (<span style=\"color:#00C000\"><b>default</b></span>: true).</li>\n            <li><b>eager_mode</b> (<i>bool</i>) &ndash; Whether to run functions eagerly instead of\n            running as a traced graph function, can be helpful for debugging\n            (<span style=\"color:#00C000\"><b>default</b></span>: false).</li>\n            <li><b>tf_log_level</b> (<i>int >= 0</i>) &ndash; TensorFlow log level, additional C++\n            logging messages can be enabled by setting os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"/\"2\"\n            before importing Tensorforce/TensorFlow\n            (<span style=\"color:#00C000\"><b>default</b></span>: 40, only error and critical).</li>\n            </ul>\n        saver (path | specification): TensorFlow checkpoints directory, or checkpoint manager\n            configuration with the following attributes, for periodic implicit saving as alternative\n            to explicit saving via agent.save()\n            (<span style=\"color:#00C000\"><b>default</b></span>: no saver):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; checkpoint directory\n            (<span style=\"color:#C00000\"><b>required</b></span>).</li>\n            <li><b>filename</b> (<i>string</i>) &ndash; checkpoint filename\n            (<span style=\"color:#00C000\"><b>default</b></span>: agent name).</li>\n            <li><b>frequency</b> (<i>int > 0</i>) &ndash; how frequently to save a checkpoint\n            (<span style=\"color:#C00000\"><b>required</b></span>).</li>\n            <li><b>unit</b> (<i>\"timesteps\" | \"episodes\" | \"updates\"</i>) &ndash; frequency unit\n            (<span style=\"color:#00C000\"><b>default</b></span>: updates).</li>\n            <li><b>max_checkpoints</b> (<i>int > 0</i>) &ndash; maximum number of checkpoints to\n            keep (<span style=\"color:#00C000\"><b>default</b></span>: 10).</li>\n            <li><b>max_hour_frequency</b> (<i>int > 0</i>) &ndash; ignoring max-checkpoints,\n            definitely keep a checkpoint in given hour frequency\n            (<span style=\"color:#00C000\"><b>default</b></span>: none).</li>\n            </ul>\n        summarizer (path | specification): TensorBoard summaries directory, or summarizer\n            configuration with the following attributes\n            (<span style=\"color:#00C000\"><b>default</b></span>: no summarizer):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; summarizer directory\n            (<span style=\"color:#C00000\"><b>required</b></span>).</li>\n            <li><b>filename</b> (<i>path</i>) &ndash; summarizer filename, max_summaries does not\n            apply if name specified\n            (<span style=\"color:#00C000\"><b>default</b></span>: \"summary-%Y%m%d-%H%M%S\").</li>\n            <li><b>max_summaries</b> (<i>int > 0</i>) &ndash; maximum number of (generically-named)\n            summaries to keep\n            (<span style=\"color:#00C000\"><b>default</b></span>: 7, number of different colors in\n            Tensorboard).</li>\n            <li><b>flush</b> (<i>int > 0</i>) &ndash; how frequently in seconds to flush the\n            summary writer (<span style=\"color:#00C000\"><b>default</b></span>: 10).</li>\n            <li><b>summaries</b> (<i>\"all\" | iter[string]</i>) &ndash; which summaries to record,\n            \"all\" implies all numerical summaries, so all summaries except \"graph\"\n            (<span style=\"color:#00C000\"><b>default</b></span>: \"all\"):</li>\n            <li>\"action-value\": value of each action (timestep-based)</li>\n            <li>\"distribution\": distribution parameters like probabilities or mean and stddev\n            (timestep-based)</li>\n            <li>\"entropy\": entropy of (per-action) policy distribution(s) (timestep-based)</li>\n            <li>\"graph\": computation graph</li>\n            <li>\"kl-divergence\": KL-divergence of previous and updated (per-action) policy\n            distribution(s) (update-based)</li>\n            <li>\"loss\": policy and baseline loss plus loss components (update-based)</li>\n            <li>\"parameters\": parameter values (according to parameter unit)</li>\n            <li>\"reward\": reward per timestep, episode length and reward, plus intermediate\n            reward/return/advantage estimates and processed values\n            (timestep/episode/update-based)</li>\n            <li>\"update-norm\": global norm of update (update-based)</li>\n            <li>\"updates\": mean and variance of update tensors per variable (update-based)</li>\n            <li>\"variables\": mean of trainable variables tensors (update-based)</li>\n            </ul>\n        tracking (\"all\" | iter[string]): Which tensors to track, available values are a subset of\n            the values of summarizer[summaries] above\n            (<span style=\"color:#00C000\"><b>default</b></span>: no tracking).\n            The current value of tracked tensors can be retrieved via tracked_tensors() at any time,\n            however, note that tensor values change at different timescales (timesteps, episodes,\n            updates).\n        recorder (path | specification): Traces recordings directory, or recorder configuration with\n            the following attributes (see\n            [record-and-pretrain script](https://github.com/tensorforce/tensorforce/blob/master/examples/record_and_pretrain.py)\n            for example application)\n            (<span style=\"color:#00C000\"><b>default</b></span>: no recorder):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; recorder directory\n            (<span style=\"color:#C00000\"><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>int > 0</i>) &ndash; how frequently in episodes to record\n            traces (<span style=\"color:#00C000\"><b>default</b></span>: every episode).</li>\n            <li><b>start</b> (<i>int >= 0</i>) &ndash; how many episodes to skip before starting to\n            record traces (<span style=\"color:#00C000\"><b>default</b></span>: 0).</li>\n            <li><b>max-traces</b> (<i>int > 0</i>) &ndash; maximum number of traces to keep\n            (<span style=\"color:#00C000\"><b>default</b></span>: all).</li>\n                '''\n\n    def __init__(self, states, actions, update, optimizer, objective, reward_estimation, max_episode_timesteps=None, policy='auto', memory=None, baseline=None, baseline_optimizer=None, baseline_objective=None, l2_regularization=0.0, entropy_regularization=0.0, state_preprocessing='linear_normalization', exploration=0.0, variable_noise=0.0, parallel_interactions=1, config=None, saver=None, summarizer=None, tracking=None, recorder=None, **kwargs):\n        pass\n\n    def experience(self, states, actions, terminal, reward, internals=None):\n        '''\n        Feed experience traces.\n        See the [act-experience-update script](https://github.com/tensorforce/tensorforce/blob/master/examples/act_experience_update_interface.py)\n        for an example application as part of the act-experience-update interface, which is an\n        alternative to the act-observe interaction pattern.\n        Args:\n            states (dict[array[state]]): Dictionary containing arrays of states\n                (<span style=\"color:#C00000\"><b>required</b></span>).\n            actions (dict[array[action]]): Dictionary containing arrays of actions\n                (<span style=\"color:#C00000\"><b>required</b></span>).\n            terminal (array[bool]): Array of terminals\n                (<span style=\"color:#C00000\"><b>required</b></span>).\n            reward (array[float]): Array of rewards\n                (<span style=\"color:#C00000\"><b>required</b></span>).\n            internals (dict[state]): Dictionary containing arrays of internal agent states\n                (<span style=\"color:#C00000\"><b>required</b></span> if agent has internal states).\n        '''\n        pass\n\n        def function(name, spec):\n                pass\n\n    def update(self, query=None, **kwargs):\n        '''\n        Perform an update.\n        See the [act-experience-update script](https://github.com/tensorforce/tensorforce/blob/master/examples/act_experience_update_interface.py)\n        for an example application as part of the act-experience-update interface, which is an\n        alternative to the act-observe interaction pattern.\n        '''\n        pass\n\n    def pretrain(self, directory, num_iterations, num_traces=1, num_updates=1, extension='.npz'):\n        '''\n        Simple pretraining approach as a combination of `experience()` and `update`, akin to\n        behavioral cloning, using experience traces obtained e.g. via recording agent interactions\n        ([see documentation](https://tensorforce.readthedocs.io/en/latest/basics/features.html#record-pretrain)).\n        For the given number of iterations, load the given number of trace files (which each contain\n        recorder[frequency] episodes), feed the experience to the agent's internal memory, and\n        subsequently trigger the given number of updates (which will use the experience in the\n        internal memory, fed in this or potentially previous iterations).\n        See the [record-and-pretrain script](https://github.com/tensorforce/tensorforce/blob/master/examples/record_and_pretrain.py)\n        for an example application.\n        Args:\n            directory (path): Directory with experience traces, e.g. obtained via recorder; episode\n                length has to be consistent with agent configuration\n                (<span style=\"color:#C00000\"><b>required</b></span>).\n            num_iterations (int > 0): Number of iterations consisting of loading new traces and\n                performing multiple updates\n                (<span style=\"color:#C00000\"><b>required</b></span>).\n            num_traces (int > 0): Number of traces to load per iteration; has to at least satisfy\n                the update batch size\n                (<span style=\"color:#00C000\"><b>default</b></span>: 1).\n            num_updates (int > 0): Number of updates per iteration\n                (<span style=\"color:#00C000\"><b>default</b></span>: 1).\n            extension (str): Traces file extension to filter the given directory for\n                (<span style=\"color:#00C000\"><b>default</b></span>: \".npz\").\n        '''\n        pass\nCorresponding class implementation:\n\nfrom tensorforce.agents import Agent\nimport os\nfrom tensorforce.core import ArrayDict\nfrom tensorforce.core.models import TensorforceModel\nfrom collections import OrderedDict\nfrom random import shuffle\nfrom tensorforce import TensorforceError, util\nimport numpy as np\n\nclass TensorforceAgent(Agent):\n    \"\"\"\n    Tensorforce agent (specification key: `tensorforce`).\n\n    Highly configurable agent and basis for a broad class of deep reinforcement learning agents,\n    which act according to a policy parametrized by a neural network, leverage a memory module for\n    periodic updates based on batches of experience, and optionally employ a baseline/critic/target\n    policy for improved reward estimation.\n\n    Args:\n        states (specification): States specification\n            (<span style=\"color:#C00000\"><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create()`), arbitrarily nested dictionary of state\n            descriptions (usually taken from `Environment.states()`) with the following attributes:\n            <ul>\n            <li><b>type</b> (<i>\"bool\" | \"int\" | \"float\"</i>) &ndash; state data type\n            (<span style=\"color:#00C000\"><b>default</b></span>: \"float\").</li>\n            <li><b>shape</b> (<i>int | iter[int]</i>) &ndash; state shape\n            (<span style=\"color:#C00000\"><b>required</b></span>).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete state values\n            (<span style=\"color:#C00000\"><b>required</b></span> for type \"int\").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum state value\n            (<span style=\"color:#00C000\"><b>optional</b></span> for type \"float\").</li>\n            </ul>\n        actions (specification): Actions specification\n            (<span style=\"color:#C00000\"><b>required</b></span>, better implicitly specified via\n            `environment` argument for `Agent.create()`), arbitrarily nested dictionary of\n            action descriptions (usually taken from `Environment.actions()`) with the following\n            attributes:\n            <ul>\n            <li><b>type</b> (<i>\"bool\" | \"int\" | \"float\"</i>) &ndash; action data type\n            (<span style=\"color:#C00000\"><b>required</b></span>).</li>\n            <li><b>shape</b> (<i>int > 0 | iter[int > 0]</i>) &ndash; action shape\n            (<span style=\"color:#00C000\"><b>default</b></span>: scalar).</li>\n            <li><b>num_values</b> (<i>int > 0</i>) &ndash; number of discrete action values\n            (<span style=\"color:#C00000\"><b>required</b></span> for type \"int\").</li>\n            <li><b>min_value/max_value</b> (<i>float</i>) &ndash; minimum/maximum action value\n            (<span style=\"color:#00C000\"><b>optional</b></span> for type \"float\").</li>\n            </ul>\n        max_episode_timesteps (int > 0): Upper bound for numer of timesteps per episode\n            (<span style=\"color:#00C000\"><b>default</b></span>: not given, better implicitly\n            specified via `environment` argument for `Agent.create()`).\n\n        policy (specification): Policy configuration, see [networks](../modules/networks.html) and\n            [policies documentation](../modules/policies.html)\n            (<span style=\"color:#00C000\"><b>default</b></span>: action distributions or value\n            functions parametrized by an automatically configured network).\n        memory (int | specification): Replay memory capacity, or memory configuration, see the\n            [memories documentation](../modules/memories.html)\n            (<span style=\"color:#00C000\"><b>default</b></span>: minimum capacity recent memory).\n        update (int | specification): Model update configuration with the following attributes\n            (<span style=\"color:#C00000\"><b>required</b>,\n            <span style=\"color:#00C000\"><b>default</b></span>: timesteps batch size</span>):\n            <ul>\n            <li><b>unit</b> (<i>\"timesteps\" | \"episodes\"</i>) &ndash; unit for update attributes\n            (<span style=\"color:#C00000\"><b>required</b></span>).</li>\n            <li><b>batch_size</b>\n            (<i><a href=\"../modules/parameters.html\">parameter</a>, int > 0</i>) &ndash;\n            size of update batch in number of units\n            (<span style=\"color:#C00000\"><b>required</b></span>).</li>\n            <li><b>frequency</b>\n            (<i>\"never\" | <a href=\"../modules/parameters.html\">parameter</a>, int > 0 | 0.0 < float <= 1.0</i>) &ndash;\n            frequency of updates, relative to batch_size if float\n            (<span style=\"color:#00C000\"><b>default</b></span>: batch_size).</li>\n            <li><b>start</b>\n            (<i><a href=\"../modules/parameters.html\">parameter</a>, int >= batch_size</i>) &ndash;\n            number of units before first update\n            (<span style=\"color:#00C000\"><b>default</b></span>: none).</li>\n            </ul>\n        optimizer (specification): Optimizer configuration, see the\n            [optimizers documentation](../modules/optimizers.html)\n            (<span style=\"color:#00C000\"><b>default</b></span>: Adam optimizer).\n        objective (specification): Optimization objective configuration, see the\n            [objectives documentation](../modules/objectives.html)\n            (<span style=\"color:#C00000\"><b>required</b></span>).\n        reward_estimation (specification): Reward estimation configuration with the following\n            attributes (<span style=\"color:#C00000\"><b>required</b></span>):\n            <ul>\n            <li><b>horizon</b>\n            (<i>\"episode\" | <a href=\"../modules/parameters.html\">parameter</a>, int >= 1</i>)\n            &ndash; Horizon of discounted-sum return estimation\n            (<span style=\"color:#C00000\"><b>required</b></span>).</li>\n            <li><b>discount</b>\n            (<i><a href=\"../modules/parameters.html\">parameter</a>, 0.0 <= float <= 1.0</i>) &ndash;\n            Discount factor of future rewards for discounted-sum return estimation\n            (<span style=\"color:#00C000\"><b>default</b></span>: 1.0).</li>\n            <li><b>predict_horizon_values</b> (<i>false | \"early\" | \"late\"</i>) &ndash; Whether to\n            include a baseline prediction of the horizon value as part of the return estimation, and\n            if so, whether to compute the horizon value prediction \"early\" when experiences are\n            stored to memory, or \"late\" when batches of experience are retrieved for the update\n            (<span style=\"color:#00C000\"><b>default</b></span>: \"late\" if baseline_policy or\n            baseline_objective are specified, else false).</li>\n            <li><b>estimate_advantage</b> (<i>False | \"early\" | \"late\"</i>) &ndash; Whether to use\n            an estimate of the advantage (return minus baseline value prediction) instead of the\n            return as learning signal, and whether to do so late after the baseline update\n            (default) or early before the baseline update\n            (<span style=\"color:#00C000\"><b>default</b></span>: false, unless baseline_policy is\n            specified but baseline_objective/optimizer are not).</li>\n            <li><b>predict_action_values</b> (<i>bool</i>) &ndash; Whether to predict state-action-\n            instead of state-values as horizon values and for advantage estimation\n            (<span style=\"color:#00C000\"><b>default</b></span>: false).</li>\n            <li><b>reward_processing</b> (<i>specification)</i>) &ndash; Reward preprocessing as\n            layer or list of layers, see the\n            [preprocessing documentation](../modules/preprocessing.html)\n            (<span style=\"color:#00C000\"><b>default</b></span>: no reward processing).</li>\n            <li><b>return_processing</b> (<i>specification</i>) &ndash; Return processing as layer\n            or list of layers, see the [preprocessing documentation](../modules/preprocessing.html)\n            (<span style=\"color:#00C000\"><b>default</b></span>: no return processing).</li>\n            <li><b>advantage_processing</b> (<i>specification</i>) &ndash; Advantage processing as\n            layer or list of layers, see the [preprocessing documentation](../modules/preprocessing.html)\n            (<span style=\"color:#00C000\"><b>default</b></span>: no advantage processing).</li>\n            <li><b>predict_terminal_values</b> (<i>bool</i>) &ndash; Whether to predict the value\n            of terminal states, usually not required since max_episode_timesteps terminals are\n            handled separately\n            (<span style=\"color:#00C000\"><b>default</b></span>: false).</li>\n            </ul>\n\n        baseline (specification): Baseline configuration, policy will be used as baseline if none,\n            see [networks](../modules/networks.html) and potentially\n            [policies documentation](../modules/policies.html)\n            (<span style=\"color:#00C000\"><b>default</b></span>: none).\n        baseline_optimizer (specification | <a href=\"../modules/parameters.html\">parameter</a>, float > 0.0):\n            Baseline optimizer configuration, see the\n            [optimizers documentation](../modules/optimizers.html),\n            main optimizer will be used for baseline if none, a float implies none and specifies a\n            custom weight for the baseline loss\n            (<span style=\"color:#00C000\"><b>default</b></span>: none).\n        baseline_objective (specification): Baseline optimization objective configuration, see the\n            [objectives documentation](../modules/objectives.html),\n            required if baseline optimizer is specified, main objective will be used for baseline if\n            baseline objective and optimizer are not specified\n            (<span style=\"color:#00C000\"><b>default</b></span>: none).\n\n        l2_regularization (<a href=\"../modules/parameters.html\">parameter</a>, float >= 0.0):\n            L2 regularization loss weight\n            (<span style=\"color:#00C000\"><b>default</b></span>: no L2 regularization).\n        entropy_regularization (<a href=\"../modules/parameters.html\">parameter</a>, float >= 0.0):\n            Entropy regularization loss weight, to discourage the policy distribution from being\n            \"too certain\"\n            (<span style=\"color:#00C000\"><b>default</b></span>: no entropy regularization).\n\n        state_preprocessing (dict[specification]): State preprocessing as layer or list of layers,\n            see the [preprocessing documentation](../modules/preprocessing.html),\n            specified per state-type or -name\n            (<span style=\"color:#00C000\"><b>default</b></span>: linear normalization of bounded\n            float states to [-2.0, 2.0]).\n        exploration (<a href=\"../modules/parameters.html\">parameter</a> | dict[<a href=\"../modules/parameters.html\">parameter</a>], float >= 0.0):\n            Exploration, defined as the probability for uniformly random output in case of `bool`\n            and `int` actions, and the standard deviation of Gaussian noise added to every output in\n            case of `float` actions, specified globally or per action-type or -name\n            (<span style=\"color:#00C000\"><b>default</b></span>: no exploration).\n        variable_noise (<a href=\"../modules/parameters.html\">parameter</a>, float >= 0.0):\n            Add Gaussian noise with given standard deviation to all trainable variables, as\n            alternative exploration mechanism\n            (<span style=\"color:#00C000\"><b>default</b></span>: no variable noise).\n\n        parallel_interactions (int > 0): Maximum number of parallel interactions to support,\n            for instance, to enable multiple parallel episodes, environments or agents within an\n            environment\n            (<span style=\"color:#00C000\"><b>default</b></span>: 1).\n        config (specification): Additional configuration options:\n            <ul>\n            <li><b>name</b> (<i>string</i>) &ndash; Agent name, used e.g. for TensorFlow scopes and\n            saver default filename\n            (<span style=\"color:#00C000\"><b>default</b></span>: \"agent\").\n            <li><b>device</b> (<i>string</i>) &ndash; Device name\n            (<span style=\"color:#00C000\"><b>default</b></span>: CPU). Different from (un)supervised\n            deep learning, RL does not always benefit from running on a GPU, depending on\n            environment and agent configuration. In particular for RL-typical environments with\n            low-dimensional state spaces (i.e., no images), one usually gets better performance by\n            running on CPU only. Consequently, Tensorforce is configured to run on CPU by default,\n            which can be changed, for instance, by setting this value to 'GPU' instead.\n            <li><b>seed</b> (<i>int</i>) &ndash; Random seed to set for Python, NumPy (both set\n            globally!) and TensorFlow, environment seed may have to be set separately for fully\n            deterministic execution, generally not recommended since results in a fully\n            deterministic setting are less meaningful/representative\n            (<span style=\"color:#00C000\"><b>default</b></span>: none).</li>\n            <li><b>buffer_observe</b> (<i>false | \"episode\" | int > 0</i>) &ndash; Number of\n            timesteps within an episode to buffer before calling the internal observe function, to\n            reduce calls to TensorFlow for improved performance\n            (<span style=\"color:#00C000\"><b>default</b></span>: configuration-specific maximum\n            number which can be buffered without affecting performance).</li>\n            <li><b>enable_int_action_masking</b> (<i>bool</i>) &ndash; Whether int action options\n            can be masked via an optional \"[ACTION-NAME]_mask\" state input\n            (<span style=\"color:#00C000\"><b>default</b></span>: true).</li>\n            <li><b>create_tf_assertions</b> (<i>bool</i>) &ndash; Whether to create internal\n            TensorFlow assertion operations\n            (<span style=\"color:#00C000\"><b>default</b></span>: true).</li>\n            <li><b>eager_mode</b> (<i>bool</i>) &ndash; Whether to run functions eagerly instead of\n            running as a traced graph function, can be helpful for debugging\n            (<span style=\"color:#00C000\"><b>default</b></span>: false).</li>\n            <li><b>tf_log_level</b> (<i>int >= 0</i>) &ndash; TensorFlow log level, additional C++\n            logging messages can be enabled by setting os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"/\"2\"\n            before importing Tensorforce/TensorFlow\n            (<span style=\"color:#00C000\"><b>default</b></span>: 40, only error and critical).</li>\n            </ul>\n        saver (path | specification): TensorFlow checkpoints directory, or checkpoint manager\n            configuration with the following attributes, for periodic implicit saving as alternative\n            to explicit saving via agent.save()\n            (<span style=\"color:#00C000\"><b>default</b></span>: no saver):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; checkpoint directory\n            (<span style=\"color:#C00000\"><b>required</b></span>).</li>\n            <li><b>filename</b> (<i>string</i>) &ndash; checkpoint filename\n            (<span style=\"color:#00C000\"><b>default</b></span>: agent name).</li>\n            <li><b>frequency</b> (<i>int > 0</i>) &ndash; how frequently to save a checkpoint\n            (<span style=\"color:#C00000\"><b>required</b></span>).</li>\n            <li><b>unit</b> (<i>\"timesteps\" | \"episodes\" | \"updates\"</i>) &ndash; frequency unit\n            (<span style=\"color:#00C000\"><b>default</b></span>: updates).</li>\n            <li><b>max_checkpoints</b> (<i>int > 0</i>) &ndash; maximum number of checkpoints to\n            keep (<span style=\"color:#00C000\"><b>default</b></span>: 10).</li>\n            <li><b>max_hour_frequency</b> (<i>int > 0</i>) &ndash; ignoring max-checkpoints,\n            definitely keep a checkpoint in given hour frequency\n            (<span style=\"color:#00C000\"><b>default</b></span>: none).</li>\n            </ul>\n        summarizer (path | specification): TensorBoard summaries directory, or summarizer\n            configuration with the following attributes\n            (<span style=\"color:#00C000\"><b>default</b></span>: no summarizer):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; summarizer directory\n            (<span style=\"color:#C00000\"><b>required</b></span>).</li>\n            <li><b>filename</b> (<i>path</i>) &ndash; summarizer filename, max_summaries does not\n            apply if name specified\n            (<span style=\"color:#00C000\"><b>default</b></span>: \"summary-%Y%m%d-%H%M%S\").</li>\n            <li><b>max_summaries</b> (<i>int > 0</i>) &ndash; maximum number of (generically-named)\n            summaries to keep\n            (<span style=\"color:#00C000\"><b>default</b></span>: 7, number of different colors in\n            Tensorboard).</li>\n            <li><b>flush</b> (<i>int > 0</i>) &ndash; how frequently in seconds to flush the\n            summary writer (<span style=\"color:#00C000\"><b>default</b></span>: 10).</li>\n            <li><b>summaries</b> (<i>\"all\" | iter[string]</i>) &ndash; which summaries to record,\n            \"all\" implies all numerical summaries, so all summaries except \"graph\"\n            (<span style=\"color:#00C000\"><b>default</b></span>: \"all\"):</li>\n            <li>\"action-value\": value of each action (timestep-based)</li>\n            <li>\"distribution\": distribution parameters like probabilities or mean and stddev\n            (timestep-based)</li>\n            <li>\"entropy\": entropy of (per-action) policy distribution(s) (timestep-based)</li>\n            <li>\"graph\": computation graph</li>\n            <li>\"kl-divergence\": KL-divergence of previous and updated (per-action) policy\n            distribution(s) (update-based)</li>\n            <li>\"loss\": policy and baseline loss plus loss components (update-based)</li>\n            <li>\"parameters\": parameter values (according to parameter unit)</li>\n            <li>\"reward\": reward per timestep, episode length and reward, plus intermediate\n            reward/return/advantage estimates and processed values\n            (timestep/episode/update-based)</li>\n            <li>\"update-norm\": global norm of update (update-based)</li>\n            <li>\"updates\": mean and variance of update tensors per variable (update-based)</li>\n            <li>\"variables\": mean of trainable variables tensors (update-based)</li>\n            </ul>\n        tracking (\"all\" | iter[string]): Which tensors to track, available values are a subset of\n            the values of summarizer[summaries] above\n            (<span style=\"color:#00C000\"><b>default</b></span>: no tracking).\n            The current value of tracked tensors can be retrieved via tracked_tensors() at any time,\n            however, note that tensor values change at different timescales (timesteps, episodes,\n            updates).\n        recorder (path | specification): Traces recordings directory, or recorder configuration with\n            the following attributes (see\n            [record-and-pretrain script](https://github.com/tensorforce/tensorforce/blob/master/examples/record_and_pretrain.py)\n            for example application)\n            (<span style=\"color:#00C000\"><b>default</b></span>: no recorder):\n            <ul>\n            <li><b>directory</b> (<i>path</i>) &ndash; recorder directory\n            (<span style=\"color:#C00000\"><b>required</b></span>).</li>\n            <li><b>frequency</b> (<i>int > 0</i>) &ndash; how frequently in episodes to record\n            traces (<span style=\"color:#00C000\"><b>default</b></span>: every episode).</li>\n            <li><b>start</b> (<i>int >= 0</i>) &ndash; how many episodes to skip before starting to\n            record traces (<span style=\"color:#00C000\"><b>default</b></span>: 0).</li>\n            <li><b>max-traces</b> (<i>int > 0</i>) &ndash; maximum number of traces to keep\n            (<span style=\"color:#00C000\"><b>default</b></span>: all).</li>\n    \"\"\"\n\n    def __init__(self, states, actions, update, optimizer, objective, reward_estimation, max_episode_timesteps=None, policy='auto', memory=None, baseline=None, baseline_optimizer=None, baseline_objective=None, l2_regularization=0.0, entropy_regularization=0.0, state_preprocessing='linear_normalization', exploration=0.0, variable_noise=0.0, parallel_interactions=1, config=None, saver=None, summarizer=None, tracking=None, recorder=None, **kwargs):\n        if 'estimate_actions' in reward_estimation:\n            raise TensorforceError.deprecated(name='Agent', argument='reward_estimation[estimate_actions]', replacement='reward_estimation[predict_action_values]')\n        if 'estimate_terminal' in reward_estimation:\n            raise TensorforceError.deprecated(name='Agent', argument='reward_estimation[estimate_terminal]', replacement='reward_estimation[predict_terminal_values]')\n        if summarizer is not None and 'labels' in summarizer:\n            raise TensorforceError.deprecated(name='Agent', argument='summarizer[labels]', replacement='summarizer[summaries]')\n        if 'baseline_policy' in kwargs:\n            raise TensorforceError.deprecated(name='Agent', argument='baseline_policy', replacement='baseline')\n        if 'reward_preprocessing' in kwargs:\n            raise TensorforceError.deprecated(name='Agent', argument='reward_preprocessing', replacement='reward_estimation[reward_processing]')\n        if 'name' in kwargs:\n            raise TensorforceError.deprecated(name='Agent', argument='name', replacement='config[name]')\n        if 'buffer_observe' in kwargs:\n            raise TensorforceError.deprecated(name='Agent', argument='buffer_observe', replacement='config[buffer_observe]')\n        if 'device' in kwargs:\n            raise TensorforceError.deprecated(name='Agent', argument='device', replacement='config[device]')\n        if 'seed' in kwargs:\n            raise TensorforceError.deprecated(name='Agent', argument='seed', replacement='config[seed]')\n        if len(kwargs) > 0:\n            raise TensorforceError.invalid(name='Agent', argument=', '.join(kwargs))\n        if not hasattr(self, 'spec'):\n            self.spec = OrderedDict(agent='tensorforce', states=states, actions=actions, max_episode_timesteps=max_episode_timesteps, policy=policy, memory=memory, update=update, optimizer=optimizer, objective=objective, reward_estimation=reward_estimation, baseline=baseline, baseline_optimizer=baseline_optimizer, baseline_objective=baseline_objective, l2_regularization=l2_regularization, entropy_regularization=entropy_regularization, state_preprocessing=state_preprocessing, exploration=exploration, variable_noise=variable_noise, parallel_interactions=parallel_interactions, config=config, saver=saver, summarizer=summarizer, tracking=tracking, recorder=recorder)\n        if memory is None:\n            memory = dict(type='recent')\n        if isinstance(update, int):\n            update = dict(unit='timesteps', batch_size=update)\n        if config is None:\n            config = dict()\n        else:\n            config = dict(config)\n        if parallel_interactions > 1:\n            if 'buffer_observe' not in config:\n                if max_episode_timesteps is None:\n                    raise TensorforceError.required(name='Agent', argument='max_episode_timesteps', condition='parallel_interactions > 1')\n                config['buffer_observe'] = 'episode'\n        elif update['unit'] == 'timesteps':\n            update_frequency = update.get('frequency', update['batch_size'])\n            if 'buffer_observe' not in config:\n                if isinstance(update_frequency, int):\n                    config['buffer_observe'] = update_frequency\n                else:\n                    config['buffer_observe'] = 1\n            elif isinstance(update_frequency, int) and (config['buffer_observe'] == 'episode' or config['buffer_observe'] > update_frequency):\n                raise TensorforceError.value(name='Agent', argument='config[buffer_observe]', value=config['buffer_observe'], hint='> update[frequency]', condition='update[unit] = \"timesteps\"')\n        elif update['unit'] == 'episodes':\n            if 'buffer_observe' not in config:\n                config['buffer_observe'] = 'episode'\n        super().__init__(states=states, actions=actions, max_episode_timesteps=max_episode_timesteps, parallel_interactions=parallel_interactions, config=config, recorder=recorder)\n        self.model = TensorforceModel(states=self.states_spec, actions=self.actions_spec, max_episode_timesteps=self.max_episode_timesteps, policy=policy, memory=memory, update=update, optimizer=optimizer, objective=objective, reward_estimation=reward_estimation, baseline=baseline, baseline_optimizer=baseline_optimizer, baseline_objective=baseline_objective, l2_regularization=l2_regularization, entropy_regularization=entropy_regularization, state_preprocessing=state_preprocessing, exploration=exploration, variable_noise=variable_noise, parallel_interactions=self.parallel_interactions, config=self.config, saver=saver, summarizer=summarizer, tracking=tracking)\n\n    def experience(self, states, actions, terminal, reward, internals=None):\n        \"\"\"\n        Feed experience traces.\n\n        See the [act-experience-update script](https://github.com/tensorforce/tensorforce/blob/master/examples/act_experience_update_interface.py)\n        for an example application as part of the act-experience-update interface, which is an\n        alternative to the act-observe interaction pattern.\n\n        Args:\n            states (dict[array[state]]): Dictionary containing arrays of states\n                (<span style=\"color:#C00000\"><b>required</b></span>).\n            actions (dict[array[action]]): Dictionary containing arrays of actions\n                (<span style=\"color:#C00000\"><b>required</b></span>).\n            terminal (array[bool]): Array of terminals\n                (<span style=\"color:#C00000\"><b>required</b></span>).\n            reward (array[float]): Array of rewards\n                (<span style=\"color:#C00000\"><b>required</b></span>).\n            internals (dict[state]): Dictionary containing arrays of internal agent states\n                (<span style=\"color:#C00000\"><b>required</b></span> if agent has internal states).\n        \"\"\"\n        if not all((len(buffer) == 0 for buffer in self.terminal_buffer)):\n            raise TensorforceError(message='Calling agent.experience is not possible mid-episode.')\n        states, batched, num_instances, is_iter_of_dicts = self._process_states_input(states=states, function_name='Agent.experience')\n        if is_iter_of_dicts:\n            if internals is None:\n                internals = ArrayDict(self.initial_internals())\n                internals = internals.fmap(function=lambda x: np.repeat(np.expand_dims(x, axis=0), repeats=num_instances, axis=0))\n            elif not isinstance(internals, (tuple, list)):\n                raise TensorforceError.type(name='Agent.experience', argument='internals', dtype=type(internals), hint='is not tuple/list')\n            else:\n                internals = [ArrayDict(internal) for internal in internals]\n                internals = internals[0].fmap(function=lambda *xs: np.stack(xs, axis=0), zip_values=internals[1:])\n            if isinstance(actions, np.ndarray):\n                actions = ArrayDict(singleton=actions)\n            elif not isinstance(actions, (tuple, list)):\n                raise TensorforceError.type(name='Agent.experience', argument='actions', dtype=type(actions), hint='is not tuple/list')\n            elif not isinstance(actions[0], dict):\n                actions = ArrayDict(singleton=np.asarray(actions))\n            elif all((list(action) == ['action'] for action in actions)):\n                actions = [ArrayDict(singleton=action['action']) for action in actions]\n                actions = actions[0].fmap(function=lambda *xs: np.stack(xs, axis=0), zip_values=actions[1:])\n            else:\n                actions = [ArrayDict(action) for action in actions]\n                actions = actions[0].fmap(function=lambda *xs: np.stack(xs, axis=0), zip_values=actions[1:])\n        else:\n            if internals is None:\n                internals = ArrayDict(self.initial_internals())\n                internals = internals.fmap(function=lambda x: np.tile(np.expand_dims(x, axis=0), reps=(num_instances,)))\n            elif not isinstance(internals, dict):\n                raise TensorforceError.type(name='Agent.experience', argument='internals', dtype=type(internals), hint='is not dict')\n            else:\n                internals = ArrayDict(internals)\n            if isinstance(actions, np.ndarray):\n                actions = ArrayDict(singleton=actions)\n            elif not isinstance(actions, dict):\n                raise TensorforceError.type(name='Agent.experience', argument='actions', dtype=type(actions), hint='is not dict')\n            elif list(actions) == ['action']:\n                actions = ArrayDict(singleton=actions['action'])\n            else:\n                actions = ArrayDict(actions)\n        if not batched:\n            internals = internals.fmap(function=lambda x: np.expand_dims(x, axis=0))\n            actions = actions.fmap(function=lambda x: np.expand_dims(x, axis=0))\n            terminal = np.asarray([terminal])\n            reward = np.asarray([reward])\n        else:\n            terminal = np.asarray(terminal)\n            reward = np.asarray(reward)\n        for name, internal in internals.items():\n            if internal.shape[0] != num_instances:\n                raise TensorforceError.value(name='Agent.experience', argument='len(internals[{}])'.format(name), value=internal.shape[0], hint='!= len(states)')\n        for name, action in actions.items():\n            if action.shape[0] != num_instances:\n                raise TensorforceError.value(name='Agent.experience', argument='len(actions[{}])'.format(name), value=action.shape[0], hint='!= len(states)')\n        if terminal.shape[0] != num_instances:\n            raise TensorforceError.value(name='Agent.experience', argument='len(terminal)'.format(name), value=terminal.shape[0], hint='!= len(states)')\n        if reward.shape[0] != num_instances:\n            raise TensorforceError.value(name='Agent.experience', argument='len(reward)'.format(name), value=reward.shape[0], hint='!= len(states)')\n\n        def function(name, spec):\n            auxiliary = ArrayDict()\n            if self.config.enable_int_action_masking and spec.type == 'int' and (spec.num_values is not None):\n                if name is None:\n                    name = 'action'\n                auxiliary['mask'] = states.pop(name + '_mask', np.ones(shape=(num_instances,) + spec.shape + (spec.num_values,), dtype=spec.np_type()))\n            return auxiliary\n        auxiliaries = self.actions_spec.fmap(function=function, cls=ArrayDict, with_names=True)\n        if self.states_spec.is_singleton() and (not states.is_singleton()):\n            states[None] = states.pop('state')\n        if terminal.dtype is util.np_dtype(dtype='bool'):\n            zeros = np.zeros_like(terminal, dtype=util.np_dtype(dtype='int'))\n            ones = np.ones_like(terminal, dtype=util.np_dtype(dtype='int'))\n            terminal = np.where(terminal, ones, zeros)\n        if terminal[-1] == 0:\n            raise TensorforceError(message='Agent.experience() requires full episodes as input.')\n        last = 0\n        for index in range(1, len(terminal) + 1):\n            if terminal[index - 1] == 0:\n                continue\n            function = lambda x: x[last:index]\n            states_batch = states.fmap(function=function)\n            internals_batch = internals.fmap(function=function)\n            auxiliaries_batch = auxiliaries.fmap(function=function)\n            actions_batch = actions.fmap(function=function)\n            terminal_batch = function(terminal)\n            reward_batch = function(reward)\n            last = index\n            states_batch = self.states_spec.to_tensor(value=states_batch, batched=True, name='Agent.experience states')\n            internals_batch = self.internals_spec.to_tensor(value=internals_batch, batched=True, recover_empty=True, name='Agent.experience internals')\n            auxiliaries_batch = self.auxiliaries_spec.to_tensor(value=auxiliaries_batch, batched=True, name='Agent.experience auxiliaries')\n            actions_batch = self.actions_spec.to_tensor(value=actions_batch, batched=True, name='Agent.experience actions')\n            terminal_batch = self.terminal_spec.to_tensor(value=terminal_batch, batched=True, name='Agent.experience terminal')\n            reward_batch = self.reward_spec.to_tensor(value=reward_batch, batched=True, name='Agent.experience reward')\n            timesteps, episodes = self.model.experience(states=states_batch, internals=internals_batch, auxiliaries=auxiliaries_batch, actions=actions_batch, terminal=terminal_batch, reward=reward_batch)\n            self.timesteps = timesteps.numpy().item()\n            self.episodes = episodes.numpy().item()\n        if self.model.saver is not None:\n            self.model.save()\n\n    def update(self, query=None, **kwargs):\n        \"\"\"\n        Perform an update.\n\n        See the [act-experience-update script](https://github.com/tensorforce/tensorforce/blob/master/examples/act_experience_update_interface.py)\n        for an example application as part of the act-experience-update interface, which is an\n        alternative to the act-observe interaction pattern.\n        \"\"\"\n        updates = self.model.update()\n        self.updates = updates.numpy().item()\n        if self.model.saver is not None:\n            self.model.save()\n\n    def pretrain(self, directory, num_iterations, num_traces=1, num_updates=1, extension='.npz'):\n        \"\"\"\n        Simple pretraining approach as a combination of `experience()` and `update`, akin to\n        behavioral cloning, using experience traces obtained e.g. via recording agent interactions\n        ([see documentation](https://tensorforce.readthedocs.io/en/latest/basics/features.html#record-pretrain)).\n\n        For the given number of iterations, load the given number of trace files (which each contain\n        recorder[frequency] episodes), feed the experience to the agent's internal memory, and\n        subsequently trigger the given number of updates (which will use the experience in the\n        internal memory, fed in this or potentially previous iterations).\n\n        See the [record-and-pretrain script](https://github.com/tensorforce/tensorforce/blob/master/examples/record_and_pretrain.py)\n        for an example application.\n\n        Args:\n            directory (path): Directory with experience traces, e.g. obtained via recorder; episode\n                length has to be consistent with agent configuration\n                (<span style=\"color:#C00000\"><b>required</b></span>).\n            num_iterations (int > 0): Number of iterations consisting of loading new traces and\n                performing multiple updates\n                (<span style=\"color:#C00000\"><b>required</b></span>).\n            num_traces (int > 0): Number of traces to load per iteration; has to at least satisfy\n                the update batch size\n                (<span style=\"color:#00C000\"><b>default</b></span>: 1).\n            num_updates (int > 0): Number of updates per iteration\n                (<span style=\"color:#00C000\"><b>default</b></span>: 1).\n            extension (str): Traces file extension to filter the given directory for\n                (<span style=\"color:#00C000\"><b>default</b></span>: \".npz\").\n        \"\"\"\n        if not os.path.isdir(directory):\n            raise TensorforceError.value(name='agent.pretrain', argument='directory', value=directory)\n        files = sorted((os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f)) and os.path.splitext(f)[1] == extension))\n        indices = list(range(len(files)))\n        for _ in range(num_iterations):\n            shuffle(indices)\n            if num_traces is None:\n                selection = indices\n            else:\n                selection = indices[:num_traces]\n            batch = None\n            for index in selection:\n                trace = ArrayDict(np.load(files[index]))\n                if batch is None:\n                    batch = trace\n                else:\n                    batch = batch.fmap(function=lambda x, y: np.concatenate([x, y], axis=0), zip_values=(trace,))\n            for name, value in batch.pop('auxiliaries', dict()).items():\n                assert name.endswith('/mask')\n                batch['states'][name[:-5] + '_mask'] = value\n            self.experience(**batch.to_kwargs())\n            for _ in range(num_updates):\n                self.update()\n\nExample skeleton:\n\n\nclass SymbolicRegressor(BaseSymbolic, RegressorMixin):\n    '''A Genetic Programming symbolic regressor.\n    A symbolic regressor is an estimator that begins by building a population\n    of naive random formulas to represent a relationship. The formulas are\n    represented as tree-like structures with mathematical functions being\n    recursively applied to variables and constants. Each successive generation\n    of programs is then evolved from the one that came before it by selecting\n    the fittest individuals from the population to undergo genetic operations\n    such as crossover, mutation or reproduction.\n    Parameters\n    ----------\n    population_size : integer, optional (default=1000)\n        The number of programs in each generation.\n    generations : integer, optional (default=20)\n        The number of generations to evolve.\n    tournament_size : integer, optional (default=20)\n        The number of programs that will compete to become part of the next\n        generation.\n    stopping_criteria : float, optional (default=0.0)\n        The required metric value required in order to stop evolution early.\n    const_range : tuple of two floats, or None, optional (default=(-1., 1.))\n        The range of constants to include in the formulas. If None then no\n        constants will be included in the candidate programs.\n    init_depth : tuple of two ints, optional (default=(2, 6))\n        The range of tree depths for the initial population of naive formulas.\n        Individual trees will randomly choose a maximum depth from this range.\n        When combined with `init_method='half and half'` this yields the well-\n        known 'ramped half and half' initialization method.\n    init_method : str, optional (default='half and half')\n        - 'grow' : Nodes are chosen at random from both functions and\n          terminals, allowing for smaller trees than `init_depth` allows. Tends\n          to grow asymmetrical trees.\n        - 'full' : Functions are chosen until the `init_depth` is reached, and\n          then terminals are selected. Tends to grow 'bushy' trees.\n        - 'half and half' : Trees are grown through a 50/50 mix of 'full' and\n          'grow', making for a mix of tree shapes in the initial population.\n    function_set : iterable, optional (default=('add', 'sub', 'mul', 'div'))\n        The functions to use when building and evolving programs. This iterable\n        can include strings to indicate either individual functions as outlined\n        below, or you can also include your own functions as built using the\n        ``make_function`` factory from the ``functions`` module.\n        Available individual functions are:\n        - 'add' : addition, arity=2.\n        - 'sub' : subtraction, arity=2.\n        - 'mul' : multiplication, arity=2.\n        - 'div' : protected division where a denominator near-zero returns 1.,\n          arity=2.\n        - 'sqrt' : protected square root where the absolute value of the\n          argument is used, arity=1.\n        - 'log' : protected log where the absolute value of the argument is\n          used and a near-zero argument returns 0., arity=1.\n        - 'abs' : absolute value, arity=1.\n        - 'neg' : negative, arity=1.\n        - 'inv' : protected inverse where a near-zero argument returns 0.,\n          arity=1.\n        - 'max' : maximum, arity=2.\n        - 'min' : minimum, arity=2.\n        - 'sin' : sine (radians), arity=1.\n        - 'cos' : cosine (radians), arity=1.\n        - 'tan' : tangent (radians), arity=1.\n    metric : str, optional (default='mean absolute error')\n        The name of the raw fitness metric. Available options include:\n        - 'mean absolute error'.\n        - 'mse' for mean squared error.\n        - 'rmse' for root mean squared error.\n        - 'pearson', for Pearson's product-moment correlation coefficient.\n        - 'spearman' for Spearman's rank-order correlation coefficient.\n        Note that 'pearson' and 'spearman' will not directly predict the target\n        but could be useful as value-added features in a second-step estimator.\n        This would allow the user to generate one engineered feature at a time,\n        using the SymbolicTransformer would allow creation of multiple features\n        at once.\n    parsimony_coefficient : float or \"auto\", optional (default=0.001)\n        This constant penalizes large programs by adjusting their fitness to\n        be less favorable for selection. Larger values penalize the program\n        more which can control the phenomenon known as 'bloat'. Bloat is when\n        evolution is increasing the size of programs without a significant\n        increase in fitness, which is costly for computation time and makes for\n        a less understandable final result. This parameter may need to be tuned\n        over successive runs.\n        If \"auto\" the parsimony coefficient is recalculated for each generation\n        using c = Cov(l,f)/Var( l), where Cov(l,f) is the covariance between\n        program size l and program fitness f in the population, and Var(l) is\n        the variance of program sizes.\n    p_crossover : float, optional (default=0.9)\n        The probability of performing crossover on a tournament winner.\n        Crossover takes the winner of a tournament and selects a random subtree\n        from it to be replaced. A second tournament is performed to find a\n        donor. The donor also has a subtree selected at random and this is\n        inserted into the original parent to form an offspring in the next\n        generation.\n    p_subtree_mutation : float, optional (default=0.01)\n        The probability of performing subtree mutation on a tournament winner.\n        Subtree mutation takes the winner of a tournament and selects a random\n        subtree from it to be replaced. A donor subtree is generated at random\n        and this is inserted into the original parent to form an offspring in\n        the next generation.\n    p_hoist_mutation : float, optional (default=0.01)\n        The probability of performing hoist mutation on a tournament winner.\n        Hoist mutation takes the winner of a tournament and selects a random\n        subtree from it. A random subtree of that subtree is then selected\n        and this is 'hoisted' into the original subtrees location to form an\n        offspring in the next generation. This method helps to control bloat.\n    p_point_mutation : float, optional (default=0.01)\n        The probability of performing point mutation on a tournament winner.\n        Point mutation takes the winner of a tournament and selects random\n        nodes from it to be replaced. Terminals are replaced by other terminals\n        and functions are replaced by other functions that require the same\n        number of arguments as the original node. The resulting tree forms an\n        offspring in the next generation.\n        Note : The above genetic operation probabilities must sum to less than\n        one. The balance of probability is assigned to 'reproduction', where a\n        tournament winner is cloned and enters the next generation unmodified.\n    p_point_replace : float, optional (default=0.05)\n        For point mutation only, the probability that any given node will be\n        mutated.\n    max_samples : float, optional (default=1.0)\n        The fraction of samples to draw from X to evaluate each program on.\n    feature_names : list, optional (default=None)\n        Optional list of feature names, used purely for representations in\n        the `print` operation or `export_graphviz`. If None, then X0, X1, etc\n        will be used for representations.\n    warm_start : bool, optional (default=False)\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more generations to the evolution, otherwise, just fit a new\n        evolution.\n    low_memory : bool, optional (default=False)\n        When set to ``True``, only the current generation is retained. Parent\n        information is discarded. For very large populations or runs with many\n        generations, this can result in substantial memory use reduction.\n    n_jobs : integer, optional (default=1)\n        The number of jobs to run in parallel for `fit`. If -1, then the number\n        of jobs is set to the number of cores.\n    verbose : int, optional (default=0)\n        Controls the verbosity of the evolution building process.\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n    Attributes\n    ----------\n    run_details_ : dict\n        Details of the evolution process. Includes the following elements:\n        - 'generation' : The generation index.\n        - 'average_length' : The average program length of the generation.\n        - 'average_fitness' : The average program fitness of the generation.\n        - 'best_length' : The length of the best program in the generation.\n        - 'best_fitness' : The fitness of the best program in the generation.\n        - 'best_oob_fitness' : The out of bag fitness of the best program in\n          the generation (requires `max_samples` < 1.0).\n        - 'generation_time' : The time it took for the generation to evolve.\n    See Also\n    --------\n    SymbolicTransformer\n    References\n    ----------\n    .. [1] J. Koza, \"Genetic Programming\", 1992.\n    .. [2] R. Poli, et al. \"A Field Guide to Genetic Programming\", 2008.\n    '''\n\n    def __init__(self, *, population_size=1000, generations=20, tournament_size=20, stopping_criteria=0.0, const_range=(-1.0, 1.0), init_depth=(2, 6), init_method='half and half', function_set=('add', 'sub', 'mul', 'div'), metric='mean absolute error', parsimony_coefficient=0.001, p_crossover=0.9, p_subtree_mutation=0.01, p_hoist_mutation=0.01, p_point_mutation=0.01, p_point_replace=0.05, max_samples=1.0, feature_names=None, warm_start=False, low_memory=False, n_jobs=1, verbose=0, random_state=None):\n        pass\n\n    def __str__(self):\n        '''Overloads `print` output of the object to resemble a LISP tree.'''\n        pass\n\n    def predict(self, X):\n        '''Perform regression on test vectors X.\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Input vectors, where n_samples is the number of samples\n            and n_features is the number of features.\n        Returns\n        -------\n        y : array, shape = [n_samples]\n            Predicted values for X.\n        '''\n        pass\nCorresponding class implementation:\n\nfrom sklearn.base import RegressorMixin, TransformerMixin, ClassifierMixin\nfrom sklearn.utils.validation import check_array, _check_sample_weight\nfrom sklearn.exceptions import NotFittedError\n\nclass SymbolicRegressor(BaseSymbolic, RegressorMixin):\n    \"\"\"A Genetic Programming symbolic regressor.\n\n    A symbolic regressor is an estimator that begins by building a population\n    of naive random formulas to represent a relationship. The formulas are\n    represented as tree-like structures with mathematical functions being\n    recursively applied to variables and constants. Each successive generation\n    of programs is then evolved from the one that came before it by selecting\n    the fittest individuals from the population to undergo genetic operations\n    such as crossover, mutation or reproduction.\n\n    Parameters\n    ----------\n    population_size : integer, optional (default=1000)\n        The number of programs in each generation.\n\n    generations : integer, optional (default=20)\n        The number of generations to evolve.\n\n    tournament_size : integer, optional (default=20)\n        The number of programs that will compete to become part of the next\n        generation.\n\n    stopping_criteria : float, optional (default=0.0)\n        The required metric value required in order to stop evolution early.\n\n    const_range : tuple of two floats, or None, optional (default=(-1., 1.))\n        The range of constants to include in the formulas. If None then no\n        constants will be included in the candidate programs.\n\n    init_depth : tuple of two ints, optional (default=(2, 6))\n        The range of tree depths for the initial population of naive formulas.\n        Individual trees will randomly choose a maximum depth from this range.\n        When combined with `init_method='half and half'` this yields the well-\n        known 'ramped half and half' initialization method.\n\n    init_method : str, optional (default='half and half')\n        - 'grow' : Nodes are chosen at random from both functions and\n          terminals, allowing for smaller trees than `init_depth` allows. Tends\n          to grow asymmetrical trees.\n        - 'full' : Functions are chosen until the `init_depth` is reached, and\n          then terminals are selected. Tends to grow 'bushy' trees.\n        - 'half and half' : Trees are grown through a 50/50 mix of 'full' and\n          'grow', making for a mix of tree shapes in the initial population.\n\n    function_set : iterable, optional (default=('add', 'sub', 'mul', 'div'))\n        The functions to use when building and evolving programs. This iterable\n        can include strings to indicate either individual functions as outlined\n        below, or you can also include your own functions as built using the\n        ``make_function`` factory from the ``functions`` module.\n\n        Available individual functions are:\n\n        - 'add' : addition, arity=2.\n        - 'sub' : subtraction, arity=2.\n        - 'mul' : multiplication, arity=2.\n        - 'div' : protected division where a denominator near-zero returns 1.,\n          arity=2.\n        - 'sqrt' : protected square root where the absolute value of the\n          argument is used, arity=1.\n        - 'log' : protected log where the absolute value of the argument is\n          used and a near-zero argument returns 0., arity=1.\n        - 'abs' : absolute value, arity=1.\n        - 'neg' : negative, arity=1.\n        - 'inv' : protected inverse where a near-zero argument returns 0.,\n          arity=1.\n        - 'max' : maximum, arity=2.\n        - 'min' : minimum, arity=2.\n        - 'sin' : sine (radians), arity=1.\n        - 'cos' : cosine (radians), arity=1.\n        - 'tan' : tangent (radians), arity=1.\n\n    metric : str, optional (default='mean absolute error')\n        The name of the raw fitness metric. Available options include:\n\n        - 'mean absolute error'.\n        - 'mse' for mean squared error.\n        - 'rmse' for root mean squared error.\n        - 'pearson', for Pearson's product-moment correlation coefficient.\n        - 'spearman' for Spearman's rank-order correlation coefficient.\n\n        Note that 'pearson' and 'spearman' will not directly predict the target\n        but could be useful as value-added features in a second-step estimator.\n        This would allow the user to generate one engineered feature at a time,\n        using the SymbolicTransformer would allow creation of multiple features\n        at once.\n\n    parsimony_coefficient : float or \"auto\", optional (default=0.001)\n        This constant penalizes large programs by adjusting their fitness to\n        be less favorable for selection. Larger values penalize the program\n        more which can control the phenomenon known as 'bloat'. Bloat is when\n        evolution is increasing the size of programs without a significant\n        increase in fitness, which is costly for computation time and makes for\n        a less understandable final result. This parameter may need to be tuned\n        over successive runs.\n\n        If \"auto\" the parsimony coefficient is recalculated for each generation\n        using c = Cov(l,f)/Var( l), where Cov(l,f) is the covariance between\n        program size l and program fitness f in the population, and Var(l) is\n        the variance of program sizes.\n\n    p_crossover : float, optional (default=0.9)\n        The probability of performing crossover on a tournament winner.\n        Crossover takes the winner of a tournament and selects a random subtree\n        from it to be replaced. A second tournament is performed to find a\n        donor. The donor also has a subtree selected at random and this is\n        inserted into the original parent to form an offspring in the next\n        generation.\n\n    p_subtree_mutation : float, optional (default=0.01)\n        The probability of performing subtree mutation on a tournament winner.\n        Subtree mutation takes the winner of a tournament and selects a random\n        subtree from it to be replaced. A donor subtree is generated at random\n        and this is inserted into the original parent to form an offspring in\n        the next generation.\n\n    p_hoist_mutation : float, optional (default=0.01)\n        The probability of performing hoist mutation on a tournament winner.\n        Hoist mutation takes the winner of a tournament and selects a random\n        subtree from it. A random subtree of that subtree is then selected\n        and this is 'hoisted' into the original subtrees location to form an\n        offspring in the next generation. This method helps to control bloat.\n\n    p_point_mutation : float, optional (default=0.01)\n        The probability of performing point mutation on a tournament winner.\n        Point mutation takes the winner of a tournament and selects random\n        nodes from it to be replaced. Terminals are replaced by other terminals\n        and functions are replaced by other functions that require the same\n        number of arguments as the original node. The resulting tree forms an\n        offspring in the next generation.\n\n        Note : The above genetic operation probabilities must sum to less than\n        one. The balance of probability is assigned to 'reproduction', where a\n        tournament winner is cloned and enters the next generation unmodified.\n\n    p_point_replace : float, optional (default=0.05)\n        For point mutation only, the probability that any given node will be\n        mutated.\n\n    max_samples : float, optional (default=1.0)\n        The fraction of samples to draw from X to evaluate each program on.\n\n    feature_names : list, optional (default=None)\n        Optional list of feature names, used purely for representations in\n        the `print` operation or `export_graphviz`. If None, then X0, X1, etc\n        will be used for representations.\n\n    warm_start : bool, optional (default=False)\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more generations to the evolution, otherwise, just fit a new\n        evolution.\n\n    low_memory : bool, optional (default=False)\n        When set to ``True``, only the current generation is retained. Parent\n        information is discarded. For very large populations or runs with many\n        generations, this can result in substantial memory use reduction.\n\n    n_jobs : integer, optional (default=1)\n        The number of jobs to run in parallel for `fit`. If -1, then the number\n        of jobs is set to the number of cores.\n\n    verbose : int, optional (default=0)\n        Controls the verbosity of the evolution building process.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Attributes\n    ----------\n    run_details_ : dict\n        Details of the evolution process. Includes the following elements:\n\n        - 'generation' : The generation index.\n        - 'average_length' : The average program length of the generation.\n        - 'average_fitness' : The average program fitness of the generation.\n        - 'best_length' : The length of the best program in the generation.\n        - 'best_fitness' : The fitness of the best program in the generation.\n        - 'best_oob_fitness' : The out of bag fitness of the best program in\n          the generation (requires `max_samples` < 1.0).\n        - 'generation_time' : The time it took for the generation to evolve.\n\n    See Also\n    --------\n    SymbolicTransformer\n\n    References\n    ----------\n    .. [1] J. Koza, \"Genetic Programming\", 1992.\n\n    .. [2] R. Poli, et al. \"A Field Guide to Genetic Programming\", 2008.\n\n    \"\"\"\n\n    def __init__(self, *, population_size=1000, generations=20, tournament_size=20, stopping_criteria=0.0, const_range=(-1.0, 1.0), init_depth=(2, 6), init_method='half and half', function_set=('add', 'sub', 'mul', 'div'), metric='mean absolute error', parsimony_coefficient=0.001, p_crossover=0.9, p_subtree_mutation=0.01, p_hoist_mutation=0.01, p_point_mutation=0.01, p_point_replace=0.05, max_samples=1.0, feature_names=None, warm_start=False, low_memory=False, n_jobs=1, verbose=0, random_state=None):\n        super(SymbolicRegressor, self).__init__(population_size=population_size, generations=generations, tournament_size=tournament_size, stopping_criteria=stopping_criteria, const_range=const_range, init_depth=init_depth, init_method=init_method, function_set=function_set, metric=metric, parsimony_coefficient=parsimony_coefficient, p_crossover=p_crossover, p_subtree_mutation=p_subtree_mutation, p_hoist_mutation=p_hoist_mutation, p_point_mutation=p_point_mutation, p_point_replace=p_point_replace, max_samples=max_samples, feature_names=feature_names, warm_start=warm_start, low_memory=low_memory, n_jobs=n_jobs, verbose=verbose, random_state=random_state)\n\n    def __str__(self):\n        \"\"\"Overloads `print` output of the object to resemble a LISP tree.\"\"\"\n        if not hasattr(self, '_program'):\n            return self.__repr__()\n        return self._program.__str__()\n\n    def predict(self, X):\n        \"\"\"Perform regression on test vectors X.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Input vectors, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        y : array, shape = [n_samples]\n            Predicted values for X.\n\n        \"\"\"\n        if not hasattr(self, '_program'):\n            raise NotFittedError('SymbolicRegressor not fitted.')\n        X = check_array(X)\n        _, n_features = X.shape\n        if self.n_features_in_ != n_features:\n            raise ValueError('Number of features of the model must match the input. Model n_features is %s and input n_features is %s.' % (self.n_features_in_, n_features))\n        y = self._program.execute(X)\n        return y\n\nNow, implement the following class. Do not explain the code. The given class skeleton is as follows:\n@dataclass\nclass OptimizationResults:\n    '''\n    Structured container for optimization results from the Successive Convexification (SCP) solver.\n    This class provides a type-safe and organized way to store and access optimization results,\n    replacing the previous dictionary-based approach. It includes core optimization data,\n    iteration history for convergence analysis, post-processing results, and flexible\n    storage for plotting and application-specific data.\n    Attributes:\n        converged (bool): Whether the optimization successfully converged\n        t_final (float): Final time of the optimized trajectory\n        u (Control): Optimized control trajectory at discretization nodes\n        x (State): Optimized state trajectory at discretization nodes\n        # SCP Iteration History (for convergence analysis)\n        x_history (list[np.ndarray]): State trajectories from each SCP iteration\n        u_history (list[np.ndarray]): Control trajectories from each SCP iteration\n        discretization_history (list[np.ndarray]): Time discretization from each iteration\n        J_tr_history (list[np.ndarray]): Trust region cost history\n        J_vb_history (list[np.ndarray]): Virtual buffer cost history\n        J_vc_history (list[np.ndarray]): Virtual control cost history\n        # Post-processing Results (added by propagate_trajectory_results)\n        t_full (Optional[np.ndarray]): Full time grid for interpolated trajectory\n        x_full (Optional[np.ndarray]): Interpolated state trajectory on full time grid\n        u_full (Optional[np.ndarray]): Interpolated control trajectory on full time grid\n        cost (Optional[float]): Total cost of the optimized trajectory\n        ctcs_violation (Optional[np.ndarray]): Continuous-time constraint violations\n        # User-defined Data\n        plotting_data (dict[str, Any]): Flexible storage for plotting and application data\n    '''\n\n    def __post_init__(self):\n        '''Initialize the results object.'''\n        pass\n\n    def update_plotting_data(self, **kwargs):\n        '''\n        Update the plotting data with additional information.\n        Args:\n            **kwargs: Key-value pairs to add to plotting_data\n        '''\n        pass\n\n    def get(self, key: str, default: Any=None) -> Any:\n        '''\n        Get a value from the results, similar to dict.get().\n        Args:\n            key: The key to look up\n            default: Default value if key is not found\n        Returns:\n            The value associated with the key, or default if not found\n                        '''\n                        pass\n\n    def __getitem__(self, key: str) -> Any:\n        '''\n        Allow dictionary-style access to results.\n        Args:\n            key: The key to look up\n        Returns:\n            The value associated with the key\n        Raises:\n            KeyError: If key is not found\n        '''\n        pass\n\n    def __setitem__(self, key: str, value: Any):\n        '''\n        Allow dictionary-style assignment to results.\n        Args:\n            key: The key to set\n            value: The value to assign\n        '''\n        pass\n\n    def __contains__(self, key: str) -> bool:\n        '''\n        Check if a key exists in the results.\n        Args:\n            key: The key to check\n        Returns:\n            True if key exists, False otherwise\n        '''\n        pass\n\n    def update_plotting_data(self, **kwargs):\n        '''\n        Update the results with additional data from a dictionary.\n        Args:\n            other: Dictionary containing additional data\n        '''\n        pass\n\n    def to_dict(self) -> dict[str, Any]:\n        '''\n        Convert the results to a dictionary for backward compatibility.\n        Returns:\n            Dictionary representation of the results\n        '''\n        pass"
  }
]