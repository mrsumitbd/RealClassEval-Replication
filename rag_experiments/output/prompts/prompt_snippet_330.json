[
  {
    "role": "assistant",
    "content": "You are an expert Python programmer who can correctly implement complete Python classes based on the provided class skeleton."
  },
  {
    "role": "user",
    "content": "For your understanding, following are some (skeleton, code) examples:\n\nExample skeleton:\n\n\nclass MllamaProcessor(ProcessorMixin):\n    '''\n    Constructs a Mllama processor which wraps [`MllamaImageProcessor`] and\n    [`PretrainedTokenizerFast`] into a single processor that inherits both the image processor and\n    tokenizer functionalities. See the [`~MllamaProcessor.__call__`] and [`~OwlViTProcessor.decode`] for more\n    information.\n    The preferred way of passing kwargs is as a dictionary per modality, see usage example below.\n        ```python\n        from transformers import MllamaProcessor\n        from PIL import Image\n        processor = MllamaProcessor.from_pretrained(\"meta-llama/Llama-3.2-11B-Vision\")\n        processor(\n            images=your_pil_image,\n            text=[\"<|image|>If I had to write a haiku for this one\"],\n            images_kwargs = {\"size\": {\"height\": 448, \"width\": 448}},\n            text_kwargs = {\"padding\": \"right\"},\n            common_kwargs = {\"return_tensors\": \"pt\"},\n        )\n        ```\n    Args:\n        image_processor ([`MllamaImageProcessor`]):\n            The image processor is a required input.\n        tokenizer ([`PreTrainedTokenizer`, `PreTrainedTokenizerFast`]):\n            The tokenizer is a required input.\n        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n            in a chat into a tokenizable string.\n    '''\n\n    def __init__(self, image_processor, tokenizer, chat_template=None):\n        pass\n\n    def __call__(self, images: Optional[ImageInput]=None, text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]]=None, audio=None, videos=None, **kwargs: Unpack[MllamaProcessorKwargs]) -> BatchFeature:\n        '''\n        Main method to prepare text(s) and image(s) to be fed as input to the model. This method forwards the `text`\n        arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] if `text` is not `None` to encode\n        the text. To prepare the image(s), this method forwards the `images` arguments to\n        MllamaImageProcessor's [`~MllamaImageProcessor.__call__`] if `images` is not `None`. Please refer\n        to the docstring of the above two methods for more information.\n        Args:\n            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                tensor. Both channels-first and channels-last formats are supported.\n            text (`str`, `list[str]`, `list[list[str]]`):\n                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                If set, will return tensors of a particular framework. Acceptable values are:\n                    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n                    - `'np'`: Return NumPy `np.ndarray` objects.\n        Returns:\n            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n              `None`).\n            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n            TODO: add aspect_ratio_ids and aspect_ratio_mask and cross_attention_mask\n        '''\n        pass\n\n    def post_process_image_text_to_text(self, generated_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False, **kwargs):\n        '''\n        Post-process the output of the model to decode the text.\n        Args:\n            generated_outputs (`torch.Tensor` or `np.ndarray`):\n                The output of the model `generate` function. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n                or `(sequence_length,)`.\n            skip_special_tokens (`bool`, *optional*, defaults to `True`):\n                Whether or not to remove special tokens in the output. Argument passed to the tokenizer's `batch_decode` method.\n            clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n                Whether or not to clean up the tokenization spaces. Argument passed to the tokenizer's `batch_decode` method.\n            **kwargs:\n                Additional arguments to be passed to the tokenizer's `batch_decode method`.\n        Returns:\n            `list[str]`: The decoded text.\n        '''\n        pass\n    @property\n    def model_input_names(self):\n        pass\nCorresponding class implementation:\n\nfrom ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\nfrom ...feature_extraction_utils import BatchFeature\nfrom ...tokenization_utils_base import PreTokenizedInput, TextInput\nfrom ...image_utils import ImageInput, make_nested_list_of_images\nfrom typing import Optional, Union\n\nclass MllamaProcessor(ProcessorMixin):\n    \"\"\"\n    Constructs a Mllama processor which wraps [`MllamaImageProcessor`] and\n    [`PretrainedTokenizerFast`] into a single processor that inherits both the image processor and\n    tokenizer functionalities. See the [`~MllamaProcessor.__call__`] and [`~OwlViTProcessor.decode`] for more\n    information.\n    The preferred way of passing kwargs is as a dictionary per modality, see usage example below.\n        ```python\n        from transformers import MllamaProcessor\n        from PIL import Image\n\n        processor = MllamaProcessor.from_pretrained(\"meta-llama/Llama-3.2-11B-Vision\")\n\n        processor(\n            images=your_pil_image,\n            text=[\"<|image|>If I had to write a haiku for this one\"],\n            images_kwargs = {\"size\": {\"height\": 448, \"width\": 448}},\n            text_kwargs = {\"padding\": \"right\"},\n            common_kwargs = {\"return_tensors\": \"pt\"},\n        )\n        ```\n\n    Args:\n        image_processor ([`MllamaImageProcessor`]):\n            The image processor is a required input.\n        tokenizer ([`PreTrainedTokenizer`, `PreTrainedTokenizerFast`]):\n            The tokenizer is a required input.\n        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n            in a chat into a tokenizable string.\n\n    \"\"\"\n    attributes = ['image_processor', 'tokenizer']\n    image_processor_class = 'MllamaImageProcessor'\n    tokenizer_class = 'PreTrainedTokenizerFast'\n\n    def __init__(self, image_processor, tokenizer, chat_template=None):\n        if not hasattr(tokenizer, 'image_token'):\n            self.image_token = '<|image|>'\n            self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)\n        else:\n            self.image_token = tokenizer.image_token\n            self.image_token_id = tokenizer.image_token_id\n        self.python_token = '<|python_tag|>'\n        self.python_token_id = tokenizer.convert_tokens_to_ids(self.python_token)\n        self.bos_token = tokenizer.bos_token\n        super().__init__(image_processor, tokenizer, chat_template=chat_template)\n\n    def __call__(self, images: Optional[ImageInput]=None, text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]]=None, audio=None, videos=None, **kwargs: Unpack[MllamaProcessorKwargs]) -> BatchFeature:\n        \"\"\"\n        Main method to prepare text(s) and image(s) to be fed as input to the model. This method forwards the `text`\n        arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] if `text` is not `None` to encode\n        the text. To prepare the image(s), this method forwards the `images` arguments to\n        MllamaImageProcessor's [`~MllamaImageProcessor.__call__`] if `images` is not `None`. Please refer\n        to the docstring of the above two methods for more information.\n\n        Args:\n            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                tensor. Both channels-first and channels-last formats are supported.\n            text (`str`, `list[str]`, `list[list[str]]`):\n                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                If set, will return tensors of a particular framework. Acceptable values are:\n                    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n                    - `'np'`: Return NumPy `np.ndarray` objects.\n        Returns:\n            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n\n            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n              `None`).\n            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n            TODO: add aspect_ratio_ids and aspect_ratio_mask and cross_attention_mask\n        \"\"\"\n        if text is None and images is None:\n            raise ValueError('You must specify either text or images.')\n        output_kwargs = self._merge_kwargs(MllamaProcessorKwargs, tokenizer_init_kwargs=self.tokenizer.init_kwargs, **kwargs)\n        text_kwargs = output_kwargs['text_kwargs']\n        text_kwargs['return_tensors'] = None\n        images_kwargs = output_kwargs['images_kwargs']\n        common_kwargs = output_kwargs['common_kwargs']\n        data = {}\n        if text is not None:\n            if isinstance(text, str):\n                text = [text]\n            elif not (isinstance(text, (list, tuple)) and all((isinstance(t, str) for t in text))):\n                raise ValueError('Invalid input text. Please provide a string, or a list of strings')\n            n_images_in_text = [t.count(self.image_token) for t in text]\n            text = [build_string_from_input(text_item, self.bos_token, self.image_token) for text_item in text]\n            _ = text_kwargs.pop('padding_side', None)\n            encoding = self.tokenizer(text, **text_kwargs)\n            self._check_special_mm_tokens(text, encoding, modalities=['image'])\n            n_images_in_ids = [token_ids.count(self.image_token_id) for token_ids in encoding['input_ids']]\n            data.update(encoding)\n        n_images_in_images = [0]\n        if images is not None:\n            images = self.image_processor.fetch_images(images)\n            images = make_nested_list_of_images(images)\n            n_images_in_images = [len(sample) for sample in images]\n        if text is not None:\n            if any((batch_img == 0 for batch_img in n_images_in_text)) and (not all((batch_img == 0 for batch_img in n_images_in_text))):\n                raise ValueError('If a batch of text is provided, there should be either no images or at least one image per sample')\n            if sum(n_images_in_text) > 0 and (n_images_in_images != n_images_in_text or n_images_in_ids != n_images_in_images):\n                if images is None:\n                    raise ValueError('No image were provided, but there are image tokens in the prompt')\n                else:\n                    add_message = ''\n                    if sum(n_images_in_images) == sum(n_images_in_text) and n_images_in_images != n_images_in_text:\n                        add_message = 'Make sure to pass your images as a nested list, where each sub-list holds images per batch'\n                    elif n_images_in_ids != n_images_in_images:\n                        add_message = \"If you activated truncation with `max_length`, increase the `max_length` so image tokens aren't cropped.\"\n                    raise ValueError(f'The number of image tokens in each text ({n_images_in_text}) should be the same as the number of provided images per batch ({n_images_in_images}). {add_message}')\n        if images is not None:\n            image_features = self.image_processor(images, **images_kwargs)\n            num_tiles = image_features.pop('num_tiles')\n            data.update(image_features)\n        if images is not None and text is not None:\n            cross_attention_token_mask = [get_cross_attention_token_mask(token_ids, self.image_token_id) for token_ids in encoding['input_ids']]\n            cross_attention_mask = convert_sparse_cross_attention_mask_to_dense(cross_attention_token_mask, num_tiles=num_tiles, max_num_tiles=self.image_processor.max_image_tiles, length=max((len(input_ids) for input_ids in encoding['input_ids'])))\n            data['cross_attention_mask'] = cross_attention_mask\n        return_tensors = common_kwargs.pop('return_tensors', None)\n        batch_feature = BatchFeature(data=data, tensor_type=return_tensors)\n        return batch_feature\n\n    def post_process_image_text_to_text(self, generated_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False, **kwargs):\n        \"\"\"\n        Post-process the output of the model to decode the text.\n\n        Args:\n            generated_outputs (`torch.Tensor` or `np.ndarray`):\n                The output of the model `generate` function. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n                or `(sequence_length,)`.\n            skip_special_tokens (`bool`, *optional*, defaults to `True`):\n                Whether or not to remove special tokens in the output. Argument passed to the tokenizer's `batch_decode` method.\n            clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n                Whether or not to clean up the tokenization spaces. Argument passed to the tokenizer's `batch_decode` method.\n            **kwargs:\n                Additional arguments to be passed to the tokenizer's `batch_decode method`.\n\n        Returns:\n            `list[str]`: The decoded text.\n        \"\"\"\n        return self.tokenizer.batch_decode(generated_outputs, skip_special_tokens=skip_special_tokens, clean_up_tokenization_spaces=clean_up_tokenization_spaces, **kwargs)\n\n    @property\n    def model_input_names(self):\n        tokenizer_input_names = self.tokenizer.model_input_names\n        image_processor_input_names = self.image_processor.model_input_names\n        image_processor_input_names = [name for name in image_processor_input_names if name != 'num_tiles']\n        return list(tokenizer_input_names + image_processor_input_names + ['cross_attention_mask'])\n\nExample skeleton:\n\n\nclass SpeechToTextV1Adapter(SpeechToTextV1):\n\n    def recognize_using_websocket(self, audio, content_type, recognize_callback, model=None, language_customization_id=None, acoustic_customization_id=None, customization_weight=None, base_model_version=None, inactivity_timeout=None, interim_results=None, keywords=None, keywords_threshold=None, max_alternatives=None, word_alternatives_threshold=None, word_confidence=None, timestamps=None, profanity_filter=None, smart_formatting=None, smart_formatting_version=None, speaker_labels=None, http_proxy_host=None, http_proxy_port=None, grammar_name=None, redaction=None, processing_metrics=None, processing_metrics_interval=None, audio_metrics=None, end_of_phrase_silence_time=None, split_transcript_at_phrase_end=None, speech_detector_sensitivity=None, background_audio_suppression=None, low_latency=None, character_insertion_bias=None, **kwargs):\n        '''\n        Sends audio for speech recognition using web sockets.\n\n        :param AudioSource audio: The audio to transcribe in the format specified by the\n            `Content-Type` header.\n        :param str content_type: The type of the input: audio/basic, audio/flac,\n            audio/l16, audio/mp3, audio/mpeg, audio/mulaw, audio/ogg, audio/ogg;codecs=opus,\n            audio/ogg;codecs=vorbis, audio/wav, audio/webm, audio/webm;codecs=opus, or\n            audio/webm;codecs=vorbis.\n        :param RecognizeCallback recognize_callback: The callback method for the websocket.\n        :param str model: (optional) The identifier of the model that is to be used\n               for the recognition request. (**Note:** The model `ar-AR_BroadbandModel` is\n               deprecated; use `ar-MS_BroadbandModel` instead.) See [Languages and\n               models](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-models)\n               and [Next-generation languages and\n               models](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-models-ng).\n        :param str language_customization_id: (optional) The customization ID\n               (GUID) of a custom language model that is to be used with the recognition\n               request. The base model of the specified custom language model must match\n               the model specified with the `model` parameter. You must make the request\n               with credentials for the instance of the service that owns the custom\n               model. By default, no custom language model is used. See [Using a custom\n               language model for speech\n               recognition](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-languageUse).\n               **Note:** Use this parameter instead of the deprecated `customization_id`\n               parameter.\n        :param str acoustic_customization_id: (optional) The customization ID\n               (GUID) of a custom acoustic model that is to be used with the recognition\n               request. The base model of the specified custom acoustic model must match\n               the model specified with the `model` parameter. You must make the request\n               with credentials for the instance of the service that owns the custom\n               model. By default, no custom acoustic model is used. See [Using a custom\n               acoustic model for speech\n               recognition](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-acousticUse).\n        :param str base_model_version: (optional) The version of the specified base\n               model that is to be used with the recognition request. Multiple versions of\n               a base model can exist when a model is updated for internal improvements.\n               The parameter is intended primarily for use with custom models that have\n               been upgraded for a new base model. The default value depends on whether\n               the parameter is used with or without a custom model. See [Making speech\n               recognition requests with upgraded custom\n               models](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-custom-upgrade-use#custom-upgrade-use-recognition).\n        :param float customization_weight: (optional) If you specify the\n               customization ID (GUID) of a custom language model with the recognition\n               request, the customization weight tells the service how much weight to give\n               to words from the custom language model compared to those from the base\n               model for the current request.\n               Specify a value between 0.0 and 1.0. Unless a different customization\n               weight was specified for the custom model when it was trained, the default\n               value is 0.3. A customization weight that you specify overrides a weight\n               that was specified when the custom model was trained.\n               The default value yields the best performance in general. Assign a higher\n               value if your audio makes frequent use of OOV words from the custom model.\n               Use caution when setting the weight: a higher value can improve the\n               accuracy of phrases from the custom model's domain, but it can negatively\n               affect performance on non-domain phrases.\n               See [Using customization\n               weight](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-languageUse#weight).\n        :param int inactivity_timeout: (optional) The time in seconds after which,\n               if only silence (no speech) is detected in streaming audio, the connection\n               is closed with a 400 error. The parameter is useful for stopping audio\n               submission from a live microphone when a user simply walks away. Use `-1`\n               for infinity. See [Inactivity\n               timeout](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-input#timeouts-inactivity).\n        :param List[str] keywords: (optional) An array of keyword strings to spot\n               in the audio. Each keyword string can include one or more string tokens.\n               Keywords are spotted only in the final results, not in interim hypotheses.\n               If you specify any keywords, you must also specify a keywords threshold.\n               Omit the parameter or specify an empty array if you do not need to spot\n               keywords.\n               You can spot a maximum of 1000 keywords with a single request. A single\n               keyword can have a maximum length of 1024 characters, though the maximum\n               effective length for double-byte languages might be shorter. Keywords are\n               case-insensitive.\n               See [Keyword\n               spotting](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-spotting#keyword-spotting).\n        :param float keywords_threshold: (optional) A confidence value that is the\n               lower bound for spotting a keyword. A word is considered to match a keyword\n               if its confidence is greater than or equal to the threshold. Specify a\n               probability between 0.0 and 1.0. If you specify a threshold, you must also\n               specify one or more keywords. The service performs no keyword spotting if\n               you omit either parameter. See [Keyword\n               spotting](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-spotting#keyword-spotting).\n        :param int max_alternatives: (optional) The maximum number of alternative\n               transcripts that the service is to return. By default, the service returns\n               a single transcript. If you specify a value of `0`, the service uses the\n               default value, `1`. See [Maximum\n               alternatives](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-metadata#max-alternatives).\n        :param float word_alternatives_threshold: (optional) A confidence value\n               that is the lower bound for identifying a hypothesis as a possible word\n               alternative (also known as \"Confusion Networks\"). An alternative word is\n               considered if its confidence is greater than or equal to the threshold.\n               Specify a probability between 0.0 and 1.0. By default, the service computes\n               no alternative words. See [Word\n               alternatives](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-spotting#word-alternatives).\n        :param bool word_confidence: (optional) If `true`, the service returns a\n               confidence measure in the range of 0.0 to 1.0 for each word. By default,\n               the service returns no word confidence scores. See [Word\n               confidence](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-metadata#word-confidence).\n        :param bool timestamps: (optional) If `true`, the service returns time\n               alignment for each word. By default, no timestamps are returned. See [Word\n               timestamps](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-metadata#word-timestamps).\n        :param bool profanity_filter: (optional) If `true`, the service filters\n               profanity from all output except for keyword results by replacing\n               inappropriate words with a series of asterisks. Set the parameter to\n               `false` to return results with no censoring. Applies to US English and\n               Japanese transcription only. See [Profanity\n               filtering](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-formatting#profanity-filtering).\n        :param bool smart_formatting: (optional) If `true`, the service converts\n               dates, times, series of digits and numbers, phone numbers, currency values,\n               and internet addresses into more readable, conventional representations in\n               the final transcript of a recognition request. For US English, the service\n               also converts certain keyword strings to punctuation symbols. By default,\n               the service performs no smart formatting.\n               **Note:** Applies to US English, Japanese, and Spanish transcription only.\n               See [Smart\n               formatting](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-formatting#smart-formatting).\n       :param  int smart_formatting_version: (optional) Smart formatting version is\n               for next-generation models and that is supported in US English, Brazilian\n               Portuguese, French and German languages.\n        :param bool speaker_labels: (optional) If `true`, the response includes\n               labels that identify which words were spoken by which participants in a\n               multi-person exchange. By default, the service returns no speaker labels.\n               Setting `speaker_labels` to `true` forces the `timestamps` parameter to be\n               `true`, regardless of whether you specify `false` for the parameter.\n               * For previous-generation models, can be used for US English, Australian\n               English, German, Japanese, Korean, and Spanish (both broadband and\n               narrowband models) and UK English (narrowband model) transcription only.\n               * For next-generation models, can be used for English (Australian, UK, and\n               US), German, and Spanish transcription only.\n               Restrictions and limitations apply to the use of speaker labels for both\n               types of models. See [Speaker\n               labels](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-speaker-labels).\n        :param str http_proxy_host: http proxy host name.\n        :param str http_proxy_port: http proxy port. If not set, set to 80.\n        :param str grammar_name: (optional) The name of a grammar that is to be\n               used with the recognition request. If you specify a grammar, you must also\n               use the `language_customization_id` parameter to specify the name of the\n               custom language model for which the grammar is defined. The service\n               recognizes only strings that are recognized by the specified grammar; it\n               does not recognize other custom words from the model's words resource. See\n               [Using a grammar for speech\n               recognition](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-grammarUse).\n        :param bool redaction: (optional) If `true`, the service redacts, or masks,\n               numeric data from final transcripts. The feature redacts any number that\n               has three or more consecutive digits by replacing each digit with an `X`\n               character. It is intended to redact sensitive numeric data, such as credit\n               card numbers. By default, the service performs no redaction.\n               When you enable redaction, the service automatically enables smart\n               formatting, regardless of whether you explicitly disable that feature. To\n               ensure maximum security, the service also disables keyword spotting\n               (ignores the `keywords` and `keywords_threshold` parameters) and returns\n               only a single final transcript (forces the `max_alternatives` parameter to\n               be `1`).\n               **Note:** Applies to US English, Japanese, and Korean transcription only.\n               See [Numeric\n               redaction](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-formatting#numeric-redaction).\n        :param bool audio_metrics: (optional) If `true`, requests detailed\n               information about the signal characteristics of the input audio. The\n               service returns audio metrics with the final transcription results. By\n               default, the service returns no audio metrics.\n               See [Audio\n               metrics](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-metrics#audio-metrics).\n        :param float end_of_phrase_silence_time: (optional) If `true`, specifies\n               the duration of the pause interval at which the service splits a transcript\n               into multiple final results. If the service detects pauses or extended\n               silence before it reaches the end of the audio stream, its response can\n               include multiple final results. Silence indicates a point at which the\n               speaker pauses between spoken words or phrases.\n               Specify a value for the pause interval in the range of 0.0 to 120.0.\n               * A value greater than 0 specifies the interval that the service is to use\n               for speech recognition.\n               * A value of 0 indicates that the service is to use the default interval.\n               It is equivalent to omitting the parameter.\n               The default pause interval for most languages is 0.8 seconds; the default\n               for Chinese is 0.6 seconds.\n               See [End of phrase silence\n               time](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-parsing#silence-time).\n        :param bool split_transcript_at_phrase_end: (optional) If `true`, directs\n               the service to split the transcript into multiple final results based on\n               semantic features of the input, for example, at the conclusion of\n               meaningful phrases such as sentences. The service bases its understanding\n               of semantic features on the base language model that you use with a\n               request. Custom language models and grammars can also influence how and\n               where the service splits a transcript. By default, the service splits\n               transcripts based solely on the pause interval.\n               See [Split transcript at phrase\n               end](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-parsing#split-transcript).\n        :param float speech_detector_sensitivity: (optional) The sensitivity of\n               speech activity detection that the service is to perform. Use the parameter\n               to suppress word insertions from music, coughing, and other non-speech\n               events. The service biases the audio it passes for speech recognition by\n               evaluating the input audio against prior models of speech and non-speech\n               activity.\n               Specify a value between 0.0 and 1.0:\n               * 0.0 suppresses all audio (no speech is transcribed).\n               * 0.5 (the default) provides a reasonable compromise for the level of\n               sensitivity.\n               * 1.0 suppresses no audio (speech detection sensitivity is disabled).\n               The values increase on a monotonic curve. See [Speech detector\n               sensitivity](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-detection#detection-parameters-sensitivity).\n        :param float background_audio_suppression: (optional) The level to which\n               the service is to suppress background audio based on its volume to prevent\n               it from being transcribed as speech. Use the parameter to suppress side\n               conversations or background noise.\n               Specify a value in the range of 0.0 to 1.0:\n               * 0.0 (the default) provides no suppression (background audio suppression\n               is disabled).\n               * 0.5 provides a reasonable level of audio suppression for general usage.\n               * 1.0 suppresses all audio (no audio is transcribed).\n               The values increase on a monotonic curve. See [Background audio\n               suppression](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-detection#detection-parameters-suppression).\n        :param bool low_latency: (optional) If `true` for next-generation\n               `Multimedia` and `Telephony` models that support low latency, directs the\n               service to produce results even more quickly than it usually does.\n               Next-generation models produce transcription results faster than\n               previous-generation models. The `low_latency` parameter causes the models\n               to produce results even more quickly, though the results might be less\n               accurate when the parameter is used.\n               **Note:** The parameter is beta functionality. It is not available for\n               previous-generation `Broadband` and `Narrowband` models. It is available\n               only for some next-generation models.\n               * For a list of next-generation models that support low latency, see\n               [Supported language\n               models](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-models-ng#models-ng-supported)\n               for next-generation models.\n               * For more information about the `low_latency` parameter, see [Low\n               latency](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-interim#low-latency).\n        :param float character_insertion_bias: (optional) For next-generation\n               `Multimedia` and `Telephony` models, an indication of whether the service\n               is biased to recognize shorter or longer strings of characters when\n               developing transcription hypotheses. By default, the service is optimized\n               for each individual model to balance its recognition of strings of\n               different lengths. The model-specific bias is equivalent to 0.0.\n               The value that you specify represents a change from a model's default bias.\n               The allowable range of values is -1.0 to 1.0.\n               * Negative values bias the service to favor hypotheses with shorter strings\n               of characters.\n               * Positive values bias the service to favor hypotheses with longer strings\n               of characters.\n               As the value approaches -1.0 or 1.0, the impact of the parameter becomes\n               more pronounced. To determine the most effective value for your scenario,\n               start by setting the value of the parameter to a small increment, such as\n               -0.1, -0.05, 0.05, or 0.1, and assess how the value impacts the\n               transcription results. Then experiment with different values as necessary,\n               adjusting the value by small increments.\n               The parameter is not available for previous-generation `Broadband` and\n               `Narrowband` models.\n               See [Character insertion\n               bias](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-parsing#insertion-bias).\n        :param dict headers: A `dict` containing the request headers\n        :return: A `dict` containing the `SpeechRecognitionResults` response.\n        :rtype: dict\n                              '''\n                              pass\nCorresponding class implementation:\n\nfrom ibm_watson.websocket import RecognizeCallback, RecognizeListener, AudioSource\nfrom urllib.parse import urlencode\nfrom .speech_to_text_v1 import SpeechToTextV1\n\nclass SpeechToTextV1Adapter(SpeechToTextV1):\n\n    def recognize_using_websocket(self, audio, content_type, recognize_callback, model=None, language_customization_id=None, acoustic_customization_id=None, customization_weight=None, base_model_version=None, inactivity_timeout=None, interim_results=None, keywords=None, keywords_threshold=None, max_alternatives=None, word_alternatives_threshold=None, word_confidence=None, timestamps=None, profanity_filter=None, smart_formatting=None, smart_formatting_version=None, speaker_labels=None, http_proxy_host=None, http_proxy_port=None, grammar_name=None, redaction=None, processing_metrics=None, processing_metrics_interval=None, audio_metrics=None, end_of_phrase_silence_time=None, split_transcript_at_phrase_end=None, speech_detector_sensitivity=None, background_audio_suppression=None, low_latency=None, character_insertion_bias=None, **kwargs):\n        \"\"\"\n        Sends audio for speech recognition using web sockets.\n\n\n        :param AudioSource audio: The audio to transcribe in the format specified by the\n            `Content-Type` header.\n        :param str content_type: The type of the input: audio/basic, audio/flac,\n            audio/l16, audio/mp3, audio/mpeg, audio/mulaw, audio/ogg, audio/ogg;codecs=opus,\n            audio/ogg;codecs=vorbis, audio/wav, audio/webm, audio/webm;codecs=opus, or\n            audio/webm;codecs=vorbis.\n        :param RecognizeCallback recognize_callback: The callback method for the websocket.\n        :param str model: (optional) The identifier of the model that is to be used\n               for the recognition request. (**Note:** The model `ar-AR_BroadbandModel` is\n               deprecated; use `ar-MS_BroadbandModel` instead.) See [Languages and\n               models](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-models)\n               and [Next-generation languages and\n               models](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-models-ng).\n        :param str language_customization_id: (optional) The customization ID\n               (GUID) of a custom language model that is to be used with the recognition\n               request. The base model of the specified custom language model must match\n               the model specified with the `model` parameter. You must make the request\n               with credentials for the instance of the service that owns the custom\n               model. By default, no custom language model is used. See [Using a custom\n               language model for speech\n               recognition](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-languageUse).\n               **Note:** Use this parameter instead of the deprecated `customization_id`\n               parameter.\n        :param str acoustic_customization_id: (optional) The customization ID\n               (GUID) of a custom acoustic model that is to be used with the recognition\n               request. The base model of the specified custom acoustic model must match\n               the model specified with the `model` parameter. You must make the request\n               with credentials for the instance of the service that owns the custom\n               model. By default, no custom acoustic model is used. See [Using a custom\n               acoustic model for speech\n               recognition](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-acousticUse).\n        :param str base_model_version: (optional) The version of the specified base\n               model that is to be used with the recognition request. Multiple versions of\n               a base model can exist when a model is updated for internal improvements.\n               The parameter is intended primarily for use with custom models that have\n               been upgraded for a new base model. The default value depends on whether\n               the parameter is used with or without a custom model. See [Making speech\n               recognition requests with upgraded custom\n               models](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-custom-upgrade-use#custom-upgrade-use-recognition).\n        :param float customization_weight: (optional) If you specify the\n               customization ID (GUID) of a custom language model with the recognition\n               request, the customization weight tells the service how much weight to give\n               to words from the custom language model compared to those from the base\n               model for the current request.\n               Specify a value between 0.0 and 1.0. Unless a different customization\n               weight was specified for the custom model when it was trained, the default\n               value is 0.3. A customization weight that you specify overrides a weight\n               that was specified when the custom model was trained.\n               The default value yields the best performance in general. Assign a higher\n               value if your audio makes frequent use of OOV words from the custom model.\n               Use caution when setting the weight: a higher value can improve the\n               accuracy of phrases from the custom model's domain, but it can negatively\n               affect performance on non-domain phrases.\n               See [Using customization\n               weight](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-languageUse#weight).\n        :param int inactivity_timeout: (optional) The time in seconds after which,\n               if only silence (no speech) is detected in streaming audio, the connection\n               is closed with a 400 error. The parameter is useful for stopping audio\n               submission from a live microphone when a user simply walks away. Use `-1`\n               for infinity. See [Inactivity\n               timeout](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-input#timeouts-inactivity).\n        :param List[str] keywords: (optional) An array of keyword strings to spot\n               in the audio. Each keyword string can include one or more string tokens.\n               Keywords are spotted only in the final results, not in interim hypotheses.\n               If you specify any keywords, you must also specify a keywords threshold.\n               Omit the parameter or specify an empty array if you do not need to spot\n               keywords.\n               You can spot a maximum of 1000 keywords with a single request. A single\n               keyword can have a maximum length of 1024 characters, though the maximum\n               effective length for double-byte languages might be shorter. Keywords are\n               case-insensitive.\n               See [Keyword\n               spotting](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-spotting#keyword-spotting).\n        :param float keywords_threshold: (optional) A confidence value that is the\n               lower bound for spotting a keyword. A word is considered to match a keyword\n               if its confidence is greater than or equal to the threshold. Specify a\n               probability between 0.0 and 1.0. If you specify a threshold, you must also\n               specify one or more keywords. The service performs no keyword spotting if\n               you omit either parameter. See [Keyword\n               spotting](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-spotting#keyword-spotting).\n        :param int max_alternatives: (optional) The maximum number of alternative\n               transcripts that the service is to return. By default, the service returns\n               a single transcript. If you specify a value of `0`, the service uses the\n               default value, `1`. See [Maximum\n               alternatives](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-metadata#max-alternatives).\n        :param float word_alternatives_threshold: (optional) A confidence value\n               that is the lower bound for identifying a hypothesis as a possible word\n               alternative (also known as \"Confusion Networks\"). An alternative word is\n               considered if its confidence is greater than or equal to the threshold.\n               Specify a probability between 0.0 and 1.0. By default, the service computes\n               no alternative words. See [Word\n               alternatives](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-spotting#word-alternatives).\n        :param bool word_confidence: (optional) If `true`, the service returns a\n               confidence measure in the range of 0.0 to 1.0 for each word. By default,\n               the service returns no word confidence scores. See [Word\n               confidence](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-metadata#word-confidence).\n        :param bool timestamps: (optional) If `true`, the service returns time\n               alignment for each word. By default, no timestamps are returned. See [Word\n               timestamps](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-metadata#word-timestamps).\n        :param bool profanity_filter: (optional) If `true`, the service filters\n               profanity from all output except for keyword results by replacing\n               inappropriate words with a series of asterisks. Set the parameter to\n               `false` to return results with no censoring. Applies to US English and\n               Japanese transcription only. See [Profanity\n               filtering](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-formatting#profanity-filtering).\n        :param bool smart_formatting: (optional) If `true`, the service converts\n               dates, times, series of digits and numbers, phone numbers, currency values,\n               and internet addresses into more readable, conventional representations in\n               the final transcript of a recognition request. For US English, the service\n               also converts certain keyword strings to punctuation symbols. By default,\n               the service performs no smart formatting.\n               **Note:** Applies to US English, Japanese, and Spanish transcription only.\n               See [Smart\n               formatting](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-formatting#smart-formatting).\n       :param  int smart_formatting_version: (optional) Smart formatting version is\n               for next-generation models and that is supported in US English, Brazilian\n               Portuguese, French and German languages.\n        :param bool speaker_labels: (optional) If `true`, the response includes\n               labels that identify which words were spoken by which participants in a\n               multi-person exchange. By default, the service returns no speaker labels.\n               Setting `speaker_labels` to `true` forces the `timestamps` parameter to be\n               `true`, regardless of whether you specify `false` for the parameter.\n               * For previous-generation models, can be used for US English, Australian\n               English, German, Japanese, Korean, and Spanish (both broadband and\n               narrowband models) and UK English (narrowband model) transcription only.\n               * For next-generation models, can be used for English (Australian, UK, and\n               US), German, and Spanish transcription only.\n               Restrictions and limitations apply to the use of speaker labels for both\n               types of models. See [Speaker\n               labels](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-speaker-labels).\n        :param str http_proxy_host: http proxy host name.\n        :param str http_proxy_port: http proxy port. If not set, set to 80.\n        :param str grammar_name: (optional) The name of a grammar that is to be\n               used with the recognition request. If you specify a grammar, you must also\n               use the `language_customization_id` parameter to specify the name of the\n               custom language model for which the grammar is defined. The service\n               recognizes only strings that are recognized by the specified grammar; it\n               does not recognize other custom words from the model's words resource. See\n               [Using a grammar for speech\n               recognition](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-grammarUse).\n        :param bool redaction: (optional) If `true`, the service redacts, or masks,\n               numeric data from final transcripts. The feature redacts any number that\n               has three or more consecutive digits by replacing each digit with an `X`\n               character. It is intended to redact sensitive numeric data, such as credit\n               card numbers. By default, the service performs no redaction.\n               When you enable redaction, the service automatically enables smart\n               formatting, regardless of whether you explicitly disable that feature. To\n               ensure maximum security, the service also disables keyword spotting\n               (ignores the `keywords` and `keywords_threshold` parameters) and returns\n               only a single final transcript (forces the `max_alternatives` parameter to\n               be `1`).\n               **Note:** Applies to US English, Japanese, and Korean transcription only.\n               See [Numeric\n               redaction](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-formatting#numeric-redaction).\n        :param bool audio_metrics: (optional) If `true`, requests detailed\n               information about the signal characteristics of the input audio. The\n               service returns audio metrics with the final transcription results. By\n               default, the service returns no audio metrics.\n               See [Audio\n               metrics](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-metrics#audio-metrics).\n        :param float end_of_phrase_silence_time: (optional) If `true`, specifies\n               the duration of the pause interval at which the service splits a transcript\n               into multiple final results. If the service detects pauses or extended\n               silence before it reaches the end of the audio stream, its response can\n               include multiple final results. Silence indicates a point at which the\n               speaker pauses between spoken words or phrases.\n               Specify a value for the pause interval in the range of 0.0 to 120.0.\n               * A value greater than 0 specifies the interval that the service is to use\n               for speech recognition.\n               * A value of 0 indicates that the service is to use the default interval.\n               It is equivalent to omitting the parameter.\n               The default pause interval for most languages is 0.8 seconds; the default\n               for Chinese is 0.6 seconds.\n               See [End of phrase silence\n               time](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-parsing#silence-time).\n        :param bool split_transcript_at_phrase_end: (optional) If `true`, directs\n               the service to split the transcript into multiple final results based on\n               semantic features of the input, for example, at the conclusion of\n               meaningful phrases such as sentences. The service bases its understanding\n               of semantic features on the base language model that you use with a\n               request. Custom language models and grammars can also influence how and\n               where the service splits a transcript. By default, the service splits\n               transcripts based solely on the pause interval.\n               See [Split transcript at phrase\n               end](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-parsing#split-transcript).\n        :param float speech_detector_sensitivity: (optional) The sensitivity of\n               speech activity detection that the service is to perform. Use the parameter\n               to suppress word insertions from music, coughing, and other non-speech\n               events. The service biases the audio it passes for speech recognition by\n               evaluating the input audio against prior models of speech and non-speech\n               activity.\n               Specify a value between 0.0 and 1.0:\n               * 0.0 suppresses all audio (no speech is transcribed).\n               * 0.5 (the default) provides a reasonable compromise for the level of\n               sensitivity.\n               * 1.0 suppresses no audio (speech detection sensitivity is disabled).\n               The values increase on a monotonic curve. See [Speech detector\n               sensitivity](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-detection#detection-parameters-sensitivity).\n        :param float background_audio_suppression: (optional) The level to which\n               the service is to suppress background audio based on its volume to prevent\n               it from being transcribed as speech. Use the parameter to suppress side\n               conversations or background noise.\n               Specify a value in the range of 0.0 to 1.0:\n               * 0.0 (the default) provides no suppression (background audio suppression\n               is disabled).\n               * 0.5 provides a reasonable level of audio suppression for general usage.\n               * 1.0 suppresses all audio (no audio is transcribed).\n               The values increase on a monotonic curve. See [Background audio\n               suppression](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-detection#detection-parameters-suppression).\n        :param bool low_latency: (optional) If `true` for next-generation\n               `Multimedia` and `Telephony` models that support low latency, directs the\n               service to produce results even more quickly than it usually does.\n               Next-generation models produce transcription results faster than\n               previous-generation models. The `low_latency` parameter causes the models\n               to produce results even more quickly, though the results might be less\n               accurate when the parameter is used.\n               **Note:** The parameter is beta functionality. It is not available for\n               previous-generation `Broadband` and `Narrowband` models. It is available\n               only for some next-generation models.\n               * For a list of next-generation models that support low latency, see\n               [Supported language\n               models](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-models-ng#models-ng-supported)\n               for next-generation models.\n               * For more information about the `low_latency` parameter, see [Low\n               latency](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-interim#low-latency).\n        :param float character_insertion_bias: (optional) For next-generation\n               `Multimedia` and `Telephony` models, an indication of whether the service\n               is biased to recognize shorter or longer strings of characters when\n               developing transcription hypotheses. By default, the service is optimized\n               for each individual model to balance its recognition of strings of\n               different lengths. The model-specific bias is equivalent to 0.0.\n               The value that you specify represents a change from a model's default bias.\n               The allowable range of values is -1.0 to 1.0.\n               * Negative values bias the service to favor hypotheses with shorter strings\n               of characters.\n               * Positive values bias the service to favor hypotheses with longer strings\n               of characters.\n               As the value approaches -1.0 or 1.0, the impact of the parameter becomes\n               more pronounced. To determine the most effective value for your scenario,\n               start by setting the value of the parameter to a small increment, such as\n               -0.1, -0.05, 0.05, or 0.1, and assess how the value impacts the\n               transcription results. Then experiment with different values as necessary,\n               adjusting the value by small increments.\n               The parameter is not available for previous-generation `Broadband` and\n               `Narrowband` models.\n               See [Character insertion\n               bias](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-parsing#insertion-bias).\n        :param dict headers: A `dict` containing the request headers\n        :return: A `dict` containing the `SpeechRecognitionResults` response.\n        :rtype: dict\n        \"\"\"\n        if audio is None:\n            raise ValueError('audio must be provided')\n        if not isinstance(audio, AudioSource):\n            raise Exception('audio is not of type AudioSource. Import the class from ibm_watson.websocket')\n        if content_type is None:\n            raise ValueError('content_type must be provided')\n        if recognize_callback is None:\n            raise ValueError('recognize_callback must be provided')\n        if not isinstance(recognize_callback, RecognizeCallback):\n            raise Exception('Callback is not a derived class of RecognizeCallback')\n        request = {}\n        headers = {}\n        if self.default_headers is not None:\n            headers = self.default_headers.copy()\n        if 'headers' in kwargs:\n            headers.update(kwargs.get('headers'))\n        request['headers'] = headers\n        if self.authenticator:\n            self.authenticator.authenticate(request)\n        url = self.service_url.replace('https:', 'wss:')\n        params = {'model': model, 'acoustic_customization_id': acoustic_customization_id, 'base_model_version': base_model_version, 'language_customization_id': language_customization_id}\n        params = {k: v for k, v in params.items() if v is not None}\n        url += '/v1/recognize?{0}'.format(urlencode(params))\n        request['url'] = url\n        options = {'customization_weight': customization_weight, 'content_type': content_type, 'inactivity_timeout': inactivity_timeout, 'interim_results': interim_results, 'keywords': keywords, 'keywords_threshold': keywords_threshold, 'max_alternatives': max_alternatives, 'word_alternatives_threshold': word_alternatives_threshold, 'word_confidence': word_confidence, 'timestamps': timestamps, 'profanity_filter': profanity_filter, 'smart_formatting': smart_formatting, 'smart_formatting_version': smart_formatting_version, 'speaker_labels': speaker_labels, 'grammar_name': grammar_name, 'redaction': redaction, 'processing_metrics': processing_metrics, 'processing_metrics_interval': processing_metrics_interval, 'audio_metrics': audio_metrics, 'end_of_phrase_silence_time': end_of_phrase_silence_time, 'split_transcript_at_phrase_end': split_transcript_at_phrase_end, 'speech_detector_sensitivity': speech_detector_sensitivity, 'background_audio_suppression': background_audio_suppression, 'character_insertion_bias': character_insertion_bias, 'low_latency': low_latency}\n        options = {k: v for k, v in options.items() if v is not None}\n        request['options'] = options\n        RecognizeListener(audio, request.get('options'), recognize_callback, request.get('url'), request.get('headers'), http_proxy_host, http_proxy_port, self.disable_ssl_verification)\n\nNow, implement the following class. Do not explain the code. The given class skeleton is as follows:\n\nclass AIRefinery:\n    '''\n    A top-level client that exposes various sub-clients in a single interface,\n    operating synchronously.\n    Example usage:\n        client = AIRefinery(\n            api_key=\"...\",\n            base_url=\"...\",\n            default_headers={\"X-Client-Version\": \"1.2.3\"}\n        )\n        # Use chat\n        response = client.chat.completions.create(\n            model=\"model-name\", messages=[...]\n        )\n        # Use embeddings\n        embeddings_response = client.embeddings.create(\n            model=\"model-name\", input=[\"Hello\"]\n        )\n        # Use tts\n        tts_response = client.audio.speech.create(\n            model=\"model-name\",\n            input=\"Hello, this is a test of text-to-speech synthesis.\",\n            voice=\"en-US-JennyNeural\",\n            response_format=\"mp3\",  # Optional\n            speed=1.0  # Optional\n        # Use asr\n        asr_response = client.speech_to_text.create(\n            model=\"model-name\",\n            file=[\"audio1.wav\", \"audio2.wav\"]\n        )\n        # Use models\n        models_list = client.models.list()\n        # Use images\n        image_response = await client.images.generate(\n            prompt=\"A cute baby sea otter\", model=\"model-name\"\n        )\n        # Use knowledge\n        knowledge_client = client.knowledge\n        knowledge_client.create_project(config)\n        knowledge_response = await knowledge_client.document_processing.parse_documents(file_path='', model='')\n        # Attempting to use client.distiller will raise an exception\n        # (not supported in sync mode).\n    '''\n\n    def __init__(self, api_key: str, base_url: str='https://api.airefinery.accenture.com', default_headers: dict[str, str] | None=None, **kwargs):\n        '''\n        Initializes the synchronous unified client with sub-clients.\n        Args:\n            api_key (str): Your API key or token for authenticated requests.\n            base_url (str, optional): Base URL for your API endpoints.\n                Defaults to \"https://api.airefinery.accenture.com\".\n            default_headers (dict[str, str] | None): Headers that apply to\n                every request made by sub-clients (e.g., {\"X-Client-Version\": \"1.2.3\"}).\n            **kwargs: Additional configuration parameters, if any.\n                        '''\n                        pass\n\n    def _update_inference_endpoint(self):\n        '''\n        Ensures the base_url ends with the '/inference' suffix if necessary.\n        This method checks whether the `base_url` ends with \"/inference\".\n        If it does not, the method appends \"/inference\" to the URL and returns\n        the updated URL. This is particularly useful for ensuring compatibility\n        with authentication mechanisms like auth.openai(), where the \"/inference\"\n        suffix is automatically appended to the base URL.\n        Returns:\n            str: The base URL with \"/inference\" appended if it was not already present.\n        '''\n        pass\n    @property\n    def distiller(self):\n        '''\n        Distiller is only supported in the asynchronous client.\n        Accessing this property in the synchronous client will raise a NotImplementedError.\n        '''\n        pass"
  }
]