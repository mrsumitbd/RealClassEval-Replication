[
  {
    "role": "assistant",
    "content": "You are an expert Python programmer who can correctly implement complete Python classes based on the provided class skeleton."
  },
  {
    "role": "user",
    "content": "For your understanding, following are some (skeleton, code) examples:\n\nExample skeleton:\n\n\nclass Qwen2_5_VLProcessor(Qwen2VLProcessor):\n    '''\n    Constructs a Qwen2.5-VL processor which wraps a Qwen2.5-VL image processor and a Qwen2 tokenizer into a single processor.\n    [`Qwen2_5_VLProcessor`] offers all the functionalities of [`Qwen2VLImageProcessor`] and [`Qwen2TokenizerFast`]. See the\n    [`~Qwen2_5_VLProcessor.__call__`] and [`~Qwen2_5_VLProcessor.decode`] for more information.\n    Args:\n        image_processor ([`Qwen2VLImageProcessor`], *optional*):\n            The image processor is a required input.\n        tokenizer ([`Qwen2TokenizerFast`], *optional*):\n            The tokenizer is a required input.\n        video_processor ([`Qwen2_5_VLVideoProcessor`], *optional*):\n            The video processor is a required input.\n        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n            in a chat into a tokenizable string.\n    '''\n    @property\n    def model_input_names(self):\n        pass\n\n    def __call__(self, images: Optional[ImageInput]=None, text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]=None, videos: Optional[VideoInput]=None, **kwargs: Unpack[Qwen2_5_VLProcessorKwargs]) -> BatchFeature:\n        '''\n        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n        and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n        the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwargs` arguments to\n        Qwen2VLImageProcessor's [`~Qwen2VLImageProcessor.__call__`] if `vision_infos` is not `None`.\n        Args:\n            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                tensor. Both channels-first and channels-last formats are supported.\n            text (`str`, `list[str]`, `list[list[str]]`):\n                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n            videos (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n                tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                If set, will return tensors of a particular framework. Acceptable values are:\n                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n                - `'np'`: Return NumPy `np.ndarray` objects.\n        Returns:\n            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n              `None`).\n            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n            - **pixel_values_videos** -- Pixel values of videos to be fed to a model. Returned when `videos` is not `None`.\n            - **image_grid_thw** -- List of image 3D grid in LLM. Returned when `images` is not `None`.\n            - **video_grid_thw** -- List of video 3D grid in LLM. Returned when `videos` is not `None`.\n            - **second_per_grid_ts** -- List of video seconds per time grid. Returned when `videos` is not `None`.\n        '''\n        pass\n\n    def _get_num_multimodal_tokens(self, image_sizes=None, video_sizes=None, **kwargs):\n        '''\n        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n        Args:\n            image_sizes (`list[list[int]]`, *optional*):\n                The input sizes formatted as (height, width) per each image.\n            video_sizes (`list[list[int]]`, *optional*):\n                The input sizes formatted as (num_frames, height, width) per each video.\n        Returns:\n            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n            input modalities, along with other useful data.\n        '''\n        pass\nCorresponding class implementation:\n\nfrom transformers.models.qwen2_vl.processing_qwen2_vl import Qwen2VLImagesKwargs, Qwen2VLProcessor\nfrom ...tokenization_utils_base import PreTokenizedInput, TextInput\nimport numpy as np\nfrom ...feature_extraction_utils import BatchFeature\nfrom ...processing_utils import MultiModalData, ProcessingKwargs, Unpack, VideosKwargs\nfrom ...video_utils import VideoInput\nfrom ...image_utils import ImageInput\nfrom typing import Optional, Union\n\nclass Qwen2_5_VLProcessor(Qwen2VLProcessor):\n    \"\"\"\n    Constructs a Qwen2.5-VL processor which wraps a Qwen2.5-VL image processor and a Qwen2 tokenizer into a single processor.\n    [`Qwen2_5_VLProcessor`] offers all the functionalities of [`Qwen2VLImageProcessor`] and [`Qwen2TokenizerFast`]. See the\n    [`~Qwen2_5_VLProcessor.__call__`] and [`~Qwen2_5_VLProcessor.decode`] for more information.\n    Args:\n        image_processor ([`Qwen2VLImageProcessor`], *optional*):\n            The image processor is a required input.\n        tokenizer ([`Qwen2TokenizerFast`], *optional*):\n            The tokenizer is a required input.\n        video_processor ([`Qwen2_5_VLVideoProcessor`], *optional*):\n            The video processor is a required input.\n        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n            in a chat into a tokenizable string.\n    \"\"\"\n    image_processor_class = 'AutoImageProcessor'\n\n    @property\n    def model_input_names(self):\n        tokenizer_input_names = self.tokenizer.model_input_names\n        image_processor_input_names = self.image_processor.model_input_names\n        names_from_processor = list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n        return names_from_processor + ['second_per_grid_ts']\n\n    def __call__(self, images: Optional[ImageInput]=None, text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]=None, videos: Optional[VideoInput]=None, **kwargs: Unpack[Qwen2_5_VLProcessorKwargs]) -> BatchFeature:\n        \"\"\"\n        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n        and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n        the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwargs` arguments to\n        Qwen2VLImageProcessor's [`~Qwen2VLImageProcessor.__call__`] if `vision_infos` is not `None`.\n\n        Args:\n            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                tensor. Both channels-first and channels-last formats are supported.\n            text (`str`, `list[str]`, `list[list[str]]`):\n                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n            videos (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n                tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                If set, will return tensors of a particular framework. Acceptable values are:\n                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n                - `'np'`: Return NumPy `np.ndarray` objects.\n\n        Returns:\n            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n\n            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n              `None`).\n            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n            - **pixel_values_videos** -- Pixel values of videos to be fed to a model. Returned when `videos` is not `None`.\n            - **image_grid_thw** -- List of image 3D grid in LLM. Returned when `images` is not `None`.\n            - **video_grid_thw** -- List of video 3D grid in LLM. Returned when `videos` is not `None`.\n            - **second_per_grid_ts** -- List of video seconds per time grid. Returned when `videos` is not `None`.\n        \"\"\"\n        output_kwargs = self._merge_kwargs(Qwen2_5_VLProcessorKwargs, tokenizer_init_kwargs=self.tokenizer.init_kwargs, **kwargs)\n        image_inputs = videos_inputs = {}\n        if images is not None:\n            image_inputs = self.image_processor(images=images, **output_kwargs['images_kwargs'])\n            image_grid_thw = image_inputs['image_grid_thw']\n        if videos is not None:\n            fps = output_kwargs['videos_kwargs'].get('fps', 2.0)\n            videos_inputs = self.video_processor(videos=videos, **output_kwargs['videos_kwargs'])\n            video_grid_thw = videos_inputs['video_grid_thw']\n            if isinstance(fps, (int, float)):\n                second_per_grid_ts = [self.video_processor.temporal_patch_size / fps] * len(video_grid_thw)\n            elif hasattr(fps, '__len__') and len(fps) == len(video_grid_thw):\n                second_per_grid_ts = [self.video_processor.temporal_patch_size / tmp for tmp in fps]\n            else:\n                raise ValueError(f\"The length of fps ({(len(fps) if hasattr(fps, '__len__') else fps)}) must be equal to the length of video_grid_thw ({len(video_grid_thw)}) or fps should be a single number.\")\n            videos_inputs.update({'second_per_grid_ts': second_per_grid_ts})\n        if not isinstance(text, list):\n            text = [text]\n        text = text.copy()\n        if images is not None:\n            merge_length = self.image_processor.merge_size ** 2\n            index = 0\n            for i in range(len(text)):\n                while self.image_token in text[i]:\n                    num_image_tokens = image_grid_thw[index].prod() // merge_length\n                    text[i] = text[i].replace(self.image_token, '<|placeholder|>' * num_image_tokens, 1)\n                    index += 1\n                text[i] = text[i].replace('<|placeholder|>', self.image_token)\n        if videos is not None:\n            merge_length = self.video_processor.merge_size ** 2\n            index = 0\n            for i in range(len(text)):\n                while self.video_token in text[i]:\n                    num_video_tokens = video_grid_thw[index].prod() // merge_length\n                    text[i] = text[i].replace(self.video_token, '<|placeholder|>' * num_video_tokens, 1)\n                    index += 1\n                text[i] = text[i].replace('<|placeholder|>', self.video_token)\n        return_tensors = output_kwargs['text_kwargs'].pop('return_tensors', None)\n        return_mm_token_type_ids = output_kwargs['text_kwargs'].pop('return_mm_token_type_ids', None)\n        text_inputs = self.tokenizer(text, **output_kwargs['text_kwargs'])\n        self._check_special_mm_tokens(text, text_inputs, modalities=['image', 'video'])\n        if return_mm_token_type_ids:\n            array_ids = np.array(text_inputs['input_ids'])\n            mm_token_type_ids = np.zeros_like(text_inputs['input_ids'])\n            mm_token_type_ids[array_ids == self.image_token_id] = 1\n            text_inputs['mm_token_type_ids'] = mm_token_type_ids.tolist()\n        return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs}, tensor_type=return_tensors)\n\n    def _get_num_multimodal_tokens(self, image_sizes=None, video_sizes=None, **kwargs):\n        \"\"\"\n        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n        Args:\n            image_sizes (`list[list[int]]`, *optional*):\n                The input sizes formatted as (height, width) per each image.\n            video_sizes (`list[list[int]]`, *optional*):\n                The input sizes formatted as (num_frames, height, width) per each video.\n        Returns:\n            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n            input modalities, along with other useful data.\n        \"\"\"\n        vision_data = {}\n        if image_sizes is not None:\n            images_kwargs = Qwen2_5_VLProcessorKwargs._defaults.get('images_kwargs', {})\n            images_kwargs.update(kwargs)\n            merge_size = images_kwargs.get('merge_size', None) or self.image_processor.merge_size\n            num_image_patches = [self.image_processor.get_number_of_image_patches(*image_size, images_kwargs) for image_size in image_sizes]\n            num_image_tokens = [num_patches // merge_size ** 2 for num_patches in num_image_patches]\n            vision_data.update({'num_image_tokens': num_image_tokens, 'num_image_patches': num_image_patches})\n        if video_sizes is not None:\n            videos_kwargs = Qwen2_5_VLProcessorKwargs._defaults.get('videos_kwargs', {})\n            videos_kwargs.update(kwargs)\n            num_video_patches = [self.video_processor.get_number_of_video_patches(*video_size, videos_kwargs) for video_size in video_sizes]\n            num_video_tokens = [num_patches // merge_size ** 2 for num_patches in num_video_patches]\n            vision_data['num_video_tokens'] = num_video_tokens\n        return MultiModalData(**vision_data)\n\nExample skeleton:\n\n\nclass Filter(ABC, Generic[FilterValueT]):\n    '''\n    A class representing a Lavalink audio filter.\n    Parameters\n    ----------\n    values: Union[Dict[str, Any], Sequence[Union[float, int]], float]\n        The values for this filter.\n    plugin_filter: :class:`bool`\n        Whether this filter is part of a Lavalink plugin. Typically, this will be ``True``\n        when creating your own filters.\n    Attributes\n    ----------\n    values: Union[Dict[str, Any], Sequence[Union[float, int]], float]\n        The values for this filter.\n    plugin_filter: :class:`bool`\n        Whether this filter is part of a Lavalink plugin.\n    '''\n\n    def __init__(self, values: FilterValueT, plugin_filter: bool=False):\n        pass\n    @abstractmethod\n    def update(self, **kwargs):\n        ''' Updates the internal values to match those provided. '''\n        pass\n    @abstractmethod\n    def serialize(self) -> Dict[str, FilterValueT]:\n        '''\n        Transforms the internal values into a dict matching the structure Lavalink expects.\n        Example:\n            .. code:: python\n                return {\"yourCustomFilter\": {\"gain\": 5}}\n        '''\n        pass\nCorresponding class implementation:\n\nfrom typing import TYPE_CHECKING, Any, Dict, Final, Generic, Optional, Sequence, TypeVar, Union, cast\nfrom abc import ABC, abstractmethod\n\nclass Filter(ABC, Generic[FilterValueT]):\n    \"\"\"\n    A class representing a Lavalink audio filter.\n\n    Parameters\n    ----------\n    values: Union[Dict[str, Any], Sequence[Union[float, int]], float]\n        The values for this filter.\n    plugin_filter: :class:`bool`\n        Whether this filter is part of a Lavalink plugin. Typically, this will be ``True``\n        when creating your own filters.\n\n    Attributes\n    ----------\n    values: Union[Dict[str, Any], Sequence[Union[float, int]], float]\n        The values for this filter.\n    plugin_filter: :class:`bool`\n        Whether this filter is part of a Lavalink plugin.\n    \"\"\"\n    __slots__ = ('values', 'plugin_filter')\n\n    def __init__(self, values: FilterValueT, plugin_filter: bool=False):\n        self.values: FilterValueT = values\n        self.plugin_filter: Final[bool] = plugin_filter\n\n    @abstractmethod\n    def update(self, **kwargs):\n        \"\"\" Updates the internal values to match those provided. \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def serialize(self) -> Dict[str, FilterValueT]:\n        \"\"\"\n        Transforms the internal values into a dict matching the structure Lavalink expects.\n\n        Example:\n\n            .. code:: python\n\n                return {\"yourCustomFilter\": {\"gain\": 5}}\n        \"\"\"\n        raise NotImplementedError\n\nNow, implement the following class. Do not explain the code. The given class skeleton is as follows:\n\nclass NunchakuIPAdapterLoader:\n    '''\n    Node for loading Nunchaku IP-Adapter pipelines.\n    .. warning::\n        This node will automatically download the IP-Adapter and associated CLIP models from Hugging Face.\n        Custom model paths are not supported for now.\n    '''\n    @classmethod\n    def INPUT_TYPES(s):\n        '''\n        Defines the input types and tooltips for the node.\n        Returns\n        -------\n        dict\n            A dictionary specifying the required inputs and their descriptions for the node interface.\n        '''\n        pass\n\n    def load(self, model):\n        '''\n        Load the IP-Adapter pipeline and attach it to the given model.\n        Parameters\n        ----------\n        model : object\n            The Nunchaku model to which the IP-Adapter will be attached.\n            It should be loaded with :class:`~comfyui_nunchaku.nodes.models.flux.NunchakuFluxDiTLoader`.\n        Returns\n        -------\n        tuple\n            The original model and the loaded IP-Adapter pipeline.\n        '''\n        pass"
  }
]