[
  {
    "role": "assistant",
    "content": "You are an expert Python programmer who can correctly implement complete Python classes based on the provided class skeleton."
  },
  {
    "role": "user",
    "content": "For your understanding, following are some (skeleton, code) examples:\n\nExample skeleton:\n\n\nclass ScatterPlot(PluginInterface):\n    '''A Scatter Plot Plugin for Feature Sets.'''\n\n    def __init__(self, show_axes: bool=True):\n        '''Initialize the Scatter Plot Plugin\n        Args:\n            show_axes (bool): Whether to show the axes and grid. Default is True.\n        '''\n        pass\n\n    def create_component(self, component_id: str) -> html.Div:\n        '''Create a Dash Graph Component without any data.\n        Args:\n            component_id (str): The ID of the web component.\n        Returns:\n            html.Div: A Dash Div Component containing the graph and dropdowns.\n        '''\n        pass\n\n    def update_properties(self, input_data: pd.DataFrame, **kwargs) -> list:\n        '''Update the property values for the plugin component.\n        Args:\n            input_data (pd.DataFrame): The input data object.\n            **kwargs: Additional keyword arguments (plugins can define their own arguments).\n                      Note: The current kwargs processed are:\n                            - x: The default x-axis column\n                            - y: The default y-axis column\n                            - color: The default color column\n                            - dropdown_columns: The columns to use for the x and y options\n                            - hover_columns: The columns to show when hovering over a point\n                            - suppress_hover_display: Suppress hover display (default: False)\n                            - custom_data: Custom data that get passed to hoverData callbacks\n        Returns:\n            list: A list of updated property values (figure, x options, y options, color options,\n                                                    label options, x default, y default,\n                                                    color default).\n        '''\n        pass\n\n    def create_scatter_plot(self, df: pd.DataFrame, x_col: str, y_col: str, color_col: str, label_col: str, regression_line: bool=False, marker_size: int=15) -> go.Figure:\n        '''Create a Plotly Scatter Plot figure.\n        Args:\n            df (pd.DataFrame): The dataframe containing the data.\n            x_col (str): The column to use for the x-axis.\n            y_col (str): The column to use for the y-axis.\n            color_col (str): The column to use for the color scale.\n            label_col (str): The column to use for point labels.\n            regression_line (bool): Whether to include a regression line.\n            marker_size (int): Size of the markers. Default is 15.\n        Returns:\n            go.Figure: A Plotly Figure object.\n        '''\n        pass\n\n        def generate_hover_text(row):\n                pass\n\n    def register_internal_callbacks(self):\n        '''Register any internal callbacks for the plugin.'''\n        pass\n        @callback(Output(f'{self.component_id}-graph', 'figure', allow_duplicate=True), [Input(f'{self.component_id}-x-dropdown', 'value'), Input(f'{self.component_id}-y-dropdown', 'value'), Input(f'{self.component_id}-color-dropdown', 'value'), Input(f'{self.component_id}-label-dropdown', 'value'), Input(f'{self.component_id}-regression-line', 'value')], prevent_initial_call=True)\n        def _update_scatter_plot(x_value, y_value, color_value, label_value, regression_line):\n                '''Update the Scatter Plot Graph based on the dropdown values.'''\n                pass\n        @callback(Output(f'{self.component_id}-overlay', 'show'), Output(f'{self.component_id}-overlay', 'bbox'), Output(f'{self.component_id}-overlay', 'children'), Input(f'{self.component_id}-graph', 'hoverData'))\n        def _scatter_overlay(hover_data):\n                pass\nCorresponding class implementation:\n\nfrom workbench.utils.theme_manager import ThemeManager\nfrom dash import dcc, html, callback, Input, Output, no_update\nimport plotly.express as px\nimport base64\nimport pandas as pd\nfrom dash.exceptions import PreventUpdate\nfrom workbench.utils.plot_utils import prediction_intervals\nimport plotly.graph_objects as go\nfrom workbench.web_interface.components.plugin_interface import PluginInterface, PluginPage, PluginInputType\n\nclass ScatterPlot(PluginInterface):\n    \"\"\"A Scatter Plot Plugin for Feature Sets.\"\"\"\n    auto_load_page = PluginPage.NONE\n    plugin_input_type = PluginInputType.DATAFRAME\n\n    def __init__(self, show_axes: bool=True):\n        \"\"\"Initialize the Scatter Plot Plugin\n\n        Args:\n            show_axes (bool): Whether to show the axes and grid. Default is True.\n        \"\"\"\n        self.component_id = None\n        self.hover_columns = []\n        self.df = None\n        self.show_axes = show_axes\n        self.theme_manager = ThemeManager()\n        self.colorscale = self.theme_manager.colorscale()\n        super().__init__()\n\n    def create_component(self, component_id: str) -> html.Div:\n        \"\"\"Create a Dash Graph Component without any data.\n\n        Args:\n            component_id (str): The ID of the web component.\n\n        Returns:\n            html.Div: A Dash Div Component containing the graph and dropdowns.\n        \"\"\"\n        self.component_id = component_id\n        self.properties = [(f'{component_id}-graph', 'figure'), (f'{component_id}-x-dropdown', 'options'), (f'{component_id}-y-dropdown', 'options'), (f'{component_id}-color-dropdown', 'options'), (f'{component_id}-label-dropdown', 'options'), (f'{component_id}-x-dropdown', 'value'), (f'{component_id}-y-dropdown', 'value'), (f'{component_id}-color-dropdown', 'value')]\n        self.signals = [(f'{component_id}-graph', 'hoverData'), (f'{component_id}-graph', 'clickData')]\n        return html.Div(children=[dcc.Graph(id=f'{component_id}-graph', figure=self.display_text('Waiting for Data...'), config={'scrollZoom': True}, style={'height': '100%'}, clear_on_unhover=True), html.Div([html.Label('X', style={'marginLeft': '40px', 'marginRight': '5px', 'fontWeight': 'bold'}), dcc.Dropdown(id=f'{component_id}-x-dropdown', className='dropdown', style={'min-width': '50px', 'flex': 1}, clearable=False), html.Label('Y', style={'marginLeft': '30px', 'marginRight': '5px', 'fontWeight': 'bold'}), dcc.Dropdown(id=f'{component_id}-y-dropdown', className='dropdown', style={'min-width': '50px', 'flex': 1}, clearable=False), html.Label('Color', style={'marginLeft': '30px', 'marginRight': '5px', 'fontWeight': 'bold'}), dcc.Dropdown(id=f'{component_id}-color-dropdown', className='dropdown', style={'min-width': '50px', 'flex': 1}, clearable=False), html.Label('Label', style={'marginLeft': '30px', 'marginRight': '5px', 'fontWeight': 'bold'}), dcc.Dropdown(id=f'{component_id}-label-dropdown', className='dropdown', style={'min-width': '50px', 'flex': 1}, options=[{'label': 'None', 'value': 'none'}], value='none', clearable=False), dcc.Checklist(id=f'{component_id}-regression-line', options=[{'label': ' Diagonal', 'value': 'show'}], value=[], style={'margin': '10px'})], style={'padding': '0px 0px 10px 0px', 'display': 'flex', 'gap': '10px'}), dcc.Tooltip(id=f'{component_id}-overlay', background_color='rgba(0,0,0,0)', border_color='rgba(0,0,0,0)', direction='bottom', loading_text='')], style={'height': '100%', 'display': 'flex', 'flexDirection': 'column'})\n\n    def update_properties(self, input_data: pd.DataFrame, **kwargs) -> list:\n        \"\"\"Update the property values for the plugin component.\n\n        Args:\n            input_data (pd.DataFrame): The input data object.\n            **kwargs: Additional keyword arguments (plugins can define their own arguments).\n                      Note: The current kwargs processed are:\n                            - x: The default x-axis column\n                            - y: The default y-axis column\n                            - color: The default color column\n                            - dropdown_columns: The columns to use for the x and y options\n                            - hover_columns: The columns to show when hovering over a point\n                            - suppress_hover_display: Suppress hover display (default: False)\n                            - custom_data: Custom data that get passed to hoverData callbacks\n\n        Returns:\n            list: A list of updated property values (figure, x options, y options, color options,\n                                                    label options, x default, y default,\n                                                    color default).\n        \"\"\"\n        limit = kwargs.get('limit', 20000)\n        if isinstance(input_data, pd.DataFrame):\n            self.df = input_data.sample(n=limit) if len(input_data) > limit else input_data\n        else:\n            raise ValueError('The input data must be a Pandas DataFrame.')\n        aws_cols = ['write_time', 'api_invocation_time', 'is_deleted', 'event_time']\n        self.df = self.df.drop(columns=aws_cols, errors='ignore')\n        self.hover_columns = kwargs.get('hover_columns', self.df.columns.tolist()[:10])\n        self.suppress_hover_display = kwargs.get('suppress_hover_display', False)\n        self.custom_data = kwargs.get('custom_data', [])\n        numeric_columns = self.df.select_dtypes(include='number').columns.tolist()\n        if len(numeric_columns) < 3:\n            raise ValueError('At least three numeric columns are required for x, y, and color.')\n        x_default = kwargs.get('x', numeric_columns[0])\n        y_default = kwargs.get('y', numeric_columns[1])\n        color_default = kwargs.get('color', numeric_columns[2])\n        regression_line = kwargs.get('regression_line', False)\n        figure = self.create_scatter_plot(self.df, x_default, y_default, color_default, 'none', regression_line)\n        dropdown_columns = kwargs.get('dropdown_columns', numeric_columns)\n        x_options = [{'label': col, 'value': col} for col in dropdown_columns]\n        y_options = x_options.copy()\n        cat_columns = self.df.select_dtypes(include=['object', 'string', 'category']).columns.tolist()\n        cat_columns = [col for col in cat_columns if self.df[col].astype(str).nunique() < 20]\n        color_columns = numeric_columns + cat_columns\n        color_options = [{'label': col, 'value': col} for col in color_columns]\n        label_options = [{'label': 'None', 'value': 'none'}]\n        label_options.extend([{'label': col, 'value': col} for col in self.df.columns])\n        return [figure, x_options, y_options, color_options, label_options, x_default, y_default, color_default]\n\n    def create_scatter_plot(self, df: pd.DataFrame, x_col: str, y_col: str, color_col: str, label_col: str, regression_line: bool=False, marker_size: int=15) -> go.Figure:\n        \"\"\"Create a Plotly Scatter Plot figure.\n\n        Args:\n            df (pd.DataFrame): The dataframe containing the data.\n            x_col (str): The column to use for the x-axis.\n            y_col (str): The column to use for the y-axis.\n            color_col (str): The column to use for the color scale.\n            label_col (str): The column to use for point labels.\n            regression_line (bool): Whether to include a regression line.\n            marker_size (int): Size of the markers. Default is 15.\n\n        Returns:\n            go.Figure: A Plotly Figure object.\n        \"\"\"\n        show_labels = label_col != 'none' and len(df) < 1000\n\n        def generate_hover_text(row):\n            return '<br>'.join([f'{col}: {row[col]}' for col in self.hover_columns])\n        hovertext = df.apply(generate_hover_text, axis=1)\n        hovertemplate = '%{hovertext}<extra></extra>'\n        hoverinfo = 'none' if self.suppress_hover_display else None\n        if pd.api.types.is_numeric_dtype(df[color_col]):\n            marker_color = df[color_col]\n            colorbar = dict(title=color_col, thickness=10)\n            data = [go.Scattergl(x=df[x_col], y=df[y_col], mode='markers', text=df[label_col].astype(str) if show_labels else None, textposition='top center', hoverinfo=hoverinfo, hovertext=hovertext, hovertemplate=hovertemplate, customdata=df[self.custom_data], marker=dict(size=marker_size, color=marker_color, colorscale=self.colorscale, colorbar=colorbar, opacity=0.8, line=dict(color='rgba(0,0,0,0.25)', width=1)))]\n            showlegend = False\n        else:\n            categories = sorted(df[color_col].astype(str).unique())\n            discrete_colors = px.colors.qualitative.Plotly\n            data = []\n            for i, cat in enumerate(categories):\n                sub_df = df[df[color_col] == cat]\n                sub_hovertext = hovertext.loc[sub_df.index]\n                trace = go.Scattergl(x=sub_df[x_col], y=sub_df[y_col], mode='markers', text=sub_df[label_col] if show_labels else None, textposition='top center', name=cat, hoverinfo=hoverinfo, hovertext=sub_hovertext, hovertemplate=hovertemplate, customdata=sub_df[self.custom_data], marker=dict(size=marker_size, color=discrete_colors[i % len(discrete_colors)], opacity=0.8, line=dict(color='rgba(0,0,0,0.25)', width=1)))\n                data.append(trace)\n            showlegend = True\n        figure = go.Figure()\n        if y_col == 'prediction' or x_col == 'prediction':\n            figure = prediction_intervals(df, figure, x_col)\n        if regression_line:\n            axis_min = min(df[x_col].min(), df[y_col].min())\n            axis_max = max(df[x_col].max(), df[y_col].max())\n            figure.add_shape(type='line', line=dict(width=4, color='rgba(128, 128, 128, 1.0)'), x0=axis_min, x1=axis_max, y0=axis_min, y1=axis_max)\n        figure.add_traces(data)\n        if self.show_axes:\n            xaxis = dict(title=dict(text=x_col, font=dict(size=16), standoff=15), tickformat='.2f', tickfont=dict(size=10))\n            yaxis = dict(title=dict(text=y_col, font=dict(size=16), standoff=25), tickformat='.2f', tickfont=dict(size=10))\n        else:\n            xaxis = dict(visible=False)\n            yaxis = dict(visible=False)\n        figure.update_layout(margin={'t': 20, 'b': 55, 'r': 0, 'l': 35, 'pad': 0}, xaxis=xaxis, yaxis=yaxis, showlegend=showlegend, dragmode='pan', modebar={'bgcolor': 'rgba(0, 0, 0, 0)'}, uirevision='constant')\n        return figure\n\n    def register_internal_callbacks(self):\n        \"\"\"Register any internal callbacks for the plugin.\"\"\"\n\n        @callback(Output(f'{self.component_id}-graph', 'figure', allow_duplicate=True), [Input(f'{self.component_id}-x-dropdown', 'value'), Input(f'{self.component_id}-y-dropdown', 'value'), Input(f'{self.component_id}-color-dropdown', 'value'), Input(f'{self.component_id}-label-dropdown', 'value'), Input(f'{self.component_id}-regression-line', 'value')], prevent_initial_call=True)\n        def _update_scatter_plot(x_value, y_value, color_value, label_value, regression_line):\n            \"\"\"Update the Scatter Plot Graph based on the dropdown values.\"\"\"\n            if not self.df.empty and x_value and y_value and color_value:\n                figure = self.create_scatter_plot(self.df, x_value, y_value, color_value, label_value, regression_line)\n                return figure\n            raise PreventUpdate\n\n        @callback(Output(f'{self.component_id}-overlay', 'show'), Output(f'{self.component_id}-overlay', 'bbox'), Output(f'{self.component_id}-overlay', 'children'), Input(f'{self.component_id}-graph', 'hoverData'))\n        def _scatter_overlay(hover_data):\n            if hover_data is None:\n                return (False, no_update, no_update)\n            bbox = hover_data['points'][0]['bbox']\n            svg = '\\n            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"100\" height=\"100\" style=\"overflow: visible;\">\\n                <!-- Circle for the node -->\\n                <circle cx=\"50\" cy=\"50\" r=\"10\" stroke=\"rgba(255, 255, 255, 1)\" stroke-width=\"3\" fill=\"none\" />\\n            </svg>\\n            '\n            encoded_svg = base64.b64encode(svg.encode('utf-8')).decode('utf-8')\n            data_uri = f'data:image/svg+xml;base64,{encoded_svg}'\n            svg_image = html.Img(src=data_uri, style={'width': '100px', 'height': '100px'})\n            center_x = (bbox['x0'] + bbox['x1']) / 2\n            center_y = (bbox['y0'] + bbox['y1']) / 2\n            adjusted_bbox = {'x0': center_x - 50, 'x1': center_x + 50, 'y0': center_y - 162, 'y1': center_y - 62}\n            return (True, adjusted_bbox, [svg_image])\n\nExample skeleton:\n\n\nclass DataWranglerProcessor(Processor):\n    '''Handles Amazon SageMaker DataWrangler tasks'''\n\n    def __init__(self, role: str=None, data_wrangler_flow_source: str=None, instance_count: int=None, instance_type: str=None, volume_size_in_gb: int=30, volume_kms_key: str=None, output_kms_key: str=None, max_runtime_in_seconds: int=None, base_job_name: str=None, sagemaker_session: Session=None, env: Dict[str, str]=None, tags: Optional[Tags]=None, network_config: NetworkConfig=None):\n        '''Initializes a ``Processor`` instance.\n        The ``Processor`` handles Amazon SageMaker Processing tasks.\n        Args:\n            role (str): An AWS IAM role name or ARN. Amazon SageMaker Processing\n                uses this role to access AWS resources, such as\n                data stored in Amazon S3.\n            data_wrangler_flow_source (str): The source of the DaraWrangler flow which will be\n                used for the DataWrangler job. If a local path is provided, it will automatically\n                be uploaded to S3 under:\n                \"s3://<default-bucket-name>/<job-name>/input/<input-name>\".\n            instance_count (int): The number of instances to run\n                a processing job with.\n            instance_type (str): The type of EC2 instance to use for\n                processing, for example, 'ml.c4.xlarge'.\n            volume_size_in_gb (int): Size in GB of the EBS volume\n                to use for storing data during processing (default: 30).\n            volume_kms_key (str): A KMS key for the processing\n                volume (default: None).\n            output_kms_key (str): The KMS key ID for processing job outputs (default: None).\n            max_runtime_in_seconds (int): Timeout in seconds (default: None).\n                After this amount of time, Amazon SageMaker terminates the job,\n                regardless of its current status. If `max_runtime_in_seconds` is not\n                specified, the default value is 24 hours.\n            base_job_name (str): Prefix for processing job name. If not specified,\n                the processor generates a default job name, based on the\n                processing image name and current timestamp.\n            sagemaker_session (:class:`~sagemaker.session.Session`):\n                Session object which manages interactions with Amazon SageMaker and\n                any other AWS services needed. If not specified, the processor creates\n                one using the default AWS configuration chain.\n            env (dict[str, str]): Environment variables to be passed to\n                the processing jobs (default: None).\n            tags (Optional[Tags]): Tags to be passed to the processing job\n                (default: None). For more, see\n                https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n            network_config (:class:`~sagemaker.network.NetworkConfig`):\n                A :class:`~sagemaker.network.NetworkConfig`\n                object that configures network isolation, encryption of\n                inter-container traffic, security group IDs, and subnets.\n        '''\n        pass\n\n    def _normalize_args(self, job_name=None, arguments=None, inputs=None, outputs=None, code=None, kms_key=None):\n        '''Normalizes the arguments so that they can be passed to the job run\n        Args:\n            job_name (str): Name of the processing job to be created. If not specified, one\n                is generated, using the base name given to the constructor, if applicable\n                (default: None).\n            arguments (list[str]): A list of string arguments to be passed to a\n                processing job (default: None).\n            inputs (list[:class:`~sagemaker.processing.ProcessingInput`]): Input files for\n                the processing job. These must be provided as\n                :class:`~sagemaker.processing.ProcessingInput` objects (default: None).\n            outputs (list[:class:`~sagemaker.processing.ProcessingOutput`]): Outputs for\n                the processing job. These can be specified as either path strings or\n                :class:`~sagemaker.processing.ProcessingOutput` objects (default: None).\n            code (str): This can be an S3 URI or a local path to a file with the framework\n                script to run (default: None). A no op in the base class.\n            kms_key (str): The ARN of the KMS key that is used to encrypt the\n                user code file (default: None).\n        '''\n        pass\n\n    def _get_recipe_input(self):\n        '''Creates a ProcessingInput with Data Wrangler recipe uri and appends it to inputs'''\n        pass\nCorresponding class implementation:\n\nfrom sagemaker.processing import ProcessingInput, Processor\nfrom sagemaker.network import NetworkConfig\nfrom typing import Dict, Optional\nfrom sagemaker.utils import format_tags, Tags\nfrom sagemaker.session import Session\nfrom sagemaker import image_uris\n\nclass DataWranglerProcessor(Processor):\n    \"\"\"Handles Amazon SageMaker DataWrangler tasks\"\"\"\n\n    def __init__(self, role: str=None, data_wrangler_flow_source: str=None, instance_count: int=None, instance_type: str=None, volume_size_in_gb: int=30, volume_kms_key: str=None, output_kms_key: str=None, max_runtime_in_seconds: int=None, base_job_name: str=None, sagemaker_session: Session=None, env: Dict[str, str]=None, tags: Optional[Tags]=None, network_config: NetworkConfig=None):\n        \"\"\"Initializes a ``Processor`` instance.\n\n        The ``Processor`` handles Amazon SageMaker Processing tasks.\n\n        Args:\n            role (str): An AWS IAM role name or ARN. Amazon SageMaker Processing\n                uses this role to access AWS resources, such as\n                data stored in Amazon S3.\n            data_wrangler_flow_source (str): The source of the DaraWrangler flow which will be\n                used for the DataWrangler job. If a local path is provided, it will automatically\n                be uploaded to S3 under:\n                \"s3://<default-bucket-name>/<job-name>/input/<input-name>\".\n            instance_count (int): The number of instances to run\n                a processing job with.\n            instance_type (str): The type of EC2 instance to use for\n                processing, for example, 'ml.c4.xlarge'.\n            volume_size_in_gb (int): Size in GB of the EBS volume\n                to use for storing data during processing (default: 30).\n            volume_kms_key (str): A KMS key for the processing\n                volume (default: None).\n            output_kms_key (str): The KMS key ID for processing job outputs (default: None).\n            max_runtime_in_seconds (int): Timeout in seconds (default: None).\n                After this amount of time, Amazon SageMaker terminates the job,\n                regardless of its current status. If `max_runtime_in_seconds` is not\n                specified, the default value is 24 hours.\n            base_job_name (str): Prefix for processing job name. If not specified,\n                the processor generates a default job name, based on the\n                processing image name and current timestamp.\n            sagemaker_session (:class:`~sagemaker.session.Session`):\n                Session object which manages interactions with Amazon SageMaker and\n                any other AWS services needed. If not specified, the processor creates\n                one using the default AWS configuration chain.\n            env (dict[str, str]): Environment variables to be passed to\n                the processing jobs (default: None).\n            tags (Optional[Tags]): Tags to be passed to the processing job\n                (default: None). For more, see\n                https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n            network_config (:class:`~sagemaker.network.NetworkConfig`):\n                A :class:`~sagemaker.network.NetworkConfig`\n                object that configures network isolation, encryption of\n                inter-container traffic, security group IDs, and subnets.\n        \"\"\"\n        self.data_wrangler_flow_source = data_wrangler_flow_source\n        self.sagemaker_session = sagemaker_session or Session()\n        image_uri = image_uris.retrieve('data-wrangler', region=self.sagemaker_session.boto_region_name)\n        super().__init__(role, image_uri, instance_count, instance_type, volume_size_in_gb=volume_size_in_gb, volume_kms_key=volume_kms_key, output_kms_key=output_kms_key, max_runtime_in_seconds=max_runtime_in_seconds, base_job_name=base_job_name, sagemaker_session=sagemaker_session, env=env, tags=format_tags(tags), network_config=network_config)\n\n    def _normalize_args(self, job_name=None, arguments=None, inputs=None, outputs=None, code=None, kms_key=None):\n        \"\"\"Normalizes the arguments so that they can be passed to the job run\n\n        Args:\n            job_name (str): Name of the processing job to be created. If not specified, one\n                is generated, using the base name given to the constructor, if applicable\n                (default: None).\n            arguments (list[str]): A list of string arguments to be passed to a\n                processing job (default: None).\n            inputs (list[:class:`~sagemaker.processing.ProcessingInput`]): Input files for\n                the processing job. These must be provided as\n                :class:`~sagemaker.processing.ProcessingInput` objects (default: None).\n            outputs (list[:class:`~sagemaker.processing.ProcessingOutput`]): Outputs for\n                the processing job. These can be specified as either path strings or\n                :class:`~sagemaker.processing.ProcessingOutput` objects (default: None).\n            code (str): This can be an S3 URI or a local path to a file with the framework\n                script to run (default: None). A no op in the base class.\n            kms_key (str): The ARN of the KMS key that is used to encrypt the\n                user code file (default: None).\n        \"\"\"\n        inputs = inputs or []\n        found = any((element.input_name == 'flow' for element in inputs))\n        if not found:\n            inputs.append(self._get_recipe_input())\n        return super()._normalize_args(job_name, arguments, inputs, outputs, code, kms_key)\n\n    def _get_recipe_input(self):\n        \"\"\"Creates a ProcessingInput with Data Wrangler recipe uri and appends it to inputs\"\"\"\n        return ProcessingInput(source=self.data_wrangler_flow_source, destination='/opt/ml/processing/flow', input_name='flow', s3_data_type='S3Prefix', s3_input_mode='File', s3_data_distribution_type='FullyReplicated')\n\nNow, implement the following class. Do not explain the code. The given class skeleton is as follows:\n\nclass SimpleDataPipeline:\n    '''\n    A simple data pipeline that orchestrates data loading, column mapping,\n    and data splitting.\n    The ingestion process is as follows:\n    1. Load data using the `data_source`. This can result in a single\n       DataFrame or an already split `DataSplit` object.\n    2. Apply column transformations (filtering, renaming) using the `column_mapper`.\n       This mapper can operate on both single DataFrames and `DataSplit` objects.\n    3. If the data after mapping is a single DataFrame, split it using the\n       `data_splitter`. If it's already a `DataSplit`, this step is skipped.\n    '''\n\n    def __init__(self, data_source: DataSource, column_mapper: ColumnMapper, data_splitter: Optional[DataSplitter]=None):\n        '''\n        Initializes the SimpleDataPipeline.\n        Args:\n            data_source (DataSource): The component responsible for loading the initial data.\n            column_mapper (ColumnMapper): The component for transforming columns.\n                                              It should handle both pd.DataFrame and DataSplit inputs.\n            data_splitter (Optional[DataSplitter]): The component for splitting a single DataFrame\n                                                    into train, validation, and test sets.\n                                                    This is not used if data_source already provides split data.\n        '''\n        pass\n\n    def __call__(self) -> pd.DataFrame | DataSplit:\n        '''\n        Executes the data ingestion pipeline with validation.\n        Returns:\n            pd.DataFrame | DataSplit: The final processed data, either as a single DataFrame\n                                       or a DataSplit object containing train, validation, and test sets.\n        Raises:\n            ValueError: If there is a configuration mismatch, such as:\n                        - A `data_splitter` is provided for a pre-split dataset.\n            TypeError: If the column mapper returns an unexpected type.\n        '''\n        pass"
  }
]