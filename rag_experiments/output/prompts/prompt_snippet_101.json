[
  {
    "role": "assistant",
    "content": "You are an expert Python programmer who can correctly implement complete Python classes based on the provided class skeleton."
  },
  {
    "role": "user",
    "content": "For your understanding, following are some (skeleton, code) examples:\n\nExample skeleton:\n\n\nclass MedianAbsolutePercentageError(EpochMetric):\n    '''Calculates the Median Absolute Percentage Error.\n    .. math::\n        \\text{MdAPE} = 100 \\cdot \\text{MD}_{j=1,n} \\left( \\frac{|A_j - P_j|}{|A_j|} \\right)\n    where :math:`A_j` is the ground truth and :math:`P_j` is the predicted value.\n    More details can be found in `Botchkarev 2018`__.\n    - ``update`` must receive output of the form ``(y_pred, y)`` or ``{'y_pred': y_pred, 'y': y}``.\n    - `y` and `y_pred` must be of same shape `(N, )` or `(N, 1)` and of type `float32`.\n    .. warning::\n        Current implementation stores all input data (output and target) in as tensors before computing a metric.\n        This can potentially lead to a memory error if the input data is larger than available RAM.\n    __ https://arxiv.org/abs/1809.03006\n    Args:\n        output_transform: a callable that is used to transform the\n            :class:`~ignite.engine.engine.Engine`'s ``process_function``'s output into the\n            form expected by the metric. This can be useful if, for example, you have a multi-output model and\n            you want to compute the metric with respect to one of the outputs.\n            By default, metrics require the output as ``(y_pred, y)`` or ``{'y_pred': y_pred, 'y': y}``.\n        device: optional device specification for internal storage.\n    Examples:\n        To use with ``Engine`` and ``process_function``, simply attach the metric instance to the engine.\n        The output of the engine's ``process_function`` needs to be in format of\n        ``(y_pred, y)`` or ``{'y_pred': y_pred, 'y': y, ...}``.\n        .. include:: defaults.rst\n            :start-after: :orphan:\n        .. testcode::\n            metric = MedianAbsolutePercentageError()\n            metric.attach(default_evaluator, 'mape')\n            y_true = torch.tensor([1, 2, 3, 4, 5])\n            y_pred = y_true * 0.75\n            state = default_evaluator.run([[y_pred, y_true]])\n            print(state.metrics['mape'])\n        .. testoutput::\n            25.0...\n    '''\n\n    def __init__(self, output_transform: Callable=lambda x: x, device: Union[str, torch.device]=torch.device('cpu')):\n        pass\nCorresponding class implementation:\n\nfrom typing import Callable, Union\nfrom ignite.metrics.epoch_metric import EpochMetric\nimport torch\n\nclass MedianAbsolutePercentageError(EpochMetric):\n    \"\"\"Calculates the Median Absolute Percentage Error.\n\n    .. math::\n        \\\\text{MdAPE} = 100 \\\\cdot \\\\text{MD}_{j=1,n} \\\\left( \\\\frac{|A_j - P_j|}{|A_j|} \\\\right)\n\n    where :math:`A_j` is the ground truth and :math:`P_j` is the predicted value.\n\n    More details can be found in `Botchkarev 2018`__.\n\n    - ``update`` must receive output of the form ``(y_pred, y)`` or ``{'y_pred': y_pred, 'y': y}``.\n    - `y` and `y_pred` must be of same shape `(N, )` or `(N, 1)` and of type `float32`.\n\n    .. warning::\n\n        Current implementation stores all input data (output and target) in as tensors before computing a metric.\n        This can potentially lead to a memory error if the input data is larger than available RAM.\n\n    __ https://arxiv.org/abs/1809.03006\n\n    Args:\n        output_transform: a callable that is used to transform the\n            :class:`~ignite.engine.engine.Engine`'s ``process_function``'s output into the\n            form expected by the metric. This can be useful if, for example, you have a multi-output model and\n            you want to compute the metric with respect to one of the outputs.\n            By default, metrics require the output as ``(y_pred, y)`` or ``{'y_pred': y_pred, 'y': y}``.\n        device: optional device specification for internal storage.\n\n    Examples:\n        To use with ``Engine`` and ``process_function``, simply attach the metric instance to the engine.\n        The output of the engine's ``process_function`` needs to be in format of\n        ``(y_pred, y)`` or ``{'y_pred': y_pred, 'y': y, ...}``.\n\n        .. include:: defaults.rst\n            :start-after: :orphan:\n\n        .. testcode::\n\n            metric = MedianAbsolutePercentageError()\n            metric.attach(default_evaluator, 'mape')\n            y_true = torch.tensor([1, 2, 3, 4, 5])\n            y_pred = y_true * 0.75\n            state = default_evaluator.run([[y_pred, y_true]])\n            print(state.metrics['mape'])\n\n        .. testoutput::\n\n            25.0...\n    \"\"\"\n\n    def __init__(self, output_transform: Callable=lambda x: x, device: Union[str, torch.device]=torch.device('cpu')):\n        super(MedianAbsolutePercentageError, self).__init__(median_absolute_percentage_error_compute_fn, output_transform=output_transform, device=device)\n\nExample skeleton:\n\n\nclass HDBSCAN(BaseEstimator, ClusterMixin):\n    '''Perform HDBSCAN clustering from vector array or distance matrix.\n    HDBSCAN - Hierarchical Density-Based Spatial Clustering of Applications\n    with Noise. Performs DBSCAN over varying epsilon values and integrates\n    the result to find a clustering that gives the best stability over epsilon.\n    This allows HDBSCAN to find clusters of varying densities (unlike DBSCAN),\n    and be more robust to parameter selection.\n    Parameters\n    ----------\n    min_cluster_size : int, optional (default=5)\n        The minimum size of clusters; single linkage splits that contain\n        fewer points than this will be considered points \"falling out\" of a\n        cluster rather than a cluster splitting into two new clusters.\n    min_samples : int, optional (default=None)\n        The number of samples in a neighborhood for a point to be considered as a core point.\n        This includes the point itself. When None, defaults to min_cluster_size.\n    metric : string, or callable, optional (default='euclidean')\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string or callable, it must be one of\n        the options allowed by metrics.pairwise.pairwise_distances for its\n        metric parameter.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square.\n    p : int, optional (default=None)\n        p value to use if using the minkowski metric.\n    alpha : float, optional (default=1.0)\n        A distance scaling parameter as used in robust single linkage.\n        See [3]_ for more information.\n    cluster_selection_epsilon: float, optional (default=0.0)\n        A distance threshold. Clusters below this value will be merged.\n        This is the minimum epsilon allowed.\n        See [5]_ for more information.\n    cluster_selection_persistence: float, optional (default=0.0)\n        A persistence threshold. Clusters with a persistence lower than this\n        value will be merged.\n    algorithm : string, optional (default='best')\n        Exactly which algorithm to use; hdbscan has variants specialised\n        for different characteristics of the data. By default this is set\n        to ``best`` which chooses the \"best\" algorithm given the nature of\n        the data. You can force other options if you believe you know\n        better. Options are:\n            * ``best``\n            * ``generic``\n            * ``prims_kdtree``\n            * ``prims_balltree``\n            * ``boruvka_kdtree``\n            * ``boruvka_balltree``\n    leaf_size: int, optional (default=40)\n        If using a space tree algorithm (kdtree, or balltree) the number\n        of points ina leaf node of the tree. This does not alter the\n        resulting clustering, but may have an effect on the runtime\n        of the algorithm.\n    memory : Instance of joblib.Memory or string (optional)\n        Used to cache the output of the computation of the tree.\n        By default, no caching is done. If a string is given, it is the\n        path to the caching directory.\n    approx_min_span_tree : bool, optional (default=True)\n        Whether to accept an only approximate minimum spanning tree.\n        For some algorithms this can provide a significant speedup, but\n        the resulting clustering may be of marginally lower quality.\n        If you are willing to sacrifice speed for correctness you may want\n        to explore this; in general this should be left at the default True.\n    gen_min_span_tree: bool, optional (default=False)\n        Whether to generate the minimum spanning tree with regard\n        to mutual reachability distance for later analysis.\n    core_dist_n_jobs : int, optional (default=4)\n        Number of parallel jobs to run in core distance computations (if\n        supported by the specific algorithm). For ``core_dist_n_jobs``\n        below -1, (n_cpus + 1 + core_dist_n_jobs) are used.\n    cluster_selection_method : string, optional (default='eom')\n        The method used to select clusters from the condensed tree. The\n        standard approach for HDBSCAN* is to use an Excess of Mass algorithm\n        to find the most persistent clusters. Alternatively you can instead\n        select the clusters at the leaves of the tree -- this provides the\n        most fine grained and homogeneous clusters. Options are:\n            * ``eom``\n            * ``leaf``\n    allow_single_cluster : bool, optional (default=False)\n        By default HDBSCAN* will not produce a single cluster, setting this\n        to True will override this and allow single cluster results in\n        the case that you feel this is a valid result for your dataset.\n    prediction_data : boolean, optional\n        Whether to generate extra cached data for predicting labels or\n        membership vectors for new unseen points later. If you wish to\n        persist the clustering object for later re-use you probably want\n        to set this to True.\n        (default False)\n    branch_detection_data : boolean, optional\n        Whether to generated extra cached data for detecting branch-\n        hierarchies within clusters. If you wish to use functions from\n        ``hdbscan.branches`` set this to True. (default False)\n    match_reference_implementation : bool, optional (default=False)\n        There exist some interpretational differences between this\n        HDBSCAN* implementation and the original authors reference\n        implementation in Java. This can result in very minor differences\n        in clustering results. Setting this flag to True will, at a some\n        performance cost, ensure that the clustering results match the\n        reference implementation.\n    cluster_selection_epsilon_max: float, optional (default=inf)\n        A distance threshold. Clusters above this value will be split.\n        Has no effect when using leaf clustering (where clusters are\n        usually small regardless) and can also be overridden in rare\n        cases by a high value for cluster_selection_epsilon. Note that\n        this should not be used if we want to predict the cluster labels\n        for new points in future (e.g. using approximate_predict), as\n        the approximate_predict function is not aware of this argument.\n        This is the maximum epsilon allowed.\n    **kwargs : optional\n        Arguments passed to the distance metric\n    Attributes\n    ----------\n    labels_ : ndarray, shape (n_samples, )\n        Cluster labels for each point in the dataset given to fit().\n        Noisy samples are given the label -1.\n    probabilities_ : ndarray, shape (n_samples, )\n        The strength with which each sample is a member of its assigned\n        cluster. Noise points have probability zero; points in clusters\n        have values assigned proportional to the degree that they\n        persist as part of the cluster.\n    cluster_persistence_ : ndarray, shape (n_clusters, )\n        A score of how persistent each cluster is. A score of 1.0 represents\n        a perfectly stable cluster that persists over all distance scales,\n        while a score of 0.0 represents a perfectly ephemeral cluster. These\n        scores can be gauge the relative coherence of the clusters output\n        by the algorithm.\n    condensed_tree_ : CondensedTree object\n        The condensed tree produced by HDBSCAN. The object has methods\n        for converting to pandas, networkx, and plotting.\n    single_linkage_tree_ : SingleLinkageTree object\n        The single linkage tree produced by HDBSCAN. The object has methods\n        for converting to pandas, networkx, and plotting.\n    minimum_spanning_tree_ : MinimumSpanningTree object\n        The minimum spanning tree of the mutual reachability graph generated\n        by HDBSCAN. Note that this is not generated by default and will only\n        be available if `gen_min_span_tree` was set to True on object creation.\n        Even then in some optimized cases a tre may not be generated.\n    outlier_scores_ : ndarray, shape (n_samples, )\n        Outlier scores for clustered points; the larger the score the more\n        outlier-like the point. Useful as an outlier detection technique.\n        Based on the GLOSH algorithm by Campello, Moulavi, Zimek and Sander.\n    prediction_data_ : PredictionData object\n        Cached data used for predicting the cluster labels of new or\n        unseen points. Necessary only if you are using functions from\n        ``hdbscan.prediction`` (see\n        :func:`~hdbscan.prediction.approximate_predict`,\n        :func:`~hdbscan.prediction.membership_vector`,\n        and :func:`~hdbscan.prediction.all_points_membership_vectors`).\n    branch_detection_data_ : BranchDetectionData object\n        Cached data used for detecting branch-hierarchies within clusters.\n        Necessary only if you are using function from ``hdbscan.branches``.\n    exemplars_ : list\n        A list of exemplar points for clusters. Since HDBSCAN supports\n        arbitrary shapes for clusters we cannot provide a single cluster\n        exemplar per cluster. Instead a list is returned with each element\n        of the list being a numpy array of exemplar points for a cluster --\n        these points are the \"most representative\" points of the cluster.\n    relative_validity_ : float\n        A fast approximation of the Density Based Cluster Validity (DBCV)\n        score [4]. The only difference, and the speed, comes from the fact\n        that this relative_validity_ is computed using the mutual-\n        reachability minimum spanning tree, i.e. minimum_spanning_tree_,\n        instead of the all-points minimum spanning tree used in the\n        reference. This score might not be an objective measure of the\n        goodness of clustering. It may only be used to compare results\n        across different choices of hyper-parameters, therefore is only a\n        relative score.\n    References\n    ----------\n    .. [1] Campello, R. J., Moulavi, D., & Sander, J. (2013, April).\n       Density-based clustering based on hierarchical density estimates.\n       In Pacific-Asia Conference on Knowledge Discovery and Data Mining\n       (pp. 160-172). Springer Berlin Heidelberg.\n    .. [2] Campello, R. J., Moulavi, D., Zimek, A., & Sander, J. (2015).\n       Hierarchical density estimates for data clustering, visualization,\n       and outlier detection. ACM Transactions on Knowledge Discovery\n       from Data (TKDD), 10(1), 5.\n    .. [3] Chaudhuri, K., & Dasgupta, S. (2010). Rates of convergence for the\n       cluster tree. In Advances in Neural Information Processing Systems\n       (pp. 343-351).\n    .. [4] Moulavi, D., Jaskowiak, P.A., Campello, R.J., Zimek, A. and\n       Sander, J., 2014. Density-Based Clustering Validation. In SDM\n       (pp. 839-847).\n    .. [5] Malzer, C., & Baum, M. (2019). A Hybrid Approach To Hierarchical\n           Density-based Cluster Selection. arxiv preprint 1911.02282.\n    '''\n\n    def __init__(self, min_cluster_size=5, min_samples=None, cluster_selection_epsilon=0.0, cluster_selection_persistence=0.0, max_cluster_size=0, metric='euclidean', alpha=1.0, p=None, algorithm='best', leaf_size=40, memory=Memory(None, verbose=0), approx_min_span_tree=True, gen_min_span_tree=False, core_dist_n_jobs=4, cluster_selection_method='eom', allow_single_cluster=False, prediction_data=False, branch_detection_data=False, match_reference_implementation=False, cluster_selection_epsilon_max=float('inf'), **kwargs):\n        pass\n\n    def fit(self, X, y=None):\n        '''Perform HDBSCAN clustering from features or distance matrix.\n        Parameters\n        ----------\n        X : array or sparse (CSR) matrix of shape (n_samples, n_features), or                 array of shape (n_samples, n_samples)\n            A feature array, or array of distances between samples if\n            ``metric='precomputed'``.\n        Returns\n        -------\n        self : object\n            Returns self\n        '''\n        pass\n\n    def fit_predict(self, X, y=None):\n        '''Performs clustering on X and returns cluster labels.\n        Parameters\n        ----------\n        X : array or sparse (CSR) matrix of shape (n_samples, n_features), or                 array of shape (n_samples, n_samples)\n            A feature array, or array of distances between samples if\n            ``metric='precomputed'``.\n        Returns\n        -------\n        y : ndarray, shape (n_samples, )\n            cluster labels\n        '''\n        pass\n\n    def generate_prediction_data(self):\n        '''\n        Create data that caches intermediate results used for predicting\n        the label of new/unseen points. This data is only useful if\n        you are intending to use functions from ``hdbscan.prediction``.\n        '''\n        pass\n\n    def generate_branch_detection_data(self):\n        '''\n        Create data that caches intermediate results used for detecting\n        branches within clusters. This data is only useful if you are\n        intending to use functions from ``hdbscan.branches``.\n        '''\n        pass\n\n    def weighted_cluster_centroid(self, cluster_id):\n        '''Provide an approximate representative point for a given cluster.\n        Note that this technique assumes a euclidean metric for speed of\n        computation. For more general metrics use the ``weighted_cluster_medoid``\n        method which is slower, but can work with the metric the model trained\n        with.\n        Parameters\n        ----------\n        cluster_id: int\n            The id of the cluster to compute a centroid for.\n        Returns\n        -------\n        centroid: array of shape (n_features,)\n            A representative centroid for cluster ``cluster_id``.\n        '''\n        pass\n\n    def weighted_cluster_medoid(self, cluster_id):\n        '''Provide an approximate representative point for a given cluster.\n        Note that this technique can be very slow and memory intensive for\n        large clusters. For faster results use the ``weighted_cluster_centroid``\n        method which is faster, but assumes a euclidean metric.\n        Parameters\n        ----------\n        cluster_id: int\n            The id of the cluster to compute a medoid for.\n        Returns\n        -------\n        centroid: array of shape (n_features,)\n            A representative medoid for cluster ``cluster_id``.\n        '''\n        pass\n\n    def dbscan_clustering(self, cut_distance, min_cluster_size=5):\n        '''Return clustering that would be equivalent to running DBSCAN* for a particular cut_distance (or epsilon)\n        DBSCAN* can be thought of as DBSCAN without the border points.  As such these results may differ slightly\n        from sklearns implementation of dbscan in the non-core points.\n        This can also be thought of as a flat clustering derived from constant height cut through the single\n        linkage tree.\n        This represents the result of selecting a cut value for robust single linkage\n        clustering. The `min_cluster_size` allows the flat clustering to declare noise\n        points (and cluster smaller than `min_cluster_size`).\n        Parameters\n        ----------\n        cut_distance : float\n            The mutual reachability distance cut value to use to generate a flat clustering.\n        min_cluster_size : int, optional\n            Clusters smaller than this value with be called 'noise' and remain unclustered\n            in the resulting flat clustering.\n        Returns\n        -------\n        labels : array [n_samples]\n            An array of cluster labels, one per datapoint. Unclustered points are assigned\n            the label -1.\n        '''\n        pass\n    @property\n    def prediction_data_(self):\n        pass\n    @prediction_data_.setter\n    def prediction_data_(self):\n        pass\n    @property\n    def branch_detection_data_(self):\n        pass\n    @branch_detection_data_.setter\n    def branch_detection_data_(self):\n        pass\n    @property\n    def outlier_scores_(self):\n        pass\n    @outlier_scores_.setter\n    def outlier_scores_(self):\n        pass\n    @property\n    def condensed_tree_(self):\n        pass\n    @condensed_tree_.setter\n    def condensed_tree_(self):\n        pass\n    @property\n    def single_linkage_tree_(self):\n        pass\n    @single_linkage_tree_.setter\n    def single_linkage_tree_(self):\n        pass\n    @property\n    def minimum_spanning_tree_(self):\n        pass\n    @minimum_spanning_tree_.setter\n    def minimum_spanning_tree_(self):\n        pass\n    @property\n    def exemplars_(self):\n        pass\n    @exemplars_.setter\n    def exemplars_(self):\n        pass\n    @property\n    def relative_validity_(self):\n        pass\n    @relative_validity_.setter\n    def relative_validity_(self):\n        pass\nCorresponding class implementation:\n\nfrom sklearn.utils import check_array\nfrom sklearn.base import BaseEstimator, ClusterMixin\nimport numpy as np\nfrom .plots import CondensedTree, SingleLinkageTree, MinimumSpanningTree\nfrom warnings import warn\nfrom joblib import Memory\nfrom ._hdbscan_tree import condense_tree, compute_stability, get_clusters, outlier_scores, simplify_hierarchy\nfrom scipy.sparse import issparse\nfrom .branch_data import BranchDetectionData\nfrom sklearn.metrics import pairwise_distances\nfrom .prediction import PredictionData\n\nclass HDBSCAN(BaseEstimator, ClusterMixin):\n    \"\"\"Perform HDBSCAN clustering from vector array or distance matrix.\n\n    HDBSCAN - Hierarchical Density-Based Spatial Clustering of Applications\n    with Noise. Performs DBSCAN over varying epsilon values and integrates\n    the result to find a clustering that gives the best stability over epsilon.\n    This allows HDBSCAN to find clusters of varying densities (unlike DBSCAN),\n    and be more robust to parameter selection.\n\n    Parameters\n    ----------\n    min_cluster_size : int, optional (default=5)\n        The minimum size of clusters; single linkage splits that contain\n        fewer points than this will be considered points \"falling out\" of a\n        cluster rather than a cluster splitting into two new clusters.\n\n    min_samples : int, optional (default=None)\n        The number of samples in a neighborhood for a point to be considered as a core point.\n        This includes the point itself. When None, defaults to min_cluster_size.\n\n    metric : string, or callable, optional (default='euclidean')\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string or callable, it must be one of\n        the options allowed by metrics.pairwise.pairwise_distances for its\n        metric parameter.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square.\n\n    p : int, optional (default=None)\n        p value to use if using the minkowski metric.\n\n    alpha : float, optional (default=1.0)\n        A distance scaling parameter as used in robust single linkage.\n        See [3]_ for more information.\n\n    cluster_selection_epsilon: float, optional (default=0.0)\n        A distance threshold. Clusters below this value will be merged.\n        This is the minimum epsilon allowed.\n        See [5]_ for more information.\n\n    cluster_selection_persistence: float, optional (default=0.0)\n        A persistence threshold. Clusters with a persistence lower than this\n        value will be merged.\n\n    algorithm : string, optional (default='best')\n        Exactly which algorithm to use; hdbscan has variants specialised\n        for different characteristics of the data. By default this is set\n        to ``best`` which chooses the \"best\" algorithm given the nature of\n        the data. You can force other options if you believe you know\n        better. Options are:\n            * ``best``\n            * ``generic``\n            * ``prims_kdtree``\n            * ``prims_balltree``\n            * ``boruvka_kdtree``\n            * ``boruvka_balltree``\n\n    leaf_size: int, optional (default=40)\n        If using a space tree algorithm (kdtree, or balltree) the number\n        of points ina leaf node of the tree. This does not alter the\n        resulting clustering, but may have an effect on the runtime\n        of the algorithm.\n\n    memory : Instance of joblib.Memory or string (optional)\n        Used to cache the output of the computation of the tree.\n        By default, no caching is done. If a string is given, it is the\n        path to the caching directory.\n\n    approx_min_span_tree : bool, optional (default=True)\n        Whether to accept an only approximate minimum spanning tree.\n        For some algorithms this can provide a significant speedup, but\n        the resulting clustering may be of marginally lower quality.\n        If you are willing to sacrifice speed for correctness you may want\n        to explore this; in general this should be left at the default True.\n\n    gen_min_span_tree: bool, optional (default=False)\n        Whether to generate the minimum spanning tree with regard\n        to mutual reachability distance for later analysis.\n\n    core_dist_n_jobs : int, optional (default=4)\n        Number of parallel jobs to run in core distance computations (if\n        supported by the specific algorithm). For ``core_dist_n_jobs``\n        below -1, (n_cpus + 1 + core_dist_n_jobs) are used.\n\n    cluster_selection_method : string, optional (default='eom')\n        The method used to select clusters from the condensed tree. The\n        standard approach for HDBSCAN* is to use an Excess of Mass algorithm\n        to find the most persistent clusters. Alternatively you can instead\n        select the clusters at the leaves of the tree -- this provides the\n        most fine grained and homogeneous clusters. Options are:\n            * ``eom``\n            * ``leaf``\n\n    allow_single_cluster : bool, optional (default=False)\n        By default HDBSCAN* will not produce a single cluster, setting this\n        to True will override this and allow single cluster results in\n        the case that you feel this is a valid result for your dataset.\n\n    prediction_data : boolean, optional\n        Whether to generate extra cached data for predicting labels or\n        membership vectors for new unseen points later. If you wish to\n        persist the clustering object for later re-use you probably want\n        to set this to True.\n        (default False)\n\n    branch_detection_data : boolean, optional\n        Whether to generated extra cached data for detecting branch-\n        hierarchies within clusters. If you wish to use functions from\n        ``hdbscan.branches`` set this to True. (default False)\n\n    match_reference_implementation : bool, optional (default=False)\n        There exist some interpretational differences between this\n        HDBSCAN* implementation and the original authors reference\n        implementation in Java. This can result in very minor differences\n        in clustering results. Setting this flag to True will, at a some\n        performance cost, ensure that the clustering results match the\n        reference implementation.\n\n    cluster_selection_epsilon_max: float, optional (default=inf)\n        A distance threshold. Clusters above this value will be split.\n        Has no effect when using leaf clustering (where clusters are\n        usually small regardless) and can also be overridden in rare\n        cases by a high value for cluster_selection_epsilon. Note that\n        this should not be used if we want to predict the cluster labels\n        for new points in future (e.g. using approximate_predict), as\n        the approximate_predict function is not aware of this argument.\n        This is the maximum epsilon allowed.\n\n    **kwargs : optional\n        Arguments passed to the distance metric\n\n    Attributes\n    ----------\n    labels_ : ndarray, shape (n_samples, )\n        Cluster labels for each point in the dataset given to fit().\n        Noisy samples are given the label -1.\n\n    probabilities_ : ndarray, shape (n_samples, )\n        The strength with which each sample is a member of its assigned\n        cluster. Noise points have probability zero; points in clusters\n        have values assigned proportional to the degree that they\n        persist as part of the cluster.\n\n    cluster_persistence_ : ndarray, shape (n_clusters, )\n        A score of how persistent each cluster is. A score of 1.0 represents\n        a perfectly stable cluster that persists over all distance scales,\n        while a score of 0.0 represents a perfectly ephemeral cluster. These\n        scores can be gauge the relative coherence of the clusters output\n        by the algorithm.\n\n    condensed_tree_ : CondensedTree object\n        The condensed tree produced by HDBSCAN. The object has methods\n        for converting to pandas, networkx, and plotting.\n\n    single_linkage_tree_ : SingleLinkageTree object\n        The single linkage tree produced by HDBSCAN. The object has methods\n        for converting to pandas, networkx, and plotting.\n\n    minimum_spanning_tree_ : MinimumSpanningTree object\n        The minimum spanning tree of the mutual reachability graph generated\n        by HDBSCAN. Note that this is not generated by default and will only\n        be available if `gen_min_span_tree` was set to True on object creation.\n        Even then in some optimized cases a tre may not be generated.\n\n    outlier_scores_ : ndarray, shape (n_samples, )\n        Outlier scores for clustered points; the larger the score the more\n        outlier-like the point. Useful as an outlier detection technique.\n        Based on the GLOSH algorithm by Campello, Moulavi, Zimek and Sander.\n\n    prediction_data_ : PredictionData object\n        Cached data used for predicting the cluster labels of new or\n        unseen points. Necessary only if you are using functions from\n        ``hdbscan.prediction`` (see\n        :func:`~hdbscan.prediction.approximate_predict`,\n        :func:`~hdbscan.prediction.membership_vector`,\n        and :func:`~hdbscan.prediction.all_points_membership_vectors`).\n\n    branch_detection_data_ : BranchDetectionData object\n        Cached data used for detecting branch-hierarchies within clusters.\n        Necessary only if you are using function from ``hdbscan.branches``.\n\n    exemplars_ : list\n        A list of exemplar points for clusters. Since HDBSCAN supports\n        arbitrary shapes for clusters we cannot provide a single cluster\n        exemplar per cluster. Instead a list is returned with each element\n        of the list being a numpy array of exemplar points for a cluster --\n        these points are the \"most representative\" points of the cluster.\n\n    relative_validity_ : float\n        A fast approximation of the Density Based Cluster Validity (DBCV)\n        score [4]. The only difference, and the speed, comes from the fact\n        that this relative_validity_ is computed using the mutual-\n        reachability minimum spanning tree, i.e. minimum_spanning_tree_,\n        instead of the all-points minimum spanning tree used in the\n        reference. This score might not be an objective measure of the\n        goodness of clustering. It may only be used to compare results\n        across different choices of hyper-parameters, therefore is only a\n        relative score.\n\n    References\n    ----------\n\n    .. [1] Campello, R. J., Moulavi, D., & Sander, J. (2013, April).\n       Density-based clustering based on hierarchical density estimates.\n       In Pacific-Asia Conference on Knowledge Discovery and Data Mining\n       (pp. 160-172). Springer Berlin Heidelberg.\n\n    .. [2] Campello, R. J., Moulavi, D., Zimek, A., & Sander, J. (2015).\n       Hierarchical density estimates for data clustering, visualization,\n       and outlier detection. ACM Transactions on Knowledge Discovery\n       from Data (TKDD), 10(1), 5.\n\n    .. [3] Chaudhuri, K., & Dasgupta, S. (2010). Rates of convergence for the\n       cluster tree. In Advances in Neural Information Processing Systems\n       (pp. 343-351).\n\n    .. [4] Moulavi, D., Jaskowiak, P.A., Campello, R.J., Zimek, A. and\n       Sander, J., 2014. Density-Based Clustering Validation. In SDM\n       (pp. 839-847).\n\n    .. [5] Malzer, C., & Baum, M. (2019). A Hybrid Approach To Hierarchical\n           Density-based Cluster Selection. arxiv preprint 1911.02282.\n\n    \"\"\"\n\n    def __init__(self, min_cluster_size=5, min_samples=None, cluster_selection_epsilon=0.0, cluster_selection_persistence=0.0, max_cluster_size=0, metric='euclidean', alpha=1.0, p=None, algorithm='best', leaf_size=40, memory=Memory(None, verbose=0), approx_min_span_tree=True, gen_min_span_tree=False, core_dist_n_jobs=4, cluster_selection_method='eom', allow_single_cluster=False, prediction_data=False, branch_detection_data=False, match_reference_implementation=False, cluster_selection_epsilon_max=float('inf'), **kwargs):\n        self.min_cluster_size = min_cluster_size\n        self.min_samples = min_samples\n        self.alpha = alpha\n        self.max_cluster_size = max_cluster_size\n        self.cluster_selection_epsilon = cluster_selection_epsilon\n        self.cluster_selection_persistence = cluster_selection_persistence\n        self.metric = metric\n        self.p = p\n        self.algorithm = algorithm\n        self.leaf_size = leaf_size\n        self.memory = memory\n        self.approx_min_span_tree = approx_min_span_tree\n        self.gen_min_span_tree = gen_min_span_tree\n        self.core_dist_n_jobs = core_dist_n_jobs\n        self.cluster_selection_method = cluster_selection_method\n        self.allow_single_cluster = allow_single_cluster\n        self.match_reference_implementation = match_reference_implementation\n        self.prediction_data = prediction_data\n        self.branch_detection_data = branch_detection_data\n        self.cluster_selection_epsilon_max = cluster_selection_epsilon_max\n        self._metric_kwargs = kwargs\n        self._condensed_tree = None\n        self._single_linkage_tree = None\n        self._min_spanning_tree = None\n        self._raw_data = None\n        self._outlier_scores = None\n        self._prediction_data = None\n        self._finite_index = None\n        self._branch_detection_data = None\n        self._relative_validity = None\n\n    def fit(self, X, y=None):\n        \"\"\"Perform HDBSCAN clustering from features or distance matrix.\n\n        Parameters\n        ----------\n        X : array or sparse (CSR) matrix of shape (n_samples, n_features), or                 array of shape (n_samples, n_samples)\n            A feature array, or array of distances between samples if\n            ``metric='precomputed'``.\n\n        Returns\n        -------\n        self : object\n            Returns self\n        \"\"\"\n        if self.metric != 'precomputed':\n            X = check_array(X, accept_sparse='csr', force_all_finite=False)\n            self._raw_data = X\n            self._all_finite = is_finite(X)\n            if ~self._all_finite:\n                finite_index = get_finite_row_indices(X)\n                clean_data = X[finite_index]\n                internal_to_raw = {x: y for x, y in zip(range(len(finite_index)), finite_index)}\n                outliers = list(set(range(X.shape[0])) - set(finite_index))\n            else:\n                clean_data = X\n        elif issparse(X):\n            X = check_array(X, accept_sparse='csr')\n            clean_data = X\n        else:\n            check_precomputed_distance_matrix(X)\n            clean_data = X\n        kwargs = self.get_params()\n        kwargs.pop('prediction_data', None)\n        kwargs.pop('branch_detection_data', None)\n        kwargs.update(self._metric_kwargs)\n        kwargs['gen_min_span_tree'] |= self.branch_detection_data\n        self.labels_, self.probabilities_, self.cluster_persistence_, self._condensed_tree, self._single_linkage_tree, self._min_spanning_tree = hdbscan(clean_data, **kwargs)\n        if self.metric != 'precomputed' and (not self._all_finite):\n            self._condensed_tree = remap_condensed_tree(self._condensed_tree, internal_to_raw, outliers)\n            self._single_linkage_tree = remap_single_linkage_tree(self._single_linkage_tree, internal_to_raw, outliers)\n            new_labels = np.full(X.shape[0], -1)\n            new_labels[finite_index] = self.labels_\n            self.labels_ = new_labels\n            new_probabilities = np.zeros(X.shape[0])\n            new_probabilities[finite_index] = self.probabilities_\n            self.probabilities_ = new_probabilities\n        if self.prediction_data:\n            self.generate_prediction_data()\n        if self.branch_detection_data:\n            self.generate_branch_detection_data()\n        return self\n\n    def fit_predict(self, X, y=None):\n        \"\"\"Performs clustering on X and returns cluster labels.\n\n        Parameters\n        ----------\n        X : array or sparse (CSR) matrix of shape (n_samples, n_features), or                 array of shape (n_samples, n_samples)\n            A feature array, or array of distances between samples if\n            ``metric='precomputed'``.\n\n        Returns\n        -------\n        y : ndarray, shape (n_samples, )\n            cluster labels\n        \"\"\"\n        self.fit(X)\n        return self.labels_\n\n    def generate_prediction_data(self):\n        \"\"\"\n        Create data that caches intermediate results used for predicting\n        the label of new/unseen points. This data is only useful if\n        you are intending to use functions from ``hdbscan.prediction``.\n        \"\"\"\n        if self.metric in FAST_METRICS:\n            min_samples = self.min_samples or self.min_cluster_size\n            if self.metric in KDTREE_VALID_METRICS:\n                tree_type = 'kdtree'\n            elif self.metric in BALLTREE_VALID_METRICS:\n                tree_type = 'balltree'\n            else:\n                warn('Metric {} not supported for prediction data!'.format(self.metric))\n                return\n            self._prediction_data = PredictionData(self._raw_data, self.condensed_tree_, min_samples, tree_type=tree_type, metric=self.metric, **self._metric_kwargs)\n        else:\n            warn('Cannot generate prediction data for non-vector space inputs -- access to the source data rather than mere distances is required!')\n\n    def generate_branch_detection_data(self):\n        \"\"\"\n        Create data that caches intermediate results used for detecting\n        branches within clusters. This data is only useful if you are\n        intending to use functions from ``hdbscan.branches``.\n        \"\"\"\n        if self.metric in FAST_METRICS:\n            min_samples = self.min_samples or self.min_cluster_size\n            if self.metric in KDTREE_VALID_METRICS:\n                tree_type = 'kdtree'\n            elif self.metric in BALLTREE_VALID_METRICS:\n                tree_type = 'balltree'\n            else:\n                warn('Metric {} not supported for branch detection!'.format(self.metric))\n                return\n            self._branch_detection_data = BranchDetectionData(self._raw_data, self.labels_, self._condensed_tree, min_samples, tree_type=tree_type, metric=self.metric, **self._metric_kwargs)\n        else:\n            warn('Branch detection for non-vector space inputs is not (yet) implemented.')\n\n    def weighted_cluster_centroid(self, cluster_id):\n        \"\"\"Provide an approximate representative point for a given cluster.\n        Note that this technique assumes a euclidean metric for speed of\n        computation. For more general metrics use the ``weighted_cluster_medoid``\n        method which is slower, but can work with the metric the model trained\n        with.\n\n        Parameters\n        ----------\n        cluster_id: int\n            The id of the cluster to compute a centroid for.\n\n        Returns\n        -------\n        centroid: array of shape (n_features,)\n            A representative centroid for cluster ``cluster_id``.\n        \"\"\"\n        if not hasattr(self, 'labels_'):\n            raise AttributeError('Model has not been fit to data')\n        if cluster_id == -1:\n            raise ValueError('Cannot calculate weighted centroid for -1 cluster since it is a noise cluster')\n        mask = self.labels_ == cluster_id\n        cluster_data = self._raw_data[mask]\n        cluster_membership_strengths = self.probabilities_[mask]\n        return np.average(cluster_data, weights=cluster_membership_strengths, axis=0)\n\n    def weighted_cluster_medoid(self, cluster_id):\n        \"\"\"Provide an approximate representative point for a given cluster.\n        Note that this technique can be very slow and memory intensive for\n        large clusters. For faster results use the ``weighted_cluster_centroid``\n        method which is faster, but assumes a euclidean metric.\n\n        Parameters\n        ----------\n        cluster_id: int\n            The id of the cluster to compute a medoid for.\n\n        Returns\n        -------\n        centroid: array of shape (n_features,)\n            A representative medoid for cluster ``cluster_id``.\n        \"\"\"\n        if not hasattr(self, 'labels_'):\n            raise AttributeError('Model has not been fit to data')\n        if cluster_id == -1:\n            raise ValueError('Cannot calculate weighted centroid for -1 cluster since it is a noise cluster')\n        mask = self.labels_ == cluster_id\n        cluster_data = self._raw_data[mask]\n        cluster_membership_strengths = self.probabilities_[mask]\n        dist_mat = pairwise_distances(cluster_data, metric=self.metric, **self._metric_kwargs)\n        dist_mat = dist_mat * cluster_membership_strengths\n        medoid_index = np.argmin(dist_mat.sum(axis=1))\n        return cluster_data[medoid_index]\n\n    def dbscan_clustering(self, cut_distance, min_cluster_size=5):\n        \"\"\"Return clustering that would be equivalent to running DBSCAN* for a particular cut_distance (or epsilon)\n        DBSCAN* can be thought of as DBSCAN without the border points.  As such these results may differ slightly\n        from sklearns implementation of dbscan in the non-core points.\n\n        This can also be thought of as a flat clustering derived from constant height cut through the single\n        linkage tree.\n\n        This represents the result of selecting a cut value for robust single linkage\n        clustering. The `min_cluster_size` allows the flat clustering to declare noise\n        points (and cluster smaller than `min_cluster_size`).\n\n        Parameters\n        ----------\n\n        cut_distance : float\n            The mutual reachability distance cut value to use to generate a flat clustering.\n\n        min_cluster_size : int, optional\n            Clusters smaller than this value with be called 'noise' and remain unclustered\n            in the resulting flat clustering.\n\n        Returns\n        -------\n\n        labels : array [n_samples]\n            An array of cluster labels, one per datapoint. Unclustered points are assigned\n            the label -1.\n        \"\"\"\n        return self.single_linkage_tree_.get_clusters(cut_distance=cut_distance, min_cluster_size=min_cluster_size)\n\n    @property\n    def prediction_data_(self):\n        if self._prediction_data is None:\n            raise AttributeError('No prediction data was generated')\n        else:\n            return self._prediction_data\n\n    @prediction_data_.setter\n    def prediction_data_(self, value):\n        self._prediction_data = value\n\n    @property\n    def branch_detection_data_(self):\n        if self._branch_detection_data is None:\n            raise AttributeError('No branch detection data was generated')\n        else:\n            return self._branch_detection_data\n\n    @branch_detection_data_.setter\n    def branch_detection_data_(self, value):\n        self._branch_detection_data = value\n\n    @property\n    def outlier_scores_(self):\n        if self._outlier_scores is not None:\n            return self._outlier_scores\n        elif self._condensed_tree is not None:\n            self._outlier_scores = outlier_scores(self._condensed_tree)\n            return self._outlier_scores\n        else:\n            raise AttributeError('No condensed tree was generated; try running fit first.')\n\n    @outlier_scores_.setter\n    def outlier_scores_(self, value):\n        self._outlier_scores = value\n\n    @property\n    def condensed_tree_(self):\n        if self._condensed_tree is not None:\n            return CondensedTree(self._condensed_tree, self.labels_)\n        else:\n            raise AttributeError('No condensed tree was generated; try running fit first.')\n\n    @condensed_tree_.setter\n    def condensed_tree_(self, value):\n        self._condensed_tree = value\n\n    @property\n    def single_linkage_tree_(self):\n        if self._single_linkage_tree is not None:\n            return SingleLinkageTree(self._single_linkage_tree)\n        else:\n            raise AttributeError('No single linkage tree was generated; try running fit first.')\n\n    @single_linkage_tree_.setter\n    def single_linkage_tree_(self, value):\n        self._single_linkage_tree = value\n\n    @property\n    def minimum_spanning_tree_(self):\n        if self._min_spanning_tree is not None:\n            if self._raw_data is not None:\n                return MinimumSpanningTree(self._min_spanning_tree, self._raw_data)\n            else:\n                warn('No raw data is available; this may be due to using a precomputed metric matrix. No minimum spanning tree will be provided without raw data.')\n                return None\n        else:\n            raise AttributeError('No minimum spanning tree was generated.This may be due to optimized algorithm variations that skip explicit generation of the spanning tree.')\n\n    @minimum_spanning_tree_.setter\n    def minimum_spanning_tree_(self, value):\n        self._min_spanning_tree = value\n\n    @property\n    def exemplars_(self):\n        if self._prediction_data is not None:\n            return self._prediction_data.exemplars\n        elif self.metric in FAST_METRICS:\n            self.generate_prediction_data()\n            return self._prediction_data.exemplars\n        else:\n            raise AttributeError('Currently exemplars require the use of vector input datawith a suitable metric. This will likely change in the future, but for now no exemplars can be provided')\n\n    @exemplars_.setter\n    def exemplars_(self, value):\n        self._exemplars = value\n\n    @property\n    def relative_validity_(self):\n        if self._relative_validity is not None:\n            return self._relative_validity\n        if not self.gen_min_span_tree:\n            raise AttributeError('Minimum spanning tree not present. ' + 'Either HDBSCAN object was created with ' + 'gen_min_span_tree=False or the tree was ' + 'not generated in spite of it owing to ' + 'internal optimization criteria.')\n            return\n        labels = self.labels_\n        sizes = np.bincount(labels + 1)\n        noise_size = sizes[0]\n        cluster_size = sizes[1:]\n        total = noise_size + np.sum(cluster_size)\n        num_clusters = len(cluster_size)\n        DSC = np.zeros(num_clusters)\n        min_outlier_sep = np.inf\n        correction_const = 2\n        DSPC_wrt = np.ones(num_clusters) * np.inf\n        max_distance = 0\n        mst_df = self.minimum_spanning_tree_.to_pandas()\n        for edge in mst_df.iterrows():\n            label1 = labels[int(edge[1]['from'])]\n            label2 = labels[int(edge[1]['to'])]\n            length = edge[1]['distance']\n            max_distance = max(max_distance, length)\n            if label1 == -1 and label2 == -1:\n                continue\n            elif label1 == -1 or label2 == -1:\n                min_outlier_sep = min(min_outlier_sep, length)\n                continue\n            if label1 == label2:\n                DSC[label1] = max(length, DSC[label1])\n            else:\n                DSPC_wrt[label1] = min(length, DSPC_wrt[label1])\n                DSPC_wrt[label2] = min(length, DSPC_wrt[label2])\n        min_outlier_sep = max_distance if min_outlier_sep == np.inf else min_outlier_sep\n        correction = correction_const * (max_distance if num_clusters > 1 else min_outlier_sep)\n        DSPC_wrt[np.where(DSPC_wrt == np.inf)] = correction\n        V_index = [(DSPC_wrt[i] - DSC[i]) / max(DSPC_wrt[i], DSC[i]) for i in range(num_clusters)]\n        score = np.sum([cluster_size[i] * V_index[i] / total for i in range(num_clusters)])\n        self._relative_validity = score\n        return self._relative_validity\n\n    @relative_validity_.setter\n    def relative_validity_(self, value):\n        self._relative_validity = value\n\nNow, implement the following class. Do not explain the code. The given class skeleton is as follows:\n\nclass KalmanFilterXYAH:\n    '''\n    A KalmanFilterXYAH class for tracking bounding boxes in image space using a Kalman filter.\n    Implements a simple Kalman filter for tracking bounding boxes in image space. The 8-dimensional state space\n    (x, y, a, h, vx, vy, va, vh) contains the bounding box center position (x, y), aspect ratio a, height h, and their\n    respective velocities. Object motion follows a constant velocity model, and bounding box location (x, y, a, h) is\n    taken as a direct observation of the state space (linear observation model).\n    Attributes:\n        _motion_mat (np.ndarray): The motion matrix for the Kalman filter.\n        _update_mat (np.ndarray): The update matrix for the Kalman filter.\n        _std_weight_position (float): Standard deviation weight for position.\n        _std_weight_velocity (float): Standard deviation weight for velocity.\n    Methods:\n        initiate: Creates a track from an unassociated measurement.\n        predict: Runs the Kalman filter prediction step.\n        project: Projects the state distribution to measurement space.\n        multi_predict: Runs the Kalman filter prediction step (vectorized version).\n        update: Runs the Kalman filter correction step.\n        gating_distance: Computes the gating distance between state distribution and measurements.\n    Examples:\n        Initialize the Kalman filter and create a track from a measurement\n        >>> kf = KalmanFilterXYAH()\n        >>> measurement = np.array([100, 200, 1.5, 50])\n        >>> mean, covariance = kf.initiate(measurement)\n        >>> print(mean)\n        >>> print(covariance)\n    '''\n\n    def __init__(self):\n        '''\n        Initialize Kalman filter model matrices with motion and observation uncertainty weights.\n        The Kalman filter is initialized with an 8-dimensional state space (x, y, a, h, vx, vy, va, vh), where (x, y)\n        represents the bounding box center position, 'a' is the aspect ratio, 'h' is the height, and their respective\n        velocities are (vx, vy, va, vh). The filter uses a constant velocity model for object motion and a linear\n        observation model for bounding box location.\n        Examples:\n            Initialize a Kalman filter for tracking:\n            >>> kf = KalmanFilterXYAH()\n        '''\n        pass\n\n    def initiate(self, measurement: np.ndarray) -> tuple:\n        '''\n        Create a track from an unassociated measurement.\n        Args:\n            measurement (ndarray): Bounding box coordinates (x, y, a, h) with center position (x, y), aspect ratio a,\n                and height h.\n        Returns:\n            (tuple[ndarray, ndarray]): Returns the mean vector (8-dimensional) and covariance matrix (8x8 dimensional)\n                of the new track. Unobserved velocities are initialized to 0 mean.\n        Examples:\n            >>> kf = KalmanFilterXYAH()\n            >>> measurement = np.array([100, 50, 1.5, 200])\n            >>> mean, covariance = kf.initiate(measurement)\n        '''\n        pass\n\n    def predict(self, mean: np.ndarray, covariance: np.ndarray) -> tuple:\n        '''\n        Run Kalman filter prediction step.\n        Args:\n            mean (ndarray): The 8-dimensional mean vector of the object state at the previous time step.\n            covariance (ndarray): The 8x8-dimensional covariance matrix of the object state at the previous time step.\n        Returns:\n            (tuple[ndarray, ndarray]): Returns the mean vector and covariance matrix of the predicted state. Unobserved\n                velocities are initialized to 0 mean.\n        Examples:\n            >>> kf = KalmanFilterXYAH()\n            >>> mean = np.array([0, 0, 1, 1, 0, 0, 0, 0])\n            >>> covariance = np.eye(8)\n            >>> predicted_mean, predicted_covariance = kf.predict(mean, covariance)\n        '''\n        pass\n\n    def project(self, mean: np.ndarray, covariance: np.ndarray) -> tuple:\n        '''\n        Project state distribution to measurement space.\n        Args:\n            mean (ndarray): The state's mean vector (8 dimensional array).\n            covariance (ndarray): The state's covariance matrix (8x8 dimensional).\n        Returns:\n            (tuple[ndarray, ndarray]): Returns the projected mean and covariance matrix of the given state estimate.\n        Examples:\n            >>> kf = KalmanFilterXYAH()\n            >>> mean = np.array([0, 0, 1, 1, 0, 0, 0, 0])\n            >>> covariance = np.eye(8)\n            >>> projected_mean, projected_covariance = kf.project(mean, covariance)\n        '''\n        pass\n\n    def multi_predict(self, mean: np.ndarray, covariance: np.ndarray) -> tuple:\n        '''\n        Run Kalman filter prediction step for multiple object states (Vectorized version).\n        Args:\n            mean (ndarray): The Nx8 dimensional mean matrix of the object states at the previous time step.\n            covariance (ndarray): The Nx8x8 covariance matrix of the object states at the previous time step.\n        Returns:\n            (tuple[ndarray, ndarray]): Returns the mean matrix and covariance matrix of the predicted states.\n                The mean matrix has shape (N, 8) and the covariance matrix has shape (N, 8, 8). Unobserved velocities\n                are initialized to 0 mean.\n        Examples:\n            >>> mean = np.random.rand(10, 8)  # 10 object states\n            >>> covariance = np.random.rand(10, 8, 8)  # Covariance matrices for 10 object states\n            >>> predicted_mean, predicted_covariance = kalman_filter.multi_predict(mean, covariance)\n        '''\n        pass\n\n    def update(self, mean: np.ndarray, covariance: np.ndarray, measurement: np.ndarray) -> tuple:\n        '''\n        Run Kalman filter correction step.\n        Args:\n            mean (ndarray): The predicted state's mean vector (8 dimensional).\n            covariance (ndarray): The state's covariance matrix (8x8 dimensional).\n            measurement (ndarray): The 4 dimensional measurement vector (x, y, a, h), where (x, y) is the center\n                position, a the aspect ratio, and h the height of the bounding box.\n        Returns:\n            (tuple[ndarray, ndarray]): Returns the measurement-corrected state distribution.\n        Examples:\n            >>> kf = KalmanFilterXYAH()\n            >>> mean = np.array([0, 0, 1, 1, 0, 0, 0, 0])\n            >>> covariance = np.eye(8)\n            >>> measurement = np.array([1, 1, 1, 1])\n            >>> new_mean, new_covariance = kf.update(mean, covariance, measurement)\n        '''\n        pass\n\n    def gating_distance(self, mean: np.ndarray, covariance: np.ndarray, measurements: np.ndarray, only_position: bool=False, metric: str='maha') -> np.ndarray:\n        '''\n        Compute gating distance between state distribution and measurements.\n        A suitable distance threshold can be obtained from `chi2inv95`. If `only_position` is False, the chi-square\n        distribution has 4 degrees of freedom, otherwise 2.\n        Args:\n            mean (ndarray): Mean vector over the state distribution (8 dimensional).\n            covariance (ndarray): Covariance of the state distribution (8x8 dimensional).\n            measurements (ndarray): An (N, 4) matrix of N measurements, each in format (x, y, a, h) where (x, y) is the\n                bounding box center position, a the aspect ratio, and h the height.\n            only_position (bool): If True, distance computation is done with respect to box center position only.\n            metric (str): The metric to use for calculating the distance. Options are 'gaussian' for the squared\n                Euclidean distance and 'maha' for the squared Mahalanobis distance.\n        Returns:\n            (np.ndarray): Returns an array of length N, where the i-th element contains the squared distance between\n                (mean, covariance) and `measurements[i]`.\n        Examples:\n            Compute gating distance using Mahalanobis metric:\n            >>> kf = KalmanFilterXYAH()\n            >>> mean = np.array([0, 0, 1, 1, 0, 0, 0, 0])\n            >>> covariance = np.eye(8)\n            >>> measurements = np.array([[1, 1, 1, 1], [2, 2, 1, 1]])\n            >>> distances = kf.gating_distance(mean, covariance, measurements, only_position=False, metric=\"maha\")\n        '''\n        pass"
  }
]