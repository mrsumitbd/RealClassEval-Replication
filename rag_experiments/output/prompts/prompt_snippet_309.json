[
  {
    "role": "assistant",
    "content": "You are an expert Python programmer who can correctly implement complete Python classes based on the provided class skeleton."
  },
  {
    "role": "user",
    "content": "For your understanding, following are some (skeleton, code) examples:\n\nExample skeleton:\n\n\nclass RefLog(List[RefLogEntry], Serializable):\n    '''A reflog contains :class:`RefLogEntry`\\s, each of which defines a certain state\n    of the head in question. Custom query methods allow to retrieve log entries by date\n    or by other criteria.\n    Reflog entries are ordered. The first added entry is first in the list. The last\n    entry, i.e. the last change of the head or reference, is last in the list.\n    '''\n\n    def __new__(cls, filepath: Union[PathLike, None]=None) -> 'RefLog':\n        pass\n\n    def __init__(self, filepath: Union[PathLike, None]=None) -> None:\n        '''Initialize this instance with an optional filepath, from which we will\n        initialize our data. The path is also used to write changes back using the\n        :meth:`write` method.'''\n        pass\n\n    def _read_from_file(self) -> None:\n        pass\n    @classmethod\n    def from_file(cls, filepath: PathLike) -> 'RefLog':\n        '''\n        :return:\n            A new :class:`RefLog` instance containing all entries from the reflog at the\n            given `filepath`.\n        :param filepath:\n            Path to reflog.\n        :raise ValueError:\n            If the file could not be read or was corrupted in some way.\n        '''\n        pass\n    @classmethod\n    def path(cls, ref: 'SymbolicReference') -> str:\n        '''\n        :return:\n            String to absolute path at which the reflog of the given ref instance would\n            be found. The path is not guaranteed to point to a valid file though.\n        :param ref:\n            :class:`~git.refs.symbolic.SymbolicReference` instance\n        '''\n        pass\n    @classmethod\n    def iter_entries(cls, stream: Union[str, 'BytesIO', mmap]) -> Iterator[RefLogEntry]:\n        '''\n        :return:\n            Iterator yielding :class:`RefLogEntry` instances, one for each line read\n            from the given stream.\n        :param stream:\n            File-like object containing the revlog in its native format or string\n            instance pointing to a file to read.\n        '''\n        pass\n    @classmethod\n    def entry_at(cls, filepath: PathLike, index: int) -> 'RefLogEntry':\n        '''\n        :return:\n            :class:`RefLogEntry` at the given index.\n        :param filepath:\n            Full path to the index file from which to read the entry.\n        :param index:\n            Python list compatible index, i.e. it may be negative to specify an entry\n            counted from the end of the list.\n        :raise IndexError:\n            If the entry didn't exist.\n        :note:\n            This method is faster as it only parses the entry at index, skipping all\n            other lines. Nonetheless, the whole file has to be read if the index is\n            negative.\n        '''\n        pass\n\n    def to_file(self, filepath: PathLike) -> None:\n        '''Write the contents of the reflog instance to a file at the given filepath.\n        :param filepath:\n            Path to file. Parent directories are assumed to exist.\n        '''\n        pass\n    @classmethod\n    def append_entry(cls, config_reader: Union[Actor, 'GitConfigParser', 'SectionConstraint', None], filepath: PathLike, oldbinsha: bytes, newbinsha: bytes, message: str, write: bool=True) -> 'RefLogEntry':\n        '''Append a new log entry to the revlog at filepath.\n        :param config_reader:\n            Configuration reader of the repository - used to obtain user information.\n            May also be an :class:`~git.util.Actor` instance identifying the committer\n            directly or ``None``.\n        :param filepath:\n            Full path to the log file.\n        :param oldbinsha:\n            Binary sha of the previous commit.\n        :param newbinsha:\n            Binary sha of the current commit.\n        :param message:\n            Message describing the change to the reference.\n        :param write:\n            If ``True``, the changes will be written right away.\n            Otherwise the change will not be written.\n        :return:\n            :class:`RefLogEntry` objects which was appended to the log.\n        :note:\n            As we are append-only, concurrent access is not a problem as we do not\n            interfere with readers.\n        '''\n        pass\n\n    def write(self) -> 'RefLog':\n        '''Write this instance's data to the file we are originating from.\n        :return:\n            self\n        '''\n        pass\n\n    def _serialize(self, stream: 'BytesIO') -> 'RefLog':\n        pass\n\n    def _deserialize(self, stream: 'BytesIO') -> 'RefLog':\n        pass\nCorresponding class implementation:\n\nfrom git.types import PathLike\nfrom mmap import mmap\nfrom typing import Iterator, List, Tuple, TYPE_CHECKING, Union\nfrom git.compat import defenc\nfrom git.util import Actor, LockedFD, LockFile, assure_directory_exists, bin_to_hex, file_contents_ro_filepath, to_native_path\nimport time as _time\nimport os.path as osp\nfrom git.objects.util import Serializable, altz_to_utctz_str, parse_date\n\nclass RefLog(List[RefLogEntry], Serializable):\n    \"\"\"A reflog contains :class:`RefLogEntry`\\\\s, each of which defines a certain state\n    of the head in question. Custom query methods allow to retrieve log entries by date\n    or by other criteria.\n\n    Reflog entries are ordered. The first added entry is first in the list. The last\n    entry, i.e. the last change of the head or reference, is last in the list.\n    \"\"\"\n    __slots__ = ('_path',)\n\n    def __new__(cls, filepath: Union[PathLike, None]=None) -> 'RefLog':\n        inst = super().__new__(cls)\n        return inst\n\n    def __init__(self, filepath: Union[PathLike, None]=None) -> None:\n        \"\"\"Initialize this instance with an optional filepath, from which we will\n        initialize our data. The path is also used to write changes back using the\n        :meth:`write` method.\"\"\"\n        self._path = filepath\n        if filepath is not None:\n            self._read_from_file()\n\n    def _read_from_file(self) -> None:\n        try:\n            fmap = file_contents_ro_filepath(self._path, stream=True, allow_mmap=True)\n        except OSError:\n            return\n        try:\n            self._deserialize(fmap)\n        finally:\n            fmap.close()\n\n    @classmethod\n    def from_file(cls, filepath: PathLike) -> 'RefLog':\n        \"\"\"\n        :return:\n            A new :class:`RefLog` instance containing all entries from the reflog at the\n            given `filepath`.\n\n        :param filepath:\n            Path to reflog.\n\n        :raise ValueError:\n            If the file could not be read or was corrupted in some way.\n        \"\"\"\n        return cls(filepath)\n\n    @classmethod\n    def path(cls, ref: 'SymbolicReference') -> str:\n        \"\"\"\n        :return:\n            String to absolute path at which the reflog of the given ref instance would\n            be found. The path is not guaranteed to point to a valid file though.\n\n        :param ref:\n            :class:`~git.refs.symbolic.SymbolicReference` instance\n        \"\"\"\n        return osp.join(ref.repo.git_dir, 'logs', to_native_path(ref.path))\n\n    @classmethod\n    def iter_entries(cls, stream: Union[str, 'BytesIO', mmap]) -> Iterator[RefLogEntry]:\n        \"\"\"\n        :return:\n            Iterator yielding :class:`RefLogEntry` instances, one for each line read\n            from the given stream.\n\n        :param stream:\n            File-like object containing the revlog in its native format or string\n            instance pointing to a file to read.\n        \"\"\"\n        new_entry = RefLogEntry.from_line\n        if isinstance(stream, str):\n            _stream = file_contents_ro_filepath(stream)\n            assert isinstance(_stream, mmap)\n        else:\n            _stream = stream\n        while True:\n            line = _stream.readline()\n            if not line:\n                return\n            yield new_entry(line.strip())\n\n    @classmethod\n    def entry_at(cls, filepath: PathLike, index: int) -> 'RefLogEntry':\n        \"\"\"\n        :return:\n            :class:`RefLogEntry` at the given index.\n\n        :param filepath:\n            Full path to the index file from which to read the entry.\n\n        :param index:\n            Python list compatible index, i.e. it may be negative to specify an entry\n            counted from the end of the list.\n\n        :raise IndexError:\n            If the entry didn't exist.\n\n        :note:\n            This method is faster as it only parses the entry at index, skipping all\n            other lines. Nonetheless, the whole file has to be read if the index is\n            negative.\n        \"\"\"\n        with open(filepath, 'rb') as fp:\n            if index < 0:\n                return RefLogEntry.from_line(fp.readlines()[index].strip())\n            for i in range(index + 1):\n                line = fp.readline()\n                if not line:\n                    raise IndexError(f'Index file ended at line {i + 1}, before given index was reached')\n            return RefLogEntry.from_line(line.strip())\n\n    def to_file(self, filepath: PathLike) -> None:\n        \"\"\"Write the contents of the reflog instance to a file at the given filepath.\n\n        :param filepath:\n            Path to file. Parent directories are assumed to exist.\n        \"\"\"\n        lfd = LockedFD(filepath)\n        assure_directory_exists(filepath, is_file=True)\n        fp = lfd.open(write=True, stream=True)\n        try:\n            self._serialize(fp)\n            lfd.commit()\n        except BaseException:\n            lfd.rollback()\n            raise\n\n    @classmethod\n    def append_entry(cls, config_reader: Union[Actor, 'GitConfigParser', 'SectionConstraint', None], filepath: PathLike, oldbinsha: bytes, newbinsha: bytes, message: str, write: bool=True) -> 'RefLogEntry':\n        \"\"\"Append a new log entry to the revlog at filepath.\n\n        :param config_reader:\n            Configuration reader of the repository - used to obtain user information.\n            May also be an :class:`~git.util.Actor` instance identifying the committer\n            directly or ``None``.\n\n        :param filepath:\n            Full path to the log file.\n\n        :param oldbinsha:\n            Binary sha of the previous commit.\n\n        :param newbinsha:\n            Binary sha of the current commit.\n\n        :param message:\n            Message describing the change to the reference.\n\n        :param write:\n            If ``True``, the changes will be written right away.\n            Otherwise the change will not be written.\n\n        :return:\n            :class:`RefLogEntry` objects which was appended to the log.\n\n        :note:\n            As we are append-only, concurrent access is not a problem as we do not\n            interfere with readers.\n        \"\"\"\n        if len(oldbinsha) != 20 or len(newbinsha) != 20:\n            raise ValueError('Shas need to be given in binary format')\n        assure_directory_exists(filepath, is_file=True)\n        first_line = message.split('\\n')[0]\n        if isinstance(config_reader, Actor):\n            committer = config_reader\n        else:\n            committer = Actor.committer(config_reader)\n        entry = RefLogEntry((bin_to_hex(oldbinsha).decode('ascii'), bin_to_hex(newbinsha).decode('ascii'), committer, (int(_time.time()), _time.altzone), first_line))\n        if write:\n            lf = LockFile(filepath)\n            lf._obtain_lock_or_raise()\n            fd = open(filepath, 'ab')\n            try:\n                fd.write(entry.format().encode(defenc))\n            finally:\n                fd.close()\n                lf._release_lock()\n        return entry\n\n    def write(self) -> 'RefLog':\n        \"\"\"Write this instance's data to the file we are originating from.\n\n        :return:\n            self\n        \"\"\"\n        if self._path is None:\n            raise ValueError('Instance was not initialized with a path, use to_file(...) instead')\n        self.to_file(self._path)\n        return self\n\n    def _serialize(self, stream: 'BytesIO') -> 'RefLog':\n        write = stream.write\n        for e in self:\n            write(e.format().encode(defenc))\n        return self\n\n    def _deserialize(self, stream: 'BytesIO') -> 'RefLog':\n        self.extend(self.iter_entries(stream))\n        return self\n\nExample skeleton:\n\n\nclass ReadLowLevel(actions.ActionPlugin):\n    '''Reads `length` bytes from `path` starting at `offset` and returns it.'''\n\n    def __init__(self, grr_worker: Optional[comms.GRRClientWorker]=None):\n        pass\n\n    def Run(self, args: rdf_read_low_level.ReadLowLevelRequest) -> None:\n        '''Reads a buffer, stores it and sends it back to the server.'''\n        pass\n\n    def _StoreDataAndHash(self, data: AnyStr, offset: int) -> None:\n        '''Uploads data as blob and replies hash to flow.\n        Args:\n          data: Bytes to be stored as a blob.\n          offset: Offset where the data was read from.\n        '''\n        pass\nCorresponding class implementation:\n\nfrom grr_response_core.lib.rdfvalues import client as rdf_client\nfrom grr_response_core.lib.rdfvalues import read_low_level as rdf_read_low_level\nimport io\nfrom grr_response_client import actions\nfrom typing import AnyStr, Optional\nimport hashlib\nimport zlib\nfrom grr_response_client import comms\nfrom grr_response_core.lib import rdfvalue\nfrom grr_response_core.lib.rdfvalues import protodict as rdf_protodict\n\nclass ReadLowLevel(actions.ActionPlugin):\n    \"\"\"Reads `length` bytes from `path` starting at `offset` and returns it.\"\"\"\n    in_rdfvalue = rdf_read_low_level.ReadLowLevelRequest\n    out_rdfvalues = [rdf_read_low_level.ReadLowLevelResult]\n\n    def __init__(self, grr_worker: Optional[comms.GRRClientWorker]=None):\n        super().__init__(grr_worker)\n        self._pre_padding = 0\n        self._partial_file_hash = hashlib.sha256()\n\n    def Run(self, args: rdf_read_low_level.ReadLowLevelRequest) -> None:\n        \"\"\"Reads a buffer, stores it and sends it back to the server.\"\"\"\n        if args.length > _READ_BYTES_LIMIT:\n            raise RuntimeError(f'Can not read buffers this large ({args.length} > {_READ_BYTES_LIMIT} bytes).')\n        blob_size = args.blob_size or _DEFAULT_BLOB_SIZE\n        pre_padding = GetPrePadding(args)\n        self._pre_padding = pre_padding\n        aligned_args = AlignArgs(args, pre_padding)\n        bytes_left_to_read = aligned_args.length\n        is_first_chunk = True\n        current_offset = aligned_args.offset\n        with open(args.path, 'rb') as fd:\n            fd.seek(current_offset, io.SEEK_SET)\n            while bytes_left_to_read > 0:\n                read_size = min(blob_size, bytes_left_to_read)\n                data = fd.read(read_size)\n                if is_first_chunk:\n                    data = data[self._pre_padding:]\n                    is_first_chunk = False\n                if data:\n                    reference_offset = current_offset - self._pre_padding if current_offset else 0\n                    self._StoreDataAndHash(data, reference_offset)\n                current_offset = current_offset + read_size\n                bytes_left_to_read -= read_size\n                self.Progress()\n\n    def _StoreDataAndHash(self, data: AnyStr, offset: int) -> None:\n        \"\"\"Uploads data as blob and replies hash to flow.\n\n        Args:\n          data: Bytes to be stored as a blob.\n          offset: Offset where the data was read from.\n        \"\"\"\n        data_blob = rdf_protodict.DataBlob(data=zlib.compress(data), compression=rdf_protodict.DataBlob.CompressionType.ZCOMPRESSION)\n        self.ChargeBytesToSession(len(data))\n        self.grr_worker.SendReply(data_blob, session_id=rdfvalue.SessionID(flow_name='TransferStore'))\n        digest = hashlib.sha256(data).digest()\n        buffer_reference = rdf_client.BufferReference(offset=offset, length=len(data), data=digest)\n        self._partial_file_hash.update(data)\n        partial_file_hash = self._partial_file_hash.digest()\n        self.SendReply(rdf_read_low_level.ReadLowLevelResult(blob=buffer_reference, accumulated_hash=partial_file_hash))\n\nNow, implement the following class. Do not explain the code. The given class skeleton is as follows:\n\nclass LocalFileBlobStore:\n    '''File-based blob storage implementation.'''\n\n    def __init__(self, root: str | None=None, janitor_after_h: int=24):\n        '''\n        Initialize the local file blob store.\n        Args:\n            root: Root directory for blob storage (defaults to env BLOB_STORE_PATH or CWD/blobs).            janitor_after_h: Number of hours after which blobs are eligible for cleanup\n        '''\n        pass\n\n    def _path(self, blob_id: str) -> str:\n        '''Get the file path for a blob's binary data.'''\n        pass\n\n    def _meta_path(self, blob_id: str) -> str:\n        '''Get the file path for a blob's metadata.'''\n        pass\n\n    def save(self, data: bytes, meta: dict[str, Any]) -> str:\n        '''\n        Save binary data with metadata.\n        Args:\n            data: Binary data to store\n            meta: Metadata dictionary\n        Returns:\n            Unique blob ID for the stored data\n        '''\n        pass\n\n    def load(self, blob_id: str) -> bytes:\n        '''\n        Load binary data by blob ID.\n        Args:\n            blob_id: The blob ID to load\n        Returns:\n            Binary data\n        Raises:\n            FileNotFoundError: If the blob doesn't exist\n        '''\n        pass\n\n    def info(self, blob_id: str) -> dict[str, Any]:\n        '''\n        Get metadata for a blob.\n        Args:\n            blob_id: The blob ID\n        Returns:\n            Metadata dictionary\n        Raises:\n            FileNotFoundError: If the blob doesn't exist\n        '''\n        pass\n\n    def delete(self, blob_id: str) -> None:\n        '''\n        Delete a blob and its metadata.\n        Args:\n            blob_id: The blob ID to delete\n        '''\n        pass\n\n    def purge_old(self) -> None:\n        '''Remove blobs older than the janitor threshold.'''\n        pass"
  }
]