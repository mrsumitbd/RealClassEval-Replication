[
  {
    "role": "assistant",
    "content": "You are an expert Python programmer who can correctly implement complete Python classes based on the provided class skeleton."
  },
  {
    "role": "user",
    "content": "For your understanding, following are some (skeleton, code) examples:\n\nExample skeleton:\n\n\nclass Job(AbstractJob):\n\n    def __init__(self, model: AbstractPriorModel, simulate_cls: Callable, perturb_model: AbstractPriorModel, simulate_instance: ModelInstance, base_instance: ModelInstance, base_fit_cls: Callable, perturb_fit_cls: Callable, paths: AbstractPaths, number: int):\n        '''\n        Job to run non-linear searches comparing how well a model and a model with a perturbation fit the image.\n        Parameters\n        ----------\n        model\n            A base model that fits the image without a perturbation\n        perturb_model\n            A model of the perturbation which has been added to the underlying image\n        base_fit_cls\n            A class which defines the function which fits the base model to each simulated dataset of the sensitivity\n            map.\n        perturb_fit_cls\n            A class which defines the function which fits the perturbed model to each simulated dataset of the\n            sensitivity map.\n        paths\n            The paths defining the output directory structure of the sensitivity mapping.\n        '''\n        pass\n    @property\n    def base_paths(self):\n        pass\n    @property\n    def perturb_paths(self):\n        pass\n    @property\n    def is_complete(self) -> bool:\n        '''\n        Returns True if the job has been completed, False otherwise.\n        '''\n        pass\n\n    def perform(self) -> JobResult:\n        '''\n        - Create one model with a perturbation and another without\n        - Fit each model against the perturbed image\n        Returns\n        -------\n        An object comprising the results of the two fits\n        '''\n        pass\nCorresponding class implementation:\n\nfrom copy import copy\nfrom itertools import count\nfrom autofit.mapper.prior_model.abstract import AbstractPriorModel\nfrom typing import Callable, Optional\nfrom autofit.non_linear.paths.abstract import AbstractPaths\nfrom autofit.mapper.model import ModelInstance\nfrom autofit.non_linear.parallel import AbstractJob, AbstractJobResult\n\nclass Job(AbstractJob):\n    _number = count()\n\n    def __init__(self, model: AbstractPriorModel, simulate_cls: Callable, perturb_model: AbstractPriorModel, simulate_instance: ModelInstance, base_instance: ModelInstance, base_fit_cls: Callable, perturb_fit_cls: Callable, paths: AbstractPaths, number: int):\n        \"\"\"\n        Job to run non-linear searches comparing how well a model and a model with a perturbation fit the image.\n\n        Parameters\n        ----------\n        model\n            A base model that fits the image without a perturbation\n        perturb_model\n            A model of the perturbation which has been added to the underlying image\n        base_fit_cls\n            A class which defines the function which fits the base model to each simulated dataset of the sensitivity\n            map.\n        perturb_fit_cls\n            A class which defines the function which fits the perturbed model to each simulated dataset of the\n            sensitivity map.\n        paths\n            The paths defining the output directory structure of the sensitivity mapping.\n        \"\"\"\n        super().__init__(number=number)\n        self.model = model\n        self.simulate_cls = simulate_cls\n        self.perturb_model = perturb_model\n        self.simulate_instance = simulate_instance\n        self.base_instance = base_instance\n        self.base_fit_cls = base_fit_cls\n        self.perturb_fit_cls = perturb_fit_cls\n        self.paths = paths\n\n    @property\n    def base_paths(self):\n        return self.paths.for_sub_analysis('[base]')\n\n    @property\n    def perturb_paths(self):\n        return self.paths.for_sub_analysis('[perturb]')\n\n    @property\n    def is_complete(self) -> bool:\n        \"\"\"\n        Returns True if the job has been completed, False otherwise.\n        \"\"\"\n        return self.base_paths.is_complete and self.perturb_paths.is_complete or ((self.paths.output_path / '[base].zip').exists() and (self.paths.output_path / '[perturb].zip').exists())\n\n    def perform(self) -> JobResult:\n        \"\"\"\n        - Create one model with a perturbation and another without\n        - Fit each model against the perturbed image\n\n        Returns\n        -------\n        An object comprising the results of the two fits\n        \"\"\"\n        if self.is_complete:\n            dataset = None\n        else:\n            dataset = self.simulate_cls(instance=self.simulate_instance, simulate_path=self.paths.image_path.with_name('simulate'))\n        result = self.base_fit_cls(model=self.model, dataset=dataset, paths=self.base_paths, instance=self.simulate_instance)\n        perturb_model = copy(self.model)\n        perturb_model.perturb = self.perturb_model\n        perturb_result = self.perturb_fit_cls(model=perturb_model, dataset=dataset, paths=self.perturb_paths, instance=self.simulate_instance)\n        return JobResult(number=self.number, result=result, perturb_result=perturb_result)\n\nExample skeleton:\n\n\nclass FixedEpsilonAttack(AttackWithDistance):\n    '''Fixed-epsilon attacks try to find adversarials whose perturbation sizes\n    are limited by a fixed epsilon'''\n    @abstractmethod\n    def run(self, model: Model, inputs: T, criterion: Any, *, epsilon: float, **kwargs: Any) -> T:\n        '''Runs the attack and returns perturbed inputs.\n        The size of the perturbations should be at most epsilon, but this\n        is not guaranteed and the caller should verify this or clip the result.\n        '''\n        pass\n    @overload\n    def __call__(self, model: Model, inputs: T, criterion: Any, *, epsilons: Sequence[Union[float, None]], **kwargs: Any) -> Tuple[List[T], List[T], T]:\n        pass\n    @overload\n    def __call__(self, model: Model, inputs: T, criterion: Any, *, epsilons: Sequence[Union[float, None]], **kwargs: Any) -> Tuple[List[T], List[T], T]:\n        pass\n    @final\n    def __call__(self, model: Model, inputs: T, criterion: Any, *, epsilons: Sequence[Union[float, None]], **kwargs: Any) -> Tuple[List[T], List[T], T]:\n        pass\nCorresponding class implementation:\n\nimport eagerpy as ep\nfrom typing_extensions import final, overload\nfrom ..models import Model\nfrom typing import Callable, TypeVar, Any, Union, Optional, Sequence, List, Tuple, Dict\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Iterable\n\nclass FixedEpsilonAttack(AttackWithDistance):\n    \"\"\"Fixed-epsilon attacks try to find adversarials whose perturbation sizes\n    are limited by a fixed epsilon\"\"\"\n\n    @abstractmethod\n    def run(self, model: Model, inputs: T, criterion: Any, *, epsilon: float, **kwargs: Any) -> T:\n        \"\"\"Runs the attack and returns perturbed inputs.\n\n        The size of the perturbations should be at most epsilon, but this\n        is not guaranteed and the caller should verify this or clip the result.\n        \"\"\"\n        ...\n\n    @overload\n    def __call__(self, model: Model, inputs: T, criterion: Any, *, epsilons: Sequence[Union[float, None]], **kwargs: Any) -> Tuple[List[T], List[T], T]:\n        ...\n\n    @overload\n    def __call__(self, model: Model, inputs: T, criterion: Any, *, epsilons: Union[float, None], **kwargs: Any) -> Tuple[T, T, T]:\n        ...\n\n    @final\n    def __call__(self, model: Model, inputs: T, criterion: Any, *, epsilons: Union[Sequence[Union[float, None]], float, None], **kwargs: Any) -> Union[Tuple[List[T], List[T], T], Tuple[T, T, T]]:\n        x, restore_type = ep.astensor_(inputs)\n        del inputs\n        verify_input_bounds(x, model)\n        criterion = get_criterion(criterion)\n        is_adversarial = get_is_adversarial(criterion, model)\n        was_iterable = True\n        if not isinstance(epsilons, Iterable):\n            epsilons = [epsilons]\n            was_iterable = False\n        N = len(x)\n        K = len(epsilons)\n        if any((eps is None for eps in epsilons)):\n            raise NotImplementedError('FixedEpsilonAttack subclasses do not yet support None in epsilons')\n        real_epsilons = [eps for eps in epsilons if eps is not None]\n        del epsilons\n        xps = []\n        xpcs = []\n        success = []\n        for epsilon in real_epsilons:\n            xp = self.run(model, x, criterion, epsilon=epsilon, **kwargs)\n            xpc = self.distance.clip_perturbation(x, xp, epsilon)\n            is_adv = is_adversarial(xpc)\n            xps.append(xp)\n            xpcs.append(xpc)\n            success.append(is_adv)\n        success_ = ep.stack(success)\n        assert success_.shape == (K, N)\n        xps_ = [restore_type(xp) for xp in xps]\n        xpcs_ = [restore_type(xpc) for xpc in xpcs]\n        if was_iterable:\n            return (xps_, xpcs_, restore_type(success_))\n        else:\n            assert len(xps_) == 1\n            assert len(xpcs_) == 1\n            return (xps_[0], xpcs_[0], restore_type(success_.squeeze(axis=0)))\n\nNow, implement the following class. Do not explain the code. The given class skeleton is as follows:\n\nclass NLAssertionsEvaluator:\n    '''\n    Judge that evaluates whether a trajectory adheres to all the natural-language assertions.\n    '''\n    @classmethod\n    def calculate_reward(cls, task: Task, full_trajectory: list[Message]) -> RewardInfo:\n        '''\n        Calculate the reward for the simulation by using an LLM to evaluate whether the trajectory adheres to all the natural-language assertions\n        '''\n        pass\n    @classmethod\n    def evaluate_nl_assertions(cls, trajectory: list[Message], nl_assertions: list[str]) -> list[NLAssertionCheck]:\n        '''\n        Evaluate whether the trajectory meets each expected outcome.\n        Args:\n            trajectory: List of messages from the conversation\n            nl_assertions: List of natural-language assertions to evaluate\n        Returns:\n            List of evaluation results for each NL assertion, containing:\n            - nl_assertion: The NL assertion being evaluated\n            - metExpectation: Boolean indicating if the assertion was met\n            - reasoning: Explanation for the evaluation\n        '''\n        pass"
  }
]