[
  {
    "role": "assistant",
    "content": "You are an expert Python programmer who can correctly implement complete Python classes based on the provided class skeleton."
  },
  {
    "role": "user",
    "content": "For your understanding, following are some (skeleton, code) examples:\n\nExample skeleton:\n\n\nclass AliasManager(object):\n\n    def __init__(self, **kwargs):\n        pass\n\n    def load_alias_table(self):\n        '''\n        Load (create, if not exist) the alias config file.\n        '''\n        pass\n\n    def load_alias_hash(self):\n        '''\n        Load (create, if not exist) the alias hash file.\n        '''\n        pass\n\n    def load_collided_alias(self):\n        '''\n        Load (create, if not exist) the collided alias file.\n        '''\n        pass\n\n    def detect_alias_config_change(self):\n        '''\n        Change if the alias configuration has changed since the last run.\n        Returns:\n            False if the alias configuration file has not been changed since the last run.\n            Otherwise, return True.\n        '''\n        pass\n\n    def transform(self, args):\n        '''\n        Transform any aliases in args to their respective commands.\n        Args:\n            args: A list of space-delimited command input extracted directly from the console.\n        Returns:\n            A list of transformed commands according to the alias configuration file.\n        '''\n        pass\n\n    def get_full_alias(self, query):\n        '''\n        Get the full alias given a search query.\n        Args:\n            query: The query this function performs searching on.\n        Returns:\n            The full alias (with the placeholders, if any).\n        '''\n        pass\n\n    def load_full_command_table(self):\n        '''\n        Perform a full load of the command table to get all the reserved command words.\n        '''\n        pass\n\n    def post_transform(self, args):\n        '''\n        Inject environment variables, and write hash to alias hash file after transforming alias to commands.\n        Args:\n            args: A list of args to post-transform.\n        '''\n        pass\n\n    def parse_error(self):\n        '''\n        Check if there is a configuration parsing error.\n        A parsing error has occurred if there are strings inside the alias config file\n        but there is no alias loaded in self.alias_table.\n        Returns:\n            True if there is an error parsing the alias configuration file. Otherwises, false.\n        '''\n        pass\n    @staticmethod\n    def build_collision_table(aliases, levels=COLLISION_CHECK_LEVEL_DEPTH):\n        '''\n        Build the collision table according to the alias configuration file against the entire command table.\n        self.collided_alias is structured as:\n        {\n            'collided_alias': [the command level at which collision happens]\n        }\n        For example:\n        {\n            'account': [1, 2]\n        }\n        This means that 'account' is a reserved command in level 1 and level 2 of the command tree because\n        (az account ...) and (az storage account ...)\n             lvl 1                        lvl 2\n        Args:\n            levels: the amount of levels we tranverse through the command table tree.\n        '''\n        pass\n    @staticmethod\n    def write_alias_config_hash(alias_config_hash='', empty_hash=False):\n        '''\n        Write self.alias_config_hash to the alias hash file.\n        Args:\n            empty_hash: True if we want to write an empty string into the file. Empty string in the alias hash file\n                means that we have to perform a full load of the command table in the next run.\n        '''\n        pass\n    @staticmethod\n    def write_collided_alias(collided_alias_dict):\n        '''\n        Write the collided aliases string into the collided alias file.\n        '''\n        pass\n    @staticmethod\n    def process_exception_message(exception):\n        '''\n        Process an exception message.\n        Args:\n            exception: The exception to process.\n        Returns:\n            A filtered string summarizing the exception.\n        '''\n        pass\nCorresponding class implementation:\n\nfrom azext_alias._const import GLOBAL_CONFIG_DIR, ALIAS_FILE_NAME, ALIAS_HASH_FILE_NAME, COLLIDED_ALIAS_FILE_NAME, CONFIG_PARSING_ERROR, DEBUG_MSG, COLLISION_CHECK_LEVEL_DEPTH, POS_ARG_DEBUG_MSG\nfrom collections import defaultdict\nimport azext_alias\nimport hashlib\nimport shlex\nfrom azext_alias.util import is_alias_command, cache_reserved_commands, get_config_parser, build_tab_completion_table\nimport os\nimport json\nfrom azext_alias.argument import build_pos_args_table, render_template\nimport re\nfrom azext_alias import telemetry\n\nclass AliasManager(object):\n\n    def __init__(self, **kwargs):\n        self.alias_table = get_config_parser()\n        self.kwargs = kwargs\n        self.collided_alias = defaultdict(list)\n        self.alias_config_str = ''\n        self.alias_config_hash = ''\n        self.load_alias_table()\n        self.load_alias_hash()\n\n    def load_alias_table(self):\n        \"\"\"\n        Load (create, if not exist) the alias config file.\n        \"\"\"\n        try:\n            open_mode = 'r+' if os.path.exists(GLOBAL_ALIAS_PATH) else 'w+'\n            with open(GLOBAL_ALIAS_PATH, open_mode) as alias_config_file:\n                self.alias_config_str = alias_config_file.read()\n            self.alias_table.read(GLOBAL_ALIAS_PATH)\n            telemetry.set_number_of_aliases_registered(len(self.alias_table.sections()))\n        except Exception as exception:\n            logger.warning(CONFIG_PARSING_ERROR, AliasManager.process_exception_message(exception))\n            self.alias_table = get_config_parser()\n            telemetry.set_exception(exception)\n\n    def load_alias_hash(self):\n        \"\"\"\n        Load (create, if not exist) the alias hash file.\n        \"\"\"\n        open_mode = 'r+' if os.path.exists(GLOBAL_ALIAS_HASH_PATH) else 'w+'\n        with open(GLOBAL_ALIAS_HASH_PATH, open_mode) as alias_config_hash_file:\n            self.alias_config_hash = alias_config_hash_file.read()\n\n    def load_collided_alias(self):\n        \"\"\"\n        Load (create, if not exist) the collided alias file.\n        \"\"\"\n        open_mode = 'r+' if os.path.exists(GLOBAL_COLLIDED_ALIAS_PATH) else 'w+'\n        with open(GLOBAL_COLLIDED_ALIAS_PATH, open_mode) as collided_alias_file:\n            collided_alias_str = collided_alias_file.read()\n            try:\n                self.collided_alias = json.loads(collided_alias_str if collided_alias_str else '{}')\n            except Exception:\n                self.collided_alias = {}\n\n    def detect_alias_config_change(self):\n        \"\"\"\n        Change if the alias configuration has changed since the last run.\n\n        Returns:\n            False if the alias configuration file has not been changed since the last run.\n            Otherwise, return True.\n        \"\"\"\n        if self.parse_error():\n            return False\n        alias_config_sha1 = hashlib.sha1(self.alias_config_str.encode('utf-8')).hexdigest()\n        if alias_config_sha1 != self.alias_config_hash:\n            self.alias_config_hash = alias_config_sha1\n            return True\n        return False\n\n    def transform(self, args):\n        \"\"\"\n        Transform any aliases in args to their respective commands.\n\n        Args:\n            args: A list of space-delimited command input extracted directly from the console.\n\n        Returns:\n            A list of transformed commands according to the alias configuration file.\n        \"\"\"\n        if self.parse_error():\n            AliasManager.write_alias_config_hash(empty_hash=True)\n            return args\n        if self.detect_alias_config_change():\n            self.load_full_command_table()\n            self.collided_alias = AliasManager.build_collision_table(self.alias_table.sections())\n            build_tab_completion_table(self.alias_table)\n        else:\n            self.load_collided_alias()\n        transformed_commands = []\n        alias_iter = enumerate(args, 1)\n        for alias_index, alias in alias_iter:\n            is_collided_alias = alias in self.collided_alias and alias_index in self.collided_alias[alias]\n            is_named_arg = alias_index > 1 and args[alias_index - 2].startswith('-')\n            is_named_arg_flag = alias.startswith('-')\n            excluded_commands = is_alias_command(['remove', 'export'], transformed_commands)\n            if not alias or is_collided_alias or is_named_arg or is_named_arg_flag or excluded_commands:\n                transformed_commands.append(alias)\n                continue\n            full_alias = self.get_full_alias(alias)\n            if self.alias_table.has_option(full_alias, 'command'):\n                cmd_derived_from_alias = self.alias_table.get(full_alias, 'command')\n                telemetry.set_alias_hit(full_alias)\n            else:\n                transformed_commands.append(alias)\n                continue\n            pos_args_table = build_pos_args_table(full_alias, args, alias_index)\n            if pos_args_table:\n                logger.debug(POS_ARG_DEBUG_MSG, full_alias, cmd_derived_from_alias, pos_args_table)\n                transformed_commands += render_template(cmd_derived_from_alias, pos_args_table)\n                for pos_arg in pos_args_table:\n                    next(alias_iter)\n            else:\n                logger.debug(DEBUG_MSG, full_alias, cmd_derived_from_alias)\n                transformed_commands += shlex.split(cmd_derived_from_alias)\n        return self.post_transform(transformed_commands)\n\n    def get_full_alias(self, query):\n        \"\"\"\n        Get the full alias given a search query.\n\n        Args:\n            query: The query this function performs searching on.\n\n        Returns:\n            The full alias (with the placeholders, if any).\n        \"\"\"\n        if query in self.alias_table.sections():\n            return query\n        return next((section for section in self.alias_table.sections() if section.split()[0] == query), '')\n\n    def load_full_command_table(self):\n        \"\"\"\n        Perform a full load of the command table to get all the reserved command words.\n        \"\"\"\n        load_cmd_tbl_func = self.kwargs.get('load_cmd_tbl_func', lambda _: {})\n        cache_reserved_commands(load_cmd_tbl_func)\n        telemetry.set_full_command_table_loaded()\n\n    def post_transform(self, args):\n        \"\"\"\n        Inject environment variables, and write hash to alias hash file after transforming alias to commands.\n\n        Args:\n            args: A list of args to post-transform.\n        \"\"\"\n        args = args[1:] if args and args[0] == 'az' else args\n        post_transform_commands = []\n        for i, arg in enumerate(args):\n            if is_alias_command(['create'], args) and i > 0 and (args[i - 1] in ['-c', '--command']):\n                post_transform_commands.append(arg)\n            else:\n                post_transform_commands.append(os.path.expandvars(arg))\n        AliasManager.write_alias_config_hash(self.alias_config_hash)\n        AliasManager.write_collided_alias(self.collided_alias)\n        return post_transform_commands\n\n    def parse_error(self):\n        \"\"\"\n        Check if there is a configuration parsing error.\n\n        A parsing error has occurred if there are strings inside the alias config file\n        but there is no alias loaded in self.alias_table.\n\n        Returns:\n            True if there is an error parsing the alias configuration file. Otherwises, false.\n        \"\"\"\n        return not self.alias_table.sections() and self.alias_config_str\n\n    @staticmethod\n    def build_collision_table(aliases, levels=COLLISION_CHECK_LEVEL_DEPTH):\n        \"\"\"\n        Build the collision table according to the alias configuration file against the entire command table.\n\n        self.collided_alias is structured as:\n        {\n            'collided_alias': [the command level at which collision happens]\n        }\n        For example:\n        {\n            'account': [1, 2]\n        }\n        This means that 'account' is a reserved command in level 1 and level 2 of the command tree because\n        (az account ...) and (az storage account ...)\n             lvl 1                        lvl 2\n\n        Args:\n            levels: the amount of levels we tranverse through the command table tree.\n        \"\"\"\n        collided_alias = defaultdict(list)\n        for alias in aliases:\n            word = alias.split()[0]\n            for level in range(1, levels + 1):\n                collision_regex = '^{}{}($|\\\\s)'.format('([a-z\\\\-]*\\\\s)' * (level - 1), word.lower())\n                if list(filter(re.compile(collision_regex).match, azext_alias.cached_reserved_commands)) and level not in collided_alias[word]:\n                    collided_alias[word].append(level)\n        telemetry.set_collided_aliases(list(collided_alias.keys()))\n        return collided_alias\n\n    @staticmethod\n    def write_alias_config_hash(alias_config_hash='', empty_hash=False):\n        \"\"\"\n        Write self.alias_config_hash to the alias hash file.\n\n        Args:\n            empty_hash: True if we want to write an empty string into the file. Empty string in the alias hash file\n                means that we have to perform a full load of the command table in the next run.\n        \"\"\"\n        with open(GLOBAL_ALIAS_HASH_PATH, 'w') as alias_config_hash_file:\n            alias_config_hash_file.write('' if empty_hash else alias_config_hash)\n\n    @staticmethod\n    def write_collided_alias(collided_alias_dict):\n        \"\"\"\n        Write the collided aliases string into the collided alias file.\n        \"\"\"\n        open_mode = 'r+' if os.path.exists(GLOBAL_COLLIDED_ALIAS_PATH) else 'w+'\n        with open(GLOBAL_COLLIDED_ALIAS_PATH, open_mode) as collided_alias_file:\n            collided_alias_file.truncate()\n            collided_alias_file.write(json.dumps(collided_alias_dict))\n\n    @staticmethod\n    def process_exception_message(exception):\n        \"\"\"\n        Process an exception message.\n\n        Args:\n            exception: The exception to process.\n\n        Returns:\n            A filtered string summarizing the exception.\n        \"\"\"\n        exception_message = str(exception)\n        for replace_char in ['\\t', '\\n', '\\\\n']:\n            exception_message = exception_message.replace(replace_char, '' if replace_char != '\\t' else ' ')\n        return exception_message.replace('section', 'alias')\n\nExample skeleton:\n\n\nclass TestCase(NcpSupportMixin, unittest.TestCase):\n    '''The base class for all thread certification test cases.\n    The `topology` member of sub-class is used to create test topology.\n        '''\n\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def skipTest(self, reason: Any) -> None:\n        pass\n\n    def setUp(self):\n        pass\n\n    def _setUp(self):\n        '''Create simulator, nodes and apply configurations.\n        '''\n        pass\n\n    def _set_up_active_dataset(self, node, params):\n        pass\n\n    def inspect(self):\n        pass\n\n    def tearDown(self):\n        '''Destroy nodes and simulator.\n        '''\n        pass\n\n    def flush_all(self):\n        '''Flush away all captured messages of all nodes.\n        '''\n        pass\n\n    def flush_nodes(self, nodes):\n        '''Flush away all captured messages of specified nodes.\n        Args:\n            nodes (list): nodes whose messages to flush.\n        '''\n        pass\n\n    def _clean_up_tmp(self):\n        '''\n        Clean up node files in tmp directory\n        '''\n        pass\n\n    def _verify_packets(self, test_info_path: str):\n        pass\n    @property\n    def test_name(self):\n        pass\n\n    def collect_ipaddrs(self):\n        pass\n\n    def collect_rloc16s(self):\n        pass\n\n    def collect_rlocs(self):\n        pass\n\n    def collect_omrs(self):\n        pass\n\n    def collect_duas(self):\n        pass\n\n    def collect_leader_aloc(self, node):\n        pass\n\n    def collect_extra_vars(self, **vars):\n        pass\n\n    def _collect_test_info_after_setup(self):\n        '''\n        Collect test info after setUp\n        '''\n        pass\n\n    def _construct_backbone_network_name(self, backbone_network_id) -> str:\n        '''\n        Construct the name of the backbone network based on the given backbone network id from TOPOLOGY. If the\n        backbone_network_id is not defined in TOPOLOGY, use the default backbone network id.\n        '''\n        pass\n\n    def _output_test_info(self):\n        '''\n        Output test info to json file after tearDown\n        '''\n        pass\n\n    def _get_thread_pcap_filename(self):\n        pass\n\n    def assure_run_ok(self, cmd, shell=False):\n        pass\n\n    def _parse_params(self, params: Optional[dict]) -> dict:\n        pass\n\n    def _has_backbone_traffic(self):\n        pass\n\n    def _prepare_backbone_network(self):\n        '''\n        Creates one or more backbone networks (Docker bridge networks) based on the TOPOLOGY definition.\n        * If `backbone_network_id` is defined in the TOPOLOGY:\n            * Network name:   `backbone{PORT_OFFSET}.{backbone_network_id}`      (e.g., \"backbone0.0\", \"backbone0.1\")\n            * Network prefix: `backbone{PORT_OFFSET}:{backbone_network_id}::/64` (e.g., \"9100:0::/64\", \"9100:1::/64\")\n        * If `backbone_network_id` is undefined:\n            * Network name:   `backbone{PORT_OFFSET}.0`    (e.g., \"backbone0.0\")\n            * Network prefix: `backbone{PORT_OFFSET}::/64` (e.g., \"9100::/64\")\n        '''\n        pass\n\n    def _remove_backbone_network(self):\n        pass\n\n    def _start_backbone_sniffer(self):\n        pass\n\n    def _get_backbone_pcap_filename(self):\n        pass\n\n    def _get_merged_pcap_filename(self):\n        pass\n\n    def _stop_backbone_sniffer(self):\n        pass\n\n    def _merge_thread_backbone_pcaps(self):\n        pass\n\n    def wait_until(self, cond: Callable[[], bool], timeout: int, go_interval: int=1):\n        pass\n\n    def wait_node_state(self, node: Union[int, Node], state: str, timeout: int):\n        pass\n\n    def wait_route_established(self, node1: int, node2: int, timeout=10):\n        pass\n\n    def assertDictIncludes(self, actual: Mapping[str, str], expected: Mapping[str, str]):\n        ''' Asserts the `actual` dict includes the `expected` dict.\n        Args:\n            actual: A dict for checking.\n            expected: The expected items that the actual dict should contains.\n        '''\n        pass\nCorresponding class implementation:\n\nimport subprocess\nimport config\nimport os\nimport sys\nimport logging\nfrom pktverify import utils as pvutils\nimport json\nfrom typing import Optional, Callable, Union, Mapping, Any\nimport stat\nimport signal\nimport unittest\nimport debug\nimport traceback\nimport time\nfrom node import Node, OtbrNode, HostNode\n\nclass TestCase(NcpSupportMixin, unittest.TestCase):\n    \"\"\"The base class for all thread certification test cases.\n\n    The `topology` member of sub-class is used to create test topology.\n    \"\"\"\n    USE_MESSAGE_FACTORY = True\n    TOPOLOGY = {}\n    CASE_WIRESHARK_PREFS = None\n    SUPPORT_THREAD_1_1 = True\n    PACKET_VERIFICATION = config.PACKET_VERIFICATION_DEFAULT\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n        self._start_time = None\n        self._do_packet_verification = PACKET_VERIFICATION and hasattr(self, 'verify') and (self.PACKET_VERIFICATION == PACKET_VERIFICATION)\n        self._backbone_network_names = []\n\n    def skipTest(self, reason: Any) -> None:\n        self._testSkipped = True\n        super(TestCase, self).skipTest(reason)\n\n    def setUp(self):\n        self._testSkipped = False\n        if ENV_THREAD_VERSION == '1.1' and (not self.SUPPORT_THREAD_1_1):\n            self.skipTest('Thread 1.1 not supported.')\n        try:\n            self._setUp()\n        except:\n            traceback.print_exc()\n            for node in list(self.nodes.values()):\n                try:\n                    node.destroy()\n                except Exception:\n                    traceback.print_exc()\n            raise\n\n    def _setUp(self):\n        \"\"\"Create simulator, nodes and apply configurations.\n        \"\"\"\n        self._clean_up_tmp()\n        key_manager = config.create_default_thread_key_manager()\n        self.simulator = config.create_default_simulator(config.create_default_thread_message_factory(key_manager) if self.USE_MESSAGE_FACTORY else None)\n        self.nodes = {}\n        os.environ['LD_LIBRARY_PATH'] = '/tmp/thread-wireshark'\n        if self._has_backbone_traffic():\n            self._prepare_backbone_network()\n            self._start_backbone_sniffer()\n        self._initial_topology = initial_topology = {}\n        for i, params in self.TOPOLOGY.items():\n            params = self._parse_params(params)\n            initial_topology[i] = params\n            backbone_network_name = self._construct_backbone_network_name(params.get('backbone_network_id')) if self._has_backbone_traffic() else None\n            logging.info('Creating node %d: %r', i, params)\n            logging.info('Backbone network: %s', backbone_network_name)\n            if params['is_otbr']:\n                nodeclass = OtbrNode\n            elif params['is_host']:\n                nodeclass = HostNode\n            else:\n                nodeclass = Node\n            node = nodeclass(i, is_mtd=params['is_mtd'], simulator=self.simulator, name=params.get('name'), version=params['version'], is_bbr=params['is_bbr'], backbone_network=backbone_network_name)\n            if 'boot_delay' in params:\n                self.simulator.go(params['boot_delay'])\n            self.nodes[i] = node\n            if node.is_host:\n                continue\n            self.nodes[i].set_mode(params['mode'])\n            if 'partition_id' in params:\n                self.nodes[i].set_preferred_partition_id(params['partition_id'])\n            if params['is_ftd']:\n                self.nodes[i].set_router_selection_jitter(params['router_selection_jitter'])\n            if 'router_upgrade_threshold' in params:\n                self.nodes[i].set_router_upgrade_threshold(params['router_upgrade_threshold'])\n            if 'router_downgrade_threshold' in params:\n                self.nodes[i].set_router_downgrade_threshold(params['router_downgrade_threshold'])\n            if 'router_eligible' in params:\n                self.nodes[i].set_router_eligible(params['router_eligible'])\n            if 'prefer_router_id' in params:\n                self.nodes[i].prefer_router_id(params['prefer_router_id'])\n            if 'timeout' in params:\n                self.nodes[i].set_timeout(params['timeout'])\n            self._set_up_active_dataset(self.nodes[i], params)\n            if 'pending_dataset' in params:\n                self.nodes[i].set_pending_dataset(params['pending_dataset']['pendingtimestamp'], params['pending_dataset']['activetimestamp'], panid=params['pending_dataset'].get('panid'), channel=params['pending_dataset'].get('channel'), delay=params['pending_dataset'].get('delay'))\n            if 'key_sequence_counter' in params:\n                self.nodes[i].set_key_sequence_counter(params['key_sequence_counter'])\n            if 'network_id_timeout' in params:\n                self.nodes[i].set_network_id_timeout(params['network_id_timeout'])\n            if 'context_reuse_delay' in params:\n                self.nodes[i].set_context_reuse_delay(params['context_reuse_delay'])\n            if 'max_children' in params:\n                self.nodes[i].set_max_children(params['max_children'])\n            if 'bbr_registration_jitter' in params:\n                self.nodes[i].set_bbr_registration_jitter(params['bbr_registration_jitter'])\n            if 'router_id_range' in params:\n                self.nodes[i].set_router_id_range(params['router_id_range'][0], params['router_id_range'][1])\n        for i, params in initial_topology.items():\n            allowlist = params['allowlist']\n            if allowlist is None:\n                continue\n            for j in allowlist:\n                rssi = None\n                if isinstance(j, tuple):\n                    j, rssi = j\n                self.nodes[i].add_allowlist(self.nodes[j].get_addr64(), rssi=rssi)\n            self.nodes[i].enable_allowlist()\n        self._inspector = debug.Inspector(self)\n        self._collect_test_info_after_setup()\n\n    def _set_up_active_dataset(self, node, params):\n        dataset = {'timestamp': 1, 'channel': config.CHANNEL, 'channel_mask': config.CHANNEL_MASK, 'extended_panid': config.EXTENDED_PANID, 'mesh_local_prefix': config.MESH_LOCAL_PREFIX.split('/')[0], 'network_key': config.DEFAULT_NETWORK_KEY, 'network_name': config.NETWORK_NAME, 'panid': config.PANID, 'pskc': config.PSKC, 'security_policy': config.SECURITY_POLICY}\n        if 'channel' in params:\n            dataset['channel'] = params['channel']\n        if 'network_key' in params:\n            dataset['network_key'] = params['network_key']\n        if 'network_name' in params:\n            dataset['network_name'] = params['network_name']\n        if 'panid' in params:\n            dataset['panid'] = params['panid']\n        if 'active_dataset' in params:\n            dataset.update(params['active_dataset'])\n        node.set_active_dataset(**dataset)\n\n    def inspect(self):\n        self._inspector.inspect()\n\n    def tearDown(self):\n        \"\"\"Destroy nodes and simulator.\n        \"\"\"\n        if self._do_packet_verification and os.uname().sysname != 'Linux':\n            raise NotImplementedError(f'{self.test_name}: Packet Verification not available on {os.uname().sysname} (Linux only).')\n        if self._do_packet_verification:\n            self.simulator.go(3)\n        if self._has_backbone_traffic():\n            self._stop_backbone_sniffer()\n        for node in list(self.nodes.values()):\n            try:\n                node.stop()\n            except:\n                traceback.print_exc()\n            finally:\n                node.destroy()\n        self.simulator.stop()\n        if self._has_backbone_traffic():\n            self._remove_backbone_network()\n        if self._do_packet_verification:\n            if self._has_backbone_traffic():\n                pcap_filename = self._merge_thread_backbone_pcaps()\n            else:\n                pcap_filename = self._get_thread_pcap_filename()\n            self._test_info['pcap'] = pcap_filename\n            test_info_path = self._output_test_info()\n            if not self._testSkipped:\n                self._verify_packets(test_info_path)\n\n    def flush_all(self):\n        \"\"\"Flush away all captured messages of all nodes.\n        \"\"\"\n        for i in list(self.nodes.keys()):\n            self.simulator.get_messages_sent_by(i)\n\n    def flush_nodes(self, nodes):\n        \"\"\"Flush away all captured messages of specified nodes.\n\n        Args:\n            nodes (list): nodes whose messages to flush.\n\n        \"\"\"\n        for i in nodes:\n            if i in list(self.nodes.keys()):\n                self.simulator.get_messages_sent_by(i)\n\n    def _clean_up_tmp(self):\n        \"\"\"\n        Clean up node files in tmp directory\n        \"\"\"\n        os.system(f'rm -f tmp/{PORT_OFFSET}_*.flash tmp/{PORT_OFFSET}_*.data tmp/{PORT_OFFSET}_*.swap')\n\n    def _verify_packets(self, test_info_path: str):\n        pv = PacketVerifier(test_info_path, self.CASE_WIRESHARK_PREFS)\n        pv.add_common_vars()\n        pv.pkts.filter_thread_unallowed_icmpv6().must_not_next()\n        self.verify(pv)\n        print('Packet verification passed: %s' % test_info_path, file=sys.stderr)\n\n    @property\n    def test_name(self):\n        return os.getenv('TEST_NAME', 'current')\n\n    def collect_ipaddrs(self):\n        if not self._do_packet_verification:\n            return\n        test_info = self._test_info\n        for i, node in self.nodes.items():\n            ipaddrs = node.get_addrs()\n            if hasattr(node, 'get_ether_addrs'):\n                ipaddrs += node.get_ether_addrs()\n            test_info['ipaddrs'][i] = ipaddrs\n            if not node.is_host:\n                mleid = node.get_mleid()\n                test_info['mleids'][i] = mleid\n\n    def collect_rloc16s(self):\n        if not self._do_packet_verification:\n            return\n        test_info = self._test_info\n        test_info['rloc16s'] = {}\n        for i, node in self.nodes.items():\n            if not node.is_host:\n                test_info['rloc16s'][i] = '0x%04x' % node.get_addr16()\n\n    def collect_rlocs(self):\n        if not self._do_packet_verification:\n            return\n        test_info = self._test_info\n        test_info['rlocs'] = {}\n        for i, node in self.nodes.items():\n            if node.is_host:\n                continue\n            test_info['rlocs'][i] = node.get_rloc()\n\n    def collect_omrs(self):\n        if not self._do_packet_verification:\n            return\n        test_info = self._test_info\n        test_info['omrs'] = {}\n        for i, node in self.nodes.items():\n            if node.is_host:\n                continue\n            test_info['omrs'][i] = node.get_ip6_address(config.ADDRESS_TYPE.OMR)\n\n    def collect_duas(self):\n        if not self._do_packet_verification:\n            return\n        test_info = self._test_info\n        test_info['duas'] = {}\n        for i, node in self.nodes.items():\n            if node.is_host:\n                continue\n            test_info['duas'][i] = node.get_ip6_address(config.ADDRESS_TYPE.DUA)\n\n    def collect_leader_aloc(self, node):\n        if not self._do_packet_verification:\n            return\n        test_info = self._test_info\n        test_info['leader_aloc'] = self.nodes[node].get_addr_leader_aloc()\n\n    def collect_extra_vars(self, **vars):\n        if not self._do_packet_verification:\n            return\n        for k in vars.keys():\n            assert isinstance(k, str), k\n        test_vars = self._test_info.setdefault('extra_vars', {})\n        test_vars.update(vars)\n\n    def _collect_test_info_after_setup(self):\n        \"\"\"\n        Collect test info after setUp\n        \"\"\"\n        if not self._do_packet_verification:\n            return\n        test_info = self._test_info = {'script': os.path.abspath(sys.argv[0]), 'testcase': self.test_name, 'start_time': time.ctime(self._start_time), 'pcap': '', 'extaddrs': {}, 'ethaddrs': {}, 'ipaddrs': {}, 'mleids': {}, 'topology': self._initial_topology, 'backbone': {'interface': config.BACKBONE_DOCKER_NETWORK_NAME, 'prefix': config.BACKBONE_PREFIX}, 'domain_prefix': config.DOMAIN_PREFIX, 'env': {'PORT_OFFSET': config.PORT_OFFSET}}\n        for i, node in self.nodes.items():\n            if not node.is_host:\n                extaddr = node.get_addr64()\n                test_info['extaddrs'][i] = ExtAddr(extaddr).format_octets()\n            if node.is_host or node.is_otbr:\n                ethaddr = node.get_ether_mac()\n                test_info['ethaddrs'][i] = EthAddr(ethaddr).format_octets()\n\n    def _construct_backbone_network_name(self, backbone_network_id) -> str:\n        \"\"\"\n        Construct the name of the backbone network based on the given backbone network id from TOPOLOGY. If the\n        backbone_network_id is not defined in TOPOLOGY, use the default backbone network id.\n        \"\"\"\n        id = backbone_network_id if backbone_network_id is not None else config.BACKBONE_DOCKER_NETWORK_DEFAULT_ID\n        backbone_name = f'{config.BACKBONE_DOCKER_NETWORK_NAME}.{id}'\n        assert backbone_name in self._backbone_network_names\n        return backbone_name\n\n    def _output_test_info(self):\n        \"\"\"\n        Output test info to json file after tearDown\n        \"\"\"\n        filename = f'{self.test_name}.json'\n        with open(filename, 'wt') as ofd:\n            ofd.write(json.dumps(self._test_info, indent=1, sort_keys=True))\n        return filename\n\n    def _get_thread_pcap_filename(self):\n        current_pcap = self.test_name + '.pcap'\n        return os.path.abspath(current_pcap)\n\n    def assure_run_ok(self, cmd, shell=False):\n        if not shell and isinstance(cmd, str):\n            cmd = cmd.split()\n        proc = subprocess.run(cmd, stdout=sys.stdout, stderr=sys.stderr, shell=shell)\n        print('>>> %s => %d' % (cmd, proc.returncode), file=sys.stderr)\n        proc.check_returncode()\n\n    def _parse_params(self, params: Optional[dict]) -> dict:\n        params = params or {}\n        if params.get('is_bbr') or params.get('is_otbr'):\n            version = params.get('version', '1.4')\n            assert version != '1.1', params\n            params['version'] = version\n            params.setdefault('bbr_registration_jitter', config.DEFAULT_BBR_REGISTRATION_JITTER)\n        elif params.get('is_host'):\n            assert params.get('version', '') == '', params\n            params['version'] = ''\n        if params.get('version') == '1.2':\n            params['version'] = '1.4'\n        is_ftd = not params.get('is_mtd') and (not params.get('is_host'))\n        effective_params = DEFAULT_PARAMS.copy()\n        if is_ftd:\n            effective_params.update(FTD_DEFAULT_PARAMS)\n        effective_params.update(params)\n        return effective_params\n\n    def _has_backbone_traffic(self):\n        for param in self.TOPOLOGY.values():\n            if param and (param.get('is_otbr') or param.get('is_host')):\n                return True\n        return False\n\n    def _prepare_backbone_network(self):\n        \"\"\"\n        Creates one or more backbone networks (Docker bridge networks) based on the TOPOLOGY definition.\n\n        * If `backbone_network_id` is defined in the TOPOLOGY:\n            * Network name:   `backbone{PORT_OFFSET}.{backbone_network_id}`      (e.g., \"backbone0.0\", \"backbone0.1\")\n            * Network prefix: `backbone{PORT_OFFSET}:{backbone_network_id}::/64` (e.g., \"9100:0::/64\", \"9100:1::/64\")\n\n        * If `backbone_network_id` is undefined:\n            * Network name:   `backbone{PORT_OFFSET}.0`    (e.g., \"backbone0.0\")\n            * Network prefix: `backbone{PORT_OFFSET}::/64` (e.g., \"9100::/64\")\n        \"\"\"\n        backbone_id_set = set()\n        for node in self.TOPOLOGY:\n            id = self.TOPOLOGY[node].get('backbone_network_id')\n            if id is not None:\n                backbone_id_set.add(id)\n        if not backbone_id_set:\n            backbone_id_set.add(config.BACKBONE_DOCKER_NETWORK_DEFAULT_ID)\n        for id in backbone_id_set:\n            backbone = f'{config.BACKBONE_DOCKER_NETWORK_NAME}.{id}'\n            backbone_prefix = f'{config.BACKBONE_IPV6_ADDR_START}:{id}::/64'\n            self._backbone_network_names.append(backbone)\n            self.assure_run_ok(f'docker network create --driver bridge --ipv6 --subnet {backbone_prefix} -o \"com.docker.network.bridge.name\"=\"{backbone}\" {backbone} || true', shell=True)\n\n    def _remove_backbone_network(self):\n        for network_name in self._backbone_network_names:\n            self.assure_run_ok(f'docker network rm {network_name}', shell=True)\n\n    def _start_backbone_sniffer(self):\n        assert self._backbone_network_names, 'Internal Error: self._backbone_network_names is empty'\n        sniffer_interface = self._backbone_network_names[0]\n        pcap_file = self._get_backbone_pcap_filename()\n        try:\n            os.remove(pcap_file)\n        except FileNotFoundError:\n            pass\n        dumpcap = pvutils.which_dumpcap()\n        self._dumpcap_proc = subprocess.Popen([dumpcap, '-i', sniffer_interface, '-w', pcap_file], stdout=sys.stdout, stderr=sys.stderr)\n        time.sleep(0.2)\n        assert self._dumpcap_proc.poll() is None, 'tshark terminated unexpectedly'\n        logging.info('Backbone sniffer launched successfully on interface %s, pid=%s', sniffer_interface, self._dumpcap_proc.pid)\n        os.chmod(pcap_file, stat.S_IWUSR | stat.S_IRUSR | stat.S_IRGRP | stat.S_IROTH)\n\n    def _get_backbone_pcap_filename(self):\n        backbone_pcap = self.test_name + '_backbone.pcap'\n        return os.path.abspath(backbone_pcap)\n\n    def _get_merged_pcap_filename(self):\n        backbone_pcap = self.test_name + '_merged.pcap'\n        return os.path.abspath(backbone_pcap)\n\n    def _stop_backbone_sniffer(self):\n        self._dumpcap_proc.send_signal(signal.SIGTERM)\n        self._dumpcap_proc.__exit__(None, None, None)\n        logging.info('Backbone sniffer terminated successfully: pid=%s' % self._dumpcap_proc.pid)\n\n    def _merge_thread_backbone_pcaps(self):\n        thread_pcap = self._get_thread_pcap_filename()\n        backbone_pcap = self._get_backbone_pcap_filename()\n        merged_pcap = self._get_merged_pcap_filename()\n        mergecap = pvutils.which_mergecap()\n        self.assure_run_ok(f'{mergecap} -w {merged_pcap} {thread_pcap} {backbone_pcap}', shell=True)\n        return merged_pcap\n\n    def wait_until(self, cond: Callable[[], bool], timeout: int, go_interval: int=1):\n        while True:\n            self.simulator.go(go_interval)\n            if cond():\n                break\n            timeout -= go_interval\n            if timeout <= 0:\n                raise RuntimeError(f'wait failed after {timeout} seconds')\n\n    def wait_node_state(self, node: Union[int, Node], state: str, timeout: int):\n        node = self.nodes[node] if isinstance(node, int) else node\n        self.wait_until(lambda: node.get_state() == state, timeout)\n\n    def wait_route_established(self, node1: int, node2: int, timeout=10):\n        node2_addr = self.nodes[node2].get_ip6_address(config.ADDRESS_TYPE.RLOC)\n        while timeout > 0:\n            if self.nodes[node1].ping(node2_addr):\n                break\n            self.simulator.go(1)\n            timeout -= 1\n        else:\n            raise Exception('Route between node %d and %d is not established' % (node1, node2))\n\n    def assertDictIncludes(self, actual: Mapping[str, str], expected: Mapping[str, str]):\n        \"\"\" Asserts the `actual` dict includes the `expected` dict.\n\n        Args:\n            actual: A dict for checking.\n            expected: The expected items that the actual dict should contains.\n        \"\"\"\n        for k, v in expected.items():\n            if k not in actual:\n                raise AssertionError(f'key {k} is not found in first dict')\n            if v != actual[k]:\n                raise AssertionError(f'{repr(actual[k])} != {repr(v)} for key {k}')\n\nNow, implement the following class. Do not explain the code. The given class skeleton is as follows:\nclass ObservabilityManager:\n    '''\n    Manages observability callbacks for MCP agents.\n    This class provides a centralized way to collect and manage callbacks\n    from various observability platforms (Langfuse, Laminar, etc.).\n    '''\n\n    def __init__(self, custom_callbacks: list | None=None):\n        '''\n        Initialize the ObservabilityManager.\n        Args:\n            custom_callbacks: Optional list of custom callbacks to use instead of defaults.\n        '''\n        pass\n\n    def _collect_available_handlers(self) -> None:\n        '''Collect all available observability handlers from configured platforms.'''\n        pass\n\n    def get_callbacks(self) -> list:\n        '''\n        Get the list of callbacks to use.\n        Returns:\n            List of callbacks - either custom callbacks if provided,\n            or all available observability handlers.\n        '''\n        pass\n\n    def get_handler_names(self) -> list[str]:\n        '''\n        Get the names of available handlers.\n        Returns:\n            List of handler names (e.g., [\"Langfuse\", \"Laminar\"])\n        '''\n        pass\n\n    def has_callbacks(self) -> bool:\n        '''\n        Check if any callbacks are available.\n        Returns:\n            True if callbacks are available, False otherwise.\n        '''\n        pass\n\n    def add_callback(self, callback) -> None:\n        '''\n        Add a callback to the custom callbacks list.\n        Args:\n            callback: The callback to add.\n        '''\n        pass\n\n    def clear_callbacks(self) -> None:\n        '''Clear all custom callbacks.'''\n        pass\n\n    def __repr__(self) -> str:\n        '''String representation of the ObservabilityManager.'''\n        pass"
  }
]