[
  {
    "role": "assistant",
    "content": "You are an expert Python programmer who can correctly implement complete Python classes based on the provided class skeleton."
  },
  {
    "role": "user",
    "content": "For your understanding, following are some (skeleton, code) examples:\n\nExample skeleton:\n\n\nclass Blip2Processor(ProcessorMixin):\n    '''\n    Constructs a BLIP-2 processor which wraps a BLIP image processor and an OPT/T5 tokenizer into a single processor.\n    [`BlipProcessor`] offers all the functionalities of [`BlipImageProcessor`] and [`AutoTokenizer`]. See the docstring\n    of [`~BlipProcessor.__call__`] and [`~BlipProcessor.decode`] for more information.\n    Args:\n        image_processor (`BlipImageProcessor`):\n            An instance of [`BlipImageProcessor`]. The image processor is a required input.\n        tokenizer (`AutoTokenizer`):\n            An instance of ['PreTrainedTokenizer`]. The tokenizer is a required input.\n        num_query_tokens (`int`, *optional*):\n            Number of tokens used by the Qformer as queries, should be same as in model's config.\n    '''\n\n    def __init__(self, image_processor, tokenizer, num_query_tokens=None, **kwargs):\n        pass\n\n    def __call__(self, images: Optional[ImageInput]=None, text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]]=None, audio=None, videos=None, **kwargs: Unpack[Blip2ProcessorKwargs]) -> BatchEncoding:\n        '''\n        This method uses [`BlipImageProcessor.__call__`] method to prepare image(s) for the model, and\n        [`BertTokenizerFast.__call__`] to prepare text for the model.\n        Please refer to the docstring of the above two methods for more information.\n        Args:\n            images (`ImageInput`):\n                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                tensor. Both channels-first and channels-last formats are supported.\n            text (`TextInput`, `PreTokenizedInput`, `list[TextInput]`, `list[PreTokenizedInput]`):\n                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                If set, will return tensors of a particular framework. Acceptable values are:\n                    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n                    - `'np'`: Return NumPy `np.ndarray` objects.\n        '''\n        pass\nCorresponding class implementation:\n\nfrom ...image_utils import ImageInput\nfrom ...image_processing_utils import BatchFeature\nfrom typing import Optional, Union\nfrom ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\nfrom ...tokenization_utils_base import AddedToken, BatchEncoding, PreTokenizedInput, TextInput\n\nclass Blip2Processor(ProcessorMixin):\n    \"\"\"\n    Constructs a BLIP-2 processor which wraps a BLIP image processor and an OPT/T5 tokenizer into a single processor.\n\n    [`BlipProcessor`] offers all the functionalities of [`BlipImageProcessor`] and [`AutoTokenizer`]. See the docstring\n    of [`~BlipProcessor.__call__`] and [`~BlipProcessor.decode`] for more information.\n\n    Args:\n        image_processor (`BlipImageProcessor`):\n            An instance of [`BlipImageProcessor`]. The image processor is a required input.\n        tokenizer (`AutoTokenizer`):\n            An instance of ['PreTrainedTokenizer`]. The tokenizer is a required input.\n        num_query_tokens (`int`, *optional*):\n            Number of tokens used by the Qformer as queries, should be same as in model's config.\n    \"\"\"\n    attributes = ['image_processor', 'tokenizer']\n    image_processor_class = ('BlipImageProcessor', 'BlipImageProcessorFast')\n    tokenizer_class = 'AutoTokenizer'\n\n    def __init__(self, image_processor, tokenizer, num_query_tokens=None, **kwargs):\n        tokenizer.return_token_type_ids = False\n        self.current_processor = image_processor\n        if not hasattr(tokenizer, 'image_token'):\n            self.image_token = AddedToken('<image>', normalized=False, special=True)\n            tokenizer.add_tokens([self.image_token], special_tokens=True)\n        else:\n            self.image_token = tokenizer.image_token\n        self.num_query_tokens = num_query_tokens\n        super().__init__(image_processor, tokenizer)\n\n    def __call__(self, images: Optional[ImageInput]=None, text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]]=None, audio=None, videos=None, **kwargs: Unpack[Blip2ProcessorKwargs]) -> BatchEncoding:\n        \"\"\"\n        This method uses [`BlipImageProcessor.__call__`] method to prepare image(s) for the model, and\n        [`BertTokenizerFast.__call__`] to prepare text for the model.\n\n        Please refer to the docstring of the above two methods for more information.\n        Args:\n            images (`ImageInput`):\n                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                tensor. Both channels-first and channels-last formats are supported.\n            text (`TextInput`, `PreTokenizedInput`, `list[TextInput]`, `list[PreTokenizedInput]`):\n                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                If set, will return tensors of a particular framework. Acceptable values are:\n                    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n                    - `'np'`: Return NumPy `np.ndarray` objects.\n        \"\"\"\n        if images is None and text is None:\n            raise ValueError('You have to specify either images or text.')\n        output_kwargs = self._merge_kwargs(Blip2ProcessorKwargs, tokenizer_init_kwargs=self.tokenizer.init_kwargs, **kwargs)\n        return_tensors = output_kwargs['text_kwargs'].pop('return_tensors', None)\n        max_length = output_kwargs['text_kwargs'].pop('max_length', None)\n        if max_length is not None:\n            output_kwargs['text_kwargs']['max_length'] = max_length - self.num_query_tokens\n        encoding = BatchFeature(tensor_type=return_tensors)\n        if text is not None:\n            if isinstance(text, str):\n                text = [text]\n            elif not isinstance(text, list) and (not isinstance(text[0], str)):\n                raise ValueError('Invalid input text. Please provide a string, or a list of strings')\n            text_encoding = self.tokenizer(text, **output_kwargs['text_kwargs'])\n            if images is not None and self.num_query_tokens is not None:\n                image_tokens = self.image_token.content * self.num_query_tokens\n                output_kwargs['text_kwargs']['add_special_tokens'] = False\n                output_kwargs['text_kwargs']['padding'] = False\n                output_kwargs['text_kwargs']['truncation'] = False\n                image_text_encoding = self.tokenizer(image_tokens, **output_kwargs['text_kwargs'])\n                for k in text_encoding:\n                    text_encoding[k] = [image_text_encoding[k] + sample for sample in text_encoding[k]]\n            encoding.update(text_encoding)\n        if images is not None:\n            image_encoding = self.image_processor(images, **output_kwargs['images_kwargs'])\n            encoding.update(image_encoding)\n        encoding = BatchFeature(encoding, tensor_type=return_tensors)\n        return encoding\n\nExample skeleton:\n\n\nclass PCAM(VisionDataset):\n    '''`PCAM Dataset   <https://github.com/basveeling/pcam>`_.\n    The PatchCamelyon dataset is a binary classification dataset with 327,680\n    color images (96px x 96px), extracted from histopathologic scans of lymph node\n    sections. Each image is annotated with a binary label indicating presence of\n    metastatic tissue.\n    This dataset requires the ``h5py`` package which you can install with ``pip install h5py``.\n    Args:\n         root (str or ``pathlib.Path``): Root directory of the dataset.\n         split (string, optional): The dataset split, supports ``\"train\"`` (default), ``\"test\"`` or ``\"val\"``.\n         transform (callable, optional): A function/transform that takes in a PIL image and returns a transformed\n             version. E.g, ``transforms.RandomCrop``.\n         target_transform (callable, optional): A function/transform that takes in the target and transforms it.\n         download (bool, optional): If True, downloads the dataset from the internet and puts it into ``root/pcam``. If\n             dataset is already downloaded, it is not downloaded again.\n             .. warning::\n                To download the dataset `gdown <https://github.com/wkentaro/gdown>`_ is required.\n    '''\n\n    def __init__(self, root: Union[str, pathlib.Path], split: str='train', transform: Optional[Callable]=None, target_transform: Optional[Callable]=None, download: bool=False):\n        pass\n\n    def __len__(self) -> int:\n        pass\n\n    def __getitem__(self, idx: int) -> tuple[Any, Any]:\n        pass\n\n    def _check_exists(self) -> bool:\n        pass\n\n    def _download(self) -> None:\n        pass\nCorresponding class implementation:\n\nfrom typing import Any, Callable, Optional, Union\nimport pathlib\nfrom .vision import VisionDataset\nfrom PIL import Image\nfrom .utils import _decompress, download_file_from_google_drive, verify_str_arg\n\nclass PCAM(VisionDataset):\n    \"\"\"`PCAM Dataset   <https://github.com/basveeling/pcam>`_.\n\n    The PatchCamelyon dataset is a binary classification dataset with 327,680\n    color images (96px x 96px), extracted from histopathologic scans of lymph node\n    sections. Each image is annotated with a binary label indicating presence of\n    metastatic tissue.\n\n    This dataset requires the ``h5py`` package which you can install with ``pip install h5py``.\n\n    Args:\n         root (str or ``pathlib.Path``): Root directory of the dataset.\n         split (string, optional): The dataset split, supports ``\"train\"`` (default), ``\"test\"`` or ``\"val\"``.\n         transform (callable, optional): A function/transform that takes in a PIL image and returns a transformed\n             version. E.g, ``transforms.RandomCrop``.\n         target_transform (callable, optional): A function/transform that takes in the target and transforms it.\n         download (bool, optional): If True, downloads the dataset from the internet and puts it into ``root/pcam``. If\n             dataset is already downloaded, it is not downloaded again.\n\n             .. warning::\n\n                To download the dataset `gdown <https://github.com/wkentaro/gdown>`_ is required.\n    \"\"\"\n    _FILES = {'train': {'images': ('camelyonpatch_level_2_split_train_x.h5', '1Ka0XfEMiwgCYPdTI-vv6eUElOBnKFKQ2', '1571f514728f59376b705fc836ff4b63'), 'targets': ('camelyonpatch_level_2_split_train_y.h5', '1269yhu3pZDP8UYFQs-NYs3FPwuK-nGSG', '35c2d7259d906cfc8143347bb8e05be7')}, 'test': {'images': ('camelyonpatch_level_2_split_test_x.h5', '1qV65ZqZvWzuIVthK8eVDhIwrbnsJdbg_', 'd8c2d60d490dbd479f8199bdfa0cf6ec'), 'targets': ('camelyonpatch_level_2_split_test_y.h5', '17BHrSrwWKjYsOgTMmoqrIjDy6Fa2o_gP', '60a7035772fbdb7f34eb86d4420cf66a')}, 'val': {'images': ('camelyonpatch_level_2_split_valid_x.h5', '1hgshYGWK8V-eGRy8LToWJJgDU_rXWVJ3', 'd5b63470df7cfa627aeec8b9dc0c066e'), 'targets': ('camelyonpatch_level_2_split_valid_y.h5', '1bH8ZRbhSVAhScTS0p9-ZzGnX91cHT3uO', '2b85f58b927af9964a4c15b8f7e8f179')}}\n\n    def __init__(self, root: Union[str, pathlib.Path], split: str='train', transform: Optional[Callable]=None, target_transform: Optional[Callable]=None, download: bool=False):\n        try:\n            import h5py\n            self.h5py = h5py\n        except ImportError:\n            raise RuntimeError('h5py is not found. This dataset needs to have h5py installed: please run pip install h5py')\n        self._split = verify_str_arg(split, 'split', ('train', 'test', 'val'))\n        super().__init__(root, transform=transform, target_transform=target_transform)\n        self._base_folder = pathlib.Path(self.root) / 'pcam'\n        if download:\n            self._download()\n        if not self._check_exists():\n            raise RuntimeError('Dataset not found. You can use download=True to download it')\n\n    def __len__(self) -> int:\n        images_file = self._FILES[self._split]['images'][0]\n        with self.h5py.File(self._base_folder / images_file) as images_data:\n            return images_data['x'].shape[0]\n\n    def __getitem__(self, idx: int) -> tuple[Any, Any]:\n        images_file = self._FILES[self._split]['images'][0]\n        with self.h5py.File(self._base_folder / images_file) as images_data:\n            image = Image.fromarray(images_data['x'][idx]).convert('RGB')\n        targets_file = self._FILES[self._split]['targets'][0]\n        with self.h5py.File(self._base_folder / targets_file) as targets_data:\n            target = int(targets_data['y'][idx, 0, 0, 0])\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            target = self.target_transform(target)\n        return (image, target)\n\n    def _check_exists(self) -> bool:\n        images_file = self._FILES[self._split]['images'][0]\n        targets_file = self._FILES[self._split]['targets'][0]\n        return all((self._base_folder.joinpath(h5_file).exists() for h5_file in (images_file, targets_file)))\n\n    def _download(self) -> None:\n        if self._check_exists():\n            return\n        for file_name, file_id, md5 in self._FILES[self._split].values():\n            archive_name = file_name + '.gz'\n            download_file_from_google_drive(file_id, str(self._base_folder), filename=archive_name, md5=md5)\n            _decompress(str(self._base_folder / archive_name))\n\nNow, implement the following class. Do not explain the code. The given class skeleton is as follows:\n\nclass SyntheticDataGenerator:\n    '''\n    A class for generating synthetic images and videos based on text prompts.\n    This class supports different prompt generation strategies and can utilize\n    various text-to-video (t2v) and text-to-image (t2i) models.\n    Attributes:\n        use_random_model: Whether to randomly select a t2v or t2i for each\n            generation task.\n        prompt_type: The type of prompt generation strategy ('random', 'annotation').\n        prompt_generator_name: Name of the prompt generation model.\n        model_name: Name of the t2v, t2i, or i2i model.\n        prompt_generator: The vlm/llm pipeline for generating input prompts for t2i/t2v models\n        output_dir: Directory to write generated data.\n    '''\n\n    def __init__(self, model_name: Optional[str]=None, use_random_model: bool=True, prompt_type: str='annotation', output_dir: Optional[Union[str, Path]]=None, image_cache: Optional[ImageCache]=None, device: str='cuda') -> None:\n        '''\n        Initialize the SyntheticDataGenerator.\n        Args:\n            model_name: Name of the generative image/video model\n            use_random_model: Whether to randomly select models for generation.\n            prompt_type: The type of prompt generation strategy.\n            output_dir: Directory to write generated data.\n            device: Device identifier.\n            image_cache: Optional image cache instance.\n        Raises:\n            ValueError: If an invalid model name is provided.\n            NotImplementedError: If an unsupported prompt type is specified.\n        '''\n        pass\n\n    def batch_generate(self, batch_size: int=5) -> None:\n        '''\n        Asynchronously generate synthetic data in batches.\n        Args:\n            batch_size: Number of prompts to generate in each batch.\n        '''\n        pass\n\n    def generate(self, image: Optional[Image.Image]=None, task: Optional[str]=None, model_name: Optional[str]=None) -> Dict[str, Any]:\n        '''\n        Generate synthetic data based on input parameters.\n        Args:\n            image: Input image for annotation-based generation.\n            modality: Type of media to generate ('image' or 'video').\n        Returns:\n            Dictionary containing generated data information.\n        Raises:\n            ValueError: If real_image is None when using annotation prompt type.\n            NotImplementedError: If prompt type is not supported.\n        '''\n        pass\n\n    def generate_prompt(self, image: Optional[Image.Image]=None, clear_gpu: bool=True) -> str:\n        '''Generate a prompt based on the specified strategy.'''\n        pass\n\n    def _run_generation(self, prompt: str, task: Optional[str]=None, model_name: Optional[str]=None, image: Optional[Image.Image]=None, label: Optional[int]=None, generate_at_target_size: bool=False) -> Dict[str, Any]:\n        '''\n        Generate synthetic data based on a text prompt.\n        Args:\n            prompt: The text prompt used to inspire the generation.\n            task: The generation task type ('t2i', 't2v', 'i2i', or None).\n            model_name: Optional model name to use for generation.\n            image: Optional input image for image-to-image generation.\n            generate_at_target_size: If True, generate at TARGET_IMAGE_SIZE dimensions.\n        Returns:\n            Dictionary containing generated data and metadata.\n        Raises:\n            RuntimeError: If generation fails.\n        '''\n        pass\n\n    def load_model(self, model_name: Optional[str]=None, modality: Optional[str]=None) -> None:\n        '''Load a Hugging Face text-to-image or text-to-video model.'''\n        pass\n\n    def clear_gpu(self) -> None:\n        '''Clear GPU memory by deleting models and running garbage collection.'''\n        pass\n\n    def unload_model(self) -> None:\n        '''Completely unload the current model from memory.'''\n        pass"
  }
]