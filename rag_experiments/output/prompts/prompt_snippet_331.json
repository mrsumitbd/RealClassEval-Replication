[
  {
    "role": "assistant",
    "content": "You are an expert Python programmer who can correctly implement complete Python classes based on the provided class skeleton."
  },
  {
    "role": "user",
    "content": "For your understanding, following are some (skeleton, code) examples:\n\nExample skeleton:\n\n\nclass InternalApiserverApi(object):\n    '''NOTE: This class is auto generated by OpenAPI Generator\n    Ref: https://openapi-generator.tech\n    Do not edit the class manually.\n        '''\n\n    def __init__(self, api_client=None):\n        pass\n\n    def get_api_group(self, **kwargs):\n        '''get_api_group  # noqa: E501\n        get information of a group  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_api_group(async_req=True)\n        >>> result = thread.get()\n        :param async_req bool: execute request asynchronously\n        :param _preload_content: if False, the urllib3.HTTPResponse object will\n                                 be returned without reading/decoding response\n                                 data. Default is True.\n        :param _request_timeout: timeout setting for this request. If one\n                                 number provided, it will be total request\n                                 timeout. It can also be a pair (tuple) of\n                                 (connection, read) timeouts.\n        :return: V1APIGroup\n                 If the method is called asynchronously,\n                 returns the request thread.\n        '''\n        pass\n\n    def get_api_group_with_http_info(self, **kwargs):\n        '''get_api_group  # noqa: E501\n        get information of a group  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_api_group_with_http_info(async_req=True)\n        >>> result = thread.get()\n        :param async_req bool: execute request asynchronously\n        :param _return_http_data_only: response data without head status code\n                                       and headers\n        :param _preload_content: if False, the urllib3.HTTPResponse object will\n                                 be returned without reading/decoding response\n                                 data. Default is True.\n        :param _request_timeout: timeout setting for this request. If one\n                                 number provided, it will be total request\n                                 timeout. It can also be a pair (tuple) of\n                                 (connection, read) timeouts.\n        :return: tuple(V1APIGroup, status_code(int), headers(HTTPHeaderDict))\n                 If the method is called asynchronously,\n                 returns the request thread.\n        '''\n        pass\nCorresponding class implementation:\n\nimport six\nfrom kubernetes.client.api_client import ApiClient\nfrom kubernetes.client.exceptions import ApiTypeError, ApiValueError\n\nclass InternalApiserverApi(object):\n    \"\"\"NOTE: This class is auto generated by OpenAPI Generator\n    Ref: https://openapi-generator.tech\n\n    Do not edit the class manually.\n    \"\"\"\n\n    def __init__(self, api_client=None):\n        if api_client is None:\n            api_client = ApiClient()\n        self.api_client = api_client\n\n    def get_api_group(self, **kwargs):\n        \"\"\"get_api_group  # noqa: E501\n\n        get information of a group  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_api_group(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool: execute request asynchronously\n        :param _preload_content: if False, the urllib3.HTTPResponse object will\n                                 be returned without reading/decoding response\n                                 data. Default is True.\n        :param _request_timeout: timeout setting for this request. If one\n                                 number provided, it will be total request\n                                 timeout. It can also be a pair (tuple) of\n                                 (connection, read) timeouts.\n        :return: V1APIGroup\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        return self.get_api_group_with_http_info(**kwargs)\n\n    def get_api_group_with_http_info(self, **kwargs):\n        \"\"\"get_api_group  # noqa: E501\n\n        get information of a group  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_api_group_with_http_info(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool: execute request asynchronously\n        :param _return_http_data_only: response data without head status code\n                                       and headers\n        :param _preload_content: if False, the urllib3.HTTPResponse object will\n                                 be returned without reading/decoding response\n                                 data. Default is True.\n        :param _request_timeout: timeout setting for this request. If one\n                                 number provided, it will be total request\n                                 timeout. It can also be a pair (tuple) of\n                                 (connection, read) timeouts.\n        :return: tuple(V1APIGroup, status_code(int), headers(HTTPHeaderDict))\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        local_var_params = locals()\n        all_params = []\n        all_params.extend(['async_req', '_return_http_data_only', '_preload_content', '_request_timeout'])\n        for key, val in six.iteritems(local_var_params['kwargs']):\n            if key not in all_params:\n                raise ApiTypeError(\"Got an unexpected keyword argument '%s' to method get_api_group\" % key)\n            local_var_params[key] = val\n        del local_var_params['kwargs']\n        collection_formats = {}\n        path_params = {}\n        query_params = []\n        header_params = {}\n        form_params = []\n        local_var_files = {}\n        body_params = None\n        header_params['Accept'] = self.api_client.select_header_accept(['application/json', 'application/yaml', 'application/vnd.kubernetes.protobuf'])\n        auth_settings = ['BearerToken']\n        return self.api_client.call_api('/apis/internal.apiserver.k8s.io/', 'GET', path_params, query_params, header_params, body=body_params, post_params=form_params, files=local_var_files, response_type='V1APIGroup', auth_settings=auth_settings, async_req=local_var_params.get('async_req'), _return_http_data_only=local_var_params.get('_return_http_data_only'), _preload_content=local_var_params.get('_preload_content', True), _request_timeout=local_var_params.get('_request_timeout'), collection_formats=collection_formats)\n\nExample skeleton:\n\n\nclass MllamaProcessor(ProcessorMixin):\n    '''\n    Constructs a Mllama processor which wraps [`MllamaImageProcessor`] and\n    [`PretrainedTokenizerFast`] into a single processor that inherits both the image processor and\n    tokenizer functionalities. See the [`~MllamaProcessor.__call__`] and [`~OwlViTProcessor.decode`] for more\n    information.\n    The preferred way of passing kwargs is as a dictionary per modality, see usage example below.\n        ```python\n        from transformers import MllamaProcessor\n        from PIL import Image\n        processor = MllamaProcessor.from_pretrained(\"meta-llama/Llama-3.2-11B-Vision\")\n        processor(\n            images=your_pil_image,\n            text=[\"<|image|>If I had to write a haiku for this one\"],\n            images_kwargs = {\"size\": {\"height\": 448, \"width\": 448}},\n            text_kwargs = {\"padding\": \"right\"},\n            common_kwargs = {\"return_tensors\": \"pt\"},\n        )\n        ```\n    Args:\n        image_processor ([`MllamaImageProcessor`]):\n            The image processor is a required input.\n        tokenizer ([`PreTrainedTokenizer`, `PreTrainedTokenizerFast`]):\n            The tokenizer is a required input.\n        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n            in a chat into a tokenizable string.\n    '''\n\n    def __init__(self, image_processor, tokenizer, chat_template=None):\n        pass\n\n    def __call__(self, images: Optional[ImageInput]=None, text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]]=None, audio=None, videos=None, **kwargs: Unpack[MllamaProcessorKwargs]) -> BatchFeature:\n        '''\n        Main method to prepare text(s) and image(s) to be fed as input to the model. This method forwards the `text`\n        arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] if `text` is not `None` to encode\n        the text. To prepare the image(s), this method forwards the `images` arguments to\n        MllamaImageProcessor's [`~MllamaImageProcessor.__call__`] if `images` is not `None`. Please refer\n        to the docstring of the above two methods for more information.\n        Args:\n            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                tensor. Both channels-first and channels-last formats are supported.\n            text (`str`, `list[str]`, `list[list[str]]`):\n                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                If set, will return tensors of a particular framework. Acceptable values are:\n                    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n                    - `'np'`: Return NumPy `np.ndarray` objects.\n        Returns:\n            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n              `None`).\n            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n            TODO: add aspect_ratio_ids and aspect_ratio_mask and cross_attention_mask\n        '''\n        pass\n\n    def post_process_image_text_to_text(self, generated_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False, **kwargs):\n        '''\n        Post-process the output of the model to decode the text.\n        Args:\n            generated_outputs (`torch.Tensor` or `np.ndarray`):\n                The output of the model `generate` function. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n                or `(sequence_length,)`.\n            skip_special_tokens (`bool`, *optional*, defaults to `True`):\n                Whether or not to remove special tokens in the output. Argument passed to the tokenizer's `batch_decode` method.\n            clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n                Whether or not to clean up the tokenization spaces. Argument passed to the tokenizer's `batch_decode` method.\n            **kwargs:\n                Additional arguments to be passed to the tokenizer's `batch_decode method`.\n        Returns:\n            `list[str]`: The decoded text.\n        '''\n        pass\n    @property\n    def model_input_names(self):\n        pass\nCorresponding class implementation:\n\nfrom ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\nfrom ...feature_extraction_utils import BatchFeature\nfrom ...tokenization_utils_base import PreTokenizedInput, TextInput\nfrom ...image_utils import ImageInput, make_nested_list_of_images\nfrom typing import Optional, Union\n\nclass MllamaProcessor(ProcessorMixin):\n    \"\"\"\n    Constructs a Mllama processor which wraps [`MllamaImageProcessor`] and\n    [`PretrainedTokenizerFast`] into a single processor that inherits both the image processor and\n    tokenizer functionalities. See the [`~MllamaProcessor.__call__`] and [`~OwlViTProcessor.decode`] for more\n    information.\n    The preferred way of passing kwargs is as a dictionary per modality, see usage example below.\n        ```python\n        from transformers import MllamaProcessor\n        from PIL import Image\n\n        processor = MllamaProcessor.from_pretrained(\"meta-llama/Llama-3.2-11B-Vision\")\n\n        processor(\n            images=your_pil_image,\n            text=[\"<|image|>If I had to write a haiku for this one\"],\n            images_kwargs = {\"size\": {\"height\": 448, \"width\": 448}},\n            text_kwargs = {\"padding\": \"right\"},\n            common_kwargs = {\"return_tensors\": \"pt\"},\n        )\n        ```\n\n    Args:\n        image_processor ([`MllamaImageProcessor`]):\n            The image processor is a required input.\n        tokenizer ([`PreTrainedTokenizer`, `PreTrainedTokenizerFast`]):\n            The tokenizer is a required input.\n        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n            in a chat into a tokenizable string.\n\n    \"\"\"\n    attributes = ['image_processor', 'tokenizer']\n    image_processor_class = 'MllamaImageProcessor'\n    tokenizer_class = 'PreTrainedTokenizerFast'\n\n    def __init__(self, image_processor, tokenizer, chat_template=None):\n        if not hasattr(tokenizer, 'image_token'):\n            self.image_token = '<|image|>'\n            self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)\n        else:\n            self.image_token = tokenizer.image_token\n            self.image_token_id = tokenizer.image_token_id\n        self.python_token = '<|python_tag|>'\n        self.python_token_id = tokenizer.convert_tokens_to_ids(self.python_token)\n        self.bos_token = tokenizer.bos_token\n        super().__init__(image_processor, tokenizer, chat_template=chat_template)\n\n    def __call__(self, images: Optional[ImageInput]=None, text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]]=None, audio=None, videos=None, **kwargs: Unpack[MllamaProcessorKwargs]) -> BatchFeature:\n        \"\"\"\n        Main method to prepare text(s) and image(s) to be fed as input to the model. This method forwards the `text`\n        arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] if `text` is not `None` to encode\n        the text. To prepare the image(s), this method forwards the `images` arguments to\n        MllamaImageProcessor's [`~MllamaImageProcessor.__call__`] if `images` is not `None`. Please refer\n        to the docstring of the above two methods for more information.\n\n        Args:\n            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                tensor. Both channels-first and channels-last formats are supported.\n            text (`str`, `list[str]`, `list[list[str]]`):\n                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                If set, will return tensors of a particular framework. Acceptable values are:\n                    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n                    - `'np'`: Return NumPy `np.ndarray` objects.\n        Returns:\n            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n\n            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n              `None`).\n            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n            TODO: add aspect_ratio_ids and aspect_ratio_mask and cross_attention_mask\n        \"\"\"\n        if text is None and images is None:\n            raise ValueError('You must specify either text or images.')\n        output_kwargs = self._merge_kwargs(MllamaProcessorKwargs, tokenizer_init_kwargs=self.tokenizer.init_kwargs, **kwargs)\n        text_kwargs = output_kwargs['text_kwargs']\n        text_kwargs['return_tensors'] = None\n        images_kwargs = output_kwargs['images_kwargs']\n        common_kwargs = output_kwargs['common_kwargs']\n        data = {}\n        if text is not None:\n            if isinstance(text, str):\n                text = [text]\n            elif not (isinstance(text, (list, tuple)) and all((isinstance(t, str) for t in text))):\n                raise ValueError('Invalid input text. Please provide a string, or a list of strings')\n            n_images_in_text = [t.count(self.image_token) for t in text]\n            text = [build_string_from_input(text_item, self.bos_token, self.image_token) for text_item in text]\n            _ = text_kwargs.pop('padding_side', None)\n            encoding = self.tokenizer(text, **text_kwargs)\n            self._check_special_mm_tokens(text, encoding, modalities=['image'])\n            n_images_in_ids = [token_ids.count(self.image_token_id) for token_ids in encoding['input_ids']]\n            data.update(encoding)\n        n_images_in_images = [0]\n        if images is not None:\n            images = self.image_processor.fetch_images(images)\n            images = make_nested_list_of_images(images)\n            n_images_in_images = [len(sample) for sample in images]\n        if text is not None:\n            if any((batch_img == 0 for batch_img in n_images_in_text)) and (not all((batch_img == 0 for batch_img in n_images_in_text))):\n                raise ValueError('If a batch of text is provided, there should be either no images or at least one image per sample')\n            if sum(n_images_in_text) > 0 and (n_images_in_images != n_images_in_text or n_images_in_ids != n_images_in_images):\n                if images is None:\n                    raise ValueError('No image were provided, but there are image tokens in the prompt')\n                else:\n                    add_message = ''\n                    if sum(n_images_in_images) == sum(n_images_in_text) and n_images_in_images != n_images_in_text:\n                        add_message = 'Make sure to pass your images as a nested list, where each sub-list holds images per batch'\n                    elif n_images_in_ids != n_images_in_images:\n                        add_message = \"If you activated truncation with `max_length`, increase the `max_length` so image tokens aren't cropped.\"\n                    raise ValueError(f'The number of image tokens in each text ({n_images_in_text}) should be the same as the number of provided images per batch ({n_images_in_images}). {add_message}')\n        if images is not None:\n            image_features = self.image_processor(images, **images_kwargs)\n            num_tiles = image_features.pop('num_tiles')\n            data.update(image_features)\n        if images is not None and text is not None:\n            cross_attention_token_mask = [get_cross_attention_token_mask(token_ids, self.image_token_id) for token_ids in encoding['input_ids']]\n            cross_attention_mask = convert_sparse_cross_attention_mask_to_dense(cross_attention_token_mask, num_tiles=num_tiles, max_num_tiles=self.image_processor.max_image_tiles, length=max((len(input_ids) for input_ids in encoding['input_ids'])))\n            data['cross_attention_mask'] = cross_attention_mask\n        return_tensors = common_kwargs.pop('return_tensors', None)\n        batch_feature = BatchFeature(data=data, tensor_type=return_tensors)\n        return batch_feature\n\n    def post_process_image_text_to_text(self, generated_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False, **kwargs):\n        \"\"\"\n        Post-process the output of the model to decode the text.\n\n        Args:\n            generated_outputs (`torch.Tensor` or `np.ndarray`):\n                The output of the model `generate` function. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n                or `(sequence_length,)`.\n            skip_special_tokens (`bool`, *optional*, defaults to `True`):\n                Whether or not to remove special tokens in the output. Argument passed to the tokenizer's `batch_decode` method.\n            clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n                Whether or not to clean up the tokenization spaces. Argument passed to the tokenizer's `batch_decode` method.\n            **kwargs:\n                Additional arguments to be passed to the tokenizer's `batch_decode method`.\n\n        Returns:\n            `list[str]`: The decoded text.\n        \"\"\"\n        return self.tokenizer.batch_decode(generated_outputs, skip_special_tokens=skip_special_tokens, clean_up_tokenization_spaces=clean_up_tokenization_spaces, **kwargs)\n\n    @property\n    def model_input_names(self):\n        tokenizer_input_names = self.tokenizer.model_input_names\n        image_processor_input_names = self.image_processor.model_input_names\n        image_processor_input_names = [name for name in image_processor_input_names if name != 'num_tiles']\n        return list(tokenizer_input_names + image_processor_input_names + ['cross_attention_mask'])\n\nNow, implement the following class. Do not explain the code. The given class skeleton is as follows:\n\nclass AsyncAIRefinery:\n    '''\n    A top-level client that exposes various sub-clients in a single interface,\n    operating asynchronously.\n    Example usage:\n        client = AsyncAIRefinery(\n            api_key=\"...\",\n            base_url=\"...\",\n            default_headers={\"X-Client-Version\": \"1.2.3\"}\n        )\n        # Use chat\n        response = await client.chat.completions.create(\n            model=\"model-name\", messages=[...]\n        )\n        # Use embeddings\n        embeddings_response = await client.embeddings.create(\n            model=\"model-name\", input=[\"Hello\"]\n        )\n        # Use tts\n        tts_response = await client.audio.speech.create(\n            model=\"model-name\",\n            input=\"Hello, this is a test of text-to-speech synthesis.\",\n            voice=\"en-US-JennyNeural\",\n            response_format=\"mp3\",  # Optional\n            speed=1.0  # Optional\n        # Use asr\n        asr_response = await client.audio.transcriptions.create(\n            model=\"model-name\",\n            file=file\n        )\n        # Use models\n        models_list = await client.models.list()\n        # Use distiller\n        async with client.distiller(project=\"...\", uuid=\"...\") as dc:\n            responses = await dc.query(query=\"hi\")\n            async for response in responses:\n                print(response)\n        # Use images\n        embeddings_response = await client.images.generate(\n            prompt=\"A cute baby sea otter\", model=\"model-name\"\n        )\n        # Use knowledge\n        graph_client = client.graph\n        graph_client.create_project(graph_config=...)\n        status = await graph_client.build(files_path=..)\n    '''\n\n    def __init__(self, api_key: str, base_url: str='https://api.airefinery.accenture.com', default_headers: dict[str, str] | None=None, **kwargs):\n        '''\n        Initializes the asynchronous unified client with sub-clients.\n        Args:\n            api_key (str): Your API key or token for authenticated requests.\n            base_url (str, optional): Base URL for your API endpoints.\n                Defaults to \"https://api.airefinery.accenture.com\".\n            default_headers (dict[str, str] | None): Headers that apply to\n                every request made by sub-clients (e.g., {\"X-Client-Version\": \"1.2.3\"}).\n            **kwargs: Additional configuration parameters, if any.\n                        '''\n                        pass\n\n    def _update_inference_endpoint(self):\n        '''\n        Ensures the base_url ends with the '/inference' suffix if necessary.\n        This method checks whether the `base_url` ends with \"/inference\".\n        If it does not, the method appends \"/inference\" to the URL and returns\n        the updated URL. This is particularly useful for ensuring compatibility\n        with authentication mechanisms like auth.openai(), where the \"/inference\"\n        suffix is automatically appended to the base URL.\n        Returns:\n            str: The base URL with \"/inference\" appended if it was not already present.\n        '''\n        pass"
  }
]