[
  {
    "role": "assistant",
    "content": "You are an expert Python programmer who can correctly implement complete Python classes based on the provided class skeleton."
  },
  {
    "role": "user",
    "content": "For your understanding, following are some (skeleton, code) examples:\n\nExample skeleton:\n\n\nclass PCAM(VisionDataset):\n    '''`PCAM Dataset   <https://github.com/basveeling/pcam>`_.\n    The PatchCamelyon dataset is a binary classification dataset with 327,680\n    color images (96px x 96px), extracted from histopathologic scans of lymph node\n    sections. Each image is annotated with a binary label indicating presence of\n    metastatic tissue.\n    This dataset requires the ``h5py`` package which you can install with ``pip install h5py``.\n    Args:\n         root (str or ``pathlib.Path``): Root directory of the dataset.\n         split (string, optional): The dataset split, supports ``\"train\"`` (default), ``\"test\"`` or ``\"val\"``.\n         transform (callable, optional): A function/transform that takes in a PIL image and returns a transformed\n             version. E.g, ``transforms.RandomCrop``.\n         target_transform (callable, optional): A function/transform that takes in the target and transforms it.\n         download (bool, optional): If True, downloads the dataset from the internet and puts it into ``root/pcam``. If\n             dataset is already downloaded, it is not downloaded again.\n             .. warning::\n                To download the dataset `gdown <https://github.com/wkentaro/gdown>`_ is required.\n    '''\n\n    def __init__(self, root: Union[str, pathlib.Path], split: str='train', transform: Optional[Callable]=None, target_transform: Optional[Callable]=None, download: bool=False):\n        pass\n\n    def __len__(self) -> int:\n        pass\n\n    def __getitem__(self, idx: int) -> tuple[Any, Any]:\n        pass\n\n    def _check_exists(self) -> bool:\n        pass\n\n    def _download(self) -> None:\n        pass\nCorresponding class implementation:\n\nfrom typing import Any, Callable, Optional, Union\nimport pathlib\nfrom .vision import VisionDataset\nfrom PIL import Image\nfrom .utils import _decompress, download_file_from_google_drive, verify_str_arg\n\nclass PCAM(VisionDataset):\n    \"\"\"`PCAM Dataset   <https://github.com/basveeling/pcam>`_.\n\n    The PatchCamelyon dataset is a binary classification dataset with 327,680\n    color images (96px x 96px), extracted from histopathologic scans of lymph node\n    sections. Each image is annotated with a binary label indicating presence of\n    metastatic tissue.\n\n    This dataset requires the ``h5py`` package which you can install with ``pip install h5py``.\n\n    Args:\n         root (str or ``pathlib.Path``): Root directory of the dataset.\n         split (string, optional): The dataset split, supports ``\"train\"`` (default), ``\"test\"`` or ``\"val\"``.\n         transform (callable, optional): A function/transform that takes in a PIL image and returns a transformed\n             version. E.g, ``transforms.RandomCrop``.\n         target_transform (callable, optional): A function/transform that takes in the target and transforms it.\n         download (bool, optional): If True, downloads the dataset from the internet and puts it into ``root/pcam``. If\n             dataset is already downloaded, it is not downloaded again.\n\n             .. warning::\n\n                To download the dataset `gdown <https://github.com/wkentaro/gdown>`_ is required.\n    \"\"\"\n    _FILES = {'train': {'images': ('camelyonpatch_level_2_split_train_x.h5', '1Ka0XfEMiwgCYPdTI-vv6eUElOBnKFKQ2', '1571f514728f59376b705fc836ff4b63'), 'targets': ('camelyonpatch_level_2_split_train_y.h5', '1269yhu3pZDP8UYFQs-NYs3FPwuK-nGSG', '35c2d7259d906cfc8143347bb8e05be7')}, 'test': {'images': ('camelyonpatch_level_2_split_test_x.h5', '1qV65ZqZvWzuIVthK8eVDhIwrbnsJdbg_', 'd8c2d60d490dbd479f8199bdfa0cf6ec'), 'targets': ('camelyonpatch_level_2_split_test_y.h5', '17BHrSrwWKjYsOgTMmoqrIjDy6Fa2o_gP', '60a7035772fbdb7f34eb86d4420cf66a')}, 'val': {'images': ('camelyonpatch_level_2_split_valid_x.h5', '1hgshYGWK8V-eGRy8LToWJJgDU_rXWVJ3', 'd5b63470df7cfa627aeec8b9dc0c066e'), 'targets': ('camelyonpatch_level_2_split_valid_y.h5', '1bH8ZRbhSVAhScTS0p9-ZzGnX91cHT3uO', '2b85f58b927af9964a4c15b8f7e8f179')}}\n\n    def __init__(self, root: Union[str, pathlib.Path], split: str='train', transform: Optional[Callable]=None, target_transform: Optional[Callable]=None, download: bool=False):\n        try:\n            import h5py\n            self.h5py = h5py\n        except ImportError:\n            raise RuntimeError('h5py is not found. This dataset needs to have h5py installed: please run pip install h5py')\n        self._split = verify_str_arg(split, 'split', ('train', 'test', 'val'))\n        super().__init__(root, transform=transform, target_transform=target_transform)\n        self._base_folder = pathlib.Path(self.root) / 'pcam'\n        if download:\n            self._download()\n        if not self._check_exists():\n            raise RuntimeError('Dataset not found. You can use download=True to download it')\n\n    def __len__(self) -> int:\n        images_file = self._FILES[self._split]['images'][0]\n        with self.h5py.File(self._base_folder / images_file) as images_data:\n            return images_data['x'].shape[0]\n\n    def __getitem__(self, idx: int) -> tuple[Any, Any]:\n        images_file = self._FILES[self._split]['images'][0]\n        with self.h5py.File(self._base_folder / images_file) as images_data:\n            image = Image.fromarray(images_data['x'][idx]).convert('RGB')\n        targets_file = self._FILES[self._split]['targets'][0]\n        with self.h5py.File(self._base_folder / targets_file) as targets_data:\n            target = int(targets_data['y'][idx, 0, 0, 0])\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            target = self.target_transform(target)\n        return (image, target)\n\n    def _check_exists(self) -> bool:\n        images_file = self._FILES[self._split]['images'][0]\n        targets_file = self._FILES[self._split]['targets'][0]\n        return all((self._base_folder.joinpath(h5_file).exists() for h5_file in (images_file, targets_file)))\n\n    def _download(self) -> None:\n        if self._check_exists():\n            return\n        for file_name, file_id, md5 in self._FILES[self._split].values():\n            archive_name = file_name + '.gz'\n            download_file_from_google_drive(file_id, str(self._base_folder), filename=archive_name, md5=md5)\n            _decompress(str(self._base_folder / archive_name))\n\nExample skeleton:\n\n\nclass Blip2Processor(ProcessorMixin):\n    '''\n    Constructs a BLIP-2 processor which wraps a BLIP image processor and an OPT/T5 tokenizer into a single processor.\n    [`BlipProcessor`] offers all the functionalities of [`BlipImageProcessor`] and [`AutoTokenizer`]. See the docstring\n    of [`~BlipProcessor.__call__`] and [`~BlipProcessor.decode`] for more information.\n    Args:\n        image_processor (`BlipImageProcessor`):\n            An instance of [`BlipImageProcessor`]. The image processor is a required input.\n        tokenizer (`AutoTokenizer`):\n            An instance of ['PreTrainedTokenizer`]. The tokenizer is a required input.\n        num_query_tokens (`int`, *optional*):\n            Number of tokens used by the Qformer as queries, should be same as in model's config.\n    '''\n\n    def __init__(self, image_processor, tokenizer, num_query_tokens=None, **kwargs):\n        pass\n\n    def __call__(self, images: Optional[ImageInput]=None, text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]]=None, audio=None, videos=None, **kwargs: Unpack[Blip2ProcessorKwargs]) -> BatchEncoding:\n        '''\n        This method uses [`BlipImageProcessor.__call__`] method to prepare image(s) for the model, and\n        [`BertTokenizerFast.__call__`] to prepare text for the model.\n        Please refer to the docstring of the above two methods for more information.\n        Args:\n            images (`ImageInput`):\n                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                tensor. Both channels-first and channels-last formats are supported.\n            text (`TextInput`, `PreTokenizedInput`, `list[TextInput]`, `list[PreTokenizedInput]`):\n                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                If set, will return tensors of a particular framework. Acceptable values are:\n                    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n                    - `'np'`: Return NumPy `np.ndarray` objects.\n        '''\n        pass\nCorresponding class implementation:\n\nfrom ...image_utils import ImageInput\nfrom ...image_processing_utils import BatchFeature\nfrom typing import Optional, Union\nfrom ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\nfrom ...tokenization_utils_base import AddedToken, BatchEncoding, PreTokenizedInput, TextInput\n\nclass Blip2Processor(ProcessorMixin):\n    \"\"\"\n    Constructs a BLIP-2 processor which wraps a BLIP image processor and an OPT/T5 tokenizer into a single processor.\n\n    [`BlipProcessor`] offers all the functionalities of [`BlipImageProcessor`] and [`AutoTokenizer`]. See the docstring\n    of [`~BlipProcessor.__call__`] and [`~BlipProcessor.decode`] for more information.\n\n    Args:\n        image_processor (`BlipImageProcessor`):\n            An instance of [`BlipImageProcessor`]. The image processor is a required input.\n        tokenizer (`AutoTokenizer`):\n            An instance of ['PreTrainedTokenizer`]. The tokenizer is a required input.\n        num_query_tokens (`int`, *optional*):\n            Number of tokens used by the Qformer as queries, should be same as in model's config.\n    \"\"\"\n    attributes = ['image_processor', 'tokenizer']\n    image_processor_class = ('BlipImageProcessor', 'BlipImageProcessorFast')\n    tokenizer_class = 'AutoTokenizer'\n\n    def __init__(self, image_processor, tokenizer, num_query_tokens=None, **kwargs):\n        tokenizer.return_token_type_ids = False\n        self.current_processor = image_processor\n        if not hasattr(tokenizer, 'image_token'):\n            self.image_token = AddedToken('<image>', normalized=False, special=True)\n            tokenizer.add_tokens([self.image_token], special_tokens=True)\n        else:\n            self.image_token = tokenizer.image_token\n        self.num_query_tokens = num_query_tokens\n        super().__init__(image_processor, tokenizer)\n\n    def __call__(self, images: Optional[ImageInput]=None, text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]]=None, audio=None, videos=None, **kwargs: Unpack[Blip2ProcessorKwargs]) -> BatchEncoding:\n        \"\"\"\n        This method uses [`BlipImageProcessor.__call__`] method to prepare image(s) for the model, and\n        [`BertTokenizerFast.__call__`] to prepare text for the model.\n\n        Please refer to the docstring of the above two methods for more information.\n        Args:\n            images (`ImageInput`):\n                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                tensor. Both channels-first and channels-last formats are supported.\n            text (`TextInput`, `PreTokenizedInput`, `list[TextInput]`, `list[PreTokenizedInput]`):\n                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                If set, will return tensors of a particular framework. Acceptable values are:\n                    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n                    - `'np'`: Return NumPy `np.ndarray` objects.\n        \"\"\"\n        if images is None and text is None:\n            raise ValueError('You have to specify either images or text.')\n        output_kwargs = self._merge_kwargs(Blip2ProcessorKwargs, tokenizer_init_kwargs=self.tokenizer.init_kwargs, **kwargs)\n        return_tensors = output_kwargs['text_kwargs'].pop('return_tensors', None)\n        max_length = output_kwargs['text_kwargs'].pop('max_length', None)\n        if max_length is not None:\n            output_kwargs['text_kwargs']['max_length'] = max_length - self.num_query_tokens\n        encoding = BatchFeature(tensor_type=return_tensors)\n        if text is not None:\n            if isinstance(text, str):\n                text = [text]\n            elif not isinstance(text, list) and (not isinstance(text[0], str)):\n                raise ValueError('Invalid input text. Please provide a string, or a list of strings')\n            text_encoding = self.tokenizer(text, **output_kwargs['text_kwargs'])\n            if images is not None and self.num_query_tokens is not None:\n                image_tokens = self.image_token.content * self.num_query_tokens\n                output_kwargs['text_kwargs']['add_special_tokens'] = False\n                output_kwargs['text_kwargs']['padding'] = False\n                output_kwargs['text_kwargs']['truncation'] = False\n                image_text_encoding = self.tokenizer(image_tokens, **output_kwargs['text_kwargs'])\n                for k in text_encoding:\n                    text_encoding[k] = [image_text_encoding[k] + sample for sample in text_encoding[k]]\n            encoding.update(text_encoding)\n        if images is not None:\n            image_encoding = self.image_processor(images, **output_kwargs['images_kwargs'])\n            encoding.update(image_encoding)\n        encoding = BatchFeature(encoding, tensor_type=return_tensors)\n        return encoding\n\nNow, implement the following class. Do not explain the code. The given class skeleton is as follows:\n\nclass NunchakuTextEncoderLoaderV2:\n    '''\n    Node for loading Nunchaku text encoders. It also supports 16-bit and FP8 variants.\n    .. note::\n        When loading our 4-bit T5, a 16-bit T5 is first initialized on a meta device,\n        then replaced by the Nunchaku T5.\n    .. warning::\n        Our 4-bit T5 currently requires a CUDA device.\n        If not on CUDA, the model will be moved automatically, which may cause out-of-memory errors.\n        Turing GPUs (20-series) are not supported for now.\n    '''\n    @classmethod\n    def INPUT_TYPES(s):\n        '''\n        Defines the input types and tooltips for the node.\n        Returns\n        -------\n        dict\n            A dictionary specifying the required inputs and their descriptions for the node interface.\n        '''\n        pass\n\n    def load_text_encoder(self, model_type: str, text_encoder1: str, text_encoder2: str, t5_min_length: int):\n        '''\n        Loads the text encoders with the given configuration.\n        Parameters\n        ----------\n        model_type : str\n            The type of model to load (e.g., \"flux.1\").\n        text_encoder1 : str\n            Filename of the first text encoder checkpoint.\n        text_encoder2 : str\n            Filename of the second text encoder checkpoint.\n        t5_min_length : int\n            Minimum sequence length for the T5 encoder.\n        Returns\n        -------\n        tuple\n            Tuple containing the loaded CLIP model.\n        '''\n        pass"
  }
]