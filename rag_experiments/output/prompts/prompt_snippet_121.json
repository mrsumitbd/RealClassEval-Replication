[
  {
    "role": "assistant",
    "content": "You are an expert Python programmer who can correctly implement complete Python classes based on the provided class skeleton."
  },
  {
    "role": "user",
    "content": "For your understanding, following are some (skeleton, code) examples:\n\nExample skeleton:\n\n\nclass SparkModel(object):\n\n    def __init__(self, model, mode='asynchronous', frequency='epoch', parameter_server_mode='http', num_workers=None, custom_objects=None, batch_size=32, port=4000, *args, **kwargs):\n        '''SparkModel\n        Base class for distributed training on RDDs. Spark model takes a Keras\n        model as master network, an optimization scheme, a parallelisation mode\n        and an averaging frequency.\n        :param model: Compiled Keras model\n        :param mode: String, choose from `asynchronous`, `synchronous` and `hogwild`\n        :param frequency: String, either `epoch` or `batch`\n        :param parameter_server_mode: String, either `http` or `socket`\n        :param num_workers: int, number of workers used for training (defaults to None)\n        :param custom_objects: Keras custom objects\n        :param batch_size: batch size used for training and inference\n        :param port: port used in case of 'http' parameter server mode\n        '''\n        pass\n\n    def get_config(self):\n        pass\n\n    def save(self, file_name):\n        pass\n    @property\n    def training_histories(self):\n        pass\n    @property\n    def master_network(self):\n        pass\n    @master_network.setter\n    def master_network(self):\n        pass\n\n    def start_server(self):\n        pass\n\n    def stop_server(self):\n        pass\n\n    def predict(self, data: Union[RDD, np.array]):\n        '''Get prediction probabilities for a numpy array of features\n        '''\n        pass\n\n    def evaluate(self, x_test, y_test, **kwargs):\n        pass\n\n    def fit(self, rdd: RDD, **kwargs):\n        '''\n        Train an elephas model on an RDD. The Keras model configuration as specified\n        in the elephas model is sent to Spark workers, abd each worker will be trained\n        on their data partition.\n        :param rdd: RDD with features and labels\n        :param epochs: number of epochs used for training\n        :param batch_size: batch size used for training\n        :param verbose: logging verbosity level (0, 1 or 2)\n        :param validation_split: percentage of data set aside for validation\n        '''\n        pass\n\n    def _fit(self, rdd: RDD, **kwargs):\n        '''Protected train method to make wrapping of modes easier\n        '''\n        pass\n\n    def _predict(self, rdd: RDD):\n        pass\n\n    def _predict(self, rdd: RDD):\n        pass\n\n        def _predict_with_indices(model_as_json, custom_objects, data):\n                pass\n\n    def _evaluate(self, rdd: RDD, **kwargs):\n        pass\n\n    def _evaluate(self, rdd: RDD, **kwargs):\n        pass\nCorresponding class implementation:\n\nfrom tensorflow.keras.optimizers import serialize as serialize_optimizer, deserialize as deserialize_optimizer\nimport json\nfrom .worker import AsynchronousSparkWorker, SparkWorker\nfrom .utils import model_to_dict\nfrom typing import Union\nimport numpy as np\nfrom functools import partial\nfrom tensorflow.keras.models import model_from_json\nfrom .parameter.factory import ClientServerFactory\nfrom itertools import tee\nfrom .utils import subtract_params, divide_by\nfrom pyspark import RDD\nfrom .utils import lp_to_simple_rdd, to_simple_rdd\nfrom tensorflow.keras.optimizers import get as get_optimizer\nimport h5py\n\nclass SparkModel(object):\n\n    def __init__(self, model, mode='asynchronous', frequency='epoch', parameter_server_mode='http', num_workers=None, custom_objects=None, batch_size=32, port=4000, *args, **kwargs):\n        \"\"\"SparkModel\n\n        Base class for distributed training on RDDs. Spark model takes a Keras\n        model as master network, an optimization scheme, a parallelisation mode\n        and an averaging frequency.\n\n        :param model: Compiled Keras model\n        :param mode: String, choose from `asynchronous`, `synchronous` and `hogwild`\n        :param frequency: String, either `epoch` or `batch`\n        :param parameter_server_mode: String, either `http` or `socket`\n        :param num_workers: int, number of workers used for training (defaults to None)\n        :param custom_objects: Keras custom objects\n        :param batch_size: batch size used for training and inference\n        :param port: port used in case of 'http' parameter server mode\n        \"\"\"\n        self._training_histories = []\n        self._master_network = model\n        if not hasattr(model, 'loss'):\n            raise Exception('Compile your Keras model before initializing an Elephas model with it')\n        metrics = model.compiled_metrics._metrics\n        loss = model.loss\n        optimizer = serialize_optimizer(model.optimizer)\n        if custom_objects is None:\n            custom_objects = {}\n        if metrics is None:\n            metrics = ['accuracy']\n        self.mode = mode\n        self.frequency = frequency\n        self.num_workers = num_workers\n        self.weights = self._master_network.get_weights()\n        self.pickled_weights = None\n        self.master_optimizer = optimizer\n        self.master_loss = loss\n        self.master_metrics = metrics\n        self.custom_objects = custom_objects\n        self.parameter_server_mode = parameter_server_mode\n        self.batch_size = batch_size\n        self.port = port\n        self.kwargs = kwargs\n        self.serialized_model = model_to_dict(model)\n        if self.mode is not 'synchronous':\n            factory = ClientServerFactory.get_factory(self.parameter_server_mode)\n            self.parameter_server = factory.create_server(self.serialized_model, self.port, self.mode, custom_objects=self.custom_objects)\n            self.client = factory.create_client(self.port)\n\n    def get_config(self):\n        base_config = {'parameter_server_mode': self.parameter_server_mode, 'mode': self.mode, 'frequency': self.frequency, 'num_workers': self.num_workers, 'batch_size': self.batch_size}\n        config = base_config.copy()\n        config.update(self.kwargs)\n        return config\n\n    def save(self, file_name):\n        model = self._master_network\n        model.save(file_name)\n        f = h5py.File(file_name, mode='a')\n        f.attrs['distributed_config'] = json.dumps({'class_name': self.__class__.__name__, 'config': self.get_config()}).encode('utf8')\n        f.flush()\n        f.close()\n\n    @property\n    def training_histories(self):\n        return self._training_histories\n\n    @property\n    def master_network(self):\n        return self._master_network\n\n    @master_network.setter\n    def master_network(self, network):\n        self._master_network = network\n\n    def start_server(self):\n        self.parameter_server.start()\n\n    def stop_server(self):\n        self.parameter_server.stop()\n\n    def predict(self, data: Union[RDD, np.array]):\n        \"\"\"Get prediction probabilities for a numpy array of features\n        \"\"\"\n        if isinstance(data, (np.ndarray,)):\n            from pyspark.sql import SparkSession\n            sc = SparkSession.builder.getOrCreate().sparkContext\n            data = sc.parallelize(data)\n        return self._predict(data)\n\n    def evaluate(self, x_test, y_test, **kwargs):\n        from pyspark.sql import SparkSession\n        sc = SparkSession.builder.getOrCreate().sparkContext\n        test_rdd = to_simple_rdd(sc, x_test, y_test)\n        return self._evaluate(test_rdd, **kwargs)\n\n    def fit(self, rdd: RDD, **kwargs):\n        \"\"\"\n        Train an elephas model on an RDD. The Keras model configuration as specified\n        in the elephas model is sent to Spark workers, abd each worker will be trained\n        on their data partition.\n\n        :param rdd: RDD with features and labels\n        :param epochs: number of epochs used for training\n        :param batch_size: batch size used for training\n        :param verbose: logging verbosity level (0, 1 or 2)\n        :param validation_split: percentage of data set aside for validation\n        \"\"\"\n        print('>>> Fit model')\n        if self.num_workers:\n            rdd = rdd.repartition(self.num_workers)\n        if self.mode in ['asynchronous', 'synchronous', 'hogwild']:\n            self._fit(rdd, **kwargs)\n        else:\n            raise ValueError('Choose from one of the modes: asynchronous, synchronous or hogwild')\n\n    def _fit(self, rdd: RDD, **kwargs):\n        \"\"\"Protected train method to make wrapping of modes easier\n        \"\"\"\n        self._master_network.compile(optimizer=get_optimizer(self.master_optimizer), loss=self.master_loss, metrics=self.master_metrics)\n        if self.mode in ['asynchronous', 'hogwild']:\n            self.start_server()\n        train_config = kwargs\n        freq = self.frequency\n        optimizer = deserialize_optimizer(self.master_optimizer)\n        loss = self.master_loss\n        metrics = self.master_metrics\n        custom = self.custom_objects\n        model_json = self._master_network.to_json()\n        init = self._master_network.get_weights()\n        parameters = rdd.context.broadcast(init)\n        if self.mode in ['asynchronous', 'hogwild']:\n            print('>>> Initialize workers')\n            worker = AsynchronousSparkWorker(model_json, parameters, self.client, train_config, freq, optimizer, loss, metrics, custom)\n            print('>>> Distribute load')\n            rdd.mapPartitions(worker.train).collect()\n            print('>>> Async training complete.')\n            new_parameters = self.client.get_parameters()\n        elif self.mode == 'synchronous':\n            worker = SparkWorker(model_json, parameters, train_config, optimizer, loss, metrics, custom)\n            training_outcomes = rdd.mapPartitions(worker.train).collect()\n            new_parameters = self._master_network.get_weights()\n            number_of_sub_models = len(training_outcomes)\n            for training_outcome in training_outcomes:\n                grad, history = training_outcome\n                self.training_histories.append(history)\n                weighted_grad = divide_by(grad, number_of_sub_models)\n                new_parameters = subtract_params(new_parameters, weighted_grad)\n            print('>>> Synchronous training complete.')\n        else:\n            raise ValueError('Unsupported mode {}'.format(self.mode))\n        self._master_network.set_weights(new_parameters)\n        if self.mode in ['asynchronous', 'hogwild']:\n            self.stop_server()\n\n    def _predict(self, rdd: RDD):\n        json_model = self.master_network.to_json()\n        weights = self.master_network.get_weights()\n        weights = rdd.context.broadcast(weights)\n        custom_objs = self.custom_objects\n\n        def _predict(model_as_json, custom_objects, data):\n            model = model_from_json(model_as_json, custom_objects)\n            model.set_weights(weights.value)\n            data = np.array([x for x in data])\n            return model.predict(data)\n\n        def _predict_with_indices(model_as_json, custom_objects, data):\n            model = model_from_json(model_as_json, custom_objects)\n            model.set_weights(weights.value)\n            data, indices = zip(*data)\n            data = np.array(data)\n            return zip(model.predict(data), indices)\n        if self.num_workers and self.num_workers > 1:\n            rdd = rdd.zipWithIndex()\n            rdd = rdd.repartition(self.num_workers)\n            predictions_and_indices = rdd.mapPartitions(partial(_predict_with_indices, json_model, custom_objs))\n            predictions_sorted_by_index = predictions_and_indices.sortBy(lambda x: x[1])\n            predictions = predictions_sorted_by_index.map(lambda x: x[0]).collect()\n        else:\n            predictions = rdd.mapPartitions(partial(_predict, json_model, custom_objs)).collect()\n        return predictions\n\n    def _evaluate(self, rdd: RDD, **kwargs):\n        json_model = self.master_network.to_json()\n        optimizer = deserialize_optimizer(self.master_optimizer)\n        loss = self.master_loss\n        weights = self.master_network.get_weights()\n        weights = rdd.context.broadcast(weights)\n        custom_objects = self.custom_objects\n        metrics = self.master_metrics\n\n        def _evaluate(model, optimizer, loss, custom_objects, metrics, kwargs, data_iterator):\n            model = model_from_json(model, custom_objects)\n            model.compile(optimizer, loss, metrics)\n            model.set_weights(weights.value)\n            feature_iterator, label_iterator = tee(data_iterator, 2)\n            x_test = np.asarray([x for x, y in feature_iterator])\n            y_test = np.asarray([y for x, y in label_iterator])\n            return [model.evaluate(x_test, y_test, **kwargs)]\n        if self.num_workers:\n            rdd = rdd.repartition(self.num_workers)\n        results = rdd.mapPartitions(partial(_evaluate, json_model, optimizer, loss, custom_objects, metrics, kwargs))\n        if not metrics:\n            return results.mean()\n        else:\n            loss_value = results.map(lambda x: x[0]).mean()\n            metric_value = results.map(lambda x: x[1]).mean()\n            return [loss_value, metric_value]\n\nExample skeleton:\n\n\nclass PCAM(VisionDataset):\n    '''`PCAM Dataset   <https://github.com/basveeling/pcam>`_.\n    The PatchCamelyon dataset is a binary classification dataset with 327,680\n    color images (96px x 96px), extracted from histopathologic scans of lymph node\n    sections. Each image is annotated with a binary label indicating presence of\n    metastatic tissue.\n    This dataset requires the ``h5py`` package which you can install with ``pip install h5py``.\n    Args:\n         root (str or ``pathlib.Path``): Root directory of the dataset.\n         split (string, optional): The dataset split, supports ``\"train\"`` (default), ``\"test\"`` or ``\"val\"``.\n         transform (callable, optional): A function/transform that takes in a PIL image and returns a transformed\n             version. E.g, ``transforms.RandomCrop``.\n         target_transform (callable, optional): A function/transform that takes in the target and transforms it.\n         download (bool, optional): If True, downloads the dataset from the internet and puts it into ``root/pcam``. If\n             dataset is already downloaded, it is not downloaded again.\n             .. warning::\n                To download the dataset `gdown <https://github.com/wkentaro/gdown>`_ is required.\n    '''\n\n    def __init__(self, root: Union[str, pathlib.Path], split: str='train', transform: Optional[Callable]=None, target_transform: Optional[Callable]=None, download: bool=False):\n        pass\n\n    def __len__(self) -> int:\n        pass\n\n    def __getitem__(self, idx: int) -> tuple[Any, Any]:\n        pass\n\n    def _check_exists(self) -> bool:\n        pass\n\n    def _download(self) -> None:\n        pass\nCorresponding class implementation:\n\nfrom typing import Any, Callable, Optional, Union\nimport pathlib\nfrom .vision import VisionDataset\nfrom PIL import Image\nfrom .utils import _decompress, download_file_from_google_drive, verify_str_arg\n\nclass PCAM(VisionDataset):\n    \"\"\"`PCAM Dataset   <https://github.com/basveeling/pcam>`_.\n\n    The PatchCamelyon dataset is a binary classification dataset with 327,680\n    color images (96px x 96px), extracted from histopathologic scans of lymph node\n    sections. Each image is annotated with a binary label indicating presence of\n    metastatic tissue.\n\n    This dataset requires the ``h5py`` package which you can install with ``pip install h5py``.\n\n    Args:\n         root (str or ``pathlib.Path``): Root directory of the dataset.\n         split (string, optional): The dataset split, supports ``\"train\"`` (default), ``\"test\"`` or ``\"val\"``.\n         transform (callable, optional): A function/transform that takes in a PIL image and returns a transformed\n             version. E.g, ``transforms.RandomCrop``.\n         target_transform (callable, optional): A function/transform that takes in the target and transforms it.\n         download (bool, optional): If True, downloads the dataset from the internet and puts it into ``root/pcam``. If\n             dataset is already downloaded, it is not downloaded again.\n\n             .. warning::\n\n                To download the dataset `gdown <https://github.com/wkentaro/gdown>`_ is required.\n    \"\"\"\n    _FILES = {'train': {'images': ('camelyonpatch_level_2_split_train_x.h5', '1Ka0XfEMiwgCYPdTI-vv6eUElOBnKFKQ2', '1571f514728f59376b705fc836ff4b63'), 'targets': ('camelyonpatch_level_2_split_train_y.h5', '1269yhu3pZDP8UYFQs-NYs3FPwuK-nGSG', '35c2d7259d906cfc8143347bb8e05be7')}, 'test': {'images': ('camelyonpatch_level_2_split_test_x.h5', '1qV65ZqZvWzuIVthK8eVDhIwrbnsJdbg_', 'd8c2d60d490dbd479f8199bdfa0cf6ec'), 'targets': ('camelyonpatch_level_2_split_test_y.h5', '17BHrSrwWKjYsOgTMmoqrIjDy6Fa2o_gP', '60a7035772fbdb7f34eb86d4420cf66a')}, 'val': {'images': ('camelyonpatch_level_2_split_valid_x.h5', '1hgshYGWK8V-eGRy8LToWJJgDU_rXWVJ3', 'd5b63470df7cfa627aeec8b9dc0c066e'), 'targets': ('camelyonpatch_level_2_split_valid_y.h5', '1bH8ZRbhSVAhScTS0p9-ZzGnX91cHT3uO', '2b85f58b927af9964a4c15b8f7e8f179')}}\n\n    def __init__(self, root: Union[str, pathlib.Path], split: str='train', transform: Optional[Callable]=None, target_transform: Optional[Callable]=None, download: bool=False):\n        try:\n            import h5py\n            self.h5py = h5py\n        except ImportError:\n            raise RuntimeError('h5py is not found. This dataset needs to have h5py installed: please run pip install h5py')\n        self._split = verify_str_arg(split, 'split', ('train', 'test', 'val'))\n        super().__init__(root, transform=transform, target_transform=target_transform)\n        self._base_folder = pathlib.Path(self.root) / 'pcam'\n        if download:\n            self._download()\n        if not self._check_exists():\n            raise RuntimeError('Dataset not found. You can use download=True to download it')\n\n    def __len__(self) -> int:\n        images_file = self._FILES[self._split]['images'][0]\n        with self.h5py.File(self._base_folder / images_file) as images_data:\n            return images_data['x'].shape[0]\n\n    def __getitem__(self, idx: int) -> tuple[Any, Any]:\n        images_file = self._FILES[self._split]['images'][0]\n        with self.h5py.File(self._base_folder / images_file) as images_data:\n            image = Image.fromarray(images_data['x'][idx]).convert('RGB')\n        targets_file = self._FILES[self._split]['targets'][0]\n        with self.h5py.File(self._base_folder / targets_file) as targets_data:\n            target = int(targets_data['y'][idx, 0, 0, 0])\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            target = self.target_transform(target)\n        return (image, target)\n\n    def _check_exists(self) -> bool:\n        images_file = self._FILES[self._split]['images'][0]\n        targets_file = self._FILES[self._split]['targets'][0]\n        return all((self._base_folder.joinpath(h5_file).exists() for h5_file in (images_file, targets_file)))\n\n    def _download(self) -> None:\n        if self._check_exists():\n            return\n        for file_name, file_id, md5 in self._FILES[self._split].values():\n            archive_name = file_name + '.gz'\n            download_file_from_google_drive(file_id, str(self._base_folder), filename=archive_name, md5=md5)\n            _decompress(str(self._base_folder / archive_name))\n\nNow, implement the following class. Do not explain the code. The given class skeleton is as follows:\n\nclass NunchakuPulidLoader:\n    '''\n    Deprecated node for loading the PuLID pipeline for a Nunchaku FLUX model.\n    .. warning::\n        This node is deprecated and will be removed in December 2025.\n        Use :class:`NunchakuPuLIDLoaderV2` instead.\n    Attributes\n    ----------\n    pulid_device : str\n        Device to load the PuLID pipeline on (default: \"cuda\").\n    weight_dtype : torch.dtype\n        Data type for model weights (default: torch.bfloat16).\n    onnx_provider : str\n        ONNX provider to use (default: \"gpu\").\n    pretrained_model : str or None\n        Path to the pretrained PuLID model, if any.\n    '''\n\n    def __init__(self):\n        '''\n        Initialize the loader with default device, dtype, and ONNX provider.\n        '''\n        pass\n    @classmethod\n    def INPUT_TYPES(s):\n        '''\n        Returns the required input types for this node.\n        Returns\n        -------\n        dict\n            Dictionary specifying required inputs.\n        '''\n        pass\n\n    def load(self, model):\n        '''\n        Load the PuLID pipeline for the given Nunchaku FLUX model.\n        .. warning::\n            This node is deprecated and will be removed in December 2025.\n            Use :class:`NunchakuPuLIDLoaderV2` instead.\n        Parameters\n        ----------\n        model : object\n            The Nunchaku FLUX model.\n        Returns\n        -------\n        tuple\n            The input model and the loaded PuLID pipeline.\n        '''\n        pass"
  }
]