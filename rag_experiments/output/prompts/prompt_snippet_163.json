[
  {
    "role": "assistant",
    "content": "You are an expert Python programmer who can correctly implement complete Python classes based on the provided class skeleton."
  },
  {
    "role": "user",
    "content": "For your understanding, following are some (skeleton, code) examples:\n\nExample skeleton:\n\n@PublicAPI(stability='alpha')\nclass MultiAgentEnvRunner(EnvRunner, Checkpointable):\n    '''The genetic environment runner for the multi-agent case.'''\n    @override(EnvRunner)\n    def __init__(self, config: AlgorithmConfig, **kwargs):\n        '''Initializes a MultiAgentEnvRunner instance.\n        Args:\n            config: An `AlgorithmConfig` object containing all settings needed to\n                build this `EnvRunner` class.\n        '''\n        pass\n    @override(EnvRunner)\n    def sample(self, *, num_timesteps: int=None, num_episodes: int=None, explore: bool=None, random_actions: bool=False, force_reset: bool=False) -> List[MultiAgentEpisode]:\n        '''Runs and returns a sample (n timesteps or m episodes) on the env(s).\n        Args:\n            num_timesteps: The number of timesteps to sample during this call.\n                Note that only one of `num_timesteps` or `num_episodes` may be provided.\n            num_episodes: The number of episodes to sample during this call.\n                Note that only one of `num_timesteps` or `num_episodes` may be provided.\n            explore: If True, will use the RLModule's `forward_exploration()`\n                method to compute actions. If False, will use the RLModule's\n                `forward_inference()` method. If None (default), will use the `explore`\n                boolean setting from `self.config` passed into this EnvRunner's\n                constructor. You can change this setting in your config via\n                `config.env_runners(explore=True|False)`.\n            random_actions: If True, actions will be sampled randomly (from the action\n                space of the environment). If False (default), actions or action\n                distribution parameters are computed by the RLModule.\n            force_reset: Whether to force-reset all (vector) environments before\n                sampling. Useful if you would like to collect a clean slate of new\n                episodes via this call. Note that when sampling n episodes\n                (`num_episodes != None`), this is fixed to True.\n        Returns:\n            A list of `MultiAgentEpisode` instances, carrying the sampled data.\n        '''\n        pass\n\n    def _sample(self, *, num_timesteps: Optional[int]=None, num_episodes: Optional[int]=None, explore: bool, random_actions: bool=False, force_reset: bool=False) -> List[MultiAgentEpisode]:\n        pass\n\n    def _reset_envs(self, episodes, shared_data, explore):\n        pass\n    @override(EnvRunner)\n    def get_spaces(self):\n        pass\n    @override(EnvRunner)\n    def get_metrics(self) -> ResultDict:\n        pass\n    @override(Checkpointable)\n    def get_state(self, components: Optional[Union[str, Collection[str]]]=None, *, not_components: Optional[Union[str, Collection[str]]]=None, **kwargs) -> StateDict:\n        pass\n    @override(Checkpointable)\n    def set_state(self, state: StateDict) -> None:\n        pass\n    @override(Checkpointable)\n    def get_ctor_args_and_kwargs(self):\n        pass\n    @override(Checkpointable)\n    def get_metadata(self):\n        pass\n    @override(Checkpointable)\n    def get_checkpointable_components(self):\n        pass\n    @override(EnvRunner)\n    def assert_healthy(self):\n        '''Checks that self.__init__() has been completed properly.\n        Ensures that the instances has a `MultiRLModule` and an\n        environment defined.\n        Raises:\n            AssertionError: If the EnvRunner Actor has NOT been properly initialized.\n        '''\n        pass\n    @override(EnvRunner)\n    def make_env(self):\n        pass\n    @override(EnvRunner)\n    def make_module(self):\n        pass\n    @override(EnvRunner)\n    def stop(self):\n        pass\n\n    def _setup_metrics(self):\n        pass\n\n    def _new_episode(self, env_index, episodes=None):\n        pass\n\n    def _make_on_episode_callback(self, which: str, idx: int, episodes: List[MultiAgentEpisode]):\n        pass\n\n    def _increase_sampled_metrics(self, num_steps, next_obs, episode):\n        pass\n\n    def _log_episode_metrics(self, length, ret, sec, agents=None, modules=None, agent_steps=None):\n        pass\n    @staticmethod\n    def _prune_zero_len_sa_episodes(episode: MultiAgentEpisode):\n        pass\n    @Deprecated(new=\"MultiAgentEnvRunner.get_state(components='rl_module')\", error=False)\n    def get_weights(self, modules=None):\n        pass\n    @Deprecated(new='MultiAgentEnvRunner.set_state()', error=False)\n    def set_weights(self, weights: ModelWeights, global_vars: Optional[Dict]=None, weights_seq_no: int=0) -> None:\n        pass\nCorresponding class implementation:\n\nfrom ray.rllib.utils.metrics import ENV_TO_MODULE_CONNECTOR, EPISODE_DURATION_SEC_MEAN, EPISODE_LEN_MAX, EPISODE_LEN_MEAN, EPISODE_LEN_MIN, EPISODE_RETURN_MAX, EPISODE_RETURN_MEAN, EPISODE_RETURN_MIN, MODULE_TO_ENV_CONNECTOR, NUM_AGENT_STEPS_SAMPLED, NUM_AGENT_STEPS_SAMPLED_LIFETIME, NUM_ENV_STEPS_SAMPLED, NUM_ENV_STEPS_SAMPLED_LIFETIME, NUM_EPISODES, NUM_EPISODES_LIFETIME, NUM_MODULE_STEPS_SAMPLED, NUM_MODULE_STEPS_SAMPLED_LIFETIME, RLMODULE_INFERENCE_TIMER, SAMPLE_TIMER, TIME_BETWEEN_SAMPLING, WEIGHTS_SEQ_NO\nfrom ray.rllib.utils.framework import get_device, try_import_torch\nimport gymnasium as gym\nfrom ray.rllib.env.env_context import EnvContext\nimport ray\nfrom ray.rllib.core.columns import Columns\nfrom ray.rllib.core.rl_module.multi_rl_module import MultiRLModule, MultiRLModuleSpec\nfrom ray.rllib.utils.pre_checks.env import check_multiagent_environments\nfrom ray.rllib.utils.annotations import override\nfrom ray.rllib.algorithms.algorithm_config import AlgorithmConfig\nfrom functools import partial\nfrom ray.rllib.callbacks.utils import make_callback\nfrom ray.rllib.env.multi_agent_episode import MultiAgentEpisode\nfrom ray.rllib.utils.checkpoints import Checkpointable\nfrom ray.tune.registry import ENV_CREATOR, _global_registry\nfrom ray.rllib.env.env_runner import EnvRunner, ENV_STEP_FAILURE\nfrom ray.rllib.env import INPUT_ENV_SPACES, INPUT_ENV_SINGLE_SPACES\nfrom ray.rllib.env.utils import _gym_env_creator\nfrom typing import Collection, DefaultDict, Dict, List, Optional, Union\nfrom ray.rllib.utils import force_list\nfrom ray.rllib.env.multi_agent_env import MultiAgentEnv\nfrom ray.rllib.env.vector.registration import make_vec\nimport time\nfrom ray._common.deprecation import Deprecated\nfrom ray.util.annotations import PublicAPI\nfrom ray.rllib.utils.typing import EpisodeID, ModelWeights, ResultDict, StateDict\nimport math\nfrom collections import defaultdict\nfrom ray.rllib.core import COMPONENT_ENV_TO_MODULE_CONNECTOR, COMPONENT_MODULE_TO_ENV_CONNECTOR, COMPONENT_RL_MODULE\nimport logging\nfrom ray.rllib.env.vector.vector_multi_agent_env import VectorMultiAgentEnv\n\n@PublicAPI(stability='alpha')\nclass MultiAgentEnvRunner(EnvRunner, Checkpointable):\n    \"\"\"The genetic environment runner for the multi-agent case.\"\"\"\n\n    @override(EnvRunner)\n    def __init__(self, config: AlgorithmConfig, **kwargs):\n        \"\"\"Initializes a MultiAgentEnvRunner instance.\n\n        Args:\n            config: An `AlgorithmConfig` object containing all settings needed to\n                build this `EnvRunner` class.\n        \"\"\"\n        super().__init__(config=config, **kwargs)\n        if not self.config.is_multi_agent:\n            raise ValueError(f'Cannot use this EnvRunner class ({type(self).__name__}), if your setup is not multi-agent! Try adding multi-agent information to your AlgorithmConfig via calling the `config.multi_agent(policies=..., policy_mapping_fn=...)`.')\n        self.spaces = kwargs.get('spaces', {})\n        self._setup_metrics()\n        self._callbacks = [cls() for cls in force_list(self.config.callbacks_class)]\n        self._device = get_device(self.config, 0 if not self.worker_index else self.config.num_gpus_per_env_runner)\n        self.env: Optional[gym.Wrapper] = None\n        self.num_envs: int = 0\n        if self.worker_index is None or self.worker_index > 0 or self.config.create_env_on_local_worker or (self.config.num_env_runners == 0):\n            self.make_env()\n        self._env_to_module = self.config.build_env_to_module_connector(env=self.env, spaces=self.spaces, device=self._device)\n        self._cached_to_module = None\n        self.module: Optional[MultiRLModule] = None\n        self.make_module()\n        self._module_to_env = self.config.build_module_to_env_connector(env=self.env.unwrapped if self.env else None, spaces=self.spaces)\n        self._needs_initial_reset: bool = True\n        self._episode: Optional[MultiAgentEpisode] = None\n        self._shared_data = None\n        self._weights_seq_no: int = 0\n        self._time_after_sampling = None\n\n    @override(EnvRunner)\n    def sample(self, *, num_timesteps: int=None, num_episodes: int=None, explore: bool=None, random_actions: bool=False, force_reset: bool=False) -> List[MultiAgentEpisode]:\n        \"\"\"Runs and returns a sample (n timesteps or m episodes) on the env(s).\n\n        Args:\n            num_timesteps: The number of timesteps to sample during this call.\n                Note that only one of `num_timesteps` or `num_episodes` may be provided.\n            num_episodes: The number of episodes to sample during this call.\n                Note that only one of `num_timesteps` or `num_episodes` may be provided.\n            explore: If True, will use the RLModule's `forward_exploration()`\n                method to compute actions. If False, will use the RLModule's\n                `forward_inference()` method. If None (default), will use the `explore`\n                boolean setting from `self.config` passed into this EnvRunner's\n                constructor. You can change this setting in your config via\n                `config.env_runners(explore=True|False)`.\n            random_actions: If True, actions will be sampled randomly (from the action\n                space of the environment). If False (default), actions or action\n                distribution parameters are computed by the RLModule.\n            force_reset: Whether to force-reset all (vector) environments before\n                sampling. Useful if you would like to collect a clean slate of new\n                episodes via this call. Note that when sampling n episodes\n                (`num_episodes != None`), this is fixed to True.\n\n        Returns:\n            A list of `MultiAgentEpisode` instances, carrying the sampled data.\n        \"\"\"\n        if self.env is None:\n            raise ValueError(f\"{self} doesn't have an env! Can't call `sample()` on it.\")\n        assert not (num_timesteps is not None and num_episodes is not None), f'Provide either `num_timesteps` or `num_episodes`. Both provided here:num_timesteps={num_timesteps!r}, num_episodes={num_episodes!r}'\n        if self._time_after_sampling is not None:\n            self.metrics.log_value(key=TIME_BETWEEN_SAMPLING, value=time.perf_counter() - self._time_after_sampling)\n        self.metrics.log_value(key=WEIGHTS_SEQ_NO, value=self._weights_seq_no, window=1)\n        with self.metrics.log_time(SAMPLE_TIMER):\n            if explore is None:\n                explore = self.config.explore\n            if num_timesteps is None and num_episodes is None and (self.config.batch_mode == 'truncate_episodes'):\n                num_timesteps = self.config.get_rollout_fragment_length(self.worker_index) * self.num_envs\n            if num_timesteps is not None:\n                samples = self._sample(num_timesteps=num_timesteps, explore=explore, random_actions=random_actions, force_reset=force_reset)\n            elif num_episodes is not None:\n                samples = self._sample(num_episodes=num_episodes, explore=explore, random_actions=random_actions)\n            else:\n                samples = self._sample(num_episodes=self.num_envs, explore=explore, random_actions=random_actions)\n            make_callback('on_sample_end', callbacks_objects=self._callbacks, callbacks_functions=self.config.callbacks_on_sample_end, kwargs=dict(env_runner=self, metrics_logger=self.metrics, samples=samples))\n        self._time_after_sampling = time.perf_counter()\n        return samples\n\n    def _sample(self, *, num_timesteps: Optional[int]=None, num_episodes: Optional[int]=None, explore: bool, random_actions: bool=False, force_reset: bool=False) -> List[MultiAgentEpisode]:\n        done_episodes_to_return: List[MultiAgentEpisode] = []\n        if force_reset or num_episodes is not None or self._needs_initial_reset:\n            episodes = self._episodes = [None for _ in range(self.num_envs)]\n            shared_data = self._shared_data = {}\n            self._reset_envs(episodes, shared_data, explore)\n            self._needs_initial_reset = False\n        else:\n            episodes = self._episodes\n            shared_data = self._shared_data\n        if num_episodes is not None:\n            self._needs_initial_reset = True\n        env_ts = 0\n        agent_ts = 0\n        eps = 0\n        while eps < num_episodes if num_timesteps is None else env_ts < num_timesteps if self.config.count_steps_by == 'env_steps' else agent_ts < num_timesteps:\n            if random_actions:\n                to_env = {Columns.ACTIONS: [{aid: self.env.envs[i].unwrapped.get_action_space(aid).sample() for aid in episodes[i].get_agents_to_act()} for i in range(self.num_envs)]}\n            else:\n                to_module = self._cached_to_module\n                assert to_module is not None\n                self._cached_to_module = None\n                if to_module:\n                    if explore:\n                        global_env_steps_lifetime = (self.metrics.peek(NUM_ENV_STEPS_SAMPLED_LIFETIME, default=0) + env_ts) * (self.config.num_env_runners or 1)\n                        with self.metrics.log_time(RLMODULE_INFERENCE_TIMER):\n                            to_env = self.module.forward_exploration(to_module, t=global_env_steps_lifetime)\n                    else:\n                        with self.metrics.log_time(RLMODULE_INFERENCE_TIMER):\n                            to_env = self.module.forward_inference(to_module)\n                    to_env = self._module_to_env(rl_module=self.module, batch=to_env, episodes=episodes, explore=explore, shared_data=shared_data, metrics=self.metrics, metrics_prefix_key=(MODULE_TO_ENV_CONNECTOR,))\n                else:\n                    to_env = {}\n            shared_data['vector_env_episodes_map'] = {}\n            actions = to_env.pop(Columns.ACTIONS, [{} for _ in episodes])\n            actions_for_env = to_env.pop(Columns.ACTIONS_FOR_ENV, actions)\n            results = self._try_env_step(actions_for_env)\n            if results == ENV_STEP_FAILURE:\n                logging.warning(f'RLlib {self.__class__.__name__}: Environment step failed. Will force reset env(s) in this EnvRunner.')\n                return self._sample(num_timesteps=num_timesteps, num_episodes=num_episodes, explore=explore, random_actions=random_actions, force_reset=True)\n            observations, rewards, terminateds, truncateds, infos = results\n            call_on_episode_start = set()\n            for env_index in range(self.num_envs):\n                extra_model_outputs = defaultdict(dict)\n                for col, ma_dict_list in to_env.items():\n                    ma_dict = ma_dict_list[env_index]\n                    for agent_id, val in ma_dict.items():\n                        extra_model_outputs[agent_id][col] = val\n                        extra_model_outputs[agent_id][WEIGHTS_SEQ_NO] = self._weights_seq_no\n                extra_model_outputs = dict(extra_model_outputs)\n                if not self._episodes[env_index].is_reset:\n                    episodes[env_index].add_env_reset(observations=observations[env_index], infos=infos[env_index])\n                    call_on_episode_start.add(env_index)\n                else:\n                    episodes[env_index].add_env_step(observations=observations[env_index], actions=actions[env_index], rewards=rewards[env_index], infos=infos[env_index], terminateds=terminateds[env_index], truncateds=truncateds[env_index], extra_model_outputs=extra_model_outputs)\n                    env_ts += self._increase_sampled_metrics(1, observations[env_index], episodes[env_index])\n                    agent_ts += len(observations[env_index])\n            done_episodes_to_run_env_to_module = []\n            for env_index in range(self.num_envs):\n                if env_index in call_on_episode_start:\n                    self._make_on_episode_callback('on_episode_start', env_index, episodes)\n                else:\n                    self._make_on_episode_callback('on_episode_step', env_index, episodes)\n                if episodes[env_index].is_done:\n                    eps += 1\n                    self._make_on_episode_callback('on_episode_end', env_index, episodes)\n                    self._prune_zero_len_sa_episodes(episodes[env_index])\n                    done_episodes_to_return.append(episodes[env_index])\n                    done_episodes_to_run_env_to_module.append(episodes[env_index])\n                    if eps == num_episodes:\n                        break\n                    old_episode_id = episodes[env_index].id_\n                    self._new_episode(env_index, episodes)\n                    shared_data['vector_env_episodes_map'].update({old_episode_id: episodes[env_index].id_})\n            if self.module is not None:\n                if done_episodes_to_run_env_to_module:\n                    self._env_to_module(episodes=done_episodes_to_run_env_to_module, explore=explore, rl_module=self.module, shared_data=shared_data, metrics=None)\n                self._cached_to_module = self._env_to_module(episodes=episodes, explore=explore, rl_module=self.module, shared_data=shared_data, metrics=self.metrics, metrics_prefix_key=(ENV_TO_MODULE_CONNECTOR,))\n            if self.config.episodes_to_numpy:\n                for episode in done_episodes_to_return:\n                    episode.to_numpy()\n        self._done_episodes_for_metrics.extend(done_episodes_to_return)\n        ongoing_episodes_to_return = []\n        if num_timesteps is not None:\n            ongoing_episodes_continuations = [eps.cut(len_lookback_buffer=self.config.episode_lookback_horizon) for eps in self._episodes]\n            for eps in self._episodes:\n                if eps.env_t == 0:\n                    continue\n                eps.validate()\n                self._ongoing_episodes_for_metrics[eps.id_].append(eps)\n                self._prune_zero_len_sa_episodes(eps)\n                if self.config.episodes_to_numpy:\n                    ongoing_episodes_to_return.append(eps.to_numpy())\n                else:\n                    ongoing_episodes_to_return.append(eps)\n            self._episodes = ongoing_episodes_continuations\n        return done_episodes_to_return + ongoing_episodes_to_return\n\n    def _reset_envs(self, episodes, shared_data, explore):\n        for env_index in range(self.num_envs):\n            self._new_episode(env_index, episodes)\n        self._ongoing_episodes_for_metrics.clear()\n        observations, infos = self._try_env_reset(seed=self._seed if self._needs_initial_reset else None, options=None)\n        for env_index in range(self.num_envs):\n            episodes[env_index].add_env_reset(observations=observations[env_index], infos=infos[env_index])\n        self._cached_to_module = None\n        if self.module:\n            self._cached_to_module = self._env_to_module(rl_module=self.module, episodes=episodes, explore=explore, shared_data=shared_data, metrics=self.metrics, metrics_key_prefix=(ENV_TO_MODULE_CONNECTOR,))\n        for env_index in range(self.num_envs):\n            self._make_on_episode_callback('on_episode_start', env_index, episodes)\n\n    @override(EnvRunner)\n    def get_spaces(self):\n        if self.env is None:\n            return self.spaces\n        return {INPUT_ENV_SPACES: (self.env.observation_space, self.env.action_space), INPUT_ENV_SINGLE_SPACES: (self.env.single_observation_space, self.env.single_action_space), **{mid: (o, self._env_to_module.action_space[mid]) for mid, o in self._env_to_module.observation_space.spaces.items()}}\n\n    @override(EnvRunner)\n    def get_metrics(self) -> ResultDict:\n        for eps in self._done_episodes_for_metrics:\n            assert eps.is_done\n            episode_length = len(eps)\n            agent_steps = defaultdict(int, {str(aid): len(sa_eps) for aid, sa_eps in eps.agent_episodes.items()})\n            episode_return = eps.get_return()\n            episode_duration_s = eps.get_duration_s()\n            agent_episode_returns = defaultdict(float, {str(sa_eps.agent_id): sa_eps.get_return() for sa_eps in eps.agent_episodes.values()})\n            module_episode_returns = defaultdict(float, {sa_eps.module_id: sa_eps.get_return() for sa_eps in eps.agent_episodes.values()})\n            if eps.id_ in self._ongoing_episodes_for_metrics:\n                for eps2 in self._ongoing_episodes_for_metrics[eps.id_]:\n                    return_eps2 = eps2.get_return()\n                    episode_length += len(eps2)\n                    episode_return += return_eps2\n                    episode_duration_s += eps2.get_duration_s()\n                    for sa_eps in eps2.agent_episodes.values():\n                        return_sa = sa_eps.get_return()\n                        agent_steps[str(sa_eps.agent_id)] += len(sa_eps)\n                        agent_episode_returns[str(sa_eps.agent_id)] += return_sa\n                        module_episode_returns[sa_eps.module_id] += return_sa\n                del self._ongoing_episodes_for_metrics[eps.id_]\n            self._log_episode_metrics(episode_length, episode_return, episode_duration_s, agent_episode_returns, module_episode_returns, dict(agent_steps))\n        self._done_episodes_for_metrics.clear()\n        return self.metrics.reduce()\n\n    @override(Checkpointable)\n    def get_state(self, components: Optional[Union[str, Collection[str]]]=None, *, not_components: Optional[Union[str, Collection[str]]]=None, **kwargs) -> StateDict:\n        state = {NUM_ENV_STEPS_SAMPLED_LIFETIME: self.metrics.peek(NUM_ENV_STEPS_SAMPLED_LIFETIME, default=0)}\n        if self._check_component(COMPONENT_RL_MODULE, components, not_components):\n            state[COMPONENT_RL_MODULE] = self.module.get_state(components=self._get_subcomponents(COMPONENT_RL_MODULE, components), not_components=self._get_subcomponents(COMPONENT_RL_MODULE, not_components), **kwargs)\n            state[WEIGHTS_SEQ_NO] = self._weights_seq_no\n        if self._check_component(COMPONENT_ENV_TO_MODULE_CONNECTOR, components, not_components):\n            state[COMPONENT_ENV_TO_MODULE_CONNECTOR] = self._env_to_module.get_state()\n        if self._check_component(COMPONENT_MODULE_TO_ENV_CONNECTOR, components, not_components):\n            state[COMPONENT_MODULE_TO_ENV_CONNECTOR] = self._module_to_env.get_state()\n        return state\n\n    @override(Checkpointable)\n    def set_state(self, state: StateDict) -> None:\n        if COMPONENT_ENV_TO_MODULE_CONNECTOR in state:\n            self._env_to_module.set_state(state[COMPONENT_ENV_TO_MODULE_CONNECTOR])\n        if COMPONENT_MODULE_TO_ENV_CONNECTOR in state:\n            self._module_to_env.set_state(state[COMPONENT_MODULE_TO_ENV_CONNECTOR])\n        if COMPONENT_RL_MODULE in state:\n            weights_seq_no = state.get(WEIGHTS_SEQ_NO, 0)\n            if weights_seq_no == 0 or self._weights_seq_no < weights_seq_no:\n                rl_module_state = state[COMPONENT_RL_MODULE]\n                if isinstance(rl_module_state, ray.ObjectRef):\n                    rl_module_state = ray.get(rl_module_state)\n                self.module.set_state(rl_module_state)\n            if weights_seq_no > 0:\n                self._weights_seq_no = weights_seq_no\n        if NUM_ENV_STEPS_SAMPLED_LIFETIME in state:\n            self.metrics.set_value(key=NUM_ENV_STEPS_SAMPLED_LIFETIME, value=state[NUM_ENV_STEPS_SAMPLED_LIFETIME], reduce='sum', with_throughput=True)\n\n    @override(Checkpointable)\n    def get_ctor_args_and_kwargs(self):\n        return ((), {'config': self.config})\n\n    @override(Checkpointable)\n    def get_metadata(self):\n        metadata = Checkpointable.get_metadata(self)\n        metadata.update({})\n        return metadata\n\n    @override(Checkpointable)\n    def get_checkpointable_components(self):\n        return [(COMPONENT_RL_MODULE, self.module), (COMPONENT_ENV_TO_MODULE_CONNECTOR, self._env_to_module), (COMPONENT_MODULE_TO_ENV_CONNECTOR, self._module_to_env)]\n\n    @override(EnvRunner)\n    def assert_healthy(self):\n        \"\"\"Checks that self.__init__() has been completed properly.\n\n        Ensures that the instances has a `MultiRLModule` and an\n        environment defined.\n\n        Raises:\n            AssertionError: If the EnvRunner Actor has NOT been properly initialized.\n        \"\"\"\n        assert self.env and self.module\n\n    @override(EnvRunner)\n    def make_env(self):\n        if self.env is not None:\n            try:\n                self.env.close()\n            except Exception as e:\n                logger.warning(f'Tried closing the existing env (multi-agent), but failed with error: {e.args[0]}')\n            del self.env\n        env_ctx = self.config.env_config\n        if not isinstance(env_ctx, EnvContext):\n            env_ctx = EnvContext(env_ctx, worker_index=self.worker_index, num_workers=self.config.num_env_runners, remote=self.config.remote_worker_envs)\n        if not self.config.env:\n            raise ValueError(\"`config.env` is not provided! You should provide a valid environment to your config through `config.environment([env descriptor e.g. 'CartPole-v1'])`.\")\n        elif isinstance(self.config.env, str) and _global_registry.contains(ENV_CREATOR, self.config.env):\n            entry_point = partial(_global_registry.get(ENV_CREATOR, self.config.env), env_ctx)\n        else:\n            entry_point = partial(_gym_env_creator, env_descriptor=self.config.env, env_context=env_ctx)\n        gym.register('rllib-multi-agent-env-v0', entry_point=entry_point, disable_env_checker=True)\n        vectorize_mode = self.config.gym_env_vectorize_mode\n        self.env = make_vec('rllib-multi-agent-env-v0', num_envs=self.config.num_envs_per_env_runner, vectorization_mode=vectorize_mode if isinstance(vectorize_mode, gym.envs.registration.VectorizeMode) else gym.envs.registration.VectorizeMode(vectorize_mode.lower()))\n        self.num_envs: int = self.env.num_envs\n        assert self.num_envs == self.config.num_envs_per_env_runner\n        if not self.config.disable_env_checking:\n            try:\n                for env in self.env.envs:\n                    check_multiagent_environments(env.unwrapped)\n            except Exception as e:\n                logger.exception(e.args[0])\n        else:\n            try:\n                assert isinstance(self.env, VectorMultiAgentEnv)\n                assert isinstance(self.env.envs[0].unwrapped, MultiAgentEnv)\n            except AssertionError:\n                logger.exception(f'When using the `MultiAgentEnvRunner`, the environment must inherit from `ray.rllib.env.vector.vector_multi_agent_env.VectorMultiAgentEnv` (but yours is {self.env}) and the individual envs must inherit from `MultiAgentEnv` (but yours is {self.env.envs[0].unwrapped})!')\n            except TypeError:\n                logger.exception('When using the `MultiAgentEnvRunner`, the env must have a subscriptable `self.envs` attribute!')\n        self._needs_initial_reset = True\n        make_callback('on_environment_created', callbacks_objects=self._callbacks, callbacks_functions=self.config.callbacks_on_environment_created, kwargs=dict(env_runner=self, metrics_logger=self.metrics, env=self.env.unwrapped, env_context=env_ctx))\n\n    @override(EnvRunner)\n    def make_module(self):\n        env = self.env.unwrapped if self.env is not None else None\n        try:\n            module_spec: MultiRLModuleSpec = self.config.get_multi_rl_module_spec(env=env, spaces=self.get_spaces(), inference_only=True)\n            self.module = module_spec.build()\n            if torch:\n                self.module.foreach_module(lambda mid, mod: mod.to(self._device) if isinstance(mod, torch.nn.Module) else mod)\n        except NotImplementedError:\n            self.module = None\n\n    @override(EnvRunner)\n    def stop(self):\n        if self.env is not None:\n            self.env.close()\n\n    def _setup_metrics(self):\n        self._done_episodes_for_metrics: List[MultiAgentEpisode] = []\n        self._ongoing_episodes_for_metrics: DefaultDict[EpisodeID, List[MultiAgentEpisode]] = defaultdict(list)\n\n    def _new_episode(self, env_index, episodes=None):\n        episodes = episodes if episodes is not None else self._episodes\n        episodes[env_index] = MultiAgentEpisode(observation_space={aid: self.env.envs[env_index].unwrapped.get_observation_space(aid) for aid in self.env.envs[env_index].unwrapped.possible_agents}, action_space={aid: self.env.envs[env_index].unwrapped.get_action_space(aid) for aid in self.env.envs[env_index].unwrapped.possible_agents}, agent_to_module_mapping_fn=self.config.policy_mapping_fn)\n        self._make_on_episode_callback('on_episode_created', env_index, episodes)\n\n    def _make_on_episode_callback(self, which: str, idx: int, episodes: List[MultiAgentEpisode]):\n        kwargs = dict(episode=episodes[idx], env_runner=self, metrics_logger=self.metrics, env=self.env.unwrapped, rl_module=self.module, env_index=idx)\n        if which == 'on_episode_end':\n            kwargs['prev_episode_chunks'] = self._ongoing_episodes_for_metrics[episodes[idx].id_]\n        make_callback(which, callbacks_objects=self._callbacks, callbacks_functions=getattr(self.config, f'callbacks_{which}'), kwargs=kwargs)\n\n    def _increase_sampled_metrics(self, num_steps, next_obs, episode):\n        self.metrics.log_value(NUM_ENV_STEPS_SAMPLED, num_steps, reduce='sum', clear_on_reduce=True)\n        self.metrics.log_value(NUM_ENV_STEPS_SAMPLED_LIFETIME, num_steps, reduce='sum', with_throughput=True)\n        if episode.is_done:\n            self.metrics.log_value(NUM_EPISODES, 1, reduce='sum', clear_on_reduce=True)\n            self.metrics.log_value(NUM_EPISODES_LIFETIME, 1, reduce='sum')\n        for aid in next_obs:\n            self.metrics.log_value((NUM_AGENT_STEPS_SAMPLED, str(aid)), 1, reduce='sum', clear_on_reduce=True)\n            self.metrics.log_value((NUM_AGENT_STEPS_SAMPLED_LIFETIME, str(aid)), 1, reduce='sum')\n            self.metrics.log_value((NUM_MODULE_STEPS_SAMPLED, episode.module_for(aid)), 1, reduce='sum', clear_on_reduce=True)\n            self.metrics.log_value((NUM_MODULE_STEPS_SAMPLED_LIFETIME, episode.module_for(aid)), 1, reduce='sum')\n        return num_steps\n\n    def _log_episode_metrics(self, length, ret, sec, agents=None, modules=None, agent_steps=None):\n        win = max(1, int(math.ceil(self.config.metrics_num_episodes_for_smoothing / (self.config.num_env_runners or 1))))\n        self.metrics.log_dict({EPISODE_LEN_MEAN: length, EPISODE_RETURN_MEAN: ret, EPISODE_DURATION_SEC_MEAN: sec, **({'agent_episode_returns_mean': agents, 'module_episode_returns_mean': modules, 'agent_steps': agent_steps} if agents is not None else {})}, window=win)\n        self.metrics.log_dict({EPISODE_LEN_MIN: length, EPISODE_RETURN_MIN: ret}, reduce='min', window=win)\n        self.metrics.log_dict({EPISODE_LEN_MAX: length, EPISODE_RETURN_MAX: ret}, reduce='max', window=win)\n\n    @staticmethod\n    def _prune_zero_len_sa_episodes(episode: MultiAgentEpisode):\n        for agent_id, agent_eps in episode.agent_episodes.copy().items():\n            if not agent_eps:\n                del episode.agent_episodes[agent_id]\n\n    @Deprecated(new=\"MultiAgentEnvRunner.get_state(components='rl_module')\", error=False)\n    def get_weights(self, modules=None):\n        rl_module_state = self.get_state(components=COMPONENT_RL_MODULE)[COMPONENT_RL_MODULE]\n        return rl_module_state\n\n    @Deprecated(new='MultiAgentEnvRunner.set_state()', error=False)\n    def set_weights(self, weights: ModelWeights, global_vars: Optional[Dict]=None, weights_seq_no: int=0) -> None:\n        assert global_vars is None\n        return self.set_state({COMPONENT_RL_MODULE: weights, WEIGHTS_SEQ_NO: weights_seq_no})\n\nExample skeleton:\n\n\nclass PPO(object):\n    '''A vectorized implementation of the PPO algorithm by John Schulman.'''\n\n    def __init__(self, batch_env, step, is_training, should_log, config):\n        '''Create an instance of the PPO algorithm.\n        Args:\n          batch_env: In-graph batch environment.\n          step: Integer tensor holding the current training step.\n          is_training: Boolean tensor for whether the algorithm should train.\n          should_log: Boolean tensor for whether summaries should be returned.\n          config: Object containing the agent configuration as attributes.\n        '''\n        pass\n\n                    def var_like(x):\n                                        pass\n\n    def begin_episode(self, agent_indices):\n        '''Reset the recurrent states and stored episode.\n        Args:\n          agent_indices: Tensor containing current batch indices.\n        Returns:\n          Summary tensor.\n        '''\n        pass\n\n    def perform(self, agent_indices, observ):\n        '''Compute batch of actions and a summary for a batch of observation.\n        Args:\n          agent_indices: Tensor containing current batch indices.\n          observ: Tensor of a batch of observations for all agents.\n        Returns:\n          Tuple of action batch tensor and summary tensor.\n        '''\n        pass\n\n    def experience(self, agent_indices, observ, action, reward, unused_done, unused_nextob):\n        '''Process the transition tuple of the current step.\n        When training, add the current transition tuple to the memory and update\n        the streaming statistics for observations and rewards. A summary string is\n        returned if requested at this step.\n        Args:\n          agent_indices: Tensor containing current batch indices.\n          observ: Batch tensor of observations.\n          action: Batch tensor of actions.\n          reward: Batch tensor of rewards.\n          unused_done: Batch tensor of done flags.\n          unused_nextob: Batch tensor of successor observations.\n        Returns:\n          Summary tensor.\n        '''\n        pass\n\n    def _define_experience(self, agent_indices, observ, action, reward):\n        '''Implement the branch of experience() entered during training.'''\n        pass\n\n    def end_episode(self, agent_indices):\n        '''Add episodes to the memory and perform update steps if memory is full.\n        During training, add the collected episodes of the batch indices that\n        finished their episode to the memory. If the memory is full, train on it,\n        and then clear the memory. A summary string is returned if requested at\n        this step.\n        Args:\n          agent_indices: Tensor containing current batch indices.\n        Returns:\n           Summary tensor.\n        '''\n        pass\n\n    def _initialize_policy(self):\n        '''Initialize the policy.\n        Run the policy network on dummy data to initialize its parameters for later\n        reuse and to analyze the policy distribution. Initializes the attributes\n        `self._network` and `self._policy_type`.\n        Raises:\n          ValueError: Invalid policy distribution.\n        Returns:\n          Parameters of the policy distribution and policy state.\n        '''\n        pass\n\n        def is_tensor(x):\n                pass\n\n        def set_batch_dim(x):\n                pass\n\n    def _initialize_memory(self, policy_params):\n        '''Initialize temporary and permanent memory.\n        Args:\n          policy_params: Nested tuple of policy parameters with all dimensions set.\n        Initializes the attributes `self._current_episodes`,\n        `self._finished_episodes`, and `self._num_finished_episodes`. The episodes\n        memory serves to collect multiple episodes in parallel. Finished episodes\n        are copied into the next free slot of the second memory. The memory index\n        points to the next free slot.\n        '''\n        pass\n\n    def _define_end_episode(self, agent_indices):\n        '''Implement the branch of end_episode() entered during training.'''\n        pass\n\n    def _training(self):\n        '''Perform multiple training iterations of both policy and value baseline.\n        Training on the episodes collected in the memory. Reset the memory\n        afterwards. Always returns a summary string.\n        Returns:\n          Summary tensor.\n        '''\n        pass\n\n    def _perform_update_steps(self, observ, action, old_policy_params, reward, length):\n        '''Perform multiple update steps of value function and policy.\n        The advantage is computed once at the beginning and shared across\n        iterations. We need to decide for the summary of one iteration, and thus\n        choose the one after half of the iterations.\n        Args:\n          observ: Sequences of observations.\n          action: Sequences of actions.\n          old_policy_params: Parameters of the behavioral policy.\n          reward: Sequences of rewards.\n          length: Batch of sequence lengths.\n        Returns:\n          Summary tensor.\n        '''\n        pass\n\n    def _update_step(self, sequence):\n        '''Compute the current combined loss and perform a gradient update step.\n        The sequences must be a dict containing the keys `length` and `sequence`,\n        where the latter is a tuple containing observations, actions, parameters of\n        the behavioral policy, rewards, and advantages.\n        Args:\n          sequence: Sequences of episodes or chunks of episodes.\n        Returns:\n          Tuple of value loss, policy loss, and summary tensor.\n        '''\n        pass\n\n    def _value_loss(self, observ, reward, length):\n        '''Compute the loss function for the value baseline.\n        The value loss is the difference between empirical and approximated returns\n        over the collected episodes. Returns the loss tensor and a summary strin.\n        Args:\n          observ: Sequences of observations.\n          reward: Sequences of reward.\n          length: Batch of sequence lengths.\n        Returns:\n          Tuple of loss tensor and summary tensor.\n        '''\n        pass\n\n    def _policy_loss(self, old_policy, policy, action, advantage, length):\n        '''Compute the policy loss composed of multiple components.\n        1. The policy gradient loss is importance sampled from the data-collecting\n           policy at the beginning of training.\n        2. The second term is a KL penalty between the policy at the beginning of\n           training and the current policy.\n        3. Additionally, if this KL already changed more than twice the target\n           amount, we activate a strong penalty discouraging further divergence.\n        Args:\n          old_policy: Action distribution of the behavioral policy.\n          policy: Sequences of distribution params of the current policy.\n          action: Sequences of actions.\n          advantage: Sequences of advantages.\n          length: Batch of sequence lengths.\n        Returns:\n          Tuple of loss tensor and summary tensor.\n        '''\n        pass\n\n    def _adjust_penalty(self, observ, old_policy_params, length):\n        '''Adjust the KL policy between the behavioral and current policy.\n        Compute how much the policy actually changed during the multiple\n        update steps. Adjust the penalty strength for the next training phase if we\n        overshot or undershot the target divergence too much.\n        Args:\n          observ: Sequences of observations.\n          old_policy_params: Parameters of the behavioral policy.\n          length: Batch of sequence lengths.\n        Returns:\n          Summary tensor.\n        '''\n        pass\n\n    def _mask(self, tensor, length, padding_value=0):\n        '''Set padding elements of a batch of sequences to a constant.\n        Useful for setting padding elements to zero before summing along the time\n        dimension, or for preventing infinite results in padding elements.\n        Args:\n          tensor: Tensor of sequences.\n          length: Batch of sequence lengths.\n          padding_value: Value to write into padding elements.\n        Returns:\n          Masked sequences.\n        '''\n        pass\nCorresponding class implementation:\n\nfrom agents import parts\nimport functools\nimport tensorflow as tf\nfrom agents.algorithms.ppo import utility\nfrom agents import tools\n\nclass PPO(object):\n    \"\"\"A vectorized implementation of the PPO algorithm by John Schulman.\"\"\"\n\n    def __init__(self, batch_env, step, is_training, should_log, config):\n        \"\"\"Create an instance of the PPO algorithm.\n\n        Args:\n          batch_env: In-graph batch environment.\n          step: Integer tensor holding the current training step.\n          is_training: Boolean tensor for whether the algorithm should train.\n          should_log: Boolean tensor for whether summaries should be returned.\n          config: Object containing the agent configuration as attributes.\n        \"\"\"\n        self._batch_env = batch_env\n        self._step = step\n        self._is_training = is_training\n        self._should_log = should_log\n        self._config = config\n        self._observ_filter = parts.StreamingNormalize(self._batch_env.observ[0], center=True, scale=True, clip=5, name='normalize_observ')\n        self._reward_filter = parts.StreamingNormalize(self._batch_env.reward[0], center=False, scale=True, clip=10, name='normalize_reward')\n        self._use_gpu = self._config.use_gpu and utility.available_gpus()\n        policy_params, state = self._initialize_policy()\n        self._initialize_memory(policy_params)\n        with tf.device('/gpu:0' if self._use_gpu else '/cpu:0'):\n            self._optimizer = self._config.optimizer(self._config.learning_rate)\n        self._penalty = tf.Variable(self._config.kl_init_penalty, False, dtype=tf.float32)\n        with tf.variable_scope('ppo_temporary'):\n            with tf.device('/gpu:0'):\n                if state is None:\n                    self._last_state = None\n                else:\n\n                    def var_like(x):\n                        return tf.Variable(lambda: tf.zeros_like(x), False)\n                    self._last_state = tools.nested.map(var_like, state)\n        with tf.variable_scope('ppo_temporary'):\n            self._last_action = tf.Variable(tf.zeros_like(self._batch_env.action), False, name='last_action')\n            self._last_policy = tools.nested.map(lambda x: tf.Variable(tf.zeros_like(x[:, 0], False)), policy_params)\n\n    def begin_episode(self, agent_indices):\n        \"\"\"Reset the recurrent states and stored episode.\n\n        Args:\n          agent_indices: Tensor containing current batch indices.\n\n        Returns:\n          Summary tensor.\n        \"\"\"\n        with tf.name_scope('begin_episode/'):\n            if self._last_state is None:\n                reset_state = tf.no_op()\n            else:\n                reset_state = utility.reinit_nested_vars(self._last_state, agent_indices)\n            reset_buffer = self._current_episodes.clear(agent_indices)\n            with tf.control_dependencies([reset_state, reset_buffer]):\n                return tf.constant('')\n\n    def perform(self, agent_indices, observ):\n        \"\"\"Compute batch of actions and a summary for a batch of observation.\n\n        Args:\n          agent_indices: Tensor containing current batch indices.\n          observ: Tensor of a batch of observations for all agents.\n\n        Returns:\n          Tuple of action batch tensor and summary tensor.\n        \"\"\"\n        with tf.name_scope('perform/'):\n            observ = self._observ_filter.transform(observ)\n            if self._last_state is None:\n                state = None\n            else:\n                state = tools.nested.map(lambda x: tf.gather(x, agent_indices), self._last_state)\n            with tf.device('/gpu:0' if self._use_gpu else '/cpu:0'):\n                output = self._network(observ[:, None], tf.ones(observ.shape[0]), state)\n            action = tf.cond(self._is_training, output.policy.sample, output.policy.mode)\n            logprob = output.policy.log_prob(action)[:, 0]\n            summary = tf.cond(self._should_log, lambda: tf.summary.merge([tf.summary.histogram('mode', output.policy.mode()[:, 0]), tf.summary.histogram('action', action[:, 0]), tf.summary.histogram('logprob', logprob)]), str)\n            if self._last_state is None:\n                assign_state = tf.no_op()\n            else:\n                assign_state = utility.assign_nested_vars(self._last_state, output.state, agent_indices)\n            remember_last_action = tf.scatter_update(self._last_action, agent_indices, action[:, 0])\n            policy_params = tools.nested.filter(lambda x: isinstance(x, tf.Tensor), output.policy.parameters)\n            assert policy_params, 'Policy has no parameters to store.'\n            remember_last_policy = tools.nested.map(lambda var, val: tf.scatter_update(var, agent_indices, val[:, 0]), self._last_policy, policy_params, flatten=True)\n            with tf.control_dependencies((assign_state, remember_last_action) + remember_last_policy):\n                return (action[:, 0], tf.identity(summary))\n\n    def experience(self, agent_indices, observ, action, reward, unused_done, unused_nextob):\n        \"\"\"Process the transition tuple of the current step.\n\n        When training, add the current transition tuple to the memory and update\n        the streaming statistics for observations and rewards. A summary string is\n        returned if requested at this step.\n\n        Args:\n          agent_indices: Tensor containing current batch indices.\n          observ: Batch tensor of observations.\n          action: Batch tensor of actions.\n          reward: Batch tensor of rewards.\n          unused_done: Batch tensor of done flags.\n          unused_nextob: Batch tensor of successor observations.\n\n        Returns:\n          Summary tensor.\n        \"\"\"\n        with tf.name_scope('experience/'):\n            return tf.cond(self._is_training, lambda: self._define_experience(agent_indices, observ, action, reward), str)\n\n    def _define_experience(self, agent_indices, observ, action, reward):\n        \"\"\"Implement the branch of experience() entered during training.\"\"\"\n        update_filters = tf.summary.merge([self._observ_filter.update(observ), self._reward_filter.update(reward)])\n        with tf.control_dependencies([update_filters]):\n            if self._config.train_on_agent_action:\n                action = self._last_action\n            policy = tools.nested.map(lambda x: tf.gather(x, agent_indices), self._last_policy)\n            batch = (observ, action, policy, reward)\n            append = self._current_episodes.append(batch, agent_indices)\n        with tf.control_dependencies([append]):\n            norm_observ = self._observ_filter.transform(observ)\n            norm_reward = tf.reduce_mean(self._reward_filter.transform(reward))\n            summary = tf.cond(self._should_log, lambda: tf.summary.merge([update_filters, self._observ_filter.summary(), self._reward_filter.summary(), tf.summary.scalar('memory_size', self._num_finished_episodes), tf.summary.histogram('normalized_observ', norm_observ), tf.summary.histogram('action', self._last_action), tf.summary.scalar('normalized_reward', norm_reward)]), str)\n            return summary\n\n    def end_episode(self, agent_indices):\n        \"\"\"Add episodes to the memory and perform update steps if memory is full.\n\n        During training, add the collected episodes of the batch indices that\n        finished their episode to the memory. If the memory is full, train on it,\n        and then clear the memory. A summary string is returned if requested at\n        this step.\n\n        Args:\n          agent_indices: Tensor containing current batch indices.\n\n        Returns:\n           Summary tensor.\n        \"\"\"\n        with tf.name_scope('end_episode/'):\n            return tf.cond(self._is_training, lambda: self._define_end_episode(agent_indices), str)\n\n    def _initialize_policy(self):\n        \"\"\"Initialize the policy.\n\n        Run the policy network on dummy data to initialize its parameters for later\n        reuse and to analyze the policy distribution. Initializes the attributes\n        `self._network` and `self._policy_type`.\n\n        Raises:\n          ValueError: Invalid policy distribution.\n\n        Returns:\n          Parameters of the policy distribution and policy state.\n        \"\"\"\n        with tf.device('/gpu:0' if self._use_gpu else '/cpu:0'):\n            network = functools.partial(self._config.network, self._config, self._batch_env.action_space)\n            self._network = tf.make_template('network', network)\n            output = self._network(tf.zeros_like(self._batch_env.observ)[:, None], tf.ones(len(self._batch_env)))\n        if output.policy.event_shape != self._batch_env.action.shape[1:]:\n            message = 'Policy event shape {} does not match action shape {}.'\n            message = message.format(output.policy.event_shape, self._batch_env.action.shape[1:])\n            raise ValueError(message)\n        self._policy_type = type(output.policy)\n\n        def is_tensor(x):\n            return isinstance(x, tf.Tensor)\n        policy_params = tools.nested.filter(is_tensor, output.policy.parameters)\n\n        def set_batch_dim(x):\n            return utility.set_dimension(x, 0, len(self._batch_env))\n        tools.nested.map(set_batch_dim, policy_params)\n        if output.state is not None:\n            tools.nested.map(set_batch_dim, output.state)\n        return (policy_params, output.state)\n\n    def _initialize_memory(self, policy_params):\n        \"\"\"Initialize temporary and permanent memory.\n\n        Args:\n          policy_params: Nested tuple of policy parameters with all dimensions set.\n\n        Initializes the attributes `self._current_episodes`,\n        `self._finished_episodes`, and `self._num_finished_episodes`. The episodes\n        memory serves to collect multiple episodes in parallel. Finished episodes\n        are copied into the next free slot of the second memory. The memory index\n        points to the next free slot.\n        \"\"\"\n        template = (self._batch_env.observ[0], self._batch_env.action[0], tools.nested.map(lambda x: x[0, 0], policy_params), self._batch_env.reward[0])\n        with tf.variable_scope('ppo_temporary'):\n            self._current_episodes = parts.EpisodeMemory(template, len(self._batch_env), self._config.max_length, 'episodes')\n        self._finished_episodes = parts.EpisodeMemory(template, self._config.update_every, self._config.max_length, 'memory')\n        self._num_finished_episodes = tf.Variable(0, False)\n\n    def _define_end_episode(self, agent_indices):\n        \"\"\"Implement the branch of end_episode() entered during training.\"\"\"\n        episodes, length = self._current_episodes.data(agent_indices)\n        space_left = self._config.update_every - self._num_finished_episodes\n        use_episodes = tf.range(tf.minimum(tf.shape(agent_indices)[0], space_left))\n        episodes = tools.nested.map(lambda x: tf.gather(x, use_episodes), episodes)\n        append = self._finished_episodes.replace(episodes, tf.gather(length, use_episodes), use_episodes + self._num_finished_episodes)\n        with tf.control_dependencies([append]):\n            increment_index = self._num_finished_episodes.assign_add(tf.shape(use_episodes)[0])\n        with tf.control_dependencies([increment_index]):\n            memory_full = self._num_finished_episodes >= self._config.update_every\n            return tf.cond(memory_full, self._training, str)\n\n    def _training(self):\n        \"\"\"Perform multiple training iterations of both policy and value baseline.\n\n        Training on the episodes collected in the memory. Reset the memory\n        afterwards. Always returns a summary string.\n\n        Returns:\n          Summary tensor.\n        \"\"\"\n        with tf.device('/gpu:0' if self._use_gpu else '/cpu:0'):\n            with tf.name_scope('training'):\n                assert_full = tf.assert_equal(self._num_finished_episodes, self._config.update_every)\n                with tf.control_dependencies([assert_full]):\n                    data = self._finished_episodes.data()\n                (observ, action, old_policy_params, reward), length = data\n                old_policy_params = tools.nested.map(lambda param: self._mask(param, length, 1), old_policy_params)\n                with tf.control_dependencies([tf.assert_greater(length, 0)]):\n                    length = tf.identity(length)\n                observ = self._observ_filter.transform(observ)\n                reward = self._reward_filter.transform(reward)\n                update_summary = self._perform_update_steps(observ, action, old_policy_params, reward, length)\n                with tf.control_dependencies([update_summary]):\n                    penalty_summary = self._adjust_penalty(observ, old_policy_params, length)\n                with tf.control_dependencies([penalty_summary]):\n                    clear_memory = tf.group(self._finished_episodes.clear(), self._num_finished_episodes.assign(0))\n                with tf.control_dependencies([clear_memory]):\n                    weight_summary = utility.variable_summaries(tf.trainable_variables(), self._config.weight_summaries)\n                    return tf.summary.merge([update_summary, penalty_summary, weight_summary])\n\n    def _perform_update_steps(self, observ, action, old_policy_params, reward, length):\n        \"\"\"Perform multiple update steps of value function and policy.\n\n        The advantage is computed once at the beginning and shared across\n        iterations. We need to decide for the summary of one iteration, and thus\n        choose the one after half of the iterations.\n\n        Args:\n          observ: Sequences of observations.\n          action: Sequences of actions.\n          old_policy_params: Parameters of the behavioral policy.\n          reward: Sequences of rewards.\n          length: Batch of sequence lengths.\n\n        Returns:\n          Summary tensor.\n        \"\"\"\n        return_ = utility.discounted_return(reward, length, self._config.discount)\n        value = self._network(observ, length).value\n        if self._config.gae_lambda:\n            advantage = utility.lambda_advantage(reward, value, length, self._config.discount, self._config.gae_lambda)\n        else:\n            advantage = return_ - value\n        mean, variance = tf.nn.moments(advantage, axes=[0, 1], keep_dims=True)\n        advantage = (advantage - mean) / (tf.sqrt(variance) + 1e-08)\n        advantage = tf.Print(advantage, [tf.reduce_mean(return_), tf.reduce_mean(value)], 'return and value: ')\n        advantage = tf.Print(advantage, [tf.reduce_mean(advantage)], 'normalized advantage: ')\n        episodes = (observ, action, old_policy_params, reward, advantage)\n        value_loss, policy_loss, summary = parts.iterate_sequences(self._update_step, [0.0, 0.0, ''], episodes, length, self._config.chunk_length, self._config.batch_size, self._config.update_epochs, padding_value=1)\n        print_losses = tf.group(tf.Print(0, [tf.reduce_mean(value_loss)], 'value loss: '), tf.Print(0, [tf.reduce_mean(policy_loss)], 'policy loss: '))\n        with tf.control_dependencies([value_loss, policy_loss, print_losses]):\n            return summary[self._config.update_epochs // 2]\n\n    def _update_step(self, sequence):\n        \"\"\"Compute the current combined loss and perform a gradient update step.\n\n        The sequences must be a dict containing the keys `length` and `sequence`,\n        where the latter is a tuple containing observations, actions, parameters of\n        the behavioral policy, rewards, and advantages.\n\n        Args:\n          sequence: Sequences of episodes or chunks of episodes.\n\n        Returns:\n          Tuple of value loss, policy loss, and summary tensor.\n        \"\"\"\n        observ, action, old_policy_params, reward, advantage = sequence['sequence']\n        length = sequence['length']\n        old_policy = self._policy_type(**old_policy_params)\n        value_loss, value_summary = self._value_loss(observ, reward, length)\n        network = self._network(observ, length)\n        policy_loss, policy_summary = self._policy_loss(old_policy, network.policy, action, advantage, length)\n        network_loss = network.get('loss', 0.0)\n        loss = policy_loss + value_loss + tf.reduce_mean(network_loss)\n        gradients, variables = zip(*self._optimizer.compute_gradients(loss))\n        optimize = self._optimizer.apply_gradients(zip(gradients, variables))\n        summary = tf.summary.merge([value_summary, policy_summary, tf.summary.histogram('network_loss', network_loss), tf.summary.scalar('avg_network_loss', tf.reduce_mean(network_loss)), tf.summary.scalar('gradient_norm', tf.global_norm(gradients)), utility.gradient_summaries(zip(gradients, variables))])\n        with tf.control_dependencies([optimize]):\n            return [tf.identity(x) for x in (value_loss, policy_loss, summary)]\n\n    def _value_loss(self, observ, reward, length):\n        \"\"\"Compute the loss function for the value baseline.\n\n        The value loss is the difference between empirical and approximated returns\n        over the collected episodes. Returns the loss tensor and a summary strin.\n\n        Args:\n          observ: Sequences of observations.\n          reward: Sequences of reward.\n          length: Batch of sequence lengths.\n\n        Returns:\n          Tuple of loss tensor and summary tensor.\n        \"\"\"\n        with tf.name_scope('value_loss'):\n            value = self._network(observ, length).value\n            return_ = utility.discounted_return(reward, length, self._config.discount)\n            advantage = return_ - value\n            value_loss = 0.5 * self._mask(advantage ** 2, length)\n            summary = tf.summary.merge([tf.summary.histogram('value_loss', value_loss), tf.summary.scalar('avg_value_loss', tf.reduce_mean(value_loss))])\n            value_loss = tf.reduce_mean(value_loss)\n            return (tf.check_numerics(value_loss, 'value_loss'), summary)\n\n    def _policy_loss(self, old_policy, policy, action, advantage, length):\n        \"\"\"Compute the policy loss composed of multiple components.\n\n        1. The policy gradient loss is importance sampled from the data-collecting\n           policy at the beginning of training.\n        2. The second term is a KL penalty between the policy at the beginning of\n           training and the current policy.\n        3. Additionally, if this KL already changed more than twice the target\n           amount, we activate a strong penalty discouraging further divergence.\n\n        Args:\n          old_policy: Action distribution of the behavioral policy.\n          policy: Sequences of distribution params of the current policy.\n          action: Sequences of actions.\n          advantage: Sequences of advantages.\n          length: Batch of sequence lengths.\n\n        Returns:\n          Tuple of loss tensor and summary tensor.\n        \"\"\"\n        with tf.name_scope('policy_loss'):\n            kl = tf.contrib.distributions.kl_divergence(old_policy, policy)\n            kl = tf.check_numerics(kl, 'kl')\n            kl = tf.reduce_mean(self._mask(kl, length), 1)\n            policy_gradient = tf.exp(policy.log_prob(action) - old_policy.log_prob(action))\n            surrogate_loss = -tf.reduce_mean(self._mask(policy_gradient * tf.stop_gradient(advantage), length), 1)\n            surrogate_loss = tf.check_numerics(surrogate_loss, 'surrogate_loss')\n            kl_penalty = self._penalty * kl\n            cutoff_threshold = self._config.kl_target * self._config.kl_cutoff_factor\n            cutoff_count = tf.reduce_sum(tf.cast(kl > cutoff_threshold, tf.int32))\n            with tf.control_dependencies([tf.cond(cutoff_count > 0, lambda: tf.Print(0, [cutoff_count], 'kl cutoff! '), int)]):\n                kl_cutoff = self._config.kl_cutoff_coef * tf.cast(kl > cutoff_threshold, tf.float32) * (kl - cutoff_threshold) ** 2\n            policy_loss = surrogate_loss + kl_penalty + kl_cutoff\n            entropy = tf.reduce_mean(policy.entropy(), axis=1)\n            if self._config.entropy_regularization:\n                policy_loss -= self._config.entropy_regularization * entropy\n            summary = tf.summary.merge([tf.summary.histogram('entropy', entropy), tf.summary.histogram('kl', kl), tf.summary.histogram('surrogate_loss', surrogate_loss), tf.summary.histogram('kl_penalty', kl_penalty), tf.summary.histogram('kl_cutoff', kl_cutoff), tf.summary.histogram('kl_penalty_combined', kl_penalty + kl_cutoff), tf.summary.histogram('policy_loss', policy_loss), tf.summary.scalar('avg_surr_loss', tf.reduce_mean(surrogate_loss)), tf.summary.scalar('avg_kl_penalty', tf.reduce_mean(kl_penalty)), tf.summary.scalar('avg_policy_loss', tf.reduce_mean(policy_loss))])\n            policy_loss = tf.reduce_mean(policy_loss, 0)\n            return (tf.check_numerics(policy_loss, 'policy_loss'), summary)\n\n    def _adjust_penalty(self, observ, old_policy_params, length):\n        \"\"\"Adjust the KL policy between the behavioral and current policy.\n\n        Compute how much the policy actually changed during the multiple\n        update steps. Adjust the penalty strength for the next training phase if we\n        overshot or undershot the target divergence too much.\n\n        Args:\n          observ: Sequences of observations.\n          old_policy_params: Parameters of the behavioral policy.\n          length: Batch of sequence lengths.\n\n        Returns:\n          Summary tensor.\n        \"\"\"\n        old_policy = self._policy_type(**old_policy_params)\n        with tf.name_scope('adjust_penalty'):\n            network = self._network(observ, length)\n            print_penalty = tf.Print(0, [self._penalty], 'current penalty: ')\n            with tf.control_dependencies([print_penalty]):\n                kl_change = tf.reduce_mean(self._mask(tf.contrib.distributions.kl_divergence(old_policy, network.policy), length))\n                kl_change = tf.Print(kl_change, [kl_change], 'kl change: ')\n                maybe_increase = tf.cond(kl_change > 1.3 * self._config.kl_target, lambda: tf.Print(self._penalty.assign(self._penalty * 1.5), [0], 'increase penalty '), float)\n                maybe_decrease = tf.cond(kl_change < 0.7 * self._config.kl_target, lambda: tf.Print(self._penalty.assign(self._penalty / 1.5), [0], 'decrease penalty '), float)\n            with tf.control_dependencies([maybe_increase, maybe_decrease]):\n                return tf.summary.merge([tf.summary.scalar('kl_change', kl_change), tf.summary.scalar('penalty', self._penalty)])\n\n    def _mask(self, tensor, length, padding_value=0):\n        \"\"\"Set padding elements of a batch of sequences to a constant.\n\n        Useful for setting padding elements to zero before summing along the time\n        dimension, or for preventing infinite results in padding elements.\n\n        Args:\n          tensor: Tensor of sequences.\n          length: Batch of sequence lengths.\n          padding_value: Value to write into padding elements.\n\n        Returns:\n          Masked sequences.\n        \"\"\"\n        with tf.name_scope('mask'):\n            range_ = tf.range(tensor.shape[1].value)\n            mask = range_[None, :] < length[:, None]\n            if tensor.shape.ndims > 2:\n                for _ in range(tensor.shape.ndims - 2):\n                    mask = mask[..., None]\n                mask = tf.tile(mask, [1, 1] + tensor.shape[2:].as_list())\n            masked = tf.where(mask, tensor, padding_value * tf.ones_like(tensor))\n            return tf.check_numerics(masked, 'masked')\n\nNow, implement the following class. Do not explain the code. The given class skeleton is as follows:\n\nclass CachedReplayDataLoader:\n    '''A DataLoader wrapper that asynchronously caches and replays data batches to\n    mitigate slow loading issues. Assumes the underlying DataLoader is infinite.\n    This class delegates all augmentation logic to an external augmentation function,\n    which takes a batch from the data loader and returns multiple augmented versions.\n    The class handles caching these augmented batches and optionally concatenating\n    them when yielded.\n    Attributes:\n        data_loader (DataLoader): The underlying infinite DataLoader.\n        cache_size (int): Maximum number of augmented batches to store in the cache.\n        cache_augmentation_fn (Callable): Function to create multiple augmented versions of each batch.\n        concat_size (int): Number of batches to concatenate when yielding from the iterator.\n        rng (numpy.random.Generator): Controlled random number generator for deterministic behavior.\n    '''\n\n    def __init__(self, data_loader: DataLoader, cache_size: int, cache_augmentation_fn: Callable[[dict], list[dict]], concat_size: int=1, name: str='cached_replay_dataloader') -> None:\n        '''Initialize the CachedReplayDataLoader.\n        Args:\n            data_loader (DataLoader): The infinite DataLoader to fetch data batches from.\n            cache_size (int): Maximum number of augmented data batches to store in the cache.\n            cache_augmentation_fn (Callable[[Dict], List[Dict]]): Function that takes a batch and returns\n                a list of augmented batches.\n            concat_size (int, optional): Number of batches to concatenate when yielding. Defaults to 1.\n        '''\n        pass\n\n    def _prefetch_loop(self) -> None:\n        '''Continuously fetch batches from the DataLoader, augment them, and store in the cache.\n        If the cache is full (reaches `cache_size`), this loop waits until space is available.\n        Catches exceptions and stores them for later propagation to the main thread.\n        '''\n        pass\n\n    def _set_exception(self, exception: Exception, context: str='') -> None:\n        '''Store an exception from the background thread with context information.\n        Args:\n            exception (Exception): The exception that was raised\n            context (str, optional): Additional context about where the error occurred\n        '''\n        pass\n\n    def _check_for_errors(self) -> None:\n        '''Check if the background thread has encountered an error and raise it if so.'''\n        pass\n\n    def __iter__(self) -> Iterator[dict]:\n        '''Yield augmented data batches from the cache, optionally concatenated based on concat_size.\n        This method starts the background prefetch thread if it hasn't been started yet.\n        If concat_size > 1, it collects multiple batches and concatenates them.\n        Raises:\n            RuntimeError: If the background thread encountered an error\n        '''\n        pass\n\n    def __len__(self) -> int:\n        '''Return the length of the underlying DataLoader.'''\n        pass\n\n    def close(self) -> None:\n        '''Stop the prefetch thread and clear the cache.\n        Also checks for any errors in the background thread and raises them.\n        '''\n        pass"
  }
]