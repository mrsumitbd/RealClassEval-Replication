[
  {
    "role": "assistant",
    "content": "You are an expert Python programmer who can correctly implement complete Python classes based on the provided class skeleton."
  },
  {
    "role": "user",
    "content": "For your understanding, following are some (skeleton, code) examples:\n\nExample skeleton:\n\n\nclass Dmlab(tfds.core.GeneratorBasedBuilder):\n    '''Dmlab dataset.'''\n\n    def _info(self):\n        pass\n\n    def _split_generators(self, dl_manager):\n        pass\n\n    def _parse_single_image(self, example_proto):\n        '''Parses single video from the input tfrecords.\n        Args:\n          example_proto: tfExample proto with a single video.\n        Returns:\n          dict with all frames, positions and actions.\n        '''\n        pass\n\n    def _generate_examples(self, images_dir_path, split_name):\n        pass\nCorresponding class implementation:\n\nimport os\nfrom tensorflow_datasets.core.utils.lazy_imports_utils import tensorflow as tf\nfrom absl import logging\nimport tensorflow_datasets.public_api as tfds\nimport io\n\nclass Dmlab(tfds.core.GeneratorBasedBuilder):\n    \"\"\"Dmlab dataset.\"\"\"\n    VERSION = tfds.core.Version('2.0.1')\n\n    def _info(self):\n        return tfds.core.DatasetInfo(builder=self, description='\\n        The Dmlab dataset contains frames observed by the agent acting in the\\n        DeepMind Lab environment, which are annotated by the distance between\\n        the agent and various objects present in the environment. The goal is to\\n        is to evaluate the ability of a visual model to reason about distances\\n        from the visual input in 3D environments. The Dmlab dataset consists of\\n        360x480 color images in 6 classes. The classes are\\n        {close, far, very far} x {positive reward, negative reward}\\n        respectively.', features=tfds.features.FeaturesDict({'image': tfds.features.Image(shape=(360, 480, 3), encoding_format='jpeg'), 'filename': tfds.features.Text(), 'label': tfds.features.ClassLabel(num_classes=6)}), homepage='https://github.com/google-research/task_adaptation', citation='@article{zhai2019visual,\\n        title={The Visual Task Adaptation Benchmark},\\n        author={Xiaohua Zhai and Joan Puigcerver and Alexander Kolesnikov and\\n               Pierre Ruyssen and Carlos Riquelme and Mario Lucic and\\n               Josip Djolonga and Andre Susano Pinto and Maxim Neumann and\\n               Alexey Dosovitskiy and Lucas Beyer and Olivier Bachem and\\n               Michael Tschannen and Marcin Michalski and Olivier Bousquet and\\n               Sylvain Gelly and Neil Houlsby},\\n                              year={2019},\\n                              eprint={1910.04867},\\n                              archivePrefix={arXiv},\\n                              primaryClass={cs.CV},\\n                              url = {https://arxiv.org/abs/1910.04867}\\n                          }', supervised_keys=('image', 'label'))\n\n    def _split_generators(self, dl_manager):\n        path = dl_manager.download_and_extract(_URL)\n        return [tfds.core.SplitGenerator(name=tfds.Split.TRAIN, gen_kwargs={'images_dir_path': path, 'split_name': 'train'}), tfds.core.SplitGenerator(name=tfds.Split.VALIDATION, gen_kwargs={'images_dir_path': path, 'split_name': 'validation'}), tfds.core.SplitGenerator(name=tfds.Split.TEST, gen_kwargs={'images_dir_path': path, 'split_name': 'test'})]\n\n    def _parse_single_image(self, example_proto):\n        \"\"\"Parses single video from the input tfrecords.\n\n        Args:\n          example_proto: tfExample proto with a single video.\n\n        Returns:\n          dict with all frames, positions and actions.\n        \"\"\"\n        feature_map = {'image': tf.io.FixedLenFeature(shape=[], dtype=tf.string), 'filename': tf.io.FixedLenFeature(shape=[], dtype=tf.string), 'label': tf.io.FixedLenFeature(shape=[], dtype=tf.int64)}\n        parse_single = tf.io.parse_single_example(example_proto, feature_map)\n        return parse_single\n\n    def _generate_examples(self, images_dir_path, split_name):\n        path_glob = os.path.join(images_dir_path, 'dmlab-{}.tfrecord*'.format(split_name))\n        files = tf.io.gfile.glob(path_glob)\n        logging.info('Reading data from %s.', ','.join(files))\n        with tf.Graph().as_default():\n            ds = tf.data.TFRecordDataset(files)\n            ds = ds.map(self._parse_single_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n            iterator = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\n            with tf.compat.v1.Session() as sess:\n                sess.run(tf.compat.v1.global_variables_initializer())\n                try:\n                    while True:\n                        result = sess.run(iterator)\n                        yield (result['filename'], {'image': io.BytesIO(result['image']), 'filename': result['filename'], 'label': result['label']})\n                except tf.errors.OutOfRangeError:\n                    return\n\nExample skeleton:\n\n\nclass DatasetBatches(ImageBatchesBase):\n    '''Class which stores batches of images from the dataset.'''\n\n    def __init__(self, datastore_client, storage_client, dataset_name):\n        '''Initializes DatasetBatches.\n        Args:\n          datastore_client: instance of CompetitionDatastoreClient\n          storage_client: instance of CompetitionStorageClient\n          dataset_name: name of the dataset ('dev' or 'final')\n        '''\n        pass\n\n    def _read_image_list(self, skip_image_ids=None):\n        '''Reads list of dataset images from the datastore.'''\n        pass\n\n    def init_from_storage_write_to_datastore(self, batch_size=100, allowed_epsilon=None, skip_image_ids=None, max_num_images=None):\n        '''Initializes dataset batches from the list of images in the datastore.\n        Args:\n          batch_size: batch size\n          allowed_epsilon: list of allowed epsilon or None to use default\n          skip_image_ids: list of image ids to skip\n          max_num_images: maximum number of images to read\n        '''\n        pass\nCorresponding class implementation:\n\nimport zipfile\nimport os\nfrom io import BytesIO\nimport copy\nimport logging\n\nclass DatasetBatches(ImageBatchesBase):\n    \"\"\"Class which stores batches of images from the dataset.\"\"\"\n\n    def __init__(self, datastore_client, storage_client, dataset_name):\n        \"\"\"Initializes DatasetBatches.\n\n        Args:\n          datastore_client: instance of CompetitionDatastoreClient\n          storage_client: instance of CompetitionStorageClient\n          dataset_name: name of the dataset ('dev' or 'final')\n        \"\"\"\n        super(DatasetBatches, self).__init__(datastore_client=datastore_client, entity_kind_batches=KIND_DATASET_BATCH, entity_kind_images=KIND_DATASET_IMAGE)\n        self._storage_client = storage_client\n        self._dataset_name = dataset_name\n\n    def _read_image_list(self, skip_image_ids=None):\n        \"\"\"Reads list of dataset images from the datastore.\"\"\"\n        if skip_image_ids is None:\n            skip_image_ids = []\n        images = self._storage_client.list_blobs(prefix=os.path.join('dataset', self._dataset_name) + '/')\n        zip_files = [i for i in images if i.endswith('.zip')]\n        if len(zip_files) == 1:\n            zip_name = zip_files[0]\n            logging.info('Reading list of images from zip file %s', zip_name)\n            blob = self._storage_client.get_blob(zip_name)\n            buf = BytesIO()\n            logging.info('Downloading zip')\n            blob.download_to_file(buf)\n            buf.seek(0)\n            logging.info('Reading content of the zip')\n            with zipfile.ZipFile(buf) as f:\n                images = [os.path.join(zip_name, os.path.basename(n)) for n in f.namelist() if n.endswith('.png')]\n            buf.close()\n            logging.info('Found %d images', len(images))\n        else:\n            logging.info('Reading list of images from png files in storage')\n            images = [i for i in images if i.endswith('.png')]\n            logging.info('Found %d images', len(images))\n        images = [i for i in images if os.path.basename(i)[:-4] not in skip_image_ids]\n        images = [(DATASET_IMAGE_ID_PATTERN.format(idx), i) for idx, i in enumerate(sorted(images))]\n        return images\n\n    def init_from_storage_write_to_datastore(self, batch_size=100, allowed_epsilon=None, skip_image_ids=None, max_num_images=None):\n        \"\"\"Initializes dataset batches from the list of images in the datastore.\n\n        Args:\n          batch_size: batch size\n          allowed_epsilon: list of allowed epsilon or None to use default\n          skip_image_ids: list of image ids to skip\n          max_num_images: maximum number of images to read\n        \"\"\"\n        if allowed_epsilon is None:\n            allowed_epsilon = copy.copy(DEFAULT_EPSILON)\n        self._dataset_batches = {}\n        images = self._read_image_list(skip_image_ids)\n        if max_num_images:\n            images = images[:max_num_images]\n        for batch_idx, batch_start in enumerate(range(0, len(images), batch_size)):\n            batch = images[batch_start:batch_start + batch_size]\n            batch_id = DATASET_BATCH_ID_PATTERN.format(batch_idx)\n            batch_epsilon = allowed_epsilon[batch_idx % len(allowed_epsilon)]\n            self.add_batch(batch_id, {'epsilon': batch_epsilon})\n            for image_id, image_path in batch:\n                self.add_image(batch_id, image_id, {'dataset_image_id': os.path.basename(image_path)[:-4], 'image_path': image_path})\n        self.write_to_datastore()\n\nNow, implement the following class. Do not explain the code. The given class skeleton is as follows:\n\nclass LoadTensor:\n    '''\n    A class for loading and processing tensor data for object detection tasks.\n    This class handles the loading and pre-processing of image data from PyTorch tensors, preparing them for\n    further processing in object detection pipelines.\n    Attributes:\n        im0 (torch.Tensor): The input tensor containing the image(s) with shape (B, C, H, W).\n        bs (int): Batch size, inferred from the shape of `im0`.\n        mode (str): Current processing mode, set to 'image'.\n        paths (List[str]): List of image paths or auto-generated filenames.\n    Methods:\n        _single_check: Validates and formats an input tensor.\n    Examples:\n        >>> import torch\n        >>> tensor = torch.rand(1, 3, 640, 640)\n        >>> loader = LoadTensor(tensor)\n        >>> paths, images, info = next(iter(loader))\n        >>> print(f\"Processed {len(images)} images\")\n    '''\n\n    def __init__(self, im0) -> None:\n        '''Initialize LoadTensor object for processing torch.Tensor image data.'''\n        pass\n    @staticmethod\n    def _single_check(im, stride=32):\n        '''Validates and formats a single image tensor, ensuring correct shape and normalization.'''\n        pass\n\n    def __iter__(self):\n        '''Yields an iterator object for iterating through tensor image data.'''\n        pass\n\n    def __next__(self):\n        '''Yields the next batch of tensor images and metadata for processing.'''\n        pass\n\n    def __len__(self):\n        '''Returns the batch size of the tensor input.'''\n        pass"
  }
]