[
  {
    "role": "assistant",
    "content": "You are an expert Python programmer who can correctly implement complete Python classes based on the provided class skeleton."
  },
  {
    "role": "user",
    "content": "For your understanding, following are some (skeleton, code) examples:\n\nExample skeleton:\n\n\nclass MobileNetV2Config(PretrainedConfig):\n    '''\n    This is the configuration class to store the configuration of a [`MobileNetV2Model`]. It is used to instantiate a\n    MobileNetV2 model according to the specified arguments, defining the model architecture. Instantiating a\n    configuration with the defaults will yield a similar configuration to that of the MobileNetV2\n    [google/mobilenet_v2_1.0_224](https://huggingface.co/google/mobilenet_v2_1.0_224) architecture.\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n    Args:\n        num_channels (`int`, *optional*, defaults to 3):\n            The number of input channels.\n        image_size (`int`, *optional*, defaults to 224):\n            The size (resolution) of each image.\n        depth_multiplier (`float`, *optional*, defaults to 1.0):\n            Shrinks or expands the number of channels in each layer. Default is 1.0, which starts the network with 32\n            channels. This is sometimes also called \"alpha\" or \"width multiplier\".\n        depth_divisible_by (`int`, *optional*, defaults to 8):\n            The number of channels in each layer will always be a multiple of this number.\n        min_depth (`int`, *optional*, defaults to 8):\n            All layers will have at least this many channels.\n        expand_ratio (`float`, *optional*, defaults to 6.0):\n            The number of output channels of the first layer in each block is input channels times expansion ratio.\n        output_stride (`int`, *optional*, defaults to 32):\n            The ratio between the spatial resolution of the input and output feature maps. By default the model reduces\n            the input dimensions by a factor of 32. If `output_stride` is 8 or 16, the model uses dilated convolutions\n            on the depthwise layers instead of regular convolutions, so that the feature maps never become more than 8x\n            or 16x smaller than the input image.\n        first_layer_is_expansion (`bool`, *optional*, defaults to `True`):\n            True if the very first convolution layer is also the expansion layer for the first expansion block.\n        finegrained_output (`bool`, *optional*, defaults to `True`):\n            If true, the number of output channels in the final convolution layer will stay large (1280) even if\n            `depth_multiplier` is less than 1.\n        hidden_act (`str` or `function`, *optional*, defaults to `\"relu6\"`):\n            The non-linear activation function (function or string) in the Transformer encoder and convolution layers.\n        tf_padding (`bool`, *optional*, defaults to `True`):\n            Whether to use TensorFlow padding rules on the convolution layers.\n        classifier_dropout_prob (`float`, *optional*, defaults to 0.8):\n            The dropout ratio for attached classifiers.\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        layer_norm_eps (`float`, *optional*, defaults to 0.001):\n            The epsilon used by the layer normalization layers.\n        semantic_loss_ignore_index (`int`, *optional*, defaults to 255):\n            The index that is ignored by the loss function of the semantic segmentation model.\n    Example:\n    ```python\n    >>> from transformers import MobileNetV2Config, MobileNetV2Model\n    >>> # Initializing a \"mobilenet_v2_1.0_224\" style configuration\n    >>> configuration = MobileNetV2Config()\n    >>> # Initializing a model from the \"mobilenet_v2_1.0_224\" style configuration\n    >>> model = MobileNetV2Model(configuration)\n    >>> # Accessing the model configuration\n    >>> configuration = model.config\n    ```'''\n\n    def __init__(self, num_channels=3, image_size=224, depth_multiplier=1.0, depth_divisible_by=8, min_depth=8, expand_ratio=6.0, output_stride=32, first_layer_is_expansion=True, finegrained_output=True, hidden_act='relu6', tf_padding=True, classifier_dropout_prob=0.8, initializer_range=0.02, layer_norm_eps=0.001, semantic_loss_ignore_index=255, **kwargs):\n        pass\nCorresponding class implementation:\n\nfrom ...configuration_utils import PretrainedConfig\n\nclass MobileNetV2Config(PretrainedConfig):\n    \"\"\"\n    This is the configuration class to store the configuration of a [`MobileNetV2Model`]. It is used to instantiate a\n    MobileNetV2 model according to the specified arguments, defining the model architecture. Instantiating a\n    configuration with the defaults will yield a similar configuration to that of the MobileNetV2\n    [google/mobilenet_v2_1.0_224](https://huggingface.co/google/mobilenet_v2_1.0_224) architecture.\n\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n\n    Args:\n        num_channels (`int`, *optional*, defaults to 3):\n            The number of input channels.\n        image_size (`int`, *optional*, defaults to 224):\n            The size (resolution) of each image.\n        depth_multiplier (`float`, *optional*, defaults to 1.0):\n            Shrinks or expands the number of channels in each layer. Default is 1.0, which starts the network with 32\n            channels. This is sometimes also called \"alpha\" or \"width multiplier\".\n        depth_divisible_by (`int`, *optional*, defaults to 8):\n            The number of channels in each layer will always be a multiple of this number.\n        min_depth (`int`, *optional*, defaults to 8):\n            All layers will have at least this many channels.\n        expand_ratio (`float`, *optional*, defaults to 6.0):\n            The number of output channels of the first layer in each block is input channels times expansion ratio.\n        output_stride (`int`, *optional*, defaults to 32):\n            The ratio between the spatial resolution of the input and output feature maps. By default the model reduces\n            the input dimensions by a factor of 32. If `output_stride` is 8 or 16, the model uses dilated convolutions\n            on the depthwise layers instead of regular convolutions, so that the feature maps never become more than 8x\n            or 16x smaller than the input image.\n        first_layer_is_expansion (`bool`, *optional*, defaults to `True`):\n            True if the very first convolution layer is also the expansion layer for the first expansion block.\n        finegrained_output (`bool`, *optional*, defaults to `True`):\n            If true, the number of output channels in the final convolution layer will stay large (1280) even if\n            `depth_multiplier` is less than 1.\n        hidden_act (`str` or `function`, *optional*, defaults to `\"relu6\"`):\n            The non-linear activation function (function or string) in the Transformer encoder and convolution layers.\n        tf_padding (`bool`, *optional*, defaults to `True`):\n            Whether to use TensorFlow padding rules on the convolution layers.\n        classifier_dropout_prob (`float`, *optional*, defaults to 0.8):\n            The dropout ratio for attached classifiers.\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        layer_norm_eps (`float`, *optional*, defaults to 0.001):\n            The epsilon used by the layer normalization layers.\n        semantic_loss_ignore_index (`int`, *optional*, defaults to 255):\n            The index that is ignored by the loss function of the semantic segmentation model.\n\n    Example:\n\n    ```python\n    >>> from transformers import MobileNetV2Config, MobileNetV2Model\n\n    >>> # Initializing a \"mobilenet_v2_1.0_224\" style configuration\n    >>> configuration = MobileNetV2Config()\n\n    >>> # Initializing a model from the \"mobilenet_v2_1.0_224\" style configuration\n    >>> model = MobileNetV2Model(configuration)\n\n    >>> # Accessing the model configuration\n    >>> configuration = model.config\n    ```\"\"\"\n    model_type = 'mobilenet_v2'\n\n    def __init__(self, num_channels=3, image_size=224, depth_multiplier=1.0, depth_divisible_by=8, min_depth=8, expand_ratio=6.0, output_stride=32, first_layer_is_expansion=True, finegrained_output=True, hidden_act='relu6', tf_padding=True, classifier_dropout_prob=0.8, initializer_range=0.02, layer_norm_eps=0.001, semantic_loss_ignore_index=255, **kwargs):\n        super().__init__(**kwargs)\n        if depth_multiplier <= 0:\n            raise ValueError('depth_multiplier must be greater than zero.')\n        self.num_channels = num_channels\n        self.image_size = image_size\n        self.depth_multiplier = depth_multiplier\n        self.depth_divisible_by = depth_divisible_by\n        self.min_depth = min_depth\n        self.expand_ratio = expand_ratio\n        self.output_stride = output_stride\n        self.first_layer_is_expansion = first_layer_is_expansion\n        self.finegrained_output = finegrained_output\n        self.hidden_act = hidden_act\n        self.tf_padding = tf_padding\n        self.classifier_dropout_prob = classifier_dropout_prob\n        self.initializer_range = initializer_range\n        self.layer_norm_eps = layer_norm_eps\n        self.semantic_loss_ignore_index = semantic_loss_ignore_index\n\nExample skeleton:\n\n\nclass HDBSCAN(BaseEstimator, ClusterMixin):\n    '''Perform HDBSCAN clustering from vector array or distance matrix.\n    HDBSCAN - Hierarchical Density-Based Spatial Clustering of Applications\n    with Noise. Performs DBSCAN over varying epsilon values and integrates\n    the result to find a clustering that gives the best stability over epsilon.\n    This allows HDBSCAN to find clusters of varying densities (unlike DBSCAN),\n    and be more robust to parameter selection.\n    Parameters\n    ----------\n    min_cluster_size : int, optional (default=5)\n        The minimum size of clusters; single linkage splits that contain\n        fewer points than this will be considered points \"falling out\" of a\n        cluster rather than a cluster splitting into two new clusters.\n    min_samples : int, optional (default=None)\n        The number of samples in a neighborhood for a point to be considered as a core point.\n        This includes the point itself. When None, defaults to min_cluster_size.\n    metric : string, or callable, optional (default='euclidean')\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string or callable, it must be one of\n        the options allowed by metrics.pairwise.pairwise_distances for its\n        metric parameter.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square.\n    p : int, optional (default=None)\n        p value to use if using the minkowski metric.\n    alpha : float, optional (default=1.0)\n        A distance scaling parameter as used in robust single linkage.\n        See [3]_ for more information.\n    cluster_selection_epsilon: float, optional (default=0.0)\n        A distance threshold. Clusters below this value will be merged.\n        This is the minimum epsilon allowed.\n        See [5]_ for more information.\n    cluster_selection_persistence: float, optional (default=0.0)\n        A persistence threshold. Clusters with a persistence lower than this\n        value will be merged.\n    algorithm : string, optional (default='best')\n        Exactly which algorithm to use; hdbscan has variants specialised\n        for different characteristics of the data. By default this is set\n        to ``best`` which chooses the \"best\" algorithm given the nature of\n        the data. You can force other options if you believe you know\n        better. Options are:\n            * ``best``\n            * ``generic``\n            * ``prims_kdtree``\n            * ``prims_balltree``\n            * ``boruvka_kdtree``\n            * ``boruvka_balltree``\n    leaf_size: int, optional (default=40)\n        If using a space tree algorithm (kdtree, or balltree) the number\n        of points ina leaf node of the tree. This does not alter the\n        resulting clustering, but may have an effect on the runtime\n        of the algorithm.\n    memory : Instance of joblib.Memory or string (optional)\n        Used to cache the output of the computation of the tree.\n        By default, no caching is done. If a string is given, it is the\n        path to the caching directory.\n    approx_min_span_tree : bool, optional (default=True)\n        Whether to accept an only approximate minimum spanning tree.\n        For some algorithms this can provide a significant speedup, but\n        the resulting clustering may be of marginally lower quality.\n        If you are willing to sacrifice speed for correctness you may want\n        to explore this; in general this should be left at the default True.\n    gen_min_span_tree: bool, optional (default=False)\n        Whether to generate the minimum spanning tree with regard\n        to mutual reachability distance for later analysis.\n    core_dist_n_jobs : int, optional (default=4)\n        Number of parallel jobs to run in core distance computations (if\n        supported by the specific algorithm). For ``core_dist_n_jobs``\n        below -1, (n_cpus + 1 + core_dist_n_jobs) are used.\n    cluster_selection_method : string, optional (default='eom')\n        The method used to select clusters from the condensed tree. The\n        standard approach for HDBSCAN* is to use an Excess of Mass algorithm\n        to find the most persistent clusters. Alternatively you can instead\n        select the clusters at the leaves of the tree -- this provides the\n        most fine grained and homogeneous clusters. Options are:\n            * ``eom``\n            * ``leaf``\n    allow_single_cluster : bool, optional (default=False)\n        By default HDBSCAN* will not produce a single cluster, setting this\n        to True will override this and allow single cluster results in\n        the case that you feel this is a valid result for your dataset.\n    prediction_data : boolean, optional\n        Whether to generate extra cached data for predicting labels or\n        membership vectors for new unseen points later. If you wish to\n        persist the clustering object for later re-use you probably want\n        to set this to True.\n        (default False)\n    branch_detection_data : boolean, optional\n        Whether to generated extra cached data for detecting branch-\n        hierarchies within clusters. If you wish to use functions from\n        ``hdbscan.branches`` set this to True. (default False)\n    match_reference_implementation : bool, optional (default=False)\n        There exist some interpretational differences between this\n        HDBSCAN* implementation and the original authors reference\n        implementation in Java. This can result in very minor differences\n        in clustering results. Setting this flag to True will, at a some\n        performance cost, ensure that the clustering results match the\n        reference implementation.\n    cluster_selection_epsilon_max: float, optional (default=inf)\n        A distance threshold. Clusters above this value will be split.\n        Has no effect when using leaf clustering (where clusters are\n        usually small regardless) and can also be overridden in rare\n        cases by a high value for cluster_selection_epsilon. Note that\n        this should not be used if we want to predict the cluster labels\n        for new points in future (e.g. using approximate_predict), as\n        the approximate_predict function is not aware of this argument.\n        This is the maximum epsilon allowed.\n    **kwargs : optional\n        Arguments passed to the distance metric\n    Attributes\n    ----------\n    labels_ : ndarray, shape (n_samples, )\n        Cluster labels for each point in the dataset given to fit().\n        Noisy samples are given the label -1.\n    probabilities_ : ndarray, shape (n_samples, )\n        The strength with which each sample is a member of its assigned\n        cluster. Noise points have probability zero; points in clusters\n        have values assigned proportional to the degree that they\n        persist as part of the cluster.\n    cluster_persistence_ : ndarray, shape (n_clusters, )\n        A score of how persistent each cluster is. A score of 1.0 represents\n        a perfectly stable cluster that persists over all distance scales,\n        while a score of 0.0 represents a perfectly ephemeral cluster. These\n        scores can be gauge the relative coherence of the clusters output\n        by the algorithm.\n    condensed_tree_ : CondensedTree object\n        The condensed tree produced by HDBSCAN. The object has methods\n        for converting to pandas, networkx, and plotting.\n    single_linkage_tree_ : SingleLinkageTree object\n        The single linkage tree produced by HDBSCAN. The object has methods\n        for converting to pandas, networkx, and plotting.\n    minimum_spanning_tree_ : MinimumSpanningTree object\n        The minimum spanning tree of the mutual reachability graph generated\n        by HDBSCAN. Note that this is not generated by default and will only\n        be available if `gen_min_span_tree` was set to True on object creation.\n        Even then in some optimized cases a tre may not be generated.\n    outlier_scores_ : ndarray, shape (n_samples, )\n        Outlier scores for clustered points; the larger the score the more\n        outlier-like the point. Useful as an outlier detection technique.\n        Based on the GLOSH algorithm by Campello, Moulavi, Zimek and Sander.\n    prediction_data_ : PredictionData object\n        Cached data used for predicting the cluster labels of new or\n        unseen points. Necessary only if you are using functions from\n        ``hdbscan.prediction`` (see\n        :func:`~hdbscan.prediction.approximate_predict`,\n        :func:`~hdbscan.prediction.membership_vector`,\n        and :func:`~hdbscan.prediction.all_points_membership_vectors`).\n    branch_detection_data_ : BranchDetectionData object\n        Cached data used for detecting branch-hierarchies within clusters.\n        Necessary only if you are using function from ``hdbscan.branches``.\n    exemplars_ : list\n        A list of exemplar points for clusters. Since HDBSCAN supports\n        arbitrary shapes for clusters we cannot provide a single cluster\n        exemplar per cluster. Instead a list is returned with each element\n        of the list being a numpy array of exemplar points for a cluster --\n        these points are the \"most representative\" points of the cluster.\n    relative_validity_ : float\n        A fast approximation of the Density Based Cluster Validity (DBCV)\n        score [4]. The only difference, and the speed, comes from the fact\n        that this relative_validity_ is computed using the mutual-\n        reachability minimum spanning tree, i.e. minimum_spanning_tree_,\n        instead of the all-points minimum spanning tree used in the\n        reference. This score might not be an objective measure of the\n        goodness of clustering. It may only be used to compare results\n        across different choices of hyper-parameters, therefore is only a\n        relative score.\n    References\n    ----------\n    .. [1] Campello, R. J., Moulavi, D., & Sander, J. (2013, April).\n       Density-based clustering based on hierarchical density estimates.\n       In Pacific-Asia Conference on Knowledge Discovery and Data Mining\n       (pp. 160-172). Springer Berlin Heidelberg.\n    .. [2] Campello, R. J., Moulavi, D., Zimek, A., & Sander, J. (2015).\n       Hierarchical density estimates for data clustering, visualization,\n       and outlier detection. ACM Transactions on Knowledge Discovery\n       from Data (TKDD), 10(1), 5.\n    .. [3] Chaudhuri, K., & Dasgupta, S. (2010). Rates of convergence for the\n       cluster tree. In Advances in Neural Information Processing Systems\n       (pp. 343-351).\n    .. [4] Moulavi, D., Jaskowiak, P.A., Campello, R.J., Zimek, A. and\n       Sander, J., 2014. Density-Based Clustering Validation. In SDM\n       (pp. 839-847).\n    .. [5] Malzer, C., & Baum, M. (2019). A Hybrid Approach To Hierarchical\n           Density-based Cluster Selection. arxiv preprint 1911.02282.\n    '''\n\n    def __init__(self, min_cluster_size=5, min_samples=None, cluster_selection_epsilon=0.0, cluster_selection_persistence=0.0, max_cluster_size=0, metric='euclidean', alpha=1.0, p=None, algorithm='best', leaf_size=40, memory=Memory(None, verbose=0), approx_min_span_tree=True, gen_min_span_tree=False, core_dist_n_jobs=4, cluster_selection_method='eom', allow_single_cluster=False, prediction_data=False, branch_detection_data=False, match_reference_implementation=False, cluster_selection_epsilon_max=float('inf'), **kwargs):\n        pass\n\n    def fit(self, X, y=None):\n        '''Perform HDBSCAN clustering from features or distance matrix.\n        Parameters\n        ----------\n        X : array or sparse (CSR) matrix of shape (n_samples, n_features), or                 array of shape (n_samples, n_samples)\n            A feature array, or array of distances between samples if\n            ``metric='precomputed'``.\n        Returns\n        -------\n        self : object\n            Returns self\n        '''\n        pass\n\n    def fit_predict(self, X, y=None):\n        '''Performs clustering on X and returns cluster labels.\n        Parameters\n        ----------\n        X : array or sparse (CSR) matrix of shape (n_samples, n_features), or                 array of shape (n_samples, n_samples)\n            A feature array, or array of distances between samples if\n            ``metric='precomputed'``.\n        Returns\n        -------\n        y : ndarray, shape (n_samples, )\n            cluster labels\n        '''\n        pass\n\n    def generate_prediction_data(self):\n        '''\n        Create data that caches intermediate results used for predicting\n        the label of new/unseen points. This data is only useful if\n        you are intending to use functions from ``hdbscan.prediction``.\n        '''\n        pass\n\n    def generate_branch_detection_data(self):\n        '''\n        Create data that caches intermediate results used for detecting\n        branches within clusters. This data is only useful if you are\n        intending to use functions from ``hdbscan.branches``.\n        '''\n        pass\n\n    def weighted_cluster_centroid(self, cluster_id):\n        '''Provide an approximate representative point for a given cluster.\n        Note that this technique assumes a euclidean metric for speed of\n        computation. For more general metrics use the ``weighted_cluster_medoid``\n        method which is slower, but can work with the metric the model trained\n        with.\n        Parameters\n        ----------\n        cluster_id: int\n            The id of the cluster to compute a centroid for.\n        Returns\n        -------\n        centroid: array of shape (n_features,)\n            A representative centroid for cluster ``cluster_id``.\n        '''\n        pass\n\n    def weighted_cluster_medoid(self, cluster_id):\n        '''Provide an approximate representative point for a given cluster.\n        Note that this technique can be very slow and memory intensive for\n        large clusters. For faster results use the ``weighted_cluster_centroid``\n        method which is faster, but assumes a euclidean metric.\n        Parameters\n        ----------\n        cluster_id: int\n            The id of the cluster to compute a medoid for.\n        Returns\n        -------\n        centroid: array of shape (n_features,)\n            A representative medoid for cluster ``cluster_id``.\n        '''\n        pass\n\n    def dbscan_clustering(self, cut_distance, min_cluster_size=5):\n        '''Return clustering that would be equivalent to running DBSCAN* for a particular cut_distance (or epsilon)\n        DBSCAN* can be thought of as DBSCAN without the border points.  As such these results may differ slightly\n        from sklearns implementation of dbscan in the non-core points.\n        This can also be thought of as a flat clustering derived from constant height cut through the single\n        linkage tree.\n        This represents the result of selecting a cut value for robust single linkage\n        clustering. The `min_cluster_size` allows the flat clustering to declare noise\n        points (and cluster smaller than `min_cluster_size`).\n        Parameters\n        ----------\n        cut_distance : float\n            The mutual reachability distance cut value to use to generate a flat clustering.\n        min_cluster_size : int, optional\n            Clusters smaller than this value with be called 'noise' and remain unclustered\n            in the resulting flat clustering.\n        Returns\n        -------\n        labels : array [n_samples]\n            An array of cluster labels, one per datapoint. Unclustered points are assigned\n            the label -1.\n        '''\n        pass\n    @property\n    def prediction_data_(self):\n        pass\n    @prediction_data_.setter\n    def prediction_data_(self):\n        pass\n    @property\n    def branch_detection_data_(self):\n        pass\n    @branch_detection_data_.setter\n    def branch_detection_data_(self):\n        pass\n    @property\n    def outlier_scores_(self):\n        pass\n    @outlier_scores_.setter\n    def outlier_scores_(self):\n        pass\n    @property\n    def condensed_tree_(self):\n        pass\n    @condensed_tree_.setter\n    def condensed_tree_(self):\n        pass\n    @property\n    def single_linkage_tree_(self):\n        pass\n    @single_linkage_tree_.setter\n    def single_linkage_tree_(self):\n        pass\n    @property\n    def minimum_spanning_tree_(self):\n        pass\n    @minimum_spanning_tree_.setter\n    def minimum_spanning_tree_(self):\n        pass\n    @property\n    def exemplars_(self):\n        pass\n    @exemplars_.setter\n    def exemplars_(self):\n        pass\n    @property\n    def relative_validity_(self):\n        pass\n    @relative_validity_.setter\n    def relative_validity_(self):\n        pass\nCorresponding class implementation:\n\nfrom sklearn.utils import check_array\nfrom sklearn.base import BaseEstimator, ClusterMixin\nimport numpy as np\nfrom .plots import CondensedTree, SingleLinkageTree, MinimumSpanningTree\nfrom warnings import warn\nfrom joblib import Memory\nfrom ._hdbscan_tree import condense_tree, compute_stability, get_clusters, outlier_scores, simplify_hierarchy\nfrom scipy.sparse import issparse\nfrom .branch_data import BranchDetectionData\nfrom sklearn.metrics import pairwise_distances\nfrom .prediction import PredictionData\n\nclass HDBSCAN(BaseEstimator, ClusterMixin):\n    \"\"\"Perform HDBSCAN clustering from vector array or distance matrix.\n\n    HDBSCAN - Hierarchical Density-Based Spatial Clustering of Applications\n    with Noise. Performs DBSCAN over varying epsilon values and integrates\n    the result to find a clustering that gives the best stability over epsilon.\n    This allows HDBSCAN to find clusters of varying densities (unlike DBSCAN),\n    and be more robust to parameter selection.\n\n    Parameters\n    ----------\n    min_cluster_size : int, optional (default=5)\n        The minimum size of clusters; single linkage splits that contain\n        fewer points than this will be considered points \"falling out\" of a\n        cluster rather than a cluster splitting into two new clusters.\n\n    min_samples : int, optional (default=None)\n        The number of samples in a neighborhood for a point to be considered as a core point.\n        This includes the point itself. When None, defaults to min_cluster_size.\n\n    metric : string, or callable, optional (default='euclidean')\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string or callable, it must be one of\n        the options allowed by metrics.pairwise.pairwise_distances for its\n        metric parameter.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square.\n\n    p : int, optional (default=None)\n        p value to use if using the minkowski metric.\n\n    alpha : float, optional (default=1.0)\n        A distance scaling parameter as used in robust single linkage.\n        See [3]_ for more information.\n\n    cluster_selection_epsilon: float, optional (default=0.0)\n        A distance threshold. Clusters below this value will be merged.\n        This is the minimum epsilon allowed.\n        See [5]_ for more information.\n\n    cluster_selection_persistence: float, optional (default=0.0)\n        A persistence threshold. Clusters with a persistence lower than this\n        value will be merged.\n\n    algorithm : string, optional (default='best')\n        Exactly which algorithm to use; hdbscan has variants specialised\n        for different characteristics of the data. By default this is set\n        to ``best`` which chooses the \"best\" algorithm given the nature of\n        the data. You can force other options if you believe you know\n        better. Options are:\n            * ``best``\n            * ``generic``\n            * ``prims_kdtree``\n            * ``prims_balltree``\n            * ``boruvka_kdtree``\n            * ``boruvka_balltree``\n\n    leaf_size: int, optional (default=40)\n        If using a space tree algorithm (kdtree, or balltree) the number\n        of points ina leaf node of the tree. This does not alter the\n        resulting clustering, but may have an effect on the runtime\n        of the algorithm.\n\n    memory : Instance of joblib.Memory or string (optional)\n        Used to cache the output of the computation of the tree.\n        By default, no caching is done. If a string is given, it is the\n        path to the caching directory.\n\n    approx_min_span_tree : bool, optional (default=True)\n        Whether to accept an only approximate minimum spanning tree.\n        For some algorithms this can provide a significant speedup, but\n        the resulting clustering may be of marginally lower quality.\n        If you are willing to sacrifice speed for correctness you may want\n        to explore this; in general this should be left at the default True.\n\n    gen_min_span_tree: bool, optional (default=False)\n        Whether to generate the minimum spanning tree with regard\n        to mutual reachability distance for later analysis.\n\n    core_dist_n_jobs : int, optional (default=4)\n        Number of parallel jobs to run in core distance computations (if\n        supported by the specific algorithm). For ``core_dist_n_jobs``\n        below -1, (n_cpus + 1 + core_dist_n_jobs) are used.\n\n    cluster_selection_method : string, optional (default='eom')\n        The method used to select clusters from the condensed tree. The\n        standard approach for HDBSCAN* is to use an Excess of Mass algorithm\n        to find the most persistent clusters. Alternatively you can instead\n        select the clusters at the leaves of the tree -- this provides the\n        most fine grained and homogeneous clusters. Options are:\n            * ``eom``\n            * ``leaf``\n\n    allow_single_cluster : bool, optional (default=False)\n        By default HDBSCAN* will not produce a single cluster, setting this\n        to True will override this and allow single cluster results in\n        the case that you feel this is a valid result for your dataset.\n\n    prediction_data : boolean, optional\n        Whether to generate extra cached data for predicting labels or\n        membership vectors for new unseen points later. If you wish to\n        persist the clustering object for later re-use you probably want\n        to set this to True.\n        (default False)\n\n    branch_detection_data : boolean, optional\n        Whether to generated extra cached data for detecting branch-\n        hierarchies within clusters. If you wish to use functions from\n        ``hdbscan.branches`` set this to True. (default False)\n\n    match_reference_implementation : bool, optional (default=False)\n        There exist some interpretational differences between this\n        HDBSCAN* implementation and the original authors reference\n        implementation in Java. This can result in very minor differences\n        in clustering results. Setting this flag to True will, at a some\n        performance cost, ensure that the clustering results match the\n        reference implementation.\n\n    cluster_selection_epsilon_max: float, optional (default=inf)\n        A distance threshold. Clusters above this value will be split.\n        Has no effect when using leaf clustering (where clusters are\n        usually small regardless) and can also be overridden in rare\n        cases by a high value for cluster_selection_epsilon. Note that\n        this should not be used if we want to predict the cluster labels\n        for new points in future (e.g. using approximate_predict), as\n        the approximate_predict function is not aware of this argument.\n        This is the maximum epsilon allowed.\n\n    **kwargs : optional\n        Arguments passed to the distance metric\n\n    Attributes\n    ----------\n    labels_ : ndarray, shape (n_samples, )\n        Cluster labels for each point in the dataset given to fit().\n        Noisy samples are given the label -1.\n\n    probabilities_ : ndarray, shape (n_samples, )\n        The strength with which each sample is a member of its assigned\n        cluster. Noise points have probability zero; points in clusters\n        have values assigned proportional to the degree that they\n        persist as part of the cluster.\n\n    cluster_persistence_ : ndarray, shape (n_clusters, )\n        A score of how persistent each cluster is. A score of 1.0 represents\n        a perfectly stable cluster that persists over all distance scales,\n        while a score of 0.0 represents a perfectly ephemeral cluster. These\n        scores can be gauge the relative coherence of the clusters output\n        by the algorithm.\n\n    condensed_tree_ : CondensedTree object\n        The condensed tree produced by HDBSCAN. The object has methods\n        for converting to pandas, networkx, and plotting.\n\n    single_linkage_tree_ : SingleLinkageTree object\n        The single linkage tree produced by HDBSCAN. The object has methods\n        for converting to pandas, networkx, and plotting.\n\n    minimum_spanning_tree_ : MinimumSpanningTree object\n        The minimum spanning tree of the mutual reachability graph generated\n        by HDBSCAN. Note that this is not generated by default and will only\n        be available if `gen_min_span_tree` was set to True on object creation.\n        Even then in some optimized cases a tre may not be generated.\n\n    outlier_scores_ : ndarray, shape (n_samples, )\n        Outlier scores for clustered points; the larger the score the more\n        outlier-like the point. Useful as an outlier detection technique.\n        Based on the GLOSH algorithm by Campello, Moulavi, Zimek and Sander.\n\n    prediction_data_ : PredictionData object\n        Cached data used for predicting the cluster labels of new or\n        unseen points. Necessary only if you are using functions from\n        ``hdbscan.prediction`` (see\n        :func:`~hdbscan.prediction.approximate_predict`,\n        :func:`~hdbscan.prediction.membership_vector`,\n        and :func:`~hdbscan.prediction.all_points_membership_vectors`).\n\n    branch_detection_data_ : BranchDetectionData object\n        Cached data used for detecting branch-hierarchies within clusters.\n        Necessary only if you are using function from ``hdbscan.branches``.\n\n    exemplars_ : list\n        A list of exemplar points for clusters. Since HDBSCAN supports\n        arbitrary shapes for clusters we cannot provide a single cluster\n        exemplar per cluster. Instead a list is returned with each element\n        of the list being a numpy array of exemplar points for a cluster --\n        these points are the \"most representative\" points of the cluster.\n\n    relative_validity_ : float\n        A fast approximation of the Density Based Cluster Validity (DBCV)\n        score [4]. The only difference, and the speed, comes from the fact\n        that this relative_validity_ is computed using the mutual-\n        reachability minimum spanning tree, i.e. minimum_spanning_tree_,\n        instead of the all-points minimum spanning tree used in the\n        reference. This score might not be an objective measure of the\n        goodness of clustering. It may only be used to compare results\n        across different choices of hyper-parameters, therefore is only a\n        relative score.\n\n    References\n    ----------\n\n    .. [1] Campello, R. J., Moulavi, D., & Sander, J. (2013, April).\n       Density-based clustering based on hierarchical density estimates.\n       In Pacific-Asia Conference on Knowledge Discovery and Data Mining\n       (pp. 160-172). Springer Berlin Heidelberg.\n\n    .. [2] Campello, R. J., Moulavi, D., Zimek, A., & Sander, J. (2015).\n       Hierarchical density estimates for data clustering, visualization,\n       and outlier detection. ACM Transactions on Knowledge Discovery\n       from Data (TKDD), 10(1), 5.\n\n    .. [3] Chaudhuri, K., & Dasgupta, S. (2010). Rates of convergence for the\n       cluster tree. In Advances in Neural Information Processing Systems\n       (pp. 343-351).\n\n    .. [4] Moulavi, D., Jaskowiak, P.A., Campello, R.J., Zimek, A. and\n       Sander, J., 2014. Density-Based Clustering Validation. In SDM\n       (pp. 839-847).\n\n    .. [5] Malzer, C., & Baum, M. (2019). A Hybrid Approach To Hierarchical\n           Density-based Cluster Selection. arxiv preprint 1911.02282.\n\n    \"\"\"\n\n    def __init__(self, min_cluster_size=5, min_samples=None, cluster_selection_epsilon=0.0, cluster_selection_persistence=0.0, max_cluster_size=0, metric='euclidean', alpha=1.0, p=None, algorithm='best', leaf_size=40, memory=Memory(None, verbose=0), approx_min_span_tree=True, gen_min_span_tree=False, core_dist_n_jobs=4, cluster_selection_method='eom', allow_single_cluster=False, prediction_data=False, branch_detection_data=False, match_reference_implementation=False, cluster_selection_epsilon_max=float('inf'), **kwargs):\n        self.min_cluster_size = min_cluster_size\n        self.min_samples = min_samples\n        self.alpha = alpha\n        self.max_cluster_size = max_cluster_size\n        self.cluster_selection_epsilon = cluster_selection_epsilon\n        self.cluster_selection_persistence = cluster_selection_persistence\n        self.metric = metric\n        self.p = p\n        self.algorithm = algorithm\n        self.leaf_size = leaf_size\n        self.memory = memory\n        self.approx_min_span_tree = approx_min_span_tree\n        self.gen_min_span_tree = gen_min_span_tree\n        self.core_dist_n_jobs = core_dist_n_jobs\n        self.cluster_selection_method = cluster_selection_method\n        self.allow_single_cluster = allow_single_cluster\n        self.match_reference_implementation = match_reference_implementation\n        self.prediction_data = prediction_data\n        self.branch_detection_data = branch_detection_data\n        self.cluster_selection_epsilon_max = cluster_selection_epsilon_max\n        self._metric_kwargs = kwargs\n        self._condensed_tree = None\n        self._single_linkage_tree = None\n        self._min_spanning_tree = None\n        self._raw_data = None\n        self._outlier_scores = None\n        self._prediction_data = None\n        self._finite_index = None\n        self._branch_detection_data = None\n        self._relative_validity = None\n\n    def fit(self, X, y=None):\n        \"\"\"Perform HDBSCAN clustering from features or distance matrix.\n\n        Parameters\n        ----------\n        X : array or sparse (CSR) matrix of shape (n_samples, n_features), or                 array of shape (n_samples, n_samples)\n            A feature array, or array of distances between samples if\n            ``metric='precomputed'``.\n\n        Returns\n        -------\n        self : object\n            Returns self\n        \"\"\"\n        if self.metric != 'precomputed':\n            X = check_array(X, accept_sparse='csr', force_all_finite=False)\n            self._raw_data = X\n            self._all_finite = is_finite(X)\n            if ~self._all_finite:\n                finite_index = get_finite_row_indices(X)\n                clean_data = X[finite_index]\n                internal_to_raw = {x: y for x, y in zip(range(len(finite_index)), finite_index)}\n                outliers = list(set(range(X.shape[0])) - set(finite_index))\n            else:\n                clean_data = X\n        elif issparse(X):\n            X = check_array(X, accept_sparse='csr')\n            clean_data = X\n        else:\n            check_precomputed_distance_matrix(X)\n            clean_data = X\n        kwargs = self.get_params()\n        kwargs.pop('prediction_data', None)\n        kwargs.pop('branch_detection_data', None)\n        kwargs.update(self._metric_kwargs)\n        kwargs['gen_min_span_tree'] |= self.branch_detection_data\n        self.labels_, self.probabilities_, self.cluster_persistence_, self._condensed_tree, self._single_linkage_tree, self._min_spanning_tree = hdbscan(clean_data, **kwargs)\n        if self.metric != 'precomputed' and (not self._all_finite):\n            self._condensed_tree = remap_condensed_tree(self._condensed_tree, internal_to_raw, outliers)\n            self._single_linkage_tree = remap_single_linkage_tree(self._single_linkage_tree, internal_to_raw, outliers)\n            new_labels = np.full(X.shape[0], -1)\n            new_labels[finite_index] = self.labels_\n            self.labels_ = new_labels\n            new_probabilities = np.zeros(X.shape[0])\n            new_probabilities[finite_index] = self.probabilities_\n            self.probabilities_ = new_probabilities\n        if self.prediction_data:\n            self.generate_prediction_data()\n        if self.branch_detection_data:\n            self.generate_branch_detection_data()\n        return self\n\n    def fit_predict(self, X, y=None):\n        \"\"\"Performs clustering on X and returns cluster labels.\n\n        Parameters\n        ----------\n        X : array or sparse (CSR) matrix of shape (n_samples, n_features), or                 array of shape (n_samples, n_samples)\n            A feature array, or array of distances between samples if\n            ``metric='precomputed'``.\n\n        Returns\n        -------\n        y : ndarray, shape (n_samples, )\n            cluster labels\n        \"\"\"\n        self.fit(X)\n        return self.labels_\n\n    def generate_prediction_data(self):\n        \"\"\"\n        Create data that caches intermediate results used for predicting\n        the label of new/unseen points. This data is only useful if\n        you are intending to use functions from ``hdbscan.prediction``.\n        \"\"\"\n        if self.metric in FAST_METRICS:\n            min_samples = self.min_samples or self.min_cluster_size\n            if self.metric in KDTREE_VALID_METRICS:\n                tree_type = 'kdtree'\n            elif self.metric in BALLTREE_VALID_METRICS:\n                tree_type = 'balltree'\n            else:\n                warn('Metric {} not supported for prediction data!'.format(self.metric))\n                return\n            self._prediction_data = PredictionData(self._raw_data, self.condensed_tree_, min_samples, tree_type=tree_type, metric=self.metric, **self._metric_kwargs)\n        else:\n            warn('Cannot generate prediction data for non-vector space inputs -- access to the source data rather than mere distances is required!')\n\n    def generate_branch_detection_data(self):\n        \"\"\"\n        Create data that caches intermediate results used for detecting\n        branches within clusters. This data is only useful if you are\n        intending to use functions from ``hdbscan.branches``.\n        \"\"\"\n        if self.metric in FAST_METRICS:\n            min_samples = self.min_samples or self.min_cluster_size\n            if self.metric in KDTREE_VALID_METRICS:\n                tree_type = 'kdtree'\n            elif self.metric in BALLTREE_VALID_METRICS:\n                tree_type = 'balltree'\n            else:\n                warn('Metric {} not supported for branch detection!'.format(self.metric))\n                return\n            self._branch_detection_data = BranchDetectionData(self._raw_data, self.labels_, self._condensed_tree, min_samples, tree_type=tree_type, metric=self.metric, **self._metric_kwargs)\n        else:\n            warn('Branch detection for non-vector space inputs is not (yet) implemented.')\n\n    def weighted_cluster_centroid(self, cluster_id):\n        \"\"\"Provide an approximate representative point for a given cluster.\n        Note that this technique assumes a euclidean metric for speed of\n        computation. For more general metrics use the ``weighted_cluster_medoid``\n        method which is slower, but can work with the metric the model trained\n        with.\n\n        Parameters\n        ----------\n        cluster_id: int\n            The id of the cluster to compute a centroid for.\n\n        Returns\n        -------\n        centroid: array of shape (n_features,)\n            A representative centroid for cluster ``cluster_id``.\n        \"\"\"\n        if not hasattr(self, 'labels_'):\n            raise AttributeError('Model has not been fit to data')\n        if cluster_id == -1:\n            raise ValueError('Cannot calculate weighted centroid for -1 cluster since it is a noise cluster')\n        mask = self.labels_ == cluster_id\n        cluster_data = self._raw_data[mask]\n        cluster_membership_strengths = self.probabilities_[mask]\n        return np.average(cluster_data, weights=cluster_membership_strengths, axis=0)\n\n    def weighted_cluster_medoid(self, cluster_id):\n        \"\"\"Provide an approximate representative point for a given cluster.\n        Note that this technique can be very slow and memory intensive for\n        large clusters. For faster results use the ``weighted_cluster_centroid``\n        method which is faster, but assumes a euclidean metric.\n\n        Parameters\n        ----------\n        cluster_id: int\n            The id of the cluster to compute a medoid for.\n\n        Returns\n        -------\n        centroid: array of shape (n_features,)\n            A representative medoid for cluster ``cluster_id``.\n        \"\"\"\n        if not hasattr(self, 'labels_'):\n            raise AttributeError('Model has not been fit to data')\n        if cluster_id == -1:\n            raise ValueError('Cannot calculate weighted centroid for -1 cluster since it is a noise cluster')\n        mask = self.labels_ == cluster_id\n        cluster_data = self._raw_data[mask]\n        cluster_membership_strengths = self.probabilities_[mask]\n        dist_mat = pairwise_distances(cluster_data, metric=self.metric, **self._metric_kwargs)\n        dist_mat = dist_mat * cluster_membership_strengths\n        medoid_index = np.argmin(dist_mat.sum(axis=1))\n        return cluster_data[medoid_index]\n\n    def dbscan_clustering(self, cut_distance, min_cluster_size=5):\n        \"\"\"Return clustering that would be equivalent to running DBSCAN* for a particular cut_distance (or epsilon)\n        DBSCAN* can be thought of as DBSCAN without the border points.  As such these results may differ slightly\n        from sklearns implementation of dbscan in the non-core points.\n\n        This can also be thought of as a flat clustering derived from constant height cut through the single\n        linkage tree.\n\n        This represents the result of selecting a cut value for robust single linkage\n        clustering. The `min_cluster_size` allows the flat clustering to declare noise\n        points (and cluster smaller than `min_cluster_size`).\n\n        Parameters\n        ----------\n\n        cut_distance : float\n            The mutual reachability distance cut value to use to generate a flat clustering.\n\n        min_cluster_size : int, optional\n            Clusters smaller than this value with be called 'noise' and remain unclustered\n            in the resulting flat clustering.\n\n        Returns\n        -------\n\n        labels : array [n_samples]\n            An array of cluster labels, one per datapoint. Unclustered points are assigned\n            the label -1.\n        \"\"\"\n        return self.single_linkage_tree_.get_clusters(cut_distance=cut_distance, min_cluster_size=min_cluster_size)\n\n    @property\n    def prediction_data_(self):\n        if self._prediction_data is None:\n            raise AttributeError('No prediction data was generated')\n        else:\n            return self._prediction_data\n\n    @prediction_data_.setter\n    def prediction_data_(self, value):\n        self._prediction_data = value\n\n    @property\n    def branch_detection_data_(self):\n        if self._branch_detection_data is None:\n            raise AttributeError('No branch detection data was generated')\n        else:\n            return self._branch_detection_data\n\n    @branch_detection_data_.setter\n    def branch_detection_data_(self, value):\n        self._branch_detection_data = value\n\n    @property\n    def outlier_scores_(self):\n        if self._outlier_scores is not None:\n            return self._outlier_scores\n        elif self._condensed_tree is not None:\n            self._outlier_scores = outlier_scores(self._condensed_tree)\n            return self._outlier_scores\n        else:\n            raise AttributeError('No condensed tree was generated; try running fit first.')\n\n    @outlier_scores_.setter\n    def outlier_scores_(self, value):\n        self._outlier_scores = value\n\n    @property\n    def condensed_tree_(self):\n        if self._condensed_tree is not None:\n            return CondensedTree(self._condensed_tree, self.labels_)\n        else:\n            raise AttributeError('No condensed tree was generated; try running fit first.')\n\n    @condensed_tree_.setter\n    def condensed_tree_(self, value):\n        self._condensed_tree = value\n\n    @property\n    def single_linkage_tree_(self):\n        if self._single_linkage_tree is not None:\n            return SingleLinkageTree(self._single_linkage_tree)\n        else:\n            raise AttributeError('No single linkage tree was generated; try running fit first.')\n\n    @single_linkage_tree_.setter\n    def single_linkage_tree_(self, value):\n        self._single_linkage_tree = value\n\n    @property\n    def minimum_spanning_tree_(self):\n        if self._min_spanning_tree is not None:\n            if self._raw_data is not None:\n                return MinimumSpanningTree(self._min_spanning_tree, self._raw_data)\n            else:\n                warn('No raw data is available; this may be due to using a precomputed metric matrix. No minimum spanning tree will be provided without raw data.')\n                return None\n        else:\n            raise AttributeError('No minimum spanning tree was generated.This may be due to optimized algorithm variations that skip explicit generation of the spanning tree.')\n\n    @minimum_spanning_tree_.setter\n    def minimum_spanning_tree_(self, value):\n        self._min_spanning_tree = value\n\n    @property\n    def exemplars_(self):\n        if self._prediction_data is not None:\n            return self._prediction_data.exemplars\n        elif self.metric in FAST_METRICS:\n            self.generate_prediction_data()\n            return self._prediction_data.exemplars\n        else:\n            raise AttributeError('Currently exemplars require the use of vector input datawith a suitable metric. This will likely change in the future, but for now no exemplars can be provided')\n\n    @exemplars_.setter\n    def exemplars_(self, value):\n        self._exemplars = value\n\n    @property\n    def relative_validity_(self):\n        if self._relative_validity is not None:\n            return self._relative_validity\n        if not self.gen_min_span_tree:\n            raise AttributeError('Minimum spanning tree not present. ' + 'Either HDBSCAN object was created with ' + 'gen_min_span_tree=False or the tree was ' + 'not generated in spite of it owing to ' + 'internal optimization criteria.')\n            return\n        labels = self.labels_\n        sizes = np.bincount(labels + 1)\n        noise_size = sizes[0]\n        cluster_size = sizes[1:]\n        total = noise_size + np.sum(cluster_size)\n        num_clusters = len(cluster_size)\n        DSC = np.zeros(num_clusters)\n        min_outlier_sep = np.inf\n        correction_const = 2\n        DSPC_wrt = np.ones(num_clusters) * np.inf\n        max_distance = 0\n        mst_df = self.minimum_spanning_tree_.to_pandas()\n        for edge in mst_df.iterrows():\n            label1 = labels[int(edge[1]['from'])]\n            label2 = labels[int(edge[1]['to'])]\n            length = edge[1]['distance']\n            max_distance = max(max_distance, length)\n            if label1 == -1 and label2 == -1:\n                continue\n            elif label1 == -1 or label2 == -1:\n                min_outlier_sep = min(min_outlier_sep, length)\n                continue\n            if label1 == label2:\n                DSC[label1] = max(length, DSC[label1])\n            else:\n                DSPC_wrt[label1] = min(length, DSPC_wrt[label1])\n                DSPC_wrt[label2] = min(length, DSPC_wrt[label2])\n        min_outlier_sep = max_distance if min_outlier_sep == np.inf else min_outlier_sep\n        correction = correction_const * (max_distance if num_clusters > 1 else min_outlier_sep)\n        DSPC_wrt[np.where(DSPC_wrt == np.inf)] = correction\n        V_index = [(DSPC_wrt[i] - DSC[i]) / max(DSPC_wrt[i], DSC[i]) for i in range(num_clusters)]\n        score = np.sum([cluster_size[i] * V_index[i] / total for i in range(num_clusters)])\n        self._relative_validity = score\n        return self._relative_validity\n\n    @relative_validity_.setter\n    def relative_validity_(self, value):\n        self._relative_validity = value\n\nNow, implement the following class. Do not explain the code. The given class skeleton is as follows:\n\nclass GMC:\n    '''\n    Generalized Motion Compensation (GMC) class for tracking and object detection in video frames.\n    This class provides methods for tracking and detecting objects based on several tracking algorithms including ORB,\n    SIFT, ECC, and Sparse Optical Flow. It also supports downscaling of frames for computational efficiency.\n    Attributes:\n        method (str): The method used for tracking. Options include 'orb', 'sift', 'ecc', 'sparseOptFlow', 'none'.\n        downscale (int): Factor by which to downscale the frames for processing.\n        prevFrame (np.ndarray): Stores the previous frame for tracking.\n        prevKeyPoints (List): Stores the keypoints from the previous frame.\n        prevDescriptors (np.ndarray): Stores the descriptors from the previous frame.\n        initializedFirstFrame (bool): Flag to indicate if the first frame has been processed.\n    Methods:\n        __init__: Initializes a GMC object with the specified method and downscale factor.\n        apply: Applies the chosen method to a raw frame and optionally uses provided detections.\n        apply_ecc: Applies the ECC algorithm to a raw frame.\n        apply_features: Applies feature-based methods like ORB or SIFT to a raw frame.\n        apply_sparseoptflow: Applies the Sparse Optical Flow method to a raw frame.\n        reset_params: Resets the internal parameters of the GMC object.\n    Examples:\n        Create a GMC object and apply it to a frame\n        >>> gmc = GMC(method=\"sparseOptFlow\", downscale=2)\n        >>> frame = np.array([[1, 2, 3], [4, 5, 6]])\n        >>> processed_frame = gmc.apply(frame)\n        >>> print(processed_frame)\n        array([[1, 2, 3],\n               [4, 5, 6]])\n    '''\n\n    def __init__(self, method: str='sparseOptFlow', downscale: int=2) -> None:\n        '''\n        Initialize a Generalized Motion Compensation (GMC) object with tracking method and downscale factor.\n        Args:\n            method (str): The method used for tracking. Options include 'orb', 'sift', 'ecc', 'sparseOptFlow', 'none'.\n            downscale (int): Downscale factor for processing frames.\n        Examples:\n            Initialize a GMC object with the 'sparseOptFlow' method and a downscale factor of 2\n            >>> gmc = GMC(method=\"sparseOptFlow\", downscale=2)\n        '''\n        pass\n\n    def apply(self, raw_frame: np.array, detections: list=None) -> np.array:\n        '''\n        Apply object detection on a raw frame using the specified method.\n        Args:\n            raw_frame (np.ndarray): The raw frame to be processed, with shape (H, W, C).\n            detections (List | None): List of detections to be used in the processing.\n        Returns:\n            (np.ndarray): Processed frame with applied object detection.\n        Examples:\n            >>> gmc = GMC(method=\"sparseOptFlow\")\n            >>> raw_frame = np.random.rand(480, 640, 3)\n            >>> processed_frame = gmc.apply(raw_frame)\n            >>> print(processed_frame.shape)\n            (480, 640, 3)\n        '''\n        pass\n\n    def apply_ecc(self, raw_frame: np.array) -> np.array:\n        '''\n        Apply the ECC (Enhanced Correlation Coefficient) algorithm to a raw frame for motion compensation.\n        Args:\n            raw_frame (np.ndarray): The raw frame to be processed, with shape (H, W, C).\n        Returns:\n            (np.ndarray): The processed frame with the applied ECC transformation.\n        Examples:\n            >>> gmc = GMC(method=\"ecc\")\n            >>> processed_frame = gmc.apply_ecc(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]))\n            >>> print(processed_frame)\n            [[1. 0. 0.]\n             [0. 1. 0.]]\n        '''\n        pass\n\n    def apply_features(self, raw_frame: np.array, detections: list=None) -> np.array:\n        '''\n        Apply feature-based methods like ORB or SIFT to a raw frame.\n        Args:\n            raw_frame (np.ndarray): The raw frame to be processed, with shape (H, W, C).\n            detections (List | None): List of detections to be used in the processing.\n        Returns:\n            (np.ndarray): Processed frame.\n        Examples:\n            >>> gmc = GMC(method=\"orb\")\n            >>> raw_frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n            >>> processed_frame = gmc.apply_features(raw_frame)\n            >>> print(processed_frame.shape)\n            (2, 3)\n        '''\n        pass\n\n    def apply_sparseoptflow(self, raw_frame: np.array) -> np.array:\n        '''\n        Apply Sparse Optical Flow method to a raw frame.\n        Args:\n            raw_frame (np.ndarray): The raw frame to be processed, with shape (H, W, C).\n        Returns:\n            (np.ndarray): Processed frame with shape (2, 3).\n        Examples:\n            >>> gmc = GMC()\n            >>> result = gmc.apply_sparseoptflow(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]))\n            >>> print(result)\n            [[1. 0. 0.]\n             [0. 1. 0.]]\n        '''\n        pass\n\n    def reset_params(self) -> None:\n        '''Reset the internal parameters including previous frame, keypoints, and descriptors.'''\n        pass"
  }
]