[
  {
    "role": "assistant",
    "content": "You are an expert Python programmer who can correctly implement complete Python classes based on the provided class skeleton."
  },
  {
    "role": "user",
    "content": "For your understanding, following are some (skeleton, code) examples:\n\nExample skeleton:\n\n\nclass HDBSCAN(BaseEstimator, ClusterMixin):\n    '''Perform HDBSCAN clustering from vector array or distance matrix.\n    HDBSCAN - Hierarchical Density-Based Spatial Clustering of Applications\n    with Noise. Performs DBSCAN over varying epsilon values and integrates\n    the result to find a clustering that gives the best stability over epsilon.\n    This allows HDBSCAN to find clusters of varying densities (unlike DBSCAN),\n    and be more robust to parameter selection.\n    Parameters\n    ----------\n    min_cluster_size : int, optional (default=5)\n        The minimum size of clusters; single linkage splits that contain\n        fewer points than this will be considered points \"falling out\" of a\n        cluster rather than a cluster splitting into two new clusters.\n    min_samples : int, optional (default=None)\n        The number of samples in a neighborhood for a point to be considered as a core point.\n        This includes the point itself. When None, defaults to min_cluster_size.\n    metric : string, or callable, optional (default='euclidean')\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string or callable, it must be one of\n        the options allowed by metrics.pairwise.pairwise_distances for its\n        metric parameter.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square.\n    p : int, optional (default=None)\n        p value to use if using the minkowski metric.\n    alpha : float, optional (default=1.0)\n        A distance scaling parameter as used in robust single linkage.\n        See [3]_ for more information.\n    cluster_selection_epsilon: float, optional (default=0.0)\n        A distance threshold. Clusters below this value will be merged.\n        This is the minimum epsilon allowed.\n        See [5]_ for more information.\n    cluster_selection_persistence: float, optional (default=0.0)\n        A persistence threshold. Clusters with a persistence lower than this\n        value will be merged.\n    algorithm : string, optional (default='best')\n        Exactly which algorithm to use; hdbscan has variants specialised\n        for different characteristics of the data. By default this is set\n        to ``best`` which chooses the \"best\" algorithm given the nature of\n        the data. You can force other options if you believe you know\n        better. Options are:\n            * ``best``\n            * ``generic``\n            * ``prims_kdtree``\n            * ``prims_balltree``\n            * ``boruvka_kdtree``\n            * ``boruvka_balltree``\n    leaf_size: int, optional (default=40)\n        If using a space tree algorithm (kdtree, or balltree) the number\n        of points ina leaf node of the tree. This does not alter the\n        resulting clustering, but may have an effect on the runtime\n        of the algorithm.\n    memory : Instance of joblib.Memory or string (optional)\n        Used to cache the output of the computation of the tree.\n        By default, no caching is done. If a string is given, it is the\n        path to the caching directory.\n    approx_min_span_tree : bool, optional (default=True)\n        Whether to accept an only approximate minimum spanning tree.\n        For some algorithms this can provide a significant speedup, but\n        the resulting clustering may be of marginally lower quality.\n        If you are willing to sacrifice speed for correctness you may want\n        to explore this; in general this should be left at the default True.\n    gen_min_span_tree: bool, optional (default=False)\n        Whether to generate the minimum spanning tree with regard\n        to mutual reachability distance for later analysis.\n    core_dist_n_jobs : int, optional (default=4)\n        Number of parallel jobs to run in core distance computations (if\n        supported by the specific algorithm). For ``core_dist_n_jobs``\n        below -1, (n_cpus + 1 + core_dist_n_jobs) are used.\n    cluster_selection_method : string, optional (default='eom')\n        The method used to select clusters from the condensed tree. The\n        standard approach for HDBSCAN* is to use an Excess of Mass algorithm\n        to find the most persistent clusters. Alternatively you can instead\n        select the clusters at the leaves of the tree -- this provides the\n        most fine grained and homogeneous clusters. Options are:\n            * ``eom``\n            * ``leaf``\n    allow_single_cluster : bool, optional (default=False)\n        By default HDBSCAN* will not produce a single cluster, setting this\n        to True will override this and allow single cluster results in\n        the case that you feel this is a valid result for your dataset.\n    prediction_data : boolean, optional\n        Whether to generate extra cached data for predicting labels or\n        membership vectors for new unseen points later. If you wish to\n        persist the clustering object for later re-use you probably want\n        to set this to True.\n        (default False)\n    branch_detection_data : boolean, optional\n        Whether to generated extra cached data for detecting branch-\n        hierarchies within clusters. If you wish to use functions from\n        ``hdbscan.branches`` set this to True. (default False)\n    match_reference_implementation : bool, optional (default=False)\n        There exist some interpretational differences between this\n        HDBSCAN* implementation and the original authors reference\n        implementation in Java. This can result in very minor differences\n        in clustering results. Setting this flag to True will, at a some\n        performance cost, ensure that the clustering results match the\n        reference implementation.\n    cluster_selection_epsilon_max: float, optional (default=inf)\n        A distance threshold. Clusters above this value will be split.\n        Has no effect when using leaf clustering (where clusters are\n        usually small regardless) and can also be overridden in rare\n        cases by a high value for cluster_selection_epsilon. Note that\n        this should not be used if we want to predict the cluster labels\n        for new points in future (e.g. using approximate_predict), as\n        the approximate_predict function is not aware of this argument.\n        This is the maximum epsilon allowed.\n    **kwargs : optional\n        Arguments passed to the distance metric\n    Attributes\n    ----------\n    labels_ : ndarray, shape (n_samples, )\n        Cluster labels for each point in the dataset given to fit().\n        Noisy samples are given the label -1.\n    probabilities_ : ndarray, shape (n_samples, )\n        The strength with which each sample is a member of its assigned\n        cluster. Noise points have probability zero; points in clusters\n        have values assigned proportional to the degree that they\n        persist as part of the cluster.\n    cluster_persistence_ : ndarray, shape (n_clusters, )\n        A score of how persistent each cluster is. A score of 1.0 represents\n        a perfectly stable cluster that persists over all distance scales,\n        while a score of 0.0 represents a perfectly ephemeral cluster. These\n        scores can be gauge the relative coherence of the clusters output\n        by the algorithm.\n    condensed_tree_ : CondensedTree object\n        The condensed tree produced by HDBSCAN. The object has methods\n        for converting to pandas, networkx, and plotting.\n    single_linkage_tree_ : SingleLinkageTree object\n        The single linkage tree produced by HDBSCAN. The object has methods\n        for converting to pandas, networkx, and plotting.\n    minimum_spanning_tree_ : MinimumSpanningTree object\n        The minimum spanning tree of the mutual reachability graph generated\n        by HDBSCAN. Note that this is not generated by default and will only\n        be available if `gen_min_span_tree` was set to True on object creation.\n        Even then in some optimized cases a tre may not be generated.\n    outlier_scores_ : ndarray, shape (n_samples, )\n        Outlier scores for clustered points; the larger the score the more\n        outlier-like the point. Useful as an outlier detection technique.\n        Based on the GLOSH algorithm by Campello, Moulavi, Zimek and Sander.\n    prediction_data_ : PredictionData object\n        Cached data used for predicting the cluster labels of new or\n        unseen points. Necessary only if you are using functions from\n        ``hdbscan.prediction`` (see\n        :func:`~hdbscan.prediction.approximate_predict`,\n        :func:`~hdbscan.prediction.membership_vector`,\n        and :func:`~hdbscan.prediction.all_points_membership_vectors`).\n    branch_detection_data_ : BranchDetectionData object\n        Cached data used for detecting branch-hierarchies within clusters.\n        Necessary only if you are using function from ``hdbscan.branches``.\n    exemplars_ : list\n        A list of exemplar points for clusters. Since HDBSCAN supports\n        arbitrary shapes for clusters we cannot provide a single cluster\n        exemplar per cluster. Instead a list is returned with each element\n        of the list being a numpy array of exemplar points for a cluster --\n        these points are the \"most representative\" points of the cluster.\n    relative_validity_ : float\n        A fast approximation of the Density Based Cluster Validity (DBCV)\n        score [4]. The only difference, and the speed, comes from the fact\n        that this relative_validity_ is computed using the mutual-\n        reachability minimum spanning tree, i.e. minimum_spanning_tree_,\n        instead of the all-points minimum spanning tree used in the\n        reference. This score might not be an objective measure of the\n        goodness of clustering. It may only be used to compare results\n        across different choices of hyper-parameters, therefore is only a\n        relative score.\n    References\n    ----------\n    .. [1] Campello, R. J., Moulavi, D., & Sander, J. (2013, April).\n       Density-based clustering based on hierarchical density estimates.\n       In Pacific-Asia Conference on Knowledge Discovery and Data Mining\n       (pp. 160-172). Springer Berlin Heidelberg.\n    .. [2] Campello, R. J., Moulavi, D., Zimek, A., & Sander, J. (2015).\n       Hierarchical density estimates for data clustering, visualization,\n       and outlier detection. ACM Transactions on Knowledge Discovery\n       from Data (TKDD), 10(1), 5.\n    .. [3] Chaudhuri, K., & Dasgupta, S. (2010). Rates of convergence for the\n       cluster tree. In Advances in Neural Information Processing Systems\n       (pp. 343-351).\n    .. [4] Moulavi, D., Jaskowiak, P.A., Campello, R.J., Zimek, A. and\n       Sander, J., 2014. Density-Based Clustering Validation. In SDM\n       (pp. 839-847).\n    .. [5] Malzer, C., & Baum, M. (2019). A Hybrid Approach To Hierarchical\n           Density-based Cluster Selection. arxiv preprint 1911.02282.\n    '''\n\n    def __init__(self, min_cluster_size=5, min_samples=None, cluster_selection_epsilon=0.0, cluster_selection_persistence=0.0, max_cluster_size=0, metric='euclidean', alpha=1.0, p=None, algorithm='best', leaf_size=40, memory=Memory(None, verbose=0), approx_min_span_tree=True, gen_min_span_tree=False, core_dist_n_jobs=4, cluster_selection_method='eom', allow_single_cluster=False, prediction_data=False, branch_detection_data=False, match_reference_implementation=False, cluster_selection_epsilon_max=float('inf'), **kwargs):\n        pass\n\n    def fit(self, X, y=None):\n        '''Perform HDBSCAN clustering from features or distance matrix.\n        Parameters\n        ----------\n        X : array or sparse (CSR) matrix of shape (n_samples, n_features), or                 array of shape (n_samples, n_samples)\n            A feature array, or array of distances between samples if\n            ``metric='precomputed'``.\n        Returns\n        -------\n        self : object\n            Returns self\n        '''\n        pass\n\n    def fit_predict(self, X, y=None):\n        '''Performs clustering on X and returns cluster labels.\n        Parameters\n        ----------\n        X : array or sparse (CSR) matrix of shape (n_samples, n_features), or                 array of shape (n_samples, n_samples)\n            A feature array, or array of distances between samples if\n            ``metric='precomputed'``.\n        Returns\n        -------\n        y : ndarray, shape (n_samples, )\n            cluster labels\n        '''\n        pass\n\n    def generate_prediction_data(self):\n        '''\n        Create data that caches intermediate results used for predicting\n        the label of new/unseen points. This data is only useful if\n        you are intending to use functions from ``hdbscan.prediction``.\n        '''\n        pass\n\n    def generate_branch_detection_data(self):\n        '''\n        Create data that caches intermediate results used for detecting\n        branches within clusters. This data is only useful if you are\n        intending to use functions from ``hdbscan.branches``.\n        '''\n        pass\n\n    def weighted_cluster_centroid(self, cluster_id):\n        '''Provide an approximate representative point for a given cluster.\n        Note that this technique assumes a euclidean metric for speed of\n        computation. For more general metrics use the ``weighted_cluster_medoid``\n        method which is slower, but can work with the metric the model trained\n        with.\n        Parameters\n        ----------\n        cluster_id: int\n            The id of the cluster to compute a centroid for.\n        Returns\n        -------\n        centroid: array of shape (n_features,)\n            A representative centroid for cluster ``cluster_id``.\n        '''\n        pass\n\n    def weighted_cluster_medoid(self, cluster_id):\n        '''Provide an approximate representative point for a given cluster.\n        Note that this technique can be very slow and memory intensive for\n        large clusters. For faster results use the ``weighted_cluster_centroid``\n        method which is faster, but assumes a euclidean metric.\n        Parameters\n        ----------\n        cluster_id: int\n            The id of the cluster to compute a medoid for.\n        Returns\n        -------\n        centroid: array of shape (n_features,)\n            A representative medoid for cluster ``cluster_id``.\n        '''\n        pass\n\n    def dbscan_clustering(self, cut_distance, min_cluster_size=5):\n        '''Return clustering that would be equivalent to running DBSCAN* for a particular cut_distance (or epsilon)\n        DBSCAN* can be thought of as DBSCAN without the border points.  As such these results may differ slightly\n        from sklearns implementation of dbscan in the non-core points.\n        This can also be thought of as a flat clustering derived from constant height cut through the single\n        linkage tree.\n        This represents the result of selecting a cut value for robust single linkage\n        clustering. The `min_cluster_size` allows the flat clustering to declare noise\n        points (and cluster smaller than `min_cluster_size`).\n        Parameters\n        ----------\n        cut_distance : float\n            The mutual reachability distance cut value to use to generate a flat clustering.\n        min_cluster_size : int, optional\n            Clusters smaller than this value with be called 'noise' and remain unclustered\n            in the resulting flat clustering.\n        Returns\n        -------\n        labels : array [n_samples]\n            An array of cluster labels, one per datapoint. Unclustered points are assigned\n            the label -1.\n        '''\n        pass\n    @property\n    def prediction_data_(self):\n        pass\n    @prediction_data_.setter\n    def prediction_data_(self):\n        pass\n    @property\n    def branch_detection_data_(self):\n        pass\n    @branch_detection_data_.setter\n    def branch_detection_data_(self):\n        pass\n    @property\n    def outlier_scores_(self):\n        pass\n    @outlier_scores_.setter\n    def outlier_scores_(self):\n        pass\n    @property\n    def condensed_tree_(self):\n        pass\n    @condensed_tree_.setter\n    def condensed_tree_(self):\n        pass\n    @property\n    def single_linkage_tree_(self):\n        pass\n    @single_linkage_tree_.setter\n    def single_linkage_tree_(self):\n        pass\n    @property\n    def minimum_spanning_tree_(self):\n        pass\n    @minimum_spanning_tree_.setter\n    def minimum_spanning_tree_(self):\n        pass\n    @property\n    def exemplars_(self):\n        pass\n    @exemplars_.setter\n    def exemplars_(self):\n        pass\n    @property\n    def relative_validity_(self):\n        pass\n    @relative_validity_.setter\n    def relative_validity_(self):\n        pass\nCorresponding class implementation:\n\nfrom sklearn.utils import check_array\nfrom sklearn.base import BaseEstimator, ClusterMixin\nimport numpy as np\nfrom .plots import CondensedTree, SingleLinkageTree, MinimumSpanningTree\nfrom warnings import warn\nfrom joblib import Memory\nfrom ._hdbscan_tree import condense_tree, compute_stability, get_clusters, outlier_scores, simplify_hierarchy\nfrom scipy.sparse import issparse\nfrom .branch_data import BranchDetectionData\nfrom sklearn.metrics import pairwise_distances\nfrom .prediction import PredictionData\n\nclass HDBSCAN(BaseEstimator, ClusterMixin):\n    \"\"\"Perform HDBSCAN clustering from vector array or distance matrix.\n\n    HDBSCAN - Hierarchical Density-Based Spatial Clustering of Applications\n    with Noise. Performs DBSCAN over varying epsilon values and integrates\n    the result to find a clustering that gives the best stability over epsilon.\n    This allows HDBSCAN to find clusters of varying densities (unlike DBSCAN),\n    and be more robust to parameter selection.\n\n    Parameters\n    ----------\n    min_cluster_size : int, optional (default=5)\n        The minimum size of clusters; single linkage splits that contain\n        fewer points than this will be considered points \"falling out\" of a\n        cluster rather than a cluster splitting into two new clusters.\n\n    min_samples : int, optional (default=None)\n        The number of samples in a neighborhood for a point to be considered as a core point.\n        This includes the point itself. When None, defaults to min_cluster_size.\n\n    metric : string, or callable, optional (default='euclidean')\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string or callable, it must be one of\n        the options allowed by metrics.pairwise.pairwise_distances for its\n        metric parameter.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square.\n\n    p : int, optional (default=None)\n        p value to use if using the minkowski metric.\n\n    alpha : float, optional (default=1.0)\n        A distance scaling parameter as used in robust single linkage.\n        See [3]_ for more information.\n\n    cluster_selection_epsilon: float, optional (default=0.0)\n        A distance threshold. Clusters below this value will be merged.\n        This is the minimum epsilon allowed.\n        See [5]_ for more information.\n\n    cluster_selection_persistence: float, optional (default=0.0)\n        A persistence threshold. Clusters with a persistence lower than this\n        value will be merged.\n\n    algorithm : string, optional (default='best')\n        Exactly which algorithm to use; hdbscan has variants specialised\n        for different characteristics of the data. By default this is set\n        to ``best`` which chooses the \"best\" algorithm given the nature of\n        the data. You can force other options if you believe you know\n        better. Options are:\n            * ``best``\n            * ``generic``\n            * ``prims_kdtree``\n            * ``prims_balltree``\n            * ``boruvka_kdtree``\n            * ``boruvka_balltree``\n\n    leaf_size: int, optional (default=40)\n        If using a space tree algorithm (kdtree, or balltree) the number\n        of points ina leaf node of the tree. This does not alter the\n        resulting clustering, but may have an effect on the runtime\n        of the algorithm.\n\n    memory : Instance of joblib.Memory or string (optional)\n        Used to cache the output of the computation of the tree.\n        By default, no caching is done. If a string is given, it is the\n        path to the caching directory.\n\n    approx_min_span_tree : bool, optional (default=True)\n        Whether to accept an only approximate minimum spanning tree.\n        For some algorithms this can provide a significant speedup, but\n        the resulting clustering may be of marginally lower quality.\n        If you are willing to sacrifice speed for correctness you may want\n        to explore this; in general this should be left at the default True.\n\n    gen_min_span_tree: bool, optional (default=False)\n        Whether to generate the minimum spanning tree with regard\n        to mutual reachability distance for later analysis.\n\n    core_dist_n_jobs : int, optional (default=4)\n        Number of parallel jobs to run in core distance computations (if\n        supported by the specific algorithm). For ``core_dist_n_jobs``\n        below -1, (n_cpus + 1 + core_dist_n_jobs) are used.\n\n    cluster_selection_method : string, optional (default='eom')\n        The method used to select clusters from the condensed tree. The\n        standard approach for HDBSCAN* is to use an Excess of Mass algorithm\n        to find the most persistent clusters. Alternatively you can instead\n        select the clusters at the leaves of the tree -- this provides the\n        most fine grained and homogeneous clusters. Options are:\n            * ``eom``\n            * ``leaf``\n\n    allow_single_cluster : bool, optional (default=False)\n        By default HDBSCAN* will not produce a single cluster, setting this\n        to True will override this and allow single cluster results in\n        the case that you feel this is a valid result for your dataset.\n\n    prediction_data : boolean, optional\n        Whether to generate extra cached data for predicting labels or\n        membership vectors for new unseen points later. If you wish to\n        persist the clustering object for later re-use you probably want\n        to set this to True.\n        (default False)\n\n    branch_detection_data : boolean, optional\n        Whether to generated extra cached data for detecting branch-\n        hierarchies within clusters. If you wish to use functions from\n        ``hdbscan.branches`` set this to True. (default False)\n\n    match_reference_implementation : bool, optional (default=False)\n        There exist some interpretational differences between this\n        HDBSCAN* implementation and the original authors reference\n        implementation in Java. This can result in very minor differences\n        in clustering results. Setting this flag to True will, at a some\n        performance cost, ensure that the clustering results match the\n        reference implementation.\n\n    cluster_selection_epsilon_max: float, optional (default=inf)\n        A distance threshold. Clusters above this value will be split.\n        Has no effect when using leaf clustering (where clusters are\n        usually small regardless) and can also be overridden in rare\n        cases by a high value for cluster_selection_epsilon. Note that\n        this should not be used if we want to predict the cluster labels\n        for new points in future (e.g. using approximate_predict), as\n        the approximate_predict function is not aware of this argument.\n        This is the maximum epsilon allowed.\n\n    **kwargs : optional\n        Arguments passed to the distance metric\n\n    Attributes\n    ----------\n    labels_ : ndarray, shape (n_samples, )\n        Cluster labels for each point in the dataset given to fit().\n        Noisy samples are given the label -1.\n\n    probabilities_ : ndarray, shape (n_samples, )\n        The strength with which each sample is a member of its assigned\n        cluster. Noise points have probability zero; points in clusters\n        have values assigned proportional to the degree that they\n        persist as part of the cluster.\n\n    cluster_persistence_ : ndarray, shape (n_clusters, )\n        A score of how persistent each cluster is. A score of 1.0 represents\n        a perfectly stable cluster that persists over all distance scales,\n        while a score of 0.0 represents a perfectly ephemeral cluster. These\n        scores can be gauge the relative coherence of the clusters output\n        by the algorithm.\n\n    condensed_tree_ : CondensedTree object\n        The condensed tree produced by HDBSCAN. The object has methods\n        for converting to pandas, networkx, and plotting.\n\n    single_linkage_tree_ : SingleLinkageTree object\n        The single linkage tree produced by HDBSCAN. The object has methods\n        for converting to pandas, networkx, and plotting.\n\n    minimum_spanning_tree_ : MinimumSpanningTree object\n        The minimum spanning tree of the mutual reachability graph generated\n        by HDBSCAN. Note that this is not generated by default and will only\n        be available if `gen_min_span_tree` was set to True on object creation.\n        Even then in some optimized cases a tre may not be generated.\n\n    outlier_scores_ : ndarray, shape (n_samples, )\n        Outlier scores for clustered points; the larger the score the more\n        outlier-like the point. Useful as an outlier detection technique.\n        Based on the GLOSH algorithm by Campello, Moulavi, Zimek and Sander.\n\n    prediction_data_ : PredictionData object\n        Cached data used for predicting the cluster labels of new or\n        unseen points. Necessary only if you are using functions from\n        ``hdbscan.prediction`` (see\n        :func:`~hdbscan.prediction.approximate_predict`,\n        :func:`~hdbscan.prediction.membership_vector`,\n        and :func:`~hdbscan.prediction.all_points_membership_vectors`).\n\n    branch_detection_data_ : BranchDetectionData object\n        Cached data used for detecting branch-hierarchies within clusters.\n        Necessary only if you are using function from ``hdbscan.branches``.\n\n    exemplars_ : list\n        A list of exemplar points for clusters. Since HDBSCAN supports\n        arbitrary shapes for clusters we cannot provide a single cluster\n        exemplar per cluster. Instead a list is returned with each element\n        of the list being a numpy array of exemplar points for a cluster --\n        these points are the \"most representative\" points of the cluster.\n\n    relative_validity_ : float\n        A fast approximation of the Density Based Cluster Validity (DBCV)\n        score [4]. The only difference, and the speed, comes from the fact\n        that this relative_validity_ is computed using the mutual-\n        reachability minimum spanning tree, i.e. minimum_spanning_tree_,\n        instead of the all-points minimum spanning tree used in the\n        reference. This score might not be an objective measure of the\n        goodness of clustering. It may only be used to compare results\n        across different choices of hyper-parameters, therefore is only a\n        relative score.\n\n    References\n    ----------\n\n    .. [1] Campello, R. J., Moulavi, D., & Sander, J. (2013, April).\n       Density-based clustering based on hierarchical density estimates.\n       In Pacific-Asia Conference on Knowledge Discovery and Data Mining\n       (pp. 160-172). Springer Berlin Heidelberg.\n\n    .. [2] Campello, R. J., Moulavi, D., Zimek, A., & Sander, J. (2015).\n       Hierarchical density estimates for data clustering, visualization,\n       and outlier detection. ACM Transactions on Knowledge Discovery\n       from Data (TKDD), 10(1), 5.\n\n    .. [3] Chaudhuri, K., & Dasgupta, S. (2010). Rates of convergence for the\n       cluster tree. In Advances in Neural Information Processing Systems\n       (pp. 343-351).\n\n    .. [4] Moulavi, D., Jaskowiak, P.A., Campello, R.J., Zimek, A. and\n       Sander, J., 2014. Density-Based Clustering Validation. In SDM\n       (pp. 839-847).\n\n    .. [5] Malzer, C., & Baum, M. (2019). A Hybrid Approach To Hierarchical\n           Density-based Cluster Selection. arxiv preprint 1911.02282.\n\n    \"\"\"\n\n    def __init__(self, min_cluster_size=5, min_samples=None, cluster_selection_epsilon=0.0, cluster_selection_persistence=0.0, max_cluster_size=0, metric='euclidean', alpha=1.0, p=None, algorithm='best', leaf_size=40, memory=Memory(None, verbose=0), approx_min_span_tree=True, gen_min_span_tree=False, core_dist_n_jobs=4, cluster_selection_method='eom', allow_single_cluster=False, prediction_data=False, branch_detection_data=False, match_reference_implementation=False, cluster_selection_epsilon_max=float('inf'), **kwargs):\n        self.min_cluster_size = min_cluster_size\n        self.min_samples = min_samples\n        self.alpha = alpha\n        self.max_cluster_size = max_cluster_size\n        self.cluster_selection_epsilon = cluster_selection_epsilon\n        self.cluster_selection_persistence = cluster_selection_persistence\n        self.metric = metric\n        self.p = p\n        self.algorithm = algorithm\n        self.leaf_size = leaf_size\n        self.memory = memory\n        self.approx_min_span_tree = approx_min_span_tree\n        self.gen_min_span_tree = gen_min_span_tree\n        self.core_dist_n_jobs = core_dist_n_jobs\n        self.cluster_selection_method = cluster_selection_method\n        self.allow_single_cluster = allow_single_cluster\n        self.match_reference_implementation = match_reference_implementation\n        self.prediction_data = prediction_data\n        self.branch_detection_data = branch_detection_data\n        self.cluster_selection_epsilon_max = cluster_selection_epsilon_max\n        self._metric_kwargs = kwargs\n        self._condensed_tree = None\n        self._single_linkage_tree = None\n        self._min_spanning_tree = None\n        self._raw_data = None\n        self._outlier_scores = None\n        self._prediction_data = None\n        self._finite_index = None\n        self._branch_detection_data = None\n        self._relative_validity = None\n\n    def fit(self, X, y=None):\n        \"\"\"Perform HDBSCAN clustering from features or distance matrix.\n\n        Parameters\n        ----------\n        X : array or sparse (CSR) matrix of shape (n_samples, n_features), or                 array of shape (n_samples, n_samples)\n            A feature array, or array of distances between samples if\n            ``metric='precomputed'``.\n\n        Returns\n        -------\n        self : object\n            Returns self\n        \"\"\"\n        if self.metric != 'precomputed':\n            X = check_array(X, accept_sparse='csr', force_all_finite=False)\n            self._raw_data = X\n            self._all_finite = is_finite(X)\n            if ~self._all_finite:\n                finite_index = get_finite_row_indices(X)\n                clean_data = X[finite_index]\n                internal_to_raw = {x: y for x, y in zip(range(len(finite_index)), finite_index)}\n                outliers = list(set(range(X.shape[0])) - set(finite_index))\n            else:\n                clean_data = X\n        elif issparse(X):\n            X = check_array(X, accept_sparse='csr')\n            clean_data = X\n        else:\n            check_precomputed_distance_matrix(X)\n            clean_data = X\n        kwargs = self.get_params()\n        kwargs.pop('prediction_data', None)\n        kwargs.pop('branch_detection_data', None)\n        kwargs.update(self._metric_kwargs)\n        kwargs['gen_min_span_tree'] |= self.branch_detection_data\n        self.labels_, self.probabilities_, self.cluster_persistence_, self._condensed_tree, self._single_linkage_tree, self._min_spanning_tree = hdbscan(clean_data, **kwargs)\n        if self.metric != 'precomputed' and (not self._all_finite):\n            self._condensed_tree = remap_condensed_tree(self._condensed_tree, internal_to_raw, outliers)\n            self._single_linkage_tree = remap_single_linkage_tree(self._single_linkage_tree, internal_to_raw, outliers)\n            new_labels = np.full(X.shape[0], -1)\n            new_labels[finite_index] = self.labels_\n            self.labels_ = new_labels\n            new_probabilities = np.zeros(X.shape[0])\n            new_probabilities[finite_index] = self.probabilities_\n            self.probabilities_ = new_probabilities\n        if self.prediction_data:\n            self.generate_prediction_data()\n        if self.branch_detection_data:\n            self.generate_branch_detection_data()\n        return self\n\n    def fit_predict(self, X, y=None):\n        \"\"\"Performs clustering on X and returns cluster labels.\n\n        Parameters\n        ----------\n        X : array or sparse (CSR) matrix of shape (n_samples, n_features), or                 array of shape (n_samples, n_samples)\n            A feature array, or array of distances between samples if\n            ``metric='precomputed'``.\n\n        Returns\n        -------\n        y : ndarray, shape (n_samples, )\n            cluster labels\n        \"\"\"\n        self.fit(X)\n        return self.labels_\n\n    def generate_prediction_data(self):\n        \"\"\"\n        Create data that caches intermediate results used for predicting\n        the label of new/unseen points. This data is only useful if\n        you are intending to use functions from ``hdbscan.prediction``.\n        \"\"\"\n        if self.metric in FAST_METRICS:\n            min_samples = self.min_samples or self.min_cluster_size\n            if self.metric in KDTREE_VALID_METRICS:\n                tree_type = 'kdtree'\n            elif self.metric in BALLTREE_VALID_METRICS:\n                tree_type = 'balltree'\n            else:\n                warn('Metric {} not supported for prediction data!'.format(self.metric))\n                return\n            self._prediction_data = PredictionData(self._raw_data, self.condensed_tree_, min_samples, tree_type=tree_type, metric=self.metric, **self._metric_kwargs)\n        else:\n            warn('Cannot generate prediction data for non-vector space inputs -- access to the source data rather than mere distances is required!')\n\n    def generate_branch_detection_data(self):\n        \"\"\"\n        Create data that caches intermediate results used for detecting\n        branches within clusters. This data is only useful if you are\n        intending to use functions from ``hdbscan.branches``.\n        \"\"\"\n        if self.metric in FAST_METRICS:\n            min_samples = self.min_samples or self.min_cluster_size\n            if self.metric in KDTREE_VALID_METRICS:\n                tree_type = 'kdtree'\n            elif self.metric in BALLTREE_VALID_METRICS:\n                tree_type = 'balltree'\n            else:\n                warn('Metric {} not supported for branch detection!'.format(self.metric))\n                return\n            self._branch_detection_data = BranchDetectionData(self._raw_data, self.labels_, self._condensed_tree, min_samples, tree_type=tree_type, metric=self.metric, **self._metric_kwargs)\n        else:\n            warn('Branch detection for non-vector space inputs is not (yet) implemented.')\n\n    def weighted_cluster_centroid(self, cluster_id):\n        \"\"\"Provide an approximate representative point for a given cluster.\n        Note that this technique assumes a euclidean metric for speed of\n        computation. For more general metrics use the ``weighted_cluster_medoid``\n        method which is slower, but can work with the metric the model trained\n        with.\n\n        Parameters\n        ----------\n        cluster_id: int\n            The id of the cluster to compute a centroid for.\n\n        Returns\n        -------\n        centroid: array of shape (n_features,)\n            A representative centroid for cluster ``cluster_id``.\n        \"\"\"\n        if not hasattr(self, 'labels_'):\n            raise AttributeError('Model has not been fit to data')\n        if cluster_id == -1:\n            raise ValueError('Cannot calculate weighted centroid for -1 cluster since it is a noise cluster')\n        mask = self.labels_ == cluster_id\n        cluster_data = self._raw_data[mask]\n        cluster_membership_strengths = self.probabilities_[mask]\n        return np.average(cluster_data, weights=cluster_membership_strengths, axis=0)\n\n    def weighted_cluster_medoid(self, cluster_id):\n        \"\"\"Provide an approximate representative point for a given cluster.\n        Note that this technique can be very slow and memory intensive for\n        large clusters. For faster results use the ``weighted_cluster_centroid``\n        method which is faster, but assumes a euclidean metric.\n\n        Parameters\n        ----------\n        cluster_id: int\n            The id of the cluster to compute a medoid for.\n\n        Returns\n        -------\n        centroid: array of shape (n_features,)\n            A representative medoid for cluster ``cluster_id``.\n        \"\"\"\n        if not hasattr(self, 'labels_'):\n            raise AttributeError('Model has not been fit to data')\n        if cluster_id == -1:\n            raise ValueError('Cannot calculate weighted centroid for -1 cluster since it is a noise cluster')\n        mask = self.labels_ == cluster_id\n        cluster_data = self._raw_data[mask]\n        cluster_membership_strengths = self.probabilities_[mask]\n        dist_mat = pairwise_distances(cluster_data, metric=self.metric, **self._metric_kwargs)\n        dist_mat = dist_mat * cluster_membership_strengths\n        medoid_index = np.argmin(dist_mat.sum(axis=1))\n        return cluster_data[medoid_index]\n\n    def dbscan_clustering(self, cut_distance, min_cluster_size=5):\n        \"\"\"Return clustering that would be equivalent to running DBSCAN* for a particular cut_distance (or epsilon)\n        DBSCAN* can be thought of as DBSCAN without the border points.  As such these results may differ slightly\n        from sklearns implementation of dbscan in the non-core points.\n\n        This can also be thought of as a flat clustering derived from constant height cut through the single\n        linkage tree.\n\n        This represents the result of selecting a cut value for robust single linkage\n        clustering. The `min_cluster_size` allows the flat clustering to declare noise\n        points (and cluster smaller than `min_cluster_size`).\n\n        Parameters\n        ----------\n\n        cut_distance : float\n            The mutual reachability distance cut value to use to generate a flat clustering.\n\n        min_cluster_size : int, optional\n            Clusters smaller than this value with be called 'noise' and remain unclustered\n            in the resulting flat clustering.\n\n        Returns\n        -------\n\n        labels : array [n_samples]\n            An array of cluster labels, one per datapoint. Unclustered points are assigned\n            the label -1.\n        \"\"\"\n        return self.single_linkage_tree_.get_clusters(cut_distance=cut_distance, min_cluster_size=min_cluster_size)\n\n    @property\n    def prediction_data_(self):\n        if self._prediction_data is None:\n            raise AttributeError('No prediction data was generated')\n        else:\n            return self._prediction_data\n\n    @prediction_data_.setter\n    def prediction_data_(self, value):\n        self._prediction_data = value\n\n    @property\n    def branch_detection_data_(self):\n        if self._branch_detection_data is None:\n            raise AttributeError('No branch detection data was generated')\n        else:\n            return self._branch_detection_data\n\n    @branch_detection_data_.setter\n    def branch_detection_data_(self, value):\n        self._branch_detection_data = value\n\n    @property\n    def outlier_scores_(self):\n        if self._outlier_scores is not None:\n            return self._outlier_scores\n        elif self._condensed_tree is not None:\n            self._outlier_scores = outlier_scores(self._condensed_tree)\n            return self._outlier_scores\n        else:\n            raise AttributeError('No condensed tree was generated; try running fit first.')\n\n    @outlier_scores_.setter\n    def outlier_scores_(self, value):\n        self._outlier_scores = value\n\n    @property\n    def condensed_tree_(self):\n        if self._condensed_tree is not None:\n            return CondensedTree(self._condensed_tree, self.labels_)\n        else:\n            raise AttributeError('No condensed tree was generated; try running fit first.')\n\n    @condensed_tree_.setter\n    def condensed_tree_(self, value):\n        self._condensed_tree = value\n\n    @property\n    def single_linkage_tree_(self):\n        if self._single_linkage_tree is not None:\n            return SingleLinkageTree(self._single_linkage_tree)\n        else:\n            raise AttributeError('No single linkage tree was generated; try running fit first.')\n\n    @single_linkage_tree_.setter\n    def single_linkage_tree_(self, value):\n        self._single_linkage_tree = value\n\n    @property\n    def minimum_spanning_tree_(self):\n        if self._min_spanning_tree is not None:\n            if self._raw_data is not None:\n                return MinimumSpanningTree(self._min_spanning_tree, self._raw_data)\n            else:\n                warn('No raw data is available; this may be due to using a precomputed metric matrix. No minimum spanning tree will be provided without raw data.')\n                return None\n        else:\n            raise AttributeError('No minimum spanning tree was generated.This may be due to optimized algorithm variations that skip explicit generation of the spanning tree.')\n\n    @minimum_spanning_tree_.setter\n    def minimum_spanning_tree_(self, value):\n        self._min_spanning_tree = value\n\n    @property\n    def exemplars_(self):\n        if self._prediction_data is not None:\n            return self._prediction_data.exemplars\n        elif self.metric in FAST_METRICS:\n            self.generate_prediction_data()\n            return self._prediction_data.exemplars\n        else:\n            raise AttributeError('Currently exemplars require the use of vector input datawith a suitable metric. This will likely change in the future, but for now no exemplars can be provided')\n\n    @exemplars_.setter\n    def exemplars_(self, value):\n        self._exemplars = value\n\n    @property\n    def relative_validity_(self):\n        if self._relative_validity is not None:\n            return self._relative_validity\n        if not self.gen_min_span_tree:\n            raise AttributeError('Minimum spanning tree not present. ' + 'Either HDBSCAN object was created with ' + 'gen_min_span_tree=False or the tree was ' + 'not generated in spite of it owing to ' + 'internal optimization criteria.')\n            return\n        labels = self.labels_\n        sizes = np.bincount(labels + 1)\n        noise_size = sizes[0]\n        cluster_size = sizes[1:]\n        total = noise_size + np.sum(cluster_size)\n        num_clusters = len(cluster_size)\n        DSC = np.zeros(num_clusters)\n        min_outlier_sep = np.inf\n        correction_const = 2\n        DSPC_wrt = np.ones(num_clusters) * np.inf\n        max_distance = 0\n        mst_df = self.minimum_spanning_tree_.to_pandas()\n        for edge in mst_df.iterrows():\n            label1 = labels[int(edge[1]['from'])]\n            label2 = labels[int(edge[1]['to'])]\n            length = edge[1]['distance']\n            max_distance = max(max_distance, length)\n            if label1 == -1 and label2 == -1:\n                continue\n            elif label1 == -1 or label2 == -1:\n                min_outlier_sep = min(min_outlier_sep, length)\n                continue\n            if label1 == label2:\n                DSC[label1] = max(length, DSC[label1])\n            else:\n                DSPC_wrt[label1] = min(length, DSPC_wrt[label1])\n                DSPC_wrt[label2] = min(length, DSPC_wrt[label2])\n        min_outlier_sep = max_distance if min_outlier_sep == np.inf else min_outlier_sep\n        correction = correction_const * (max_distance if num_clusters > 1 else min_outlier_sep)\n        DSPC_wrt[np.where(DSPC_wrt == np.inf)] = correction\n        V_index = [(DSPC_wrt[i] - DSC[i]) / max(DSPC_wrt[i], DSC[i]) for i in range(num_clusters)]\n        score = np.sum([cluster_size[i] * V_index[i] / total for i in range(num_clusters)])\n        self._relative_validity = score\n        return self._relative_validity\n\n    @relative_validity_.setter\n    def relative_validity_(self, value):\n        self._relative_validity = value\n\nExample skeleton:\n\n\nclass SymbolicRegressor(BaseSymbolic, RegressorMixin):\n    '''A Genetic Programming symbolic regressor.\n    A symbolic regressor is an estimator that begins by building a population\n    of naive random formulas to represent a relationship. The formulas are\n    represented as tree-like structures with mathematical functions being\n    recursively applied to variables and constants. Each successive generation\n    of programs is then evolved from the one that came before it by selecting\n    the fittest individuals from the population to undergo genetic operations\n    such as crossover, mutation or reproduction.\n    Parameters\n    ----------\n    population_size : integer, optional (default=1000)\n        The number of programs in each generation.\n    generations : integer, optional (default=20)\n        The number of generations to evolve.\n    tournament_size : integer, optional (default=20)\n        The number of programs that will compete to become part of the next\n        generation.\n    stopping_criteria : float, optional (default=0.0)\n        The required metric value required in order to stop evolution early.\n    const_range : tuple of two floats, or None, optional (default=(-1., 1.))\n        The range of constants to include in the formulas. If None then no\n        constants will be included in the candidate programs.\n    init_depth : tuple of two ints, optional (default=(2, 6))\n        The range of tree depths for the initial population of naive formulas.\n        Individual trees will randomly choose a maximum depth from this range.\n        When combined with `init_method='half and half'` this yields the well-\n        known 'ramped half and half' initialization method.\n    init_method : str, optional (default='half and half')\n        - 'grow' : Nodes are chosen at random from both functions and\n          terminals, allowing for smaller trees than `init_depth` allows. Tends\n          to grow asymmetrical trees.\n        - 'full' : Functions are chosen until the `init_depth` is reached, and\n          then terminals are selected. Tends to grow 'bushy' trees.\n        - 'half and half' : Trees are grown through a 50/50 mix of 'full' and\n          'grow', making for a mix of tree shapes in the initial population.\n    function_set : iterable, optional (default=('add', 'sub', 'mul', 'div'))\n        The functions to use when building and evolving programs. This iterable\n        can include strings to indicate either individual functions as outlined\n        below, or you can also include your own functions as built using the\n        ``make_function`` factory from the ``functions`` module.\n        Available individual functions are:\n        - 'add' : addition, arity=2.\n        - 'sub' : subtraction, arity=2.\n        - 'mul' : multiplication, arity=2.\n        - 'div' : protected division where a denominator near-zero returns 1.,\n          arity=2.\n        - 'sqrt' : protected square root where the absolute value of the\n          argument is used, arity=1.\n        - 'log' : protected log where the absolute value of the argument is\n          used and a near-zero argument returns 0., arity=1.\n        - 'abs' : absolute value, arity=1.\n        - 'neg' : negative, arity=1.\n        - 'inv' : protected inverse where a near-zero argument returns 0.,\n          arity=1.\n        - 'max' : maximum, arity=2.\n        - 'min' : minimum, arity=2.\n        - 'sin' : sine (radians), arity=1.\n        - 'cos' : cosine (radians), arity=1.\n        - 'tan' : tangent (radians), arity=1.\n    metric : str, optional (default='mean absolute error')\n        The name of the raw fitness metric. Available options include:\n        - 'mean absolute error'.\n        - 'mse' for mean squared error.\n        - 'rmse' for root mean squared error.\n        - 'pearson', for Pearson's product-moment correlation coefficient.\n        - 'spearman' for Spearman's rank-order correlation coefficient.\n        Note that 'pearson' and 'spearman' will not directly predict the target\n        but could be useful as value-added features in a second-step estimator.\n        This would allow the user to generate one engineered feature at a time,\n        using the SymbolicTransformer would allow creation of multiple features\n        at once.\n    parsimony_coefficient : float or \"auto\", optional (default=0.001)\n        This constant penalizes large programs by adjusting their fitness to\n        be less favorable for selection. Larger values penalize the program\n        more which can control the phenomenon known as 'bloat'. Bloat is when\n        evolution is increasing the size of programs without a significant\n        increase in fitness, which is costly for computation time and makes for\n        a less understandable final result. This parameter may need to be tuned\n        over successive runs.\n        If \"auto\" the parsimony coefficient is recalculated for each generation\n        using c = Cov(l,f)/Var( l), where Cov(l,f) is the covariance between\n        program size l and program fitness f in the population, and Var(l) is\n        the variance of program sizes.\n    p_crossover : float, optional (default=0.9)\n        The probability of performing crossover on a tournament winner.\n        Crossover takes the winner of a tournament and selects a random subtree\n        from it to be replaced. A second tournament is performed to find a\n        donor. The donor also has a subtree selected at random and this is\n        inserted into the original parent to form an offspring in the next\n        generation.\n    p_subtree_mutation : float, optional (default=0.01)\n        The probability of performing subtree mutation on a tournament winner.\n        Subtree mutation takes the winner of a tournament and selects a random\n        subtree from it to be replaced. A donor subtree is generated at random\n        and this is inserted into the original parent to form an offspring in\n        the next generation.\n    p_hoist_mutation : float, optional (default=0.01)\n        The probability of performing hoist mutation on a tournament winner.\n        Hoist mutation takes the winner of a tournament and selects a random\n        subtree from it. A random subtree of that subtree is then selected\n        and this is 'hoisted' into the original subtrees location to form an\n        offspring in the next generation. This method helps to control bloat.\n    p_point_mutation : float, optional (default=0.01)\n        The probability of performing point mutation on a tournament winner.\n        Point mutation takes the winner of a tournament and selects random\n        nodes from it to be replaced. Terminals are replaced by other terminals\n        and functions are replaced by other functions that require the same\n        number of arguments as the original node. The resulting tree forms an\n        offspring in the next generation.\n        Note : The above genetic operation probabilities must sum to less than\n        one. The balance of probability is assigned to 'reproduction', where a\n        tournament winner is cloned and enters the next generation unmodified.\n    p_point_replace : float, optional (default=0.05)\n        For point mutation only, the probability that any given node will be\n        mutated.\n    max_samples : float, optional (default=1.0)\n        The fraction of samples to draw from X to evaluate each program on.\n    feature_names : list, optional (default=None)\n        Optional list of feature names, used purely for representations in\n        the `print` operation or `export_graphviz`. If None, then X0, X1, etc\n        will be used for representations.\n    warm_start : bool, optional (default=False)\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more generations to the evolution, otherwise, just fit a new\n        evolution.\n    low_memory : bool, optional (default=False)\n        When set to ``True``, only the current generation is retained. Parent\n        information is discarded. For very large populations or runs with many\n        generations, this can result in substantial memory use reduction.\n    n_jobs : integer, optional (default=1)\n        The number of jobs to run in parallel for `fit`. If -1, then the number\n        of jobs is set to the number of cores.\n    verbose : int, optional (default=0)\n        Controls the verbosity of the evolution building process.\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n    Attributes\n    ----------\n    run_details_ : dict\n        Details of the evolution process. Includes the following elements:\n        - 'generation' : The generation index.\n        - 'average_length' : The average program length of the generation.\n        - 'average_fitness' : The average program fitness of the generation.\n        - 'best_length' : The length of the best program in the generation.\n        - 'best_fitness' : The fitness of the best program in the generation.\n        - 'best_oob_fitness' : The out of bag fitness of the best program in\n          the generation (requires `max_samples` < 1.0).\n        - 'generation_time' : The time it took for the generation to evolve.\n    See Also\n    --------\n    SymbolicTransformer\n    References\n    ----------\n    .. [1] J. Koza, \"Genetic Programming\", 1992.\n    .. [2] R. Poli, et al. \"A Field Guide to Genetic Programming\", 2008.\n    '''\n\n    def __init__(self, *, population_size=1000, generations=20, tournament_size=20, stopping_criteria=0.0, const_range=(-1.0, 1.0), init_depth=(2, 6), init_method='half and half', function_set=('add', 'sub', 'mul', 'div'), metric='mean absolute error', parsimony_coefficient=0.001, p_crossover=0.9, p_subtree_mutation=0.01, p_hoist_mutation=0.01, p_point_mutation=0.01, p_point_replace=0.05, max_samples=1.0, feature_names=None, warm_start=False, low_memory=False, n_jobs=1, verbose=0, random_state=None):\n        pass\n\n    def __str__(self):\n        '''Overloads `print` output of the object to resemble a LISP tree.'''\n        pass\n\n    def predict(self, X):\n        '''Perform regression on test vectors X.\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Input vectors, where n_samples is the number of samples\n            and n_features is the number of features.\n        Returns\n        -------\n        y : array, shape = [n_samples]\n            Predicted values for X.\n        '''\n        pass\nCorresponding class implementation:\n\nfrom sklearn.base import RegressorMixin, TransformerMixin, ClassifierMixin\nfrom sklearn.utils.validation import check_array, _check_sample_weight\nfrom sklearn.exceptions import NotFittedError\n\nclass SymbolicRegressor(BaseSymbolic, RegressorMixin):\n    \"\"\"A Genetic Programming symbolic regressor.\n\n    A symbolic regressor is an estimator that begins by building a population\n    of naive random formulas to represent a relationship. The formulas are\n    represented as tree-like structures with mathematical functions being\n    recursively applied to variables and constants. Each successive generation\n    of programs is then evolved from the one that came before it by selecting\n    the fittest individuals from the population to undergo genetic operations\n    such as crossover, mutation or reproduction.\n\n    Parameters\n    ----------\n    population_size : integer, optional (default=1000)\n        The number of programs in each generation.\n\n    generations : integer, optional (default=20)\n        The number of generations to evolve.\n\n    tournament_size : integer, optional (default=20)\n        The number of programs that will compete to become part of the next\n        generation.\n\n    stopping_criteria : float, optional (default=0.0)\n        The required metric value required in order to stop evolution early.\n\n    const_range : tuple of two floats, or None, optional (default=(-1., 1.))\n        The range of constants to include in the formulas. If None then no\n        constants will be included in the candidate programs.\n\n    init_depth : tuple of two ints, optional (default=(2, 6))\n        The range of tree depths for the initial population of naive formulas.\n        Individual trees will randomly choose a maximum depth from this range.\n        When combined with `init_method='half and half'` this yields the well-\n        known 'ramped half and half' initialization method.\n\n    init_method : str, optional (default='half and half')\n        - 'grow' : Nodes are chosen at random from both functions and\n          terminals, allowing for smaller trees than `init_depth` allows. Tends\n          to grow asymmetrical trees.\n        - 'full' : Functions are chosen until the `init_depth` is reached, and\n          then terminals are selected. Tends to grow 'bushy' trees.\n        - 'half and half' : Trees are grown through a 50/50 mix of 'full' and\n          'grow', making for a mix of tree shapes in the initial population.\n\n    function_set : iterable, optional (default=('add', 'sub', 'mul', 'div'))\n        The functions to use when building and evolving programs. This iterable\n        can include strings to indicate either individual functions as outlined\n        below, or you can also include your own functions as built using the\n        ``make_function`` factory from the ``functions`` module.\n\n        Available individual functions are:\n\n        - 'add' : addition, arity=2.\n        - 'sub' : subtraction, arity=2.\n        - 'mul' : multiplication, arity=2.\n        - 'div' : protected division where a denominator near-zero returns 1.,\n          arity=2.\n        - 'sqrt' : protected square root where the absolute value of the\n          argument is used, arity=1.\n        - 'log' : protected log where the absolute value of the argument is\n          used and a near-zero argument returns 0., arity=1.\n        - 'abs' : absolute value, arity=1.\n        - 'neg' : negative, arity=1.\n        - 'inv' : protected inverse where a near-zero argument returns 0.,\n          arity=1.\n        - 'max' : maximum, arity=2.\n        - 'min' : minimum, arity=2.\n        - 'sin' : sine (radians), arity=1.\n        - 'cos' : cosine (radians), arity=1.\n        - 'tan' : tangent (radians), arity=1.\n\n    metric : str, optional (default='mean absolute error')\n        The name of the raw fitness metric. Available options include:\n\n        - 'mean absolute error'.\n        - 'mse' for mean squared error.\n        - 'rmse' for root mean squared error.\n        - 'pearson', for Pearson's product-moment correlation coefficient.\n        - 'spearman' for Spearman's rank-order correlation coefficient.\n\n        Note that 'pearson' and 'spearman' will not directly predict the target\n        but could be useful as value-added features in a second-step estimator.\n        This would allow the user to generate one engineered feature at a time,\n        using the SymbolicTransformer would allow creation of multiple features\n        at once.\n\n    parsimony_coefficient : float or \"auto\", optional (default=0.001)\n        This constant penalizes large programs by adjusting their fitness to\n        be less favorable for selection. Larger values penalize the program\n        more which can control the phenomenon known as 'bloat'. Bloat is when\n        evolution is increasing the size of programs without a significant\n        increase in fitness, which is costly for computation time and makes for\n        a less understandable final result. This parameter may need to be tuned\n        over successive runs.\n\n        If \"auto\" the parsimony coefficient is recalculated for each generation\n        using c = Cov(l,f)/Var( l), where Cov(l,f) is the covariance between\n        program size l and program fitness f in the population, and Var(l) is\n        the variance of program sizes.\n\n    p_crossover : float, optional (default=0.9)\n        The probability of performing crossover on a tournament winner.\n        Crossover takes the winner of a tournament and selects a random subtree\n        from it to be replaced. A second tournament is performed to find a\n        donor. The donor also has a subtree selected at random and this is\n        inserted into the original parent to form an offspring in the next\n        generation.\n\n    p_subtree_mutation : float, optional (default=0.01)\n        The probability of performing subtree mutation on a tournament winner.\n        Subtree mutation takes the winner of a tournament and selects a random\n        subtree from it to be replaced. A donor subtree is generated at random\n        and this is inserted into the original parent to form an offspring in\n        the next generation.\n\n    p_hoist_mutation : float, optional (default=0.01)\n        The probability of performing hoist mutation on a tournament winner.\n        Hoist mutation takes the winner of a tournament and selects a random\n        subtree from it. A random subtree of that subtree is then selected\n        and this is 'hoisted' into the original subtrees location to form an\n        offspring in the next generation. This method helps to control bloat.\n\n    p_point_mutation : float, optional (default=0.01)\n        The probability of performing point mutation on a tournament winner.\n        Point mutation takes the winner of a tournament and selects random\n        nodes from it to be replaced. Terminals are replaced by other terminals\n        and functions are replaced by other functions that require the same\n        number of arguments as the original node. The resulting tree forms an\n        offspring in the next generation.\n\n        Note : The above genetic operation probabilities must sum to less than\n        one. The balance of probability is assigned to 'reproduction', where a\n        tournament winner is cloned and enters the next generation unmodified.\n\n    p_point_replace : float, optional (default=0.05)\n        For point mutation only, the probability that any given node will be\n        mutated.\n\n    max_samples : float, optional (default=1.0)\n        The fraction of samples to draw from X to evaluate each program on.\n\n    feature_names : list, optional (default=None)\n        Optional list of feature names, used purely for representations in\n        the `print` operation or `export_graphviz`. If None, then X0, X1, etc\n        will be used for representations.\n\n    warm_start : bool, optional (default=False)\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more generations to the evolution, otherwise, just fit a new\n        evolution.\n\n    low_memory : bool, optional (default=False)\n        When set to ``True``, only the current generation is retained. Parent\n        information is discarded. For very large populations or runs with many\n        generations, this can result in substantial memory use reduction.\n\n    n_jobs : integer, optional (default=1)\n        The number of jobs to run in parallel for `fit`. If -1, then the number\n        of jobs is set to the number of cores.\n\n    verbose : int, optional (default=0)\n        Controls the verbosity of the evolution building process.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Attributes\n    ----------\n    run_details_ : dict\n        Details of the evolution process. Includes the following elements:\n\n        - 'generation' : The generation index.\n        - 'average_length' : The average program length of the generation.\n        - 'average_fitness' : The average program fitness of the generation.\n        - 'best_length' : The length of the best program in the generation.\n        - 'best_fitness' : The fitness of the best program in the generation.\n        - 'best_oob_fitness' : The out of bag fitness of the best program in\n          the generation (requires `max_samples` < 1.0).\n        - 'generation_time' : The time it took for the generation to evolve.\n\n    See Also\n    --------\n    SymbolicTransformer\n\n    References\n    ----------\n    .. [1] J. Koza, \"Genetic Programming\", 1992.\n\n    .. [2] R. Poli, et al. \"A Field Guide to Genetic Programming\", 2008.\n\n    \"\"\"\n\n    def __init__(self, *, population_size=1000, generations=20, tournament_size=20, stopping_criteria=0.0, const_range=(-1.0, 1.0), init_depth=(2, 6), init_method='half and half', function_set=('add', 'sub', 'mul', 'div'), metric='mean absolute error', parsimony_coefficient=0.001, p_crossover=0.9, p_subtree_mutation=0.01, p_hoist_mutation=0.01, p_point_mutation=0.01, p_point_replace=0.05, max_samples=1.0, feature_names=None, warm_start=False, low_memory=False, n_jobs=1, verbose=0, random_state=None):\n        super(SymbolicRegressor, self).__init__(population_size=population_size, generations=generations, tournament_size=tournament_size, stopping_criteria=stopping_criteria, const_range=const_range, init_depth=init_depth, init_method=init_method, function_set=function_set, metric=metric, parsimony_coefficient=parsimony_coefficient, p_crossover=p_crossover, p_subtree_mutation=p_subtree_mutation, p_hoist_mutation=p_hoist_mutation, p_point_mutation=p_point_mutation, p_point_replace=p_point_replace, max_samples=max_samples, feature_names=feature_names, warm_start=warm_start, low_memory=low_memory, n_jobs=n_jobs, verbose=verbose, random_state=random_state)\n\n    def __str__(self):\n        \"\"\"Overloads `print` output of the object to resemble a LISP tree.\"\"\"\n        if not hasattr(self, '_program'):\n            return self.__repr__()\n        return self._program.__str__()\n\n    def predict(self, X):\n        \"\"\"Perform regression on test vectors X.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Input vectors, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        y : array, shape = [n_samples]\n            Predicted values for X.\n\n        \"\"\"\n        if not hasattr(self, '_program'):\n            raise NotFittedError('SymbolicRegressor not fitted.')\n        X = check_array(X)\n        _, n_features = X.shape\n        if self.n_features_in_ != n_features:\n            raise ValueError('Number of features of the model must match the input. Model n_features is %s and input n_features is %s.' % (self.n_features_in_, n_features))\n        y = self._program.execute(X)\n        return y\n\nNow, implement the following class. Do not explain the code. The given class skeleton is as follows:\n\nclass BinningAutoBatcher:\n    '''Batcher that groups states into bins of similar computational cost.\n    Divides a collection of states into batches that can be processed efficiently\n    without exceeding GPU memory. States are grouped based on a memory scaling\n    metric to maximize GPU utilization. This approach is ideal for scenarios where\n    all states need to be evolved the same number of steps.\n    To avoid a slow memory estimation step, set the `max_memory_scaler` to a\n    known value.\n    Attributes:\n        model (ModelInterface): Model used for memory estimation and processing.\n        memory_scales_with (str): Metric type used for memory estimation.\n        max_memory_scaler (float): Maximum memory metric allowed per system.\n        max_atoms_to_try (int): Maximum number of atoms to try when estimating memory.\n        return_indices (bool): Whether to return original indices with batches.\n        state_slices (list[SimState]): Individual states to be batched.\n        memory_scalers (list[float]): Memory scaling metrics for each state.\n        index_to_scaler (dict): Mapping from state index to its scaling metric.\n        index_bins (list[list[int]]): Groups of state indices that can be batched\n            together.\n        batched_states (list[list[SimState]]): Grouped states ready for batching.\n        current_state_bin (int): Index of the current batch being processed.\n    Example::\n        # Create a batcher with a Lennard-Jones model\n        batcher = BinningAutoBatcher(\n            model=lj_model, memory_scales_with=\"n_atoms\", max_memory_scaler=1000.0\n        )\n        # Load states and process them in batches\n        batcher.load_states(states)\n        final_states = []\n        for batch in batcher:\n            final_states.append(evolve_batch(batch))\n        # Restore original order\n        ordered_final_states = batcher.restore_original_order(final_states)\n    '''\n\n    def __init__(self, model: ModelInterface, *, memory_scales_with: MemoryScaling='n_atoms_x_density', max_memory_scaler: float | None=None, return_indices: bool=False, max_atoms_to_try: int=500000, memory_scaling_factor: float=1.6, max_memory_padding: float=1.0) -> None:\n        '''Initialize the binning auto-batcher.\n        Args:\n            model (ModelInterface): Model to batch for, used to estimate memory\n                requirements.\n            memory_scales_with (\"n_atoms\" | \"n_atoms_x_density\"): Metric to use\n                for estimating memory requirements:\n                - \"n_atoms\": Uses only atom count\n                - \"n_atoms_x_density\": Uses atom count multiplied by number density\n                Defaults to \"n_atoms_x_density\".\n            max_memory_scaler (float | None): Maximum metric value allowed per system. If\n                None, will be automatically estimated. Defaults to None.\n            return_indices (bool): Whether to return original indices along with batches.\n                Defaults to False.\n            max_atoms_to_try (int): Maximum number of atoms to try when estimating\n                max_memory_scaler. Defaults to 500,000.\n            memory_scaling_factor (float): Factor to multiply batch size by in each\n                iteration. Larger values will get a batch size more quickly, smaller\n                values will get a more accurate limit. Must be greater than 1. Defaults\n                to 1.6.\n            max_memory_padding (float): Multiply the autodetermined max_memory_scaler\n                by this value to account for fluctuations in max memory. Defaults to 1.0.\n        '''\n        pass\n\n    def load_states(self, states: list[SimState] | SimState) -> float:\n        '''Load new states into the batcher.\n        Processes the input states, computes memory scaling metrics for each,\n        and organizes them into optimal batches using a bin-packing algorithm\n        to maximize GPU utilization.\n        Args:\n            states (list[SimState] | SimState): Collection of states to batch. Either a\n                list of individual SimState objects or a single batched SimState that\n                will be split into individual states. Each SimState has shape\n                information specific to its instance.\n        Returns:\n            float: Maximum memory scaling metric that fits in GPU memory.\n        Raises:\n            ValueError: If any individual state has a memory scaling metric greater\n                than the maximum allowed value.\n        Example::\n            # Load individual states\n            batcher.load_states([state1, state2, state3])\n            # Or load a batched state that will be split\n            batcher.load_states(batched_state)\n        Notes:\n            This method resets the current state bin index, so any ongoing iteration\n            will be restarted when this method is called.\n        '''\n        pass\n\n    def next_batch(self, *, return_indices: bool=False) -> SimState | tuple[SimState, list[int]] | None:\n        '''Get the next batch of states.\n        Returns batches sequentially until all states have been processed. Each batch\n        contains states grouped together to maximize GPU utilization without exceeding\n        memory constraints.\n        Args:\n            return_indices (bool): Whether to return original indices along with the\n                batch. Overrides the value set during initialization. Defaults to False.\n        Returns:\n            SimState | tuple[SimState, list[int]] | None:\n                - If return_indices is False: A concatenated SimState containing the next\n                  batch of states, or None if no more batches.\n                - If return_indices is True: Tuple of (concatenated SimState, indices),\n                  where indices are the original positions of the states, or None if no\n                  more batches.\n        Example::\n            # Get batches one by one\n            all_converged_state, convergence = [], None\n            while (result := batcher.next_batch(state, convergence))[0] is not None:\n                state, converged_states = result\n                all_converged_states.extend(converged_states)\n                evolve_batch(state)\n                convergence = convergence_criterion(state)\n            else:\n                all_converged_states.extend(result[1])\n        '''\n        pass\n\n    def __iter__(self) -> Iterator[SimState | tuple[SimState, list[int]]]:\n        '''Return self as an iterator.\n        Allows using the batcher in a for loop to iterate through all batches.\n        Resets the current state bin index to start iteration from the beginning.\n        Returns:\n            Iterator[SimState | tuple[SimState, list[int]]]: Self as an iterator.\n        Example::\n            # Iterate through all batches\n            for batch in batcher:\n                process_batch(batch)\n        '''\n        pass\n\n    def __next__(self) -> SimState | tuple[SimState, list[int]]:\n        '''Get the next batch for iteration.\n        Implements the iterator protocol to allow using the batcher in a for loop.\n        Automatically includes indices if return_indices was set to True during\n        initialization.\n        Returns:\n            SimState | tuple[SimState, list[int]]: The next batch of states,\n                potentially with indices.\n        Raises:\n            StopIteration: When there are no more batches.\n        '''\n        pass\n\n    def restore_original_order(self, batched_states: list[SimState]) -> list[SimState]:\n        '''Reorder processed states back to their original sequence.\n        Takes states that were processed in batches and restores them to the\n        original order they were provided in. This is essential after batch\n        processing to ensure results correspond to the input states.\n        Args:\n            batched_states (list[SimState]): State batches to reorder. These can be\n                either concatenated batch states that will be split, or already\n                split individual states.\n        Returns:\n            list[SimState]: States in their original order, with shape information\n                matching the original input states.\n        Raises:\n            ValueError: If the number of states doesn't match the number of\n                original indices.\n        Example::\n            # Process batches and restore original order\n            results = []\n            for batch in batcher:\n                results.append(process_batch(batch))\n            ordered_results = batcher.restore_original_order(results)\n        '''\n        pass"
  }
]