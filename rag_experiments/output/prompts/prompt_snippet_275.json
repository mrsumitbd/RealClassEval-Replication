[
  {
    "role": "assistant",
    "content": "You are an expert Python programmer who can correctly implement complete Python classes based on the provided class skeleton."
  },
  {
    "role": "user",
    "content": "For your understanding, following are some (skeleton, code) examples:\n\nExample skeleton:\n\n\nclass TransitionReplayEnvRoller(ReplayEnvRollerBase):\n    '''\n    Calculate environment rollouts using a replay buffer for experience replay.\n    Replay buffer is parametrized\n    Samples transitions from the replay buffer (individual frame transitions)\n    '''\n\n    def __init__(self, environment, device, replay_buffer: ReplayBuffer, discount_factor: typing.Optional[float]=None, normalize_returns: bool=False, forward_steps: int=1, action_noise: typing.Optional[nn.Module]=None):\n        pass\n    @property\n    def environment(self):\n        ''' Return environment of this env roller '''\n        pass\n    @torch.no_grad()\n    def rollout(self, batch_info: BatchInfo, model: RlModel, number_of_steps: int) -> Rollout:\n        ''' Calculate env rollout '''\n        pass\n\n    def sample(self, batch_info: BatchInfo, model: RlModel, number_of_steps: int) -> Rollout:\n        ''' Sample experience from replay buffer and return a batch '''\n        pass\n\n    def is_ready_for_sampling(self) -> bool:\n        ''' If buffer is ready for drawing samples from it (usually checks if there is enough data) '''\n        pass\n\n    def initial_memory_size_hint(self) -> typing.Optional[int]:\n        ''' Hint how much data is needed to begin sampling, required only for diagnostics '''\n        pass\n\n    def update(self, rollout, batch_info):\n        ''' Perform update of the internal state of the buffer - e.g. for the prioritized replay weights '''\n        pass\nCorresponding class implementation:\n\nimport torch\nimport numpy as np\nfrom vel.util.tensor_accumulator import TensorAccumulator\nimport torch.nn as nn\nfrom vel.rl.api import Trajectories, Rollout, ReplayEnvRollerBase, ReplayEnvRollerFactoryBase, RlModel, ReplayBuffer, ReplayBufferFactory\nfrom vel.openai.baselines.common.running_mean_std import RunningMeanStd\nfrom vel.api import BatchInfo, ModelFactory\nimport typing\n\nclass TransitionReplayEnvRoller(ReplayEnvRollerBase):\n    \"\"\"\n    Calculate environment rollouts using a replay buffer for experience replay.\n    Replay buffer is parametrized\n    Samples transitions from the replay buffer (individual frame transitions)\n    \"\"\"\n\n    def __init__(self, environment, device, replay_buffer: ReplayBuffer, discount_factor: typing.Optional[float]=None, normalize_returns: bool=False, forward_steps: int=1, action_noise: typing.Optional[nn.Module]=None):\n        self._environment = environment\n        self.device = device\n        self.replay_buffer = replay_buffer\n        self.normalize_returns = normalize_returns\n        self.forward_steps = forward_steps\n        self.discount_factor = discount_factor\n        self.action_noise = action_noise.to(self.device) if action_noise is not None else None\n        if self.normalize_returns:\n            assert self.discount_factor is not None, 'TransitionReplayEnvRoller must have a discount factor defined if normalize_returns is turned on'\n        if self.forward_steps > 1:\n            assert self.discount_factor is not None, 'TransitionReplayEnvRoller must have a discount factor defined if forward_steps is larger than one'\n        self.ret_rms = RunningMeanStd(shape=()) if normalize_returns else None\n        self.last_observation_cpu = torch.from_numpy(self.environment.reset()).clone()\n        self.last_observation = self.last_observation_cpu.to(self.device)\n        self.clip_obs = 5.0\n        self.accumulated_returns = np.zeros(environment.num_envs, dtype=np.float32)\n\n    @property\n    def environment(self):\n        \"\"\" Return environment of this env roller \"\"\"\n        return self._environment\n\n    @torch.no_grad()\n    def rollout(self, batch_info: BatchInfo, model: RlModel, number_of_steps: int) -> Rollout:\n        \"\"\" Calculate env rollout \"\"\"\n        assert not model.is_recurrent, 'Replay env roller does not support recurrent models'\n        accumulator = TensorAccumulator()\n        episode_information = []\n        for step_idx in range(number_of_steps):\n            step = model.step(self.last_observation)\n            if self.action_noise is not None:\n                step['actions'] = self.action_noise(step['actions'], batch_info=batch_info)\n            replay_extra_information = {}\n            accumulator.add('observations', self.last_observation_cpu)\n            for name, tensor in step.items():\n                tensor_cpu = tensor.cpu()\n                accumulator.add(name, tensor_cpu)\n                if name != 'actions':\n                    replay_extra_information[name] = tensor_cpu.numpy()\n            actions_numpy = step['actions'].detach().cpu().numpy()\n            new_obs, new_rewards, new_dones, new_infos = self.environment.step(actions_numpy)\n            self.replay_buffer.store_transition(frame=self.last_observation_cpu.numpy(), action=actions_numpy, reward=new_rewards, done=new_dones, extra_info=replay_extra_information)\n            if self.ret_rms is not None:\n                self.accumulated_returns = new_rewards + self.discount_factor * self.accumulated_returns\n                self.ret_rms.update(self.accumulated_returns)\n            dones_tensor = torch.from_numpy(new_dones.astype(np.float32)).clone()\n            accumulator.add('dones', dones_tensor)\n            if self.action_noise is not None:\n                self.action_noise.reset_training_state(dones_tensor, batch_info=batch_info)\n            self.accumulated_returns = self.accumulated_returns * (1.0 - new_dones.astype(np.float32))\n            self.last_observation_cpu = torch.from_numpy(new_obs).clone()\n            self.last_observation = self.last_observation_cpu.to(self.device)\n            if self.ret_rms is not None:\n                new_rewards = np.clip(new_rewards / np.sqrt(self.ret_rms.var + 1e-08), -self.clip_obs, self.clip_obs)\n            accumulator.add('rewards', torch.from_numpy(new_rewards.astype(np.float32)).clone())\n            episode_information.append(new_infos)\n        accumulated_tensors = accumulator.result()\n        return Trajectories(num_steps=accumulated_tensors['observations'].size(0), num_envs=accumulated_tensors['observations'].size(1), environment_information=episode_information, transition_tensors=accumulated_tensors, rollout_tensors={}).to_transitions()\n\n    def sample(self, batch_info: BatchInfo, model: RlModel, number_of_steps: int) -> Rollout:\n        \"\"\" Sample experience from replay buffer and return a batch \"\"\"\n        if self.forward_steps > 1:\n            transitions = self.replay_buffer.sample_forward_transitions(batch_size=number_of_steps, batch_info=batch_info, forward_steps=self.forward_steps, discount_factor=self.discount_factor)\n        else:\n            transitions = self.replay_buffer.sample_transitions(batch_size=number_of_steps, batch_info=batch_info)\n        if self.ret_rms is not None:\n            rewards = transitions.transition_tensors['rewards']\n            new_rewards = torch.clamp(rewards / np.sqrt(self.ret_rms.var + 1e-08), -self.clip_obs, self.clip_obs)\n            transitions.transition_tensors['rewards'] = new_rewards\n        return transitions\n\n    def is_ready_for_sampling(self) -> bool:\n        \"\"\" If buffer is ready for drawing samples from it (usually checks if there is enough data) \"\"\"\n        return self.replay_buffer.is_ready_for_sampling()\n\n    def initial_memory_size_hint(self) -> typing.Optional[int]:\n        \"\"\" Hint how much data is needed to begin sampling, required only for diagnostics \"\"\"\n        return self.replay_buffer.initial_memory_size_hint()\n\n    def update(self, rollout, batch_info):\n        \"\"\" Perform update of the internal state of the buffer - e.g. for the prioritized replay weights \"\"\"\n        self.replay_buffer.update(rollout, batch_info)\n\nExample skeleton:\n\n\nclass PPO(object):\n    '''A vectorized implementation of the PPO algorithm by John Schulman.'''\n\n    def __init__(self, batch_env, step, is_training, should_log, config):\n        '''Create an instance of the PPO algorithm.\n        Args:\n          batch_env: In-graph batch environment.\n          step: Integer tensor holding the current training step.\n          is_training: Boolean tensor for whether the algorithm should train.\n          should_log: Boolean tensor for whether summaries should be returned.\n          config: Object containing the agent configuration as attributes.\n        '''\n        pass\n\n                    def var_like(x):\n                                        pass\n\n    def begin_episode(self, agent_indices):\n        '''Reset the recurrent states and stored episode.\n        Args:\n          agent_indices: Tensor containing current batch indices.\n        Returns:\n          Summary tensor.\n        '''\n        pass\n\n    def perform(self, agent_indices, observ):\n        '''Compute batch of actions and a summary for a batch of observation.\n        Args:\n          agent_indices: Tensor containing current batch indices.\n          observ: Tensor of a batch of observations for all agents.\n        Returns:\n          Tuple of action batch tensor and summary tensor.\n        '''\n        pass\n\n    def experience(self, agent_indices, observ, action, reward, unused_done, unused_nextob):\n        '''Process the transition tuple of the current step.\n        When training, add the current transition tuple to the memory and update\n        the streaming statistics for observations and rewards. A summary string is\n        returned if requested at this step.\n        Args:\n          agent_indices: Tensor containing current batch indices.\n          observ: Batch tensor of observations.\n          action: Batch tensor of actions.\n          reward: Batch tensor of rewards.\n          unused_done: Batch tensor of done flags.\n          unused_nextob: Batch tensor of successor observations.\n        Returns:\n          Summary tensor.\n        '''\n        pass\n\n    def _define_experience(self, agent_indices, observ, action, reward):\n        '''Implement the branch of experience() entered during training.'''\n        pass\n\n    def end_episode(self, agent_indices):\n        '''Add episodes to the memory and perform update steps if memory is full.\n        During training, add the collected episodes of the batch indices that\n        finished their episode to the memory. If the memory is full, train on it,\n        and then clear the memory. A summary string is returned if requested at\n        this step.\n        Args:\n          agent_indices: Tensor containing current batch indices.\n        Returns:\n           Summary tensor.\n        '''\n        pass\n\n    def _initialize_policy(self):\n        '''Initialize the policy.\n        Run the policy network on dummy data to initialize its parameters for later\n        reuse and to analyze the policy distribution. Initializes the attributes\n        `self._network` and `self._policy_type`.\n        Raises:\n          ValueError: Invalid policy distribution.\n        Returns:\n          Parameters of the policy distribution and policy state.\n        '''\n        pass\n\n        def is_tensor(x):\n                pass\n\n        def set_batch_dim(x):\n                pass\n\n    def _initialize_memory(self, policy_params):\n        '''Initialize temporary and permanent memory.\n        Args:\n          policy_params: Nested tuple of policy parameters with all dimensions set.\n        Initializes the attributes `self._current_episodes`,\n        `self._finished_episodes`, and `self._num_finished_episodes`. The episodes\n        memory serves to collect multiple episodes in parallel. Finished episodes\n        are copied into the next free slot of the second memory. The memory index\n        points to the next free slot.\n        '''\n        pass\n\n    def _define_end_episode(self, agent_indices):\n        '''Implement the branch of end_episode() entered during training.'''\n        pass\n\n    def _training(self):\n        '''Perform multiple training iterations of both policy and value baseline.\n        Training on the episodes collected in the memory. Reset the memory\n        afterwards. Always returns a summary string.\n        Returns:\n          Summary tensor.\n        '''\n        pass\n\n    def _perform_update_steps(self, observ, action, old_policy_params, reward, length):\n        '''Perform multiple update steps of value function and policy.\n        The advantage is computed once at the beginning and shared across\n        iterations. We need to decide for the summary of one iteration, and thus\n        choose the one after half of the iterations.\n        Args:\n          observ: Sequences of observations.\n          action: Sequences of actions.\n          old_policy_params: Parameters of the behavioral policy.\n          reward: Sequences of rewards.\n          length: Batch of sequence lengths.\n        Returns:\n          Summary tensor.\n        '''\n        pass\n\n    def _update_step(self, sequence):\n        '''Compute the current combined loss and perform a gradient update step.\n        The sequences must be a dict containing the keys `length` and `sequence`,\n        where the latter is a tuple containing observations, actions, parameters of\n        the behavioral policy, rewards, and advantages.\n        Args:\n          sequence: Sequences of episodes or chunks of episodes.\n        Returns:\n          Tuple of value loss, policy loss, and summary tensor.\n        '''\n        pass\n\n    def _value_loss(self, observ, reward, length):\n        '''Compute the loss function for the value baseline.\n        The value loss is the difference between empirical and approximated returns\n        over the collected episodes. Returns the loss tensor and a summary strin.\n        Args:\n          observ: Sequences of observations.\n          reward: Sequences of reward.\n          length: Batch of sequence lengths.\n        Returns:\n          Tuple of loss tensor and summary tensor.\n        '''\n        pass\n\n    def _policy_loss(self, old_policy, policy, action, advantage, length):\n        '''Compute the policy loss composed of multiple components.\n        1. The policy gradient loss is importance sampled from the data-collecting\n           policy at the beginning of training.\n        2. The second term is a KL penalty between the policy at the beginning of\n           training and the current policy.\n        3. Additionally, if this KL already changed more than twice the target\n           amount, we activate a strong penalty discouraging further divergence.\n        Args:\n          old_policy: Action distribution of the behavioral policy.\n          policy: Sequences of distribution params of the current policy.\n          action: Sequences of actions.\n          advantage: Sequences of advantages.\n          length: Batch of sequence lengths.\n        Returns:\n          Tuple of loss tensor and summary tensor.\n        '''\n        pass\n\n    def _adjust_penalty(self, observ, old_policy_params, length):\n        '''Adjust the KL policy between the behavioral and current policy.\n        Compute how much the policy actually changed during the multiple\n        update steps. Adjust the penalty strength for the next training phase if we\n        overshot or undershot the target divergence too much.\n        Args:\n          observ: Sequences of observations.\n          old_policy_params: Parameters of the behavioral policy.\n          length: Batch of sequence lengths.\n        Returns:\n          Summary tensor.\n        '''\n        pass\n\n    def _mask(self, tensor, length, padding_value=0):\n        '''Set padding elements of a batch of sequences to a constant.\n        Useful for setting padding elements to zero before summing along the time\n        dimension, or for preventing infinite results in padding elements.\n        Args:\n          tensor: Tensor of sequences.\n          length: Batch of sequence lengths.\n          padding_value: Value to write into padding elements.\n        Returns:\n          Masked sequences.\n        '''\n        pass\nCorresponding class implementation:\n\nfrom agents import parts\nimport functools\nimport tensorflow as tf\nfrom agents.algorithms.ppo import utility\nfrom agents import tools\n\nclass PPO(object):\n    \"\"\"A vectorized implementation of the PPO algorithm by John Schulman.\"\"\"\n\n    def __init__(self, batch_env, step, is_training, should_log, config):\n        \"\"\"Create an instance of the PPO algorithm.\n\n        Args:\n          batch_env: In-graph batch environment.\n          step: Integer tensor holding the current training step.\n          is_training: Boolean tensor for whether the algorithm should train.\n          should_log: Boolean tensor for whether summaries should be returned.\n          config: Object containing the agent configuration as attributes.\n        \"\"\"\n        self._batch_env = batch_env\n        self._step = step\n        self._is_training = is_training\n        self._should_log = should_log\n        self._config = config\n        self._observ_filter = parts.StreamingNormalize(self._batch_env.observ[0], center=True, scale=True, clip=5, name='normalize_observ')\n        self._reward_filter = parts.StreamingNormalize(self._batch_env.reward[0], center=False, scale=True, clip=10, name='normalize_reward')\n        self._use_gpu = self._config.use_gpu and utility.available_gpus()\n        policy_params, state = self._initialize_policy()\n        self._initialize_memory(policy_params)\n        with tf.device('/gpu:0' if self._use_gpu else '/cpu:0'):\n            self._optimizer = self._config.optimizer(self._config.learning_rate)\n        self._penalty = tf.Variable(self._config.kl_init_penalty, False, dtype=tf.float32)\n        with tf.variable_scope('ppo_temporary'):\n            with tf.device('/gpu:0'):\n                if state is None:\n                    self._last_state = None\n                else:\n\n                    def var_like(x):\n                        return tf.Variable(lambda: tf.zeros_like(x), False)\n                    self._last_state = tools.nested.map(var_like, state)\n        with tf.variable_scope('ppo_temporary'):\n            self._last_action = tf.Variable(tf.zeros_like(self._batch_env.action), False, name='last_action')\n            self._last_policy = tools.nested.map(lambda x: tf.Variable(tf.zeros_like(x[:, 0], False)), policy_params)\n\n    def begin_episode(self, agent_indices):\n        \"\"\"Reset the recurrent states and stored episode.\n\n        Args:\n          agent_indices: Tensor containing current batch indices.\n\n        Returns:\n          Summary tensor.\n        \"\"\"\n        with tf.name_scope('begin_episode/'):\n            if self._last_state is None:\n                reset_state = tf.no_op()\n            else:\n                reset_state = utility.reinit_nested_vars(self._last_state, agent_indices)\n            reset_buffer = self._current_episodes.clear(agent_indices)\n            with tf.control_dependencies([reset_state, reset_buffer]):\n                return tf.constant('')\n\n    def perform(self, agent_indices, observ):\n        \"\"\"Compute batch of actions and a summary for a batch of observation.\n\n        Args:\n          agent_indices: Tensor containing current batch indices.\n          observ: Tensor of a batch of observations for all agents.\n\n        Returns:\n          Tuple of action batch tensor and summary tensor.\n        \"\"\"\n        with tf.name_scope('perform/'):\n            observ = self._observ_filter.transform(observ)\n            if self._last_state is None:\n                state = None\n            else:\n                state = tools.nested.map(lambda x: tf.gather(x, agent_indices), self._last_state)\n            with tf.device('/gpu:0' if self._use_gpu else '/cpu:0'):\n                output = self._network(observ[:, None], tf.ones(observ.shape[0]), state)\n            action = tf.cond(self._is_training, output.policy.sample, output.policy.mode)\n            logprob = output.policy.log_prob(action)[:, 0]\n            summary = tf.cond(self._should_log, lambda: tf.summary.merge([tf.summary.histogram('mode', output.policy.mode()[:, 0]), tf.summary.histogram('action', action[:, 0]), tf.summary.histogram('logprob', logprob)]), str)\n            if self._last_state is None:\n                assign_state = tf.no_op()\n            else:\n                assign_state = utility.assign_nested_vars(self._last_state, output.state, agent_indices)\n            remember_last_action = tf.scatter_update(self._last_action, agent_indices, action[:, 0])\n            policy_params = tools.nested.filter(lambda x: isinstance(x, tf.Tensor), output.policy.parameters)\n            assert policy_params, 'Policy has no parameters to store.'\n            remember_last_policy = tools.nested.map(lambda var, val: tf.scatter_update(var, agent_indices, val[:, 0]), self._last_policy, policy_params, flatten=True)\n            with tf.control_dependencies((assign_state, remember_last_action) + remember_last_policy):\n                return (action[:, 0], tf.identity(summary))\n\n    def experience(self, agent_indices, observ, action, reward, unused_done, unused_nextob):\n        \"\"\"Process the transition tuple of the current step.\n\n        When training, add the current transition tuple to the memory and update\n        the streaming statistics for observations and rewards. A summary string is\n        returned if requested at this step.\n\n        Args:\n          agent_indices: Tensor containing current batch indices.\n          observ: Batch tensor of observations.\n          action: Batch tensor of actions.\n          reward: Batch tensor of rewards.\n          unused_done: Batch tensor of done flags.\n          unused_nextob: Batch tensor of successor observations.\n\n        Returns:\n          Summary tensor.\n        \"\"\"\n        with tf.name_scope('experience/'):\n            return tf.cond(self._is_training, lambda: self._define_experience(agent_indices, observ, action, reward), str)\n\n    def _define_experience(self, agent_indices, observ, action, reward):\n        \"\"\"Implement the branch of experience() entered during training.\"\"\"\n        update_filters = tf.summary.merge([self._observ_filter.update(observ), self._reward_filter.update(reward)])\n        with tf.control_dependencies([update_filters]):\n            if self._config.train_on_agent_action:\n                action = self._last_action\n            policy = tools.nested.map(lambda x: tf.gather(x, agent_indices), self._last_policy)\n            batch = (observ, action, policy, reward)\n            append = self._current_episodes.append(batch, agent_indices)\n        with tf.control_dependencies([append]):\n            norm_observ = self._observ_filter.transform(observ)\n            norm_reward = tf.reduce_mean(self._reward_filter.transform(reward))\n            summary = tf.cond(self._should_log, lambda: tf.summary.merge([update_filters, self._observ_filter.summary(), self._reward_filter.summary(), tf.summary.scalar('memory_size', self._num_finished_episodes), tf.summary.histogram('normalized_observ', norm_observ), tf.summary.histogram('action', self._last_action), tf.summary.scalar('normalized_reward', norm_reward)]), str)\n            return summary\n\n    def end_episode(self, agent_indices):\n        \"\"\"Add episodes to the memory and perform update steps if memory is full.\n\n        During training, add the collected episodes of the batch indices that\n        finished their episode to the memory. If the memory is full, train on it,\n        and then clear the memory. A summary string is returned if requested at\n        this step.\n\n        Args:\n          agent_indices: Tensor containing current batch indices.\n\n        Returns:\n           Summary tensor.\n        \"\"\"\n        with tf.name_scope('end_episode/'):\n            return tf.cond(self._is_training, lambda: self._define_end_episode(agent_indices), str)\n\n    def _initialize_policy(self):\n        \"\"\"Initialize the policy.\n\n        Run the policy network on dummy data to initialize its parameters for later\n        reuse and to analyze the policy distribution. Initializes the attributes\n        `self._network` and `self._policy_type`.\n\n        Raises:\n          ValueError: Invalid policy distribution.\n\n        Returns:\n          Parameters of the policy distribution and policy state.\n        \"\"\"\n        with tf.device('/gpu:0' if self._use_gpu else '/cpu:0'):\n            network = functools.partial(self._config.network, self._config, self._batch_env.action_space)\n            self._network = tf.make_template('network', network)\n            output = self._network(tf.zeros_like(self._batch_env.observ)[:, None], tf.ones(len(self._batch_env)))\n        if output.policy.event_shape != self._batch_env.action.shape[1:]:\n            message = 'Policy event shape {} does not match action shape {}.'\n            message = message.format(output.policy.event_shape, self._batch_env.action.shape[1:])\n            raise ValueError(message)\n        self._policy_type = type(output.policy)\n\n        def is_tensor(x):\n            return isinstance(x, tf.Tensor)\n        policy_params = tools.nested.filter(is_tensor, output.policy.parameters)\n\n        def set_batch_dim(x):\n            return utility.set_dimension(x, 0, len(self._batch_env))\n        tools.nested.map(set_batch_dim, policy_params)\n        if output.state is not None:\n            tools.nested.map(set_batch_dim, output.state)\n        return (policy_params, output.state)\n\n    def _initialize_memory(self, policy_params):\n        \"\"\"Initialize temporary and permanent memory.\n\n        Args:\n          policy_params: Nested tuple of policy parameters with all dimensions set.\n\n        Initializes the attributes `self._current_episodes`,\n        `self._finished_episodes`, and `self._num_finished_episodes`. The episodes\n        memory serves to collect multiple episodes in parallel. Finished episodes\n        are copied into the next free slot of the second memory. The memory index\n        points to the next free slot.\n        \"\"\"\n        template = (self._batch_env.observ[0], self._batch_env.action[0], tools.nested.map(lambda x: x[0, 0], policy_params), self._batch_env.reward[0])\n        with tf.variable_scope('ppo_temporary'):\n            self._current_episodes = parts.EpisodeMemory(template, len(self._batch_env), self._config.max_length, 'episodes')\n        self._finished_episodes = parts.EpisodeMemory(template, self._config.update_every, self._config.max_length, 'memory')\n        self._num_finished_episodes = tf.Variable(0, False)\n\n    def _define_end_episode(self, agent_indices):\n        \"\"\"Implement the branch of end_episode() entered during training.\"\"\"\n        episodes, length = self._current_episodes.data(agent_indices)\n        space_left = self._config.update_every - self._num_finished_episodes\n        use_episodes = tf.range(tf.minimum(tf.shape(agent_indices)[0], space_left))\n        episodes = tools.nested.map(lambda x: tf.gather(x, use_episodes), episodes)\n        append = self._finished_episodes.replace(episodes, tf.gather(length, use_episodes), use_episodes + self._num_finished_episodes)\n        with tf.control_dependencies([append]):\n            increment_index = self._num_finished_episodes.assign_add(tf.shape(use_episodes)[0])\n        with tf.control_dependencies([increment_index]):\n            memory_full = self._num_finished_episodes >= self._config.update_every\n            return tf.cond(memory_full, self._training, str)\n\n    def _training(self):\n        \"\"\"Perform multiple training iterations of both policy and value baseline.\n\n        Training on the episodes collected in the memory. Reset the memory\n        afterwards. Always returns a summary string.\n\n        Returns:\n          Summary tensor.\n        \"\"\"\n        with tf.device('/gpu:0' if self._use_gpu else '/cpu:0'):\n            with tf.name_scope('training'):\n                assert_full = tf.assert_equal(self._num_finished_episodes, self._config.update_every)\n                with tf.control_dependencies([assert_full]):\n                    data = self._finished_episodes.data()\n                (observ, action, old_policy_params, reward), length = data\n                old_policy_params = tools.nested.map(lambda param: self._mask(param, length, 1), old_policy_params)\n                with tf.control_dependencies([tf.assert_greater(length, 0)]):\n                    length = tf.identity(length)\n                observ = self._observ_filter.transform(observ)\n                reward = self._reward_filter.transform(reward)\n                update_summary = self._perform_update_steps(observ, action, old_policy_params, reward, length)\n                with tf.control_dependencies([update_summary]):\n                    penalty_summary = self._adjust_penalty(observ, old_policy_params, length)\n                with tf.control_dependencies([penalty_summary]):\n                    clear_memory = tf.group(self._finished_episodes.clear(), self._num_finished_episodes.assign(0))\n                with tf.control_dependencies([clear_memory]):\n                    weight_summary = utility.variable_summaries(tf.trainable_variables(), self._config.weight_summaries)\n                    return tf.summary.merge([update_summary, penalty_summary, weight_summary])\n\n    def _perform_update_steps(self, observ, action, old_policy_params, reward, length):\n        \"\"\"Perform multiple update steps of value function and policy.\n\n        The advantage is computed once at the beginning and shared across\n        iterations. We need to decide for the summary of one iteration, and thus\n        choose the one after half of the iterations.\n\n        Args:\n          observ: Sequences of observations.\n          action: Sequences of actions.\n          old_policy_params: Parameters of the behavioral policy.\n          reward: Sequences of rewards.\n          length: Batch of sequence lengths.\n\n        Returns:\n          Summary tensor.\n        \"\"\"\n        return_ = utility.discounted_return(reward, length, self._config.discount)\n        value = self._network(observ, length).value\n        if self._config.gae_lambda:\n            advantage = utility.lambda_advantage(reward, value, length, self._config.discount, self._config.gae_lambda)\n        else:\n            advantage = return_ - value\n        mean, variance = tf.nn.moments(advantage, axes=[0, 1], keep_dims=True)\n        advantage = (advantage - mean) / (tf.sqrt(variance) + 1e-08)\n        advantage = tf.Print(advantage, [tf.reduce_mean(return_), tf.reduce_mean(value)], 'return and value: ')\n        advantage = tf.Print(advantage, [tf.reduce_mean(advantage)], 'normalized advantage: ')\n        episodes = (observ, action, old_policy_params, reward, advantage)\n        value_loss, policy_loss, summary = parts.iterate_sequences(self._update_step, [0.0, 0.0, ''], episodes, length, self._config.chunk_length, self._config.batch_size, self._config.update_epochs, padding_value=1)\n        print_losses = tf.group(tf.Print(0, [tf.reduce_mean(value_loss)], 'value loss: '), tf.Print(0, [tf.reduce_mean(policy_loss)], 'policy loss: '))\n        with tf.control_dependencies([value_loss, policy_loss, print_losses]):\n            return summary[self._config.update_epochs // 2]\n\n    def _update_step(self, sequence):\n        \"\"\"Compute the current combined loss and perform a gradient update step.\n\n        The sequences must be a dict containing the keys `length` and `sequence`,\n        where the latter is a tuple containing observations, actions, parameters of\n        the behavioral policy, rewards, and advantages.\n\n        Args:\n          sequence: Sequences of episodes or chunks of episodes.\n\n        Returns:\n          Tuple of value loss, policy loss, and summary tensor.\n        \"\"\"\n        observ, action, old_policy_params, reward, advantage = sequence['sequence']\n        length = sequence['length']\n        old_policy = self._policy_type(**old_policy_params)\n        value_loss, value_summary = self._value_loss(observ, reward, length)\n        network = self._network(observ, length)\n        policy_loss, policy_summary = self._policy_loss(old_policy, network.policy, action, advantage, length)\n        network_loss = network.get('loss', 0.0)\n        loss = policy_loss + value_loss + tf.reduce_mean(network_loss)\n        gradients, variables = zip(*self._optimizer.compute_gradients(loss))\n        optimize = self._optimizer.apply_gradients(zip(gradients, variables))\n        summary = tf.summary.merge([value_summary, policy_summary, tf.summary.histogram('network_loss', network_loss), tf.summary.scalar('avg_network_loss', tf.reduce_mean(network_loss)), tf.summary.scalar('gradient_norm', tf.global_norm(gradients)), utility.gradient_summaries(zip(gradients, variables))])\n        with tf.control_dependencies([optimize]):\n            return [tf.identity(x) for x in (value_loss, policy_loss, summary)]\n\n    def _value_loss(self, observ, reward, length):\n        \"\"\"Compute the loss function for the value baseline.\n\n        The value loss is the difference between empirical and approximated returns\n        over the collected episodes. Returns the loss tensor and a summary strin.\n\n        Args:\n          observ: Sequences of observations.\n          reward: Sequences of reward.\n          length: Batch of sequence lengths.\n\n        Returns:\n          Tuple of loss tensor and summary tensor.\n        \"\"\"\n        with tf.name_scope('value_loss'):\n            value = self._network(observ, length).value\n            return_ = utility.discounted_return(reward, length, self._config.discount)\n            advantage = return_ - value\n            value_loss = 0.5 * self._mask(advantage ** 2, length)\n            summary = tf.summary.merge([tf.summary.histogram('value_loss', value_loss), tf.summary.scalar('avg_value_loss', tf.reduce_mean(value_loss))])\n            value_loss = tf.reduce_mean(value_loss)\n            return (tf.check_numerics(value_loss, 'value_loss'), summary)\n\n    def _policy_loss(self, old_policy, policy, action, advantage, length):\n        \"\"\"Compute the policy loss composed of multiple components.\n\n        1. The policy gradient loss is importance sampled from the data-collecting\n           policy at the beginning of training.\n        2. The second term is a KL penalty between the policy at the beginning of\n           training and the current policy.\n        3. Additionally, if this KL already changed more than twice the target\n           amount, we activate a strong penalty discouraging further divergence.\n\n        Args:\n          old_policy: Action distribution of the behavioral policy.\n          policy: Sequences of distribution params of the current policy.\n          action: Sequences of actions.\n          advantage: Sequences of advantages.\n          length: Batch of sequence lengths.\n\n        Returns:\n          Tuple of loss tensor and summary tensor.\n        \"\"\"\n        with tf.name_scope('policy_loss'):\n            kl = tf.contrib.distributions.kl_divergence(old_policy, policy)\n            kl = tf.check_numerics(kl, 'kl')\n            kl = tf.reduce_mean(self._mask(kl, length), 1)\n            policy_gradient = tf.exp(policy.log_prob(action) - old_policy.log_prob(action))\n            surrogate_loss = -tf.reduce_mean(self._mask(policy_gradient * tf.stop_gradient(advantage), length), 1)\n            surrogate_loss = tf.check_numerics(surrogate_loss, 'surrogate_loss')\n            kl_penalty = self._penalty * kl\n            cutoff_threshold = self._config.kl_target * self._config.kl_cutoff_factor\n            cutoff_count = tf.reduce_sum(tf.cast(kl > cutoff_threshold, tf.int32))\n            with tf.control_dependencies([tf.cond(cutoff_count > 0, lambda: tf.Print(0, [cutoff_count], 'kl cutoff! '), int)]):\n                kl_cutoff = self._config.kl_cutoff_coef * tf.cast(kl > cutoff_threshold, tf.float32) * (kl - cutoff_threshold) ** 2\n            policy_loss = surrogate_loss + kl_penalty + kl_cutoff\n            entropy = tf.reduce_mean(policy.entropy(), axis=1)\n            if self._config.entropy_regularization:\n                policy_loss -= self._config.entropy_regularization * entropy\n            summary = tf.summary.merge([tf.summary.histogram('entropy', entropy), tf.summary.histogram('kl', kl), tf.summary.histogram('surrogate_loss', surrogate_loss), tf.summary.histogram('kl_penalty', kl_penalty), tf.summary.histogram('kl_cutoff', kl_cutoff), tf.summary.histogram('kl_penalty_combined', kl_penalty + kl_cutoff), tf.summary.histogram('policy_loss', policy_loss), tf.summary.scalar('avg_surr_loss', tf.reduce_mean(surrogate_loss)), tf.summary.scalar('avg_kl_penalty', tf.reduce_mean(kl_penalty)), tf.summary.scalar('avg_policy_loss', tf.reduce_mean(policy_loss))])\n            policy_loss = tf.reduce_mean(policy_loss, 0)\n            return (tf.check_numerics(policy_loss, 'policy_loss'), summary)\n\n    def _adjust_penalty(self, observ, old_policy_params, length):\n        \"\"\"Adjust the KL policy between the behavioral and current policy.\n\n        Compute how much the policy actually changed during the multiple\n        update steps. Adjust the penalty strength for the next training phase if we\n        overshot or undershot the target divergence too much.\n\n        Args:\n          observ: Sequences of observations.\n          old_policy_params: Parameters of the behavioral policy.\n          length: Batch of sequence lengths.\n\n        Returns:\n          Summary tensor.\n        \"\"\"\n        old_policy = self._policy_type(**old_policy_params)\n        with tf.name_scope('adjust_penalty'):\n            network = self._network(observ, length)\n            print_penalty = tf.Print(0, [self._penalty], 'current penalty: ')\n            with tf.control_dependencies([print_penalty]):\n                kl_change = tf.reduce_mean(self._mask(tf.contrib.distributions.kl_divergence(old_policy, network.policy), length))\n                kl_change = tf.Print(kl_change, [kl_change], 'kl change: ')\n                maybe_increase = tf.cond(kl_change > 1.3 * self._config.kl_target, lambda: tf.Print(self._penalty.assign(self._penalty * 1.5), [0], 'increase penalty '), float)\n                maybe_decrease = tf.cond(kl_change < 0.7 * self._config.kl_target, lambda: tf.Print(self._penalty.assign(self._penalty / 1.5), [0], 'decrease penalty '), float)\n            with tf.control_dependencies([maybe_increase, maybe_decrease]):\n                return tf.summary.merge([tf.summary.scalar('kl_change', kl_change), tf.summary.scalar('penalty', self._penalty)])\n\n    def _mask(self, tensor, length, padding_value=0):\n        \"\"\"Set padding elements of a batch of sequences to a constant.\n\n        Useful for setting padding elements to zero before summing along the time\n        dimension, or for preventing infinite results in padding elements.\n\n        Args:\n          tensor: Tensor of sequences.\n          length: Batch of sequence lengths.\n          padding_value: Value to write into padding elements.\n\n        Returns:\n          Masked sequences.\n        \"\"\"\n        with tf.name_scope('mask'):\n            range_ = tf.range(tensor.shape[1].value)\n            mask = range_[None, :] < length[:, None]\n            if tensor.shape.ndims > 2:\n                for _ in range(tensor.shape.ndims - 2):\n                    mask = mask[..., None]\n                mask = tf.tile(mask, [1, 1] + tensor.shape[2:].as_list())\n            masked = tf.where(mask, tensor, padding_value * tf.ones_like(tensor))\n            return tf.check_numerics(masked, 'masked')\n\nNow, implement the following class. Do not explain the code. The given class skeleton is as follows:\n\nclass UltrasoundStateMachine:\n    '''State machine for ultrasound procedure control.'''\n\n    def __init__(self, modules: Dict[str, BaseControlModule], device: str='cuda:0'):\n        '''Initialize the state machine.\n        Args:\n            modules (Dict[str, BaseControlModule]): Dictionary of control modules\n            device (str): Device to run computations on\n        '''\n        pass\n\n    def compute_action(self, env, robot_obs: torch.Tensor) -> torch.Tensor:\n        '''Compute combined action from all active modules.\n        Args:\n            env: The environment instance\n            robot_obs: Current robot observations\n        Returns:\n            Tuple of relative and absolute commands\n        '''\n        pass\n\n    def get_normal_force(self):\n        '''\n        Get the average normal force from the contact sensor.\n        '''\n        pass\n\n    def reset(self) -> None:\n        '''Reset the state machine and all modules.'''\n        pass\n\n    def assert_module_order(self):\n        '''Assert that the modules are in the correct order.'''\n        pass"
  }
]